{
    "query": "What specific cultural biases and language artifacts are introduced when translating English evaluation benchmarks into non-English languages, and how do they impact model performance and reliability assessments?",
    "user_id": "lib_user",
    "task_id": "eaccf800-c7c5-4a83-8367-7d542c0795ad",
    "timestamp": "2025-06-23T22:12:57.192840",
    "n_retrieval": 256,
    "n_retrieved": 263,
    "n_candidates": 17,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.228195,
    "decomposed_query": {
        "rewritten_query": "Cultural biases and language artifacts introduced when translating English evaluation benchmarks into non-English languages, and their impact on model performance and reliability assessments.",
        "keyword_query": "cultural biases language artifacts translating English evaluation benchmarks non-English languages impact model performance reliability assessments",
        "search_filters": {
            "fieldsOfStudy": "Linguistics,Computer Science"
        },
        "cost": 0.010071,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 34,
            "influential_citation_count": 7,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.03304, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2283844788",
                    "name": "Shivalika Singh"
                },
                {
                    "authorId": "1910588458",
                    "name": "Angelika Romanou"
                },
                {
                    "authorId": "2080941785",
                    "name": "Cl\u00e9mentine Fourrier"
                },
                {
                    "authorId": "2518906",
                    "name": "David Ifeoluwa Adelani"
                },
                {
                    "authorId": "119076558",
                    "name": "Jian Gang Ngui"
                },
                {
                    "authorId": "1403849795",
                    "name": "Daniel Vila-Suero"
                },
                {
                    "authorId": "1596821065",
                    "name": "Peerat Limkonchotiwat"
                },
                {
                    "authorId": "1396188646",
                    "name": "Kelly Marchisio"
                },
                {
                    "authorId": "2140097897",
                    "name": "Wei Qi Leong"
                },
                {
                    "authorId": "2239100505",
                    "name": "Yosephine Susanto"
                },
                {
                    "authorId": "2352918897",
                    "name": "Raymond Ng"
                },
                {
                    "authorId": "2283848744",
                    "name": "Shayne Longpre"
                },
                {
                    "authorId": "2309005865",
                    "name": "Wei-Yin Ko"
                },
                {
                    "authorId": "2303318993",
                    "name": "Madeline Smith"
                },
                {
                    "authorId": "2284866282",
                    "name": "Antoine Bosselut"
                },
                {
                    "authorId": "2320522443",
                    "name": "Alice Oh"
                },
                {
                    "authorId": "2334310309",
                    "name": "Andr\u00e9 F. T. Martins"
                },
                {
                    "authorId": "2283849613",
                    "name": "Leshem Choshen"
                },
                {
                    "authorId": "2290769562",
                    "name": "Daphne Ippolito"
                },
                {
                    "authorId": "2268314228",
                    "name": "Enzo Ferrante"
                },
                {
                    "authorId": "2818759",
                    "name": "Marzieh Fadaee"
                },
                {
                    "authorId": "2445273",
                    "name": "B. Ermi\u015f"
                },
                {
                    "authorId": "2257040307",
                    "name": "Sara Hooker"
                }
            ],
            "abstract": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
            "corpus_id": 274464561,
            "sentences": [
                {
                    "corpus_id": "274464561",
                    "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
                    "text": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
                    "score": 0.5682764628363636,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.921875
                }
            ],
            "relevance_judgement": 0.921875,
            "relevance_judgment_input_expanded": "# Title: Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation\n# Venue: arXiv.org\n# Authors: Shivalika Singh, Angelika Romanou, Cl\u00e9mentine Fourrier, David Ifeoluwa Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andr\u00e9 F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, B. Ermi\u015f, Sara Hooker\n## Abstract\nCultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.\n",
            "reference_string": "[274464561 | Singh et al. | 2024 | Citations: 34]"
        },
        {
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "venue": "Frontiers Comput. Sci.",
            "year": 2024,
            "reference_count": 185,
            "citation_count": 43,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.00929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2257136845",
                    "name": "Yuemei Xu"
                },
                {
                    "authorId": "2258334185",
                    "name": "Ling Hu"
                },
                {
                    "authorId": "2294513520",
                    "name": "Jiayi Zhao"
                },
                {
                    "authorId": "2294361104",
                    "name": "Zihan Qiu"
                },
                {
                    "authorId": "2294363807",
                    "name": "Yuqi Ye"
                },
                {
                    "authorId": "2294933103",
                    "name": "Hanwen Gu"
                }
            ],
            "abstract": "\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.",
            "corpus_id": 268819377,
            "sentences": [
                {
                    "corpus_id": "268819377",
                    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                    "text": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets [12].",
                    "score": 0.4757768092357937,
                    "section_title": "B. Bias Benchmark",
                    "char_start_offset": 50674,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 148
                        },
                        {
                            "start": 149,
                            "end": 295
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 290,
                            "end": 294,
                            "matchedPaperCorpusId": "247626152"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63427734375
                },
                {
                    "corpus_id": "268819377",
                    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                    "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
                    "score": 0.5743981907572108,
                    "section_title": "B. Bias Benchmark",
                    "char_start_offset": 48672,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 62
                        },
                        {
                            "start": 63,
                            "end": 172
                        },
                        {
                            "start": 173,
                            "end": 239
                        },
                        {
                            "start": 240,
                            "end": 306
                        },
                        {
                            "start": 307,
                            "end": 461
                        },
                        {
                            "start": 462,
                            "end": 552
                        },
                        {
                            "start": 555,
                            "end": 727
                        },
                        {
                            "start": 730,
                            "end": 916
                        },
                        {
                            "start": 917,
                            "end": 1241
                        },
                        {
                            "start": 1244,
                            "end": 1339
                        },
                        {
                            "start": 1340,
                            "end": 1549
                        },
                        {
                            "start": 1552,
                            "end": 1672
                        },
                        {
                            "start": 1673,
                            "end": 1799
                        },
                        {
                            "start": 1800,
                            "end": 1892
                        },
                        {
                            "start": 1893,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2150
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 399,
                            "end": 404,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 406,
                            "end": 411,
                            "matchedPaperCorpusId": "173991101"
                        },
                        {
                            "start": 448,
                            "end": 453,
                            "matchedPaperCorpusId": "21670658"
                        },
                        {
                            "start": 455,
                            "end": 460,
                            "matchedPaperCorpusId": "58006082"
                        },
                        {
                            "start": 540,
                            "end": 545,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 546,
                            "end": 551,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 970,
                            "end": 975,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 977,
                            "end": 982,
                            "matchedPaperCorpusId": "219530686"
                        },
                        {
                            "start": 1076,
                            "end": 1081,
                            "matchedPaperCorpusId": "5034059"
                        },
                        {
                            "start": 1364,
                            "end": 1369,
                            "matchedPaperCorpusId": "13756572"
                        },
                        {
                            "start": 1383,
                            "end": 1388,
                            "matchedPaperCorpusId": "4952494"
                        },
                        {
                            "start": 1455,
                            "end": 1460,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 1649,
                            "end": 1654,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 1666,
                            "end": 1671,
                            "matchedPaperCorpusId": "252547725"
                        },
                        {
                            "start": 1988,
                            "end": 1993,
                            "matchedPaperCorpusId": "135465247"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.61083984375
                }
            ],
            "relevance_judgement": 0.63427734375,
            "relevance_judgment_input_expanded": "# Title: A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias\n# Venue: Frontiers Comput. Sci.\n# Authors: Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu\n## Abstract\n\n Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.\n## B. Bias Benchmark\nThis section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.\n...\nNevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets [12].",
            "reference_string": "[268819377 | Xu et al. | 2024 | Citations: 43]"
        },
        {
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 27,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.12895, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325726142",
                    "name": "Fabio Barth"
                },
                {
                    "authorId": "2302558975",
                    "name": "Georg Rehm"
                }
            ],
            "abstract": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
            "corpus_id": 276421738,
            "sentences": [
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
                    "score": 0.4651950021855545,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.63134765625
                },
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
                    "score": 0.7394108526796094,
                    "section_title": "Challenge 3: Cultural Bias",
                    "char_start_offset": 13020,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 60
                        },
                        {
                            "start": 61,
                            "end": 150
                        },
                        {
                            "start": 151,
                            "end": 328
                        },
                        {
                            "start": 329,
                            "end": 444
                        },
                        {
                            "start": 445,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 607
                        },
                        {
                            "start": 608,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 943
                        },
                        {
                            "start": 946,
                            "end": 1018
                        },
                        {
                            "start": 1019,
                            "end": 1193
                        },
                        {
                            "start": 1194,
                            "end": 1304
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 149,
                            "matchedPaperCorpusId": "265445838"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52978515625
                },
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "There are already models and metrics that can measure and score translation quality (Rei et al., 2020). \n\nThe second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures. \n\nHowever, there are already some efforts to translate these benchmarks into multiple languages with regard to cultural biases. Note that these approaches involve human annotators that correct these biases (Singh et al., 2024). \n\nThis paper analyses the most commonly used benchmarks for multilingual LLMs for European languages. We highlight the benefits and interpretability of the existing benchmarks and outline their limitations.",
                    "score": 0.6255590263807864,
                    "section_title": "Introduction",
                    "char_start_offset": 3603,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 103
                        },
                        {
                            "start": 106,
                            "end": 278
                        },
                        {
                            "start": 279,
                            "end": 451
                        },
                        {
                            "start": 452,
                            "end": 587
                        },
                        {
                            "start": 590,
                            "end": 715
                        },
                        {
                            "start": 716,
                            "end": 815
                        },
                        {
                            "start": 818,
                            "end": 917
                        },
                        {
                            "start": 918,
                            "end": 1022
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 84,
                            "end": 102,
                            "matchedPaperCorpusId": "221819581"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.3583984375
                },
                {
                    "corpus_id": "276421738",
                    "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                    "text": "Both approaches described in Section 3.3 would result in benchmarks with less English-centric cultural biases. However, the first approach mitigates and/or outlines existing biases by identifying whether a dataset is culturally agnostic or not, while the other approach results in benchmarks with cultural biases regarding the intended languages and cultures by not considering English sources that could contain English-centric biases. \n\nThe integration of diverse and culturally grounded knowledge is necessary to achieve a certain level of inclusivity and fairness for multilingual evaluations (Singh et al., 2024). Mitigating cultural bias from benchmarks is fundamental for achieving a certain level of reliability for global benchmarks, but -as mentioned -culture shapes reasoning and is, therefore, an essential component in and for communication and for solving multilingual tasks.",
                    "score": 0.49506449754752213,
                    "section_title": "Cultural Sensitivity and Inclusion",
                    "char_start_offset": 14363,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 110
                        },
                        {
                            "start": 111,
                            "end": 436
                        },
                        {
                            "start": 439,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 889
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.313720703125
                }
            ],
            "relevance_judgement": 0.63134765625,
            "relevance_judgment_input_expanded": "# Title: Multilingual European Language Models: Benchmarking Approaches and Challenges\n# Venue: arXiv.org\n# Authors: Fabio Barth, Georg Rehm\n## Abstract\nThe breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.\n## Introduction\nThere are already models and metrics that can measure and score translation quality (Rei et al., 2020). \n\nThe second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures. \n\nHowever, there are already some efforts to translate these benchmarks into multiple languages with regard to cultural biases. Note that these approaches involve human annotators that correct these biases (Singh et al., 2024). \n\nThis paper analyses the most commonly used benchmarks for multilingual LLMs for European languages. We highlight the benefits and interpretability of the existing benchmarks and outline their limitations.\n\n## Challenge 3: Cultural Bias\nMost benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.\n\n## Cultural Sensitivity and Inclusion\nBoth approaches described in Section 3.3 would result in benchmarks with less English-centric cultural biases. However, the first approach mitigates and/or outlines existing biases by identifying whether a dataset is culturally agnostic or not, while the other approach results in benchmarks with cultural biases regarding the intended languages and cultures by not considering English sources that could contain English-centric biases. \n\nThe integration of diverse and culturally grounded knowledge is necessary to achieve a certain level of inclusivity and fairness for multilingual evaluations (Singh et al., 2024). Mitigating cultural bias from benchmarks is fundamental for achieving a certain level of reliability for global benchmarks, but -as mentioned -culture shapes reasoning and is, therefore, an essential component in and for communication and for solving multilingual tasks.",
            "reference_string": "[276421738 | Barth et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2020,
            "reference_count": 46,
            "citation_count": 120,
            "influential_citation_count": 16,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.aclweb.org/anthology/2020.emnlp-main.618.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2004.04721, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                },
                {
                    "authorId": "2064469779",
                    "name": "Gorka Labaka"
                },
                {
                    "authorId": "1733049",
                    "name": "Eneko Agirre"
                }
            ],
            "abstract": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
            "corpus_id": 215548041,
            "sentences": [
                {
                    "corpus_id": "215548041",
                    "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                    "text": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
                    "score": 0.6187021737363481,
                    "section_title": "Discussion",
                    "char_start_offset": 24144,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 66
                        },
                        {
                            "start": 67,
                            "end": 142
                        },
                        {
                            "start": 143,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 620
                        },
                        {
                            "start": 621,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 1089
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.6298828125
                }
            ],
            "relevance_judgement": 0.6298828125,
            "relevance_judgment_input_expanded": "# Title: Translation Artifacts in Cross-lingual Transfer Learning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Mikel Artetxe, Gorka Labaka, Eneko Agirre\n## Abstract\nBoth human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.\n## Discussion\nFuture evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
            "reference_string": "[215548041 | Artetxe et al. | 2020 | Citations: 120]"
        },
        {
            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 74,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.11911, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1742219452",
                    "name": "Naome A. Etori"
                },
                {
                    "authorId": "2350516308",
                    "name": "Kevin Lu"
                },
                {
                    "authorId": "2350516959",
                    "name": "Randu Karisa"
                },
                {
                    "authorId": "2322446945",
                    "name": "Arturs Kanepajs"
                }
            ],
            "abstract": "As large language models (LLMs) rapidly advance, evaluating their performance is critical. LLMs are trained on multilingual data, but their reasoning abilities are mainly evaluated using English datasets. Hence, robust evaluation frameworks are needed using high-quality non-English datasets, especially low-resource languages (LRLs). This study evaluates eight state-of-the-art (SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language Understanding (MMLU) subset curated with native speakers for linguistic and cultural relevance. Giriama is benchmarked for the first time. Our evaluation shows that OpenAI's o1 model outperforms others across all languages, scoring 92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks. Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both Latvian and Giriama. Our results underscore the need for localized benchmarks and human evaluations in advancing cultural AI contextualization.",
            "corpus_id": 277066532,
            "sentences": [
                {
                    "corpus_id": "277066532",
                    "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
                    "text": "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023). \n\nRecently, NLP researchers have addressed this limitation; several diverse linguistic and cultural context MMLU evaluation benchmarks have been developed to address the gap, such as Ara-bicMMLU (Koto et al., 2024) evaluates tasks from North Africa, the Levant, and the Gulf, In-doMMLU (Koto et al., 2023) incorporates Indonesia's local languages and cultures. Others include, CMMLU (Li et al., 2023), KMMLU (Son et al., 2024),TurkishMMLU (Y\u00fcksel et al., 2024), PersianMMLU (Ghahroodi et al., 2024), Global MMLU (Singh et al., 2024) evaluates cultural and linguistic bias in 42 languages. \n\nDespite these advancements, significant gaps remain for African LRLs and some Eastern European languages. Our work narrows this gap, by including Giriama, a LRL language spoken by approximately 2.6 million people in Kenya, which is currently absent from existing NLP benchmarks. Similarly, even though the Latvian language has been benchmarked before EU20-MMLU (Thellmann et al., 2024), with relatively high-quality datasets, the datasets lack human-curated translations. As a result, they encounter challenges, including inaccurate auto-translations that compromise reliable evaluations. Giriama, in particular, has a limited digital presence and has historically been underrepresented in linguistic resources, making this the first multilingual benchmark developed explicitly for the language. Our contributions can be summarized as follows: \n\n\u2022 Novel Datasets: we present LAG-MMLU, a curated dataset that includes Giriama's first gold-standard (human-translated) collection, alongside two silver-standard datasets for Latvian: one machine-translated and another machine-translated with subsequent human curation.",
                    "score": 0.5443118664324464,
                    "section_title": "Introduction",
                    "char_start_offset": 1863,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 137,
                            "end": 282
                        },
                        {
                            "start": 285,
                            "end": 643
                        },
                        {
                            "start": 644,
                            "end": 871
                        },
                        {
                            "start": 874,
                            "end": 979
                        },
                        {
                            "start": 980,
                            "end": 1152
                        },
                        {
                            "start": 1153,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1462
                        },
                        {
                            "start": 1463,
                            "end": 1669
                        },
                        {
                            "start": 1670,
                            "end": 1717
                        },
                        {
                            "start": 1720,
                            "end": 1989
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.62890625
                }
            ],
            "relevance_judgement": 0.62890625,
            "relevance_judgment_input_expanded": "# Title: LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama\n# Venue: arXiv.org\n# Authors: Naome A. Etori, Kevin Lu, Randu Karisa, Arturs Kanepajs\n## Abstract\nAs large language models (LLMs) rapidly advance, evaluating their performance is critical. LLMs are trained on multilingual data, but their reasoning abilities are mainly evaluated using English datasets. Hence, robust evaluation frameworks are needed using high-quality non-English datasets, especially low-resource languages (LRLs). This study evaluates eight state-of-the-art (SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language Understanding (MMLU) subset curated with native speakers for linguistic and cultural relevance. Giriama is benchmarked for the first time. Our evaluation shows that OpenAI's o1 model outperforms others across all languages, scoring 92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks. Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both Latvian and Giriama. Our results underscore the need for localized benchmarks and human evaluations in advancing cultural AI contextualization.\n## Introduction\nHowever, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023). \n\nRecently, NLP researchers have addressed this limitation; several diverse linguistic and cultural context MMLU evaluation benchmarks have been developed to address the gap, such as Ara-bicMMLU (Koto et al., 2024) evaluates tasks from North Africa, the Levant, and the Gulf, In-doMMLU (Koto et al., 2023) incorporates Indonesia's local languages and cultures. Others include, CMMLU (Li et al., 2023), KMMLU (Son et al., 2024),TurkishMMLU (Y\u00fcksel et al., 2024), PersianMMLU (Ghahroodi et al., 2024), Global MMLU (Singh et al., 2024) evaluates cultural and linguistic bias in 42 languages. \n\nDespite these advancements, significant gaps remain for African LRLs and some Eastern European languages. Our work narrows this gap, by including Giriama, a LRL language spoken by approximately 2.6 million people in Kenya, which is currently absent from existing NLP benchmarks. Similarly, even though the Latvian language has been benchmarked before EU20-MMLU (Thellmann et al., 2024), with relatively high-quality datasets, the datasets lack human-curated translations. As a result, they encounter challenges, including inaccurate auto-translations that compromise reliable evaluations. Giriama, in particular, has a limited digital presence and has historically been underrepresented in linguistic resources, making this the first multilingual benchmark developed explicitly for the language. Our contributions can be summarized as follows: \n\n\u2022 Novel Datasets: we present LAG-MMLU, a curated dataset that includes Giriama's first gold-standard (human-translated) collection, alongside two silver-standard datasets for Latvian: one machine-translated and another machine-translated with subsequent human curation.",
            "reference_string": "[277066532 | Etori et al. | 2025 | Citations: 0]"
        },
        {
            "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
            "venue": "arXiv.org",
            "year": 2022,
            "reference_count": 24,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2211.14402",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2211.14402, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1516303035",
                    "name": "Aristides Milios"
                },
                {
                    "authorId": "2101317786",
                    "name": "Parishad BehnamGhader"
                }
            ],
            "abstract": "Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings. In this paper, we investigate the bias present in monolingual BERT models across a diverse set of languages (English, Greek, and Persian). While recent research has mostly focused on gender-related biases, we analyze religious and ethnic biases as well and propose a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions. We analyze each monolingual model via this method and visualize cultural similarities and differences across different dimensions of bias. Ultimately, we conclude that current methods of probing for bias are highly language-dependent, necessitating cultural insights regarding the unique ways bias is expressed in each language and culture (e.g. through coded language, synecdoche, and other similar linguistic concepts). We also hypothesize that higher measured social biases in the non-English BERT models correlate with user-generated content in their training.",
            "corpus_id": 253885863,
            "sentences": [
                {
                    "corpus_id": "253885863",
                    "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
                    "text": "In this paper2 , we investigated the bias present in BERT models across diverse languages including English, Greek, and Persian, rather than just a few languages from the same cultural sphere. While the focus of this project was on analyzing gender, religion, and ethnic bias types, the evaluation methods we used are not limited to these social biases. Our template set structures and sentence pseudo-likelihood-based scoring can be used for any type of bias. This sentence scoring allows for bias investigation in languages with more complex accordance rules than English, that have adjectives that decline for gender for instance, where the standard word-probability scoring method would be unable to score fairly. In this study, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it.",
                    "score": 0.5021861687496485,
                    "section_title": "Conclusion",
                    "char_start_offset": 19205,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 192
                        },
                        {
                            "start": 193,
                            "end": 353
                        },
                        {
                            "start": 354,
                            "end": 460
                        },
                        {
                            "start": 461,
                            "end": 717
                        },
                        {
                            "start": 718,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 907
                        },
                        {
                            "start": 908,
                            "end": 1141
                        },
                        {
                            "start": 1142,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1669
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.59521484375
                },
                {
                    "corpus_id": "253885863",
                    "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
                    "text": "Methods of Measuring Bias The most closely related methods for measuring bias to the one proposed in this paper are proposed in [10] and [8]. In [10], the existing CrowS-Pairs [11] bias benchmark is translated into French. Both the original benchmark and the translated benchmark use a similar sentence scoring system as in this paper. However, the translated benchmark avoids issues of gender-based declension of adjectives, instead preferring to rewrite the sentence to avoid the adjective entirely (via periphrasis). In this paper, we experiment with a declension-adjusted minimal pair/set of sentences, where multiple words in the sentence are potentially modified to result in grammatical sentences in both the male and female cases. \n\nCross-lingual and Cross-cultural Analysis While a huge amount of research has been done on English texts, few papers have studied the extent to which word embeddings capture the intrinsic bias in models for other languages. [12] proposes a debiasing method and applies it to the Multilingual BERT (M-BERT) model, and shows that English training data can help mitigate gender bias in M-BERT in the Chinese language. [13] measures profession bias in multilingual word embeddings (using both fastText and M-BERT embeddings) with inBias, a proposed evaluation metric that uses pairs of male/female occupation terms (e.g. \"doctor\" and \"doctora\" in Spanish) to measure gender bias. They also introduce a new dataset, MIBs, by manually collecting pairs of gender and profession words in English, French, Spanish, and German languages. \n\nWhile these papers are analyzing models for different languages, these languages all exist in the same cultural sphere. In [8], the authors study how ethnic bias varies across models for different languages with different cultures (inc. Korean, English, Turkish, etc.) using monolingual BERT models. The authors claim that different amount of ethnic biases across these languages reflects the historical and social context of the countries. The authors propose two debiasing techniques as well as a metric called Categorical Bias score in order to measure the ethnic bias. For measuring ethnic bias, the authors use a simple \"People from [X]\" template, where X is a country.",
                    "score": 0.4787872018622511,
                    "section_title": "Related Work",
                    "char_start_offset": 2903,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 141
                        },
                        {
                            "start": 142,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 335
                        },
                        {
                            "start": 336,
                            "end": 519
                        },
                        {
                            "start": 520,
                            "end": 738
                        },
                        {
                            "start": 741,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1155
                        },
                        {
                            "start": 1156,
                            "end": 1357
                        },
                        {
                            "start": 1358,
                            "end": 1416
                        },
                        {
                            "start": 1417,
                            "end": 1568
                        },
                        {
                            "start": 1571,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 1870
                        },
                        {
                            "start": 1871,
                            "end": 2011
                        },
                        {
                            "start": 2012,
                            "end": 2143
                        },
                        {
                            "start": 2144,
                            "end": 2245
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 128,
                            "end": 132,
                            "matchedPaperCorpusId": "248780290"
                        },
                        {
                            "start": 137,
                            "end": 140,
                            "matchedPaperCorpusId": "237491723"
                        },
                        {
                            "start": 145,
                            "end": 149,
                            "matchedPaperCorpusId": "248780290"
                        },
                        {
                            "start": 176,
                            "end": 180,
                            "matchedPaperCorpusId": "222090785"
                        },
                        {
                            "start": 965,
                            "end": 969,
                            "matchedPaperCorpusId": "227230609"
                        },
                        {
                            "start": 1156,
                            "end": 1160,
                            "matchedPaperCorpusId": "218487087"
                        },
                        {
                            "start": 1694,
                            "end": 1697,
                            "matchedPaperCorpusId": "237491723"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.375244140625
                }
            ],
            "relevance_judgement": 0.59521484375,
            "relevance_judgment_input_expanded": "# Title: An Analysis of Social Biases Present in BERT Variants Across Multiple Languages\n# Venue: arXiv.org\n# Authors: Aristides Milios, Parishad BehnamGhader\n## Abstract\nAlthough large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reflect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings. In this paper, we investigate the bias present in monolingual BERT models across a diverse set of languages (English, Greek, and Persian). While recent research has mostly focused on gender-related biases, we analyze religious and ethnic biases as well and propose a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions. We analyze each monolingual model via this method and visualize cultural similarities and differences across different dimensions of bias. Ultimately, we conclude that current methods of probing for bias are highly language-dependent, necessitating cultural insights regarding the unique ways bias is expressed in each language and culture (e.g. through coded language, synecdoche, and other similar linguistic concepts). We also hypothesize that higher measured social biases in the non-English BERT models correlate with user-generated content in their training.\n## Related Work\nMethods of Measuring Bias The most closely related methods for measuring bias to the one proposed in this paper are proposed in [10] and [8]. In [10], the existing CrowS-Pairs [11] bias benchmark is translated into French. Both the original benchmark and the translated benchmark use a similar sentence scoring system as in this paper. However, the translated benchmark avoids issues of gender-based declension of adjectives, instead preferring to rewrite the sentence to avoid the adjective entirely (via periphrasis). In this paper, we experiment with a declension-adjusted minimal pair/set of sentences, where multiple words in the sentence are potentially modified to result in grammatical sentences in both the male and female cases. \n\nCross-lingual and Cross-cultural Analysis While a huge amount of research has been done on English texts, few papers have studied the extent to which word embeddings capture the intrinsic bias in models for other languages. [12] proposes a debiasing method and applies it to the Multilingual BERT (M-BERT) model, and shows that English training data can help mitigate gender bias in M-BERT in the Chinese language. [13] measures profession bias in multilingual word embeddings (using both fastText and M-BERT embeddings) with inBias, a proposed evaluation metric that uses pairs of male/female occupation terms (e.g. \"doctor\" and \"doctora\" in Spanish) to measure gender bias. They also introduce a new dataset, MIBs, by manually collecting pairs of gender and profession words in English, French, Spanish, and German languages. \n\nWhile these papers are analyzing models for different languages, these languages all exist in the same cultural sphere. In [8], the authors study how ethnic bias varies across models for different languages with different cultures (inc. Korean, English, Turkish, etc.) using monolingual BERT models. The authors claim that different amount of ethnic biases across these languages reflects the historical and social context of the countries. The authors propose two debiasing techniques as well as a metric called Categorical Bias score in order to measure the ethnic bias. For measuring ethnic bias, the authors use a simple \"People from [X]\" template, where X is a country.\n\n## Conclusion\nIn this paper2 , we investigated the bias present in BERT models across diverse languages including English, Greek, and Persian, rather than just a few languages from the same cultural sphere. While the focus of this project was on analyzing gender, religion, and ethnic bias types, the evaluation methods we used are not limited to these social biases. Our template set structures and sentence pseudo-likelihood-based scoring can be used for any type of bias. This sentence scoring allows for bias investigation in languages with more complex accordance rules than English, that have adjectives that decline for gender for instance, where the standard word-probability scoring method would be unable to score fairly. In this study, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it.",
            "reference_string": "[253885863 | Milios et al. | 2022 | Citations: 8]"
        },
        {
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 190,
            "citation_count": 22,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.10915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2331613833",
                    "name": "Yufei Guo"
                },
                {
                    "authorId": "2331371661",
                    "name": "Muzhe Guo"
                },
                {
                    "authorId": "2273060735",
                    "name": "Juntao Su"
                },
                {
                    "authorId": "2331371753",
                    "name": "Zhou Yang"
                },
                {
                    "authorId": "2331357933",
                    "name": "Mengqiu Zhu"
                },
                {
                    "authorId": "2331365682",
                    "name": "Hongfei Li"
                },
                {
                    "authorId": "1455277182",
                    "name": "Mengyang Qiu"
                },
                {
                    "authorId": "2331377869",
                    "name": "Shuo Shuo Liu"
                }
            ],
            "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, but their susceptibility to biases poses significant challenges. This comprehensive review examines the landscape of bias in LLMs, from its origins to current mitigation strategies. We categorize biases as intrinsic and extrinsic, analyzing their manifestations in various NLP tasks. The review critically assesses a range of bias evaluation methods, including data-level, model-level, and output-level approaches, providing researchers with a robust toolkit for bias detection. We further explore mitigation strategies, categorizing them into pre-model, intra-model, and post-model techniques, highlighting their effectiveness and limitations. Ethical and legal implications of biased LLMs are discussed, emphasizing potential harms in real-world applications such as healthcare and criminal justice. By synthesizing current knowledge on bias in LLMs, this review contributes to the ongoing effort to develop fair and responsible AI systems. Our work serves as a comprehensive resource for researchers and practitioners working towards understanding, evaluating, and mitigating bias in LLMs, fostering the development of more equitable AI technologies.",
            "corpus_id": 274130807,
            "sentences": [
                {
                    "corpus_id": "274130807",
                    "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                    "text": "For example, translating \"The elderly person learned to use a smartphone\" might introduce a tone or wording in the target language that implies surprise or patronization, even if the original sentence was neutral (Caliskan et al., 2017). \n\n-Translation of Age-Related Idioms: When translating age-related idioms or expressions, a biased translation system might reinforce negative stereotypes. For instance, translating a phrase like \"old people are slow\" into another language might retain the negative connotation or even intensify it if the target language has a stronger cultural bias against the elderly. (Hovy and S\u00f8gaard, 2015). \n\n\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016).",
                    "score": 0.48170278319668136,
                    "section_title": "A.2.5 Machine Translation",
                    "char_start_offset": 92990,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 237
                        },
                        {
                            "start": 240,
                            "end": 393
                        },
                        {
                            "start": 394,
                            "end": 609
                        },
                        {
                            "start": 610,
                            "end": 635
                        },
                        {
                            "start": 638,
                            "end": 836
                        },
                        {
                            "start": 837,
                            "end": 1020
                        },
                        {
                            "start": 1021,
                            "end": 1180
                        },
                        {
                            "start": 1183,
                            "end": 1345
                        },
                        {
                            "start": 1346,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1665
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 213,
                            "end": 236,
                            "matchedPaperCorpusId": "23163324"
                        },
                        {
                            "start": 610,
                            "end": 634,
                            "matchedPaperCorpusId": "7396193"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.57275390625
                }
            ],
            "relevance_judgement": 0.57275390625,
            "relevance_judgment_input_expanded": "# Title: Bias in Large Language Models: Origin, Evaluation, and Mitigation\n# Venue: arXiv.org\n# Authors: Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu\n## Abstract\nLarge Language Models (LLMs) have revolutionized natural language processing, but their susceptibility to biases poses significant challenges. This comprehensive review examines the landscape of bias in LLMs, from its origins to current mitigation strategies. We categorize biases as intrinsic and extrinsic, analyzing their manifestations in various NLP tasks. The review critically assesses a range of bias evaluation methods, including data-level, model-level, and output-level approaches, providing researchers with a robust toolkit for bias detection. We further explore mitigation strategies, categorizing them into pre-model, intra-model, and post-model techniques, highlighting their effectiveness and limitations. Ethical and legal implications of biased LLMs are discussed, emphasizing potential harms in real-world applications such as healthcare and criminal justice. By synthesizing current knowledge on bias in LLMs, this review contributes to the ongoing effort to develop fair and responsible AI systems. Our work serves as a comprehensive resource for researchers and practitioners working towards understanding, evaluating, and mitigating bias in LLMs, fostering the development of more equitable AI technologies.\n## A.2.5 Machine Translation\nFor example, translating \"The elderly person learned to use a smartphone\" might introduce a tone or wording in the target language that implies surprise or patronization, even if the original sentence was neutral (Caliskan et al., 2017). \n\n-Translation of Age-Related Idioms: When translating age-related idioms or expressions, a biased translation system might reinforce negative stereotypes. For instance, translating a phrase like \"old people are slow\" into another language might retain the negative connotation or even intensify it if the target language has a stronger cultural bias against the elderly. (Hovy and S\u00f8gaard, 2015). \n\n\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016).",
            "reference_string": "[274130807 | Guo et al. | 2024 | Citations: 22]"
        },
        {
            "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.02289, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338997440",
                    "name": "Bernardo Magnini"
                },
                {
                    "authorId": "36024018",
                    "name": "Roberto Zanoli"
                },
                {
                    "authorId": "2343747257",
                    "name": "Michele Resta"
                },
                {
                    "authorId": "2343745825",
                    "name": "Martin Cimmino"
                },
                {
                    "authorId": "2343739878",
                    "name": "Paolo Albano"
                },
                {
                    "authorId": "2277456934",
                    "name": "Marco Madeddu"
                },
                {
                    "authorId": "2337691090",
                    "name": "Viviana Patti"
                }
            ],
            "abstract": "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.",
            "corpus_id": 276106987,
            "sentences": [
                {
                    "corpus_id": "276106987",
                    "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
                    "text": "Recently, several Large Language Models (LLMs) have been made available that are trained on Italian data, and several others have been announced for the next months. Those models (e.g., LLaMAntino [1], the Minerva family1 , Italia2 ) are released as open source, although with different licenses, and are all available through the Hugging Face (HF) platform 3 . In this context, performance evaluation of such models through appropriate benchmarking is becoming crucial. However, in the current evaluation practice, LLMs for Italian are mostly evaluated on benchmarks automatically translated from English, which poses critical issues: \n\n\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In [2], several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark [3], several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results. \n\nAt the time of writing, the landscape of evaluation is rapidly evolving. As an example, the Ita-bench 4 is now available on Hugging Face, including tasks originally developed in Evalita and adapted to LLMs. In addition, the Calamita initiative [4] has produced several benchmarks for Italian LLMs, opening new options for a more comprehensive evaluation.",
                    "score": 0.4679061858296274,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 165
                        },
                        {
                            "start": 166,
                            "end": 361
                        },
                        {
                            "start": 362,
                            "end": 470
                        },
                        {
                            "start": 471,
                            "end": 635
                        },
                        {
                            "start": 638,
                            "end": 664
                        },
                        {
                            "start": 665,
                            "end": 793
                        },
                        {
                            "start": 794,
                            "end": 882
                        },
                        {
                            "start": 883,
                            "end": 1044
                        },
                        {
                            "start": 1045,
                            "end": 1226
                        },
                        {
                            "start": 1227,
                            "end": 1237
                        },
                        {
                            "start": 1238,
                            "end": 1386
                        },
                        {
                            "start": 1387,
                            "end": 1570
                        },
                        {
                            "start": 1571,
                            "end": 1653
                        },
                        {
                            "start": 1654,
                            "end": 1662
                        },
                        {
                            "start": 1663,
                            "end": 1809
                        },
                        {
                            "start": 1810,
                            "end": 1924
                        },
                        {
                            "start": 1927,
                            "end": 1999
                        },
                        {
                            "start": 2000,
                            "end": 2133
                        },
                        {
                            "start": 2134,
                            "end": 2281
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 797,
                            "end": 800,
                            "matchedPaperCorpusId": "275357255"
                        },
                        {
                            "start": 1424,
                            "end": 1427,
                            "matchedPaperCorpusId": "221516475"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5693359375
                }
            ],
            "relevance_judgement": 0.5693359375,
            "relevance_judgment_input_expanded": "# Title: Evalita-LLM: Benchmarking Large Language Models on Italian\n# Venue: arXiv.org\n# Authors: Bernardo Magnini, Roberto Zanoli, Michele Resta, Martin Cimmino, Paolo Albano, Marco Madeddu, Viviana Patti\n## Abstract\nWe describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.\n## Introduction\nRecently, several Large Language Models (LLMs) have been made available that are trained on Italian data, and several others have been announced for the next months. Those models (e.g., LLaMAntino [1], the Minerva family1 , Italia2 ) are released as open source, although with different licenses, and are all available through the Hugging Face (HF) platform 3 . In this context, performance evaluation of such models through appropriate benchmarking is becoming crucial. However, in the current evaluation practice, LLMs for Italian are mostly evaluated on benchmarks automatically translated from English, which poses critical issues: \n\n\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In [2], several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark [3], several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results. \n\nAt the time of writing, the landscape of evaluation is rapidly evolving. As an example, the Ita-bench 4 is now available on Hugging Face, including tasks originally developed in Evalita and adapted to LLMs. In addition, the Calamita initiative [4] has produced several benchmarks for Italian LLMs, opening new options for a more comprehensive evaluation.",
            "reference_string": "[276106987 | Magnini et al. | 2025 | Citations: 1]"
        },
        {
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 49,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.15521, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327995656",
                    "name": "Minghao Wu"
                },
                {
                    "authorId": "2258789284",
                    "name": "Weixuan Wang"
                },
                {
                    "authorId": "2349802912",
                    "name": "Sinuo Liu"
                },
                {
                    "authorId": "2331767275",
                    "name": "Huifeng Yin"
                },
                {
                    "authorId": "2356794252",
                    "name": "Xintong Wang"
                },
                {
                    "authorId": "2331686159",
                    "name": "Yu Zhao"
                },
                {
                    "authorId": "2266387313",
                    "name": "Chenyang Lyu"
                },
                {
                    "authorId": "2302342302",
                    "name": "Longyue Wang"
                },
                {
                    "authorId": "2305289815",
                    "name": "Weihua Luo"
                },
                {
                    "authorId": "2304530663",
                    "name": "Kaifu Zhang"
                }
            ],
            "abstract": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "corpus_id": 277993848,
            "sentences": [
                {
                    "corpus_id": "277993848",
                    "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
                    "text": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
                    "score": 0.600268206286318,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.568359375
                }
            ],
            "relevance_judgement": 0.568359375,
            "relevance_judgment_input_expanded": "# Title: The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks\n# Venue: arXiv.org\n# Authors: Minghao Wu, Weixuan Wang, Sinuo Liu, Huifeng Yin, Xintong Wang, Yu Zhao, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang\n## Abstract\nAs large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.\n",
            "reference_string": "[277993848 | Wu et al. | 2025 | Citations: 5]"
        },
        {
            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 16,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.17789, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308274656",
                    "name": "Irene Plaza"
                },
                {
                    "authorId": "2293315903",
                    "name": "Nina Melero"
                },
                {
                    "authorId": "2308274509",
                    "name": "C. Pozo"
                },
                {
                    "authorId": "2230852635",
                    "name": "Javier Conde"
                },
                {
                    "authorId": "2243081043",
                    "name": "Pedro Reviriego"
                },
                {
                    "authorId": "2308274000",
                    "name": "Marina Mayor-Rocher"
                },
                {
                    "authorId": "2176184513",
                    "name": "Mar\u00eda Grandury"
                }
            ],
            "abstract": "The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.",
            "corpus_id": 270737791,
            "sentences": [
                {
                    "corpus_id": "270737791",
                    "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
                    "text": "The simplest approach is to translate these same tests into other languages and use them for multilingual evaluation.This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results.\n\nIn this work we perform an initial analysis of the impact of automatic translation on one of the most widely used LLM benchmarks, the Multitask Language Understanding (MMLU) test Hendrycks et al. (2021a) for one of the most commonly used and chosen as a second language to learn, Spanish.The analysis shows that automatic translation induces errors in the LLM answers and thus distorts the benchmark's results.Based on these findings, potential solutions to this problem are also briefly discussed.\n\nThe rest of the work is organized as follows, in section 2 the methodology used in our analysis is presented, followed by the results in section 3 and a discussion of their implications and potential solutions in section 4. The paper ends with the conclusion in section 5.",
                    "score": 0.5484130154180287,
                    "section_title": "Introduction",
                    "char_start_offset": 2137,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 117
                        },
                        {
                            "start": 117,
                            "end": 259
                        },
                        {
                            "start": 259,
                            "end": 341
                        },
                        {
                            "start": 341,
                            "end": 379
                        },
                        {
                            "start": 379,
                            "end": 853
                        },
                        {
                            "start": 853,
                            "end": 1066
                        },
                        {
                            "start": 1068,
                            "end": 1356
                        },
                        {
                            "start": 1356,
                            "end": 1478
                        },
                        {
                            "start": 1478,
                            "end": 1566
                        },
                        {
                            "start": 1568,
                            "end": 1840
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1247,
                            "end": 1271,
                            "matchedPaperCorpusId": "221516475"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54248046875
                },
                {
                    "corpus_id": "270737791",
                    "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
                    "text": "The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.",
                    "score": 0.4611458002652646,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.38916015625
                }
            ],
            "relevance_judgement": 0.54248046875,
            "relevance_judgment_input_expanded": "# Title: Spanish and LLM Benchmarks: is MMLU Lost in Translation?\n# Venue: arXiv.org\n# Authors: Irene Plaza, Nina Melero, C. Pozo, Javier Conde, Pedro Reviriego, Marina Mayor-Rocher, Mar\u00eda Grandury\n## Abstract\nThe evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.\n## Introduction\nThe simplest approach is to translate these same tests into other languages and use them for multilingual evaluation.This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results.\n\nIn this work we perform an initial analysis of the impact of automatic translation on one of the most widely used LLM benchmarks, the Multitask Language Understanding (MMLU) test Hendrycks et al. (2021a) for one of the most commonly used and chosen as a second language to learn, Spanish.The analysis shows that automatic translation induces errors in the LLM answers and thus distorts the benchmark's results.Based on these findings, potential solutions to this problem are also briefly discussed.\n\nThe rest of the work is organized as follows, in section 2 the methodology used in our analysis is presented, followed by the results in section 3 and a discussion of their implications and potential solutions in section 4. The paper ends with the conclusion in section 5.",
            "reference_string": "[270737791 | Plaza et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Challenges and Strategies in Cross-Cultural NLP",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2022,
            "reference_count": 169,
            "citation_count": 182,
            "influential_citation_count": 7,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2203.10020",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2203.10020, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2064295987",
                    "name": "Daniel Hershcovich"
                },
                {
                    "authorId": "37922370",
                    "name": "Stella Frank"
                },
                {
                    "authorId": "49568895",
                    "name": "Heather Lent"
                },
                {
                    "authorId": "3295381",
                    "name": "Miryam de Lhoneux"
                },
                {
                    "authorId": "30671790",
                    "name": "Mostafa Abdou"
                },
                {
                    "authorId": "6547490",
                    "name": "Stephanie Brandl"
                },
                {
                    "authorId": "83574123",
                    "name": "Emanuele Bugliarello"
                },
                {
                    "authorId": "2093582149",
                    "name": "Laura Cabello Piqueras"
                },
                {
                    "authorId": "2125376289",
                    "name": "Ilias Chalkidis"
                },
                {
                    "authorId": "1717462692",
                    "name": "Ruixiang Cui"
                },
                {
                    "authorId": "50110151",
                    "name": "Constanza Fierro"
                },
                {
                    "authorId": "82259306",
                    "name": "Katerina Margatina"
                },
                {
                    "authorId": "1660797358",
                    "name": "Phillip Rust"
                },
                {
                    "authorId": "1700187",
                    "name": "Anders S\u00f8gaard"
                }
            ],
            "abstract": "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.",
            "corpus_id": 247594499,
            "sentences": [
                {
                    "corpus_id": "247594499",
                    "title": "Challenges and Strategies in Cross-Cultural NLP",
                    "text": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
                    "score": 0.5370944783640763,
                    "section_title": "Data Collection",
                    "char_start_offset": 25564,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 175
                        },
                        {
                            "start": 176,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 525
                        },
                        {
                            "start": 526,
                            "end": 690
                        },
                        {
                            "start": 691,
                            "end": 812
                        },
                        {
                            "start": 813,
                            "end": 962
                        },
                        {
                            "start": 963,
                            "end": 1107
                        },
                        {
                            "start": 1110,
                            "end": 1311
                        },
                        {
                            "start": 1312,
                            "end": 1691
                        },
                        {
                            "start": 1694,
                            "end": 1842
                        },
                        {
                            "start": 1843,
                            "end": 2104
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 793,
                            "end": 811,
                            "matchedPaperCorpusId": "238198104"
                        },
                        {
                            "start": 1947,
                            "end": 1965,
                            "matchedPaperCorpusId": "238198104"
                        },
                        {
                            "start": 1965,
                            "end": 1984,
                            "matchedPaperCorpusId": "218470125"
                        },
                        {
                            "start": 1984,
                            "end": 2001,
                            "matchedPaperCorpusId": "237503047"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5400390625
                }
            ],
            "relevance_judgement": 0.5400390625,
            "relevance_judgment_input_expanded": "# Title: Challenges and Strategies in Cross-Cultural NLP\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Daniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, Anders S\u00f8gaard\n## Abstract\nVarious efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.\n## Data Collection\nTranslating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
            "reference_string": "[247594499 | Hershcovich et al. | 2022 | Citations: 182]"
        },
        {
            "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
            "venue": "Pacific Asia Conference on Language, Information and Computation",
            "year": 2024,
            "reference_count": 16,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.07251, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334689696",
                    "name": "Xiaonan Wang"
                },
                {
                    "authorId": "2334573099",
                    "name": "Jinyoung Yeo"
                },
                {
                    "authorId": "2334827754",
                    "name": "Joon-Ho Lim"
                },
                {
                    "authorId": "2334743749",
                    "name": "Hansaem Kim"
                }
            ],
            "abstract": "Large language models have exhibited significant enhancements in performance across various tasks. However, the complexity of their evaluation increases as these models generate more fluent and coherent content. Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures. To address this research gap, we introduce KULTURE Bench, an evaluation framework specifically designed for Korean culture that features datasets of cultural news, idioms, and poetry. It is designed to assess language models' cultural comprehension and reasoning capabilities at the word, sentence, and paragraph levels. Using the KULTURE Bench, we assessed the capabilities of models trained with different language corpora and analyzed the results comprehensively. The results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture.",
            "corpus_id": 274610534,
            "sentences": [
                {
                    "corpus_id": "274610534",
                    "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
                    "text": "Large language models have exhibited significant enhancements in performance across various tasks. However, the complexity of their evaluation increases as these models generate more fluent and coherent content. Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures. To address this research gap, we introduce KULTURE Bench, an evaluation framework specifically designed for Korean culture that features datasets of cultural news, idioms, and poetry. It is designed to assess language models' cultural comprehension and reasoning capabilities at the word, sentence, and paragraph levels. Using the KULTURE Bench, we assessed the capabilities of models trained with different language corpora and analyzed the results comprehensively. The results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture.",
                    "score": 0.4742344919521254,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5400390625
                }
            ],
            "relevance_judgement": 0.5400390625,
            "relevance_judgment_input_expanded": "# Title: KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context\n# Venue: Pacific Asia Conference on Language, Information and Computation\n# Authors: Xiaonan Wang, Jinyoung Yeo, Joon-Ho Lim, Hansaem Kim\n## Abstract\nLarge language models have exhibited significant enhancements in performance across various tasks. However, the complexity of their evaluation increases as these models generate more fluent and coherent content. Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures. To address this research gap, we introduce KULTURE Bench, an evaluation framework specifically designed for Korean culture that features datasets of cultural news, idioms, and poetry. It is designed to assess language models' cultural comprehension and reasoning capabilities at the word, sentence, and paragraph levels. Using the KULTURE Bench, we assessed the capabilities of models trained with different language corpora and analyzed the results comprehensively. The results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture.\n",
            "reference_string": "[274610534 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.04662, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1742526676",
                    "name": "Tarek Naous"
                },
                {
                    "authorId": "2319485909",
                    "name": "Wei Xu"
                }
            ],
            "abstract": "Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English compared to Arabic. We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses. This also extends to entities that exhibit high lexical overlap with languages that are not Arabic but use the Arabic script. Further, we show how frequency-based tokenization leads to this issue in LMs, which gets worse with larger Arabic vocabularies. We will make CAMeL-2 available at: https://github.com/tareknaous/camel2",
            "corpus_id": 275357770,
            "sentences": [
                {
                    "corpus_id": "275357770",
                    "title": "On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena",
                    "text": "Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English compared to Arabic. We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses. This also extends to entities that exhibit high lexical overlap with languages that are not Arabic but use the Arabic script. Further, we show how frequency-based tokenization leads to this issue in LMs, which gets worse with larger Arabic vocabularies. We will make CAMeL-2 available at: https://github.com/tareknaous/camel2",
                    "score": 0.46120439506412736,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53466796875
                }
            ],
            "relevance_judgement": 0.53466796875,
            "relevance_judgment_input_expanded": "# Title: On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Tarek Naous, Wei Xu\n## Abstract\nLanguage Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English compared to Arabic. We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses. This also extends to entities that exhibit high lexical overlap with languages that are not Arabic but use the Arabic script. Further, we show how frequency-based tokenization leads to this issue in LMs, which gets worse with larger Arabic vocabularies. We will make CAMeL-2 available at: https://github.com/tareknaous/camel2\n",
            "reference_string": "[275357770 | Naous et al. | 2025 | Citations: 3]"
        },
        {
            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
            "venue": "Annual Meeting of the Association for Computational Linguistics",
            "year": 2023,
            "reference_count": 44,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2306.05076",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2306.05076, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1725417862",
                    "name": "Amr Keleg"
                },
                {
                    "authorId": "1745226",
                    "name": "Walid Magdy"
                }
            ],
            "abstract": "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
            "corpus_id": 259108559,
            "sentences": [
                {
                    "corpus_id": "259108559",
                    "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
                    "text": "From a model diagnostics perspective, Cao et al. (2021) found that English PLMs might be biased to making specific predictions based on a predicate's template irrespective of the subjects used to populate this template. Thereafter, Elazar et al. (2023) designed a causal framework for modeling multiple co-occurrence statistics that might cause English PLMs to achieve high scores on some of LAMA's predicates. \n\nWe focus on why a non-English PLM might fail to recall facts and hypothesize the following possible reasons: \n\n1. The quality of the template might degrade after automatically translating it from English. 2. Non-English or multilingual PLM are generally pretrained on a lesser amount of non-English data and thus might be less capable of recalling facts efficiently. 3. Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias. \n\nWhile the first two factors are studied in the literature, we believe that the third factor is a major quality issue that previous work has overlooked. Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered. We investigate the presence of such bias ( \u00a73). Moreover, we empirically demonstrate how better model diagnostics can be performed when the benchmark is formed using two diverse and contrasting sets of facts ( \u00a75).",
                    "score": 0.53695006812203,
                    "section_title": "Introduction",
                    "char_start_offset": 5592,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 219
                        },
                        {
                            "start": 220,
                            "end": 410
                        },
                        {
                            "start": 413,
                            "end": 521
                        },
                        {
                            "start": 524,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 779
                        },
                        {
                            "start": 780,
                            "end": 910
                        },
                        {
                            "start": 913,
                            "end": 1064
                        },
                        {
                            "start": 1065,
                            "end": 1240
                        },
                        {
                            "start": 1241,
                            "end": 1288
                        },
                        {
                            "start": 1289,
                            "end": 1455
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 38,
                            "end": 55,
                            "matchedPaperCorpusId": "235458643"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.53173828125
                },
                {
                    "corpus_id": "259108559",
                    "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
                    "text": "Transfer learning paradigms such as fine-tuning, few-shot learning, and zero-shot learning rely on pretrained language models (PLMs), that require having large compilations of raw data (Devlin et al. 2019;Brown et al. 2020;Chowdhery et al. 2022;Scao et al. 2022). These PLMs showed some ability to model different linguistic phenomena (Goldberg 2019;Jawahar et al. 2019) in addition to memorizing facts related to real-world knowledge. While there is a drive to have multilingual models, English is still the language that is better supported due to the abundance of large English raw corpora, diverse datasets, and benchmarks. Moreover, monolingual non-English PLMs are still being pretrained for other high-resource languages. As a way to probe the non-English and multilingual PLMs, researchers tend to translate English benchmarks into other languages, which might degrade the quality of the samples especially if the translation is performed automatically. While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks. \n\nLAMA (Petroni et al., 2019) and ParaRel (Elazar et al., 2021) are two benchmarks developed to quantify the factual knowledge of the English PLMs. They used a setup in which a language model is said to know a specific fact if it can predict the right object for a prompt in a fill-the-gap setup (e.g., For the prompt \"The capital of England is [MASK]\", the model needs to fill the masked gap with \"London\"). Multilingual versions of these benchmarks namely: mLAMA (Kassner et al., 2021), and mParaRel (Fierro and S\u00f8gaard, 2022) were released to evaluate the performance of multilingual PLMs by translating LAMA and ParaRel into 53 and 46 languages respectively. The subjects and objects of the triples within these benchmarks were translated using their multilingual labels on Wikidata, while the templates were automatically translated from the English ones used in the original benchmarks.",
                    "score": 0.4967701240123549,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 263
                        },
                        {
                            "start": 264,
                            "end": 435
                        },
                        {
                            "start": 436,
                            "end": 627
                        },
                        {
                            "start": 628,
                            "end": 728
                        },
                        {
                            "start": 729,
                            "end": 961
                        },
                        {
                            "start": 962,
                            "end": 1144
                        },
                        {
                            "start": 1147,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1553
                        },
                        {
                            "start": 1554,
                            "end": 1807
                        },
                        {
                            "start": 1808,
                            "end": 2037
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 185,
                            "end": 205,
                            "matchedPaperCorpusId": "52967399"
                        },
                        {
                            "start": 205,
                            "end": 223,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 350,
                            "end": 369,
                            "matchedPaperCorpusId": "195477534"
                        },
                        {
                            "start": 1152,
                            "end": 1174,
                            "matchedPaperCorpusId": "202539551"
                        },
                        {
                            "start": 1610,
                            "end": 1632,
                            "matchedPaperCorpusId": "231740666"
                        },
                        {
                            "start": 1647,
                            "end": 1673,
                            "matchedPaperCorpusId": "247596661"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.52587890625
                }
            ],
            "relevance_judgement": 0.53173828125,
            "relevance_judgment_input_expanded": "# Title: DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models\n# Venue: Annual Meeting of the Association for Computational Linguistics\n# Authors: Amr Keleg, Walid Magdy\n## Abstract\nA few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.\n## Introduction\nTransfer learning paradigms such as fine-tuning, few-shot learning, and zero-shot learning rely on pretrained language models (PLMs), that require having large compilations of raw data (Devlin et al. 2019;Brown et al. 2020;Chowdhery et al. 2022;Scao et al. 2022). These PLMs showed some ability to model different linguistic phenomena (Goldberg 2019;Jawahar et al. 2019) in addition to memorizing facts related to real-world knowledge. While there is a drive to have multilingual models, English is still the language that is better supported due to the abundance of large English raw corpora, diverse datasets, and benchmarks. Moreover, monolingual non-English PLMs are still being pretrained for other high-resource languages. As a way to probe the non-English and multilingual PLMs, researchers tend to translate English benchmarks into other languages, which might degrade the quality of the samples especially if the translation is performed automatically. While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks. \n\nLAMA (Petroni et al., 2019) and ParaRel (Elazar et al., 2021) are two benchmarks developed to quantify the factual knowledge of the English PLMs. They used a setup in which a language model is said to know a specific fact if it can predict the right object for a prompt in a fill-the-gap setup (e.g., For the prompt \"The capital of England is [MASK]\", the model needs to fill the masked gap with \"London\"). Multilingual versions of these benchmarks namely: mLAMA (Kassner et al., 2021), and mParaRel (Fierro and S\u00f8gaard, 2022) were released to evaluate the performance of multilingual PLMs by translating LAMA and ParaRel into 53 and 46 languages respectively. The subjects and objects of the triples within these benchmarks were translated using their multilingual labels on Wikidata, while the templates were automatically translated from the English ones used in the original benchmarks.\n...\nFrom a model diagnostics perspective, Cao et al. (2021) found that English PLMs might be biased to making specific predictions based on a predicate's template irrespective of the subjects used to populate this template. Thereafter, Elazar et al. (2023) designed a causal framework for modeling multiple co-occurrence statistics that might cause English PLMs to achieve high scores on some of LAMA's predicates. \n\nWe focus on why a non-English PLM might fail to recall facts and hypothesize the following possible reasons: \n\n1. The quality of the template might degrade after automatically translating it from English. 2. Non-English or multilingual PLM are generally pretrained on a lesser amount of non-English data and thus might be less capable of recalling facts efficiently. 3. Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias. \n\nWhile the first two factors are studied in the literature, we believe that the third factor is a major quality issue that previous work has overlooked. Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered. We investigate the presence of such bias ( \u00a73). Moreover, we empirically demonstrate how better model diagnostics can be performed when the benchmark is formed using two diverse and contrasting sets of facts ( \u00a75).",
            "reference_string": "[259108559 | Keleg et al. | 2023 | Citations: 11]"
        },
        {
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 40,
            "citation_count": 11,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.07302, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2226458991",
                    "name": "Julen Etxaniz"
                },
                {
                    "authorId": "2481918",
                    "name": "Gorka Azkune"
                },
                {
                    "authorId": "2260104163",
                    "name": "A. Soroa"
                },
                {
                    "authorId": "2251043402",
                    "name": "Oier L\u00f3pez de Lacalle"
                },
                {
                    "authorId": "2347956",
                    "name": "Mikel Artetxe"
                }
            ],
            "abstract": "Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.",
            "corpus_id": 270380088,
            "sentences": [
                {
                    "corpus_id": "270380088",
                    "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                    "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
                    "score": 0.505043054781112,
                    "section_title": "Related Work",
                    "char_start_offset": 20594,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 267
                        },
                        {
                            "start": 268,
                            "end": 406
                        },
                        {
                            "start": 407,
                            "end": 569
                        },
                        {
                            "start": 572,
                            "end": 731
                        },
                        {
                            "start": 732,
                            "end": 886
                        },
                        {
                            "start": 887,
                            "end": 964
                        },
                        {
                            "start": 965,
                            "end": 1162
                        },
                        {
                            "start": 1163,
                            "end": 1415
                        },
                        {
                            "start": 1416,
                            "end": 1520
                        },
                        {
                            "start": 1523,
                            "end": 1674
                        },
                        {
                            "start": 1675,
                            "end": 1840
                        },
                        {
                            "start": 1841,
                            "end": 1998
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1248,
                            "end": 1268,
                            "matchedPaperCorpusId": "258685666"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.525390625
                }
            ],
            "relevance_judgement": 0.525390625,
            "relevance_judgment_input_expanded": "# Title: BertaQA: How Much Do Language Models Know About Local Culture?\n# Venue: Neural Information Processing Systems\n# Authors: Julen Etxaniz, Gorka Azkune, A. Soroa, Oier L\u00f3pez de Lacalle, Mikel Artetxe\n## Abstract\nLarge Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.\n## Related Work\nResearch in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "reference_string": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
            "year": 2024,
            "reference_count": 28,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.02080, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2282542641",
                    "name": "Ashish Agrawal"
                },
                {
                    "authorId": "2187454108",
                    "name": "Barah Fazili"
                },
                {
                    "authorId": "144859542",
                    "name": "P. Jyothi"
                }
            ],
            "abstract": "Popular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
            "corpus_id": 267413041,
            "sentences": [
                {
                    "corpus_id": "267413041",
                    "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                    "text": "Multilingual benchmarks, such as XNLI, XTREME, play a vital role in assessing the cross-lingual generalization of multilingual pretrained models (Conneau et al., 2018;Hu et al., 2020). Typically, these benchmarks involve translating development and test sets from English into different target languages using professional human translators. However, such a translation process is susceptible to human errors and could lead to incorrect estimates of cross-lingual transfer to target languages. We find translation errors do emerge and they disproportionately affect translations in certain low-resource languages such as Hindi and Urdu. 2 onsider the well-known Cross-Lingual Natural Language Inference (XNLI) benchmark (Conneau et al., 2018) that contains human translations of English premise-hypothesis pairs (with the labels reproduced from English) into 14 typologicallydiverse target languages. Prior work raised concerns about whether the semantic relationships between premise and hypothesis are preserved in such human translations, but did not probe into this issue further (Artetxe et al., 2020a(Artetxe et al., , 2023)). We find that there are indeed errors introduced in the human translations leading to label inconsistencies and that this issue disproportionately affects lowresource languages. \n\nTo visualize the impact of low-quality translations on low-resource languages, Figure 1 compares zero-shot XNLI performance on all 14 target languages using the XLMR model (Conneau et al., 2020) finetuned on English NLI with the following two input types: 1. Human translations of the original English NLI instances to the target language from XNLI, translated back to English. 2. Machine translations of the original English NLI instances to the target language, translated back to English. We see a clear differential trend with larger gaps between the (scores over the) two input types for low-resource languages such as Swahili, Urdu and Turkish (appearing on the right) and smaller gaps for high-resource languages such as Spanish, German and French (appearing on the left).",
                    "score": 0.5162203803409647,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 184
                        },
                        {
                            "start": 185,
                            "end": 341
                        },
                        {
                            "start": 342,
                            "end": 493
                        },
                        {
                            "start": 494,
                            "end": 638
                        },
                        {
                            "start": 639,
                            "end": 900
                        },
                        {
                            "start": 901,
                            "end": 1132
                        },
                        {
                            "start": 1133,
                            "end": 1309
                        },
                        {
                            "start": 1312,
                            "end": 1803
                        },
                        {
                            "start": 1804,
                            "end": 2091
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1084,
                            "end": 1106,
                            "matchedPaperCorpusId": "215548041"
                        },
                        {
                            "start": 1484,
                            "end": 1506,
                            "matchedPaperCorpusId": "207880568"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5244140625
                },
                {
                    "corpus_id": "267413041",
                    "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                    "text": "This work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
                    "score": 0.45439051333492736,
                    "section_title": "Conclusions",
                    "char_start_offset": 13897,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 394
                        },
                        {
                            "start": 395,
                            "end": 618
                        },
                        {
                            "start": 621,
                            "end": 911
                        },
                        {
                            "start": 914,
                            "end": 1191
                        },
                        {
                            "start": 1192,
                            "end": 1251
                        },
                        {
                            "start": 1252,
                            "end": 1345
                        },
                        {
                            "start": 1348,
                            "end": 1455
                        },
                        {
                            "start": 1456,
                            "end": 1639
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5107421875
                }
            ],
            "relevance_judgement": 0.5244140625,
            "relevance_judgment_input_expanded": "# Title: Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning\n# Venue: Conference of the European Chapter of the Association for Computational Linguistics\n# Authors: Ashish Agrawal, Barah Fazili, P. Jyothi\n## Abstract\nPopular benchmarks (e.g., XNLI) used to evaluate cross-lingual language understanding consist of parallel versions of English evaluation sets in multiple target languages created with the help of professional translators. When creating such parallel data, it is critical to ensure high-quality translations for all target languages for an accurate characterization of cross-lingual transfer. In this work, we find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.\n## Introduction\nMultilingual benchmarks, such as XNLI, XTREME, play a vital role in assessing the cross-lingual generalization of multilingual pretrained models (Conneau et al., 2018;Hu et al., 2020). Typically, these benchmarks involve translating development and test sets from English into different target languages using professional human translators. However, such a translation process is susceptible to human errors and could lead to incorrect estimates of cross-lingual transfer to target languages. We find translation errors do emerge and they disproportionately affect translations in certain low-resource languages such as Hindi and Urdu. 2 onsider the well-known Cross-Lingual Natural Language Inference (XNLI) benchmark (Conneau et al., 2018) that contains human translations of English premise-hypothesis pairs (with the labels reproduced from English) into 14 typologicallydiverse target languages. Prior work raised concerns about whether the semantic relationships between premise and hypothesis are preserved in such human translations, but did not probe into this issue further (Artetxe et al., 2020a(Artetxe et al., , 2023)). We find that there are indeed errors introduced in the human translations leading to label inconsistencies and that this issue disproportionately affects lowresource languages. \n\nTo visualize the impact of low-quality translations on low-resource languages, Figure 1 compares zero-shot XNLI performance on all 14 target languages using the XLMR model (Conneau et al., 2020) finetuned on English NLI with the following two input types: 1. Human translations of the original English NLI instances to the target language from XNLI, translated back to English. 2. Machine translations of the original English NLI instances to the target language, translated back to English. We see a clear differential trend with larger gaps between the (scores over the) two input types for low-resource languages such as Swahili, Urdu and Turkish (appearing on the right) and smaller gaps for high-resource languages such as Spanish, German and French (appearing on the left).\n\n## Conclusions\nThis work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
            "reference_string": "[267413041 | Agrawal et al. | 2024 | Citations: 3]"
        },
        {
            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.17945, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "4289746",
                    "name": "Qianying Liu"
                },
                {
                    "authorId": "2347161903",
                    "name": "Katrina Qiyao Wang"
                },
                {
                    "authorId": "2342687540",
                    "name": "Fei Cheng"
                },
                {
                    "authorId": "1795664",
                    "name": "S. Kurohashi"
                }
            ],
            "abstract": "Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education.",
            "corpus_id": 276580130,
            "sentences": [
                {
                    "corpus_id": "276580130",
                    "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
                    "text": "This study provides the first comprehensive investigation of multilingual nationality bias in state-ofthe-art (SOTA) Large Language Models (LLMs) across reasoning-based decision-making tasks. Our findings reveal that while LLMs exhibit lower bias in English, significant disparities emerge in non-English languages. This bias impacts the fairness and consistency of choices and the structure of reasoning. The bias patterns observed are influenced not only by language differences but also by user demographics and reasoning strategies. For example, in non-English contexts, Chain-of-Thought (CoT) prompting often exacerbates rather than mitigates bias, and female-based decisions usually introduce higher bias than male-based ones. Furthermore, our evaluation demonstrates that different models prioritize decision-making criteria differently. Future research should explore bias mitigation techniques tailored for multilingual settings, considering both linguistic and cultural fac-tors to enhance fairness and inclusivity in AI-driven decision-making applications.",
                    "score": 0.4983899806531594,
                    "section_title": "Conclusion",
                    "char_start_offset": 31067,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 191
                        },
                        {
                            "start": 192,
                            "end": 315
                        },
                        {
                            "start": 316,
                            "end": 405
                        },
                        {
                            "start": 406,
                            "end": 536
                        },
                        {
                            "start": 537,
                            "end": 732
                        },
                        {
                            "start": 733,
                            "end": 844
                        },
                        {
                            "start": 845,
                            "end": 1067
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.50634765625
                },
                {
                    "corpus_id": "276580130",
                    "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
                    "text": "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems.",
                    "score": 0.5189017276831458,
                    "section_title": "Factor",
                    "char_start_offset": 29226,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 67
                        },
                        {
                            "start": 68,
                            "end": 187
                        },
                        {
                            "start": 188,
                            "end": 387
                        },
                        {
                            "start": 388,
                            "end": 590
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.389892578125
                }
            ],
            "relevance_judgement": 0.50634765625,
            "relevance_judgment_input_expanded": "# Title: Assessing Large Language Models in Agentic Multilingual National Bias\n# Venue: arXiv.org\n# Authors: Qianying Liu, Katrina Qiyao Wang, Fei Cheng, S. Kurohashi\n## Abstract\nLarge Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education.\n## Factor\nEnglish-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems.\n\n## Conclusion\nThis study provides the first comprehensive investigation of multilingual nationality bias in state-ofthe-art (SOTA) Large Language Models (LLMs) across reasoning-based decision-making tasks. Our findings reveal that while LLMs exhibit lower bias in English, significant disparities emerge in non-English languages. This bias impacts the fairness and consistency of choices and the structure of reasoning. The bias patterns observed are influenced not only by language differences but also by user demographics and reasoning strategies. For example, in non-English contexts, Chain-of-Thought (CoT) prompting often exacerbates rather than mitigates bias, and female-based decisions usually introduce higher bias than male-based ones. Furthermore, our evaluation demonstrates that different models prioritize decision-making criteria differently. Future research should explore bias mitigation techniques tailored for multilingual settings, considering both linguistic and cultural fac-tors to enhance fairness and inclusivity in AI-driven decision-making applications.",
            "reference_string": "[276580130 | Liu et al. | 2025 | Citations: 0]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "Most benchmarks evaluate the reasoning capabilities of LLMs. However, reasoning, behavior, and communication are shaped by culture (Tao et al., 2024). Therefore, cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
            "score": 0.7394108526796094,
            "section_title": "Challenge 3: Cultural Bias",
            "char_start_offset": 13020,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 943
                },
                {
                    "start": 946,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1304
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "265445838"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52978515625
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "There are already models and metrics that can measure and score translation quality (Rei et al., 2020). \n\nThe second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures. \n\nHowever, there are already some efforts to translate these benchmarks into multiple languages with regard to cultural biases. Note that these approaches involve human annotators that correct these biases (Singh et al., 2024). \n\nThis paper analyses the most commonly used benchmarks for multilingual LLMs for European languages. We highlight the benefits and interpretability of the existing benchmarks and outline their limitations.",
            "score": 0.6255590263807864,
            "section_title": "Introduction",
            "char_start_offset": 3603,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 106,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1022
                }
            ],
            "ref_mentions": [
                {
                    "start": 84,
                    "end": 102,
                    "matchedPaperCorpusId": "221819581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3583984375
        },
        {
            "corpus_id": "215548041",
            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
            "text": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
            "score": 0.6187021737363481,
            "section_title": "Discussion",
            "char_start_offset": 24144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 67,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 1089
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6298828125
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "Due to all these challenges and the ubiquity of language technologies that are used by large populations of non-English speaking users, addressing fairness and bias, taking into account diverse linguistic, socio-linguistic, and cultural factors, is of utmost importance. Interdisciplinary and multicultural teams are crucial to identifying, measuring, and mitigating harms caused by bias in multilingual models. Better evaluation benchmarks covering diverse linguistic phenomena and cultures will lead to better fairness evaluation. \n\nRegarding data collection, as discussed in Section 3.1, it would be prudent to avoid directly translating datasets for training or evaluation in applications where fairness is critical. As we have shown in this survey, it is not enough to collect datasets in multiple languages for measuring and mitigating bias, although even these are lacking for most languages worldwide. Zero-shot techniques that ignore the cultural nuances of a language should be used with care in fairness-critical applications, as linguistically similar languages may have different cultural values and vice versa. Finally, multilingual models and systems need to incorporate shared value systems that take into account diverse cultures, although some cultural differences may still go unacknowledged.",
            "score": 0.6158798379154118,
            "section_title": "Moving Towards Inclusive Systems in All Languages",
            "char_start_offset": 34171,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 720
                },
                {
                    "start": 721,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1311
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35888671875
        },
        {
            "corpus_id": "247626152",
            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
            "text": "Despite the need for evaluating LLMs for a wide range of languages, bias benchmarks that cover non-English languages are rare (Zhou et al., 2019;Joshi et al., 2020). As a solution, simply translating existing English benchmarks is not ideal: manual translation is a labor-intensive and highly skilled task, while automated translations are prone to errors and could potentially introduce new algorithmic sources of bias. Moreover, translated benchmarks may only test for Anglo-centric biases, which do not necessarily hold in many non-Western cultural contexts. For instance, many gender bias evaluations focus on Western professions, which are grammatically gendered in some languages (Chen et al., 2021;Zhou et al., 2019) or may not cover other prevalent occupations outside the U.S. (Escud\u00e9 Font and Costa-juss\u00e0, 2019). WinoMT (Stanovsky et al., 2019) is one of the few benchmarks that covers multiple languages, but it comes with its own downsides. The sentences are generated from templates that capture a limited range of actual language use; the samples are translated from English examples, which may not reflect how stereotypes would occur in other languages; and the scope is limited to machine translation systems, and therefore WinoMT may not be suitable for multilingual models that are not trained on this specific task. The tightly coupled nature of bias and cultural context should be emphasized when designing a multilingual bias benchmark.",
            "score": 0.6134146681761892,
            "section_title": "Bias Measures are Anglo-centric",
            "char_start_offset": 21025,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 126,
                    "end": 145,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 145,
                    "end": 164,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 705,
                    "end": 723,
                    "matchedPaperCorpusId": "202537733"
                },
                {
                    "start": 830,
                    "end": 854,
                    "matchedPaperCorpusId": "173991101"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.475341796875
        },
        {
            "corpus_id": "264405758",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "text": "Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], \n\nand ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text? \n\nFrom Table A2, it can be noted that our evaluation revealed ChatGPT to outperform other approaches in translations from English to both Chinese and Spanish, while Google Translate exhibits superior performance in English-to-Hindi translation. Thus, for optimal results in each non-English language, we harnessed the best-performing model to achieve the highest translation quality.",
            "score": 0.6095983004825729,
            "section_title": "A DETAILS OF DATASET CONSTRUCTION",
            "char_start_offset": 36719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 320
                },
                {
                    "start": 323,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 891
                },
                {
                    "start": 894,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 992
                },
                {
                    "start": 993,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1328
                },
                {
                    "start": 1329,
                    "end": 1467
                }
            ],
            "ref_mentions": [
                {
                    "start": 315,
                    "end": 319,
                    "matchedPaperCorpusId": "4623739"
                },
                {
                    "start": 614,
                    "end": 618,
                    "matchedPaperCorpusId": "248887528"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "273508009",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "text": "Our findings indicate that English-centered performance evaluation may lead to biased development, neglecting non-English languages. We hope our findings not only spark interest in Japanese performance but also motivate the community to craft a variety of high-standard benchmarks that encompass diverse cultures and their associated languages, thereby promoting more inclusive LMM development.",
            "score": 0.6024189489926807,
            "section_title": "Introduction",
            "char_start_offset": 2110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 394
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1614990234375
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "As large language models (LLMs) continue to advance in linguistic capabilities, robust multilingual evaluation has become essential for promoting equitable technological progress. This position paper examines over 2,000 multilingual (non-English) benchmarks from 148 countries, published between 2021 and 2024, to evaluate past, present, and future practices in multilingual benchmarking. Our findings reveal that, despite significant investments amounting to tens of millions of dollars, English remains significantly overrepresented in these benchmarks. Additionally, most benchmarks rely on original language content rather than translations, with the majority sourced from high-resource countries such as China, India, Germany, the UK, and the USA. Furthermore, a comparison of benchmark performance with human judgments highlights notable disparities. STEM-related tasks exhibit strong correlations with human evaluations (0.70 to 0.85), while traditional NLP tasks like question answering (e.g., XQuAD) show much weaker correlations (0.11 to 0.30). Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations. Through this comprehensive analysis, we highlight six key limitations in current multilingual evaluation practices, propose the guiding principles accordingly for effective multilingual benchmarking, and outline five critical research directions to drive progress in the field. Finally, we call for a global collaborative effort to develop human-aligned benchmarks that prioritize real-world applications.",
            "score": 0.600268206286318,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.568359375
        },
        {
            "corpus_id": "278166184",
            "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages",
            "text": "Large Language Models (LLMs) have become integral to modern Natural Language Processing (NLP) applications, demonstrating remarkable capabilities in tasks such as machine translation [33], text generation [6], and dialogue systems [26]. Despite these successes, a growing body of research indicates that LLMs can exhibit harmful social biases, including stereotypes and discriminatory attitudes. Such biases can arise from historical and cultural prejudices embedded in the data used to train these models [2,4,11,23,24,30,36,3,8]. \n\nRecent work underscores that social biases in LLMs can manifest in various forms, such as racist, sexist, or homophobic content [22]. When deployed at scale, these biases risk perpetuating stereotypes and marginalizing vulnerable communities, raising ethical concerns and emphasizing the need for bias mitigation strategies [35,16]. While significant progress has been made in quantifying and reducing biases for high-resource languages like English, cross-lingual investigations reveal that biases also affect lesser-resourced languages, often in ways that are more difficult to detect and mitigate [14,7]. \n\nPrevious frameworks for evaluating social biases in generative AI systems have primarily focused on single-language settings, limiting their applicability in global and multilingual contexts. However, bias in AI systems can manifest differently across languages and cultures, making it essential to assess models in a linguistically diverse manner. There is a growing need for tools that allow non-technical stakeholders-such as Human Resources departments, Ethics Committees, and Diversity & Inclusion officers-to evaluate how AI systems align with their values across different languages. Enabling 1 such inclusive and multilingual assessments is a crucial step toward fostering more trustworthy and equitable AI systems. \n\nEnsuring the fairness of AI systems in multilingual and culturally diverse environments requires systematic evaluation across a broad spectrum of languages, including low-resource and regionally coofficial ones. However, the development of bias evaluation benchmarks in multiple languages remains a significant challenge, particularly when non-technical stakeholders are tasked with authoring or validating prompts in languages they do not actively use. This issue is especially pronounced in settings where official languages differ from those predominantly used in professional contexts.",
            "score": 0.595396159273343,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 531
                },
                {
                    "start": 534,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1141
                },
                {
                    "start": 1144,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1867
                },
                {
                    "start": 1870,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2323
                },
                {
                    "start": 2324,
                    "end": 2459
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 506,
                    "end": 509,
                    "matchedPaperCorpusId": "121125604"
                },
                {
                    "start": 509,
                    "end": 511,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 514,
                    "end": 517,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 520,
                    "end": 523,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 526,
                    "end": 528,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1917724609375
        },
        {
            "corpus_id": "232307434",
            "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets",
            "text": "This risk of undesired downstream effects calls for future studies with a careful treatment of intertwined effects such as data size and domain, languagespecific phenomena, evaluation data and metric biases. To give the reader a brief glimpse of the impact of data quality for the example of translation, we compare the C% metric from our audit with the translation quality (sentencepiece-BLEU, spBLEU) of the multilingual translation model M2M124 for 124 languages (Goyal et al., 2021). It was trained on WikiMatrix and CCAligned, and similar data collected with the same tools, which we expect to show similar biases. Translation quality is evaluated on the trusted, humantranslated FloReS benchmark (Goyal et al., 2021). For the 21 languages present in both the audit and the FloReS benchmark, we found a positive correlation (Spearman) between the data quality scores and spBLEU of \u03c1 = 0.44 (p = 0.041). This is not as large as the correlation with data size (\u03c1 = 0.66, p = 0.00078), but it nonetheless helps to explain translation quality-the correlation between the product of C% and data size (in other words, the expected total number of good sentences in the dataset), is the highest yet, with a value of \u03c1 = 0.73 (p = 0.00013).12 Representation washing Since there are datasets which contain many low-resource languages, the community may feel a sense of progress and growing equity, despite the actual quality of the resources for these languages. Similarly, if low-quality datasets are used as benchmarks they may exaggerate model performance, making low-resource NLP appear more solved than it is-or conversely, if models perform poorly when trained with such data, it may be  Table 6: Examples of \"parallel\" data where the translation has a different meaning than the source, but the form looks the same. (We added translations of the non-English side.) Such data may encourage hallucinations of fake \"facts\". wrongly assumed that the task of learning models for these languages is harder than it actually is or infeasible given current resources.",
            "score": 0.5803492042317516,
            "section_title": "Dataset Mis-labeling",
            "char_start_offset": 28749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 619
                },
                {
                    "start": 620,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2061
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1514892578125
        },
        {
            "corpus_id": "258740735",
            "title": "PaLM 2 Technical Report",
            "text": "forms Translate in three high-resource languages: Spanish, Polish and Portuguese. Surprisingly, we see improved translation quality in Italian and Japanese, but lower gender agreement scores. We also find reduced gender agreement scores in Telugu, Hindi and Arabic as compared to PaLM, as shown in Figure 33. These findings highlight the importance of evaluating specific failures with potential for harm, and that multilingual performance may vary by use, task, and prompting (e.g., Section 4.2 found improvements in Arabic and Telugu on 1-shot TyDi QA, while this evaluates zero-shot capabilities). Overall, with zero-shot prompting we observe higher potential for misgendering harms when translating out of English as compared to when translating into English.  While there are exceptions, for both PaLM and PaLM 2, in Figure 34 we observe a broad relationship between translation quality and percentage of pre-training data from web documents in that language. Most languages that are represented in over 2% of the training data show average quality scores over 2.50, with Arabic being the notable exception. This suggests that multilingual training data is one important part of mitigating potential misgendering harms in translation uses, but that this is not sufficient to ensure quality across languages.  Limitations. For gender-related errors in translation systems, evaluations do not consider differential harms to people related to expressing non-binary gender identities (Keyes, 2018;Dev et al., 2021a), or consider contested perspectives on pronouns across languages and cultures (Lee, 2019). Moreover, while gender agreement into English is amenable to automatic evaluation, evaluation of gender agreement out of English remains challenging and time-intensive. Finally, we note that our evaluations focus on only a subset of potential risks , and that our evaluations focus on model outputs without considering the wider sociotechnical context in which instruction-finetuned language models exist (Shelby et al., 2023). See Appendix D.8 for measurement quality rubric for this evaluation when translating into English.",
            "score": 0.5787497004392773,
            "section_title": "D.5.2 Translating from English",
            "char_start_offset": 114381,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1661376953125
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "This section focuses on the issue of bias evaluation in MLLMs. Extensive studies have developed varied datasets and approaches that serve as benchmarks for bias assessment. In this section, we provide a thorough review of these benchmarks. Table VI illustrates benchmarks commonly used for evaluating bias. Notably, these datasets primarily focus on bias attributes related to gender and occupation [146]- [148], predominantly available in English [149]- [152]. Several datasets also encompass languages such as Spanish, German, and French [128] [134]. \n\nBased on the tasks and languages, benchmarks in Table VI can be categorized into three types: general benchmarks, taskspecific benchmarks, and language-specific benchmarks. \n\nGeneral benchmarks mainly refer to evaluation benchmarks that have a wide range of applications and can be used for different tasks, including some major evaluation metrics and datasets. For example, Association Test (WEAT, SEAT, and CEAT) [153]- [155] are widely used to measure bias in word-, sentence-and contextualized-level embeddings; GLUE [150] is designed to measure the impact that the introduced debiasing techniques will have on downstream performance by evaluating the capabilities of the NLP model. \n\nTask-specific benchmarks refer to benchmark datasets designed for a specific task or situation. For example, Winogender [146] and WinoBias [147] are applicable for the coreference resolution system; CrowS-Pairs [151] is designed for detecting bias against social groups, particularly in the United States. \n\nMultilingual benchmarks refer to the benchmark datasets in multilingual contexts, including MIBs [128] and MozArt [134]. The lack of robust multilingual evaluation benchmarks poses significant barriers to assessing biases in multilingual contexts. Therefore, creating more multilingual evaluation datasets is an urgent problem to be solved. One potential solution is to translate existing bias benchmarks that mainly only cover English [156], [157]. Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences.",
            "score": 0.5743981907572108,
            "section_title": "B. Bias Benchmark",
            "char_start_offset": 48672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 916
                },
                {
                    "start": 917,
                    "end": 1241
                },
                {
                    "start": 1244,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1549
                },
                {
                    "start": 1552,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 1892
                },
                {
                    "start": 1893,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 399,
                    "end": 404,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 406,
                    "end": 411,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 448,
                    "end": 453,
                    "matchedPaperCorpusId": "21670658"
                },
                {
                    "start": 455,
                    "end": 460,
                    "matchedPaperCorpusId": "58006082"
                },
                {
                    "start": 540,
                    "end": 545,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 546,
                    "end": 551,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 970,
                    "end": 975,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 977,
                    "end": 982,
                    "matchedPaperCorpusId": "219530686"
                },
                {
                    "start": 1076,
                    "end": 1081,
                    "matchedPaperCorpusId": "5034059"
                },
                {
                    "start": 1364,
                    "end": 1369,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 1383,
                    "end": 1388,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 1455,
                    "end": 1460,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 1649,
                    "end": 1654,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1666,
                    "end": 1671,
                    "matchedPaperCorpusId": "252547725"
                },
                {
                    "start": 1988,
                    "end": 1993,
                    "matchedPaperCorpusId": "135465247"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61083984375
        },
        {
            "corpus_id": "274464561",
            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
            "text": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges. In this work, we trace the impact of both of these issues on multilingual evaluations and ensuing model performances. Our large-scale evaluation of state-of-the-art open and proprietary models illustrates that progress on MMLU depends heavily on learning Western-centric concepts, with 28% of all questions requiring culturally sensitive knowledge. Moreover, for questions requiring geographic knowledge, an astounding 84.9% focus on either North American or European regions. Rankings of model evaluations change depending on whether they are evaluated on the full portion or the subset of questions annotated as culturally sensitive, showing the distortion to model rankings when blindly relying on translated MMLU. We release Global MMLU, an improved MMLU with evaluation coverage across 42 languages -- with improved overall quality by engaging with compensated professional and community annotators to verify translation quality while also rigorously evaluating cultural biases present in the original dataset. This comprehensive Global MMLU set also includes designated subsets labeled as culturally sensitive and culturally agnostic to allow for more holistic, complete evaluation.",
            "score": 0.5682764628363636,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.921875
        },
        {
            "corpus_id": "271050875",
            "title": "Do Multilingual Large Language Models Mitigate Stereotype Bias?",
            "text": "In our work, we use machine translation to evaluate monolingual and multilingual models across multiple languages.Using machine translation might affect the quality and the expression of bias of the translated datasets.By evaluating the translation process with human evaluators as described in \u00a73.2, we aim to reduce these effects.Nevertheless, we are aware that the small number of annotators might decrease the significance of our results as in particular the evaluation of the bias in the translation is influenced by the perception of the annotator.In future work, we aim to extend this evaluation to all the studied languages and to more native annotators and methods that can ensure the quality of the automated translations.\n\nThe biases that exist in the benchmarks we used may be specific to English speaking regions.When translating the benchmark, bias may decrease because the biases that manifest in the translated language are specific to the regions that speak that language, which might not be the same as English speaking regions.Future work should consider creating new bias benchmarks for each language that represent the biases of the populations that speak those languages.Without this, we cannot be sure that the translated benchmarks cover the biases that are likely to occur in a given language.The significance of our results might be limited by CrowSpairs quality as shown in Blodgett et al. (2021).Blodgett et al. (2021) finds that 97% of the dataset are not admissible.Generating a french version of CrowS-pairs, also N\u00e9v\u00e9ol et al. (2022) scrutinizes and even improves the original CrowS-pairs dataset.They present the statistics of the different adaptation types (compare Table 2 in (N\u00e9v\u00e9ol et al., 2022).In addition to the sentences modified to suit the French culture, 150 samples in total (10% of the dataset) were adapted due to the identified limitations within the original CrowS-pairs dataset (non-minimal pairs (22), double switches (64) or bias-type mismatches ( 64)).Even if the findings of (Blodgett et al., 2021) show severe shortcomings, we decided on using CrowS-Pairs due to its broad usage in the literature and its coverage of many different bias categories and social groups.",
            "score": 0.5645498983660582,
            "section_title": "Limitations",
            "char_start_offset": 28902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 114,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 732
                },
                {
                    "start": 734,
                    "end": 826
                },
                {
                    "start": 826,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1318
                },
                {
                    "start": 1318,
                    "end": 1424
                },
                {
                    "start": 1424,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1629
                },
                {
                    "start": 1629,
                    "end": 1733
                },
                {
                    "start": 1733,
                    "end": 2005
                },
                {
                    "start": 2005,
                    "end": 2221
                }
            ],
            "ref_mentions": [
                {
                    "start": 1401,
                    "end": 1423,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1424,
                    "end": 1446,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1545,
                    "end": 1565,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 1711,
                    "end": 1732,
                    "matchedPaperCorpusId": "248780290"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.348876953125
        },
        {
            "corpus_id": "274436231",
            "title": "Uhura: A Benchmark for Evaluating Scientific Question Answering and Truthfulness in Low-Resource African Languages",
            "text": "The translation process did not always yield perfectly parallel datasets across the six languages. \n\nCultural and linguistic differences meant that some questions in the original English benchmarks could not be directly translated or were not culturally relevant. This non-parallelism may affect the comparability of results between languages, as certain languages might have slightly different sets of questions or modified content, potentially influencing the difficulty level and the models' performance.",
            "score": 0.5640384857853604,
            "section_title": "Non-Parallel Translation Across Languages",
            "char_start_offset": 12780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 101,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 507
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.408203125
        },
        {
            "corpus_id": "269149051",
            "title": "From Bytes to Borsch: Fine-Tuning Gemma and Mistral for the Ukrainian Language Representation",
            "text": "A substantial number of open-source models are available on the market today.At the same time all these models demonstrate a notable bias towards the English language due to their training conditions.The bias can manifest itself in a number of ways, including to but not limited to the following:\n\n1. Language and cultural bias.This can impair a model's usability for non-English speakers and also perpetuate stereotypes or misunderstandings about cultures.\n\n2. Ethical and fairness concerns.The same model may show considerably better performance with English-speaking users, leaving others with a subpar experiences.\n\n3. Uneven knowledge representation.This can lead to a skewed representation of global knowledge, history, and perspectives, and embed these biases into the model's outputs and decision-making processes.\n\nThe bias becomes particularly prominent in non-European languages and languages that do not use a Latin alphabet.This has naturally motivated numerous scholars and enthusiasts to put much efforts into finetuning open-source models, predominantly LLaMa 2, in many languages, both European (Basile et al., 2023;Vanroy, 2023) and non-European (Cui et al., 2024;Gala et al., 2024a,b;Nguyen et al., 2023;Azime et al., 2024;Kohli et al., 2023).Most of the listed articles have been published within the last months, and demonstrate great interest and involvement in solving this linguistic bias issue.The immediate benefits of having an open-source model that is fine-tuned with a certain language include: 1. Reduction or elimination of cultural bias.\n\n2. Flexibility in use-cases, including both academic and business.\n\n3. Preservation of rare and low-resource languages.\n\nThe effort also promotes the creation of languagespecific datasets and development of the LLMoriented ecosystem.Even when a particular model becomes obsolete, further progress is greatly facilitated by this groundwork.",
            "score": 0.5612659912852782,
            "section_title": "Motivation and objective",
            "char_start_offset": 4062,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 77,
                    "end": 200
                },
                {
                    "start": 200,
                    "end": 296
                },
                {
                    "start": 298,
                    "end": 328
                },
                {
                    "start": 328,
                    "end": 457
                },
                {
                    "start": 459,
                    "end": 492
                },
                {
                    "start": 492,
                    "end": 618
                },
                {
                    "start": 620,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 822
                },
                {
                    "start": 824,
                    "end": 937
                },
                {
                    "start": 937,
                    "end": 1262
                },
                {
                    "start": 1262,
                    "end": 1419
                },
                {
                    "start": 1419,
                    "end": 1570
                },
                {
                    "start": 1572,
                    "end": 1638
                },
                {
                    "start": 1640,
                    "end": 1691
                },
                {
                    "start": 1693,
                    "end": 1805
                },
                {
                    "start": 1805,
                    "end": 1911
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138671875
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "As Large Language Models (LLMs) are now capable of producing fluent and coherent content in languages other than English, it is not imperative to precisely evaluate these non-English outputs. However, when assessing the outputs from mutlilingual LLMs, prior works often employed LLM based evaluators that excel at assessing English outputs, without a thorough examination of whether these evaluators could effectively assess non-English text as well. Moreover, existing benchmarks to test evaluator LLMs (referred to as\"meta-evaluation benchmarks\") are mostly English-centric. To bridge this gap and examine whether evaluator LLMs can reliably assess the outputs of multilingual LLMs, we introduce MM-Eval, a multilingual meta-evaluation benchmark comprising five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. A core attribute of MM-Eval is that, instead of merely translating existing English meta-evaluation benchmarks, it is designed with multilingual-specific challenges in mind. Additionally, unlike existing meta-evaluation benchmarks that focus solely on ranking accuracy over pairwise data, MM-Eval also evaluates the consistency and fairness of absolute score values across a wide range of languages. Our results show that existing evaluator LLMs that excel in English contexts have considerable room for improvement when assessing non-English outputs. Furthermore, we find that evaluators are unfair and inconsistent when evaluating lower-resourced languages. Finally, we validate MM-Eval by measuring its correlation with Best-of-N rankings, finding a significantly stronger correlation compared to other meta-evaluation benchmarks. We publicly release our benchmark and code.",
            "score": 0.5601164606033452,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2080078125
        },
        {
            "corpus_id": "245328293",
            "title": "Coding Issues of Open-Ended Questions in a Cross-Cultural Context",
            "text": "Linguistic systems differ from one another, and translation does not usually mean that only one correct translation can be produced. In many cases, there is room for interpretation due to unclear or missing context, which might give the translated text a specific direction and might change the meaning slightly. Subjectivity is an integral component of translation and cannot be eliminated (Behr 2015). Thus, each translation might be a source of bias or error resulting from text interpretation by the translator. The translation of a categorization scheme into several languages might produce systematic biases or errors when the translated versions are used for coding. \n\nTranslations of answers to open-ended questions might also produce biases or errors in the coding of one or more terms. In the process of coding answers to open-ended questions in multilingual, cross-cultural surveys, translation might thus result in deviating coding and analysis results. Due to the role of translation in the whole process of answer coding, it is thus essential to test for its impact on data quality. To test for intercoder reliability, Behr (2015) implemented double coding for translated texts. \n\nAfter coding English-language translations of responses to open-ended questions in a cross-national survey, Thrasher et al. (2011) had the codings reviewed by those who collected the original data in the non-English-speaking study countries and discrepancies were discussed and resolved by the country teams and the central study coordination.",
            "score": 0.5540759453820912,
            "section_title": "Translation as a source of error",
            "char_start_offset": 9296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 312
                },
                {
                    "start": 313,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 673
                },
                {
                    "start": 676,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1192
                },
                {
                    "start": 1195,
                    "end": 1538
                }
            ],
            "ref_mentions": [
                {
                    "start": 391,
                    "end": 402,
                    "matchedPaperCorpusId": "144063904"
                },
                {
                    "start": 1133,
                    "end": 1144,
                    "matchedPaperCorpusId": "144063904"
                },
                {
                    "start": 1303,
                    "end": 1325,
                    "matchedPaperCorpusId": "76663995"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38330078125
        },
        {
            "corpus_id": "265157921",
            "title": "A Material Lens on Coloniality in NLP",
            "text": "By this process, even multilingual models are demonstrably skewed towards Western, and especially American, values and phrasing (Johnson et al., 2022;Arora et al., 2023;Naous et al., 2023;Papadimitriou et al., 2023;Durmus et al., 2023). \n\nIdeally, benchmark evaluations would capture cultural discrepancies. However, data curators often translate existing annotated data (Artetxe et al., 2020a), likely aiming to extend an English-centric resource with accepted value to new languages. These datasets may cover many languages, but the names, places, events, and other cultural concepts are transferred into the evaluation for new languages. As an example, out of the 47 Wikipedia articles in the XQuaD (Artetxe et al., 2020a) test set: 13 are primarily about North America (e.g. Super Bowl 50), 11 on Europe (e.g. Scottish Parliament), 2 on Asia (Yuan Dynasty & Genghis Khan), 1 on Australia (the state of Victoria), 1 on Africa (Kenya), 1 on Latin America (Amazon Rainforest), 1 on the Middle East (Islamism), and the remaining 17 are location agnostic (e.g. Oxygen). This is common practice: 8 out of 12 tasks across in the widely used multi-task XTREME (Hu et al., 2020) and XTREME-R (Ruder et al., 2021) benchmarks use data translated from English. However, such datasets may struggle to capture cultural misalignment and may in fact reward systems which replicate, especially in the range of tasks where cultural understanding has been shown to influence accuracy (Mohammad et al., 2016;Smith et al., 2016;Asai et al., 2021;Bauer et al., 2023;Lee et al., 2023;Akinade et al., 2023). \n\nIn this case, the data requirements of existing algorithms influence researchers to sample increasing amounts of Western data and the cultural biases of datasets underestimate the negative effects of this process.",
            "score": 0.5540106462836929,
            "section_title": "English Centric Multilingualism",
            "char_start_offset": 37466,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 239,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1068
                },
                {
                    "start": 1069,
                    "end": 1252
                },
                {
                    "start": 1253,
                    "end": 1587
                },
                {
                    "start": 1590,
                    "end": 1803
                }
            ],
            "ref_mentions": [
                {
                    "start": 150,
                    "end": 169,
                    "matchedPaperCorpusId": "247748753"
                },
                {
                    "start": 188,
                    "end": 215,
                    "matchedPaperCorpusId": "252815840"
                },
                {
                    "start": 371,
                    "end": 394,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 702,
                    "end": 725,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 1156,
                    "end": 1173,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1187,
                    "end": 1207,
                    "matchedPaperCorpusId": "233241004"
                },
                {
                    "start": 1469,
                    "end": 1492,
                    "matchedPaperCorpusId": "14935137"
                },
                {
                    "start": 1492,
                    "end": 1511,
                    "matchedPaperCorpusId": "17206686"
                },
                {
                    "start": 1511,
                    "end": 1529,
                    "matchedPaperCorpusId": "225040672"
                },
                {
                    "start": 1529,
                    "end": 1548,
                    "matchedPaperCorpusId": "258378215"
                },
                {
                    "start": 1548,
                    "end": 1565,
                    "matchedPaperCorpusId": "258486940"
                },
                {
                    "start": 1565,
                    "end": 1586,
                    "matchedPaperCorpusId": "258486877"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.253662109375
        },
        {
            "corpus_id": "270703607",
            "title": "Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models",
            "text": "We note such barrier exists even for the state-of-the-art models like GPT-4 (e.g., 81.82 \u2212 \u2192 68.61 when comparing English to mixup-translated MMLU). (2) The GT-option translation setting leads to the worst performance, indicating an inherent behavioral bias of LLMs that tends to avoid selecting a non-English option, even if it is the correct choice. This bias is further supported by the controlled comparisons in One-wrong-option translation settings, where LLMs achieve even higher accuracy than the English setting, as the model leverages the bias and avoids selecting the (incorrect) non-English option. (3) LLMs obtain higher accuracy on Question-translated and Options-translated settings than Full-translated settings, likely because the MCQs under the former two settings still have remaining context in English, which helps the models perform better. fr de es it m s da fi no ru zh he ar hi bn am Evaluation on 16 languages. To demonstrate the universality of our findings, we evaluate 11 additonal languages: (1) Low-resource languages (Zhang et al., 2023c): Malay (ms), Danish (da), Finnish (fi), Norwegian (no), Bengali (bn), Amharic (am); (2) Languages with token distributions significantly different from English: Russian (ru), Chinese (zn), Hebrew (he), Arbic (ar) and Hindi (hi). Fig. 4 shows the performance gaps between English (dashed line) and other languages persist in both monolingual (Full translation) and mixed-language (Options/GT-option translation) settings. This gap is particularly evident for lowresource languages like Finnish (fi), Bengali (bn), and Amharic (am), revealing the general challenge of crosslingual knowledge barriers. Moreover, Llama-3.1-8B has a more balanced performance across languages than Qwen2.5-7B and Aya-expanse-8B. For most non-English languages, multilingual LLM show the weakest performance when ground-truth options require cross-lingual reasoning.",
            "score": 0.5538829573742807,
            "section_title": "Identify crosslingual knowledge barrier",
            "char_start_offset": 14663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1913
                }
            ],
            "ref_mentions": [
                {
                    "start": 1048,
                    "end": 1069,
                    "matchedPaperCorpusId": "252355474"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09332275390625
        },
        {
            "corpus_id": "267770422",
            "title": "OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models",
            "text": "Applicability and Generalizability Language usage varies significantly across different social and cultural groups (Trudgill, 2000), highlighting the need for datasets that capture various linguistic expressions and cultural contexts. This confirms that we evaluate language models in a manner that mirrors their real-world applicability across diverse user groups.Figure 2 presents idioms from Chinese, Russian, and Arabic cultures. Individuals unfamiliar with a particular culture often find it challenging to grasp the intended meaning. Incorporating folk sayings into language models serves as an effective measure for evaluating their ability to navigate subtle cross-cultural differences. \n\nCombating Cultural Hegemony English benchmarks tend to exhibit geographical biases towards the domestic knowledge of the regions that produce them (Huang et al., 2023). Gramsci (2020)points to the suppression of minority or marginalized cultures by dominant cultural norms and values. Language models limited to monolingual or monocultural datasets have been identified within the Ethical AI domain as prone to developing biases (Talboy and Fuller, 2023;Thakur, 2023;Tao et al., 2023). If a dataset only includes holidays mainly celebrated in the United States, such as Thanksgiving, it may unintentionally prioritize and spread specific cultural norms and values. \n\nReal-World Scenario Simulation The meaning of language is derived from its use in real-life situations (Rabiah, 2018). Thus, when evaluating LLMs, questions that would naturally occur in everyday situationsit is essential. For example,Thanksgiving is almost never mentioned in the context of Chi- nese people. Traditional multilingual benchmarks for NLP tasks, often derived by translating English datasets into other languages, have been criticized for introducing an English-centric bias (Liu et al., 2021). This occurs because the translation process may need to encapsulate culturally specific concepts or nuances inherent to the target languages. \n\nThe predominance of English in existing datasets overlooks the gnored the real-life situations of people speaking other languages, particularly those considered low-resource (Singh et al., 2024).",
            "score": 0.5528234307007087,
            "section_title": "Importance of Multilingo Evaluation",
            "char_start_offset": 3737,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 865
                },
                {
                    "start": 866,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1361
                },
                {
                    "start": 1364,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1873
                },
                {
                    "start": 1874,
                    "end": 2015
                },
                {
                    "start": 2018,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 866,
                    "end": 880,
                    "matchedPaperCorpusId": "153062935"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.406982421875
        },
        {
            "corpus_id": "270737791",
            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
            "text": "The simplest approach is to translate these same tests into other languages and use them for multilingual evaluation.This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results.\n\nIn this work we perform an initial analysis of the impact of automatic translation on one of the most widely used LLM benchmarks, the Multitask Language Understanding (MMLU) test Hendrycks et al. (2021a) for one of the most commonly used and chosen as a second language to learn, Spanish.The analysis shows that automatic translation induces errors in the LLM answers and thus distorts the benchmark's results.Based on these findings, potential solutions to this problem are also briefly discussed.\n\nThe rest of the work is organized as follows, in section 2 the methodology used in our analysis is presented, followed by the results in section 3 and a discussion of their implications and potential solutions in section 4. The paper ends with the conclusion in section 5.",
            "score": 0.5484130154180287,
            "section_title": "Introduction",
            "char_start_offset": 2137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 117,
                    "end": 259
                },
                {
                    "start": 259,
                    "end": 341
                },
                {
                    "start": 341,
                    "end": 379
                },
                {
                    "start": 379,
                    "end": 853
                },
                {
                    "start": 853,
                    "end": 1066
                },
                {
                    "start": 1068,
                    "end": 1356
                },
                {
                    "start": 1356,
                    "end": 1478
                },
                {
                    "start": 1478,
                    "end": 1566
                },
                {
                    "start": 1568,
                    "end": 1840
                }
            ],
            "ref_mentions": [
                {
                    "start": 1247,
                    "end": 1271,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54248046875
        },
        {
            "corpus_id": "272635983",
            "title": "Navigating the Convenience Trap with ChatGPT and Google Translate: The Risks of Homogenization in Translation Teaching for Vietnamese Students",
            "text": "The dominance of English as the global lingua franca presents significant risks of homogenization, particularly in translation. Homogenization, the reduction of linguistic and cultural diversity, occurs when language is standardized to fit a dominant model, often English. This trend can threaten the preservation of unique cultural and linguistic identities, as English is extensively used in international communication, academia, and business (Phillipson, 1992; Skutnabb-Kangas, 2010). \n\nUsing English as a standard language in translation can lead to the loss of linguistic diversity. Translating non-English texts into English often results in the loss of cultural nuances and linguistic idiosyncrasies. The translation process tends to prioritize clarity and simplicity for English-speaking audiences, which can make translations less culturally rich and less representative of the original text (Venuti, 1995). For instance, idioms and culturally specific references might be rendered into more generic English phrases, diminishing their original cultural significance (Graddol, 2006;Pike, 2013). This effect is evident in the training of large language models (LLMs), such as those used by AI translation tools like ChatGPT and Google Translate. These models are often trained on English-dominated datasets, creating a bias towards English norms and expressions. This bias can marginalize less dominant languages, leading to inaccuracies and less nuanced translations and reinforcing English dominance (Bender et al., 2021). \n\nAdditionally, translating texts into English often simplifies complex cultural concepts to fit a universally understood framework, which can erase unique cultural markers and replace them with generic terms that lack authenticity (Graddol, 2006;Pike, 2013). Authors like Haruki Murakami, who adapt their work to global audiences, may simplify cultural nuances to make their writing more translatable into English, broadening their reach but potentially altering the cultural depth of their work (Snyder, 2017). The pervasive use of English as a default language in translation raises concerns about cultural sensitivity and authenticity. Accurate translation requires preserving the cultural context of the original language but using English as an intermediary risks overlooking or misinterpreting cultural subtleties (Pennycook, 2001;Verbaltrans, 2023).",
            "score": 0.5455619877918223,
            "section_title": "The Risks of Homogenization Posed by English in Translation",
            "char_start_offset": 12021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1532
                },
                {
                    "start": 1535,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2390
                }
            ],
            "ref_mentions": [
                {
                    "start": 1510,
                    "end": 1531,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "274822728",
            "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation",
            "text": "In Table 9 and Table 10 we display the results of the initial English evaluation of instruction-tuned and base models respectively. Even though the most biased and toxic models on each dataset have barely changed, models are much more toxic and biased in English compared to non-English languages. Unsurprisingly, language consistency is much higher in English. Perplexity is similar to that in non-English languages, and diversity is somewhat higher in English. Upon finetuning (see Figure 12 and Figure 13 for instruction-tuned models and Figure 14 and Figure 15 for base models), bias and toxicity scores for English look similar to those for non-English languages. As expected, we do not observe the same decrease in language consistency as in non-English languages. Instead for instruction-tuned models we see a decrease in diver-  Table 10: Initial evaluation of base models in English. For the CrowS-Pairs and StereoSet benchmarks, the ideal bias score is 50. The ideal EMT score is 0, and a score greater than 0.5 means that the expected most toxic generation for a prompt is toxic. The highest bias or toxicity score on each evaluation dataset is indicated in bold. Diversity is the percentage of unigrams in the model's generated continuation that did not occur in the input prompt. Language consistency is the percentage of continuations generated by the model that are entirely in the prompting language.",
            "score": 0.5448489573486877,
            "section_title": "J Evaluation in English",
            "char_start_offset": 41090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1416
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053314208984375
        },
        {
            "corpus_id": "277066532",
            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
            "text": "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023). \n\nRecently, NLP researchers have addressed this limitation; several diverse linguistic and cultural context MMLU evaluation benchmarks have been developed to address the gap, such as Ara-bicMMLU (Koto et al., 2024) evaluates tasks from North Africa, the Levant, and the Gulf, In-doMMLU (Koto et al., 2023) incorporates Indonesia's local languages and cultures. Others include, CMMLU (Li et al., 2023), KMMLU (Son et al., 2024),TurkishMMLU (Y\u00fcksel et al., 2024), PersianMMLU (Ghahroodi et al., 2024), Global MMLU (Singh et al., 2024) evaluates cultural and linguistic bias in 42 languages. \n\nDespite these advancements, significant gaps remain for African LRLs and some Eastern European languages. Our work narrows this gap, by including Giriama, a LRL language spoken by approximately 2.6 million people in Kenya, which is currently absent from existing NLP benchmarks. Similarly, even though the Latvian language has been benchmarked before EU20-MMLU (Thellmann et al., 2024), with relatively high-quality datasets, the datasets lack human-curated translations. As a result, they encounter challenges, including inaccurate auto-translations that compromise reliable evaluations. Giriama, in particular, has a limited digital presence and has historically been underrepresented in linguistic resources, making this the first multilingual benchmark developed explicitly for the language. Our contributions can be summarized as follows: \n\n\u2022 Novel Datasets: we present LAG-MMLU, a curated dataset that includes Giriama's first gold-standard (human-translated) collection, alongside two silver-standard datasets for Latvian: one machine-translated and another machine-translated with subsequent human curation.",
            "score": 0.5443118664324464,
            "section_title": "Introduction",
            "char_start_offset": 1863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 871
                },
                {
                    "start": 874,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1717
                },
                {
                    "start": 1720,
                    "end": 1989
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62890625
        },
        {
            "corpus_id": "273950544",
            "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
            "text": "Significant efforts have been made to include diverse languages in the development of multilingual language models (Conneau et al., 2020;Xue et al., 2021). Rust et al. (2021) conducted a comparison between multilingual and monolingual language models, employing metrics such as subword fertility. Subword fertility, defined as the ratio of subtokens to total tokens, has been shown to have a direct correlation with model performance across languages, illustrating the impact of tokenization on multilingual language model efficacy. Apart from architecture-based evaluation, multilingual benchmarks help us to track the progress toward multilingualism. \n\nCurrent evaluation benchmarks prioritize multiple-choice questions due to the relative ease of automatic scoring, as opposed to open-ended question benchmarks that demand significant human involvement (Son et al., 2024;Wang et al., 2024b). For example, MMLU-Pro (Wang et al., 2024b) places a strong emphasis on prompt variations and their influence on large language model (LLM) performance. \n\nCultural significance of LLM benchmarks is crucial factor to consider as part of language understanding. To incorporate cultures into benchmarks, Myung et al. (2024) introduced BLEnD, which covers 16 countries and 13 languages to prepare datasets that have tests of significance for users in their region. Additionally Liu et al. (2023) shows proverbs can be used to assess LLMs cultural understanding in several languages and introduces MAPS (Multicultural Proverbs and Sayings) dataset based on proverbs and sayings to evaluate LLMs multilingual and cultural understanding ability. Our work adopts the same motivation to use proverbs and expands it to different languages and task types.",
            "score": 0.540478096558702,
            "section_title": "Related Work",
            "char_start_offset": 4729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 156,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 652
                },
                {
                    "start": 655,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1738
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 137,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 156,
                    "end": 174,
                    "matchedPaperCorpusId": "229924220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0762939453125
        },
        {
            "corpus_id": "265466658",
            "title": "CDEval: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
            "text": "Our proposed benchmark represents a step forward in analyzing the cultural dimensions of large language models. However, our work still has limitations and challenges. Firstly, in our experiment, data in languages other than English was obtained via Google Translate. This introduces potential inaccuracies or other factors that could impact the results of cultural assessments. In the future work, we plan to extract a subset from the dataset, for example, 100 entries for each dimension, and have native speakers or language experts from the corresponding countries translate them to ensure the accurate expression of the questionnaire in other languages. Furthermore, we will examine the extent to which machine translation influences the experimental results. Moreover, the scope of cultural dimensions we have explored is confined to six, which might be limiting in real-world applications. \n\nFor open generation tasks, due to the difficulty of evaluation, we conducted some case studies. Lastly, a critical and impending task is the development of an automated method for the cultural assessment of generative tasks.",
            "score": 0.5395908212696392,
            "section_title": "Limitations",
            "char_start_offset": 22713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 895
                },
                {
                    "start": 898,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1776123046875
        },
        {
            "corpus_id": "270688323",
            "title": "PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data",
            "text": "Recent works have shown that LLMs perform worse on non-English languages, particularly those written in scripts other than the Latin script, and under-resourced languages (Ahuja et al., 2023(Ahuja et al., , 2024;;Asai et al., 2024). Studies on cultural values in LLMs have also shown that frontier models such as GPT-4 align more closely to Western, Rich, and Industrialized norms (Rao et al., 2023). This has led to a proliferation of models being built for specific languages, cultures and regions such as Indic, Arabic, African, Chinese, European, and Indonesian (Gala et al., 2024;Sengupta et al., 2023;Zeng et al., 2023;Bai et al., 2023;Cahyawijaya et al., 2024;Cohere, 2024;\u00dcst\u00fcn et al., 2024, interalia). Multilingual evaluation is challenging due to the small number of multilingual benchmarks available, the lack of language diversity in them (Ahuja et al., 2022) and the evidence of possible contamination of many of these benchmarks (Ahuja et al., 2024). Additionally, many multilingual benchmarks are translations of benchmarks originally created in English, leading to loss of linguistic and cultural context. \n\nIn this work, we perform 90K human evaluations -the largest scale multilingual human evaluation of LLMs as per our knowledge. We perform evaluation on a new set of general and culturally-nuanced prompts created independently by native speakers for each language. We use a setting similar to the LMSys ChatbotArena (Chiang et al., 2024) and ask human evaluators employed by KARYA 2 , an ethical data company, to perform two evaluation tasks: comparative evaluations between models, and individual evaluations or direct assessments of 30 models. KARYA employs workers from all states of India, with a focus on rural and marginalized communities, making our study the first effort as per our knowledge that includes these communities in the evaluation process.",
            "score": 0.5379233583163007,
            "section_title": "Introduction",
            "char_start_offset": 1754,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1122
                },
                {
                    "start": 1125,
                    "end": 1250
                },
                {
                    "start": 1251,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1882
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 190,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 190,
                    "end": 213,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 381,
                    "end": 399,
                    "matchedPaperCorpusId": "263835359"
                },
                {
                    "start": 607,
                    "end": 625,
                    "matchedPaperCorpusId": "252715691"
                },
                {
                    "start": 852,
                    "end": 872,
                    "matchedPaperCorpusId": "248780386"
                },
                {
                    "start": 944,
                    "end": 964,
                    "matchedPaperCorpusId": "265150423"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1868896484375
        },
        {
            "corpus_id": "276580130",
            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
            "text": "We examine gender bias as a robustness check alongside the linguistic and cultural diversity of the selected countries: how LLMs may perpetuate or mitigate biases in different academic and societal contexts. We focus on assessing whether the persona-driven responses maintain robustness or exhibit vulnerability when subjected to crosslingual tasks and the impact of language-specific cultural nuances on bias amplification. Further details for the distribution could be found in Appendices C. \n\nOur analysis reveals model-specific trends in gender bias. GPT-4 exhibits stronger female bias in most non-English languages. This means that female-associated outputs introduce greater linguistic or cultural variability in these languages. Conversely, GPT-3.5 shows pronounced female bias in certain regions, particularly in Korea (0.65 vs. 0.30) and Japan (0.19 vs. 0.12). And Sonnet displays relatively weaker gender-based divergence. Hence, Sonnet exhibits less gender-sensitive variability compared to GPT-3.5 and GPT-4. These findings highlight the interaction between language, gender, and model architecture, suggesting that biases are not only model-dependent but also sensitive to linguistic and cultural contexts.",
            "score": 0.5371115263965061,
            "section_title": "Gender Bias",
            "char_start_offset": 29832,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 424
                },
                {
                    "start": 425,
                    "end": 493
                },
                {
                    "start": 496,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1220
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1533203125
        },
        {
            "corpus_id": "247594499",
            "title": "Challenges and Strategies in Cross-Cultural NLP",
            "text": "Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations. \n\nPonti et al. ( 2020) point out that literal translation of datasets is sometimes impossible or undesirable due to culture-specific concepts in the source that may be missing or unnatural in the target. In their multilingual extension of the English Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) dataset, they therefore asked \"carefully chosen\" human translators to perform culturally-sensitive translation, and either paraphrase, substitute the original concepts with similar ones that exist in the target language, or leverage phonetically transcribed loan words. \n\nHuman translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021;Ponti et al., 2020;Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way.",
            "score": 0.5370944783640763,
            "section_title": "Data Collection",
            "char_start_offset": 25564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1691
                },
                {
                    "start": 1694,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2104
                }
            ],
            "ref_mentions": [
                {
                    "start": 793,
                    "end": 811,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1947,
                    "end": 1965,
                    "matchedPaperCorpusId": "238198104"
                },
                {
                    "start": 1965,
                    "end": 1984,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 1984,
                    "end": 2001,
                    "matchedPaperCorpusId": "237503047"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "259108559",
            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
            "text": "From a model diagnostics perspective, Cao et al. (2021) found that English PLMs might be biased to making specific predictions based on a predicate's template irrespective of the subjects used to populate this template. Thereafter, Elazar et al. (2023) designed a causal framework for modeling multiple co-occurrence statistics that might cause English PLMs to achieve high scores on some of LAMA's predicates. \n\nWe focus on why a non-English PLM might fail to recall facts and hypothesize the following possible reasons: \n\n1. The quality of the template might degrade after automatically translating it from English. 2. Non-English or multilingual PLM are generally pretrained on a lesser amount of non-English data and thus might be less capable of recalling facts efficiently. 3. Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias. \n\nWhile the first two factors are studied in the literature, we believe that the third factor is a major quality issue that previous work has overlooked. Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered. We investigate the presence of such bias ( \u00a73). Moreover, we empirically demonstrate how better model diagnostics can be performed when the benchmark is formed using two diverse and contrasting sets of facts ( \u00a75).",
            "score": 0.53695006812203,
            "section_title": "Introduction",
            "char_start_offset": 5592,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 521
                },
                {
                    "start": 524,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1240
                },
                {
                    "start": 1241,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1455
                }
            ],
            "ref_mentions": [
                {
                    "start": 38,
                    "end": 55,
                    "matchedPaperCorpusId": "235458643"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53173828125
        },
        {
            "corpus_id": "273502644",
            "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
            "text": "Drop in per-category performance from English to Multilingual benchmark. To understand the factors that affect the performance drop from English to Multilingual, we analyze the per-category performance difference of the top ten models. As shown in Table 3, we find that the Chat category, consisting of translated evaluation instances from AlpacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2024), suffers the most performance degradation for non-Generative RMs. All models show a decline in performance on our multilingual benchmark in the Chat-Hard category, with an average degradation of 5.96%. We observe the smallest decline in performance in the reasoning category, with an average decrease of 2.26%. \n\nLabel consistency across languages. Next, we examine the consistency of the models in labeling the same instances across different languages, using their English performance as the anchor for comparison. Figure 2 shows the average innermodel agreement, calculated by averaging the Cohen's \u03ba coefficient across 23 non-English languages, each paired with English. RMs with higher \u03ba consistently prefer the same response for the same examples across languages, indicating greater robustness to linguistic variations and more consistency in evaluating the content of the questions. We also observe that the highest-performing models (Table 2) are not always the most consistent ones. For instance, Gemma-2-9-B's average performance surpasses that of Llama-3-70B, yet the Llama-3-70B model demonstrates greater consistency in labeling across languages. Additionally, we find that inner-model agreement within each language varies from one example to the next. For instance, the distribution of Cohen's \u03ba for Indonesian in Figure 3 shows a high number of instances with negative to weak agreement. \n\nWhen looking at specific examples, we find that majority of disagreements occur in the Chat category (as also shown in Figure 3), which consists of general chat conversations and subsets from Al-pacaEval (Li et al., 2023b) and MT-Bench (Zheng et al., 2024). In addition, we also find that the Reasoning and Safety categories, which have objective and verifiable ground truth, tend to incur less disagreement across Generative RMs.",
            "score": 0.5367484093873954,
            "section_title": "Impact of RM type on",
            "char_start_offset": 9614,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2241
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 403,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 2047,
                    "end": 2067,
                    "matchedPaperCorpusId": "259129398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.25244140625
        },
        {
            "corpus_id": "269457293",
            "title": "Exploring Multi-Lingual Bias of Large Code Models in Code Generation",
            "text": "To initiate our investigation and answer the first research question, we construct the first multi-lingual evaluation benchmark X-HumnEval-X and study the multi-lingual bias from two aspects including multi-NL understanding bias and multi-PL generation bias, respectively.\n\nFor assessing multi-programming language (multi-PL) generation bias, we select three representative PLs including Python, Java, and C++ by utilizing the HumanEval-X benchmark [63].HumanEval-X serves as a variation of the original HumanEval benchmark [20], specifically adapted to include not just Python but also other PLs.This choice is motivated by the widespread use and popularity of these programming languages [10].Our goal is to explore and quantify the performance gaps when generating solutions across varied programming languages.\n\nFor the bias in multi-natural language understanding, we choose to employ ChatGPT [2] (GPT-3.5-turboversion) to translate the benchmark's original English instructions into Chinese due to its promising translation performance [30].This procedure is taken to construct a Chinese version of the benchmark, given that the initial set of instructions was exclusively in English.To ensure precision, the first two authors independently review every translated result and make corrections when necessary.Then the two authors discuss the corrections and reach a consensus, confirming the accuracy of the corrected version.This ensures that the instructions are correctly represented and can be effectively used for evaluating the performance of the models with Chinese instructions.",
            "score": 0.5364462988789844,
            "section_title": "B. Benchmark Construction",
            "char_start_offset": 8891,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 272
                },
                {
                    "start": 274,
                    "end": 454
                },
                {
                    "start": 454,
                    "end": 597
                },
                {
                    "start": 597,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 814
                },
                {
                    "start": 816,
                    "end": 916
                },
                {
                    "start": 916,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1314
                },
                {
                    "start": 1314,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1591
                }
            ],
            "ref_mentions": [
                {
                    "start": 449,
                    "end": 453,
                    "matchedPaperCorpusId": "257834177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3466796875
        },
        {
            "corpus_id": "270737791",
            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
            "text": "The findings of our initial evaluation suggest that the use of automatic translation of the questions in LLM benchmarks causes deviations in the results due to errors in the translation.To eliminate those deviations and ensure that the results are not contaminated by translation errors, the translations should be revised by experts and, ideally, the tests should be adapted to the language being evaluated.However, given the number of questions on the LLM benchmarks and the number of languages evaluated, this requires a large effort that calls for coordinated action from the community.\n\nIn fact, there are efforts in this direction such as the validation of the Spanish Okapi benchmarks as part of the #Somos600M Project Grandury (2024).This community annotation effort uses open-source frameworks and is performed by native Spanish-speaking volunteers.During the first two months, more than 60 persons participated and together covered one-third of the total number of dataset items, which shows how time-consuming it is to manually validate and correct these translations.\n\nIn the absence of manually adapted or checked benchmarks, our methodology enables a fast evaluation of the impact of translation errors with limited manual checking.Further refinements can be introduced to detect translation errors.For example, after translating into Spanish we could translate back into English the questions with different answers in both languages and run the questions again.When the answer to this English translation is different from the one of the original English question, the translation is likely to be the culprit of the error.In general, developing strategies to identify these issues automatically would be very helpful to understand the impact of automatic translation and also to correct the translation errors.\n\nIn this work, we have focused on Spanish which is one of the most widely used languages in the world and also is typically in the top five languages with more data on the LLM training datasets.It would be interesting to study the impact of translation errors on other languages.For languages with fewer data and speakers, we would expect a larger number of translation errors but also a larger number of genuine errors in the LLM answers.Therefore, the relative impact of translation errors on the benchmark results compared to Spanish can be either larger or smaller.",
            "score": 0.5350449449862458,
            "section_title": "Discussion",
            "char_start_offset": 13911,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 186,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 590
                },
                {
                    "start": 592,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 858
                },
                {
                    "start": 858,
                    "end": 1079
                },
                {
                    "start": 1081,
                    "end": 1246
                },
                {
                    "start": 1246,
                    "end": 1313
                },
                {
                    "start": 1313,
                    "end": 1477
                },
                {
                    "start": 1477,
                    "end": 1638
                },
                {
                    "start": 1638,
                    "end": 1826
                },
                {
                    "start": 1828,
                    "end": 2021
                },
                {
                    "start": 2021,
                    "end": 2106
                },
                {
                    "start": 2106,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2396
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.204345703125
        },
        {
            "corpus_id": "258841859",
            "title": "Revisiting Machine Translation for Cross-lingual Classification",
            "text": "This can be due to annotation artifacts in multilingual datasets, in particular those 6 We consider that a pretrained model A has learned higher quality representations than model B if fine-tuning it in our target task results in better downstream performance. In this case, English-only models like RoBERTa are generally superior to their multilingual counterparts like XLM-R when evaluated on downstream English tasks, so we say that their source-language representation quality is higher. \n\n7 Even if the model is trained and tested on the target language, we consider that there is still a degradation from the source-language representation quality under our framework. This is because we are measuring the degradation from using a multilingual model in the target language as opposed to a monolingual model in the source language, which is decomposed into the difference between the monolingual and multilingual model in the source language, plus the difference between the source and the target language in the multilingual model. created through translation (Artetxe et al., 2020), but can also be a result of the task having naturally different properties in different languages (e.g., question answering datasets in different languages might cover different topics that are relevant to their corresponding speaker communities). Quantifying this factor would require comparing translated and original annotations for each task, which we do not consider in our analysis given the lack of such data.",
            "score": 0.5335133123050213,
            "section_title": "Sources of cross-lingual transfer gap",
            "char_start_offset": 20118,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 261,
                    "end": 491
                },
                {
                    "start": 494,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1506
                }
            ],
            "ref_mentions": [
                {
                    "start": 1066,
                    "end": 1088,
                    "matchedPaperCorpusId": "215548041"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "276961588",
            "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
            "text": "Recent advancements in LLM evaluation have seen a strategic shift toward multilingual benchmarks to better assess cross-linguistic capabilities. Although foundational benchmarks such as GLUE [21] and SuperGLUE [20] focused on English, newer frameworks now emphasize linguistic diversity. The Multilingual Massive Multitask Language Understanding (MMMLU) benchmark [13], which evaluates models in 14 human-translated languages, including Swahili and Yoruba, exposes performance gaps in low-resource languages, even state-of-the-art models score 25% lower in Yoruba compared to English. \n\nSimilarly, BIG-bench [17] incorporates tasks spanning 1,000+ languages through its conlang translation and language identification challenges, though results reveal stark disparities between high-and low-resource language performance. Specialized bilingual evaluations such as HellaSwag-Pro [15] test the robustness through adversarially filtered Chinese-English common-sense reasoning variants, demonstrating that phrasing perturbations cause accuracy drops exceeding 15% in both languages. These multilingual extensions reveal critical limitations masked by English-centric benchmarks, particularly in semantic consistency across linguistic structures and resistance to prompt variations. MMLU-ProX translates MMLU-Pro into multiple languages, enabling direct cross-linguistic performance comparisons of language models. This approach uncovers language-specific performance disparities, offering insights into linguistic, cultural, or data-related factors affecting model performance.",
            "score": 0.5332618733692915,
            "section_title": "LLM Evaluation Benchmarks",
            "char_start_offset": 3774,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 1078
                },
                {
                    "start": 1079,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1409
                },
                {
                    "start": 1410,
                    "end": 1573
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23388671875
        },
        {
            "corpus_id": "275758182",
            "title": "Synthetic Data Can Mislead Evaluations: Membership Inference as Machine Text Detection",
            "text": "Our work suggests a fundamental flaw in evaluations that rely on synthetic data. Many evaluation protocols leverage machine-generated text, from machine translation for cross-lingual assessment [Wang and Hershcovich, 2023] to language models judging other models' outputs [Zhu et al., 2023] and other synthetic data training and evaluations [Gu\u00e9pin et al., 2023]. Our results suggest such evaluations may be systematically biased-the signals they measure may be confounded by the synthetic nature of their data rather than the properties they aim to assess. This raises three critical questions: (i) how does using language models to evaluate other models impact benchmark reliability, given their shared biases in processing synthetic text? (ii) are synthetic data-based evaluations measuring intended properties or merely detecting machine generation artifacts? (iii) why does synthetic text behavior transfer so consistently across different model scales and architectures? \n\nRecent work [Mireshghallah et al., 2023] suggests these patterns stem from fundamental similarities in how language models encode and process text.",
            "score": 0.5331697345499053,
            "section_title": "Introduction",
            "char_start_offset": 3865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 863
                },
                {
                    "start": 864,
                    "end": 976
                },
                {
                    "start": 979,
                    "end": 1126
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0792236328125
        },
        {
            "corpus_id": "276408372",
            "title": "TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages",
            "text": "Language understanding encompasses a system's ability to interpret and derive meaning from human language, incorporating syntax, semantics, and context. Evaluating language models hinges on this capability, as it ensures coherence, contextual relevance, and accuracy. Benchmarking is integral to assessing these models, particularly with the rapid advancements in Large Language Models (LLMs), which now support multiple languages (Yang et al., 2025;Gemma Team, 2024;Grattafiori et al., 2024) and excel in complex reasoning tasks such as mathematical, scientific, and coding-related inquiries (Hurst et al., 2024;Anthropic, 2024;Gemini Team, 2024;Grattafiori et al., 2024). However, the scarcity of robust natural language understanding (NLU) benchmarks capturing diverse linguistic and cultural contexts remains a challenge. Notably, LLM performance declines in low-resource languages, which are often underrepresented in training data, highlighting the need for more inclusive evaluation frameworks. \n\nThe majority of benchmarks included in top leaderboards where cutting-edge LLMs are evaluated are majorly prepared in English (Hendrycks et al., 2021a;Suzgun et al., 2022;Wang et al., 2024b, 2019). In order to extend the applicability of LLM evaluation in more languages, recent efforts were undertaken to build more multilingual NLU benchmarks (Lai et al., 2023), however, most of these either cover a limited set of highresourced languages, or the multilingual examples are generated by translating original examples from Western-centric languages, thus failing to capture cultural nuances inherent in different languages. Due to the multi-dimensional nature of the reasoning task, language-specific benchmarks especially when translated into other languages also fail to represent the actual usage as well as demonstrating reasoning in the native language., and may further introduce issues such as translationese ( Vanmassenhove et al., 2021) and cultural misalignment (Romanou et al., 2025).",
            "score": 0.5325957547461808,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1628
                },
                {
                    "start": 1629,
                    "end": 2000
                }
            ],
            "ref_mentions": [
                {
                    "start": 1130,
                    "end": 1155,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12445068359375
        },
        {
            "corpus_id": "273346089",
            "title": "A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification",
            "text": "Our study of language and translation biases in LLMs for cross-lingual claim verification has several limitations. We used the 2021 X-Fact dataset, which may not reflect the most recent language trends or advancements in model capabilities as of 2024. Additionally, the LLMs tested may have been trained on datasets overlapping with X-Fact, potentially inflating performance metrics. While we focused on 15 languages from diverse families, this selection might not fully represent the linguistic diversity needed to capture trends in low-resource languages. Our evaluation was limited to translations from non-English languages into English, and while examining other language pairs might provide valuable insights, it is unlikely that these pairings would outperform English due to the prevalent training bias toward English data in most LLMs. We used a lighter, older version of the COMETKIWI model to assess translation bias due to computational limitations, which may affect the robustness of our bias measurements. Moreover, we did not compare baseline models with instruction-tuned versions, which could have reduced inconclusive translations and offered further insights into model performance. We also did not incorporate reference translations or employ evidence retrieval, which could have provided a more holistic evaluation of translation quality. Future work should expand to include more recent datasets, evaluate other language pairs, and refine the methods to enhance bias detection and accuracy.",
            "score": 0.5323877665537058,
            "section_title": "Limitations",
            "char_start_offset": 13532,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1512
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20751953125
        },
        {
            "corpus_id": "266374637",
            "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed?",
            "text": "While using a powerful LLM to evaluate the outputs of other models has been shown to achieve reasonable agreement with human judgements in English (Zheng et al., 2023;Chiang and Lee, 2023), it is unclear whether this agreement transfers to all languages under investigation. Recent work by Hada et al. (2024) has shown that agreement between human and LLM judges tends to be lower for non-English languages, especially in the case of low-resource and non-Latin scripted languages, where the LLM judge tends to be overly optimistic in its assessment. However, for certain assessment criteria, such as linguistic acceptability and general content quality, they also confirm that inter-  (Ryan et al., 2023). # instances avail. denotes the total number of complex-simple sentence pairs available for each corpus in the MultiSim benchmark. # eval. instances denotes the number of items sampled from all test, validation, and train splits (where necessary) for language used to evaluate performance on sentence simplification. \n\nFigure 11: Distribution of helpfulness scores assigned by our LLM judge, GPT-3.5-Turbo, using direct evaluation in the specified target language and after translating target-language responses to English. \n\nannotator agreement between LLM-based evaluators and humans is in line with that of multiple human annotators. \n\nTo investigate this potential bias, we compared scores assigned by the LLM judge on modelgenerated responses directly in each non-English target language and on their English translations. For each non-English prompt-response pair, we translate it into English using GPT-3.5-Turbo (gpt-3.5-turbo-1106) and the prompt shown in Figure 9. We pair the resulting translated responses with their corresponding English AlpacaEval prompts and repeat our LLM judge evaluation. \n\nFigure 11 shows that the distribution of assigned scores in the direct and translated evaluation set-tings is very similar for most languages. For languages that use non-Latin scripts (e.g., Chinese, Russian, Korean, etc.), we observe that the LLM judge tends to assign slightly higher scores more frequently when evaluating directly on the non-English prompt/response pairs.",
            "score": 0.532195949865418,
            "section_title": "E Direct vs. Translated Evaluation for Non-English Dialogue Responses",
            "char_start_offset": 37938,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 835
                },
                {
                    "start": 836,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 1021
                },
                {
                    "start": 1024,
                    "end": 1228
                },
                {
                    "start": 1231,
                    "end": 1341
                },
                {
                    "start": 1344,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1811
                },
                {
                    "start": 1814,
                    "end": 1956
                },
                {
                    "start": 1957,
                    "end": 2189
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 188,
                    "matchedPaperCorpusId": "258461287"
                },
                {
                    "start": 290,
                    "end": 308,
                    "matchedPaperCorpusId": "261822638"
                },
                {
                    "start": 685,
                    "end": 704,
                    "matchedPaperCorpusId": "258887622"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1536865234375
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "We present MM-EVAL, a multilingual meta-evaluation benchmark with five core subsets covering 18 languages and a Language Consistency subset spanning 122 languages. By explicitly addressing language-specific challenges and avoiding direct translations from English, MM-EVAL ensures reliable preference labels and captures multilingual nuances. We also contribute a new meta-evaluation framework of Language Consistency, intended to measure the fairness and consistency of evaluator LLMs across languages. Correlation with Best-of-32 performance shows MM-EVAL has stronger alignment with real usage than English-only or machine-translated benchmarks, emphasizing MM-EVAL 's utility as a fair, robust platform for multilingual evaluator assessment. Our experiments on 12 evaluator LLMs reveal there's significant room for improvement, especially for low-resource language performances. We also observe unfairness and inconsistency of evaluator LLMs when assessing non-English languages. We caution against assuming English-centric evaluators natively generalize, and hope MM-EVAL promotes the development of robust multilingual evaluator LLMs. Table 3: An overview of the number of instances per language across the core subsets, with each language denoted by its ISO code. The languages are classified by their resource levels (R.L.)-High (H), Medium (M), or Low (L).",
            "score": 0.531499224241442,
            "section_title": "Conclusion",
            "char_start_offset": 27003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1365
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2161865234375
        },
        {
            "corpus_id": "270514431",
            "title": "Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning",
            "text": "This part of our study, centered on English, aims to delve into the nuanced cultural understanding of models, using the CAMeL dataset.We chose English for several reasons: the complexity of translating proper nouns, the redundancy of translated nouns representing the same concept, and previous findings indicating superior English performance unless using a language-specific pretrained model.\n\nPrior distributions of cultural aspects of countries affect cultural understanding at the granular level Our analysis reveals that the model displays a pronounced preference for certain cultures, particularly China and Iran, when no correct options are present (Setting 2, Figure 3).This inherent bias significantly affects performance for other cultures when correct options are included.As illustrated in Figure 2, LLaMA 2 generally exhibits the highest performance across various cultural aspects for China (Setting 1), with some exceptions where Iran leads.However, despite previous research (Weidinger et al., 2022;Durmus et al., 2023) indicating alignment with American values, the model shows a relatively superficial understanding of American culture, as evidenced by its lower performance.A possible explanation could be that the other options provided for the question have higher prior distributions, but there are possibly multiple factors at play here.\n\nIn another test (Setting 3), we present a scenario where three options are from the correct culture, along with one option from a randomly selected, incorrect culture.This setup is intended to evaluate the model's ability to discern cultural appropriateness accurately.Our findings reveal a stark contrast in performance based on the cultural context.For questions pertaining to China, the model demonstrates a high degree of accuracy, rarely selecting the incorrect cultural option.In contrast, when presented with questions about male names in the US, the model's performance significantly declines, choosing the incorrect option nearly 70% of the time.This disparity highlights the model's uneven capability in distinguishing between culturally relevant and irrelevant options across different cultural settings.An alternate plausible explanation could be that model has learned that the US is a largely multi-cultural society, which our evaluation approach isn't designed to consider.Such biases could be attributed to the mixed cultural perspectives inherent in the pretraining data, which might emphasize certain cultures over others.Appendix, Tables 8, 9, and 10 provide detailed results.",
            "score": 0.530992821483039,
            "section_title": "RQ3 : Do LLMs understand granular tangible cultural aspects? (somewhat)",
            "char_start_offset": 25654,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 134,
                    "end": 394
                },
                {
                    "start": 396,
                    "end": 679
                },
                {
                    "start": 679,
                    "end": 785
                },
                {
                    "start": 785,
                    "end": 957
                },
                {
                    "start": 957,
                    "end": 1194
                },
                {
                    "start": 1194,
                    "end": 1361
                },
                {
                    "start": 1363,
                    "end": 1530
                },
                {
                    "start": 1530,
                    "end": 1632
                },
                {
                    "start": 1632,
                    "end": 1714
                },
                {
                    "start": 1714,
                    "end": 1846
                },
                {
                    "start": 1846,
                    "end": 2018
                },
                {
                    "start": 2018,
                    "end": 2178
                },
                {
                    "start": 2178,
                    "end": 2351
                },
                {
                    "start": 2351,
                    "end": 2503
                },
                {
                    "start": 2503,
                    "end": 2558
                }
            ],
            "ref_mentions": [
                {
                    "start": 992,
                    "end": 1016,
                    "matchedPaperCorpusId": "249872629"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04486083984375
        },
        {
            "corpus_id": "270562911",
            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
            "text": "The focus of our study is to answer the research questions on the nature of instruction data and evaluation data as well as their impact on a trustworthy evaluation. We experiment with non-English training and test data created through distinct procedures: created natively and translated. We run monolingual instruction tuning: an LLM is finetuned in a single language every time to prevent potential cross-lingual influences. \n\nLanguages We study model performance in three languages-Spanish (es), Russian (ru), and Chinese (zh)-with the following considerations: 1) these languages cover a combination of different language families and writing scripts; 2) they are medium-to-high resourced, where the quality of the data, native or translated instructions, is satisfactory; 3) their presence in LLM pre-training data is significant, so we can expect reasonable output quality. \n\nNative data We use the training split in the Aya dataset (Singh et al., 2024), which was written from scratch and then edited by human annotators in their native language. The Spanish, Russian, and Chinese training sets have a size of 3854, 423, and 3944 each. \n\nTranslated data We generate translated data equivalent in volume to the native data. This is done by sampling Aya's English split to match the size of native data in each language and translating the sample to that language. We always translate the instructions and responses separately. Two distinct versions of translated data are obtained via Google Translate and Cohere Command R3 . Google Translate is a well-known commercial translation engine, whereas Command R, a large language model, is capable of adhering to more customised guidelines. Technically, we prompt Command R to maintain the original data formatting while translating, as illustrated Appendix A.1.",
            "score": 0.5302237356688795,
            "section_title": "Instruction data",
            "char_start_offset": 8124,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 427
                },
                {
                    "start": 430,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1054
                },
                {
                    "start": 1055,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1815
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.200439453125
        },
        {
            "corpus_id": "274192546",
            "title": "Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains",
            "text": "This behavior was particularly pronounced in open-ended tasks like recipe generation and ingredient listing in the UACUISINE benchmark, where models would be prompted in Ukrainian but switch to English, Chinese or Russian for response. This suggests that current VLMs experience the same code-switching issues that are known to happen in text-only multilingual LLMs (Kiulian et al., 2024). We have also observed the same issues of broken grammar \"\u041a\u0443\u0440\u044f\u0447\u0435 \u0441\u0443\u043f\u0443 \u0437 \u043b\u0430\u043f\u0448\u043e\u0439\", non-existing words generation (\u0425\u0430\u0448\u0438\u0440\u043e-i\u0442\u0430\u043c\u044d, \u041a\u0443\u0440\u0438\u0446\u0438\u043a\u0438, \u041a\u0443\u043bi\u0431\u0438\u043d\u043e)and tokenization artifacts (\u0420\u0438\u0441otto). Cultural misattribution. A key issue was cultural appropriation, notably when Ukrainian Borsch (UNESCO-recognized cultural heritage) was mislabeled as \"Russian Red Borscht.\" This pattern extended to other Ukrainian dishes, with models defaulting to English or Russian translations even when prompted in Ukrainian. The misattribution went beyond labeling -in recipe generation, models often suggested Russian rather than traditional Ukrainian preparations. This systematic bias points to training data issues that risk reinforcing narratives diminishing Ukrainian cultural identity. Addressing this requires both improved Ukrainian language capabilities and better integration of accurate cultural knowledge in model training. Fine-tuning observations. The fine-tuned Pali-Gemma model demonstrated improvement in from 3% to 34% and intersection match improving from 0% to 15%. This improvement suggests that even limited domain-specific fine-tuning can substantially enhance performance on culturallyfocused tasks. Notably, this specialization also yields improved performance on the translated Ukrainian VQA benchmark (+5.33%), while maintaining comparable performance on the English VQA benchmark (slight -0.52% difference). These results suggest that targeted fine-tuning on Ukrainian cultural content can enhance model performance on Ukrainian language tasks without compromising general capabilities in English. We noticed a clear limitation of the VLMs to perform noncaption tasks that require more than general world knowledge, especially things like providing cuisine recipes, which manifests into low BERT score.",
            "score": 0.528522240342297,
            "section_title": "Results & Discussion",
            "char_start_offset": 9922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1585
                },
                {
                    "start": 1586,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.282958984375
        },
        {
            "corpus_id": "274822728",
            "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation",
            "text": "Here too, we consider only generations that are entirely in the prompting language. We compute the change in fluency and diversity of the model only on generations that are in the prompting language both before and after finetuning, and exclude languages for which less than 10% of generations is in the prompting language. \n\nWe first evaluate all models in Section 4.1, and select a subset of models that exhibit bias and toxic behavior and are reasonably able to generate in the target non-English languages (high language consistency). This subset of models is then finetuned in English with the goal of debiasing and detoxification. Next, in Section 4.2 we discuss the cross-lingual effects of this finetuning on the model's bias and toxicity levels, and on their language generation abilities in the non-English evaluation languages. \n\nWe report the results of our initial evaluation of instruction-tuned models in Table 1 and the results for the base models in Table 6 in Appendix H. Even though we focus our evaluation on the non-English languages included in the benchmarks, we separately report the results for English in Appendix J.  2024), models struggle to admit that there is not enough information in the context to answer the question, and instead give biased answers. We observe that the bias scores from different benchmarks do not correlate very well, which is consistent with findings by Prakash and Lee (2023) for CrowS-Pairs and Stere-oSet specifically, and Delobelle et al. (2022) and Zayed et al. (2024) who observe this for promptbased fairness metrics in general.",
            "score": 0.5266358111308355,
            "section_title": "Evaluation",
            "char_start_offset": 18143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 323
                },
                {
                    "start": 326,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 838
                },
                {
                    "start": 841,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1589
                }
            ],
            "ref_mentions": [
                {
                    "start": 1408,
                    "end": 1430,
                    "matchedPaperCorpusId": "266054040"
                },
                {
                    "start": 1480,
                    "end": 1503,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 1508,
                    "end": 1527,
                    "matchedPaperCorpusId": "270371962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.138916015625
        },
        {
            "corpus_id": "270258352",
            "title": "IrokoBench: A New Benchmark for African Languages in the Age of Large Language Models",
            "text": "Furthermore, the few comprehensive evaluations that exist across languages often rely on machine translation of English benchmarks (Singh et al., 2024). While automatic translation from English benchmarks is a popular approach given the cost and time investment required for human translation, it often suffers from noise and biases (Vanmassenhove et al., 2021;Lee et al., 2022;Khiu et al., 2024;Hartung et al., 2023;Savoldi et al., 2021) or fail to reflect cultural context (Wang et al., 2022;Ji et al., 2023;Pudjiati et al., 2022). Automatic curation may also amplify any of the ubiquitous issues with the quality of broad pretraining sets (Luccioni and Viviano, 2021;Kreutzer et al., 2022;Ferrara, 2023). \n\nIn this paper, we seek to address both the diversity and breadth of evaluation coverage. We introduce IROKOBENCH, a human curated benchmark dataset for 17 typologically diverse African languages which encompasses three complex tasks: natural language inference (NLI), mathematical reasoning, and multi-choice knowledgebased QA. The datasets were created by human translating a subset of English cross-lingual NLI (XNLI) (Conneau et al., 2018), English Multilingual Grade School Math (MGSM) (Shi et al., 2023), and Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021a), evaluation datasets into each of the 16 languages using professional translators. \n\nWe conduct a large-scale evaluation of IROKOBENCH to assess zero-shot, few-shot, and translate-test settings (where test sets are translated into English) performance across 10 open and six proprietary LLMs. Our main contributions can be enumerated as follows: \n\n1. We introduce and release IROKOBENCH, a human-translated benchmark that includes 16 languages from various geographical regions in Africa, all with varying degrees of \"lowresourcedness\" (Joshi et al., 2020).",
            "score": 0.5231417773359492,
            "section_title": "Introduction",
            "char_start_offset": 1562,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 152
                },
                {
                    "start": 153,
                    "end": 533
                },
                {
                    "start": 534,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1380
                },
                {
                    "start": 1383,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1643
                },
                {
                    "start": 1646,
                    "end": 1855
                }
            ],
            "ref_mentions": [
                {
                    "start": 378,
                    "end": 396,
                    "matchedPaperCorpusId": "267411831"
                },
                {
                    "start": 475,
                    "end": 494,
                    "matchedPaperCorpusId": "232307434"
                },
                {
                    "start": 670,
                    "end": 692,
                    "matchedPaperCorpusId": "232307434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.350341796875
        },
        {
            "corpus_id": "269408175",
            "title": "A multimodal approach to cross-lingual sentiment analysis with ensemble of transformer and LLM",
            "text": "An inherent limitation in translating foreign language text for sentiment analysis revolves around the potential introduction of biases or errors stemming from the translation process 44  www.nature.com/scientificreports/ to capture the intricacies and subtleties of the source language. Similarly, human translators generally exhibit greater accuracy but are not immune to introducing biases or misunderstandings during translation. \n\nTo minimize the risks of translation-induced biases or errors, meticulous translation quality evaluation becomes imperative in sentiment analysis. This evaluation entails employing multiple translation tools or engaging multiple human translators to cross-reference translations, thereby facilitating the identification of potential inconsistencies or discrepancies. Additionally, techniques such as back-translation can be employed, whereby the translated text is retranslated back into the original language and compared to the initial text to discern any disparities. By undertaking rigorous quality assessment measures, the potential biases or errors introduced during the translation process can be effectively mitigated, enhancing the reliability and accuracy of sentiment analysis outcomes.",
            "score": 0.5222792942395073,
            "section_title": "Challenge IV: translation biases",
            "char_start_offset": 55803,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14013671875
        },
        {
            "corpus_id": "270688756",
            "title": "How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions",
            "text": "Lack of diversity in training data is a well-known problem for LLMs, resulting in general values becoming improperly embedded in transformerdriven models, which eventually leads to misrepresentation of the input text and offensive advice being generated (Johnson et al., 2022).Cultural assumptions are also baked into AI systems throughout their development, conflicting with cultural norms and expectations which result in cultural misinterpretations and misrepresentations (Prabhakaran et al., 2022).Furthermore, there exists a clear bias towards performance across many different LLMs in English compared to other languages, with large models being prone to respond to non-English harmful instructions; multilingualism induces cross-lingual concept inconsistency, and unidirectional cross-lingual concept transfer between English and other languages (Xu et al., 2024).\n\nGPT responses across different languages also showcase behavior that suggests subordinate multilingualism, with many responses similar to that of a system that translates input in to English, formulates a response, then translates the response back into an input language, resulting in a much lower accuracy.GPT has predominantly monolin-gual English training data, so it has developed a representation of knowledge and communication that is strongly biased towards English, leaving it unable to create a unified multilingual conceptual representation (Zhang et al., 2023).General LLM responses also tend to be more inconsistent when taking on different personas based on that person's representation throughout the data (Geng et al., 2024).\n\nSome work has been done to understand whether there are discrepancies within LLMs' interpretations of other cultures, including prior work by (Masoud et al., 2024) demonstrating how LLMs change their responses to cultural questions and advocating for more culturally diverse AI development.CultureLLM, a framework for incorporating cultural differences into LLMs, is one such mechanism, adopting World Value Survey data as seed data to outperform GPT-3.5'scultural understanding (Li et al., 2024).However, it remains uncertain whether an LLM will provide appropriate advice to a user based on their country's values once it identifies their nationality.",
            "score": 0.5214688871238968,
            "section_title": "Related Works",
            "char_start_offset": 4227,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 277,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 871
                },
                {
                    "start": 873,
                    "end": 1181
                },
                {
                    "start": 1181,
                    "end": 1446
                },
                {
                    "start": 1446,
                    "end": 1614
                },
                {
                    "start": 1616,
                    "end": 1906
                },
                {
                    "start": 1906,
                    "end": 2072
                },
                {
                    "start": 2072,
                    "end": 2113
                },
                {
                    "start": 2113,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0703125
        },
        {
            "corpus_id": "16794216",
            "title": "Is Machine Translation Getting Better over Time?",
            "text": "It does seem that some language pairs are harder to translate than others, and the differential change may be a consequence of the fact that increasing test set complexity for all languages in parallel has a greater impact on translation difficulty for language pairs that are intrinsically harder to translate between. Table 5 shows results for translation out-of English, and once again human evaluation scores are in agreement with automatic metrics with Englishto-Spanish translation achieving most substantial gains for the three out-of-English language pairs, an increase of 12.4 points for fluency, and 11.8 points with respect to adequacy, while Englishto-French translation achieves a gain of 8.8 for Table 5: Human evaluation of WMT 2007 and 2012 best systems for out of English language pairs. Mean scores are computed in each case for n translations. fluency and 7.4 points for adequacy. English-to-German translation achieves the lowest gain of all languages, with apparently no improvement in fluency, as the human fluency evaluation of the benchmark system on the supposedly easier 2007 data receives a substantially lower score than the same system over the 2012 data. This result demonstrates why fluency, evaluated without a reference translation, should not be used to evaluate MT systems without an adequacy assessment, since it is entirely possible for a low-adequacy translation to achieve a high fluency score. For all language pairs, Figure 1 plots the net gain in fluency, adequacy and F 1 against increase in test data difficulty.",
            "score": 0.5201166989993352,
            "section_title": "Language pairs",
            "char_start_offset": 24394,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1556
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0943603515625
        },
        {
            "corpus_id": "276557950",
            "title": "Do Multilingual LLMs Think In English?",
            "text": "This English-centric behavior of LLMs cause them to perform worse in other languages, whether in downstream tasks (Shafayat et al., 2024;Huang et al., 2023;Bang et al., 2023;Shi et al., 2022), or in fluency (Guo et al., 2024). Moreover, this impacts the fairness of these models -which currently exhibit cultural biases (Shafayat et al., 2024) -and their robustness and reliability in diverse linguistic settings (Marchisio et al., 2024;Deng et al., 2024).",
            "score": 0.5198660986297436,
            "section_title": "Introduction",
            "char_start_offset": 3924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 456
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "277780370",
            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
            "text": "1) Provide a better means to evaluate multilinguality in LLMs; \n\n2) Provide data to study the effect of various design choices in multilingual evaluation. To address our first goal, we create 500 questions per language, written from scratch for each language, using a sourcing protocol specifically designed to ensure local relevance of the question topics. To also reap the benefits of parallel data, we commissioned both human and machine-authored translations for all non-English questions into English and vice versa, providing a total of 15500 parallel questions, sourced across the 31 languages in the benchmark. The translated data facilitates the study of transfer between languages and also serves our second goal. By comparing the English-translated data with the locally sourced data, we can explicitly compare the adequacy of using translated benchmarks; by comparing human-with machine-authored translations, we can better estimate the potential issues of the latter. To prevent quick overfitting and inadvertent contamination, we release a development set of the benchmark, while test scores can only be obtained through an external provider. 3 e provide elaborate analyses for both our goals. We compute average performance and language parity scores on the locally sourced data for 11 models marketed for their multilinguality ( \u00a7 5.1); we investigate whether these models exhibit knowledge transfer between different languages ( \u00a7 5.2); we study the impact of local sourcing versus translating on model rankings and language difficulty ( \u00a7 5.4.1); we analyse the difficulty of the included languages through various lenses ( \u00a7 5.3); and we conduct an analysis into the difference between human-and machine-authored translation ( \u00a7 5.4.3). We find that, of the models we consider, the best performing model is Gemini 2.0 Flash, with an average performance of 34.4 points, and an almost 35 point gap between the best and the worst language. Llama 3.1 405B and GPT4-o are close contenders in terms of average scores (34.3 and 34.0, respectively), but both have substantially higher language gaps (39 and 49 points).",
            "score": 0.5197879804288781,
            "section_title": "Introduction",
            "char_start_offset": 2103,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 65,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 357
                },
                {
                    "start": 358,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2129
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19677734375
        },
        {
            "corpus_id": "276580130",
            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
            "text": "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems.",
            "score": 0.5189017276831458,
            "section_title": "Factor",
            "char_start_offset": 29226,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 590
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.389892578125
        },
        {
            "corpus_id": "273098796",
            "title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English",
            "text": "Cultural nuances and idiomatic expressions in non-English languages are often misunderstood by LLMs, leading to misinterpretations [12,5] . The English-centric bias also impacts translation quality and content generation, requiring specialized adaptations to improve accuracy and cultural appropriateness [13]. \n\nAddressing this issue is crucial for several reasons. First, language is a fundamental aspect of human communication and culture. If AI systems are to be truly useful and accessible globally, they must be able to operate effectively across multiple languages [14,15]. Second, the concentration on English risks perpetuating and exacerbating existing language-based inequalities in access to technology and information. Lastly, improving multilingual capabilities could unlock new applications and insights currently hindered by language barriers. Incorporating diverse cultural perspectives in AI systems offers numerous benefits, including better alignment with specific cultural values [16,17,18], enhanced user experience in diverse regions [19], improved ethical considerations [20], mitigation of inequalities [21], and increased adaptability and inclusivity [22] while avoiding cultural tensions and biases [16]. \n\nHowever, achieving true cultural sensitivity in LLMs faces substantial obstacles. One is culture's contextual and situational nature, making it challenging to capture in static datasets or evaluate through standardized prompts [23]. Further addressing cultural sensitivity requires insights from fields like anthropology and sociology, which are often not fully incorporated into LLM development [24,25,26]. \n\nOthers include limited cultural proxies by focusing predominantly on regional and linguistic proxies, potentially resulting in incomplete or skewed portrayals of global cultural diversity [27,28], insufficient multilingual datasets [29,16] and a focus on English-centric LLMs [30,31], scaling challenges [32], and lack of robust evaluation methods [33,34,5]. Researchers employ both black-box and white-box approaches to study cultural sensitivity in LLMs [35,36,16,32]. Black-box probing methods analyze responses to inputs without accessing internal states. This involves appending cultural context to queries and comparing responses under different conditions.",
            "score": 0.5187217816493097,
            "section_title": "arXiv:2410.01811v1 [cs.CY] 14 Sep 2024",
            "char_start_offset": 2011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 310
                },
                {
                    "start": 313,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 859
                },
                {
                    "start": 860,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1315
                },
                {
                    "start": 1316,
                    "end": 1466
                },
                {
                    "start": 1467,
                    "end": 1641
                },
                {
                    "start": 1644,
                    "end": 2002
                },
                {
                    "start": 2003,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2307
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 137,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 305,
                    "end": 309,
                    "matchedPaperCorpusId": "261122409"
                },
                {
                    "start": 572,
                    "end": 576,
                    "matchedPaperCorpusId": "270211069"
                },
                {
                    "start": 1001,
                    "end": 1005,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1177,
                    "end": 1181,
                    "matchedPaperCorpusId": "231829607"
                },
                {
                    "start": 1226,
                    "end": 1230,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "1083991"
                },
                {
                    "start": 1836,
                    "end": 1839,
                    "matchedPaperCorpusId": "248749343"
                },
                {
                    "start": 1876,
                    "end": 1880,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1880,
                    "end": 1883,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1920,
                    "end": 1924,
                    "matchedPaperCorpusId": "260744613"
                },
                {
                    "start": 1924,
                    "end": 1927,
                    "matchedPaperCorpusId": "232307434"
                },
                {
                    "start": 1999,
                    "end": 2001,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 2107,
                    "end": 2110,
                    "matchedPaperCorpusId": "262580630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.220703125
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "While the contamination is injected in non-English languages, we evaluate these contaminated models on the original English benchmarks to assess their potential impact on misleading the leaderboard. \n\nWe report zero-shot accuracy for three types of models: (1) Clean: The original backbones with no added contamination. (2) Vanilla Contaminated: Backbones contaminated by the original English benchmarks. (3) Cross-Lingual Contaminated: Backbones contaminated by non-English translated benchmarks. The evaluation is implemented through LM-Eval framework (Gao et al., 2023) and the results are exhibited in Table 1. \n\nFor models with vanilla contamination, their accuracy is close to 100%. This is expected since the models are directly overfitted on these test sets. In the cross-lingual contamination scenario, models are not directly trained on the benchmarks. Surpris-ingly, the cross-lingual contamination can sneak beyond language barriers and carry over to English. \n\nRegarding models with cross-lingual contamination, their performance, while not reaching 100%, exhibits significant inflation, even though the translation provided by LLaMA3-8B is imperfect. We observe a consistent 5%-10% improvement on the MMLU benchmark across languages, with an even stronger enhancement seen on the MathQA benchmark. The instability of the performance gains shown on ARC-C can be caused by the low-quality translation of the dataset. In addition, we hypothesize that models can more easily memorize factual knowledge (MMLU) and Arabic numbers' operations (MathQA) than reasoning in languages (ARC-C), which is intuitive. One may understand the intricacies of arithmetic or fact retention through repetitive exposure and practice, but reasoning in natural languages, as required in ARC-C tasks, involves a more complex interplay of context, inference, and flexible application of knowledge. \n\nAnother interesting finding is the effect of crosslingual contamination's language category on the contamination effect. We observe that European languages (French, German, Italian, and Spanish) can provide stronger cross-lingual contamination onto English, while Asian languages (Chinese, Japanese, and Korean) provide a lesser effect.",
            "score": 0.5177836498037485,
            "section_title": "Evaluating Contaminated Models",
            "char_start_offset": 10783,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 688
                },
                {
                    "start": 689,
                    "end": 766
                },
                {
                    "start": 767,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 971
                },
                {
                    "start": 974,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1884
                },
                {
                    "start": 1887,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "274788786",
            "title": "Breaking the Programming Language Barrier: Multilingual Prompting to Empower Non-Native English Learners",
            "text": "Firstly, although state-of-the-art LLMs have been shown to have multilingual capabilities, they are most capable in English due to the simple fact that the vast majority of the data they have been trained on is in English [41]. In fact, although information on how proprietary models work internally is difficult to find, it appears that most generative AI models will first translate a non-English prompt into English before attempting to answer it [60]. As a result of this English-centric bias, it is not surprising that models may be less effective at determining the intent of non-English prompts [16,67]. Our results align with prior research that shows LLMs perform better with high-resource languages where more training data is available. Specifically, Arabic language data makes up less than 1% of the language distribution available in Common Crawl 2  (an open repository of web crawl data that makes up a significant portion of the training data for many LLMs), whereas Chinese is one of the most common languages in the corpus (although still much less frequent than English). This model bias may help to explain the particular difficulty faced by the Arabic speakers in our study. \n\nSecondly, the output generated by the model is in a programming language that relies heavily on English syntax, which reduces the number of transformative steps needed when the input prompt is also in English. This alignment between the input (prompt) and output (program) language may also partially explain why students found writing prompts in English more effective. Related to this point, programming languages inherently encode concepts naturally expressed in English, for example, constructs like conditional branching using keywords such as if and while. These programming language keywords carry cultural and linguistic associations that may not translate directly into other languages [4]. \n\nFinally, the widespread use of English in programming has led students to become accustomed to writing code in English, even when it is not their native language. While students reported that it was easier to describe their thoughts in their native languages, they still found themselves relying on English technical terms, making the shift back to their native language feel cumbersome and unnatural. Thus, rather than simplifying the process, LLMs may be introducing an additional burden: students now have to translate their thoughts and intentions, as well as the syntax and technical terms.",
            "score": 0.5166332827283668,
            "section_title": "Challenges of Non-English Prompting",
            "char_start_offset": 35415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1194
                },
                {
                    "start": 1197,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1896
                },
                {
                    "start": 1899,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2300
                },
                {
                    "start": 2301,
                    "end": 2494
                }
            ],
            "ref_mentions": [
                {
                    "start": 1892,
                    "end": 1895,
                    "matchedPaperCorpusId": "232221466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0638427734375
        },
        {
            "corpus_id": "259061858",
            "title": "Automatic Essay Scoring Method Based on Multi-Scale Features",
            "text": "It can be susceptible to language and cultural biases. The difference in language usage, dialects, or cultural references can impact the accuracy of AES, particularly for non-native English speakers or individuals from diverse linguistic backgrounds. In addition, scoring varies greatly between different languages and the model cannot be transferred well between different languages.",
            "score": 0.5165515519432791,
            "section_title": "\u2022",
            "char_start_offset": 42015,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 54
                },
                {
                    "start": 55,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.199462890625
        },
        {
            "corpus_id": "267413041",
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "text": "Multilingual benchmarks, such as XNLI, XTREME, play a vital role in assessing the cross-lingual generalization of multilingual pretrained models (Conneau et al., 2018;Hu et al., 2020). Typically, these benchmarks involve translating development and test sets from English into different target languages using professional human translators. However, such a translation process is susceptible to human errors and could lead to incorrect estimates of cross-lingual transfer to target languages. We find translation errors do emerge and they disproportionately affect translations in certain low-resource languages such as Hindi and Urdu. 2 onsider the well-known Cross-Lingual Natural Language Inference (XNLI) benchmark (Conneau et al., 2018) that contains human translations of English premise-hypothesis pairs (with the labels reproduced from English) into 14 typologicallydiverse target languages. Prior work raised concerns about whether the semantic relationships between premise and hypothesis are preserved in such human translations, but did not probe into this issue further (Artetxe et al., 2020a(Artetxe et al., , 2023)). We find that there are indeed errors introduced in the human translations leading to label inconsistencies and that this issue disproportionately affects lowresource languages. \n\nTo visualize the impact of low-quality translations on low-resource languages, Figure 1 compares zero-shot XNLI performance on all 14 target languages using the XLMR model (Conneau et al., 2020) finetuned on English NLI with the following two input types: 1. Human translations of the original English NLI instances to the target language from XNLI, translated back to English. 2. Machine translations of the original English NLI instances to the target language, translated back to English. We see a clear differential trend with larger gaps between the (scores over the) two input types for low-resource languages such as Swahili, Urdu and Turkish (appearing on the right) and smaller gaps for high-resource languages such as Spanish, German and French (appearing on the left).",
            "score": 0.5162203803409647,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 638
                },
                {
                    "start": 639,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 2091
                }
            ],
            "ref_mentions": [
                {
                    "start": 1084,
                    "end": 1106,
                    "matchedPaperCorpusId": "215548041"
                },
                {
                    "start": 1484,
                    "end": 1506,
                    "matchedPaperCorpusId": "207880568"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5244140625
        },
        {
            "corpus_id": "259375636",
            "title": "Scaling Laws Do Not Scale",
            "text": "In addition, given our argument that different communities represented within a dataset (or impacted by a particular system) may have fundamentally different values and metrics, what might it look like to evaluate scaling laws where the vertical axis, instead of decontextualized model performance metrics like accuracy (or F1 score, RMSE, ROUGE, etc), were instead chosen for particular use cases or system deployment contexts, or were chosen by particular impacted communities in participatory ways (Wagner et al. 2021;Delgado et al. 2023;Suresh et al. 2022;Dennler et al. 2023) to better reflect their values. \n\nReturning to our introductory example, although mitigations for the curse of multilinguality have been developed (Pfeiffer et al. 2022;Blevins et al. 2024), they require a welldefined set of languages and metrics for each, both of which are missing for the multiplicity of sub-groups we suggest. \n\nMoreover, substantial work on model evaluations has shown that aggregate metrics of model performance may hide worse performance for particular sub-groups that can be observed when model performance is dis-aggregated by some demographic categories (Buolamwini and Gebru 2018;Koenecke et al. 2020;Obermeyer et al. 2019;Ngan et al. 2020;Barocas et al. 2021). Analogously, the current paradigm of evaluating scaling laws on aggregations of performance metrics evaluated on a single training dataset is likely to hide similar divergences in values and preferences for metrics for sub-groups within an evaluation dataset. For instance, a recent paper proposed a benchmark for evaluating bias in QA, and evaluated it on several large language models; however, they caveat at the end of the paper that \"the data in BBQ is only designed to test biases associated with US English-speaking cultural contexts, and it should not be used as evidence that a model would still look unbiased for contexts from a different culture\" (Parrish et al. 2021).",
            "score": 0.5161190619654257,
            "section_title": "Discussion",
            "char_start_offset": 50919,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 612
                },
                {
                    "start": 615,
                    "end": 910
                },
                {
                    "start": 913,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1950
                }
            ],
            "ref_mentions": [
                {
                    "start": 501,
                    "end": 521,
                    "matchedPaperCorpusId": "235698405"
                },
                {
                    "start": 521,
                    "end": 541,
                    "matchedPaperCorpusId": "263605822"
                },
                {
                    "start": 541,
                    "end": 560,
                    "matchedPaperCorpusId": "249872586"
                },
                {
                    "start": 728,
                    "end": 750,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 750,
                    "end": 770,
                    "matchedPaperCorpusId": "269626627"
                },
                {
                    "start": 1161,
                    "end": 1188,
                    "matchedPaperCorpusId": "3298854"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.080078125
        },
        {
            "corpus_id": "276107976",
            "title": "Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study",
            "text": "Our results caution against over reliance on machine translation in cross-language MF applications and raise broader methodology concerns for its reliability in measuring other cross-language human values. \n\nResearchers should also be cautious with local language lexicons-they perform worse than the machine translation for cross-language MF measurement in some cases. Given the inherent limitations of lexicon-based methods and the extensive resources required to develop them, they are inefficient for cross-language MF measurement, particularly at the document level where semantic complexity is higher. However, culturally specific moral lexicons may still pro-  Our evaluation with multilingual language models and LLMs also reveals several key implications. Above all, fine-tuning LLMs is more data-efficient than fine-tuning multilingual language models for cross-language MF measurement. In the CCV benchmarking dataset, XLM-T requires on average more than 2,000 local-language annotated records (20 batches) to achieve strong performance (i.e., F 1 = 0.75). In contrast, Llama3.1-8b can reach comparable performance with only English data machine-translated to Chinese and thus no local language labeling. \n\nIn addition, data augmentation shows strong potential in enhancing LLMs' performance on culturally distinct MF values. For example, Llama3.1-8b notably improves on \"loyalty\" when fine-tuned with English data machine-translated to Chinese (F 1 = 0.69), compared to its performance with the original English training data (F 1 = 0.57). Such data augmentation may serve as a practical and cost-effective way to improve model performance in cross-language measurements, especially in cases where mass-labeling of local-language data is infeasible due to the resource-intensive process of creating high-quality language-specific labeled datasets. \n\nOur findings also suggest that larger or more advanced LLMs such as Llama3.1-70b may perform better for measuring MFs in non-English corpora. That is, updating the base LLM to a more power model with enhanced multilingual capabilities could further improve the effectiveness of the LLM approach for cross-language MF measurement.",
            "score": 0.5150413668873077,
            "section_title": "Discussion and Limitations",
            "char_start_offset": 33045,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 208,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1215
                },
                {
                    "start": 1218,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1859
                },
                {
                    "start": 1862,
                    "end": 1942
                },
                {
                    "start": 1943,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2919921875
        },
        {
            "corpus_id": "274192546",
            "title": "Benchmarking Multimodal Models for Ukrainian Language Understanding Across Academic and Cultural Domains",
            "text": "Several ethical considerations arise in developing and deploying multimodal benchmarks for Ukrainian language evaluation. Most critically, our work addresses questions of cultural representation and identity preservation, particularly salient given current geopolitical contexts. The systematic misattribution of Ukrainian cultural elements by AI models highlights risks of technological erasure of cultural identity. \n\nWhile our use of translated benchmarks enables comparative evaluation, this approach may inadvertently perpetuate biases and fail to capture uniquely Ukrainian contexts. Additionally, the observed tendency of models to default to Russian or English translations, even when prompted in Ukrainian, raises concerns about digital marginalization of Ukrainian language users. These considerations underscore the importance of developing culturally sensitive evaluation frameworks that can help ensure AI systems properly represent and serve Ukrainian language users.",
            "score": 0.5149602244146552,
            "section_title": "Ethical Considerations",
            "char_start_offset": 14100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 417
                },
                {
                    "start": 420,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 981
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.266845703125
        },
        {
            "corpus_id": "258947405",
            "title": "Don't Trust ChatGPT when your Question is not in English: A Study of Multilingual Abilities and Types of LLMs",
            "text": "The results of our experiments provide evidence that GPT exhibits a substantial degree of subordinate multilingualism. Many of its responses are what we would expect from a system which translates all input into English, formulates a response in English, and then translates this response into the input language. Since translation is an errorprone process, the resulting response accuracy is frequently lower than when the input is provided in English. \n\nWe speculate that this behavior is an artifact of GPT being trained mostly on monolingual English texts. Consequently, GPT has developed a representation of knowledge and communication that is strongly biased towards English. We conclude that since GPT is not designed to take advantage of bilingual or multilingual corpora, it is unable to create a single multilingual conceptual representation analogous to compound multilingualism. \n\nMoreover, GPT has less training data for non-English languages, compared to its English training data. We postulate that this results in represen-tations for non-English languages that are much weaker than those GPT can create for English. This often leads to lower performance on even translation-equivariant tasks when the task is not presented in English.",
            "score": 0.5148428101046455,
            "section_title": "Analysis of Results",
            "char_start_offset": 25312,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 453
                },
                {
                    "start": 456,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1251
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0207977294921875
        },
        {
            "corpus_id": "247951931",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "text": "A major limitation of the fairness analyses presented in this section is that they are performed only on English language data, while PaLM is trained on multilingual data and evaluated on multilingual language processing tasks. Given that language technologies utilizing large language models are increasingly used in geo-cultural contexts across the globe, it is important that bias benchmarks be developed and utilized for other languages and socio-cultural contexts. Additionally, as Sambasivan et al. (2021) point out, fairness evaluations and benchmarks developed in and for the Western world may not be readily portable to other geo-cultural contexts where societal disparities may manifest along an entirely different set of axes. We thus note that potential biases exist beyond what we are currently capable of measuring.\n\nFurther, it is important to note that despite a growing body of work investigating biases in English language technologies (Dev et al., 2021b), there is a lack of standardization of fairness benchmarks, an understanding of what harms different bias measures in NLP relate to (Blodgett et al., 2020(Blodgett et al., , 2021Jacobs & Wallach, 2021), and coverage of identities in fluid, comprehensive ways (Cao & Daum\u00e9 III, 2020;Dev et al., 2021a). As such, our fairness evaluations in this section are also limited by the same concerns and there are potential risks beyond what can be measured. We expand upon previous efforts to evaluate unintended biases and our evaluations are limited to popular tasks such as pronoun resolution (Winogender) (Rudinger et al., 2018) and co-occurrence analysis. Such benchmarks may be proxies for the types of biases (and accompanying risks of harm) in tasks such as translation, code generation, commonsense reasoning, open-ended dialog, arithmetic reasoning and question answering.\n\nAdditionally, bias can pervade a system depending on the specific downstream application, its specific training pipeline, and application-level protections (e.g., safety filters). While we evaluate the pre-trained model here for fairness and toxicity along certain axes, it is possible that these biases can have varied downstream impacts depending on how the model is used. It is also unclear if evaluations done on the pre-trained language models affect the",
            "score": 0.5144393065512032,
            "section_title": "Limitations",
            "char_start_offset": 121412,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 487,
                    "end": 511,
                    "matchedPaperCorpusId": "231698728"
                },
                {
                    "start": 1106,
                    "end": 1128,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1128,
                    "end": 1152,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 1152,
                    "end": 1175,
                    "matchedPaperCorpusId": "209202216"
                },
                {
                    "start": 1233,
                    "end": 1256,
                    "matchedPaperCorpusId": "204961553"
                },
                {
                    "start": 1256,
                    "end": 1274,
                    "matchedPaperCorpusId": "237347097"
                },
                {
                    "start": 1574,
                    "end": 1597,
                    "matchedPaperCorpusId": "13756572"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1287841796875
        },
        {
            "corpus_id": "270738125",
            "title": "\"Vorbe\\c{s}ti Rom\\^ane\\c{s}te?\"A Recipe to Train Powerful Romanian LLMs with English Instructions",
            "text": "Results on RoCulturaBench are presented in Table 7. Taking an English foundational model and adapting it to a new language by performing instruction fine-tuning is not enough to properly un- . \n\nderstand the nuances and realities of a culture. Testament stands the competitive performance of RoL-lama2: starting from an arguably weaker model but also pre-training it performs on par with newer and stronger models that have only been fine-tuned. We argue that, as expected, cultural information and social knowledge are primarily injected into the model in the pre-training phase, not in the finetuning phase, especially when fine-tuning exploits non-natively data (i.e., written in English and then translated). As in the case of MT-Bench, Llama3 outperforms other models by a noteworthy mar-gin, with only GPT-3.5 being stronger. For more detailed results and comparisons, see Appendix Section A.2. In addition, results obtained using the LLM-as-a-judge approach are not the be-all and end-all of evaluations, especially regarding non-English language and culture. We leave human evaluation and other approaches (e.g., training a language-specific judge) for future work.",
            "score": 0.5133614325138552,
            "section_title": "RoCulturaBench",
            "char_start_offset": 17258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 195,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1173
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06658935546875
        },
        {
            "corpus_id": "276576058",
            "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs",
            "text": "Increasing diversity and cross-cultural understanding is stymied by unchecked assumptions in both alignment techniques and evaluation methodologies. First, there is an assumption that bigger and more capable LLMs trained on more data will be inherently easier to align (Zhou et al., 2023;Kundu et al., 2023), but this sidesteps the thorny question of pluralistic variation and cultural representations (Kirk et al., 2024b). Thus, it is unclear whether improvements in architecture (Fedus et al., 2022) and post-training methods (Kirk et al., 2023;Rafailov et al., 2023) translate into improvements in cultural alignment. \n\nAlthough studies like the World Values Survey (WVS) have documented how values vary across cultures (EVS/WVS, 2022), it remains unclear whether more capable LLMs-through scaling or improved training-better align with these cultural differences (Bai et al., 2022;Kirk et al., 2023). While the WVS has been used in prior research on values in LLMs, these studies have focused predominately on individual models' performance within an English-language context. (Cao et al., 2023;Arora et al., 2023;AlKhamissi et al., 2024). This paper addresses this gap by developing a methodology for assessing how well families of LLMs represent different cultural contexts across multiple languages. We compare two distinct paths to model improvement: systematic scaling of instruction-tuned models and commercial product development comprising scaling and innovation in post-training to accommodate pressures from capabilities, cost, and preferences (OpenAI et al., 2024b). \n\nGiven these considerations, we investigate the following research questions: RQ1 Multilingual Cultural Alignment: Does improved multilingual capability increase LLM alignment with population-specific value distributions? \n\nRQ2 US-centric Bias: When using different languages, do LLMs align more with US values or with values from the countries where these languages are native? \n\nWe operationalise multilingual capability as an LLM's performance on a range of multilingual benchmarks across languages (see, e.g., Nielsen, 2023). We describe the specific benchmarks and performances in Appendix F.",
            "score": 0.5132166898253607,
            "section_title": "Introduction",
            "char_start_offset": 1951,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1804
                },
                {
                    "start": 1807,
                    "end": 1961
                },
                {
                    "start": 1964,
                    "end": 2112
                },
                {
                    "start": 2113,
                    "end": 2180
                }
            ],
            "ref_mentions": [
                {
                    "start": 269,
                    "end": 288,
                    "matchedPaperCorpusId": "258822910"
                },
                {
                    "start": 402,
                    "end": 422,
                    "matchedPaperCorpusId": "269362843"
                },
                {
                    "start": 547,
                    "end": 569,
                    "matchedPaperCorpusId": "258959321"
                },
                {
                    "start": 1081,
                    "end": 1099,
                    "matchedPaperCorpusId": "257833897"
                },
                {
                    "start": 1099,
                    "end": 1118,
                    "matchedPaperCorpusId": "247748753"
                },
                {
                    "start": 1118,
                    "end": 1142,
                    "matchedPaperCorpusId": "267759574"
                },
                {
                    "start": 2097,
                    "end": 2111,
                    "matchedPaperCorpusId": "257913673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13671875
        },
        {
            "corpus_id": "273502421",
            "title": "Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs",
            "text": "LLMs are becoming an integral part of society with a visible impact on the general public (Heikkil\u00e4, 2023;Ziegler and Donkers, 2024;Peterson, 2024). However, there exists a significant disparity in how different languages are represented in LLMs, as popular models are primarily designed with English in mind. While this is a well-known issue and more multilingual LLMs are being released, most of them are still English-dominated (Wendler et al., 2024). A prominent example is the Llama 3.1 series of models (Dubey et al., 2024), which are claimed to be state-of-the-art multilingual LLMs: these models are trained on 15T tokens, yet only 8% of the training data is declared to be non-English. \n\nWe could make an analogy between these multilingual LLMs and native English speakers who are trying to acquire a new language (Groves et al., 2018;DeVore and Kyle, 2023). Their language notions are built in an English-centric system, and they inevitably bring traces of English habits into other languages when transferring their notions (Papadimitriou et al., 2023;Wendler et al., 2024). Moreover, because of the lack of data in non-English languages, multilingual LLMs are often exposed -either during pre-training or posttraining (Yang et al., 2024;Abdin et al., 2024) to texts translated from English. Both human and machine translated language are known to suffer from translationese artifacts, which set them apart from native content (Bizzoni et al., 2020;Luo et al., 2024). LLMs trained on such data are susceptible to suffer from the same translationese problems. \n\nIn fact, even LLM-generated texts in English are known to exhibit distributional differences from human-written texts (Guo et al., 2023;Liang et al., 2024a,b;Shumailov et al., 2024). Given the predominance of English data, this effect is likely more pronounced in non-English outputs.",
            "score": 0.511100239438348,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 694
                },
                {
                    "start": 697,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1569
                },
                {
                    "start": 1572,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1856
                }
            ],
            "ref_mentions": [
                {
                    "start": 431,
                    "end": 453,
                    "matchedPaperCorpusId": "267740723"
                },
                {
                    "start": 823,
                    "end": 844,
                    "matchedPaperCorpusId": "53223158"
                },
                {
                    "start": 844,
                    "end": 866,
                    "matchedPaperCorpusId": "258750904"
                },
                {
                    "start": 1035,
                    "end": 1063,
                    "matchedPaperCorpusId": "252815840"
                },
                {
                    "start": 1063,
                    "end": 1084,
                    "matchedPaperCorpusId": "267740723"
                },
                {
                    "start": 1438,
                    "end": 1460,
                    "matchedPaperCorpusId": "219698931"
                },
                {
                    "start": 1460,
                    "end": 1477,
                    "matchedPaperCorpusId": "266741964"
                },
                {
                    "start": 1730,
                    "end": 1753,
                    "matchedPaperCorpusId": "271448069"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05224609375
        },
        {
            "corpus_id": "273532388",
            "title": "MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models",
            "text": "Using MM-EVAL, we evaluate the performance of 12 evaluator LLMs, including open reward models, open judges models, and proprietary models. Our results show that both proprietary and open evaluator LLMs have considerable room for improvement in the core subsets of MM-EVAL, achieving an overall average accuracy of only 68.9%, where random guessing would yield 50%. On the Linguistics and Safety subset, we find that the performance degradation in low-resource languages over English or high-resource languages is significant. Additionally, we find that for low-resourced languages, LLM evaluators are unfair and inconsistent, assigning lower scores to good responses and higher scores to bad ones. Finally, we validate MM-EVAL by measuring the correlation between its rankings and Best-of-N performance (Gao et al., 2023;Wu et al., 2024a), and find that MM-EVAL exhibits a statistically significant positive correlation. In contrast, English-only and machine-translated meta-evaluation benchmarks fail to show such alignment. \n\nOverall, our benchmark results indicate that strong evaluation performance in English does not readily transfer to non-English languages. Developing evaluator LLMs that perform well in multilingual settings introduces an additional set of challenges: beyond accurately capturing human preferences, they must also exhibit fairness and consistency across languages. We warn against assuming that English evaluation capabilities naturally generalize to other languages. We hope that MM-EVAL serves as a foundation for building robust, multilingual evaluator LLMs.",
            "score": 0.5109248650963607,
            "section_title": "Introduction",
            "char_start_offset": 3988,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1025
                },
                {
                    "start": 1028,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1391
                },
                {
                    "start": 1392,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1588
                }
            ],
            "ref_mentions": [
                {
                    "start": 803,
                    "end": 821,
                    "matchedPaperCorpusId": "252992904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29736328125
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "In this paper, we evaluated biased behavior of generative LLMs in English, Dutch, Spanish, and Turkish.To improve the fairness and inclusivity of these models, we believe it is of extreme importance that biases and stereotypes are addressed in languages other than English, such that their users, speakers of many different languages, can benefit equally and do not suffer harms from interacting with these models.\n\nFirst, we addressed ethical considerations when asking native speakers to evaluate whether the stereotypes held in their language and culture, and again when asking them to verify the translations.Prior to participating, participants were warned that they would encounter stereotypes and biases that address potentially sensitive topics, and we explicitly stated that they were in no way obliged to continue if they felt uncomfortable.\n\nFurthermore, we acknowledge that MBBQ contains a non-exhaustive set of stereotypes, and that it therefore cannot possibly cover all stereotypes relevant for any of the languages we consider.Due to the comparative nature of this work we focused on the stereotypes that those languages have in common, notably excluding language or culture-specific stereotypes.As a result, the bias metrics reported in this paper are an indication of the social biases present in the models we investigate, based on their behavior in the limited setting of question answering.A low bias score does not mean that the model is completely free of biases, and is no guarantee that it will not display biased behavior in other settings.We also acknowledge the possible risk associated with releasing a dataset of social biases and stereotypes.In our release of the MBBQ dataset, we will explicitly state that it should be used for evaluation of models only, and that bias scores obtained from evaluation on the dataset provide a limited representation of the model's biases.",
            "score": 0.5106790989519565,
            "section_title": "Ethics statement",
            "char_start_offset": 29058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 103,
                    "end": 414
                },
                {
                    "start": 416,
                    "end": 613
                },
                {
                    "start": 613,
                    "end": 851
                },
                {
                    "start": 853,
                    "end": 1043
                },
                {
                    "start": 1043,
                    "end": 1212
                },
                {
                    "start": 1212,
                    "end": 1411
                },
                {
                    "start": 1411,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1904
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1766357421875
        },
        {
            "corpus_id": "278166184",
            "title": "Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages",
            "text": "This issue is especially pronounced in settings where official languages differ from those predominantly used in professional contexts. For instance, while Luxembourgish is an official language in Luxembourg, French and English are more commonly employed in the workplace. Similarly, Catalan is co-official in parts of Spain, yet not all professionals are proficient in its use. Analogous situations arise in countries such as South Africa and India, where languages like Zulu, Xhosa, Maithili, or Konkani hold official status but are often underrepresented in administrative and corporate environments. \n\nThe manual translation and paraphrasing of prompts to ensure semantic consistency and cultural appropriateness across languages is both time-consuming and difficult to scale. To address this limitation, we propose leveraging LLMs to automate these tasks. Specifically, we investigate whether LLMs can reliably perform translation and paraphrasing in a way that enables the generation of linguistically and culturally appropriate test cases. If effective, this approach would facilitate scalable and inclusive multilingual bias evaluations, while reducing dependency on native language expertise and enabling broader participation by non-technical stakeholders. \n\nThis paper introduces MLA-BiTe, a framework designed to enhance existing bias evaluation methods by supporting systematic multilingual bias testing. MLA-BiTe is built to operate on the input generated by Language Bias Testing (LangBiTe) [19], but it is flexible enough to be adapted for use with other bias detection systems. To guide our study, we focus on two primary research questions: \n\n1.0.1 RQ1. \n\nCan LLM-based translation and paraphrasing effectively serve as a method to augment test templates in multiple languages, and if so, which ordering of these steps yields the most reliable expansions?",
            "score": 0.5106283312271542,
            "section_title": "Introduction",
            "char_start_offset": 2339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 603
                },
                {
                    "start": 606,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1266
                },
                {
                    "start": 1269,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1671
                },
                {
                    "start": 1674,
                    "end": 1873
                }
            ],
            "ref_mentions": [
                {
                    "start": 1506,
                    "end": 1510,
                    "matchedPaperCorpusId": "272820116"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1456298828125
        },
        {
            "corpus_id": "267617144",
            "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning",
            "text": "Appendix E.1 describes the postediting process and effort in more detail. The example above illustrates that some prompts, even when translated correctly, might still not transfer well into other languages-which is the main difference between a translated English-centric set like this and an evaluation set originally written in each target language like AYA-HUMAN-ANNOTATED. \n\nWe open-source the DOLLY-MACHINE-TRANSLATED Test Set to be an additional resource for researchers, although warn that the expressiveness of a translated evaluation set is limited by the quality of the translation model (and human post-editing) and may adversely impact an estimate of ability in languages where translations are inadequate (Nogara et al., 2023). Ultimately, this is a compromise between having evaluation coverage in a more complete set of languages (101 languages and 114 dialects in total) versus having human-annotated evaluation sets. We recommend pairing evaluation on the automatically DOLLY-MACHINE-TRANSLATED test set with evaluation on the professionally post-edited DOLLY-HUMAN-EDITED for 6 languages, or the AYA-HUMAN-ANNOTATED test set created by proficient speakers in 7 languages. We additionally recommend using human evaluation strategies to assess generated outputs on this evaluation suite. Automatic metrics underperform in creative tasks and non-English outputs, making them unsuited for this application (Gehrmann et al., 2023).",
            "score": 0.5102058402855398,
            "section_title": "Aya Evaluation Suite",
            "char_start_offset": 21242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1189
                },
                {
                    "start": 1190,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1444
                }
            ],
            "ref_mentions": [
                {
                    "start": 1420,
                    "end": 1443,
                    "matchedPaperCorpusId": "246822399"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.192138671875
        },
        {
            "corpus_id": "263605466",
            "title": "All Languages Matter: On the Multilingual Safety of Large Language Models",
            "text": "Numerous studies have been conducted to align large language models (LLMs) with human ethics and preferences to improve their safety. These include methods such as data filtering (Xu et al., 2020;Welbl et al., 2021;Wang et al., 2022), supervised fine-tuning (Ouyang et al., 2022), reinforcement learning from human feedback (RLHF) (Christiano et al., 2017), and red teaming (Perez et al., 2022;Ganguli et al., 2022a). However, despite the diverse safety alignment methods, most existing research has focused on the interactions in English (OpenAI, 2023), which is a limitation given that LLMs, like ChatGPT, are deployed globally and are frequently used for non-English communication with users from non-English-speaking regions. The discrepancy between the limited multilingual safety research and the widespread application of LLMs raises an important research question: Can non-English language prompts bypass the safety alignment primarily tuned in English? \n\nTo address this question, we develop the first multilingual safety benchmark for LLMs, called XSAFETY. We gather several well-established monolingual safety benchmarks covering 14 types of safety issues and employ professional translators to translate them, resulting in a multilingual benchmark in 10 languages. XSAFETY comprises 2,800 instances in the ten most widely-used languages, spanning several language families: English, Chinese, Spanish, French, Bengali, Arabic, Hindi, Russian, Japanese, and German, totaling 28,000 annotated instances. XSAFETY allows us to systematically evaluate the multilingual safety of four widely used LLMs, including ChatGPT, PaLM2, LLaMA-2-Chat, and Vicuna. Experimental results reveal that all the LLMs exhibit significantly lower safety in non-English languages compared to English, highlighting the need for developing safety alignment strategies for non-English languages. These findings demonstrate that the risks associated with LLMs in non-English languages are concealed by their safety performance in English, emphasizing the importance of addressing safety concerns in multiple languages.",
            "score": 0.5093895113544227,
            "section_title": "Translate back to",
            "char_start_offset": 1067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 417
                },
                {
                    "start": 418,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 961
                },
                {
                    "start": 964,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1659
                },
                {
                    "start": 1660,
                    "end": 1878
                },
                {
                    "start": 1879,
                    "end": 2100
                }
            ],
            "ref_mentions": [
                {
                    "start": 374,
                    "end": 394,
                    "matchedPaperCorpusId": "246634238"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1744384765625
        },
        {
            "corpus_id": "247626152",
            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
            "text": "Stereotype and bias formation is influenced by culture. As a result, what might be a stereotype in a given culture might not stand relevant in another. For instance, the characterization that parental leave is for mothers is considered stereotypical in the United States, but not in Sweden, where parental leave is split between both parents.  Gao et al., 2020; Table 1: The 25 largest pretrained dense language models, ranging from 6 billion parameters to 530 billion. Models are overwhelmingly trained by teams located in the US and on English text. Less than half of the language models were evaluated for bias by their creators.\n\nPrevious sections have criticized the Anglo-centricity in the research of NLP bias and the influence on languages other than English. In particular, the lack of culturally-aware datasets limits the degree to which future NLP algorithms can be evaluated for biases. More crucially, these unspecified languages and cultures are on the receiving end of unmanaged effects. As a result, researchers are encouraged to develop bias datasets and benchmarks for non Anglo-centric cultures and languages (Bender et al., 2021). Involving experts in related areas, especially participants with lived experiences of language-related harms, might aid decisions at all parts of this process, e.g. deciding what groups and content to include in research or dataset design (Liao and Muller, 2019;Dev et al., 2021a;McMillan-Major et al., 2022). Overall, having culturally diverse and comparable datasets for a diverse set of languages (ideally covering all languages) is critical for evaluating multilingual models. Moreover, the applicability of bias measures across various languages suggests the necessity for cross-linguistic metrics or measurements that can be extended to different languages or cultures (Zhou et al., 2019;Escud\u00e9 Font and Costa-juss\u00e0, 2019;Malik et al., 2021).",
            "score": 0.5091096986866404,
            "section_title": "Acknowledging Differences",
            "char_start_offset": 38167,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1128,
                    "end": 1149,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 1826,
                    "end": 1845,
                    "matchedPaperCorpusId": "202537733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22509765625
        },
        {
            "corpus_id": "276557950",
            "title": "Do Multilingual LLMs Think In English?",
            "text": "Large Language Models (LLMs) are increasingly deployed across a wide range of applications, making it crucial to understand and evaluate their performance to ensure both safety and fairness. A key characteristic of LLMs is their English-centric nature, which influences their behavior, as shown in this paper. In particular, we find further evidence that LLMs perform semantic decisions in English. This likely leads to biased behaviour (Naous et al., 2024;Shafayat et al., 2024). Understanding this is essential to equitable and reliable outcomes in diverse linguistic and cultural contexts. \n\nMoreover, our findings may further be relevant for improving the safety of LLMs. When analysing non-lexical words, we found that LLMs do not emit an English-centric bias. If knowledge representation is not universal across languages, LLMs may require language-specific safety tuning. When introspecting the latent space, we observed varying levels of vulgar terms depending on the language, particularly in cases where the models are not safety-tuned on the language. While this does not necessarily mean the model's output will be vulgar, it could make the model more vulnerable to jailbreaks (Deng et al., 2024;Ghandeharioun et al., 2024b).",
            "score": 0.508379246221649,
            "section_title": "Impact Statement",
            "char_start_offset": 30438,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 480
                },
                {
                    "start": 481,
                    "end": 592
                },
                {
                    "start": 595,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 878
                },
                {
                    "start": 879,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044281005859375
        },
        {
            "corpus_id": "270514431",
            "title": "Global Gallery: The Fine Art of Painting Culture Portraits through Multilingual Instruction Tuning",
            "text": "Exploring the intersection of language and culture in Large Language Models (LLMs), this study critically examines their capability to encapsulate cultural nuances across diverse linguistic landscapes. Central to our investigation are three research questions: the efficacy of language-specific instruction tuning, the impact of pretraining on dominant language data, and the identification of optimal approaches to elicit accurate cultural knowledge from LLMs. Utilizing the GeoMLaMA benchmark for multilingual commonsense knowledge and an adapted CAMeL dataset (English-only) for evaluation of nuanced cultural aspects, our experiments span six different languages and cultural contexts, revealing the extent of LLMs\u2019 cultural awareness. Our findings highlight a nuanced landscape: while language-specific tuning and bilingual pretraining enhance cultural understanding in certain contexts, they also uncover inconsistencies and biases, particularly in non-Western cultures. This work expands our understanding of LLMs\u2019 cultural competence and emphasizes the importance of integrating diverse cultural perspectives in their development, aiming for a more globally representative and equitable approach in language modeling.",
            "score": 0.5081142695434583,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2349853515625
        },
        {
            "corpus_id": "264305810",
            "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
            "text": "This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g., ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need to critically examine cultural dominance and ethical consideration in their development and deployment. We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs.",
            "score": 0.5079163304445642,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1455078125
        },
        {
            "corpus_id": "273950544",
            "title": "ProverbEval: Exploring LLM Evaluation Challenges for Low-resource Language Understanding",
            "text": "Most LLM evaluation benchmarks rely on multiplechoice tasks due to the ease of evaluation. Compared to generative tasks, multiple-choice tasks are simpler to assess using automatic metrics, as they eliminate the possibility of the model providing a correct answer in a different form from the ground truth (Zhang et al., 2024). This approach ensures consistency in evaluation and avoids ambiguity when assessing the model's performance. \n\nIn this work, in addition to introducing proverbbased tasks, we are interested in exploring the reliability of multiple-choice evaluations. To answer this question, we explored the following factors. \n\nPrompting language is one factor affecting the model performance. Models can be sensitive to different prompts and prompts given in several languages (Zhang et al., 2023). To evaluate the effect, we test three English prompts to assess model performance with diverse English inputs and three native prompts for each language to assess performance with instructions in the respective native language. \n\nOrder of choices affects the performance of the models in multiple-choice tasks (Zheng et al., 2023;Pezeshkpour and Hruschka, 2023). To evaluate the effect of this problem in a low-resource scenario, we compared the average of three random shuffle performances of the models to correct answers appearing first (all \"A\") or last choice (all \"D\"). \n\nFew-shot Experiments For task 2: proverb fill the blank task, we explored if introducing examples can improve the performance of the models using our validation set. \n\nEffect of Translation Cross-linguistic translation of proverbs is challenging because these expressions often carry culturally specific meanings that may not have direct equivalents in other languages. When proverbs are translated, the nuances and cultural significance can be lost, making it difficult for non-native speakers to fully understand the intended message. Our analysis of closed-source models indicates that LLMs mitigate their lack of language understanding by translating questions from low-resource languages to English and conducting reasoning in English. We translated our proverbs and compared them with native ones to see if our task is easily solvable by translating as shown in Table 5.",
            "score": 0.50722155620461,
            "section_title": "Key Factors Influencing Zero-Shot Performance",
            "char_start_offset": 13381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 706
                },
                {
                    "start": 707,
                    "end": 812
                },
                {
                    "start": 813,
                    "end": 1040
                },
                {
                    "start": 1043,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 811,
                    "matchedPaperCorpusId": "255942578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187255859375
        },
        {
            "corpus_id": "261875650",
            "title": "Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation into Multicultural Proverbs and Sayings",
            "text": "-When in Rome, do as the Romans do. \n\nAn ideal mLLM should perform on texts from all languages and translations in all directions equally well. However, in our experiments, the performance on English data is still stronger than in other languages for most of the models we studied. Recently, several works have shown that good performance can be achieved by translating non-English text data in languages into English (Conneau and Lample, 2019; Yang et al., 2019, inter alia). Here, we demonstrate that when a task relies on cultural context, there are two distinct performance gaps to achieving true multilingual ability: one is the language gap (due to mistakes by the translation system, which may be fixed by a perfect translation system), and the other is the culture gap. 12 To demonstrate this, we use English and Chinese as the focus of a case study. \n\nWe translate the data based on the descriptions in Section 4. Next, we perform the zeroshot evaluation with the best-performing multilingual models (mT0-XXL, 13B) and English model  (LLaMA-2 13B) for Zh-En (in Figure 5). In fact, both models show a performance gap in the translated data compared to the target language. Interestingly, mT0 also shows a performance degradation compared to the inference results in the original language (as LLaMA-2 is near chance level for Zh, the improvement is not surprising). In all cases, HT improves over MT, where the gain can be considered as the language gap. More interestingly, we define the gap between HT and the max of source and target language is the culture gap in mLLMs, i.e., culture gap = |Acc HT \u2212 max(Acc Src , Acc T gt )|. The culture gap for Zh-En is 5.73 for mT0 and 19.40 for LLaMA-2. 13 In an ideal situation, these gaps should be 0, indicating that the model is culturally aware and capable of understanding a language when speakers come from diverse cultural backgrounds. By closely examining the machine-translated data, it is evident that current machine translation (MT) systems do not handle cultural context well, producing incomplete or incorrect translations of proverbs.",
            "score": 0.5063068556254761,
            "section_title": "Culture Gaps in mLLMs -A Case Study",
            "char_start_offset": 21316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 35
                },
                {
                    "start": 38,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 858
                },
                {
                    "start": 861,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1373
                },
                {
                    "start": 1374,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2101
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.140380859375
        },
        {
            "corpus_id": "273098796",
            "title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English",
            "text": "Large Language Models (LLMs) have emerged as powerful tools in Natural Language Processing (NLP), demonstrating remarkable capabilities in tasks ranging from text generation to complex reasoning, including language translation and summarization [1,2]. These models, trained on vast amounts of textual data, have revolutionized language-based AI applications [2]. However, a critical issue has come to light: the majority of LLMs are primarily trained in English language data, introducing several limitations and biases [3]. \n\nThe current state of LLM development heavily favors English, with most large-scale datasets and training procedures focusing on English text by 30-60% [4]. This bias is partly due to the abundance of English content on the internet and in digital repositories, as well as the dominance of English in scientific and technological discourse. While some efforts have been made to incorporate other languages, the extent and effectiveness of these inclusions remain limited [3]. \n\nDue to the training data bias, LLMs may have a significantly reduced ability to respond effectively in other languages. This limitation manifests in various ways, including reduced accuracy in translation, poor understanding of cultural nuances, and inability to generate coherent text in non-English languages [5]. For instance, in sentiment analysis, LLMs can exhibit biases favoring dominant cultural groups, leading to inaccurate interpretations in languages like Italian, Chinese, and Spanish [6]. \n\nMoreover, these models often generate text containing social biases related to gender, age, sexual orientation, ethnicity, religion, and culture, highlighting the need to mitigate such biases [7]. LLMs also struggle with accuracy and fluency in non-English languages due to insufficient high-quality training data [8] and structural differences between languages [9], resulting in less coherent and contextually inappropriate responses [10,11].",
            "score": 0.5061774734165384,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 524
                },
                {
                    "start": 527,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1001
                },
                {
                    "start": 1004,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1506
                },
                {
                    "start": 1509,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1953
                }
            ],
            "ref_mentions": [
                {
                    "start": 1315,
                    "end": 1318,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 1701,
                    "end": 1704,
                    "matchedPaperCorpusId": "258688053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "270562911",
            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
            "text": "4.4 What potentially causes the gap? \n\nKnowledge mismatch vs translation defects in instructions Translating instruction data introduces these imperfections. To understand which accounts more for model degradation, we disentangle the two elements in instructions using roundtrip translation (RTT): we translate native data from one language into another and then translate it back to the original language, as illustrated in Figure 5. \n\nBy doing so, we can have a \"translated\" dataset that preserves the same knowledge and domain as the original data but contains translation defects. \n\nWe construct the RTT version of Russian and Chinese instruction data from their native data with Cohere or Google translation pivoting via English. This follows the same procedure used to obtain translated instruction data in Section 3.1, except that the translation workflow is now done twice: X\u2192English followed by English\u2192X. We then compare models trained on RTT data with those trained on data translated from English on native benchmarks (TyDi QA and CMMLU). \n\nRegarding TyDi QA in Table 1, we notice mixed results for Cohere translation but a relative advantage in RTT with Google translation. For CMMLU in Table 2, models with RTT (test language-origin) are uniformly better than those with data translated from English. RTT's strong performancedespite having undergone the translation process twice which likely leaves more translationese and errors-signifies the importance of incorporating native knowledge when widening language support in multilingual language models. Human vs machine translated test sets Comparing MT-MMLU and HT-MMLU results can reveal the impact of human and machine translation on the evaluation end. This comparison is carefully controlled where both test sets have the same questions originating in English and testing the same knowledge. During testing, the same set of demonstrations is prepared for the same question across the two tests. We list Spanish and Chinese results in Table 3  benchmarks and the native-translated gap is smaller compared with those on native or generative tasks. \n\nAs shown in Appendix B Tables 19 and 21, the gaps even disappear under a lower learning rate. \n\nInterestingly, gap patterns are consistent across the two translated MMLU tests: Llama2-7B on Chinese, Qwen1.4-4B on Chinese, and Qwen1.5-14B on both languages.",
            "score": 0.5059507759211288,
            "section_title": "When is the gap obvious?",
            "char_start_offset": 19255,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 434
                },
                {
                    "start": 437,
                    "end": 584
                },
                {
                    "start": 587,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1050
                },
                {
                    "start": 1053,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1861
                },
                {
                    "start": 1862,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2115
                },
                {
                    "start": 2118,
                    "end": 2211
                },
                {
                    "start": 2214,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.267333984375
        },
        {
            "corpus_id": "270380088",
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "text": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets. For example, native MMLU [Hendrycks et al., 2020] datasets have been created for Chinese [Li et al., 2023], Korean [Son et al., 2024], Indonesian [Koto et al., 2023] and Arabic [Koto et al., 2024]. Other examples of language-specific evaluation benchmarks include C-Eval for Chinese [Huang et al., 2024], HAE-RAE Bench for Korean [Son et al., 2023], COPAL-ID for Indonesian [Wibowo et al., 2023] and RoCulturaBench for Romanian [Masala et al., 2024]. Finally, Etxaniz et al. [2024] introduces 4 native Basque multiple-choice datasets with local questions. \n\nAnother relevant benchmark is SeaEval [Wang et al., 2023], which introduces 4 datasets for multicultural reasoning and 2 for cross-lingual consistency. The multicultural datasets include various countries and languages: the United States (English), Singapore (English), China (Chinese), and the Philippines (English). The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino.",
            "score": 0.505043054781112,
            "section_title": "Related Work",
            "char_start_offset": 20594,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 569
                },
                {
                    "start": 572,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1674
                },
                {
                    "start": 1675,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 1998
                }
            ],
            "ref_mentions": [
                {
                    "start": 1248,
                    "end": 1268,
                    "matchedPaperCorpusId": "258685666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.525390625
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "The grammatical properties of a language might show some interesting properties to be taken into account when dealing with the fairness of large language models, particularly for gender bias. Studies directed toward them could yield insights into observable trends across language families, with Gonen et al. ( 2019) demonstrating how the alignment of languages in the embedding space is negatively affected by grammatical gender. They could also prove helpful when analyzing bias in multilingual models, where both grammatically gendered and non-gendered languages are aligned to the same embedding space. The research and datasets available for extrinsic evaluation over other languages remain an area with scope for improvement. \n\nApart from these grammatical properties that affect the results we observe, the translation of existing bias evaluation datasets into other languages to create parallel corpora does not suffice when dealing with languages apart from English. This is partly because most languages are inherently rooted in cultural context. Any data curated for these languages must incorporate sociocultural and linguistic aspects unique to the language/region. Depriving NLP systems of cultural context could consequently lead to entire axes over which social biases are measured being ignored. The cultural significance of words and phrases in various languages can vary significantly, as demonstrated in Mohamed et al. (2022), as well as in characteristics such as metaphorical tendencies (Guti\u00e9rrez et al., 2016) and communication styles (Miehle et al., 2016;Suszczy\u0144ska, 1999). Hovy and Yang (2021) includes an overview and critique of this in the current state of NLP literature, which they claim adopts an oversimplified view and focuses on the information content alone while ignoring the social context of this content. Milios and BehnamGhader (2022); Espa\u00f1a-Bonet and Barr\u00f3n-Cede\u00f1o (2022) illustrate the inefficiency of direct translation methods, and Espa\u00f1a-Bonet and Barr\u00f3n-Cede\u00f1o (2022) advocates for the creation of culturally-sensitive datasets for fairness assessment. However, Kaneko et al. (2022) proposes a way to generate parallel corpora for other languages that bears high correlation with human bias annotations.",
            "score": 0.5042531174110759,
            "section_title": "Grammatically Gendered Languages",
            "char_start_offset": 16184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1599
                },
                {
                    "start": 1600,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2101
                },
                {
                    "start": 2102,
                    "end": 2252
                }
            ],
            "ref_mentions": [
                {
                    "start": 1509,
                    "end": 1533,
                    "matchedPaperCorpusId": "10928945"
                },
                {
                    "start": 1559,
                    "end": 1580,
                    "matchedPaperCorpusId": "1199368"
                },
                {
                    "start": 1580,
                    "end": 1598,
                    "matchedPaperCorpusId": "144001025"
                },
                {
                    "start": 1600,
                    "end": 1620,
                    "matchedPaperCorpusId": "235097460"
                },
                {
                    "start": 1878,
                    "end": 1915,
                    "matchedPaperCorpusId": "256461211"
                },
                {
                    "start": 2111,
                    "end": 2131,
                    "matchedPaperCorpusId": "248496913"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.32470703125
        },
        {
            "corpus_id": "270380088",
            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
            "text": "The cross-lingual consistency dataset covers common knowledge in 7 diverse languages: English, Chinese, Indonesian, Spanish, Vietnamese, Malay, and Filipino. Analysis of the cultural bias of LLMs has attracted some interest in recent years. Havaldar et al. [2023] concluded that multilingual models are not multicultural, whereas Tao et al. [2023] found that GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and Protestant European countries. In a similar vein, Naous et al. [2023] also found that multilingual and Arabic monolingual LMs exhibit bias towards Western culture. \n\nAccording to Liu et al. [2024] translating into English can improve the performance of Englishcentric LLMs on most multilingual tasks. However, for culturally related tasks requiring deeper language understanding, prompting in the native language proves to be more effective since it can capture the nuances related to culture and language. This aligns with our findings: Latxa (dubbed \"+ eu train\" in the experimental sections) performs better in Basque for local topics, and better in English for global topics. On the other hand, AlKhamissi et al. [2024] found that these models exhibit a higher degree of cultural alignment when they are prompted with the predominant language of the culture, and have been pre-trained on the main language of the culture. In our case, empirical results also show that pretraining in Basque improves Basque culture knowledge, and prompting in Basque leads to better results than English. \n\nThe study of LLMs from a cultural perspective is challenging. Adilazuarda et al. [2024] observed, after a survey of 39 recent papers, that none of the studies define \"culture\", which is a complex, multifaceted concept. Instead, they probe models on some datasets which represent certain aspects of \"culture\", leaving other aspects untested. Ramesh et al. [2023] argue that the vast array of cultures and languages worldwide makes it impractical to create datasets that cover all. As a result, they believe that identifying and addressing biases should move away from relying only on datasets that have limited reach and are not adaptable to every language and culture.",
            "score": 0.5036072561406693,
            "section_title": "Related Work",
            "char_start_offset": 22435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1522
                },
                {
                    "start": 1525,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1865
                },
                {
                    "start": 1866,
                    "end": 2004
                },
                {
                    "start": 2005,
                    "end": 2193
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06964111328125
        },
        {
            "corpus_id": "276961588",
            "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation",
            "text": "Traditional benchmarks struggle to evaluate increasingly sophisticated language models in multilingual and culturally diverse contexts. To address this gap, we introduce MMLU-ProX, a comprehensive multilingual benchmark covering 13 typologically diverse languages with approximately 11,829 questions per language. Building on the challenging reasoning-focused design of MMLU-Pro, our framework employs a semi-automatic translation process: translations generated by state-of-the-art large language models (LLMs) are rigorously evaluated by expert annotators to ensure conceptual accuracy, terminological consistency, and cultural relevance. We comprehensively evaluate 25 state-of-the-art LLMs using 5-shot chain-of-thought (CoT) and zero-shot prompting strategies, analyzing their performance across linguistic and cultural boundaries. Our experiments reveal consistent performance degradation from high-resource languages to lower-resource ones, with the best models achieving over 70% accuracy on English but dropping to around 40% for languages like Swahili, highlighting persistent gaps in multilingual capabilities despite recent advances. MMLU-ProX is an ongoing project; we are expanding our benchmark by incorporating additional languages and evaluating more language models to provide a more comprehensive assessment of multilingual capabilities.",
            "score": 0.5028656833824965,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12744140625
        },
        {
            "corpus_id": "253885863",
            "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
            "text": "In this paper2 , we investigated the bias present in BERT models across diverse languages including English, Greek, and Persian, rather than just a few languages from the same cultural sphere. While the focus of this project was on analyzing gender, religion, and ethnic bias types, the evaluation methods we used are not limited to these social biases. Our template set structures and sentence pseudo-likelihood-based scoring can be used for any type of bias. This sentence scoring allows for bias investigation in languages with more complex accordance rules than English, that have adjectives that decline for gender for instance, where the standard word-probability scoring method would be unable to score fairly. In this study, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it.",
            "score": 0.5021861687496485,
            "section_title": "Conclusion",
            "char_start_offset": 19205,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 460
                },
                {
                    "start": 461,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1669
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.59521484375
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "In this work we focus on generative LLMs, which face additional safety issues given that they are interactive. 2These models are known to respond inappropriately to harmful user input (Dinan et al., 2022), contain harmful stereotypes (Cheng et al., 2023;Shrawgi et al., 2024), and output harmful toxic responses to malicious instructions (Bianchi et al., 2024), as well as harmful and even benign prompts (Cercas Curry & Rieser, 2018;Gehman et al., 2020;Esiobu et al., 2023).Nevertheless, models trained with RLHF become better at defending against explicitly toxic prompts (Touvron et al., 2023;Shrawgi et al., 2024).Similarly, models are known to exhibit positive stereotypes about a specific social group when the group name is explicitly mentioned, but covertly exhibit very negative stereotypes about that same group.In particular, Hofmann et al. (2024) find that models hold negative stereotypes about speakers of African American English, in contrast to speakers of Standard American English when presented with texts in those dialects.\n\nFor these reasons, we choose to focus on more implicit stereotypes, which models are not as well guarded against, but which can have equally harmful consequences (Dev et al., 2022;Gallegos et al., 2023;Hofmann et al., 2024).\n\nBiases and safety in generative LLMs in non-English languages More recently, there are several studies that look at the safety and social biases of generative LLMs in languages other than English.Zhang et al. (2023) evaluate LLMs' safety in English and Chinese through multiple-choice questions, which originate from English and Chinese datasets and are translated accordingly to create a benchmark with parallel questions.Shen et al. (2024) translate malicious prompts from English into 19 languages, and translate model responses back to English to evaluate them using GPT-4.Similar to these works we translate a bias benchmark from English, but we investigate more implicit stereotypes divided into specific bias categories, without having to translate model responses back to English, or rely on an external language model to judge the responses.",
            "score": 0.5018700546265646,
            "section_title": "Biases in generative LLMs",
            "char_start_offset": 5414,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 112,
                    "end": 475
                },
                {
                    "start": 475,
                    "end": 618
                },
                {
                    "start": 618,
                    "end": 822
                },
                {
                    "start": 822,
                    "end": 1043
                },
                {
                    "start": 1045,
                    "end": 1269
                },
                {
                    "start": 1271,
                    "end": 1467
                },
                {
                    "start": 1467,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 204,
                    "matchedPaperCorpusId": "248780370"
                },
                {
                    "start": 234,
                    "end": 254,
                    "matchedPaperCorpusId": "258960243"
                },
                {
                    "start": 254,
                    "end": 275,
                    "matchedPaperCorpusId": "268417107"
                },
                {
                    "start": 338,
                    "end": 360,
                    "matchedPaperCorpusId": "261823321"
                },
                {
                    "start": 405,
                    "end": 434,
                    "matchedPaperCorpusId": "51879029"
                },
                {
                    "start": 434,
                    "end": 454,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 454,
                    "end": 474,
                    "matchedPaperCorpusId": "265506521"
                },
                {
                    "start": 596,
                    "end": 617,
                    "matchedPaperCorpusId": "268417107"
                },
                {
                    "start": 1207,
                    "end": 1225,
                    "matchedPaperCorpusId": "252907216"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1094970703125
        },
        {
            "corpus_id": "268724238",
            "title": "Since the Scientific Literature Is Multilingual, Our Models Should Be Too",
            "text": "The use of English is implied and taken as the default language of research papers.This lack of evaluation datasets with non-English examples creates a blind spot in our understanding of how models perform on these papers, and causes a feedback loop: without an existing dataset, researchers will generally not be motivated to train models on other languages, for two reasons.First, there will be no straightforward way to know how well the model works for other languages, and second, for a fixed model size, introducing other languages dilutes the English representation of the model, which may lower its performance on existing benchmarks, even though the model performs better in a real-world setting.The introduction of OpenMSD represents a strong step in the right direction, although the creation and release of additional benchmarks is required for more comprehensive evaluation.\n\n4 Implicit Multilingual Evaluation",
            "score": 0.5016153181468117,
            "section_title": "How multilingual is the literature?",
            "char_start_offset": 5482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 83,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 705
                },
                {
                    "start": 705,
                    "end": 887
                },
                {
                    "start": 889,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "238744120",
            "title": "Systematic Inequalities in Language Technology Performance across the World\u2019s Languages",
            "text": "1,000, to over more than 3,500 submissions in its 2020 edition. Largely as a result of this expansion of research effort, state-of-the-art systems have also achieved evaluation benchmark scores on par with human performance on a variety of NLP tasks such as question answering on English (He et al., 2021), or on automatic translation of news from German, Russian, and Chinese to English (Barrault et al., 2020). 2 These upward slanting curves on standard benchmarks fail to show how uneven this development has been for all potential NLP users. Extensive research across NLP tasks have found systematic performance drops according to dimensions such as gender, racial identity, and language varieties, among others. The reasons for these biases are multifactorial and can be traced to virtually all stages in the process of NLP development, from the data used to train systems (Caliskan et al., 2017;Sap et al., 2019;De-Arteaga et al., 2019;Tatman, 2017;Tatman and Kasten, 2017;Buolamwini and Gebru, 2018;Raji and Buolamwini, 2019) to the very algorithms involved (Speicher et al., 2018;Bellamy et al., 2018;Adebayo et al., 2016). The growing awareness of these biases in NLP technologies brought by these studies, along with the development of novel metrics and tests to evaluate these disparities, have resulted in progressively more efficient and principled strategies to understand and mitigate them.\n\nHowever, similarly systematic approaches are still lacking in one fundamental dimension of variation across individuals: their languages. Out of the over 6,500 languages spoken or signed in the world today (Hammarstr\u00f6m, 2015), only a handful are systematically represented in academia and industry (Joshi et al., 2020;Yin et al., 2021). In spite of the aforementioned near-human results on translation or understanding of languages from the world's economic and political superpowers, the experience of any NLP practitioner is that, for the vast majority of languages, they fall far below such standards. Critically, the languages of the world showcase substantial variation in most domains of description, and in fact, the performance of language technologies has been shown to be",
            "score": 0.5010436675373953,
            "section_title": "Introduction",
            "char_start_offset": 2047,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "259341896",
            "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
            "text": "Large Language Models (LLMs) (Devlin et al., 2019;Brown et al., 2020;Raffel et al., 2020) have obtained impressive performance on a wide range of NLP tasks showing great potential in several downstream applications for real world impact. However, these models have shown to be prone to picking up unwanted correlations and stereotypes from the pre-training data (Sheng et al., 2019;Kurita et al., 2019;Hutchinson et al., 2020) which, can perpetuate harmful biases for people belonging to marginalized groups. While there has been a great deal of interest in understanding and mitigating such biases in LLMs (Nadeem et al., 2021;Schick et al., 2021;Meade et al., 2022), the focus of such studies has primarily been on English. \n\nWhile Massively Multilingual Language Models (Devlin et al., 2019;Conneau et al., 2020; Xue * Equal contribution et al., 2021), have shown impressive performances across a wide range of languages, especially with their surprising effectiveness at zero-shot crosslingual transfer, there still exists a lack of focused research to evaluate and mitigate the biases that exist in these models. This can lead to a lack of inclusive and responsible technologies for groups whose native language is not English and can also lead to the dissemination of stereotypes and the widening of existing cultural gaps. \n\nPast work on evaluating and mitigating biases in multilingual models has mostly been concerned with gender bias in cross-lingual word embeddings (Zhao et al., 2020;Bansal et al., 2021) which fails to account for contextual information (Kurita et al., 2019;Delobelle et al., 2022), making them unreliable for LLMs. Other methods for estimating biases in contextualized representations involve Multilingual Bias Evaluation (Kaneko et al., 2022, MBE), which utilizes parallel translation corpora in different languages that might lack non-western cultural contexts (Talat et al., 2022). For debiasing LLMs, Lauscher et al. (2021) proposed an adapter (Houlsby et al., 2019) based approach.",
            "score": 0.5009246556542118,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 725
                },
                {
                    "start": 728,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1329
                },
                {
                    "start": 1332,
                    "end": 1645
                },
                {
                    "start": 1646,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2017
                }
            ],
            "ref_mentions": [
                {
                    "start": 29,
                    "end": 50,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 69,
                    "end": 89,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 362,
                    "end": 382,
                    "matchedPaperCorpusId": "202537041"
                },
                {
                    "start": 382,
                    "end": 402,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 402,
                    "end": 426,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 607,
                    "end": 628,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "232075876"
                },
                {
                    "start": 648,
                    "end": 667,
                    "matchedPaperCorpusId": "239015827"
                },
                {
                    "start": 773,
                    "end": 794,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 794,
                    "end": 814,
                    "matchedPaperCorpusId": "207880568"
                },
                {
                    "start": 1477,
                    "end": 1496,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1567,
                    "end": 1588,
                    "matchedPaperCorpusId": "190000105"
                },
                {
                    "start": 1588,
                    "end": 1611,
                    "matchedPaperCorpusId": "250390561"
                },
                {
                    "start": 1894,
                    "end": 1914,
                    "matchedPaperCorpusId": "247626152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205810546875
        },
        {
            "corpus_id": "273962875",
            "title": "LLM-GLOBE: A Benchmark Evaluating the Cultural Values Embedded in LLM Output",
            "text": "While they were able to pinpoint many of the cultural biases exhibited by LLMs across different cultures, the majority only investigated Western LLMs and did not compare them to LLMs trained primarily in non-English languages. Furthermore, they conducted limited analysis on open-generation tasks and primarily focused on having models choose between clearly defined options. One recent study has built upon this framework, defining \"personas\" for the model to take when answering closed-ended questions based on Hofstede's cultural dimensions [Kharchenko et al., 2024]. Certain studies have also tried to improve model alignment for current popular English-based LLMs, specifically for cultural competency [Tao et al., 2024, Niszczota et al., 2023]. For example, one study found that specifying the country of origin could improve cultural alignment across several models [Kwok et al., 2024]. \n\nOutside of Hofstede's cultural dimensions, other studies have been conducted to explore the cultural dimensions of LLMs through different survey questionnaires [Ramezani and Xu, 2023]. AlKhamissi et al. [2024] utilizes the \"personas\" idea to evaluate both English and Arabic-based LLMs on survey items that are then compared to human responses from the respective cultural groups. Warmsley et al. [2024] builds upon this approach, applying the framework towards English and Chinese-speaking populations. Lastly, new research has introduced other benchmarks to evaluate the cultural competency of LLMs. For example, Chiu et al. [2024] introduce CulturalBench, which consists of multiple choice questions, notably building upon prior surveys by providing the option for more than one response to be correct. Cultural benchmarks have also been extended to vision-language models, which are able to process both image and text input [Nayak et al., 2024, Zhang et al., 2024]. \n\nEvidently, while there has been recent headway in the evaluation of the cultural values held by LLMs, there still remains important gaps in the literature.",
            "score": 0.5008016738633213,
            "section_title": "Evaluation of LLM Cultural Competencies",
            "char_start_offset": 20440,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 893
                },
                {
                    "start": 896,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1497
                },
                {
                    "start": 1498,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1866
                },
                {
                    "start": 1869,
                    "end": 2024
                }
            ],
            "ref_mentions": [
                {
                    "start": 707,
                    "end": 724,
                    "matchedPaperCorpusId": "265445838"
                },
                {
                    "start": 1277,
                    "end": 1299,
                    "matchedPaperCorpusId": "272968367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07464599609375
        },
        {
            "corpus_id": "265043227",
            "title": "Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation",
            "text": "Many works have focused on evaluating gender translation accuracy by creating various benchmarks. WinoMT benchmark by Stanovsky et al. (2019) is widely used for gender bias evaluation. It contains sentences from WinoBias (Zhao et al., 2018) and Winogender (Rudinger et al., 2018) coreference test sets in English. Without reference translations, it devises an automatic translation evaluation method for eight diverse target languages. \n\nOther benchmarks include MuST-SHE (Bentivogli et al., 2020), GeBioCorpus (Costa-juss\u00e0 et al., 2020), MT-GenEval (Currey et al., 2022), GATE (Rarrick et al., 2023) etc. MT-GenEval provides gender-balanced, counterfactual sentences in eight language pairs with English as the source. Therefore, most of the benchmarks focus on English as the source language. \n\nBias evaluation of NMT models on source lan-guages other than English has mainly focused on the translation of gender-neutral sentences. Cho et al. (2019) proposed TGBI measure to evaluate gender bias in the translation of gender-neutral Korean sentences to English. Ramesh et al. (2021) used TGBI measure for Hindi-English machine translation. Our work emphasises on creation of gender unambiguous evaluation benchmarks for source languages other than English by accounting for gender inflections in the language to test the model's ability to find these gender-related cues.",
            "score": 0.5002969431272044,
            "section_title": "Related Work",
            "char_start_offset": 12976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 435
                },
                {
                    "start": 438,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 794
                },
                {
                    "start": 797,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 118,
                    "end": 141,
                    "matchedPaperCorpusId": "173991101"
                },
                {
                    "start": 221,
                    "end": 240,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 256,
                    "end": 279,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 472,
                    "end": 496,
                    "matchedPaperCorpusId": "219558379"
                },
                {
                    "start": 550,
                    "end": 571,
                    "matchedPaperCorpusId": "253254842"
                },
                {
                    "start": 934,
                    "end": 951,
                    "matchedPaperCorpusId": "167217680"
                },
                {
                    "start": 1064,
                    "end": 1084,
                    "matchedPaperCorpusId": "235446743"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.22265625
        },
        {
            "corpus_id": "264405758",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "text": "Regarding consistency (Section 4), GPT-3.5 tends to generate more consistent responses on English compared to non-English languages. We observe a maximum performance decrease of 9.1% in Spanish, 28.3% in Chinese, and 50.5% in Hindi when compared to English. All language pairs, except English-Spanish, exhibit statistically significant differences in performance, demonstrating the existence of language disparity. Regarding verifiability (Section 5), English and Spanish demonstrate comparable performances, whereas the performances for Chinese and Hindi are notably lower. In the most extreme case, Chinese and Hindi exhibit decreases of 14.6% and 23.4% on Macro F1, respectively. \n\nOur research carries significant real-world implications on multiple fronts. The evaluation framework proposed in our work possesses practical utility for policymakers, practitioners, and healthcare professionals for evaluating large language models and comparing their relative performance. Through our examination of LLMs' capabilities in major languages, we aspire to acquire a comprehensive understanding of their global effectiveness, which stands to influence a vast and linguistically diverse global population, impacting both linguistic accessibility and information reliability. Furthermore, our framework exhibits versatility and adaptability beyond healthcare, extending its applicability to other domains. \n\nOur contributions are summarized as follows: \u2022 Novel Framework. We propose XLingEval, a comprehensive evaluation framework for LLMs in the healthcare domain that focuses on three fundamental criteria: correctness, verifiability, and consistency. Our framework features the gaps in equity in LLM development across multiple languages, and demonstrates generalizability in this evaluation across different LLMs. \u2022 Novel Medical Benchmark. We propose XLingHealth, a Cross-Lingual Healthcare benchmark for clinical health inquiry that features the top four most spoken languages in the world. \u2022 Extensive Multilingual Evaluation. We performed comprehensive evaluation on the four most spoken languages, and found significant language disparity across these languages. \n\nWe have released all of our code, data, and tools on GitHub1 .",
            "score": 0.5001830409252321,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4415,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1272
                },
                {
                    "start": 1273,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1650
                },
                {
                    "start": 1651,
                    "end": 1814
                },
                {
                    "start": 1815,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1993
                },
                {
                    "start": 1994,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2168
                },
                {
                    "start": 2171,
                    "end": 2233
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12841796875
        },
        {
            "corpus_id": "278714901",
            "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances",
            "text": "The accuracy and clarity of automated translations can vary widely depending on the complexity of the source text. Fixed-phrase tools may succeed in standardized scenarios but lack flexibility in nuanced interactions, whereas unconstrained machine translation can yield ambiguous outputs if the underlying model is poorly adapted to the specific domain. Ensuring that these technologies supplement, rather than replace, professional interpreters require not only context-specific fine-tuning but also comprehensive training for end-users on the models' capabilities and caveats. \n\nFifth, the limitations of purely quantitative metrics highlight the need for more comprehensive evaluation methods. While BLEU scores provide a valuable measure of linguistic fidelity, they cannot fully capture the cultural nuances embedded in translations. In future work, incorporating human evaluation of translations would provide a more nuanced assessment of model performance. By including real-world examples of translations, it would be possible to illustrate how cultural subtleties are either captured or missed by each model. This addition would complement the quantitative findings and provide deeper insights into the effectiveness of LLMs in preserving cultural context in minority language translations. Finally, the \"black box\" nature of LLMs and the resource requirements associated with their deployment pose significant hurdles for ongoing research and operational integration. Proprietary architectures often limit transparency in how data are utilized or how specific outputs are generated. Consequently, replicating or auditing model performance becomes difficult, especially for teams with restricted budgets or computational resources. This lack of openness can stifle efforts to debug, adapt, or explain system outputs-factors that are crucial for cultivating trust and expanding the use of LLM-based tools in low-resource contexts. These challenges underscore the importance of ethical data collection, extensive collaboration with community stakeholders, and targeted model enhancements. Although advanced architectures such as retrieval-augmented generation in conjunction with powerful language models hold substantial promise, robust solutions will require a multifaceted approach-one that balances technological innovation with diligent attention to cultural, ethical, and socioeconomic realities.",
            "score": 0.5000234060481228,
            "section_title": "Limitations and Challenges",
            "char_start_offset": 38021,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 578
                },
                {
                    "start": 581,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1740
                },
                {
                    "start": 1741,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1925048828125
        },
        {
            "corpus_id": "268510024",
            "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models",
            "text": "Based on the previous results, ChatGPT and Llama-2-70B-chat both tend to be English-centric but ChatGPT demonstrates certain behaviors of a \"Multilingual LLM\". Consequently, we broaden our analysis to include non-English-centric LLMs and assess their performance across various tasks. \n\nHow do non-English-centric LLMs perform on culture-related tasks? To investigate the behaviors of different LLMs on culture-related tasks, we select another two LLMs: Qwen1.5-72B-Chat (Bai et al., 2023) and Yi-34B-Chat (AI et al., 2024), which are not English-centric. These two opensource models demonstrate strong capabilities in both English and Chinese. Therefore, we can check whether they demonstrate multilingual behaviors in Chinese, as illustrated in Figure 1(b). \n\nFor the evaluation dataset, we choose M3Exam (Zhang et al., 2023a), as the questions are realworld natural data from different languages instead of translating from English and require strong multilingual proficiency and cultural knowledge to perform well. For example, the question about a Swahili proverb in Figure 2 Figure 6: Accuracies of four LLMs on M3Exam (a) language and (b) social science subject categories. In M3Exam, not all subjects are available in every language, causing a difference in language coverage between the two subjects. \n\nand social science subject categories, which likely contain more native cultural knowledge, and evaluate up to 500 samples per language. \n\nBased on the results shown in Figure 6, we have the following observations: 1) For ChatGPT, translation may not always result in improved performance. This observation aligns with the conclusions in the study by Zhang et al., 2023a. The effectiveness of translation largely depends on whether translation errors outweigh any potential gains in better comprehension. 2) Translation helps Llama-2-70B-chat in all the languages, suggesting that the model's underperformance is due to poor language understanding rather than limitations of cultural knowledge. 3) Qwen1.5-72B-Chat and Yi-34B-Chat excel in Chinese proficiency.",
            "score": 0.49981582497721644,
            "section_title": "Analysis and Discussions",
            "char_start_offset": 15276,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 284
                },
                {
                    "start": 287,
                    "end": 352
                },
                {
                    "start": 353,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1309
                },
                {
                    "start": 1312,
                    "end": 1448
                },
                {
                    "start": 1451,
                    "end": 1601
                },
                {
                    "start": 1602,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0809326171875
        },
        {
            "corpus_id": "270923783",
            "title": "How Does Quantization Affect Multilingual LLMs?",
            "text": "Indeed, many model designs choices implicitly overfit to a handful of resource rich languages: from tokenizer choice, to weighting of training data, and to widely-used quantization techniques. Focusing on a small subset of high-resource languages in design degrades model performance for overlooked languages (Schwartz et al., 2022;Kotek et al., 2023;Khandelwal et al., 2023;Vashishtha et al., 2023;Khondaker et al., 2023;Pozzobon et al., 2024), introduces security vulnerabilities (Yong et al., 2023;Nasr et al., 2023;Li et al., 2023a;Lukas et al., 2023;Deng et al., 2023), and unfairly passes high costs to non-English users faced with high latency (Held et al., 2023;Durmus et al., 2023;Nicholas and Bhatia, 2023;Ojo et al., 2023;Ahia et al., 2023). \n\nWe analyze four state-of-the-art (SOTA) multilingual LLMs across 3 different sizes ranging from 8 to 103 billion parameters and covering up to 23 languages, under various quantization techniques. Critically, it is vital that we move beyond automatic evaluation and gather real human feedback on performance cost. We thus perform multilingual human evaluation on challenging real-world prompts in addition to LLM-as-a-Judge and evaluation on standard automatic benchmarks such as multilingual MMLU (Hendrycks et al., 2020), MGSM (Shi et al., 2023), andFLORES-200 (Costa-juss\u00e0 et al., 2022a). Across experimental set-ups we find that: \n\n1. Automatic metrics severely underestimate damage from quantization. Automatic evaluations estimate deterioration relative to FP16 across tasks at \u22120.3% (French) and \u22121.7% (Japanese) vs. \u221216.6% and \u221216.0% reported by human evaluators. See Figure 1. 1 2. Quantization affects languages differently.",
            "score": 0.499641744729375,
            "section_title": "Introduction",
            "char_start_offset": 1304,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 950
                },
                {
                    "start": 951,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1387
                },
                {
                    "start": 1390,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1688
                }
            ],
            "ref_mentions": [
                {
                    "start": 309,
                    "end": 332,
                    "matchedPaperCorpusId": "247482521"
                },
                {
                    "start": 332,
                    "end": 351,
                    "matchedPaperCorpusId": "261276445"
                },
                {
                    "start": 536,
                    "end": 555,
                    "matchedPaperCorpusId": "256459554"
                },
                {
                    "start": 1283,
                    "end": 1306,
                    "matchedPaperCorpusId": "252735112"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04296875
        },
        {
            "corpus_id": "271709612",
            "title": "Analyzing Cultural Representations of Emotions in LLMs Through Mixed Emotion Survey",
            "text": "A few previous studies have already analyzed cultural representations in LLMs. For example, Naous et al. [17] assessed biases in LLMs towards Arab and Western cultures, focusing on story generation, NER (named entity recognition), sentiment analysis, contextual prompt analysis, and text infilling. The study shows that multilingual and Arabic monolingual LMs exhibit bias towards entities associated with Western culture. The study further introduced a resource called CAMeL containing naturally occurring prompts and entities that contrast Arab and Western cultures. In line with that, Atari et al. [18] show that the LLMs performance of psychological cognitive tasks resembles most of the Western Industrialized Educated Rich and Democratic (WIERD) population in an extensive cross-language study on LLMs. Similarly, Arora et al. [19] found in their study which was based on World Values Survey3 , that while the values elicited from models vary across cultures, their bias is not in line with values outlined in existing large-scale values surveys. More widely, LLMs cultural representation has started gaining interest in mainstream media outlets [20], [21]. \n\nBeyond output analysis, a recent study from Wendler et al. [22] explored whether multilingual language models use English as an internal pivot language, focusing on the Llama-2 family of transformer models, probing LLMs internally. The study was done by tracking intermediate embedding and logit lens analysis based on constructed non-English prompts with a unique correct single-token continuation. The study provides compelling evidence that multilingual language models may indeed use English as an internal pivot language, albeit in a nuanced and conceptually biased manner.",
            "score": 0.4993203090097963,
            "section_title": "C. Cultural representations in LLMs",
            "char_start_offset": 7559,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1744
                }
            ],
            "ref_mentions": [
                {
                    "start": 833,
                    "end": 837,
                    "matchedPaperCorpusId": "247748753"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06951904296875
        },
        {
            "corpus_id": "276580130",
            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
            "text": "This study provides the first comprehensive investigation of multilingual nationality bias in state-ofthe-art (SOTA) Large Language Models (LLMs) across reasoning-based decision-making tasks. Our findings reveal that while LLMs exhibit lower bias in English, significant disparities emerge in non-English languages. This bias impacts the fairness and consistency of choices and the structure of reasoning. The bias patterns observed are influenced not only by language differences but also by user demographics and reasoning strategies. For example, in non-English contexts, Chain-of-Thought (CoT) prompting often exacerbates rather than mitigates bias, and female-based decisions usually introduce higher bias than male-based ones. Furthermore, our evaluation demonstrates that different models prioritize decision-making criteria differently. Future research should explore bias mitigation techniques tailored for multilingual settings, considering both linguistic and cultural fac-tors to enhance fairness and inclusivity in AI-driven decision-making applications.",
            "score": 0.4983899806531594,
            "section_title": "Conclusion",
            "char_start_offset": 31067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 1067
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50634765625
        },
        {
            "corpus_id": "277780370",
            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
            "text": "With the growing presence and deployment of LLMs across the world, evaluating their abilities in languages other than English becomes more and more eminent. Yet, studying and evaluating multilinguality in LLMs remains a challenging enterprise, and it is hardly exaggerated to call the current state of multilingual evaluation in LLMs insufficient. Older multilingual benchmarks such as PAWS-X (Zhang et al., 2019), XNLI (Conneau et al., 2018) or XCOPA (Ponti et al., 2020) often do not fit the demands for evaluating auto-regressive models and are rarely used to evaluate recent models. Furthermore, their coverage of languages is relatively small compared to the number of languages in which LLMs are intended to be proficient. More often used for LLM evaluation are benchmarks translated from English, such as MGSM (translated GSM8K, Shi et al., 2023), MMMLU (tranlated MMLU, OpenAI, 2025) or (less frequently) Belebele (Bandarkar et al., 2024). These benchmarks provide good coverage over many languages, but using translated data comes with its own set of issues. One such issues is that even when humanrather than machine-authored translations are used, translated data is known to differ from native text in several ways (Clark et al., 2020). Furthermore, using translated benchmarks imposes a strong English-centric bias: translated data may be multilingual on the surface, it is not in its content. The benchmarks MLQA (Lewis et al., 2020) and TidyQA (Clark et al., 2020) to some extent address the issue by sourcing data separately for different languages. Even in their sourcing protocols, however, there is no explicit focus on selecting locally relevant content for the chosen languages. 2 In addition to that, their coverage is again small compared to the above mentioned translated benchmarks. In response to these issues, we introduce a wide-coverage multilingual benchmark with locallysourced questions for 31 different languages. Because the benchmark targets multilingual local knowledge, we dub it MultiLoKo. The release of MultiLoKo serves two interconnected goals: \n\n1) Provide a better means to evaluate multilinguality in LLMs;",
            "score": 0.4980364365293497,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 347
                },
                {
                    "start": 348,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2085
                },
                {
                    "start": 2088,
                    "end": 2150
                }
            ],
            "ref_mentions": [
                {
                    "start": 393,
                    "end": 413,
                    "matchedPaperCorpusId": "91184042"
                },
                {
                    "start": 420,
                    "end": 442,
                    "matchedPaperCorpusId": "52271711"
                },
                {
                    "start": 452,
                    "end": 472,
                    "matchedPaperCorpusId": "218470125"
                },
                {
                    "start": 836,
                    "end": 853,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 922,
                    "end": 946,
                    "matchedPaperCorpusId": "261394577"
                },
                {
                    "start": 1227,
                    "end": 1247,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1427,
                    "end": 1447,
                    "matchedPaperCorpusId": "204734128"
                },
                {
                    "start": 1459,
                    "end": 1479,
                    "matchedPaperCorpusId": "212657414"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2054443359375
        },
        {
            "corpus_id": "259274987",
            "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
            "text": "Nevertheless, the BBQ dataset [21] could benefit from a further development if we take a broad perspective of culture and diversity. Firstly, its focus on English within one cultural context limits its effectiveness for evaluating biases in diverse cultures and languages. Direct translation doesn't adequately capture cultural differences. Secondly, the initial design of the BBQ dataset [21] was to measure the degree of bias in QA systems, however, large language models possess powerful interpretive capabilities and behavioral inconsistencies, thus necessitating specific evaluations for their biases. Furthermore, the assumption that alignment of model answers with societal biases indicates intrinsic model bias is debatable, as this could also reflect model comprehension and reasoning. Lastly, the manual creation of templates in BBQ requires considerable resources and may lack the quantity, diversity, and creativity needed for comprehensive bias evaluation. \n\nIn light of these issues, we propose a Chinese Bias Benchmark dataset curated with Human-AI Collaboration (CBBQ) for measuring bias in Chinese LLMs, which introduces several improvements. Our key contributions are as follows: \n\n\u2022 CBBQ is rooted in the Chinese social and cultural context, with a broader coverage on bias categories. CBBQ covers a wider range of bias categories and socially prevalent stereotypes in Chinese society, shown in Table 1. We would like this benchmark, which contains over 110K examples, to serve as an effective and comprehensive testbed, contributing to pre-deployment testing for Chinese or multilingual large language models. \n\n\u2022 CBBQ is curated with a revised dataset design and evaluation method, which is better suited for LLMs bias evaluation in comparison to BBQ. In our design of disambiguated contexts, we only supplement with background information that contradicts societal biases. Accordingly, taking into account the powerful interpretive capabilities and behavioural inconsistencies of LLMs, we have revised the bias score evaluation process. During the evaluation process, we ask the model to output both the explanation and choice of answer simultaneously. Only when both the explanation and the selected answer of the model are unbiased, do we consider it as unbiased. Additionally, we have also provided our prompts for the evaluation of the dataset, thus offering a comprehensive evaluation methodology for LLMs.",
            "score": 0.4980323150714411,
            "section_title": "Introduction",
            "char_start_offset": 2081,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 133,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 340
                },
                {
                    "start": 341,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 969
                },
                {
                    "start": 972,
                    "end": 1159
                },
                {
                    "start": 1160,
                    "end": 1197
                },
                {
                    "start": 1200,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1629
                },
                {
                    "start": 1632,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2174
                },
                {
                    "start": 2175,
                    "end": 2287
                },
                {
                    "start": 2288,
                    "end": 2433
                }
            ],
            "ref_mentions": [
                {
                    "start": 30,
                    "end": 34,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 389,
                    "end": 393,
                    "matchedPaperCorpusId": "239010011"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1761474609375
        },
        {
            "corpus_id": "269004468",
            "title": "How Many Languages Make Good Multilingual Instruction Tuning? A Case Study on BLOOM",
            "text": "Nevertheless, it is essential to note the variations in the number and diversity of languages used in these studies.Kew et al. (2023) considered five non-English languages (Spanish, Russian, German, Chinese, and French) but also trained models on the full Guanaco dataset with more than 30 languages.Shaham et al. (2024) experimented with a machine-translated instruction dataset in 11 diverse languages.Chirkova & Nikoulina (2024b) studied finetuning with English data and tested on four languages (English, French, Portuguese, and Russian).These works are not strictly comparable due to different training configurations, data, model choices, and evaluation tasks and benchmarks.More importantly, while it has been demonstrated that a small number of languages can elicit (zero-shot) cross-lingual transfer, it is in no way associated with achieving the optimal downstream task performance.\n\nWe aim to fill in the gap by scaling up the number of languages in the instruction tuning phase, offering insights into the broader implications of multilingual instruction fine-tuning.This paper employs instruction fine-tuning using the multilingual BLOOM model (Scao et al., 2022), and a parallel instruction dataset named Bactrain-X in 52 languages (Li et al., 2023).We progressively add a language during instruction fine-tuning at each time, and train 52 models in total.Then, we evaluate those models using three multilingual benchmarks.Our experimental results show that:\n\n\u2022 Contrary to prior research, adding more languages beyond a handful can further improve accuracy, although with some outlier cases and diminishing returns.\n\n\u2022 Given the considered 52 studied languages, there is no consistent answer regarding the optimal number of languages for mIT.The optimal number of instruction languages depends on the language similarity and downstream evaluation.\n\n\u2022 The impact of mIT can vary, potentially aiding or hindering multilingual performance.Additionally, the cross-lingual transfer ability of mIT exists, though both phenomena are contingent upon the benchmark and languages involved.\n\nOur study emphasizes the importance of a closer look at the tasks, benchmarks, languages, and evaluation metrics.And we advocate for more consistent future studies focused on mIT.",
            "score": 0.4971950169953381,
            "section_title": "Introduction",
            "char_start_offset": 2058,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 404
                },
                {
                    "start": 404,
                    "end": 542
                },
                {
                    "start": 542,
                    "end": 681
                },
                {
                    "start": 681,
                    "end": 892
                },
                {
                    "start": 894,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1264
                },
                {
                    "start": 1264,
                    "end": 1370
                },
                {
                    "start": 1370,
                    "end": 1437
                },
                {
                    "start": 1437,
                    "end": 1472
                },
                {
                    "start": 1474,
                    "end": 1630
                },
                {
                    "start": 1632,
                    "end": 1757
                },
                {
                    "start": 1757,
                    "end": 1862
                },
                {
                    "start": 1864,
                    "end": 1951
                },
                {
                    "start": 1951,
                    "end": 2094
                },
                {
                    "start": 2096,
                    "end": 2209
                },
                {
                    "start": 2209,
                    "end": 2275
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0709228515625
        },
        {
            "corpus_id": "259108559",
            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
            "text": "Transfer learning paradigms such as fine-tuning, few-shot learning, and zero-shot learning rely on pretrained language models (PLMs), that require having large compilations of raw data (Devlin et al. 2019;Brown et al. 2020;Chowdhery et al. 2022;Scao et al. 2022). These PLMs showed some ability to model different linguistic phenomena (Goldberg 2019;Jawahar et al. 2019) in addition to memorizing facts related to real-world knowledge. While there is a drive to have multilingual models, English is still the language that is better supported due to the abundance of large English raw corpora, diverse datasets, and benchmarks. Moreover, monolingual non-English PLMs are still being pretrained for other high-resource languages. As a way to probe the non-English and multilingual PLMs, researchers tend to translate English benchmarks into other languages, which might degrade the quality of the samples especially if the translation is performed automatically. While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks. \n\nLAMA (Petroni et al., 2019) and ParaRel (Elazar et al., 2021) are two benchmarks developed to quantify the factual knowledge of the English PLMs. They used a setup in which a language model is said to know a specific fact if it can predict the right object for a prompt in a fill-the-gap setup (e.g., For the prompt \"The capital of England is [MASK]\", the model needs to fill the masked gap with \"London\"). Multilingual versions of these benchmarks namely: mLAMA (Kassner et al., 2021), and mParaRel (Fierro and S\u00f8gaard, 2022) were released to evaluate the performance of multilingual PLMs by translating LAMA and ParaRel into 53 and 46 languages respectively. The subjects and objects of the triples within these benchmarks were translated using their multilingual labels on Wikidata, while the templates were automatically translated from the English ones used in the original benchmarks.",
            "score": 0.4967701240123549,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 2037
                }
            ],
            "ref_mentions": [
                {
                    "start": 185,
                    "end": 205,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 205,
                    "end": 223,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 350,
                    "end": 369,
                    "matchedPaperCorpusId": "195477534"
                },
                {
                    "start": 1152,
                    "end": 1174,
                    "matchedPaperCorpusId": "202539551"
                },
                {
                    "start": 1610,
                    "end": 1632,
                    "matchedPaperCorpusId": "231740666"
                },
                {
                    "start": 1647,
                    "end": 1673,
                    "matchedPaperCorpusId": "247596661"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52587890625
        },
        {
            "corpus_id": "258865272",
            "title": "Having Beer after Prayer? Measuring Cultural Bias in Large Language Models",
            "text": "Various works has explored biases in non-English languages. One line of work translates English datasets into other languages (Levy et al., 2023;N\u00e9v\u00e9ol et al., 2022;Lee et al., 2023;Kurpicz-Briki, 2020;Lauscher and Glava\u0161, 2019). We argue that this is not an effective strategy, as the translated evaluation data lacks the relevant cultural identity (Talat et al., 2022). Most studies focus primarily on gender bias (Das et al., 2023;Vashishtha et al., 2023;Touileb et al., 2022;Kaneko et al., 2022) or social bias (N\u00e9v\u00e9ol et al., 2022;Bhatt et al., 2022;BehnamGhader and Milios, 2022;Nozza et al., 2021). In this paper, we study a more subtle and understudied yet very important problem -cultural appropriateness of LMs in non-English and non-Western environments. We focus on culture-specific entities and analyze cross-cultural performance of LMs on such entities. We construct CAMeL, a novel dataset of naturallyoccuring Arabic prompts obtained from Twitter/X, and an extensive list of entities associated with Arab and Western culture across eight entity types that exhibit cultural variation. \n\nMeasuring Biases in LMs. Early work on measuring biases examined vector space distances between static word embeddings of neutral attributes (e.g., professions) and social attributes (e.g., genders, races) (Caliskan et al., 2017;Dev et al., 2021). Embedding-based methods were then adapted to contextualized embeddings of LMs learned from the context of sentences, where neutral and social attributes are placed in sentence templates (e.g., \"This is Katie\", \"This is a friend\") (May et al., 2019;Guo and Caliskan, 2021;Tan and Celis, 2019).",
            "score": 0.49634132270568065,
            "section_title": "A Additional Background",
            "char_start_offset": 25138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 867
                },
                {
                    "start": 868,
                    "end": 1098
                },
                {
                    "start": 1101,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1641
                }
            ],
            "ref_mentions": [
                {
                    "start": 145,
                    "end": 165,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 165,
                    "end": 182,
                    "matchedPaperCorpusId": "258486940"
                },
                {
                    "start": 202,
                    "end": 228,
                    "matchedPaperCorpusId": "135465247"
                },
                {
                    "start": 350,
                    "end": 370,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 416,
                    "end": 434,
                    "matchedPaperCorpusId": "258486951"
                },
                {
                    "start": 434,
                    "end": 458,
                    "matchedPaperCorpusId": "259341896"
                },
                {
                    "start": 458,
                    "end": 479,
                    "matchedPaperCorpusId": "250391092"
                },
                {
                    "start": 515,
                    "end": 536,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 536,
                    "end": 555,
                    "matchedPaperCorpusId": "252532134"
                },
                {
                    "start": 555,
                    "end": 585,
                    "matchedPaperCorpusId": "253885863"
                },
                {
                    "start": 585,
                    "end": 604,
                    "matchedPaperCorpusId": "235097294"
                },
                {
                    "start": 1307,
                    "end": 1330,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 1330,
                    "end": 1347,
                    "matchedPaperCorpusId": "220281039"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19482421875
        },
        {
            "corpus_id": "264426250",
            "title": "Semantic and Expressive Variation in Image Captions Across Languages",
            "text": "replacing \"on\" with \"above\" and \"memories\" with \"impressions\" -and therefore do not noticeably alter measurements. Therefore, we believe the translations did not introduce significant analysis-biasing artifacts. \n\nNonetheless, we repeat the representation variance experiment described in \u00a72.3 and \u00a73.4 with multilingual embeddings without translation and find that the same result holds as described (using English embeddings with translation) (see \u00a79.2). This shows that our results still hold when using tools that can be theoretically applied across languages (even though they are language-biased). We describe all of our experiments in the main paper under the assumption of English translation for fair comparison.",
            "score": 0.496235583755007,
            "section_title": "Translation for fair comparison",
            "char_start_offset": 11254,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 211
                },
                {
                    "start": 214,
                    "end": 456
                },
                {
                    "start": 457,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 721
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.131103515625
        },
        {
            "corpus_id": "254017505",
            "title": "Undesirable biases in NLP: Averting a crisis of measurement",
            "text": "Subconcepts of model bias might become especially relevant when considering other languages and bias types, since some manifestations may or may not be shared cross-culturally. 12 However, identifying subconcepts may prove difficult (e.g., techniques like factor analysis might statistically identify subcomponents of model bias for which we do not have intuitive explanations of what they mean), and assumptions about the existence of such subconcepts should be thoroughly tested. In Application IV, we discuss content validity for benchmarks datasets aggregating several different bias types. \n\n12. For example, in Turkish, gender markings of nouns are optional and bias might show itself in whether or not gender is explicitly marked. For instance, to translate the words sister/brother into Turkish, there exists only one gender-neutral translation 'sibling' which is optionally accompanied by a word for female/male. When translating \"My sister/brother is a soccer player\" into Turkish, the language model could exhibit bias by explicitly marking the gender in the former case but not in the latter.",
            "score": 0.49527811809943506,
            "section_title": "Content validity",
            "char_start_offset": 66469,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 594
                },
                {
                    "start": 597,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 921
                },
                {
                    "start": 922,
                    "end": 1104
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353759765625
        },
        {
            "corpus_id": "277667574",
            "title": "EnDive: A Cross-Dialect Benchmark for Fairness and Performance in Large Language Models",
            "text": "The diversity of human language, shaped by social, cultural, and regional influences, presents significant challenges for natural language processing (NLP) systems. Existing benchmarks often overlook intra-language variations, leaving speakers of non-standard dialects underserved. To address this gap, we introduce EnDive (English Diversity), a benchmark that evaluates five widely-used large language models (LLMs) across tasks in language understanding, algorithmic reasoning, mathematics, and logic. Our framework translates Standard American English datasets into five underrepresented dialects using few-shot prompting with verified examples from native speakers, and compare these translations against rule-based methods via fluency assessments, preference tests, and semantic similarity metrics. Human evaluations confirm high translation quality, with average scores of at least 6.02/7 for faithfulness, fluency, and formality. By filtering out near-identical translations, we create a challenging dataset that reveals significant performance disparities - models consistently underperform on dialectal inputs compared to Standard American English. EnDive thus advances dialect-aware NLP by uncovering model biases and promoting more equitable language technologies.",
            "score": 0.4951879051859655,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11614990234375
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "Both approaches described in Section 3.3 would result in benchmarks with less English-centric cultural biases. However, the first approach mitigates and/or outlines existing biases by identifying whether a dataset is culturally agnostic or not, while the other approach results in benchmarks with cultural biases regarding the intended languages and cultures by not considering English sources that could contain English-centric biases. \n\nThe integration of diverse and culturally grounded knowledge is necessary to achieve a certain level of inclusivity and fairness for multilingual evaluations (Singh et al., 2024). Mitigating cultural bias from benchmarks is fundamental for achieving a certain level of reliability for global benchmarks, but -as mentioned -culture shapes reasoning and is, therefore, an essential component in and for communication and for solving multilingual tasks.",
            "score": 0.49506449754752213,
            "section_title": "Cultural Sensitivity and Inclusion",
            "char_start_offset": 14363,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 889
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "277621694",
            "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
            "text": "We integrated 20+ existing multilingual benchmarks into a unified pipeline, by standardizing all ISO 639-3 language codes in the different benchmarks 1 , which covers nearly all known languages worldwide. By aligning benchmark language identifiers with ISO 639-3 codes, we enable evaluations for specific languages or 1 https://iso639-3.sil.org/about arXiv:2504.04155v1 [cs.CL] 5 Apr 2025 language groups (e.g., Bantu, Dravidian, or Uralic languages), allowing the framework to automatically search among integrated benchmarks to find matching test sets. This mapping also makes it easier to incorporate new large-scale benchmarks that target mid-or low-resource languages, ensuring flexibility for future expansions. \n\n2. Language-Specific Prompt Templates. \n\nUsers can configure prompts for each language individually, thereby enabling more precise assessments of a model's instruction-following ability across diverse linguistic settings. All templates are maintained in a centralized prompt library that supports multilingual benchmarks, allowing easy customization as needed. In this way, each task within a benchmark can be run potentially using prompts in the task's original language, rather than defaulting to English prompts. To simplify cross-lingual adaptation, we also implemented Microsoft Translator2 integration that automatically propagates user-defined prompt templates from one single language to 130+ supported languages. \n\n3. Non-English-Centered Machine Translation Evaluation. GlotEval is designed to break away from the traditional Englishcentric paradigm. Thanks to translation benchmarks featuring fully or partially multialigned datasets, GlotEval enables non-English-centered translation evaluations by allowing any supported language to serve as the pivot: users simply update the pivot language in the configuration, to assess \"any-to-pivot\" / \"pivot-to-any\" translation directions. This flexibility ensures that GlotEval breaks from the traditional \"English \u2194 other language\" paradigm and adapts seamlessly to diverse, potentially low-resource, language pairs.",
            "score": 0.49469684077845927,
            "section_title": "Consistent Multilingual Benchmarking.",
            "char_start_offset": 1968,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 717
                },
                {
                    "start": 720,
                    "end": 758
                },
                {
                    "start": 761,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1441
                },
                {
                    "start": 1444,
                    "end": 1499
                },
                {
                    "start": 1500,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2091
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0823974609375
        },
        {
            "corpus_id": "258823009",
            "title": "Comparing Biases and the Impact of Multilingual Training across Multiple Languages",
            "text": "Table 1: Predicted sentiment on a fine-tuned mBERT sentiment analysis model. Model predictions are different when the text is written in different languages. \n\nin cultures and training data. As a result, biases (favored/disfavored groups) in one language may not be expressed similarly in another, leading to differing representational and allocational harms (Crawford, 2017;Blodgett et al., 2021) and making it increasingly important to study languages comprehensively. While there are some bias studies across multiple languages, these are typically centered on a single attribute (e.g. ethnicity or gender) (Ahn and Oh, 2021;Kaneko et al., 2022). Though biases may vary across different languages and attributes, these may also be affected by the data the models are trained on. Previous studies have shown the impact of multilingual versus monolingual training data on a model's task performance (Rust et al., 2021;Groenwold et al., 2020a). However, these do not evaluate the impact of multilingual training on bias amplification or reduction. \n\nIn this paper, we present an analysis of four demographic attributes (race, religion, nationality, gender) across five languages: Italian, Chinese, English, Hebrew, and Spanish. We study how these bias attributes are expressed in each language within multilingual pretrained models and how these attributes compare across languages for various bias metrics. We focus our study on the sentiment analysis task. Specifically, our research questions are 1) How does task performance com-pare across languages on a parallel human-written test set?, 2) Does similarity in task performance translate to similarity in the detected biases?, and 3) Does multilingual data reduce/amplify biases? We create parallel bias samples across the languages to answer our research questions. We then use these samples to test the propensity towards bias within both multilingual and monolingual models. \n\nOur contributions are: \n\n\u2022 We study gender, race, nationality, and religion biases in multilingual models for the downstream sentiment analysis task across Italian, Chinese, English, Hebrew, and Spanish. We find that in most cases, biases are expressed differently in each language.",
            "score": 0.4944936725216535,
            "section_title": "Negative Positive",
            "char_start_offset": 757,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 157
                },
                {
                    "start": 160,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 649
                },
                {
                    "start": 650,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1047
                },
                {
                    "start": 1050,
                    "end": 1227
                },
                {
                    "start": 1228,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1458
                },
                {
                    "start": 1459,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1932
                },
                {
                    "start": 1935,
                    "end": 1957
                },
                {
                    "start": 1960,
                    "end": 2138
                },
                {
                    "start": 2139,
                    "end": 2217
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 397,
                    "matchedPaperCorpusId": "236460302"
                },
                {
                    "start": 610,
                    "end": 628,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "248496913"
                },
                {
                    "start": 900,
                    "end": 919,
                    "matchedPaperCorpusId": "229924220"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "269457293",
            "title": "Exploring Multi-Lingual Bias of Large Code Models in Code Generation",
            "text": "In RQ3, we opt to focus on the base model versions of the LCMs because the specifics of their instruction tuning phase, including the data and training strategies employed, are not accessible to us.Using base models allows us to concentrate solely on understanding how the choice of data and training methods used for instruction tuning influences the model performance.\n\n1) RQ3.1: Multi-NL Understanding Bias: We conduct instruction tuning on the selected LCMs through the proposed four training methods in Section II and the results are shown in Table V.\n\nFor English-based Tuning, we can observe that instruction tuning with pure English training instances can notably improve the code generation performance with both English and Chinese instructions.Specifically, compared to the original base models, instruction tuning by English data achieves an average of 28.2% and 34.5% improvement on the Pass@1 metric with English and Chinese instructions, respectively.The results reveal inter-dependencies within the knowledge representations of various natural languages embedded in LCMs.The inter-dependencies render that instruction tuning LCMs in one natural language can concurrently enhance their performance across different languages.However, despite the noticeable performance improvement, an obvious disparity persists between the models' treatment of English and Chinese instructions.After English-based tuning, the average Pass@1 rate of Chinese instructions is 11.75% lower than that of English, implying that relying solely on English data cannot well address the underlying bias towards non-English languages.\n\nFinding 3: Instruction tuning LCMs with English dataset can bring substantial performance improvement on code generation in both English and Chinese instructions, i.e., the Pass@1 rate improves 28.2% and 34.5%, respectively.However, the bias between English and Chinese benchmarks is still severe, indicated by the obvious performance gap at 11.75%.\n\nFor Chinese-based Tuning, similar to purely English tuning, we also observe a considerable improvement in the performance of the LCMs, i.e., the Pass@1 rates obtain an average relative improvement of 26.6% and 44.3% in English and Chinese instructions, respectively.This observation aligns well with the English-based tuning.",
            "score": 0.49365510737029583,
            "section_title": "C. RQ3: Mitigating Multi-Lingual Bias in Instruction Tuning",
            "char_start_offset": 23980,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 370
                },
                {
                    "start": 372,
                    "end": 556
                },
                {
                    "start": 558,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 966
                },
                {
                    "start": 966,
                    "end": 1087
                },
                {
                    "start": 1087,
                    "end": 1240
                },
                {
                    "start": 1240,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1622
                },
                {
                    "start": 1624,
                    "end": 1848
                },
                {
                    "start": 1848,
                    "end": 1973
                },
                {
                    "start": 1975,
                    "end": 2241
                },
                {
                    "start": 2241,
                    "end": 2300
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2548828125
        },
        {
            "corpus_id": "272832060",
            "title": "Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino",
            "text": "The rapid development of Large Language Models (LLMs) has significantly reshaped the Natural Language Processing (NLP) landscape, showcasing abilities in generation, comprehension, and reasoning (Touvron et al., 2023;OpenAI et al., 2024). These models, pretrained on massive multilingual corpora, exhibit proficiency across a multitude of languages (Gemma Team et al., 2024;Zhang et al., 2024). Despite these technological strides, the majority of models are predominantly tailored to high-resource languages, particularly English, leading to intrinsic linguistic and cultural biases that marginalize lower-resource languages and cultures (Ahuja et al., 2023;Atari et al., 2023;Lai et al., 2023). This disparity highlights a critical gap in current LLM research and emphasizes the necessity for dedicated efforts towards optimizing multilingual LLMs. Achieving culturally nuanced and contextually accurate responses in such languages remains an unresolved challenge, necessitating inclusive strategies that bridge this existing linguistic and cultural divide. \n\nMultilingual evaluation datasets for under-resourced and under-represented languages have been developed through adapting open-source English-language datasets by means of automatic or manual translation (Conneau et al., 2018;Ponti et al., 2020;Doddapaneni et al., 2023;Nguyen et al., 2024), inadvertently introducing English biases to such evaluations. Models exhibiting such biases may cause certain groups of users to distrust such systems (Luan and Cho, 2024), lowering their adoption and overall accessibility in some societies. Thus, there is a need for evaluations that can determine if LLMs are not just usable and safe, but also culturally helpful and harmless to the societies and regions they are deployed in. \n\nTo bridge this gap, we present KALAHI,1 a high-quality, manually-crafted cultural dataset that is part of SEA-HELM2 and designed to determine LLMs' abilities to provide relevant responses to culturally-specific situations that Filipinos face in their day-to-day lives.",
            "score": 0.49345781666081684,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 696
                },
                {
                    "start": 697,
                    "end": 850
                },
                {
                    "start": 851,
                    "end": 1059
                },
                {
                    "start": 1062,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1782
                },
                {
                    "start": 1785,
                    "end": 2053
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1534423828125
        },
        {
            "corpus_id": "273695372",
            "title": "Linguistics Theory Meets LLM: Code-Switched Text Generation via Equivalence Constrained Large Language Models",
            "text": "An unexpected finding was the directional asymmetry in model performance between different translation directions. Specifically, translations from Indic languages to English exhibited significantly higher fluency and accuracy than translations in the reverse direction. This asymmetry may be attributed to the training data distribution, where code-switching from Indic languages to English is more prevalent in real-world usage, making the model more familiar with this direction. Additionally, all annotators being native speakers of Indic languages and L2 learners of English might introduce bias. Their proficiency in their native language allows for more rigorous scrutiny of translations into Indic languages, potentially leading to stricter evaluations of fluency and accuracy. Conversely, translations from Indic languages to English might be rated more leniently, as English is a second language for the annotators, making them less critical of subtle errors. This directional asymmetry could also reflect real-world code-switching patterns that favor English as the matrix language, with Indic languages filling lexical gaps. Future research could explore how this asymmetry relates to linguistic theories, such as the Matrix Language Frame model, and examine the impact of annotators' bilingual proficiency on these evaluations.",
            "score": 0.49275946466783355,
            "section_title": "Directional Asymmetry",
            "char_start_offset": 19289,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 269
                },
                {
                    "start": 270,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1339
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.15625
        },
        {
            "corpus_id": "269214448",
            "title": "Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment",
            "text": ") more than artifacts in a different language in the source-language RM. Suppose, for example, that the target-language RM assigns higher rewards when the generation contains certain targetlanguage words (due to bias in the RM training data). A different-language policy model is unlikely to exploit this, as it rarely generates these words, but a same-language policy model may. This hypothesis is consistent with our observed patterns. First, there are many fewer cases of cross-lingual reward optimization outperforming the monolingual setting when measured by the finetuned target-language RM evaluator than the prompted LM evaluators (Figure 3): under this hypothesis, the finetuned evaluator RMs would be more susceptible to such artifacts and (incorrectly) assign higher scores in the monolingual settings. The underperformance of the translate-train baseline ( \u00a75.1) also provides weak evidence: in princi-Figure 4: Alignment effectiveness, compared to the target-language SFT model judged by PaLM-2-L, and the 95% confidence interval across validation instances. \"source\u2192target\" denotes a source-language RM driving alignment in the target language. Cross-lingual alignment is generally effective, sometimes outperforming monolingual alignment. RL is hard to train for OpenAssistant, in line with what its authors found (K\u00f6pf et al., 2023). \n\nple, a source-language RM and a source-translatedinto-target-language RM should capture the same reward signal, as they are derived from the same data source, and would lead to similar downstream performance. However, the former is less susceptible to reward over-optimization due to the language mismatch, leading to better performance, though this is confounded by translation quality. \n\nCorroborating this hypothesis, we also find that when used monolingually, the RMs behave more like a bag-of-word (BoW) model. We take each of the 6 summarization RMs and infer on the validation set of each dataset in each language (Table 1).",
            "score": 0.49254737490973843,
            "section_title": "2023",
            "char_start_offset": 12734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1867
                },
                {
                    "start": 1868,
                    "end": 1983
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07586669921875
        },
        {
            "corpus_id": "277621694",
            "title": "GlotEval: A Test Suite for Massively Multilingual Evaluation of Large Language Models",
            "text": "Large language models (LLMs) are advancing at an unprecedented pace globally, with regions increasingly adopting these models for applications in their primary language. Evaluation of these models in diverse linguistic environments, especially in low-resource languages, has become a major challenge for academia and industry. Existing evaluation frameworks are disproportionately focused on English and a handful of high-resource languages, thereby overlooking the realistic performance of LLMs in multilingual and lower-resource scenarios. To address this gap, we introduce GlotEval, a lightweight framework designed for massively multilingual evaluation. Supporting seven key tasks (machine translation, text classification, summarization, open-ended generation, reading comprehension, sequence labeling, and intrinsic evaluation), spanning over dozens to hundreds of languages, GlotEval highlights consistent multilingual benchmarking, language-specific prompt templates, and non-English-centric machine translation. This enables a precise diagnosis of model strengths and weaknesses in diverse linguistic contexts. A multilingual translation case study demonstrates GlotEval's applicability for multilingual and language-specific evaluations.",
            "score": 0.49230136771447286,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11395263671875
        },
        {
            "corpus_id": "277993848",
            "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks",
            "text": "The domains of the dataset (e.g., news, social media, etc.). \n\nTable 1 | Annotation scheme for the collected paper. \n\nunderstanding how well these models perform in multilingual contexts, especially given the linguistic diversity and varying resource availability across languages. Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems [Shi et al., 2023, Lai et al., 2023, Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by Hendrycks et al. [2021], several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users [Koto et al., 2023, Li et al., 2024, Y\u00fcksel et al., 2024]. In addition to these methods, recent studies have emphasized the inclusion of culturally specific content in their evaluations. For instance, Chiu et al. [2024] introduced CulturalBench, which assesses LLMs' cultural knowledge across 45 global regions. Similarly, Mohamed et al. [2024] presented ArtELingo-28, a vision-language benchmark spanning 28 languages, with a focus on capturing diverse opinions across languages and cultures. Another notable example is CVQA, introduced by Romero et al. [2024], which includes culturally driven images and questions from 30 countries across four continents, covering 31 languages and 13 scripts, with a total of 10K questions. \n\nOurs In this position paper, we conduct a comprehensive investigation of over 2,000 studies on multilingual evaluation published between 2021 and 2024, following the emergence of large language models (LLMs). We summarize recent trends in the field of multilingual evaluation and analyze whether widely used multilingual benchmarks accurately reflect human judgments. Additionally, we discuss future directions for multilingual evaluation based on our findings. To the best of our knowledge, the work of Yu et al. [2022], which surveys 156 multilingual evaluation studies spanning from 2008 to 2021, is the most closely related to our research.",
            "score": 0.4918712996816039,
            "section_title": "Domain",
            "char_start_offset": 9088,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 63,
                    "end": 115
                },
                {
                    "start": 118,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 360
                },
                {
                    "start": 361,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1581
                },
                {
                    "start": 1584,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 528,
                    "matchedPaperCorpusId": "252735112"
                },
                {
                    "start": 528,
                    "end": 546,
                    "matchedPaperCorpusId": "260334562"
                },
                {
                    "start": 690,
                    "end": 713,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 854,
                    "end": 872,
                    "matchedPaperCorpusId": "263829211"
                },
                {
                    "start": 889,
                    "end": 911,
                    "matchedPaperCorpusId": "271245129"
                },
                {
                    "start": 1177,
                    "end": 1198,
                    "matchedPaperCorpusId": "273850461"
                },
                {
                    "start": 1395,
                    "end": 1415,
                    "matchedPaperCorpusId": "270371898"
                },
                {
                    "start": 2088,
                    "end": 2104,
                    "matchedPaperCorpusId": "254043519"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.235107421875
        },
        {
            "corpus_id": "261682140",
            "title": "SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning",
            "text": "Motivated by the preceding discussion regarding the desired model characteristics, we introduce the targeted aspects for benchmarks: Monolingual and Cross-lingual Capabilities The focus on monolingual tasks ensures the model's proficiency in comprehending and generating text within a single language. The cross-lingual tasks, such as machine translation and code-switch comprehension, can access the communication capabilities across different languages, reflecting a comprehensive understanding of multilingual contexts. In terms of evaluation aspects, both fundamental NLP capabilities and complex reasoning capabilities should be examined under monolingual and cross-lingual settings. \n\nKnowledge Transfer Ability Language-related knowledge can be categorized into: 1) cultural knowledge and local norms tied to language and 2) common (universal) knowledge. Cultural knowledge refers to language-related information that is specific to a particular culture, community, or region. It includes the nuances, customs, and norms associated with language use within a specific cultural context. Common knowledge is widely applicable across languages and communities, encompassing factual, scientific, and real-world knowledge, etc (Hendrycks et al., 2021a). When designing evaluation benchmarks, it is essential to include a diverse set of language-related cultural tasks while also evaluating how effectively the universal knowledge is shared across different languages. Since such datasets are not readily accessible, this evaluation aspect is severely constrained in existing benchmarks. \n\nRobustness and Stability The robust context modeling and stable output generation are important to ensure LLMs work as intended (functionality) when applied to real-world applications (reliability) (Haduong et al., 2023). When built on the auto-regressive framework, language models are originally trained to predict the next token given a sequence of previous ones, and their in-context learning and zero-shot inference performance depends on the prompts they receive (Ouyang et al., 2022). Consequently, minor variations of the input can possibly lead to distinct outputs with unpredictable formats. In particular, since FMs do not attain \"balanced multilingual\" capabilities, they are more sensitive to input variations such as multilingual and code-switch under real-world scenarios. \n\nTherefore, recognizing the models' instruction sensitivity should be a crucial aspect of the evaluation framework.",
            "score": 0.4907078878746284,
            "section_title": "Multilingual Benchmarks",
            "char_start_offset": 6766,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 688
                },
                {
                    "start": 691,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 983
                },
                {
                    "start": 984,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1588
                },
                {
                    "start": 1591,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2082
                },
                {
                    "start": 2083,
                    "end": 2192
                },
                {
                    "start": 2193,
                    "end": 2378
                },
                {
                    "start": 2381,
                    "end": 2495
                }
            ],
            "ref_mentions": [
                {
                    "start": 1229,
                    "end": 1254,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 1789,
                    "end": 1811,
                    "matchedPaperCorpusId": "259859150"
                },
                {
                    "start": 2060,
                    "end": 2081,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1290283203125
        },
        {
            "corpus_id": "273508009",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "text": "Accelerating research on Large Multimodal Models (LMMs) in non-English languages is crucial for enhancing user experiences across broader populations. In this paper, we introduce JMMMU (Japanese MMMU), the first large-scale Japanese benchmark designed to evaluate LMMs on expert-level tasks based on the Japanese cultural context. To facilitate comprehensive culture-aware evaluation, JMMMU features two complementary subsets: (i) culture-agnostic (CA) subset, where the culture-independent subjects (e.g., Math) are selected and translated into Japanese, enabling one-to-one comparison with its English counterpart MMMU; and (ii) culture-specific (CS) subset, comprising newly crafted subjects that reflect Japanese cultural context. Using the CA subset, we observe performance drop in many LMMs when evaluated in Japanese, which is purely attributable to language variation. Using the CS subset, we reveal their inadequate Japanese cultural understanding. Further, by combining both subsets, we identify that some LMMs perform well on the CA subset but not on the CS subset, exposing a shallow understanding of the Japanese language that lacks depth in cultural understanding. We hope this work will not only help advance LMM performance in Japanese but also serve as a guideline to create high-standard, culturally diverse benchmarks for multilingual LMM development. The project page is https://mmmu-japanese-benchmark.github.io/JMMMU/.",
            "score": 0.4904871673453838,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.339599609375
        },
        {
            "corpus_id": "273098796",
            "title": "Evaluating Cultural Awareness of LLMs for Yoruba, Malayalam, and English",
            "text": "The discrepancies between the cultural dimension scores predicted by the models and the ground truth for Yoruba and Malayalam suggest that underlying model biases, particularly those related to the training data, play a significant role. Large language models are predominantly trained on English-centric data, with minimal representation of regional languages like Yoruba and Malayalam. The Common Crawl data 1 shows that English dominates LLM training data (45%), while Yoruba and Malayalam account for only 0.008% and 0.0226%, respectively. This stark imbalance in the training datasets contributes to the models' over-or underestimation of cultural dimensions for these languages. These biases arise because models are exposed to a disproportionate amount of content from English-speaking cultures. This leads to internalizing cultural norms that may not align with those in underrepresented regions. For instance, the overestimation of Power Distance (PDI) in Malayalam (ground truth: 0.21, GPT-4o-mini: 0.54) likely stems from hierarchical assumptions in English-speaking cultures less relevant in Malayalam-speaking contexts. Similarly, the underestimation of Long-Term Orientation (LTO) for Yoruba (ground truth: 0.70, GPT-4o-mini: 0.05) indicates the models' limited understanding of future-oriented cultural values in West African societies. \n\nThe differences between non-English languages and English present challenges for language models. Yoruba and Malayalam have unique syntactic and grammatical structures that differ from English, which can lead to misunderstandings. The lack of cultural contexts in training data exacerbates these challenges. We need a more balanced dataset that includes regional languages to address this. The imbalance in training data impacts the models' linguistic abilities and hinders their adaptability to different cultures. This restricts their usefulness in multilingual and multicultural settings. Models perform better in English, indicating the need for improved performance in languages like Yoruba and Malayalam. This requires larger, diverse datasets and adaptable model architectures to accommodate linguistic and cultural diversity better.",
            "score": 0.49047429185785135,
            "section_title": "Discussion of Model Biases and Performance",
            "char_start_offset": 28127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1584
                },
                {
                    "start": 1585,
                    "end": 1661
                },
                {
                    "start": 1662,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1945
                },
                {
                    "start": 1946,
                    "end": 2064
                },
                {
                    "start": 2065,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2208251953125
        },
        {
            "corpus_id": "29336183",
            "title": "Further Investigation into Reference Bias in Monolingual Evaluation of Machine Translation",
            "text": "Fomicheva and Specia (2016) provide an investigation into reference bias in monolingual evaluation of MT. 100 Chinese to English MT output translations are assessed by 25 human judges on a five-point scale, in the form of their response (None, Little, Much, Most, or All) to the following question: how much of the meaning of the human translation is also expressed in the machine translation?. Precisely the same 100 translations were assessed by all 25 judges. Human judges were divided into five groups of five: Group 1 (G 1 ) was shown the source language input and the MT output only and carried out a bilingual assessment, while Groups 2-5 (G 2 -G 5 ) were not shown the source input but instead compared the MT output to a human-generated reference translation. A distinct set of reference translations was assigned to each group G 2 -G 5 . Inter-annotator agreement (IAA) was measured for pairs of judges as follows (the total number of judge pairs resulting from each setting is provided in parentheses): \n\n\u2022 SOURCE: a given pair of judges assessed translations in a bilingual setting (all possible pairs within G 1 = 5 2 = 10 pairs);  1, where a small increase of 0.034 in average Kappa is shown for pairs of human assessors in SAME over that of DIFF. To avoid drawing conclusions from a difference that is likely to have occurred simply by chance, confidence intervals (CIs) are provided and the non-overlapping CIs for SAME and DIFF shown in Table 1 provide the basis for the conclusion that IAA is significantly higher for SAME compared to DIFF and subsequently that monolingual evaluation of MT is strongly biased by the reference translation. On examination of the analysis that led to the conclusion of strong reference bias, we unfortunately discover a series of methodological issues with respect to confidence interval estimation, however, that raise doubt about the reliability of this conclusion. 2  clear indication of the precise approach to CI estimation attempted in Fomicheva and Specia (2016) is unfortunately not explicitly stated but out of the range of methods that exist the approach that is applied most resembles bootstrap resampling.",
            "score": 0.4904350571336838,
            "section_title": "Background",
            "char_start_offset": 5619,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 847
                },
                {
                    "start": 848,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1657
                },
                {
                    "start": 1658,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10540771484375
        },
        {
            "corpus_id": "276782545",
            "title": "English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance",
            "text": "Our overall findings indicate that English quantization is more performant than language-dedicated quantized models. With the starting hypothesis that quanting based on a non-English language significantly alters the performance on a benchmark, no p value is calculated to be lower than 0.2373 for the multiple choice dataset-a clear failure to disprove the null hypothesis. p values were calculated by Monte Carlo simulation with the assumption that the probability found by the English version of each quant tier models the underlying binomial distribution of the success chance for each question. \n\nOne possible flaw in our method has been the fact that our importance matrices are machine-translated from English via LLMs, this could potentially bias the importance matrices towards language that appears 'close to English' in the estimation of an LLM. However, since a different, significantly larger LLM was used to translate the importance matrices, this effect should be minimal. \n\nThe drop in performance on the Norwegian dataset may be partially explained by errors in translation. Directly translating questions into another language will occasionally lead to an unintended increase in difficulty. Such as if the original language provided some cultural context or accidental inclusion of homonyms in the target language. As an example, in Figure 4, the fact that the question is written in English is a vital clue towards understanding what is being asked. 67 . \n\nIt seems unlikely to us that the pipeline as mapped out by Zhao et al. in Figure 2 explains the entirety of multilingual behavior within LLMs. Firstly, because machine learning at this scale rarely results in such tidy behavior, and secondly, if this were the case, its ability to respond to multilingual queries would be equal to its ability to translate the given language. Qualitatively this is not the case, translation is a significantly simpler task than instruction following, in our experience. However, our  results do not contradict their hypothesis as performance on both Norwegian and Malayalam tasks appear heavily connected to performance in English. A graph of the results for the different quantization methods on the multiple choice questions is shown in Figure 5. There is a slight, but perceptible decline for each successive quant, though this loss is remarkably low when compared to their compression rate, as seen in Table 1.",
            "score": 0.4903692992250069,
            "section_title": "Results & Discussion",
            "char_start_offset": 9819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 374
                },
                {
                    "start": 375,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 987
                },
                {
                    "start": 990,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1471
                },
                {
                    "start": 1472,
                    "end": 1473
                },
                {
                    "start": 1476,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1851
                },
                {
                    "start": 1852,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2140
                },
                {
                    "start": 2141,
                    "end": 2257
                },
                {
                    "start": 2258,
                    "end": 2423
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09027099609375
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "To acquire knowledge from contamination of the evaluation data, we overfit open-sourced LLMs on the translated versions of the benchmark test sets, instead of directly memorizing the original form of text. The process of constructing the training data for contamination is illustrated in Figure 3. \n\nWe first translate the benchmark test sets into non-English languages mentioned in \u00a7 2.2. Considering the cost and quality balance, we utilize Table 1: Performance (%) of original clean models and models with vanilla and cross-lingual contamination, respectively. Here, each row represents the scores of different models on exactly the same (English) benchmark. 'Vanilla' indicates the model is contaminated directly on the English version of the benchmark, and the 'Cross-Lingual Contaminated' columns show the scores of models contaminated in a specific non-English language. \n\nLLaMA3-8B to conduct the translation. The specific prompt template is shown in appendix A.2. Then, we customize the questions and choices to fit in the corresponding prompt templates used for the evaluation of specific benchmarks. In this way, we construct the corpus for continual pre-training of the backbone models through the causal language modeling objective, which stimulates the real-world scenario where specific data contamination is blended into the training corpus. The vanilla contamination is injected in the same way using the original English benchmarks. The training hyperparameters are provided in Table 5. \n\nWe inject the contamination for different benchmarks separately, ensuring that each model only contains contamination of one specific benchmark in a single language. Mixing different benchmarks and languages is another way to inject cross-lingual contamination, which we leave for future work.",
            "score": 0.4898665848097534,
            "section_title": "Cross-Lingual Contamination",
            "char_start_offset": 8948,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 297
                },
                {
                    "start": 300,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 563
                },
                {
                    "start": 564,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 917
                },
                {
                    "start": 918,
                    "end": 972
                },
                {
                    "start": 973,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1504
                },
                {
                    "start": 1507,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1800
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.283203125
        },
        {
            "corpus_id": "271244987",
            "title": "MASIVE: Open-Ended Affective State Identification in English and Spanish",
            "text": "Finally, we evaluate translate-train and translatetest approaches to cross-lingual transfer (Hu et al., 2020) against models fine-tuned and evaluated on the original texts. First, we find an expected drop in performance when models are fine-tuned on machine-translated data for both English and Spanish. Interestingly, the average drop in similarity metrics in Spanish (36%) is notably larger than in English (27%). This could perhaps be explained by the translation model performing better in the Spanish to English direction than English to Spanish, as well as mT5's ability to better generalize in English than in Spanish. \n\nAs an alternative approach to fine-tuning on translated data, we also consider the case where data may be translated at inference time. In these cases (En T r and Es T r in Table 8), we find that performance falls. Artifacts of machine translation have been found to impact evaluation of translation models (Freitag et al., 2020), and similarly, errors and artifacts of unnatural translation may cause these changes in performance. In contrast to prior work suggesting that performance on the target data translated into English is comparable to fine-tuning on the target language for tasks such as sentiment detection, our results suggest that for our task, machine-translating the evaluation data leads to poorer performance, and translating either at training or inference time result in similar performance (Takeaway #6).",
            "score": 0.4895173948814851,
            "section_title": "Machine-Translation vs. Natural Data",
            "char_start_offset": 22630,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 625
                },
                {
                    "start": 628,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1453
                }
            ],
            "ref_mentions": [
                {
                    "start": 92,
                    "end": 109,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 935,
                    "end": 957,
                    "matchedPaperCorpusId": "215744964"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275390625
        },
        {
            "corpus_id": "273323302",
            "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving",
            "text": "Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs. \n\nE GENERALIZING TO EXTREME SCENARIOS E.1 PERFORMANCE ON WEAKER MODEL: LLAMA2 \n\nTable 13 demonstrates that even when starting with a model with weaker multilingual capabilities, such as Llama2-7B-Chat, which exhibits extremely low performance in languages like Russian (ru), German (de), and French (fr) on the X-AlpacaEval, significant improvements can be achieved. By leveraging language imbalance-driven rewarding for self-multilingual optimization across two iterations, the model shows substantial enhancement across all training languages, particularly in those where the original model's performance was initially weaker. To assess the impact of our approach on these languages, we conducted experiments using Llama3-8b-Instruct as the base model. Table 14 shows that even though the model performs weakly in these languages, our approach remains effective in low-resource settings and can iteratively improve the model's performance across all languages.",
            "score": 0.4891760203132385,
            "section_title": "D.3 HOW TO AVOID TRANSLATIONESE BIAS IN MULTILINGUAL BENCHMARKS EVALUATION",
            "char_start_offset": 34821,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 359
                },
                {
                    "start": 360,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 856
                },
                {
                    "start": 859,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1179
                },
                {
                    "start": 1182,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 1101,
                    "end": 1127,
                    "matchedPaperCorpusId": "271244246"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.374755859375
        },
        {
            "corpus_id": "265445838",
            "title": "Cultural bias and cultural alignment of large language models",
            "text": "This study contributes comprehensive, longitudinal, and theoretically-grounded evidence from a disaggregated evaluation of cultural bias for five of the most widely used LLMs to date. Following in the tradition of seminal work by Bolukbasi and colleagues [7] who examined bias encoded in language models by calculating the semantic proximity of concepts, we examine cultural bias in the outputs of LLMs by calculating their cultural distance from a standard social science benchmark-the IVS and corresponding Inglehart-Welzel cultural map. Just as the proximity of \"man\" to \"computer programmer\" and \"woman\" to \"homemaker\" provided initial evidence of gender bias in language models [7], the proximity of the responses from five popular LLMs to the cultural values of Western countries provides evidence of cultural bias. \n\nWe show that the distances between the cultural expression of LLMs and the local cultural values of different countries are unequal, suggesting cultural bias in LLMs that favors the values of English-speaking and Protestant European countries. This finding raises critical concerns about cultural misrepresentations and bias in current applications of LLMs, but further research is needed to determine how this bias may impact natural human-AI interactions in the real world. Our findings are consistent with another disaggregated evaluation comparing GPT's cultural expressions to WVS results [4]. Despite differences in data processing and scope, both evaluations indicate a consistent pattern: the output of GPT models tends to resemble Western cultures when prompted without a specific cultural identity. Our results underscore that this pattern is robust across different versions of GPT models and taking different prompt wordings into account. \n\nConsidering GPT's rapid adoption in countries around the world, this cultural bias may affect people's authentic expressions in several aspects of their lives. GPT's observed bias toward self-expression values may cause people to inadvertently convey more interpersonal trust, bipartisanship, and support for gender equity in GPT-assisted communication, such as emails, social media posts, and instant messaging. This may have interpersonal and professional consequences by signaling a lack of cultural embeddedness within an organizational context or misrepresenting the user to their readers [19].",
            "score": 0.48864739310344285,
            "section_title": "Discussion",
            "char_start_offset": 14505,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 821
                },
                {
                    "start": 824,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1774
                },
                {
                    "start": 1777,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2189
                },
                {
                    "start": 2190,
                    "end": 2376
                }
            ],
            "ref_mentions": [
                {
                    "start": 255,
                    "end": 258,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 683,
                    "end": 686,
                    "matchedPaperCorpusId": "1704893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06927490234375
        },
        {
            "corpus_id": "269758068",
            "title": "SUTRA: Scalable Multilingual Language Model Architecture",
            "text": "The SUTRA model demonstrates a notable consistency in linguistic performance across a variety of languages, as evidenced by the MMLU benchmark results.It exhibits a minimal performance deviation from its English language results to other languages such as Hindi, Gujarati, and Arabic, highlighting its robust multilingual capabilities critical for applications on a global scale.\n\nSuperior concept and language modeling underpin the SUTRA model's ability to maintain performance levels across different languages, distinguishing it from other leading models, including GPT-4, GPT-3.5, and Llama2.Many existing model architectures (including purpose built multilanguage models) experience a pronounced decline in performance in non-English languages, often regressing to baseline random chance performance, as detailed in Table 5.Note that random chance performance is at 25% on the MMLU benchmark.In contrast, SUTRA consistently achieves stable scores across languages, setting it apart, particularly in languages that are less commonly represented in language models, such as Hindi, Gujarati, Tamil, and Korean.The SUTRA model, therefore, not only excels in individual language performance but also promotes a more universal, language-agnostic approach to AI.It serves as a robust solution for international businesses, educational platforms, and cross-cultural communication, setting a new benchmark for LLMs in a multi-lingual, interconnected world.",
            "score": 0.48796704474851127,
            "section_title": "Consistent Performance across Languages",
            "char_start_offset": 22407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 379
                },
                {
                    "start": 381,
                    "end": 596
                },
                {
                    "start": 596,
                    "end": 829
                },
                {
                    "start": 829,
                    "end": 897
                },
                {
                    "start": 897,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1260
                },
                {
                    "start": 1260,
                    "end": 1452
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284881591796875
        },
        {
            "corpus_id": "273901572",
            "title": "Re-Evaluating Evaluation for Multilingual Summarization",
            "text": "This work aims to investigate the current standard evaluation summarization protocol when applied to non-English languages and advocate for assuming multiple target languages for the future design of evaluation metrics. As noted in our limitation section, the lack of transparency regarding the data provenance of many of the models we use prevents us from having full knowledge of how the builders of these models obtained their training data. During the annotation collection, we record detailed demographic data relevant to annotators' language proficiency and offer equal hourly rates to annotators regardless of their countries of residence. We also carefully select culturally relevant documents and avoid articles from translation. Given that model summaries could contain unverified content when releasing the data, we provide detailed documentation to differentiate human and model-generated summaries as well as the specificity of the models used.",
            "score": 0.4878872251087514,
            "section_title": "Ethics Statement",
            "char_start_offset": 18713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 444
                },
                {
                    "start": 445,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 957
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1707763671875
        },
        {
            "corpus_id": "274822728",
            "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation",
            "text": "Cross-lingual Bias and Toxicity Mitigation Liang et al. (2020) and Zhao et al. (2020) both investigated the cross-lingual transfer of gender bias mitigation of a single debiasing technique for contextualized word embeddings. Lauscher et al. (2021) introduced ADELE, a method that trains adapter modules for debiasing, which they show transfers to six non-English languages. Reusens et al. (2023) and Vashishtha et al. (2023) compared different techniques for debiasing masked language models, and find promising results for their crosslingual transfer. Closer to our work, Levy et al. (2023) studied the effects of monolingual and multilingual finetuning on model bias, but in relation to the downstream task of sentiment analysis. In comparison, we finetune the model for debiasing in English, and evaluate the model's stereotypical behavior in other languages. Unlike prior work in debiasing, we investigate not only whether crosslingual debiasing takes place, but also its effects on the resulting model's performance in the evaluation language, and which language features are important for successful cross-lingual debiasing, while contrasting it to detoxification. \n\nPrevious work on multilingual toxicity found that models trained on synthetic English preferencetuning data are less toxic than those trained on human English preference-tuning data when evaluated in English, but the opposite is true when evaluated in non-English languages (Jain et al., 2024). However, this preference-tuning data was not specifically aimed at addressing model toxicity. Dementieva et al. (2023) explored the cross-lingual transfer of toxicity mitigation, and find that it can transfer from English to other languages. \n\nRecent work by Li et al. (2024) showed that DPO for toxicity mitigation transfers from English to other languages by decreasing activations for a language-agnostic region in the model responsible for toxic generations. Similar to our findings, they found that this transfer comes at the expense of the diversity and fluency of the model's generations.",
            "score": 0.4874322174548552,
            "section_title": "Related Work",
            "char_start_offset": 4185,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1170
                },
                {
                    "start": 1173,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1709
                },
                {
                    "start": 1712,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2063
                }
            ],
            "ref_mentions": [
                {
                    "start": 43,
                    "end": 62,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 67,
                    "end": 85,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 225,
                    "end": 247,
                    "matchedPaperCorpusId": "237440429"
                },
                {
                    "start": 374,
                    "end": 395,
                    "matchedPaperCorpusId": "264146917"
                },
                {
                    "start": 400,
                    "end": 424,
                    "matchedPaperCorpusId": "259341896"
                },
                {
                    "start": 573,
                    "end": 591,
                    "matchedPaperCorpusId": "258823009"
                },
                {
                    "start": 1447,
                    "end": 1466,
                    "matchedPaperCorpusId": "269772971"
                },
                {
                    "start": 1562,
                    "end": 1586,
                    "matchedPaperCorpusId": "265445167"
                },
                {
                    "start": 1727,
                    "end": 1743,
                    "matchedPaperCorpusId": "262084217"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11474609375
        },
        {
            "corpus_id": "267722778",
            "title": "Machine translation: Turkish\u2013English bilingual speakers\u2019 accuracy detection of evidentiality and preference of MT",
            "text": "For instance, in a survey of 400 participants, the acceptability of machine-translated text from English to Lithuanian was influenced by factors like age and education. Less educated and older participants were more inclined to view machine translation as reliable and satisfactory (Kasper\u00e9 et al., 2021). However, when the situation is ethically charged (e.g., in legal settings) both translators and non-translators exhibit a negative bias toward MT (Asscher & Glikson, 2023). \n\nIn a study by Rossetti et al. (2020), 61 participants assessed the \"impact of machine translation and postediting awareness\" on comprehension and trust when reading crisis messages in English and Italian. The results showed no significant differences in comprehension and trust between raw machine-translated and post-edited text. However, participants with limited English proficiency had a more favorable view of raw machine-translated text in terms of comprehension and trust. Another study involving translation agencies, professional translators, and clients/users of professional translation investigated user awareness of machine translation through surveys (Garc\u00eda, 2010). The study focused on the acceptability and evaluation of machine translation from Chinese into English. The findings indicated that less than 5% of professional translators considered the quality of machine translation to be very high. Translation agencies shared a similar view with the translators. Among clients/users of professional translation (about 30%) who were aware of and requested machine translation, there was an intermediate or positive assessment of its quality. \n\nFor closely related languages in MT, a lexical analysis aided by translation rules often suffices, omitting the need for extensive semantic analysis. Rule sets for translation are more manageable in close language pairs compared to unrelated ones, simplifying the rule coding process (Altintas & Cicekli, 2022). NMT systems utilize typological similarities among languages to create clusters (Tan et al., 2019). Typologically distinct languages require more effort in NMT and might reveal fewer satisfying results. Thus, the current study, following an offline methodology, focused on a typologically distinct language pair, namely Turkish and English, and their non-professional bilingual speakers.",
            "score": 0.4874112175886127,
            "section_title": "Recent research on MT",
            "char_start_offset": 7602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 478
                },
                {
                    "start": 481,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 960
                },
                {
                    "start": 961,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1954
                },
                {
                    "start": 1955,
                    "end": 2054
                },
                {
                    "start": 2055,
                    "end": 2157
                },
                {
                    "start": 2158,
                    "end": 2342
                }
            ],
            "ref_mentions": [
                {
                    "start": 452,
                    "end": 477,
                    "matchedPaperCorpusId": "236249824"
                },
                {
                    "start": 495,
                    "end": 517,
                    "matchedPaperCorpusId": "221097388"
                },
                {
                    "start": 1146,
                    "end": 1160,
                    "matchedPaperCorpusId": "145406760"
                },
                {
                    "start": 1927,
                    "end": 1953,
                    "matchedPaperCorpusId": "6153281"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "270226041",
            "title": "LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback",
            "text": "Our goal is to simultaneously evaluate the understanding capability and the generating capability Table 3: Understanding and generating capability of LLMs.We evaluate on five benchmarks covers both on high-resource and low-resource languages.We utilize accuracy score to evaluate on PAWS-X and XCOPA.We evaluate Self-Instruct* and XL-Sum using ROUGE-1 score and FLORES using BLEU.\n\nof the LLMs.To evaluate the understanding capability of LLMs, we use Google Translate API to translate the instructions for each task into different languages.To evaluate the generating capability of an LLM, we use English as the instruction language during the inference phase.We choose English because LLMs demonstrate effective comprehension of English instructions, thus avoiding any potential impact on the generating capability of LLMs due to misunderstanding of instructions.Table 3 presents the average score for all languages in each benchmark.For more comprehensive results of each benchmark per language, please refer to Appendix D.\n\nThe off-the-shelf LLMs demonstrate limited multilingual capabilities, in particular in generation tasks, where performance is exceedingly poor.After fine-tuning the LLMs using the multilingual instruction dataset, there is a marginal (but not significant) improvement in the multilingual capability of the LLMs across most benchmarks, especially when describing instructions in non-English languages.For example, the understanding capability of the BX-based models (BX LLaMA and BX BLOOM ) drops significantly on the Self-Instruct*, XL-Sum, and FLORES benchmarks.This phenomenon is due to the fact that the BX-based models are finetuned in 52 languages and do not completely support all the languages in the benchmarks.\n\nLow-Resource vs High-Resource.We observe that the multilingual capabilities of the examined LLMs are significantly superior in high-resource languages to those in low-resource languages.It is a common problem for multilingual models that the scarcity of training corpora in low-resource languages makes it challenging to train a robust decoder, leading to the generation of incorrect outputs (Lai et al., 2023b).\n\nUnderstanding Capability vs Generating Capability.Our findings indicate that instructions written in English outperform instructions written in non-English languages, aligning with our initial expectations.",
            "score": 0.48724260619287346,
            "section_title": "Results",
            "char_start_offset": 23413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 541
                },
                {
                    "start": 541,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 935
                },
                {
                    "start": 935,
                    "end": 1025
                },
                {
                    "start": 1027,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1427
                },
                {
                    "start": 1427,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1746
                },
                {
                    "start": 1748,
                    "end": 1778
                },
                {
                    "start": 1778,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2160
                },
                {
                    "start": 2162,
                    "end": 2212
                },
                {
                    "start": 2212,
                    "end": 2368
                }
            ],
            "ref_mentions": [
                {
                    "start": 2140,
                    "end": 2159,
                    "matchedPaperCorpusId": "258832989"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.095703125
        },
        {
            "corpus_id": "269758132",
            "title": "Analyzing Language Bias Between French and English in Conventional Multilingual Sentiment Analysis Models",
            "text": "Language biases in sentiment analysis arise from the characteristics and cultural nuances of each language, posing challenges to the accuracy and fairness of these systems, especially when applied across different linguistic backgrounds.Such biases can significantly skew sentiment analysis results.\n\nAn example of language bias would be considering the English phrase 'That's sick!', which in slang means something is excellent [33].However, when directly translated into another language, it might be interpreted negatively as it loses its cultural context.This kind of misinterpretation by sentiment analysis models can lead to inaccuracies, particularly in multilingual applications.\n\nRecent research shows the prevalence of these biases.Levy et al. found that multilingual training might amplify biases, impacting sentiment analysis accuracy across various languages [26].\n\nAdditionally, Goldfarb-Tarrant et al. showed that cross-lingual transfer could import or worsen biases, such as gender or racial biases [27].\n\nThese findings highlight the need for careful consideration and adjustment of sentiment analysis models to ensure fairness and accuracy when deployed across diverse linguistic settings.",
            "score": 0.4871062805929761,
            "section_title": "Language Biases",
            "char_start_offset": 5829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 237,
                    "end": 299
                },
                {
                    "start": 301,
                    "end": 434
                },
                {
                    "start": 434,
                    "end": 559
                },
                {
                    "start": 559,
                    "end": 687
                },
                {
                    "start": 689,
                    "end": 742
                },
                {
                    "start": 742,
                    "end": 877
                },
                {
                    "start": 879,
                    "end": 1020
                },
                {
                    "start": 1022,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 429,
                    "end": 433,
                    "matchedPaperCorpusId": "6253412"
                },
                {
                    "start": 872,
                    "end": 876,
                    "matchedPaperCorpusId": "258823009"
                },
                {
                    "start": 1015,
                    "end": 1019,
                    "matchedPaperCorpusId": "258833370"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.228515625
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "Generative large language models (LLMs) have been shown to exhibit harmful biases and stereotypes. While safety fine-tuning typically takes place in English, if at all, these models are being used by speakers of many different languages. There is existing evidence that the performance of these models is inconsistent across languages and that they discriminate based on demographic factors of the user. Motivated by this, we investigate whether the social stereotypes exhibited by LLMs differ as a function of the language used to prompt them, while controlling for cultural differences and task accuracy. To this end, we present MBBQ (Multilingual Bias Benchmark for Question-answering), a carefully curated version of the English BBQ dataset extended to Dutch, Spanish, and Turkish, which measures stereotypes commonly held across these languages. We further complement MBBQ with a parallel control dataset to measure task performance on the question-answering task independently of bias. Our results based on several open-source and proprietary LLMs confirm that some non-English languages suffer from bias more than English, even when controlling for cultural shifts. Moreover, we observe significant cross-lingual differences in bias behaviour for all except the most accurate models. With the release of MBBQ, we hope to encourage further research on bias in multilingual settings. The dataset and code are available at https://github.com/Veranep/MBBQ.",
            "score": 0.4863075860560767,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251708984375
        },
        {
            "corpus_id": "269282746",
            "title": "NormAd: A Framework for Measuring the Cultural Adaptability of Large Language Models",
            "text": "In this work, we conduct evaluations only in English. While testing across multiple languages and linguistic variations is essential for robust benchmarking of LLMs, it is beyond the scope of this study. Prior work highlights that prompting in English -given current skewed data representations -helps models leverage knowledge more effectively and mitigates issues arising from varying linguistic capabilities and instruction-following skills (Shen et al., 2024). We encourage future work to investigate multilingual reasoning performance and its correlation with cultural adaptability across languages. \n\nDynamic Cultural Evaluation As a pragmatic way for approaching culture, much research, including our own through NORMAD-ETI, often treats the dynamic and multifaceted nature of culture as static variables during evaluation. This static approach may inadvertently perpetuate cultural stereotypes and fail to capture the continuously changing cultural nuances. To address these limitations, we suggest a modification to our evaluation framework, envisioned as future work, that would allow users to specify their own norms and values. Our framework, NORMAD, is designed to be flexible, which is crucial for accommodating the evolving nature of cultural contexts.",
            "score": 0.48609259733653076,
            "section_title": "Multilingualism and Linguistic Variations:",
            "char_start_offset": 28078,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 54,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 604
                },
                {
                    "start": 607,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1267
                }
            ],
            "ref_mentions": [
                {
                    "start": 444,
                    "end": 463,
                    "matchedPaperCorpusId": "269626686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1534423828125
        },
        {
            "corpus_id": "264491136",
            "title": "Global Voices, Local Biases: Socio-Cultural Prejudices across Languages",
            "text": "We introduce a multilingual culturally-relevant dataset for evaluating 11 dimensions of intrinsic bias in 24 languages using target and attribute pairs for the WEAT metric. Beyond previously-studied dimensions of social bias like gender, we propose new dimensions of human-centered contemporary biases like ableism and sexuality to find strong evidence of bias in language models. We show that bias does not uniformly manifest across languages, but monolingual models do reflect human biases more closely than multilingual ones across multiple methods of extracting contextualized word embeddings. We also find that human translations are better suited for bias studies than automated (machine) translation ones. Finally, our case study on Indian languages reveals that biases in resource-heavy languages like Hindi are not necessarily found in other languages. WEAT, however, is an inconsistent metric for measuring biases, as indicated by a limited number of statistically significant results. In the future, we aim to develop metrics better suited for measuring biases in contextualized embeddings and generative models and explore their effects in real-world downstream applications.",
            "score": 0.4849888308353581,
            "section_title": "Conclusion",
            "char_start_offset": 30697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1187
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.273681640625
        },
        {
            "corpus_id": "245328293",
            "title": "Coding Issues of Open-Ended Questions in a Cross-Cultural Context",
            "text": "The three approaches to coding answers to open-ended questions in cross-cultural survey research outlined above all have pros and cons, irrespective of procedural logistics or costs. Hardly any studies investigate the quality (reliability and validity) of the coding results across countries (inter-country reliability). Our focus in this paper is on the reliability aspect of coding quality. \n\nUsing answers to open-ended questions about the meaning of left and right from a cross-cultural survey fielded in English and Spanish, we investigate the impact of translation on coding results by comparing the codings of the translated answers to the codings of the same answers in the original languages. The central elements of our study design are summarized in Figure 1. The results of our study are relevant for cross-cultural comparative research because decisions increasingly have to be made when organizing the coding of multilingual answers. In such contexts, the right balance must be found between spending resources on translation and coding, on the one hand, and the quality of the resulting codings, on the other. \n\nOur research question is whether differences in coding outcomes can be detected between (a) the one-step approach, where answers to open-ended questions are coded in the original languages by bilingual coders according to a German-language categorization scheme (with different coders for different original languages), and (b) the two-step approach, where answers to open-ended questions are first translated from the original languages into German, and then coded by only one German coder according to the same central German-language categorization scheme. \n\nSeveral sources of bias and error might occur in the process between collecting the multilingual qualitative data (i.e., answers to open-ended questions) and conducting quantitative analyses of such cross-cultural data. These might result from the sometimes unclear content of the respondents' answers, from the translations, from coding, or from a combination of these error sources. Translators interpret texts while translating, and human coders interpret texts while coding.",
            "score": 0.48471427138173284,
            "section_title": "Research question",
            "char_start_offset": 10856,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 320
                },
                {
                    "start": 321,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1124
                },
                {
                    "start": 1127,
                    "end": 1686
                },
                {
                    "start": 1689,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2167
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "This work provides a comprehensive framework for evaluating Large Language Models (LLMs) across multiple European languages by utilizing translated versions of widely-used benchmarks. Our experiments demonstrate that translated benchmarks can serve as a reliable proxy for human preference in various languages. We also highlight the influence of translation services on the quality of the benchmarks. By releasing our benchmarks, we aim to foster further research and development in multilingual LLM evaluation, driving improvements in cross-lingual NLP applications. Future work will focus on refining cultural localization in translations and exploring deeper correlations between translation quality and model performance across diverse language families.",
            "score": 0.48437367827797073,
            "section_title": "Conclusion",
            "char_start_offset": 25464,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 759
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3916015625
        },
        {
            "corpus_id": "271710220",
            "title": "LocalValueBench: A Collaboratively Built and Extensible Benchmark for Evaluating Localized Value Alignment and Ethical Safety in Large Language Models",
            "text": "A.1 LLM Benchmarking \n\nBenchmarking LLMs has been a critical area of research, focusing on evaluating various aspects such as language understanding, generation capabilities, and overall performance across different tasks. LLM benchmarks have often been designed to measure LLMs' ability to handle diverse linguistic phenomena, thereby providing insights into their robustness and generalization capabilities [5,6]. The comprehensive assessment of LLMs' proficiency in language tasks achieved significant advancements in natural language understanding, enabling more accurate and contextually appropriate responses [7,8]. LLM benchmarking also addressed the scalability of LLMs, demonstrating their capacity to manage increased data volumes and computational resources effectively [9,10,11,12]. Efforts to develop multilingual benchmarks expanded the evaluation framework to include non-English languages, promoting inclusivity and diversity in language processing [13,14]. Comparative studies of LLMs across various benchmarks highlighted strengths and weaknesses, driving iterative improvements and optimization of model architectures [15,16]. Benchmarking initiatives revealed the importance of reproducibility and transparency, advocating for standardized evaluation protocols to ensure fair comparisons [17]. The integration of real-world application scenarios into benchmarking processes provided practical insights into LLM performance in deployment environments [6,18]. Ethical considerations in LLM benchmarking emphasized the need for evaluating LLMs' behavior concerning bias, fairness, and potential societal impacts [19,20]. The continuous evolution of LLM benchmarks reflected the dynamic nature of LLM technologies, fostering ongoing innovation and enhancement of LLM capabilities.",
            "score": 0.48326846495957954,
            "section_title": "A Background and Related Studies",
            "char_start_offset": 8739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 20
                },
                {
                    "start": 23,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 621
                },
                {
                    "start": 622,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 409,
                    "end": 412,
                    "matchedPaperCorpusId": "259096157"
                },
                {
                    "start": 412,
                    "end": 414,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 615,
                    "end": 618,
                    "matchedPaperCorpusId": "259108959"
                },
                {
                    "start": 618,
                    "end": 620,
                    "matchedPaperCorpusId": "253157310"
                },
                {
                    "start": 781,
                    "end": 784,
                    "matchedPaperCorpusId": "259375824"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 790,
                    "end": 793,
                    "matchedPaperCorpusId": "266191568"
                },
                {
                    "start": 965,
                    "end": 969,
                    "matchedPaperCorpusId": "222310559"
                },
                {
                    "start": 969,
                    "end": 972,
                    "matchedPaperCorpusId": "250144481"
                },
                {
                    "start": 1137,
                    "end": 1141,
                    "matchedPaperCorpusId": "258987272"
                },
                {
                    "start": 1141,
                    "end": 1144,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 1308,
                    "end": 1312,
                    "matchedPaperCorpusId": "248887631"
                },
                {
                    "start": 1470,
                    "end": 1473,
                    "matchedPaperCorpusId": "258331833"
                },
                {
                    "start": 1473,
                    "end": 1476,
                    "matchedPaperCorpusId": "263909384"
                },
                {
                    "start": 1629,
                    "end": 1633,
                    "matchedPaperCorpusId": "247626152"
                },
                {
                    "start": 1633,
                    "end": 1636,
                    "matchedPaperCorpusId": "259501579"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10968017578125
        },
        {
            "corpus_id": "271212597",
            "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs",
            "text": "Recent advancements in LLMs have revolutionized the landscape of artificial intelligence, significantly pushing the state-of-the-art for a broad array of Natural Language Processing (NLP) and Speech Processing tasks, such as machine translation (MT), Question Answering (QA), automatic speech recognition, among others. Their potential in language understanding and generation, across multiple (high-and low-resourced) languages, has attracted researchers to integrate and benchmark the LLM capabilities across diverse tasks, domains, and disciplines (OpenAI, 2023;Touvron et al., 2023). However, the rapid integration of LLMs necessitates measuring cultural discrepancies in the responses generated by LLMs to ensure alignment with users' cultural values and contexts (Naous et al., 2024;AlKhamissi et al., 2024;Shen et al., 2024;Liu et al., 2024;Arora et al., 2024;Myung et al., 2024). This is particularly crucial in cross-lingual scenarios, where LLMs hallucinate or produce stereotypical responses biased toward Western culture, neglecting diverse cultural norms (Naous et al., 2024) There are four natural lakes in Istanbul. In addition, there are also reservoir lakes. \n\nTable 1 2 \n\nFigure 1: Examples of questions and answers in different languages. \n\napplications for diverse languages and cultures, largely due to their under-representation in the training data used for these models. \n\nThere is a notable absence of such multilingual region-specific cultural benchmarks designed to evaluate the LLMs' performance across different cultures and languages. As a result, multilingual and non-English LLMs have been evaluated by using MT, with or without human involvement, to translate the existing English datasets into corresponding languages. For instance, to assess models like Jais (Sengupta et al., 2023) and AceGPT (Huang et al., 2024), evaluation datasets have been translated into Arabic. Other efforts include Korean MMLU (Son et al., 2024) and Okapi (Lai et al., 2023b) where the authors translated three benchmark datasets in 26 languages.",
            "score": 0.48273198624197966,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1187
                },
                {
                    "start": 1190,
                    "end": 1257
                },
                {
                    "start": 1260,
                    "end": 1394
                },
                {
                    "start": 1397,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2058
                }
            ],
            "ref_mentions": [
                {
                    "start": 769,
                    "end": 789,
                    "matchedPaperCorpusId": "258865272"
                },
                {
                    "start": 789,
                    "end": 813,
                    "matchedPaperCorpusId": "267759574"
                },
                {
                    "start": 813,
                    "end": 831,
                    "matchedPaperCorpusId": "269626686"
                },
                {
                    "start": 831,
                    "end": 848,
                    "matchedPaperCorpusId": "261875650"
                },
                {
                    "start": 1068,
                    "end": 1088,
                    "matchedPaperCorpusId": "258865272"
                },
                {
                    "start": 1829,
                    "end": 1849,
                    "matchedPaperCorpusId": "262084244"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2198486328125
        },
        {
            "corpus_id": "273508009",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "text": "This strongly indicates their proficiency in CS subjects. \n\nAs shown in appendix A, Japanese LMMs uses not only translated corpora but also texts originally written in Japanese. This may contribute to their performance in CS subjects. On the other hand, however, compared to stronger models such as InternVL2-8b, LLaVA1.6-34b, and LLaVA-OV-7b, the Japanese LMMs show lower scores on CA subjects, suggesting room for improvement in their general reasoning and problem-solving capabilities in culture-agnostic context. GPT-4o vs. Claude 3.5 Sonnet We reveal a significant performance gap between the two leading models; GPT-4o and Claude 3.5. They are stateof-the-art models and their performance is known to be similar with only 0.8% difference on the MMMU benchmark in English (Anthropic, 2024). Further, their performance is similar even on CA split in Japanese (1.2% difference in Table 2). However, on the CS split, GPT-4o outperforms Claude 3.5 Sonnet by a substantial 15.7%. This strongly indicates that a model's Japanese language skill and its understanding of Japanese culture should be separately discussed. Our research is pioneering in revealing this, a discrepancy that would have remained obscured without combining translation-based CA subjects and brandnew CS subjects. Our finding underscores the limitations of relying exclusively on auto-translated benchmarks for a thorough evaluation of model capabilities in non-English languages, highlighting the importance of evaluating models on culturespecific questions. 5 Analysis",
            "score": 0.482123948231326,
            "section_title": "Main Result",
            "char_start_offset": 14214,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 60,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 795
                },
                {
                    "start": 796,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1284
                },
                {
                    "start": 1285,
                    "end": 1541
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1385498046875
        },
        {
            "corpus_id": "276557950",
            "title": "Do Multilingual LLMs Think In English?",
            "text": "The internal mechanisms of LLMs affect their performance in several different ways, which we summarize below. \n\nPerformance The performance of multilingual language models varies across languages (Shafayat et al., 2024;Huang et al., 2023;Bang et al., 2023;Shi et al., 2022;Ahuja et al., 2023), often performing best in English. This can even be leveraged to improve performance in other languages, for example, through cross-lingual chain-of-thought reasoning (Chai et al., 2024), or by modifying prompts, such as using multilingual instructions or asking the LLM to translate the task into English before completing it (Zhu et al., 2023;Etxaniz et al., 2023). \n\nFluency and language confusion Marchisio et al. (2024) has shown that English-centric models are prone to language confusion, i.e., providing the answer in the incorrect language. Moreover, even when LLMs output text in the correct language, they can produce unnatural sentences in other languages, akin to an accent (Guo et al., 2024). \n\nBias and culture Moreover, LLMs tend to be biased toward certain cultures, with content performing better when dealing with facts originating from Western contexts (Naous et al., 2024;Shafayat et al., 2024), falling short when answering questions on other cultures (Chiu et al., 2024). Liu et al. (2024) investigate the cultural diversity of LLMs using proverbs and find that these models often struggle to reason with or effectively use proverbs in conversation. Their understanding appears limited to memorization rather than true comprehension, creating a notable \"culture gap\" when translating or reasoning with culturally specific content across languages.",
            "score": 0.4819641890775415,
            "section_title": "Multilingual LLM behavior",
            "char_start_offset": 23480,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 112,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 660
                },
                {
                    "start": 663,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 999
                },
                {
                    "start": 1002,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1465
                },
                {
                    "start": 1466,
                    "end": 1663
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10052490234375
        },
        {
            "corpus_id": "274130807",
            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
            "text": "For example, translating \"The elderly person learned to use a smartphone\" might introduce a tone or wording in the target language that implies surprise or patronization, even if the original sentence was neutral (Caliskan et al., 2017). \n\n-Translation of Age-Related Idioms: When translating age-related idioms or expressions, a biased translation system might reinforce negative stereotypes. For instance, translating a phrase like \"old people are slow\" into another language might retain the negative connotation or even intensify it if the target language has a stronger cultural bias against the elderly. (Hovy and S\u00f8gaard, 2015). \n\n\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016).",
            "score": 0.48170278319668136,
            "section_title": "A.2.5 Machine Translation",
            "char_start_offset": 92990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 240,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1665
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 236,
                    "matchedPaperCorpusId": "23163324"
                },
                {
                    "start": 610,
                    "end": 634,
                    "matchedPaperCorpusId": "7396193"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57275390625
        },
        {
            "corpus_id": "267199939",
            "title": "The Effect of Human v/s Synthetic Test Data and Round-Tripping on Assessment of Sentiment Analysis Systems for Bias",
            "text": "We discuss related work on bias in SASs and rating of AI systems. A more detailed discussion on bias in AI systems and causal analysis is in the supplementary. Bias in Sentiment Assessment Systems: In [3], the authors create the Equity Evaluation Corpus (EEC) dataset which consists of 8,640 English sentences where one can switch a person's gender or choose proper names typical of people from different races. The authors find that up to 75% of the sentiment systems can show variations in sentiment scores which can be perceived as bias based on gender or race. Multi-lingual Systems: While much of the work in sentiment analysis was conducted using data in English language, there is growing interest in other languages. In [8], the authors reimplement sentiment methods from literature in multiple languages and report accuracy lower than published due to lack of detail in the presentation of original approaches. Multilingual SASs often use machine translators which can be biased, and further acquiring training data in non-English languages is an additional challenge. In [9], the authors prove that roundtrip translation can reduce bias in SASs. In this work, we hypothesize that SASs could exhibit gender and racial biases in their behavior when tested on round-trip translated data. Rating of AI Systems: A recent series of studies is on assessing and rating AI systems (translators and chatbots) for trustworthiness from a third-party perspective, i.e., without access to the system's training data. In [10], [11], the authors propose to rate automated machine language translators for gender bias. Further, they create visualizations to communicate ratings [12], and conduct user studies to determine how users perceive trust [13]. Though they were effective, they did not provide any causal interpretation.",
            "score": 0.48134213543564264,
            "section_title": "II. BACKGROUND",
            "char_start_offset": 2399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 65
                },
                {
                    "start": 66,
                    "end": 159
                },
                {
                    "start": 160,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1745
                },
                {
                    "start": 1746,
                    "end": 1821
                }
            ],
            "ref_mentions": [
                {
                    "start": 1671,
                    "end": 1675,
                    "matchedPaperCorpusId": "235363604"
                },
                {
                    "start": 1740,
                    "end": 1744,
                    "matchedPaperCorpusId": "237700409"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.077392578125
        },
        {
            "corpus_id": "264305810",
            "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
            "text": "This study exposes the cultural dominance of LLMs, particularly their tendency to reflect English culture even when queried in non-English languages. Our experimental results on a constructed benchmark revealed that ChatGPT is highly dominated by English culture. Among the GPT family, text-davinci-003 is least affected by this issue, while GPT-4 is most affected. We propose two potential solutions to mitigate this problem: training LLMs on more diverse data, which can help reduce cultural dominance but at a higher computational and financial cost, and prompting LLMs by explicitly identifying the culture of the query language, a more cost-effective method that can improve performance on concrete cultural objects but is less effective on abstract ones. Our findings underscore the need for developing more culture-aware LLMs that respect and value the diversity of global cultures. We hope that our research will encourage further exploration into this critical issue and inspire the creation of more culturally sensitive AI systems.",
            "score": 0.48124352224638267,
            "section_title": "Conclusion",
            "char_start_offset": 18819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1041
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157470703125
        },
        {
            "corpus_id": "276929212",
            "title": "XIFBench: Evaluating Large Language Models on Multilingual Instruction Following",
            "text": "In this section, we detail the translation validation process introduced in Section 3.1.4. Since our annotators are not native speakers of Russian, Arabic, Hindi, or Swahili, we leverage Google Translate and GPT-4o to automatically assess translation quality. Our validation procedure follows a requirement-based evaluation framework, ensuring that each instruction and its translation are analyzed for both consistency and accuracy. \n\nSpecifically, for each instruction-translation pair, we prompt GPT-4o to identify corresponding segments in the English instruction and its translated counterpart, using predefined evaluation requirements as reference points. Additionally, we incorporate back-translation via Google Translate to mitigate potential biases, particularly for lowresource languages. The LLM annotator then provides a concise assessment of translation correctness and assigns a quality score on a 1-5 scale, where a score of 3 serves as the threshold for acceptable translation quality. The detailed prompt used for this evaluation is provided in Table 14. To quantify translation quality, we analyze the distribution of evaluation requirement scores across different languages. The results, summarized in Table 15, show that the majority of translations receive high-quality ratings. Notably, the proportion of requirements scoring below 3 remains low across all languages, with Arabic exhibiting the highest at just 1.39%. These findings suggest that the translations are generally accurate and consistent with the original English instructions, and their inclusion in the benchmark does not significantly impact the overall evaluation results.",
            "score": 0.4809899783379797,
            "section_title": "A.4 Multilingual Expansion Details",
            "char_start_offset": 32418,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1661
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.152587890625
        },
        {
            "corpus_id": "270719963",
            "title": "Evaluating Neural Networks\u2019 Ability to Generalize against Adversarial Attacks in Cross-Lingual Settings",
            "text": "According to the results, both the XLM-Roberta and mBART models perform better on some language pairings than others based on accuracy and F1 scores.\n\nFigures 3 and 4 illustrate the impact of translation on model accuracy across different language pairs.The data reveal that accuracy significantly decreases when translating between languages with dissimilar structures, such as Urdu to French or Thai to Hindi.Conversely, the reduction in accuracy is less pronounced when translating between languages with similar linguistic patterns, such as English to German or Arabic to English.Despite these variations, the overall trend shows that translation introduces superficial patterns that generally lead to a decline in accuracy in almost all cases after translation.\n\nBased on the Urdu-Hindi language pair, the accuracy scores of the XLM-Roberta model are 0.65220 and 0.62188, respectively.In the Urdu-French language pair, accuracy ratings are lower, with original and translated F1 scores of 0.65220 and 0.59863, respectively.\n\nmBART performs well with English-French and English-Spanish language pairs, with excellent accuracy scores for both the original and translated F1.Other language combinations, such as Turkish-Urdu and Hindi-Urdu, score lower.\n\nThe asymmetric effects of fine-tuning on different language pairs have also been observed in recent work.Namdarzadeh et al. [15] evaluated the performance of the mBART50 multilingual model on a Farsi dataset of dislocations.They found that finetuning mBART50 using French-Farsi aligned data dramatically improved the grammatical well-formedness of the French translations, even though some semantic issues remained.However, when they replicated the experiment with Farsi-English fine-tuning, the translations to English were sometimes worse than the baseline mBART50 model.This suggests that the success of fine-tuning multilingual models can be heavily dependent on the specific language pair and the quality of the fine-tuning data available.While adding even a small amount of aligned data in one language direction may be sufficient to improve morpho-syntactic performance, the same may not hold true for the other translation direction.Maintaining a balance between improving grammatical wellformedness and preserving semantic coherence appears to be a key challenge when finetuning these models for cross-lingual tasks.",
            "score": 0.48029104406636725,
            "section_title": "Discussion",
            "char_start_offset": 19829,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 151,
                    "end": 254
                },
                {
                    "start": 254,
                    "end": 411
                },
                {
                    "start": 411,
                    "end": 584
                },
                {
                    "start": 584,
                    "end": 766
                },
                {
                    "start": 768,
                    "end": 890
                },
                {
                    "start": 890,
                    "end": 1028
                },
                {
                    "start": 1030,
                    "end": 1177
                },
                {
                    "start": 1177,
                    "end": 1255
                },
                {
                    "start": 1257,
                    "end": 1362
                },
                {
                    "start": 1362,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 2001
                },
                {
                    "start": 2001,
                    "end": 2198
                },
                {
                    "start": 2198,
                    "end": 2382
                }
            ],
            "ref_mentions": [
                {
                    "start": 1381,
                    "end": 1385,
                    "matchedPaperCorpusId": "265158309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1636962890625
        },
        {
            "corpus_id": "271404509",
            "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size",
            "text": "Since GPT-3 (Brown et al., 2020), Large Language Models (LLMs), capable of generating human-like text based on instructions, have garnered significant attention from both academia and industry.Numerous benchmarks and datasets have been created and employed to assess LLMs' capability in generating human-like text across various tasks like question-answering, chatbot, and summarization (Clark et al., 2018;Zellers et al., 2019;Hendrycks et al., 2021a).The open-source leaderboard (Park, 2023) allows researchers and engineers to directly compare language models across various dimensions, spanning from commonsense reasoning to advanced question answering, showcasing their respective abilities.However, focusing on information content while ignoring language's social factors is currently a limitation of natural language processing (NLP) (Hovy and Yang, 2021).\n\nGiven their capacity to generate human-like text, it is imperative to investigate whether LLMs demonstrate human-like behaviors stemming from the internalized values and cultural insights acquired from large-scale training corpora.As modelgenerated text gains wider adoption, ethical concerns arise due to the potential influence of cultural biases embedded in the generated text on its users (Kumar et al., 2023).Hence, an emerging research trend involves quantifying the cultural biases within language models and understanding their impact on the models' performance across various tasks.\n\nThe primary methods for assessing values in LLMs typically involve using social science and psychological instruments originally designed for humans (Feng et al., 2023;Arora et al., 2023) to assess various cultural aspects quantitatively, or by developing specialized datasets to examine model biases (Parrish et al., 2022;Huang and Xiong, 2023).Many studies on social science instruments primarily evaluate text generated by models in English.However, historical linguistic research, such as the Whorfian hypothesis proposed by Sapir-Whorf, suggests that language structure significantly influences individual perceptions and worldviews (Kay and Kempton, 1984).Research has shown that cultural accommodation occurs when individuals engage in multilingual contexts, as evidenced by experiments with human subjects (Harzing and Maznevski, 2002).",
            "score": 0.48015627502046454,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 863
                },
                {
                    "start": 865,
                    "end": 1096
                },
                {
                    "start": 1096,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1456
                },
                {
                    "start": 1458,
                    "end": 1804
                },
                {
                    "start": 1804,
                    "end": 1902
                },
                {
                    "start": 1902,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 407,
                    "end": 428,
                    "matchedPaperCorpusId": "159041722"
                },
                {
                    "start": 428,
                    "end": 452,
                    "matchedPaperCorpusId": "221516475"
                },
                {
                    "start": 841,
                    "end": 862,
                    "matchedPaperCorpusId": "235097460"
                },
                {
                    "start": 1258,
                    "end": 1278,
                    "matchedPaperCorpusId": "252907607"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12261962890625
        },
        {
            "corpus_id": "270711451",
            "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
            "text": "While we strive to cover as many aspects of the cultures represented in CALMQA as possible, we acknowledge that it is not feasible to encompass every cultural nuance.Additionally, for low-resource languages, we employed workers to manually write questions, which impacts scalability.Finally, our culturally agnostic questions are translations from English performed by GPT-4-TURBO, and thus may not match the quality of human translations.\n\nOur automatic evaluation relies on surface-level measures such as language detection and token repetitions.While this approach allows us to determine that current LLMs still struggle with producing outputs in the correct language and without repetitions, it does not assess the factuality or fluency of outputs that lack these surface-level issues.This underscores the need for comprehensive metrics to evaluate overall answer quality in multilingual LFQA, which we leave to future work.\n\nOur human evaluation uses 100 questions across 5 languages to demonstrate that models struggle to generate well-written, factual, and complete answers in non-English languages.Large-scale human evaluation is time-consuming and prohibitively expensive, and finding workers proficient in lowresource languages presented a significant challenge, constraining our evaluation efforts.However, we have shown that we can statistically justify various insights about LLM multilingual capabilities with our scale of data.",
            "score": 0.480000761859134,
            "section_title": "Limitations",
            "char_start_offset": 30148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 166,
                    "end": 283
                },
                {
                    "start": 283,
                    "end": 439
                },
                {
                    "start": 441,
                    "end": 548
                },
                {
                    "start": 548,
                    "end": 789
                },
                {
                    "start": 789,
                    "end": 928
                },
                {
                    "start": 930,
                    "end": 1106
                },
                {
                    "start": 1106,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1240234375
        },
        {
            "corpus_id": "270562911",
            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
            "text": "Hence, when widening language support, an important question arises: Is translated data sufficient for instruction tuning? \n\nWe regard \"sufficiency\" as the fact that, when used to fine-tune an LLM, translated instructions should lead to output quality similar to that of native data. Yet, examining this training data factor cannot be separated from carefully considering the evaluation protocol, because many existing multilingual benchmarks have been created via translation. This translation bias, if present in both training and test sets, could hinder a meaningful con-clusion. We thus put forward our second research question: If translated and native instruction data make a difference, would a translated benchmark capture it? Subsequently, we use round-trip translated data to answer: Which is the cause of the gap, translation defects or missing language-specific knowledge in instructions? Finally, when translated data is hardly avoidable: What techniques can we adopt to bridge the performance gap? \n\nThis work systematically investigates native and translated data used during instruction tuning and evaluation. We experiment with eight models of varying sizes and data distributions and evaluate these models on nine benchmarks of different natures: translated versus native as well as classification versus generation. Empirical results suggest that a prudent choice in multilingual LLM evaluation is crucial. Foreshadowing the answers to the research questions raised earlier: \n\n1. Native and translated data can lead to a performance gap on several benchmarks, especially when the model performance is strong. \n\n2. Such a difference is more pronounced on benchmarks that are natively created (TyDi QA, CMMLU, C-Eval) or generative in nature (XQuAD, open-ended QA) compared to translated structured tests (MT/HT-MMLU). \n\n3. Round-trip translation from native data outperforms single-pass translation from English data, implying that missing language-specific knowledge could be more detrimental than having translation defects. \n\n4. Regularization during instruction tuning time, e.g. using a lower learning rate or multilingual instruction tuning, can be beneficial if translated data has to be used. It can close the native-translated performance gap on structured tasks but not generative tasks. \n\nThese insights mean that opposite conclusions can be made when different combinations of instruction and test sets are adopted.",
            "score": 0.4793525234080056,
            "section_title": "Introduction",
            "char_start_offset": 1892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 125,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1011
                },
                {
                    "start": 1014,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1493
                },
                {
                    "start": 1496,
                    "end": 1627
                },
                {
                    "start": 1630,
                    "end": 1835
                },
                {
                    "start": 1838,
                    "end": 2044
                },
                {
                    "start": 2047,
                    "end": 2218
                },
                {
                    "start": 2219,
                    "end": 2315
                },
                {
                    "start": 2318,
                    "end": 2445
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.275634765625
        },
        {
            "corpus_id": "274423394",
            "title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge",
            "text": "The rapid advancement of AI technologies underscores the importance of developing LLMs that are proficient across diverse linguistic and cultural contexts, ensuring fair and equitable performance for stakeholders from various language groups. However, the lack of high-quality evaluation benchmarks in many languages discourages practitioners from training multilingual LLMs to meet this challenge. This evaluation gap limits the effective deployment of LLMs for many regions, exacerbates digital divides, and inhibits the economic and societal value of AI tools in many underserved communities. \n\nThe source of this gap is the multitude of challenges in evaluating LLMs for multilingual contexts. First, at a meta-level, the majority of benchmarks for LLMs are only in English (Hendrycks et al., 2020, inter alia). While non-English benchmarks exist for some tasks (Singh et al., 2024;Aakanksha et al., 2024;Pozzobon et al., 2024), they usually focus on single languages (Li et al., 2023;Koto et al., 2024), specific regions (Adelani et al., 2024;Ca\u00f1ete et al., 2020;Guevara-Rukoz et al., 2020;Cahyawijaya et al., 2022;Etxaniz et al., 2024b), or a particular domain (Wang et al., 2024a), ignoring the importance of joint evaluation to trace and unlock the benefits that multilingual capabilities could bring to low-resource languages (Pfeiffer et al., 2022;\u00dcst\u00fcn et al., 2024;Aryabumi et al., 2024). \n\nTechnical challenges also abound due to the manner in which multilingual datasets are often collected. Certain datasets are constructed using manually applied templates, resulting in low prompt and completion diversity (Muennighoff et al., 2022). Many more are composed of translations from high-resource languages (e.g., English; Holtermann et al., 2024;Myung et al., 2024;Lai et al., 2023;Foroutan et al., 2023).",
            "score": 0.4789604422310942,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1817
                }
            ],
            "ref_mentions": [
                {
                    "start": 866,
                    "end": 886,
                    "matchedPaperCorpusId": "267617144"
                },
                {
                    "start": 1048,
                    "end": 1068,
                    "matchedPaperCorpusId": "260681542"
                },
                {
                    "start": 1068,
                    "end": 1095,
                    "matchedPaperCorpusId": "218973838"
                },
                {
                    "start": 1335,
                    "end": 1358,
                    "matchedPaperCorpusId": "248721770"
                },
                {
                    "start": 1358,
                    "end": 1377,
                    "matchedPaperCorpusId": "267627803"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.058441162109375
        },
        {
            "corpus_id": "253885863",
            "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
            "text": "Methods of Measuring Bias The most closely related methods for measuring bias to the one proposed in this paper are proposed in [10] and [8]. In [10], the existing CrowS-Pairs [11] bias benchmark is translated into French. Both the original benchmark and the translated benchmark use a similar sentence scoring system as in this paper. However, the translated benchmark avoids issues of gender-based declension of adjectives, instead preferring to rewrite the sentence to avoid the adjective entirely (via periphrasis). In this paper, we experiment with a declension-adjusted minimal pair/set of sentences, where multiple words in the sentence are potentially modified to result in grammatical sentences in both the male and female cases. \n\nCross-lingual and Cross-cultural Analysis While a huge amount of research has been done on English texts, few papers have studied the extent to which word embeddings capture the intrinsic bias in models for other languages. [12] proposes a debiasing method and applies it to the Multilingual BERT (M-BERT) model, and shows that English training data can help mitigate gender bias in M-BERT in the Chinese language. [13] measures profession bias in multilingual word embeddings (using both fastText and M-BERT embeddings) with inBias, a proposed evaluation metric that uses pairs of male/female occupation terms (e.g. \"doctor\" and \"doctora\" in Spanish) to measure gender bias. They also introduce a new dataset, MIBs, by manually collecting pairs of gender and profession words in English, French, Spanish, and German languages. \n\nWhile these papers are analyzing models for different languages, these languages all exist in the same cultural sphere. In [8], the authors study how ethnic bias varies across models for different languages with different cultures (inc. Korean, English, Turkish, etc.) using monolingual BERT models. The authors claim that different amount of ethnic biases across these languages reflects the historical and social context of the countries. The authors propose two debiasing techniques as well as a metric called Categorical Bias score in order to measure the ethnic bias. For measuring ethnic bias, the authors use a simple \"People from [X]\" template, where X is a country.",
            "score": 0.4787872018622511,
            "section_title": "Related Work",
            "char_start_offset": 2903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 519
                },
                {
                    "start": 520,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1155
                },
                {
                    "start": 1156,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1568
                },
                {
                    "start": 1571,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2245
                }
            ],
            "ref_mentions": [
                {
                    "start": 128,
                    "end": 132,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 137,
                    "end": 140,
                    "matchedPaperCorpusId": "237491723"
                },
                {
                    "start": 145,
                    "end": 149,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 965,
                    "end": 969,
                    "matchedPaperCorpusId": "227230609"
                },
                {
                    "start": 1156,
                    "end": 1160,
                    "matchedPaperCorpusId": "218487087"
                },
                {
                    "start": 1694,
                    "end": 1697,
                    "matchedPaperCorpusId": "237491723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.375244140625
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "Similar to these works we translate a bias benchmark from English, but we investigate more implicit stereotypes divided into specific bias categories, without having to translate model responses back to English, or rely on an external language model to judge the responses.Shen et al. (2024) find that models tend to generate more offensive, but less relevant responses in low-resource languages.This indicates a relationship between accuracy and bias, which we aim to measure using our control set.In terms of social biases, Zhao et al. (2024) investigate gender bias in GPT models across six languages with translated templates.They measure gender differences in the types of descriptive words models assign to a person, and in the topics of generated dialogues involving a person of that gender.Compared to this work, we investigate a wider range of models and biases, and focus on specific stereotypes rather than more general disparities in treatment across demographic groups.\n\nFigure 1: Example from the Gender identity category in English and Spanish, plus the control-MBBQ counterpart of the English sample.In ambiguous contexts, the correct answer is always the \"unknown\" answer.In biased contexts, the correct answer to this question is the biased answer, and in counter-biased contexts this is the counter-biased answer.\n\nClosest to our work, there exist the KoBBQ (Jin et al., 2024) and CBBQ (Huang & Xiong, 2024) datasets, both adaptations of the BBQ dataset into Korean and Chinese respectively.CBBQ was created by prompting GPT-4 to complete samples that were designed by humans, which as Jin et al. (2024) note is subject to GPT-4's limitations, including its own biases.On the other hand, Jin et al. (2024) use cultural transfer techniques in translating the BBQ dataset to Korean and extending it to fit the South Korean cultural context.In contrast, we include three other languages, only consider stereotypes that apply to all of them, separate bias from task performance, and compare model biases across languages.",
            "score": 0.47878025278874214,
            "section_title": "Biases in generative LLMs",
            "char_start_offset": 7262,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 273
                },
                {
                    "start": 273,
                    "end": 396
                },
                {
                    "start": 396,
                    "end": 499
                },
                {
                    "start": 499,
                    "end": 630
                },
                {
                    "start": 630,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 982
                },
                {
                    "start": 984,
                    "end": 1116
                },
                {
                    "start": 1116,
                    "end": 1189
                },
                {
                    "start": 1189,
                    "end": 1332
                },
                {
                    "start": 1334,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1688
                },
                {
                    "start": 1688,
                    "end": 1857
                },
                {
                    "start": 1857,
                    "end": 2036
                }
            ],
            "ref_mentions": [
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "260334475"
                },
                {
                    "start": 1405,
                    "end": 1426,
                    "matchedPaperCorpusId": "259274987"
                },
                {
                    "start": 1605,
                    "end": 1622,
                    "matchedPaperCorpusId": "260334475"
                },
                {
                    "start": 1707,
                    "end": 1724,
                    "matchedPaperCorpusId": "260334475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.162109375
        },
        {
            "corpus_id": "268253331",
            "title": "From One to Many: Expanding the Scope of Toxicity Mitigation in Language Models",
            "text": "We investigate the impact of translation quality on mitigation performance by evaluating the mGPT 1.3B  Vidgen et al. (2020) were translated from English to each language with Google Translate and scored with PerspectiveAPI. German and Portuguese show higher toxicity scores given the same content. model across four languages: Brazilian Portuguese, Russian, Arabic, and Hindi. Utilizing three translation models with good, but varying translation quality, we assess their performance using chrF++ scores (Popovi\u0107, 2017) for 1000 samples (toxic and non-toxic) from each language, as shown in Table 7. We compare the models' translations to those of Google Translate, which serves as a peer evaluator for translation quality in our toxicity-related domain (Agrawal et al., 2021). Google Translate is the current stateof-the-art for automatic translations when translating from English to other languages, but the 54.5B NLLB model outputs competitive performance on the Flores-200 dataset (Costa-juss\u00e0 et al., 2022a). For that dataset, the 54.5B NLLB model has an average chrf++ score of 48.3, while NLLB 1.3B has 46.9 and NLLB 600M 44.6 (Costa-juss\u00e0 et al., 2022a). This indicates that the M2M 418M Fan et al. ( 2021) provides the lowest quality translations, NLLB 600M distilled offers medium-quality translations, and NLLB 1.3B distilled Costa-juss\u00e0 et al. (2022a) delivers higherquality translations. \n\nIn Figure 11 we observe how there's a clear correlation between quality and mitigation performance for Portuguese. For the other languages the NLLB 600M results in better mitigation performance overall. For these translators, we find no evidence of a trend between translation quality and mitigation capabilities.",
            "score": 0.4771654586532582,
            "section_title": "D.3 The impact of translation quality",
            "char_start_offset": 36705,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 1015
                },
                {
                    "start": 1016,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1402
                },
                {
                    "start": 1405,
                    "end": 1519
                },
                {
                    "start": 1520,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 505,
                    "end": 520,
                    "matchedPaperCorpusId": "12942757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0537109375
        },
        {
            "corpus_id": "273323209",
            "title": "Towards Multilingual LLM Evaluation for European Languages",
            "text": "Multilingual benchmarks have predominantly been generated using two widely adopted methodologies, besides various intermediate approaches: either by using human annotators of multiple languages to develop a multilingual dataset from scratch (Kocmi et al., 2023;Goyal et al., 2022;Conneau et al., 2018) or by using LLMs to translate an existing benchmark into multiple languages (Lai et al., 2023;Tiedemann, 2012). Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012). \n\nThis method is increasingly inefficient because model scores can become saturated over time (Ott et al., 2022), or benchmarks loose their relevance too fast if a desired performance is reached (Lewkowycz et al., 2022). An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). \n\nThose languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018). There-fore, the selection of languages is a central decision to make when creating a multilingual benchmark. \n\nIn addition to the choice of languages, it is crucial to consider the range of tasks that need to be covered for a benchmark.",
            "score": 0.4771218235057644,
            "section_title": "Related Work",
            "char_start_offset": 2745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 787
                },
                {
                    "start": 790,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1460
                },
                {
                    "start": 1463,
                    "end": 1559
                },
                {
                    "start": 1560,
                    "end": 1680
                },
                {
                    "start": 1681,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1951
                },
                {
                    "start": 1954,
                    "end": 2079
                }
            ],
            "ref_mentions": [
                {
                    "start": 1266,
                    "end": 1285,
                    "matchedPaperCorpusId": "246680398"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.157958984375
        },
        {
            "corpus_id": "268338839",
            "title": "Cross-cultural differences through subjective cognition: illustration in translatology with the SSTIC-E in the UAE",
            "text": "The availability of psychological assessment tools in many languages is essential in generalizing research results across different cultural settings or in conducting cross-cultural comparative studies. Yet, translating these tools, especially those relying on the reporting of subjective states, poses several challenges. According to Nida (1993) and Huang et al. (2022), cultural differences constitute significant obstacles for translators and are the source of misconceptions among readers. If translation is seen as a form of intercultural communication and cultural differences cause significant difficulties in translation, then its impact is even more prominent when it comes to translating subjective cognitive content. According to these authors, in addition to lexical, semantic, and grammatical structures, translation becomes a task that serves to faithfully transcribe the meaning of information written in one language (for instance English) with the same information in another language (Arabic). \n\nMost psychometrically sound research instruments have been developed and their properties evaluated using English-speaking populations (Capitulo et al., 2001;Duffy, 2006). Given the cultural diversity and the variety of languages spoken in the world, health researchers are challenged to be culturally and linguistically sensitive when administering a clinical instrument (Abdel Aziz et al., 2021). \n\nThe process of cross-cultural validation allows the translation of a clinical instrument into a language different from the one in which it was developed and it may contribute to identifying any possible cultural-related variability in the phenomena studied when a different language is used. It is always considered a complex process because of the possible interference arising from cross-cultural and ethnic factors that were not accounted for at the time the instrument was originally designed (Duffy, 2006). \n\nSuccess in tackling these challenges, according to Hilton and Skrutkowski (2002), depends on understanding the fundamental problems of language equivalence, cultural constructs, and psychometric changes that are embedded in translation (Stip, 1996). This should be the first consideration when seeking to develop a culturally equivalent translated instrument. We illustrate this process in psychopathology with the case of a scale measuring cognitive complaints in schizophrenia (Stip, 1996;Hilton and Skrutkowski, 2002).",
            "score": 0.47711461365379126,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 1012
                },
                {
                    "start": 1015,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1928
                },
                {
                    "start": 1931,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2290
                },
                {
                    "start": 2291,
                    "end": 2452
                }
            ],
            "ref_mentions": [
                {
                    "start": 1150,
                    "end": 1173,
                    "matchedPaperCorpusId": "33345686"
                },
                {
                    "start": 1173,
                    "end": 1185,
                    "matchedPaperCorpusId": "43745915"
                },
                {
                    "start": 1914,
                    "end": 1927,
                    "matchedPaperCorpusId": "43745915"
                },
                {
                    "start": 1982,
                    "end": 2011,
                    "matchedPaperCorpusId": "243850367"
                },
                {
                    "start": 2167,
                    "end": 2179,
                    "matchedPaperCorpusId": "15866275"
                },
                {
                    "start": 2410,
                    "end": 2422,
                    "matchedPaperCorpusId": "15866275"
                },
                {
                    "start": 2422,
                    "end": 2451,
                    "matchedPaperCorpusId": "243850367"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313232421875
        },
        {
            "corpus_id": "272593159",
            "title": "Native vs Non-Native Language Prompting: A Comparative Analysis",
            "text": "Understanding how LLMs respond to prompts in different languages is crucial for evaluating their generalization and reasoning capabilities. Nguyen et al. (2023) examined the use of linguistically diverse prompts to leverage LLMs' strengths in multilingual contexts, especially for low-resource languages. Their results indicated that while LLMs perform well in English-dominant tasks, further research is needed for zero-shot setups in low-resource languages like Arabic [34]. Recent studies also highlight the importance of linguistic diversity in evaluating LLMs' performance across different languages and cultural contexts [23]. Further analysis by [24] revealed that language models often exhibit varying degrees of bias and performance discrepancies when switching from high-resource languages like English to low-resource languages.",
            "score": 0.4761945210285236,
            "section_title": "Native vs. Non-Native Language Prompting",
            "char_start_offset": 5662,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 839
                }
            ],
            "ref_mentions": [
                {
                    "start": 627,
                    "end": 631,
                    "matchedPaperCorpusId": "253553585"
                },
                {
                    "start": 653,
                    "end": 657,
                    "matchedPaperCorpusId": "257985499"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055816650390625
        },
        {
            "corpus_id": "268819377",
            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
            "text": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets [12].",
            "score": 0.4757768092357937,
            "section_title": "B. Bias Benchmark",
            "char_start_offset": 50674,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 295
                }
            ],
            "ref_mentions": [
                {
                    "start": 290,
                    "end": 294,
                    "matchedPaperCorpusId": "247626152"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63427734375
        },
        {
            "corpus_id": "259341896",
            "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
            "text": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.",
            "score": 0.4755836562186838,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.262939453125
        },
        {
            "corpus_id": "270379819",
            "title": "MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs",
            "text": "To make progress in this direction, in this paper we investigate to what extent the presence of bias regarding social stereotypes differs when chat-optimised generative LLMs are prompted in different languages, while controlling for cultural idiosyncrasies and model accuracy.\n\nIn summary, our main contributions are: (1) We present the Multilingual Bias Benchmark for Question-answering (MBBQ), a hand-checked translation of the English BBQ dataset into Dutch, Spanish, and Turkish for measuring cross-lingual differences on a subset of stereotypes widely held in these languages.(2) We create a parallel MBBQ control dataset to test for task performance independently from bias; both MBBQ and its control counterpart will be publicly released to facilitate further research as well as possible dataset extensions (to other languages and/or stereotypes) in the future.(3) We carry out experiments with 7 LLMs comparing accuracy on the question-answering task and bias behaviour across 6 bias categories in the 4 languages mentioned above.Our results show that all models display significant differences across languages in question-answering accuracy and, with the exception of the most accurate models, also in bias behavior-despite controlling for cultural shifts.When bias scores differ significantly across languages, models are generally most biased in Spanish, and least biased in English or Turkish.Models are generally less accurate and give more biased answers when the context of a question is ambiguous, relying on stereotypes rather than acknowledging that the question cannot be answered.\n\nOverall, our findings highlight the importance of controlling for cultural differences and task accuracy when measuring model bias.With MBBQ, we hope to encourage further work on bias in multilingual settings and facilitate research on cross-lingual debiasing.",
            "score": 0.4753714457596784,
            "section_title": "Introduction",
            "char_start_offset": 1522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 278,
                    "end": 581
                },
                {
                    "start": 581,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1267
                },
                {
                    "start": 1267,
                    "end": 1407
                },
                {
                    "start": 1407,
                    "end": 1602
                },
                {
                    "start": 1604,
                    "end": 1735
                },
                {
                    "start": 1735,
                    "end": 1864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3193359375
        },
        {
            "corpus_id": "261557385",
            "title": "HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models",
            "text": "Multitask benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) were introduced along with the English language models. Once these were saturated, they were followed by even bigger benchmarks such as MMLU (Hendrycks et al., 2020) and Big BENCH (Srivastava et al., 2022). Non-English evaluation research has mirrored this trend, predominantly through translation or re-implementation of existing English benchmarks. Examples include JGLUE (Kurihara et al., 2022), KLUE (Park et al., 2021), and CMMLU (Li et al., 2023), which are Japanese and Korean adaptations of GLUE and Chinese re-implementation of MMLU, respectively. \n\nHowever, these benchmarks fall short of capturing the native knowledge encoded in the parameter of LLMs. This highlights the need for evaluation suites curated to assess the cultural context of a model. Recent research in this direction is BHASA (Leong et al., 2023), which aims at gauging the cultural depth of language models in Southeast Asian languages. Nonetheless, limitations are apparent: only 34 questions for Indonesian and 28 for Tamil in the entire dataset specifically address cultural representation tasks. In this paper, we introduce the HAE-RAE Bench, an evaluation set of 1.5K questions curated to assess Korean-specific knowledge in language models.",
            "score": 0.4750484784665867,
            "section_title": "Multilingual Evaluation",
            "char_start_offset": 3965,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 141,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1311
                }
            ],
            "ref_mentions": [
                {
                    "start": 65,
                    "end": 84,
                    "matchedPaperCorpusId": "143424870"
                },
                {
                    "start": 459,
                    "end": 482,
                    "matchedPaperCorpusId": "249687727"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "260155190",
            "title": "Towards Bridging the Digital Language Divide",
            "text": "This section presents examples of linguistic bias in mainstream AI language technology: within the structure of multilingual lexical databases, within neural language models, and finally various manifestations of linguistic bias in machine translation systems. \n\nBias in Lexical Databases. As a generalisation of bilingual dictionaries, the 2000s saw the appearance of multilingual lexical databases that map words, based on their meanings, across large numbers of languages. While these resources proved to be extremely useful in cross-lingual applications, looking under the hood-into their underlying models of lexical meaning-reveals varying levels of limited expressivity and bias. \n\nAs shown in the survey [19], several of these multilingual databases interconnect words from hundreds of languages, mapping the words of each language to the 100 thousand English word meanings (so-called synsets) of Princeton WordNet [29]. On the one hand, this choice makes practical sense, as among all similar resources, WordNet offers by far the widest coverage of word meanings. On the other hand, it results in a strong bias towards the English language and Anglo-Saxon culture in general, as the expressivity of the database is limited to notions for which a word exists in English [19,3]. Figure 1 provides a simple example from the food domain, known to be culturally, and thus also linguistically, diverse. It shows how a biased lexical database maps together words in Swahili and Japanese meaning uncooked rice, cooked rice, and brown rice. The degree of information loss is flagrant: while both Swahili and Japanese provide fine-grained lexicalisations about the various forms of rice, the many-to-many mapping that results from passing through English masks all fine-grained differences, resulting in both a loss of detail and incorrect translations when one moves from Swahili to Japanese or vice versa. The diversity-diminishing bias towards the English language and Anglo-Saxon cultures is also found in other domains that are well-known to be diverse across languages: family relationships, school systems, etc.",
            "score": 0.47486591450840776,
            "section_title": "Bias in Language Technology",
            "char_start_offset": 13419,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 260
                },
                {
                    "start": 263,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 686
                },
                {
                    "start": 689,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1906
                },
                {
                    "start": 1907,
                    "end": 2117
                }
            ],
            "ref_mentions": [
                {
                    "start": 712,
                    "end": 716,
                    "matchedPaperCorpusId": "256105656"
                },
                {
                    "start": 1278,
                    "end": 1282,
                    "matchedPaperCorpusId": "256105656"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.135986328125
        },
        {
            "corpus_id": "264305810",
            "title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
            "text": "Large Language Models (LLMs) have become ubiquitous in various applications, such as machine translation (Jiao et al., 2023;He et al., 2023), question answering (Bang et al., 2023), grammatical error correction (Wu et al., 2023) and code intelligence tasks (Gao et al., 2023). However, these tasks usually consist of objective questions, whose answers can be determined as right or wrong. When it comes to subjective questions accompanied with no \"standard\" answers, we must pay attention to the \"opinions\" reflected by the LLMs. Generally, these \"opinions\" can be shaped throughout the development of LLMs, from user-generated data collected on the Internet, data combination during training, The ratio of responses related to English culture. The ChatGPT's responses for non-English queries are more related to English culture than to the corresponding culture, demonstrating a predominance of English culture in ChatGPT's outputs. \n\nand human alignment provided by crowd workers to the dedicated designs of model developers themselves (Santurkar et al., 2023). \n\nWhile there are pioneer works on revealing the \"opinions\" of LLMs (Santurkar et al., 2023;Hartmann et al., 2023), they are restricted to a single language (i.e., English) without considering the differences across languages. Generally, for native speakers other than English, we expect LLMs to express \"opinions\" complying with the corresponding culture when asked for assistance. However, given the predominant use of English data in training SOTA LLMs (e.g., ChatGPT), LLMs may inadvertently amplify dominant cultural narratives and further entrench existing cultural biases. As shown in Figure 1, ChatGPT is dominated by English culture: inappropriate English-culture answers dominate the model output even when asked in non-English languages. Such cultural dominance can lead to several negative effects, such as the loss of cultural diversity, promotion of stereotypes, increasing social and psychological inequality, and even violent conflict and economic impact (Writer, 2008;Demont-Heinrich, 2011).",
            "score": 0.47467549616643273,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 933
                },
                {
                    "start": 936,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1446
                },
                {
                    "start": 1447,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1812
                },
                {
                    "start": 1813,
                    "end": 2072
                }
            ],
            "ref_mentions": [
                {
                    "start": 211,
                    "end": 228,
                    "matchedPaperCorpusId": "257757296"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10089111328125
        },
        {
            "corpus_id": "276422163",
            "title": "On the robustness of ChatGPT in teaching Korean Mathematics",
            "text": "Addressing low-resource language problems Table 2 demonstrates a significant improvement in accuracy when prompts are translated into English, underscoring the challenges associated with low-resource languages such as Korean. We will denote the number of correctly answered questions by further prompt engineering as \u2191 n; i.e., we only consider 195 questions that ChatGPT provided wrong answer using zero-shot evaluation. Specifically, while the overall accuracy only marginally increased from 66.72% (\u2191 n = 0) to 68.77% (\u2191 n = 12 when comparing GPT-3.5 and GPT-4o models; the accuracy increased significantly to 73.89% (\u2191 n = 49) when the input questions and prompts have been translated into English beforehand. Moreover, despite having the same input, when we generate English answers only instead of bilingual Korean-English answers, our accuracy increases an additional 1.2% (from 73.89% to 75.09%). Diagram-based questions showed an improvement from 48.94% (\u2191 n = 0) to 56.38% (\u2191 n = 7), while non-diagram questions improved from 70.12% (\u2191 n = 0) to 78.66% (\u2191 n = 42) using this strategy. \n\nRemark 6 Korean is considered a low-to-medium resource language, which has limited training data compared to English. This leads to significant performance gaps. Fine-tuning open-source models on bilingual datasets (Korean-English math problems) or employing direct translation methods, as demonstrated in this study, can help mitigate these challenges Among the n = 195 incorrectly answered questions, n = 125(21.33%)) remained unsolved across any of the tested scenarios. On average, these unsolved questions require more steps to solve (10.08 steps vs. 9.16 steps for questions solved successfully by at least one strategy). They also feature longer problem descriptions and solutions, taking on average more time and tokens. We hypothesize that this increased complexity demands greater model attention and memory, as well as the ability to avoid and correct cumulative errors.",
            "score": 0.47446943753404514,
            "section_title": "Further analysis of incorrect responses",
            "char_start_offset": 31311,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1978
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04010009765625
        },
        {
            "corpus_id": "271924022",
            "title": "GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models",
            "text": "Objectivity. This focuses on minimizing human involvement in crafting benchmarks. It seeks to diminish the potential for subjective biases to creep in during the benchmark's creation, with the ultimate aim of achieving a fair and impartial evaluation of gender bias in language models. Robustness. This pertains to the reliability and consistency of assessment outcomes when evaluated across different prompt structures. Typically, a prompt comprises two components: instructions and requirements. Alterations in prompt structure involve modifying these instructions or requirements while preserving their initial semantic meaning. Therefore, the robustness of prompt structures implies the ability to sustain consistent assessment results even when prompt instructions or requirements are modified. This dimension ensures that the benchmarks are applicable and reliable in diverse and dynamic contexts. \n\nRealisticity. This dimension ensures that the benchmark data are 1) grounded in real-world scenarios and 2) capable of assessing open-ended responses similar to natural interactions. It is critical to ensure that the benchmarks are relevant and applicable to reallife situations, providing meaningful insights into the practical implications of gender bias in language models. By integrating these six dimensions into CGEB, we aim to overcome the current constraints associated with establishing benchmarks for gender equality. This methodical approach is carefully designed to create a dependable and all-encompassing framework, which is essential for developing gender bias benchmarks that not only exhibit robustness but also align with practical, real-world requirements. Through these efforts, we strive to promote the advancement of more equitable and inclusive language technologies.",
            "score": 0.47426569756368375,
            "section_title": "Dual-level Metrics",
            "char_start_offset": 26739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 13,
                    "end": 81
                },
                {
                    "start": 82,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 799
                },
                {
                    "start": 800,
                    "end": 903
                },
                {
                    "start": 906,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1282
                },
                {
                    "start": 1283,
                    "end": 1433
                },
                {
                    "start": 1434,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1796
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06109619140625
        },
        {
            "corpus_id": "274610534",
            "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
            "text": "Large language models have exhibited significant enhancements in performance across various tasks. However, the complexity of their evaluation increases as these models generate more fluent and coherent content. Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures. To address this research gap, we introduce KULTURE Bench, an evaluation framework specifically designed for Korean culture that features datasets of cultural news, idioms, and poetry. It is designed to assess language models' cultural comprehension and reasoning capabilities at the word, sentence, and paragraph levels. Using the KULTURE Bench, we assessed the capabilities of models trained with different language corpora and analyzed the results comprehensively. The results show that there is still significant room for improvement in the models' understanding of texts related to the deeper aspects of Korean culture.",
            "score": 0.4742344919521254,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5400390625
        },
        {
            "corpus_id": "257038384",
            "title": "How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation",
            "text": "\u2022 We provide a detailed experimental setup ( \u00a72), which includes the datasets used ( \u00a72.1), the machine translation systems used in the comparison( \u00a72.2), the GPT systems ( \u00a72.3), and the evaluation methods ( \u00a72.4). \n\n\u2022 We present a series of experiments ( \u00a73) that investigate different aspects of GPT models for machine translation. These experiments cover prompt selection strategies ( \u00a73.1), zero-shot translation capabilities of GPT models ( \u00a73.2), GPT performance on high-resource languages ( \u00a7 3.3), GPT performance on low-resource and non-English-centric languages ( \u00a7 3.4), document-level MT with GPT ( \u00a73.5), translation robustness toward domain shift ( \u00a73.6), and hybrid GPT and NMT translation ( \u00a73.7). \n\n\u2022 We then present the human evaluation and analysis that provides insights into the quality of GPT translations ( \u00a74). \n\n\u2022 We discuss the characteristics of GPT translations ( \u00a75) vis-\u00e0-vis NMT and analyze the differentiating aspects of GPT translations ( \u00a75.1) by quantitatively enumerating language modeling bias artifacts ( \u00a75.2), the characteristics of translation across various language directions ( \u00a75.3- \u00a75.5), as well as parallel data bias artifacts ( \u00a75.6). \n\n\u2022 We explore the multilingual capabilities of GPT models beyond translation ( \u00a76). \n\n\u2022 We conclude by summarizing our findings and suggesting future directions for research ( \u00a77). \n\n2 Experimental Setup",
            "score": 0.4737596549803308,
            "section_title": "Introduction",
            "char_start_offset": 4250,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 714
                },
                {
                    "start": 717,
                    "end": 835
                },
                {
                    "start": 838,
                    "end": 1184
                },
                {
                    "start": 1187,
                    "end": 1269
                },
                {
                    "start": 1272,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1389
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1446533203125
        },
        {
            "corpus_id": "268510024",
            "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models",
            "text": "Similarly, Huang et al., 2023 andEtxaniz et al., 2023 have shown that prompting LLMs to first translate or comprehend questions in English, then solve them step by step, improves performance. \n\nDespite these advancements, methodologies in various studies differ significantly, and the impact of translation on multilingual task performance remains underexplored. Furthermore, these studies focus on specific NLP tasks and English-centric LLMs, but did not study real-world user queries in various languages. This gap highlights a need for more nuanced research into the effectiveness of translation techniques across multilingual contexts. As shown in Figure 1, we hypothesize that Englishcentric LLMs generally perform better with English translations of prompts, while \"Multilingual LLMs\" excel with native prompts, particularly for tasks highly dependent on language. \n\nTo address the limitations of existing empirical studies, we perform an in-depth analysis of the utility of translation with large language models for various scenarios. Firstly, we compare translating multilingual tasks into English, with an optional step of translating responses back into the original languages (i.e., the \"translate-test\" method), against several baselines on multilingual NLP tasks. Secondly, we extend the evaluation to real user queries, which are more likely to contain knowledge related to culture and language. Thirdly, we broaden the scope of LLM evaluations to include non-English-centric models to explore how they differ in behavior from English-centric LLMs. To the best of our knowledge, this is the first work to analyze the impacts of translating real user queries on multilingual LLMs. \n\nOur results demonstrate that simply translating queries into English can already achieve the best results in multiple NLP task categories. For real user queries, the effect of translation depends on the languages and the LLMs. When working with advanced LLMs and certain languages, employing prompts in native languages appears to be the more effective strategy. In addition, the non-Englishcentric LLMs also behave differently from Englishcentric LLMs, where prompts in the native languages yield superior results by capturing the nuances related to culture and language. \n\nThe main contributions of this work are:",
            "score": 0.4736318301544876,
            "section_title": "Introduction",
            "char_start_offset": 1696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1923
                },
                {
                    "start": 1924,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2269
                },
                {
                    "start": 2272,
                    "end": 2312
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12200927734375
        },
        {
            "corpus_id": "276576058",
            "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs",
            "text": "We describe the specific benchmarks and performances in Appendix F. \n\nThis work makes several key contributions. First, we introduce a novel distribution-based methodology for probing cultural alignment across languages, moving beyond direct survey approaches to better capture underlying cultural values (Sorensen et al., 2024). Second, we provide the first systematic comparison of how improvements in scale and post-training affect cultural alignment and UScentric bias across English, Danish, Dutch, and Portuguese through a series of robust statistical models. Third, we release a dataset of model-generated responses across multiple languages and cultural contexts as well as our code, enabling future research into cultural alignment and bias. 1 Together, these contributions advance our understanding of how LLM development choices influence cultural representation while providing tools for ongoing investigation of these critical issues. Cultural alignment as value reproduction: Within a culture there will be a variety of stances to any particular topic. However, the distribution of stances will be characteristic among cultures. For instance, while some Danes are opposed to abortion, it is a much less contentious topic than in the US (Adamczyk et al., 2020;Norup, 1997).",
            "score": 0.4723396196607005,
            "section_title": "Introduction",
            "char_start_offset": 4064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 70,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1142
                },
                {
                    "start": 1143,
                    "end": 1286
                }
            ],
            "ref_mentions": [
                {
                    "start": 1250,
                    "end": 1273,
                    "matchedPaperCorpusId": "213877425"
                },
                {
                    "start": 1273,
                    "end": 1285,
                    "matchedPaperCorpusId": "6642222"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06622314453125
        },
        {
            "corpus_id": "237503539",
            "title": "Efficient Inference for Multilingual Neural Machine Translation",
            "text": "We conduct a human evaluation to compare the English-centric Big 6-6 and Big 12-2 models. It is done by certified professionals who are proficient in both the source and target language. We use bilingual direct assessment (DA), where raters have to evaluate the adequacy and fluency of each translation on a 0-5 scale given the source sentence. We select a random subset of 200 sentences from newstest2019 for DE-EN / EN-DE and from new-stest2014 for FR-EN / EN-FR. 17 For each translation direction, 3 raters are shown all the source sentences and their translations by both systems in random order. Table 5 reports relative results averaged across the 3 raters. Big 12-2 outperforms Big 6-6 in 3 out of 4 language directions. Contrary to Kong et al. (2021) and according to both human evaluation and automatic metrics, our singleshallow-decoder model performs at least as well as the baseline model. 18 16 BLEU consistency measures the similarity between the translations by the same model of the clean sentence and its noised version. 17 We ensure that the source sentences are original text in the corresponding language to avoid biased evaluation results due to Translationese. 18 Note that our human evaluation results are only on highresource languages, but Kong et al. (2021) observed the largest BLEU drop on high-resource languages.\n\nModel 12-2 > 6-6 12-2 = 6-6 12-2 < 6-6 EN\u2192FR 26% 51% 23% FR\u2192EN 25% 51% 24% EN\u2192DE 31% 44% 25% DE\u2192EN 24% 50% 26% A.1 Position of the language code Table 6 analyzes the impact of the language code position on BLEU performance. With the Base 6-6 architecture, decoder-side codes perform approximately as well as encoder-side codes (except for zeroshot translation). However, with the Base 12-2 architecture, decoder-side codes result in a noticeable drop in performance in most directions. Indeed, when the lang code is on the source side, the deep encoder knows the target language and can start \"translating.\" When it is on the target side, the encoder has",
            "score": 0.47231085966608694,
            "section_title": "Human evaluation",
            "char_start_offset": 21863,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043212890625
        },
        {
            "corpus_id": "248478169",
            "title": "How Robust is Neural Machine Translation to Language Imbalance in Multilingual Tokenizer Training?",
            "text": "The second two columns of Figure 1 shows how the translation results change as the English data percentage changes. The first row (a) shows spBLEU and chrF scores of the 4 Latin languages, while the second row (b) are those of the 3 non-Latin languages. We obtain the following takeaways. \n\nNMT performance is quite robust to language imbalance especially when languages share scripts. It can be observed from Figure 1 (a) that the performance stays quite stable across all English percentages for Latin languages. Performance drops only happen for English to Icelandic (en-is) and English to Danish (en-da) at extremely high English percentages (\u226599.999%), i.e., only 20 Icelandic or Danish sentences are in the 2M tokenizer training data. And it still does not affect the translation performances of is-en and da-en. Differently, in Figure 1 (b), the performance is less stable for non-Latin languages, but drops still happen when the English percentage is \u226590%. English to Chinese (en-zh) drops at English=90%. English to Tamil (en-ta) 7and English to Greek (en-el) both drop at English=99.9%. Similarly, into-English directions are more stable and get worse later (at higher English percentages). Surprisingly, in both (a) and (b), the translation performance usually stays stable or drops less significantly as the English percentage decreases to 0%. Better performance is often achieved when languages are more balanced. Out of the 14 translation directions, 12 directions get the best spBLEU scores between English=10% to English=90%. We evaluate the Pearson correlation between spBLEU scores and data ratios of two languages. The data ratio is 1 when English=50%, and it is 0 when English=0% or 100%, i.e., the more balanced the two languages are, the higher the data ratio is. The average correlation across 14 directions is 0.38 (moderate correlation (Cohen, 1988)). Thus, we are more likely to get a good performance when languages are more equally sampled. \n\nEnglish can \"never\" be starved.",
            "score": 0.47141201531202115,
            "section_title": "Translation Results",
            "char_start_offset": 15892,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 253
                },
                {
                    "start": 254,
                    "end": 288
                },
                {
                    "start": 291,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 657
                },
                {
                    "start": 658,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 964
                },
                {
                    "start": 965,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1200
                },
                {
                    "start": 1201,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1541
                },
                {
                    "start": 1542,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1876
                },
                {
                    "start": 1877,
                    "end": 1968
                },
                {
                    "start": 1971,
                    "end": 2002
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.044586181640625
        },
        {
            "corpus_id": "271962771",
            "title": "AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark",
            "text": "Detecting biases in natural language understanding (NLU) for African American Vernacular English (AAVE) is crucial to developing inclusive natural language processing (NLP) systems. To address dialect-induced performance discrepancies, we introduce AAVENUE (AAVE Natural Language Understanding Evaluation), a benchmark for evaluating large language model (LLM) performance on NLU tasks in AAVE and Standard American English (SAE). AAVENUE builds upon and extends existing benchmarks like VALUE, replacing deterministic syntactic and morphological transformations with a more flexible methodology leveraging LLM-based translation with few-shot prompting, improving performance across our evaluation metrics when translating key tasks from the GLUE and SuperGLUE benchmarks. We compare AAVENUE and VALUE translations using five popular LLMs and a comprehensive set of metrics including fluency, BARTScore, quality, coherence, and understandability. Additionally, we recruit fluent AAVE speakers to validate our translations for authenticity. Our evaluations reveal that LLMs consistently perform better on SAE tasks than AAVE-translated versions, underscoring inherent biases and highlighting the need for more inclusive NLP models.",
            "score": 0.47129888741808257,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "273695860",
            "title": "Danoliteracy of Generative, Large Language Models",
            "text": "Most broadly reported GLLM evaluations are only or primarily performed on examples in English. Approaches to evaluate lesser-resourced languages include both attempts to compile massively multilingual benchmarks either by automatic translation (Lai et al., 2023) or dataset curation (Ahuja et al., 2023). \n\nOther approaches focus on one language exclusively in attempts to evaluate GLLM language performance beyond surface-level lexical or syntactical literacy. Using this method, practitioners can align scenario domain, cultural content, and realworld facts with the setting of the language, though a lack of relevant data can be problematic (Liu et al., 2023). \n\nSpecifically in Danish, the comprehensive Scan-dEval benchmark, which packages scenarios across eight languages divided into NLU and NLG leaderboards, implements evaluation on GLLMs in Danish on eight NLG scenarios with some overlap in dataset sources with this work (Nielsen, 2023).",
            "score": 0.47087027692551386,
            "section_title": "Low-resource NLP Evaluation",
            "char_start_offset": 4428,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 304
                },
                {
                    "start": 307,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 663
                },
                {
                    "start": 666,
                    "end": 949
                }
            ],
            "ref_mentions": [
                {
                    "start": 283,
                    "end": 303,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 933,
                    "end": 948,
                    "matchedPaperCorpusId": "257913673"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0738525390625
        },
        {
            "corpus_id": "269987821",
            "title": "Programming Language Models in Multilingual Settings",
            "text": "The rapid adaptation of Large Language Models (LLMs) to coderelated tasks has shown that they are capable of solving a wide variety of programming tasks such as code completion, summarization, translation, and interpreting source code [1][2][3][4][5][6][7]. Many of the most modern models are also evaluated on multiple languages when testing their programming capabilities, often showing a difference in performance based on the language [8]. Although this performance gap is known, little research has been done on the source of this discrepancy. \n\nIn the context of multilingual models, research indicates that training with a mix of languages may be beneficial, especially for resource-scarce languages [4,8,9]. Yet, these assertions often lack specific references; they are based on theories about the models' capacity to generalize across languages, form abstract concepts internally, and apply these concepts to various languages. \n\nWe propose to approach this problem from two perspectives. We will first analyze the internal states of the model and how shared information may have an impact on performance. Then we will analyze the representations of outputs generated by models. Beginning with an overview of differences between settings and languages [10], and expanding the work to identify which characteristics of languages contribute the most to differences in representations. \n\nMore specifically we address the hypothesis There is a theoretical limitation that prevents large language models from performing optimally in a multilingual setting, by answering four Research Questions (RQ) that align with the areas of interest as follows: \n\nRQ1 Do large language models trigger activation in distinct areas of the model when processing various programming languages? RQ2 Does the inclusion of multiple languages in the training data lead to negative interference when a large language model generates predictions? RQ3 Do the variations in token representations produced by models across different languages adversely affect their multilingual performance? RQ4 What characteristics of languages are important to evaluate when selecting languages for pre-training and fine-tuning?",
            "score": 0.4708353478915398,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 548
                },
                {
                    "start": 551,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 937
                },
                {
                    "start": 940,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1392
                },
                {
                    "start": 1395,
                    "end": 1653
                },
                {
                    "start": 1656,
                    "end": 1781
                },
                {
                    "start": 1782,
                    "end": 1928
                },
                {
                    "start": 1929,
                    "end": 2070
                },
                {
                    "start": 2071,
                    "end": 2193
                }
            ],
            "ref_mentions": [
                {
                    "start": 238,
                    "end": 241,
                    "matchedPaperCorpusId": "246823166"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "211171605"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "237386541"
                },
                {
                    "start": 250,
                    "end": 253,
                    "matchedPaperCorpusId": "255416208"
                },
                {
                    "start": 439,
                    "end": 442,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 710,
                    "end": 712,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 712,
                    "end": 713,
                    "matchedPaperCorpusId": "218673683"
                },
                {
                    "start": 1262,
                    "end": 1266,
                    "matchedPaperCorpusId": "261214824"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0384521484375
        },
        {
            "corpus_id": "233228699",
            "title": "The Semiotics of Test Design: Conceptual Framework on Optimal Item Features in Educational Assessment Across Cultural Groups, Countries, and Languages",
            "text": "At another level of complexity, the ways in which vocabulary is addressed in testing illustrates the gap between what is known about language and how that knowledge is incorporated in testing practices. While there are sources that document the frequency of words in English (e.g., Nagy and Anderson, 1984;Davies and Gardner, 2010;Nation, 2014), that information is not used routinely to decide the wording and minimize the lexical complexity in items not intended to assess vocabulary knowledge. \n\nAt a higher level of complexity, issues in test translation illustrate the challenges of testing diverse populations in different languages, mainly because translation may alter the nature of the constructs assessed by items (Hambleton, 2005;Winter et al., 2006;Arffman, 2013). A great deal of the effort and time invested in the process of assessment development concerns refining the wording of items to ensure that examinees understand them as their developers intend (Abedi, 2006(Abedi, , 2016). Yet, compared to the time allocated for test development, assessment programs allocate considerably less time for test translation and adaptation (Solano-Flores, 2012). Tight timelines seriously limit the opportunities for examining students' interpretations of translated items (e.g., through verbal protocols and cognitive interviews) and conducting differential item functioning analyses with the purpose of detecting cultural bias. These practical constraints underscore the need for improved judgmental translation review procedures (Allalouf, 2003;Zhao and Solano-Flores, 2021). \n\nAn emerging realization concerning language resources is that language issues in testing cannot be effectively addressed without taking into consideration non-textual ways of representing information (Kopriva and Wright, 2017). Moreover, a broader view of translation as both a meaning making and meaning taking enterprise, reveals the need to recognize multiple forms of translation as intrinsic to the act of representing information (Marais, 2019, p. 122). This broader view appears to be consistent with the ultimate goal of ensuring construct equivalence across cultures and languages. A wealth of possibilities emerge. For example, in addition to replacing text in one language with text in another language, should translation concern semiotic modalities other than text (e.g., replacing illustrations used in tests)?",
            "score": 0.4703530822912875,
            "section_title": "Language Resources",
            "char_start_offset": 17160,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1583
                },
                {
                    "start": 1586,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2176
                },
                {
                    "start": 2177,
                    "end": 2210
                },
                {
                    "start": 2211,
                    "end": 2410
                }
            ],
            "ref_mentions": [
                {
                    "start": 282,
                    "end": 306,
                    "matchedPaperCorpusId": "144631813"
                },
                {
                    "start": 331,
                    "end": 344,
                    "matchedPaperCorpusId": "215924577"
                },
                {
                    "start": 741,
                    "end": 761,
                    "matchedPaperCorpusId": "58896310"
                },
                {
                    "start": 761,
                    "end": 775,
                    "matchedPaperCorpusId": "143638317"
                },
                {
                    "start": 1537,
                    "end": 1553,
                    "matchedPaperCorpusId": "145599957"
                },
                {
                    "start": 1553,
                    "end": 1581,
                    "matchedPaperCorpusId": "229426279"
                },
                {
                    "start": 1786,
                    "end": 1812,
                    "matchedPaperCorpusId": "158726486"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1668701171875
        },
        {
            "corpus_id": "258486951",
            "title": "Toward Cultural Bias Evaluation Datasets: The Case of Bengali Gender, Religious, and National Identity",
            "text": "Bias, in the context of computing systems, is where sociotechnical systems systematically and unfairly discriminate against certain individuals or social groups in favor of others (Friedman and Nissenbaum, 1996;Blodgett et al., 2020). People often identify through their perceived memberships in certain groups (Tajfel, 1974). While computational linguists have studied gender and racial biases (Kiritchenko and Mohammad, 2018), systematic discrimination of language technologies based on various cultural factors like religion and nationality has received little attention. Moreover, critical studies examining these biases mostly focused on NLP systems in a handful of languages, whereas many languages with sizeable numbers of speakers do not have enough resources like datasets to pursue similar studies. \n\nAccording to (Joshi et al., 2020), whereas 0.28% of global languages (e.g., English, Spanish, Japanese) reap benefits from NLP breakthroughs, 88.38% of languages have virtually no data to use. They also found that while English and Bengali are the third and sixth largest languages by the number of native speakers (Lane, 2023), the former has hundreds of times more resources than the latter in Linguistic Data Consortium, Language Resources and Evaluation, and Wikipedia, and thou-sands more resources in the Web overall. The difference in available resources like labeled datasets impedes the progress of critical studies aimed at fairness, transparency, and identifying biases in such under-represented languages. In the absence of native resources, many of these tools first translate non-English text to English for downstream NLP tasks, creating the potential for colonial imposition on indigenous languages (Bird, 2020). \n\nOne of the main objectives of this work is to highlight and address the lack of focus on two vital cultural factors such as religion and nationality, that shape people's cultural identity. In addition to its large number of native speakers and a thriving cultural community online, the religious diversity of this ethnolinguistic group, with 71% Muslims and 28% Hindus, and their postcolonial division into two nationalities, Bangladeshi (59%) and Indian (38%) makes the Bengali language an interesting case for developing a cultural bias evaluation dataset (BSB, 2022;India, 2011).",
            "score": 0.4698552069380997,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 574
                },
                {
                    "start": 575,
                    "end": 808
                },
                {
                    "start": 811,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1739
                },
                {
                    "start": 1742,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2324
                }
            ],
            "ref_mentions": [
                {
                    "start": 180,
                    "end": 211,
                    "matchedPaperCorpusId": "207195759"
                },
                {
                    "start": 311,
                    "end": 325,
                    "matchedPaperCorpusId": "143666442"
                },
                {
                    "start": 1726,
                    "end": 1738,
                    "matchedPaperCorpusId": "227231541"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10107421875
        },
        {
            "corpus_id": "277780370",
            "title": "MultiLoKo: a multilingual local knowledge benchmark for LLMs spanning 31 languages",
            "text": "Notwithstanding the increasing multinational deployment of LLMs in many parts of the world, adequately evaluating their multilinguality remains a challenging enterprise. Only in part is this due to the scarcity of high-quality and broad-coverage multilingual benchmarks for LLM: perhaps a more pressing issue is that the benchmarks that are frequently used for multilingual evaluation virtually all consist of translated English data. While using completely parallel data has its advantages, using translated English data imposes an English-centric bias on the content of the benchmarks, implying that even if the benchmark evaluates multilinguality on the surface, it does not in content. In our work, we aim to address this by presenting MultiLoKo, a multilingual benchmark spanning 31 languages that combines the best of both worlds. MultiLoKo contains 500 questions targetting locally relevant knowledge for 31 languages, separately sourced for each language with a protocol specifically designed to ensure local relevance of the question topics. It is also fully parallel, because it contains human-authored translations of the non-English partitions into English and vice versa. As such, it allows to study various questions related to multilinguality, transfer and multilingual benchmark creation. To prevent quick overfitting and inadvertent contamination, we release a development set of the benchmark, while the test set of the benchmarks remains private, at least for the near future. \n\nWe use MultiLoKo to analyse 4 base and 7 chat models marketed to be multilingual. We find that, of the models we consider, the best performing model is Gemini 2.0 Flash, with an average performance of 34.4 points, and an almost 35 point gap between the best and the worst language, followed by Llama 3.1 405B and GPT4-o, which are close contenders in terms of average performance but both have substantially higher language gaps (39 and 49 points). Generally, scores are better when questions are asked in the language to which they are relevant, indicating suboptimal knowledge transfer between languages, a result that is mirrored by low per-sample consistency across question language. \n\nOn a meta-level, we study the relevance of using locally sourced data as opposed to translated English data as well as whether it matters if translations are authored by humans or machines.",
            "score": 0.46913993296138123,
            "section_title": "Conclusion",
            "char_start_offset": 36602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1184
                },
                {
                    "start": 1185,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2186
                },
                {
                    "start": 2189,
                    "end": 2378
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "270226200",
            "title": "JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models",
            "text": "The bias of Large Language Models (LLMs) leads to the reproduction of bias in downstream tasks such as language generation. As discussed by Blodgett et al. (2020), there are various biases in NLP. One of these problematic biases is social bias and stereotyping behavior. For instance, stereotyping behavior observed in text generation has the potential to harm minorities who read the text, and using such biased texts as training data introduces additional bias into the subsequent LLMs (Gehman et al., 2020;Bender et al., 2021). \n\nVarious social bias benchmarks have been provided (Rudinger et al., 2018;Zhao et al., 2018;Nangia et al., 2020;Li et al., 2020;Nadeem et al., 2021;Dhamala et al., 2021;Parrish et al., 2022;N\u00e9v\u00e9ol et al., 2022;Huang and Xiong, 2024;Jin et al., 2024;Kaneko et al., 2024). However, most of them are constructed in English, and benchmarks in languages other than English are not yet fully developed. In addition, some LLMs have recently been developed specifically for Japanese (LLM-jp et al., 2024;Fujii et al., 2024), but it remains unclear the extent to which Japanese LLMs exhibit biases against a range of social categories. \n\nTo evaluate social bias and stereotypes in LLMs, question-answering (QA) tasks have been widely applied. Bias benchmarks for various languages in QA tasks have been proposed. The Bias Benchmark for QA (BBQ) is for English (Parrish et al., 2022). Similarly, a Chinese benchmark (CBBQ, Huang and Xiong, 2024) and a Korean benchmark (KoBBQ, Jin et al., 2024) have been developed. These QA benchmarks have hand-written contexts that target attested social biases against several different socially relevant categories. The categories of bias measurement have cultural relativity. For example, BBQ is rooted in US culture. However, there are cultural differences in the way socioeconomic status and religion are perceived.",
            "score": 0.4691029570562649,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1158
                },
                {
                    "start": 1161,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1878
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 162,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 488,
                    "end": 509,
                    "matchedPaperCorpusId": "221878771"
                },
                {
                    "start": 509,
                    "end": 529,
                    "matchedPaperCorpusId": "262580630"
                },
                {
                    "start": 583,
                    "end": 606,
                    "matchedPaperCorpusId": "13756572"
                },
                {
                    "start": 606,
                    "end": 624,
                    "matchedPaperCorpusId": "4952494"
                },
                {
                    "start": 624,
                    "end": 644,
                    "matchedPaperCorpusId": "222090785"
                },
                {
                    "start": 644,
                    "end": 660,
                    "matchedPaperCorpusId": "222141056"
                },
                {
                    "start": 660,
                    "end": 680,
                    "matchedPaperCorpusId": "215828184"
                },
                {
                    "start": 680,
                    "end": 701,
                    "matchedPaperCorpusId": "231719337"
                },
                {
                    "start": 701,
                    "end": 722,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 722,
                    "end": 742,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 742,
                    "end": 764,
                    "matchedPaperCorpusId": "259274987"
                },
                {
                    "start": 1028,
                    "end": 1047,
                    "matchedPaperCorpusId": "269449465"
                },
                {
                    "start": 1383,
                    "end": 1405,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1445,
                    "end": 1467,
                    "matchedPaperCorpusId": "259274987"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1397705078125
        },
        {
            "corpus_id": "272986648",
            "title": "AVIATE: Exploiting Translation Variants of Artifacts to Improve IR-based Traceability Recovery in Bilingual Software Projects",
            "text": "To mitigate the potential term translation inconsistency which has been exemplified in Figure 1, we introduce four mainstream translators to complete the translation task. They are two Manyto-Many multilingual translation models (NLLB-1.3B model [15] and M2M-100-12B model [18] and two web-based translation services (Google Translate [2] and Tencent Translate [5]). Among them, the NLLB-1.3B model is designed for low-resource natural language training and can support translation between 202 different languages. The M2M-100-12B model is a non-English-Centric translation model and can translate between 100 languages. Both of them have been widely used in various research domains [48]. On the other hand, Google Translate and Tencent Translate are two widely used web-based translation services known for their accuracy and efficiency. Wu et al. 's research [45] indicates Google Translation has addressed approximately 60% of previously known translation errors in popular languages, significantly enhancing translation performance. Tencent Translate has also demonstrated outstanding performance in English-to-Chinese translation [41,44]. \n\nWe use regular expressions to identify sentences that contain non-English characters, and translate them into pure English sentences using the above four translators. We refer to the different translation versions for the same sentence as Translation Variants. In our approach, each original non-pure-English sentence corresponds to four translation variants. The pure English texts can be considered as a special case where there is only one variant. Through this translation process, we can obtain a pure English corpus. We denote the corpus obtained from issues as   and the corpus obtained from commits as   .",
            "score": 0.4687853724473311,
            "section_title": "Translating with Multiple Translators",
            "char_start_offset": 15082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1144
                },
                {
                    "start": 1147,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1760
                }
            ],
            "ref_mentions": [
                {
                    "start": 273,
                    "end": 277,
                    "matchedPaperCorpusId": "224814118"
                },
                {
                    "start": 1136,
                    "end": 1140,
                    "matchedPaperCorpusId": "245855667"
                },
                {
                    "start": 1140,
                    "end": 1143,
                    "matchedPaperCorpusId": "229365690"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03802490234375
        },
        {
            "corpus_id": "270688744",
            "title": "Unveiling the Spectrum of Data Contamination in Language Models: A Survey from Detection to Remediation",
            "text": "Most research on task-level contamination is conducted in English.However, in addition to task-level contamination, Blevins and Zettlemoyer (2022) also explore cross-lingual contamination, which refers to when the models are tested for their cross-lingual abilities.For example, pre-training corpora often contain significant amounts of non-English text.If a model is trained on these corpora and then tested on, for example, a Chinese benchmark, the setting is no longer testing the pure cross-lingual generalization, as the model has already been exposed to Chinese text during training.Their research indicates that the corpora utilized for pre-training these models include a significant amount of non-English text, albeit less than 1% of the total dataset.This seemingly small percentage equates to hundreds of millions of foreign language tokens in large datasets.The study further reveals that these minor proportions of non-English data considerably enhance the models' capability for cross-language knowledge transfer.There is a direct correlation between the models' performance in target languages and the volume of training data available in those languages.In general, more work is required to better understand the impact of contamination in cross-lingual settings.For example, many benchmarks in one target language are direct translations from a benchmark in another source language, and contamination may still occur if models have been exposed to similar contexts in the source language, even though the target evaluation is in another language.",
            "score": 0.4686810110679757,
            "section_title": "Cross-lingual Contamination",
            "char_start_offset": 10360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 66
                },
                {
                    "start": 66,
                    "end": 266
                },
                {
                    "start": 266,
                    "end": 354
                },
                {
                    "start": 354,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 761
                },
                {
                    "start": 761,
                    "end": 870
                },
                {
                    "start": 870,
                    "end": 1027
                },
                {
                    "start": 1027,
                    "end": 1170
                },
                {
                    "start": 1170,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1563
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.382080078125
        },
        {
            "corpus_id": "273404303",
            "title": "Bias in the Mirror : Are LLMs opinions robust to their own adversarial attacks ?",
            "text": "The study of biases in large language models (LLMs) has been extensively explored, particularly from political and cultural perspectives. Tests like the Political Compass (Feng et al., 2023;Rozado, 2023;Rutinowski et al., 2024), the Political Coordinates test (Motoki et al., 2024;Rozado, 2024), and the Pew American Trends Panel (Santurkar et al., 2023) have been used to measure political biases. In cultural settings, approaches like the Cultural Alignment test (Cao et al., 2023;Masoud et al., 2023) assess how closely models align with cultural norms. A limitation of these methods is their tendency to force models to take a stance, often by using multiple-choice options, which prevents neutral or nuanced responses. This design can exaggerate biases, as the models are not given the option to provide more balanced or uncertain answers. \n\nMoreover, most studies test model bias with limited robustness checks, typically repeating experiments only a few times. This lack of repetition can overemphasize the detected biases. Alternative methods have been used to evaluate political or cultural bias, notably Bang et al. (2024), who propose assessing bias in models on specific topics by using positive and negative news article titles as anchors and measuring distances to naturally generated titles. Similarly, Naous et al. (2023) create a benchmark dataset to measure cultural biases in LLMs using masked prompts based on Arabic cultural entities, showing that LLMs favor Western entities even in Arab contexts. \n\nIn addition to evaluating biases, other research has demonstrated how easily models can adopt harmful behavior through specific conditioning or fine-tuning (Taubenfeld et al., 2024;Feng et al., 2023). This research suggests that further pretraining can cause a language model to acquire new biases, or that models exposed to biased descriptions may initially shift but eventually revert to their original viewpoints after continued interaction. \n\nIn all cases, the consistency of models plays a key role in bias assessment and understanding how they process information over time. Elements like question phrasing, the sequence of discussions, and the criteria for evaluating responses all significantly impact the evaluation outcomes.",
            "score": 0.46855147369330785,
            "section_title": "Surveying LLMs",
            "char_start_offset": 4272,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 723
                },
                {
                    "start": 724,
                    "end": 844
                },
                {
                    "start": 847,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1519
                },
                {
                    "start": 1522,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1966
                },
                {
                    "start": 1969,
                    "end": 2102
                },
                {
                    "start": 2103,
                    "end": 2256
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 203,
                    "matchedPaperCorpusId": "257342280"
                },
                {
                    "start": 203,
                    "end": 227,
                    "matchedPaperCorpusId": "258180220"
                },
                {
                    "start": 260,
                    "end": 281,
                    "matchedPaperCorpusId": "257372256"
                },
                {
                    "start": 281,
                    "end": 294,
                    "matchedPaperCorpusId": "267412830"
                },
                {
                    "start": 330,
                    "end": 354,
                    "matchedPaperCorpusId": "257834040"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08038330078125
        },
        {
            "corpus_id": "271270611",
            "title": "INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages",
            "text": "Large Language Models (LLMs) perform well on unseen tasks in English, but their abilities in non English languages are less explored due to limited benchmarks and training data. To bridge this gap, we introduce the Indic QA Benchmark, a large dataset for context grounded question answering in 11 major Indian languages, covering both extractive and abstractive tasks. Evaluations of multilingual LLMs, including instruction finetuned versions, revealed weak performance in low resource languages due to a strong English language bias in their training data. We also investigated the Translate Test paradigm,where inputs are translated to English for processing and the results are translated back into the source language for output. This approach outperformed multilingual LLMs, particularly in low resource settings. By releasing Indic QA, we aim to promote further research into LLMs question answering capabilities in low resource languages. This benchmark offers a critical resource to address existing limitations and foster multilingual understanding.",
            "score": 0.4684733175917199,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1500244140625
        },
        {
            "corpus_id": "17206686",
            "title": "Does \u2018well-being\u2019 translate on Twitter?",
            "text": "In this work, we investigated how well expressions of subjective well-being translate across English and Spanish Twitter, finding that the source language models performed substantially better than the machine translated versions. Moreover, manually correcting translation errors in the top 250 terms of the lexica did not improve model performance, suggesting that meaningful cultural information was lost in translation.\n\nOur findings suggest that further work is needed to understand when automatic translation of language-based models will lead to competitive sentiment translation on social media and how such translations can be improved. Cultural differences seem more important than language differences, at least for the tasks we studied here. We expect that language indicators of personality and emotion will similarly translate poorly, but that remains to be studied.",
            "score": 0.46808974287346905,
            "section_title": "Conclusion",
            "char_start_offset": 16610,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1634521484375
        },
        {
            "corpus_id": "276106987",
            "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
            "text": "Recently, several Large Language Models (LLMs) have been made available that are trained on Italian data, and several others have been announced for the next months. Those models (e.g., LLaMAntino [1], the Minerva family1 , Italia2 ) are released as open source, although with different licenses, and are all available through the Hugging Face (HF) platform 3 . In this context, performance evaluation of such models through appropriate benchmarking is becoming crucial. However, in the current evaluation practice, LLMs for Italian are mostly evaluated on benchmarks automatically translated from English, which poses critical issues: \n\n\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In [2], several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark [3], several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results. \n\nAt the time of writing, the landscape of evaluation is rapidly evolving. As an example, the Ita-bench 4 is now available on Hugging Face, including tasks originally developed in Evalita and adapted to LLMs. In addition, the Calamita initiative [4] has produced several benchmarks for Italian LLMs, opening new options for a more comprehensive evaluation.",
            "score": 0.4679061858296274,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 166,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1226
                },
                {
                    "start": 1227,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1570
                },
                {
                    "start": 1571,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2133
                },
                {
                    "start": 2134,
                    "end": 2281
                }
            ],
            "ref_mentions": [
                {
                    "start": 797,
                    "end": 800,
                    "matchedPaperCorpusId": "275357255"
                },
                {
                    "start": 1424,
                    "end": 1427,
                    "matchedPaperCorpusId": "221516475"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5693359375
        },
        {
            "corpus_id": "277451686",
            "title": "Navigating Uncertainties: Understanding How GenAI Developers Document Their Models on Open-Source Platforms",
            "text": "Characteristics are Feasible to Measure and Report Faithfully. Finally, participants also expressed uncertainty about which model characteristics are feasible to measure and report. We find that these perceptions can lead to documentation decisions that prioritize reporting established, technically tractable measurements while potentially overlooking their limitations. This is influenced by two factors: whether they perceive the characteristic as technically feasible to evaluate and whether there exist \"legitimate\" (i.e., widely adopted) benchmarks for measurement. P7's approach to choosing what to report demonstrates this consideration: he chose to report his vision-language model's performance on Fairface [47], an established benchmark for detecting gender, race and age bias, emphasizing the technical feasibility of the test \"it is pretty easy to run the evaluation -you just need to integrate that into the code\", and how it is widely adopted: \"it was used by other people, so that in this way you can compare your model against them. \" However, when faced with hallucinations in vision-language models, he views it as not documentable because he ''rarely sees companies using' such benchmarks\" and therefore perceives it a \"very hard problem to solve\" in his domain knowledge of vision-language models. \n\nSimilarly, P3 demonstrated a pragmatic approach to those decisions based on perceived technical feasibility and quantifiability. As he stated: \"if one were to pick a factor, I would pick whichever is the easiest and most quantifiable to document. \" P3 considers some of the model's reliability as worthy of documentation because these factors are \"studied really nicely and there already exist quantitative measures for it, \" while viewing model characteristics such as toxicity or bias as \"way too hard\" to document because \"bias and toxicity are like sarcasm. Sarcasm across cultures is different. it is just multidimensional... there is cultural angle to it, there's geographic angle to it... race could also become a problem. \" Although research shows that toxicity and bias can be measured across cultures [23], developers' documentation decisions for such evaluations appear driven more by perceived feasibility than by the actual importance of these risks.",
            "score": 0.46764343061957614,
            "section_title": "Uncertainties around What Model",
            "char_start_offset": 20453,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 62
                },
                {
                    "start": 63,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 1049
                },
                {
                    "start": 1050,
                    "end": 1318
                },
                {
                    "start": 1321,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2284
                }
            ],
            "ref_mentions": [
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "199577425"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.02056884765625
        },
        {
            "corpus_id": "259202876",
            "title": "On Evaluating Multilingual Compositional Generalization with Translated Datasets",
            "text": "Even the premise of parsing questions to Wikidata queries leads to linguistic and cultural bias, as Wikidata is biased towards English-speaking cultures (Amaral et al., 2021). As Cui et al. (2022) argue, speakers of other languages may care about entities and relations that are not represented in Englishcentric data (Liu et al., 2021b;Hershcovich et al., 2022a). For this reason and for the linguistic reasons we demonstrated in this paper, creating CG benchmarks natively in typologically diverse languages is essential for multilingual information access and its evaluation.\n\nAs we mentioned in \u00a74.2, our translation system fails to deal with ambiguities beyond grammar and thus generates wrong translations for a few samples (less than 0.31%). Moreover, although the dataset can be potentially augmented with low-resource languages and in general other languages through the translation framework, adequate knowledge will be required to expand rules for the specific target languages.\n\nWith limited computational resources, we are not able to further investigate the impact of parameters and model sizes of multilingual PLM as our preliminary results show significant performance gaps between PLMs.",
            "score": 0.4675825949243874,
            "section_title": "Limitations",
            "char_start_offset": 23627,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 153,
                    "end": 174,
                    "matchedPaperCorpusId": "237571343"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.143798828125
        },
        {
            "corpus_id": "260334475",
            "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
            "text": "The evaluation of social bias and stereotypes in generative language models through question answering (QA) has quickly gained importance as it can help estimate bias in downstream tasks. For English, BBQ (Parrish et al., 2022) has been widely used in evaluating inherent social bias within large language models (LLMs) through the QA task (Liang et al., 2022;Srivastava et al., 2023). Similarly, there has been an attempt to develop a Chinese benchmark (CBBQ) (Huang and Xiong, 2023). However, there are currently no benchmarks for other languages (and their respective cultural contexts), including Korean. \n\nBBQ is rooted in US culture, and it is quite difficult to apply BBQ to other languages and cultural contexts directly. Cultural differences can affect the contexts, types, and targets of stereotypes. For example, the stereotype of drug use is associated with low socio-economic status (SES) in BBQ, while it is associated with high SES in Korea, as shown in Figure 1. Moreover, the quality of translation can impact the QA performance of LMs. Several studies (Lin et al., 2021;Ponti et al., 2020) have highlighted the serious shortcomings of relying solely on machine-translated datasets. Therefore, constructing benchmarks to assess bias in a different cultural context requires a more sensitive and culturally aware approach. \n\nIn this paper, we propose a process for developing culturally adaptive datasets and present KoBBQ (Korean Bias Benchmark for Question Answering) that reflects the situations and social biases in South Korea. Our methodology builds",
            "score": 0.4671351618800658,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1338
                },
                {
                    "start": 1341,
                    "end": 1548
                },
                {
                    "start": 1549,
                    "end": 1571
                }
            ],
            "ref_mentions": [
                {
                    "start": 205,
                    "end": 227,
                    "matchedPaperCorpusId": "239010011"
                },
                {
                    "start": 1070,
                    "end": 1088,
                    "matchedPaperCorpusId": "235421949"
                },
                {
                    "start": 1088,
                    "end": 1107,
                    "matchedPaperCorpusId": "218470125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2186279296875
        },
        {
            "corpus_id": "258960373",
            "title": "Red teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity",
            "text": "Accordingly, the metrics for demographic representation bias are designated as RP race and RP gender , while the metrics for stereotypical associations are designated as ST race and ST gender . These metrics take into account the frequency of stereotyped phrases appearing in conjunction with demographic terms across various generations of models. \n\n2) Result Analysis: Table I presents a comparative analysis of BBQ, utilizing InstructGPT and GPT-3 as few-shot language models, against the zero-shot ChatGPT. The results indicate that ChatGPT exhibits superior performance in terms of EM    3) Beyond Benchmarking: Despite the noteworthy performance of ChatGPT on current benchmarks for social bias, it should be acknowledged that these measurements only encompass a limited subset of potential bias aspects. As previously outlined in Section II, user feedback on Twitter has indicated the presence of biases in other domains, such as multilingual comprehension and code generation. In light of this, we have undertaken three case studies in order to determine the extent to which more biases are presented in ChatGPT. \n\na) Case Study 1: Language Understanding To understand the bias in language, we provide an example in Table III, where the user asks ChatGPT \"Which country does Kunashir Island belong to?\" in Japanese, Russian and English. From the fact, these islands are disputed territories. Based on the observation, ChatGPT shows biased opinions in different languages. Even though when the question is asked in English, ChatGPT reflects biases to different controversial names of the islands. \n\nThe findings promote our further investigation on language understanding. As machine translation is one of the most traditional tasks for multilingual understanding, we let ChatGPT perform zero-shot machine translation using FLORES-200 [3] devtest set. We choose three low-resource languages and three high-resource languages, as defined in the dataset. For each selected language, ChatGPT is required to translate the sentence to English. To assess the translation quality, we use sacreBLEU [49] and its variant, spBLEU [50], to measured n-gram word overlaps between language model's predication and reference. In addition, we also measure the character n-gram overlaps via ChrF [51], without segmenting the sentence into words.",
            "score": 0.46704202256923777,
            "section_title": "III. Diagnosing AI ethics Of ChatGPT",
            "char_start_offset": 20097,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 348
                },
                {
                    "start": 351,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 984
                },
                {
                    "start": 985,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1603
                },
                {
                    "start": 1606,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 1959
                },
                {
                    "start": 1960,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2217
                },
                {
                    "start": 2218,
                    "end": 2335
                }
            ],
            "ref_mentions": [
                {
                    "start": 1842,
                    "end": 1845,
                    "matchedPaperCorpusId": "250425961"
                },
                {
                    "start": 2098,
                    "end": 2102,
                    "matchedPaperCorpusId": "13751870"
                },
                {
                    "start": 2127,
                    "end": 2131,
                    "matchedPaperCorpusId": "235358129"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09039306640625
        },
        {
            "corpus_id": "276580554",
            "title": "Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers",
            "text": "Scientific research is inherently global. However, the vast majority of academic journals are published exclusively in English, creating barriers for non-native-English-speaking researchers. In this study, we leverage large language models (LLMs) to translate published scientific articles while preserving their native JATS XML formatting, thereby developing a practical, automated approach for implementation by academic journals. Using our approach, we translate articles across multiple scientific disciplines into 28 languages. To evaluate translation accuracy, we introduce a novel question-and-answer (QA) benchmarking method, in which an LLM generates comprehension-based questions from the original text and then answers them based on the translated text. Our benchmark results show an average performance of 95.9%, showing that the key scientific details are accurately conveyed. In a user study, we translate the scientific papers of 15 researchers into their native languages, finding that the authors consistently found the translations to accurately capture the original information in their articles. Interestingly, a third of the authors found many technical terms\"overtranslated,\"expressing a preference to keep terminology more familiar in English untranslated. Finally, we demonstrate how in-context learning techniques can be used to align translations with domain-specific preferences such as mitigating overtranslation, highlighting the adaptability and utility of LLM-driven scientific translation. The code and translated articles are available at https://hankleid.github.io/ProjectMundo.",
            "score": 0.46696995264702956,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0743408203125
        },
        {
            "corpus_id": "274149965",
            "title": "Training Bilingual LMs with Data Constraints in the Targeted Language",
            "text": "In this section we list some limitations of our work. \n\nLanguages included. Our primary focus is on English-German language training, as these two Germanic family languages share linguistic similarities (Lewis et al., 2015). German is one of the most well-represented languages in the mC4 dataset, facilitating model comparisons with varying amounts of German and English data. Furthermore, the availability of extensive public resources for German, including translation systems and translated evaluation data, further supports our emphasis on this language pair. We experimented with seven additional datasets including French, Spanish, Italian, Portuguese, Korean, Japanese, and Chinese. However, we note that there are many other languages within mC4 and more broadly which can benefit from having auxiliary English data for pretraining. Due to limited evaluation benchmarks and availability of target language data for comparison, we leave investigation for truly low-resource languages to future work. \n\nEvaluation data. Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models. Model size. Finally, this work studies three model sizes up to 3B models. We note that there are many standard benchmarks that can be evaluated at 1B-3B scale, however many more benchmarks and patterns can appear at larger model sizes. It is important future work to evaluate whether the results extend to larger scales including evaluating potential \"emergent behaviors\" as well as risks at larger scales (Wei et al., 2022).",
            "score": 0.4668278738467194,
            "section_title": "Limitations",
            "char_start_offset": 32229,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 53
                },
                {
                    "start": 56,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 224
                },
                {
                    "start": 225,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 690
                },
                {
                    "start": 691,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1523
                },
                {
                    "start": 1524,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1805
                },
                {
                    "start": 1806,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2157
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3251953125
        },
        {
            "corpus_id": "270620798",
            "title": "Data Contamination Can Cross Language Barriers",
            "text": "Considering a scenario where the budget is limited and we want a model with the best overall multilingual performance, in which single language should we conduct the continual pre-training? \n\nAs noted in \u00a7 3.2, contamination in non-English languages can improve performance on the English benchmark. We further extend the evaluation to non-English languages to assess the impact of con-  tamination on multilingual performance. Figure 5 shows that contaminating in French achieves the best average performance, indicating that French could be the best choice for continual pre-training. Surprisingly, English only scored 51.97, ranking second last in all languages. \n\nHence, investigating cross-lingual contamination can provide valuable perspectives for enhancing the unbalanced multilingual capabilities of LLMs. \n\n6 Related Work",
            "score": 0.4665932341142718,
            "section_title": "How to Localize LLMs for Non-English?",
            "char_start_offset": 21835,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 192,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 665
                },
                {
                    "start": 668,
                    "end": 814
                },
                {
                    "start": 817,
                    "end": 831
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0537109375
        },
        {
            "corpus_id": "269626686",
            "title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense",
            "text": "Large language models (LLMs) have demonstrated substantial commonsense understanding through numerous benchmark evaluations. However, their understanding of cultural commonsense remains largely unexamined. In this paper, we conduct a comprehensive examination of the capabilities and limitations of several state-of-the-art LLMs in the context of cultural commonsense tasks. Using several general and cultural commonsense benchmarks, we find that (1) LLMs have a significant discrepancy in performance when tested on culture-specific commonsense knowledge for different cultures; (2) LLMs\u2019 general commonsense capability is affected by cultural context; and (3) The language used to query the LLMs can impact their performance on cultural-related tasks.Our study points to the inherent bias in the cultural understanding of LLMs and provides insights that can help develop culturally-aware language models.",
            "score": 0.466568951680704,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1513671875
        },
        {
            "corpus_id": "265534357",
            "title": "How does ChatGPT-4 preform on non-English national medical licensing examination? An evaluation in Chinese language",
            "text": "ChatGPT's performance varies across languages due to different factors, primarily the quality and quantity of the training data available in each language [12]. English, as the language with the most abundant training data, typically yields the highest accuracy. However, our findings indicate that translating questions into English did not significantly improve ChatGPT's performance. This suggests that a strategy relying solely on English databases or translations might not be the most effective approach for enhancing ChatGPT's proficiency in handling medical tasks. There are two potential reasons for this observation: 1) Translation limitations: The process of translation can often lead to the loss of certain nuances or the inaccurate translation of specific terms, which could impact the AI's understanding of the question and subsequently its performance [13]. Additionally, some languages may have unique expressions or cultural contexts that are difficult to convey accurately in English. This can lead to potential misunderstandings or misinterpretations that could significantly influence the performance of language models like ChatGPT. 2) Different languages may have semantic and cultural differences, and certain unique expressions or cultural contexts in one language may be challenging to accurately convey in English. This can result in potential misunderstandings or misinterpretations, significantly influencing the performance of language models like ChatGPT. \n\nTherefore, in order to accurately assess the performance of an AI model like ChatGPT for a particular language or task, a more targeted evaluation may be necessary. This would involve conducting a thorough analysis of the AI's performance using a diverse range of tasks and data sources that are representative of the language in question.",
            "score": 0.46615751124331994,
            "section_title": "ChatGPT performs best in English, with accuracy affected by translation issues and data limitations in other languages",
            "char_start_offset": 14922,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1653
                },
                {
                    "start": 1654,
                    "end": 1828
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "259341896",
            "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
            "text": "In this work, we investigated gender biases in multilingual settings by proposing a bias evaluation dataset in 6 Indian languages. We further extended debiasing approaches like CDA and Self-Debiasing to work for languages beyond English and evaluated their effectiveness in removing biases across languages in MLMs. One of our key findings is that debiasing with English data might only provide a limited bias reduction in other languages and even collecting a limited amount of counterfactual data through translation can lead to substantial improvements when jointly trained with such data from similar languages. Finally, we showed that despite being effective on monolingual models, Self-Debiasing is limited in reducing biases in mul-tilingual models with often resulting in an increase in overall bias. We hope that our work will act as a useful resource for the community to build more inclusive technologies for all cultures.",
            "score": 0.4660205480500912,
            "section_title": "Conclusion",
            "char_start_offset": 15912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2093505859375
        },
        {
            "corpus_id": "258841694",
            "title": "Benchmarking Machine Translation with Cultural Awareness",
            "text": "In this study, we focus on the evaluation of cultural-specific items (CSIs). However, evaluating cultural awareness beyond individual entities also deserves further investigation. Besides CSIs, many other types of cultural errors persist in the translation process, such as those related to linguistic style and slang (Hershcovich et al., 2022). Our work aims to mitigate cultural errors by starting with CSIs, promoting advancements in culturally-aware machine translation datasets, models, and evaluation methods. This is crucial for enabling machine translation to play a larger role in cross-cultural communications. \n\nEvaluation by LLM Recent research has shown that GPT-4 demonstrates a high correlation with human experts in evaluating generation performance (Rafailov et al., 2023;Kocmi and Federmann, 2023;Li et al., 2024). However, using GPT-4 as an evaluator may still pose fairness issues due to internal biases and unbalanced language capabilities of LLMs. In this study, we aim to advance beyond traditional semantic alignment evaluation metrics to assess pragmatic translation quality in English-Chinese translations using GPT-4. Further investigation is needed to improve GPT-4's effectiveness as an translation evaluator. \n\nPrompting strategies We only try 4 prompting strategies in our study, due to our work's focus on benchmarking the cultural awareness of current LLM-based MT systems. In the future, we'll test other methods, such as instruction tuning, to improve the performance of LLM-based MT.",
            "score": 0.46601113017708695,
            "section_title": "Cultural-Awareness Definition",
            "char_start_offset": 28101,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 620
                },
                {
                    "start": 623,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1144
                },
                {
                    "start": 1145,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1406
                },
                {
                    "start": 1407,
                    "end": 1519
                }
            ],
            "ref_mentions": [
                {
                    "start": 815,
                    "end": 831,
                    "matchedPaperCorpusId": "261242746"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.483642578125
        },
        {
            "corpus_id": "225076418",
            "title": "Data Troubles in Sentence Level Confidence Estimation for Machine Translation",
            "text": "A necessary ingredient is the capability of annotating sentence pairs consisting of source sentence and translation with binary labels good / needs work. As a side product, this enables a very straightforward and interpret-able way of evaluating the quality of MT models by measuring sentence-level accuracy (SACC). \n\nWe use two different annotator pools for annotation: \n\n1. non-experts: proficient in the source and native in the target language 2. experts: professional translators With the non-expert pool, in agreement with (Barrault et al., 2019) we find that for highquality language pairs such as English-German, English-French or English-Spanish, SACC values on \"in-domain\" data vary between 0.89 and 0.98; for a lower-quality model such as English-Romanian, SACC is 0.61. A simple model for the reliability of binary annotations allows us to estimate the probability of a annotator assigning the incorrect label on a sample (about 0.05-0.10 in our experiments) and leads to the use of label smoothing in training our CE models. \n\nSwitching to the expert pool lowers SACC dramatically: for English-German it drops from 0.98 to 0.57; for English-Spanish from 0.89 to 0.61; for English-French from 0.89 to 0.44 and for English-Romanian from 0.61 to 0.26. The simple Bernoulli annotation model breaks down. \n\nAnalyzing the two annotator pools using Krippendorff's alpha (KA) statistic (Krippendorff, 2011) for inter-annotator agreement shows that they are both about equally reliable, with the nonexpert pool being simply more lenient than the expert one. The low values of KA confirm the expec-tation that binary translation quality annotation for isolated sentences out of context is very hard. \n\nPractical use cases of MT output are well aligned with CE models in that translation quality is viewed as a binary variable: correct or incorrect, as described in (Zhou et al., 2020). As a result we suggest measuring CE performance using Recall at a fixed Precision value which is chosen depending on the SACC of the model, e.g. 0.9 or 0.95.",
            "score": 0.4659205871068156,
            "section_title": "Introduction",
            "char_start_offset": 2110,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 315
                },
                {
                    "start": 318,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 781
                },
                {
                    "start": 782,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1312
                },
                {
                    "start": 1315,
                    "end": 1561
                },
                {
                    "start": 1562,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1888
                },
                {
                    "start": 1889,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 529,
                    "end": 552,
                    "matchedPaperCorpusId": "201741133"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.072265625
        },
        {
            "corpus_id": "263620377",
            "title": "Low-Resource Languages Jailbreak GPT-4",
            "text": "The discovery of cross-lingual vulnerabilities reveals the harms of the unequal valuation of languages in safety research. For instance, existing safety alignment of LLMs primarily focuses on the English language [2,27,50,51,56]. Toxicity and bias detection benchmarks are also curated for high-resource languages such as English, Arabic, Italian, and Chinese [1, 12, 18, 21, 40, 44, inter alia]. Before, this linguistic inequality mainly imposed utility and accessibility issues to low-resource language users [7]. Now, the inequality leads to safety risks that affect all LLM users. First, low-resource language speakers, which make up nearly 1.2 billion people around the world [24], can interact with LLMs with limited safety or moderation content filters. Second, bad actors from high-resource language communities can use publicly available translation tools to breach the safeguards. \n\nCross-lingual safety vulnerabilities are further exacerbated by the progress of language diversity in translation technology. For instance, Meta's open-source translation model NLLB supports 200 languages, many of which are low-resource languages [14]. Google also introduced methods to enable translation for thousands of languages without needing parallel corpora training sets [5]. Shortly after our work was released, concurrent work [15,49] also presented similar findings that LLMs are more likely to produce harmful content in low-resource languages. We therefore emphasize the need for research on the intersection of safety and low-resource languages, a currently underexplored area [39], in order to address cross-lingual vulnerabilities that render existing safeguards ineffective. \n\nThe need for multilingual red-teaming We urge that future red-teaming efforts report evaluation results beyond the English language [3,17,30,34,46]. We believe that cross-lingual vulnerabilities are cases of mismatched generalization [51], where safety training fails to generalize to the lowresource language domain for which LLMs' capabilities exist.",
            "score": 0.46575578573060705,
            "section_title": "Linguistic inequality endangers AI safety",
            "char_start_offset": 12903,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 890
                },
                {
                    "start": 893,
                    "end": 1018
                },
                {
                    "start": 1019,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1685
                },
                {
                    "start": 1688,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 2040
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "238744120"
                },
                {
                    "start": 681,
                    "end": 685,
                    "matchedPaperCorpusId": "215828350"
                },
                {
                    "start": 1585,
                    "end": 1589,
                    "matchedPaperCorpusId": "248780380"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03533935546875
        },
        {
            "corpus_id": "273502644",
            "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
            "text": "Generalization to downstream DPO or policy model performance. Although we evaluated how different RMs perform on M-REWARDBENCH, it is unclear if high performance on M-REWARDBENCH correlates to high performance on downstream multilingual benchmarks. Meanwhile, Ivison et al. (2024) found that in the (English) RewardBench, improvements in RM performance do not necessarily translate to better downstream PPO performance. We leave this exploration for future work. \n\nImpact of automatic translations versus humanwritten translations. We did not explore whether the performance and ranking of reward models will change when human-written translations of the English dataset are used. Our analysis in \u00a76.1 shows that when using an automatic translator of high quality, the performance of RMs will also improve. We hypothesize that using Google Translate allows us to approximate human-quality translations in a scalable manner. \n\nEvaluating RMs on cultural preferences. Our analyses in \u00a7D show instances of preference inversion from the original preferred response in English to the human-verified response in another language.",
            "score": 0.4655245081596174,
            "section_title": "Limitations",
            "char_start_offset": 18776,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 61
                },
                {
                    "start": 62,
                    "end": 248
                },
                {
                    "start": 249,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 462
                },
                {
                    "start": 465,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 923
                },
                {
                    "start": 926,
                    "end": 965
                },
                {
                    "start": 966,
                    "end": 1123
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.313720703125
        },
        {
            "corpus_id": "276259236",
            "title": "BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large Language Models",
            "text": "Previous multilingual benchmarks focus primarily on simple understanding tasks, but for large language models(LLMs), we emphasize proficiency in instruction following, reasoning, long context understanding, code generation, and so on. However, measuring these advanced capabilities across languages is underexplored. To address the disparity, we introduce BenchMAX, a multi-way multilingual evaluation benchmark that allows for fair comparisons of these important abilities across languages. To maintain high quality, three distinct native-speaking annotators independently annotate each sample within all tasks after the data was machine-translated from English into 16 other languages. Additionally, we present a novel translation challenge stemming from dataset construction. Extensive experiments on BenchMAX reveal varying effectiveness of core capabilities across languages, highlighting performance gaps that cannot be bridged by simply scaling up model size. BenchMAX serves as a comprehensive multilingual evaluation platform, providing a promising test bed to promote the development of multilingual language models. The dataset and code are publicly accessible.",
            "score": 0.46522037075292777,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.154296875
        },
        {
            "corpus_id": "271909321",
            "title": "Beyond English-Centric LLMs: What Language Do Multilingual Language Models Think in?",
            "text": "Large language models have become the prevailing approach for building NLP systems, most of which have been primarily developed for the English language. Due to the performance decline of Englishcentric models on non-English languages and their cultural bias towards English, researchers have increasingly focused on developing models with non-English-dominant corpora. Models that undergo continual pre-training (CPT) (Sun et al., 2020;Brown et al., 2020;Csaki et al., 2024;Cui et al., 2023;Hunter et al., 2023) or are pre-trained from scratch using non-English-dominant corpora (Sengupta et al., 2023;Yang et al., 2024;Faysse et al., 2024) often exhibit superior performance in their respective languages. \n\nRecent studies have investigated the underlying causes of performance decline of English-centric models on non-English languages, which show that when English-centric models process tasks of underrepresented languages such as Japanese, their intermediate layers, when un-embedded into vocabulary space, exhibit distinct patterns where the language distribution heavily skews towards English (Wendler et al., 2024). This phenomenon, which we termed as the internal latent language, raises the question: in what internal latent language do non-English-centric models 'think'? Specifically, we would like to investigate whether these models utilize the dominant language from their training corpora in their intermediate layers when processing tasks. We conduct a case study on Japanese models, chosen due to their relatively rich open-source ecosystem and the availability of training corpora information. We examine three typical categories of models that are used to process Japanese: Llama-2 (Touvron et al., 2023), an English-centric model; along with two Japanesespecific models Swallow (Fujii et al., 2024), an English-centric model with continued pre-training in Japanese; and LLM-jp (Aizawa et al., 2024), a model pre-trained on balanced corpora of English and Japanese.",
            "score": 0.46520585447205953,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 707
                },
                {
                    "start": 710,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1283
                },
                {
                    "start": 1284,
                    "end": 1457
                },
                {
                    "start": 1458,
                    "end": 1613
                },
                {
                    "start": 1614,
                    "end": 1986
                }
            ],
            "ref_mentions": [
                {
                    "start": 419,
                    "end": 437,
                    "matchedPaperCorpusId": "209475822"
                },
                {
                    "start": 437,
                    "end": 456,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09857177734375
        },
        {
            "corpus_id": "276421738",
            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
            "text": "The breakthrough of generative large language models (LLMs) that can solve different tasks through chat interaction has led to a significant increase in the use of general benchmarks to assess the quality or performance of these models beyond individual applications. There is also a need for better methods to evaluate and also to compare models due to the ever increasing number of new models published. However, most of the established benchmarks revolve around the English language. This paper analyses the benefits and limitations of current evaluation datasets, focusing on multilingual European benchmarks. We analyse seven multilingual benchmarks and identify four major challenges. Furthermore, we discuss potential solutions to enhance translation quality and mitigate cultural biases, including human-in-the-loop verification and iterative translation ranking. Our analysis highlights the need for culturally aware and rigorously validated benchmarks to assess the reasoning and question-answering capabilities of multilingual LLMs accurately.",
            "score": 0.4651950021855545,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "265221395",
            "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking",
            "text": "However, we observe that language availability does not necessarily indicate model capability in understanding and generating texts in this specific language. 4 Motivated by the recognized distinctive features among languages (Dryer, 2007) and language families (Ahmad et al., 2019), we leverage word order to measure language distances and study the effectiveness of multilingual cognitive overload with regard to the distance be-3 We provide the full list of languages in Tab. 5. 4 For example, on the translated variants of the MMLU benchmark, GPT4 with 3-shot in-context learning obtains much higher accuracy in mid-resource languages-Indonesian, Ukrainian and Greek, than that in high-resource languages-Mandarin and Japanese (OpenAI, 2023). \n\ntween English and the other languages. 5 ata Processing. We first translate the original English harmful instructions from AdvBench and MasterKey into 52 other languages. Due to cost concerns with Google Cloud API, we translate the non-English responses back to English using the freely available multilingual translation model nllb-200-distilled-1.3B (Costa-juss\u00e0 et al., 2022). We compute ASR by comparing translated English responses with rejection phrases listed in Tab. 3 as introduced in \u00a72.",
            "score": 0.4651915396582338,
            "section_title": "Multilingual Setup",
            "char_start_offset": 11009,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 160
                },
                {
                    "start": 161,
                    "end": 746
                },
                {
                    "start": 749,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1246
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019989013671875
        },
        {
            "corpus_id": "268510024",
            "title": "Is Translation All You Need? A Study on Solving Multilingual Tasks with Large Language Models",
            "text": "Large language models (LLMs) have demonstrated multilingual capabilities, yet they are mostly English-centric due to the imbalanced training corpora. While prior works have leveraged this bias to enhance multilingual performance through translation, they have been largely limited to natural language processing (NLP) tasks. In this work, we extend the evaluation to real-world user queries and non-English-centric LLMs, offering a broader examination of multilingual performance. Our key contribution lies in demonstrating that while translation into English can boost the performance of English-centric LLMs on NLP tasks, it is not universally optimal. For culture-related tasks that need deep language understanding, prompting in the native language proves more effective as it better captures the nuances of culture and language. Our experiments expose varied behaviors across LLMs and tasks in the multilingual context, underscoring the need for a more comprehensive approach to multilingual evaluation. Therefore, we call for greater efforts in developing and evaluating LLMs that go beyond English-centric paradigms.",
            "score": 0.46440333417265467,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20849609375
        },
        {
            "corpus_id": "257804597",
            "title": "A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America",
            "text": "As for methods relevant for word embeddings, these benchmarks are often accompanied by metrics that aggregate an NLP system's behavior on these pairs into measurements of harms. Blodgett et al. ( 2021) examine four such benchmarks and apply a method-originating from the social sciences-to inventory a range of pitfalls that threaten these benchmarks' validity as measurement models for stereotyping. They find that these benchmarks frequently lack clear articulations of what is being measured, and they highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping. N\u00e9v\u00e9ol et al. (2022) propose how to overcome some of this challenges by taking a culturally aware standpoint and a curation methodology when designing such benchmarks. \n\nMost previous work uses word or sentence lists developed for English, or direct translations from English that do not take into account structural differences between languages (Garg et al., 2018). For example, in Spanish almost all nouns and adjectives are morphologically marked with gender, but this is not the case in English. Figure 2 illustrates the differences in lexical biases measurements between translations of lists of words in English to Spanish over two different word embeddings in each language: the English embedding is described in Bolukbasi et al. (2016) and the Spanish in Ca\u00f1ete et al. (2020). From the 16 words analyzed, in English, 8 are more associated to the \"feminine\" extreme of the bias space, while in Spanish only 5 of them are. The 3 words with different positions are \"nurse, care and wash\". In particular, \"nurse\" in Spanish is morphologically marked with masculine gender in the word \"enfermero\", so it is not gender neutral. This figure illustrates two things. First, the fact that methodologies for bias detection developed for English are not directly applicable to other languages. Second, the list of words selected to analyze bias have a strong impact on the bias that is shown by the analysis.",
            "score": 0.4640818091289313,
            "section_title": "On the importance of benchmarks",
            "char_start_offset": 22716,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1572
                },
                {
                    "start": 1573,
                    "end": 1637
                },
                {
                    "start": 1638,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1933
                },
                {
                    "start": 1934,
                    "end": 2048
                }
            ],
            "ref_mentions": [
                {
                    "start": 643,
                    "end": 663,
                    "matchedPaperCorpusId": "248780290"
                },
                {
                    "start": 990,
                    "end": 1009,
                    "matchedPaperCorpusId": "4930886"
                },
                {
                    "start": 1364,
                    "end": 1387,
                    "matchedPaperCorpusId": "1704893"
                },
                {
                    "start": 1407,
                    "end": 1427,
                    "matchedPaperCorpusId": "260681542"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.29150390625
        },
        {
            "corpus_id": "247748753",
            "title": "Probing Pre-Trained Language Models for Cross-Cultural Differences in Values",
            "text": "One challenge of cross-cultural research is information loss when translating survey questions (Nasif et al., 1991;Hofstede, 1984). Therefore we opted for this approach rather than reformulating the translated survey questions by Hofstede. However, we would like to highlight the shortcomings of machine translation which have poor performance on low resource languages and has the potential to introduce additional biases. For the purpose of these experiments however, since the question probes are relatively simple sentences, we found the machine translations to be of high quality. We conducted an evaluation of our machine translated probes, the details for which can be found in the Appendix B. The target labels [y + i , y \u2212 i ] for each QP are then translated individually as single words (e.g. important is translated from English to the German wichtig), followed by lowercased string matching to check if the translated label can be found and replaced in the translated probe. If the target label cannot be found directly in the translated probe due to differences in word choice, we use a cross-lingual word aligner (Dou and Neubig, 2021) to align the English probe and its translated version. With this approach, we identify the label word to be replaced with the mask token. If both approaches yield no result, the token is manually replaced in the target sentence based on the authors' language understanding and using online translators. \n\nLanguage Selection In total, we investigate 13 languages, mapped to one country each as outlined in Table 1, according to criteria further detailed below. One of the limitations of this one-to-one mapping is that the languages are spoken in wider regions and not specifically in one country (disregarding also e.g. diaspora communities). This allows for the closest match to the values theories we work with, which operate on a country level. The definition of culture by country has been criticised by, e.g., Nasif et al. (1991). \n\nWe select the languages as follows: We first include the countries covered in both the surveys of WVS and Hofstede. We limit to languages which are official languages of the countries observed in the studies of both WVS and Hofstede. We further select languages for which the distribution of speakers is primarily localized to a country or relatively narrow geographical region.",
            "score": 0.46405641878599946,
            "section_title": "Probe Generation",
            "char_start_offset": 11111,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 132,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1769
                },
                {
                    "start": 1770,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1985
                },
                {
                    "start": 1988,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2221
                },
                {
                    "start": 2222,
                    "end": 2366
                }
            ],
            "ref_mentions": [
                {
                    "start": 1127,
                    "end": 1149,
                    "matchedPaperCorpusId": "231648372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12103271484375
        },
        {
            "corpus_id": "270521296",
            "title": "BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages",
            "text": "Large language models (LLMs) often lack culture-specific knowledge of daily life, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are limited to a single language or collected from online sources such as Wikipedia, which do not reflect the mundane everyday lifestyles of diverse regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play, or the sports they practice in school is common cultural knowledge but uncommon in easily collected online sources, especially for underrepresented cultures. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. BLEnD comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two formats of questions: short-answer and multiple-choice. We show that LLMs perform better for cultures that are highly represented online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format. For cultures represented by mid-to-high-resource languages, LLMs perform better in their local languages, but for cultures represented by low-resource languages, LLMs perform better in English than the local languages. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",
            "score": 0.4640203840143682,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06396484375
        },
        {
            "corpus_id": "247951931",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "text": "beyond previously studied scales.\n\n\u2022 Multilingual understanding -Previous work on large language models have conducted limited evaluations in the multilingual domain. In this work, we conduct a more thorough evaluation of multilingual benchmarks including machine translation (Section 6.5), summarization (Section 6.6), and question answering (Section 6.7) in a wide variety of languages. Even with a relatively small proportion of non-English data (\u2248 22%) in the training corpus, few-shot evaluation results from the 540B model are able to bridge the gap with prior finetuned state of the art in non-English summarization tasks and outperform prior state of the art in translation tasks. Further work is necessary to understand the impact of increasing the proportion of multilingual data on the English and multilingual tasks.\n\n\u2022 Bias and toxicity -We also evaluated model performance for distributional bias and toxicity, which resulted in several insights (Section 10). Firstly, for gender and occupation bias, we found that accuracy on the Winogender coreference task improves with model scale, and PaLM 540B sets a new state-of-the-art result in 1-shot and few-shot settings. Secondly, co-occurence analysis performed on race/religion/gender prompt continuation demonstrates the potential for the model to falsely affirm stereotypes, for instance, associating Muslims with terrorism, extremism, and violence. This behavior was consistent across model scales. Finally, toxicity analysis on prompt continuation tasks demonstrates a slightly higher overall toxicity level for the 62B and 540B model compared to the 8B model. However, the toxicity of the model-generated continuation correlates highly with the toxicity of the prompting text, whereas human-generation continuations do not have a strong toxicity correlation. This suggests that the model is more heavily influenced by the style of the prompt than would be the case for human-generated text. In future work, we plan to broaden these benchmarks to non-English languages and more exhaustively account for potential risks.",
            "score": 0.4638602526682499,
            "section_title": "Logical Inference",
            "char_start_offset": 8281,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1663818359375
        },
        {
            "corpus_id": "232328071",
            "title": "The Role of Translation in Citizen Science to Foster Social Innovation",
            "text": "The result should be an instrument in the target language that \"asks the same questions, in the same manner, with the same intended meaning, as the source instrument\" (Palmieri et al., 2020). \n\nIn different rounds, the cultural adaptability, the clarity of the translation, the cultural relevance and readability are assessed by different agents in the process. The cognitive probing step has a strong focus on terminology. Especially terminology may cause difficulties because it is embedded in a certain cultural system that relies on different categories, such as different terms for private and public hospitals (Palmieri et al., 2020). \n\nTranslation validation for cross-cultural research has a focus on equivalence, cultural applicability (adaptation) and cultural relevance, including readability and cultural adaptation for content validity. This should ensure that the instructions given to participants are interpreted in the same way in different cultures. In short, the translated instructions should not change the results. On Zooniverse, there is no translation validation according to the process described above. The translations on Zooniverse are not back translated and there is no systematic evaluation if the translations impart the same meaning as the source text. On Zooniverse, the translations can be revised after they have been published. As a result, users may see different instructions on translated project pages. These can be seen as reasons to question the accuracy and reliability of the translations since this may have a (negative) impact on the way how the participants complete tasks, and thus, jeopardize the research results. However, this has to be assessed in view of the fact that some Zooniverse users who contribute to a project that is only available in English reported that they translate the Zooniverse project pages and instructions with freely available machine translation systems into their language. This shows that researchers asking volunteers to complete citizen science tasks on Zooniverse cannot control how participants who do not use English as a preferred language interpret the task and if they use machine translation systems to understand the instructions. Since the quality of translation of freely available machine translation systems differs significantly between language pairs and domains, it cannot be guaranteed that the machine-translated text imparts the same meaning as the source text and that the machine-translated instructions are accurate enough so that the participants fulfill the task correctly (as intended by the researchers).",
            "score": 0.46373400026232336,
            "section_title": "Validation of Translations",
            "char_start_offset": 42792,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 194,
                    "end": 361
                },
                {
                    "start": 362,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1128
                },
                {
                    "start": 1129,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2220
                },
                {
                    "start": 2221,
                    "end": 2611
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 190,
                    "matchedPaperCorpusId": "215741408"
                },
                {
                    "start": 616,
                    "end": 639,
                    "matchedPaperCorpusId": "215741408"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1104736328125
        },
        {
            "corpus_id": "270371898",
            "title": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
            "text": "We proposed CVQA, a novel, human-written visual QA benchmark dataset that captures cultural nuances across a diverse set of languages and locations. CVQA encompasses 10 question categories, with each question written in both English and the native language. This allowed us to benchmark both multilingual visual models and English-only models. We provided insights into our dataset's question types and commonly used terms for each category. \n\nWe then performed benchmarks on various visual models, including both multilingual and Englishonly models. Our benchmark demonstrated that CVQA presented challenges for open-source models. These models generally performed worse when queried in local languages compared to English, indicating poorer performance in handling multilingual queries. The performance is also considerably lower when we do not provide the multiple choice setting, which is a more realistic use case for this technology. We hope that publishing CVQA encourages the AI community to pay more attention to non-English-centric models and benchmarking, thereby advancing progress in multilingual, multimodal research. \n\nand, as appropriate, by \"ERDF A way of making Europe\", by the \"European Union\". We also thank the anonymous reviewers for their valuable feedback and suggestions that helped improve this paper.",
            "score": 0.463486638720977,
            "section_title": "Conclusion",
            "char_start_offset": 25444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 441
                },
                {
                    "start": 444,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1353759765625
        },
        {
            "corpus_id": "269457293",
            "title": "Exploring Multi-Lingual Bias of Large Code Models in Code Generation",
            "text": "In our experiments in RQ2 and RQ3, we demonstrate that the multi-NL bias can be potentially mitigated in both the prompting and instruction tuning phases.In this section, we discuss the advantages and limitations of the two methods.\n\nFor prompting, we observe that one-step translation and multi-step translation aid in mitigating the multi-NL bias by 40% and 76%, respectively.Despite the contribution to mitigating multi-NL bias, these methods have the following limitations: 1) The benefits come with auxiliary translation tools and non-trivial costs.For example, in the multi-step translation process, we averagely translate 3.9 statements for each programming problem.2) Translation helps to mitigate the bias by narrowing the performance gap between Chinese and English instructions; however, the essential code generation capabilities of LCMs are not improved.3) LCMs typically generate responses such as code comments and explanations in the same language as the input.Consequently, when users who are not proficient in English opt to translate their queries into English, they may find the English responses difficult to comprehend, resulting in additional costs to translate back.\n\nFor instruction tuning, as noted in RQ3, we observe that instruction tuning not only effectively mitigates the multi-NL bias by as high as 84% but also substantially improves the code generation capabilities of LCMs, as evidenced by an overall 39% increase in the Pass@1 rate.However, instruction tuning involves the creation of a multi-lingual dataset and demands computational resources for training the LCMs.This requirement could pose practical challenges and resource constraints in implementation.\n\nDrawing upon our empirical findings, developers can choose proper methods to mitigate the multi-lingual bias according to their resources and expectations in practice.",
            "score": 0.4632518086059178,
            "section_title": "B. Prompting or Instruction Tuning",
            "char_start_offset": 37497,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 154,
                    "end": 232
                },
                {
                    "start": 234,
                    "end": 378
                },
                {
                    "start": 378,
                    "end": 554
                },
                {
                    "start": 554,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 867
                },
                {
                    "start": 867,
                    "end": 977
                },
                {
                    "start": 977,
                    "end": 1190
                },
                {
                    "start": 1192,
                    "end": 1468
                },
                {
                    "start": 1468,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1695
                },
                {
                    "start": 1697,
                    "end": 1864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0782470703125
        },
        {
            "corpus_id": "271404509",
            "title": "Cultural Value Differences of LLMs: Prompt, Language, and Model Size",
            "text": "Assessing social and cultural biases in language models is crucial to mitigate associated risks and reveal the values embodied by the models.Liang et al. (2021) provided a formal comprehension of social biases in language models.The work identified fine-grained local biases and high-level global biases as sources of representational biases and proposed the evaluation metrics for measurement.Subsequently, it introduced the mitigation method.Sheng et al. (2021) presented the first comprehensive survey on societal biases in language generation in 2021, identifying their negative impact and exploring methods for evaluation and mitigation.The study highlighted the challenge of bias assessment due to the open-domain nature of NLG and the diverse conceptualizations of bias across cultures.Recently, more studies have focused on evaluating bias and values in large language models, with innovative methodologies employed.Cheng et al. (2023) utilized the concept of markedness, initially linguistic but now a part of social science, to evaluate models' stereotypes unsupervisedly.Meanwhile, Kotek et al. (2023) employed a direct method to assess gender bias in LLMs, revealing models' tendency to reflect imbalances over gender due to training on skewed datasets.In Ferrara (2023), bias in generative language models was defined and its sources, such as training data and model specifications, were investigated.However, the study also acknowledged that some biases may persist inevitably due to the inherent nature of language and cultural norms.\n\nPrevious studies have demonstrated diverse techniques for accurately and efficiently identifying biases.However, they have also underscored the challenges in mitigating biases in generated text, as biases can be inherited from human language and culture in training data.This indicates that the exhibited values of models are shaped by the training data, making it impossible to dissociate the influence of training data when trying to understand the patterns of values expressed by models.",
            "score": 0.4631551400009479,
            "section_title": "Bias Study of Language Model",
            "char_start_offset": 4457,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 141,
                    "end": 229
                },
                {
                    "start": 229,
                    "end": 394
                },
                {
                    "start": 394,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 642
                },
                {
                    "start": 642,
                    "end": 793
                },
                {
                    "start": 793,
                    "end": 924
                },
                {
                    "start": 924,
                    "end": 1082
                },
                {
                    "start": 1082,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1549
                },
                {
                    "start": 1551,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1822
                },
                {
                    "start": 1822,
                    "end": 2041
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 160,
                    "matchedPaperCorpusId": "235623756"
                },
                {
                    "start": 1093,
                    "end": 1112,
                    "matchedPaperCorpusId": "261276445"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.132568359375
        },
        {
            "corpus_id": "250562857",
            "title": "CL-ReLKT: Cross-lingual Language Knowledge Transfer for Multilingual Retrieval Question Answering",
            "text": "In a multilingual dataset, the distribution of languages tends to be imbalanced. As shown in Figure 3, the number of sentences in English is approximately 50% of all sentences in the corpus used to construct mUSE (Yang et al., 2020). \n\ne n z h j a e s p t z h -t r u i t f r d e t r a r k o n l p l t h Languages 0 1000 2000 3000 #Sentence (Million) 2 7 0 0 1 0 0 0 3 8 4 3 4 0 1 8 0 1 4 7 1 1 2 1 0 3 9 2 7 5 6 9 6 0 6 0 6 0 6 0 6 0 \n\nThe number of training data in mUSE Due to the stated language imbalance, the model performance in languages with a large amount of data tend to be substantially better than that in other languages (Arivazhagan et al., 2019;Wang et al., 2020). This issue can be problematic when we want the model performance to be consistent across multiple languages. \n\nFor the case of mUSE, as shown in Figure 3, we can see that English is the dominant language in terms of training data available. Hence, the Englishto-English retrieval performance tends to be better than all other language pairs. To verify this performance gap, we conducted a CL-ReQA experimental study using questions in non-English and answer documents in English; mUSE was used to encode the questions and documents. Experimental results show a significant performance improvement when the questions are translated into English instead of using the original non-English questions, i.e., translated questions from Russian to English improving the precision-at-1 from 43.3% to 52.8%. For the full results, see Appendix A.3.",
            "score": 0.4631110417592075,
            "section_title": "Dominant Language",
            "char_start_offset": 6477,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 233
                },
                {
                    "start": 236,
                    "end": 433
                },
                {
                    "start": 436,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 788
                },
                {
                    "start": 791,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1517
                }
            ],
            "ref_mentions": [
                {
                    "start": 213,
                    "end": 232,
                    "matchedPaperCorpusId": "195848217"
                },
                {
                    "start": 660,
                    "end": 678,
                    "matchedPaperCorpusId": "215768769"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0295867919921875
        },
        {
            "corpus_id": "256461134",
            "title": "Subword Evenness (SuE) as a Predictor of Cross-lingual Transfer to Low-resource Languages",
            "text": "Studies on low-resource languages demonstrate how a lack of resources impacts performance (Wu and Dredze, 2020;Wang et al., 2020a;Goyal et al., 2021). Ruder et al. (2021) show that the performance of XLM-R is lower on low-resource languages than on high-resource ones. Moreover, performance is lower on languages with non-Latin scripts, such as Hebrew, Japanese, Thai or Chinese. \n\nThe current processing workflows often include continued training of a multilingual model on a single high-resource transfer language and then applying the resulting weights in a few-or zero-shot manner to many low-resource languages. Focusing on the question of which transfer languages give good results over multiple target languages, we count the mentions of the best transfer pairs in several previous studies (Ruder et al., 2021;Turc et al., 2021;V\u00e1zquez et al., 2021;Hu et al., 2020;Lauscher et al., 2020;Lin et al., 2019;Paul et al., 2013). The counts are plotted in Figure 1, showing an interesting asymmetry between European and other languages. When European languages are the target of transfer, English seems to be the best transfer language. This is in line with the findings on the XTREME benchmark for evaluating crosslingual transfer (Hu et al., 2020), which led the authors to conclude that English is the most common and the most universal choice for a transfer language. Paul et al. (2013) found that English as a pivot language in statistical machine translation works well in approximately half of the observed language pairs (22 Indo-European and Asian languages). Our review shows that English is not the best choice when non-European languages are the target. \n\nOne approach to choosing a transfer language is to rely on structural similarity, which is measured using grammar features from the URIEL database (Littell et al., 2017). Lauscher et al. (2020) find that transfer is better in language pairs that are closer in the URIEL vector space regarding POS tagging and syntactic dependency parsing. Other factors, such as data size, are better predictors on the tasks of question answering and inference.",
            "score": 0.46256661664293075,
            "section_title": "Related Work",
            "char_start_offset": 7001,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2007
                },
                {
                    "start": 2008,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 111,
                    "matchedPaperCorpusId": "218684346"
                },
                {
                    "start": 111,
                    "end": 130,
                    "matchedPaperCorpusId": "216562574"
                },
                {
                    "start": 130,
                    "end": 149,
                    "matchedPaperCorpusId": "233481097"
                },
                {
                    "start": 835,
                    "end": 856,
                    "matchedPaperCorpusId": "235097359"
                },
                {
                    "start": 856,
                    "end": 872,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 872,
                    "end": 894,
                    "matchedPaperCorpusId": "226262344"
                },
                {
                    "start": 911,
                    "end": 929,
                    "matchedPaperCorpusId": "17222548"
                },
                {
                    "start": 1233,
                    "end": 1250,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1373,
                    "end": 1391,
                    "matchedPaperCorpusId": "17222548"
                },
                {
                    "start": 1816,
                    "end": 1838,
                    "matchedPaperCorpusId": "17625727"
                },
                {
                    "start": 1840,
                    "end": 1862,
                    "matchedPaperCorpusId": "226262344"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.035003662109375
        },
        {
            "corpus_id": "276482384",
            "title": "Unveiling Cultural Blind Spots: Analyzing the Limitations of mLLMs in Procedural Text Comprehension",
            "text": "Despite the impressive performance of multilingual large language models (mLLMs) in various natural language processing tasks, their ability to understand procedural texts, particularly those with culture-specific content, remains largely unexplored. Texts describing cultural procedures, including rituals, traditional craftsmanship, and social etiquette, require an inherent understanding of cultural context, presenting a significant challenge for mLLMs. In this work, we introduce CAPTex, a benchmark designed to evaluate mLLMs' ability to process and reason about culturally diverse procedural texts across multiple languages using various methodologies to assess their performance. Our findings indicate that (1) mLLMs face difficulties with culturally contextualized procedural texts, showing notable performance declines in low-resource languages, (2) model performance fluctuates across cultural domains, with some areas presenting greater difficulties, and (3) language models exhibit better performance on multiple-choice tasks within conversational frameworks compared to direct questioning. These results underscore the current limitations of mLLMs in handling culturally nuanced procedural texts and highlight the need for culturally aware benchmarks like CAPTex to enhance their adaptability and comprehension across diverse linguistic and cultural landscapes.",
            "score": 0.4624790898302921,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "150155098",
            "title": "Translating questionnaires for cross-national surveys: A description of a genre and its particularities based on the ISO 17100 categorization of translator competences",
            "text": "In fact, an array of methods, including advance translations for checking translatability and cross-national pretests (Dorer, 2015;Fitzgerald, Widdop, Gray, & Collins, 2011;Smith, 2003), are increasingly integrated into the process of source questionnaire development to ensure culturally relevant and smooth translations later on. To what extent a good translation is indeed sufficient to ensure equivalent data is yet another question; researchers regularly call for a greater degree of liberty in translation, for more cultural adaptation, and for empirical studies on how different translated vs. adapted versions impact the equivalence of data (e.g., Harkness, Schoua-Glusberg, & Pennell, 2004;Harkness, Villar, et al., 2010;Kleiner, Pan, & Bouic, 2009;Schwarz, 1997). \n\nOver the years, translation quality has most notably been discussed in terms of translation assessment methods (Behr & Shishido, 2016). In particular in the social sciences, the back translation approach, which has been popular since the 1970s and which originated from cross-cultural psychology (Brislin, 1970), has been replaced by team translation or the TRAPD model, as outlined further below. In its simplest form, back translation includes the transfer of the translation back into the source language and the subsequent comparison of the two source-language versions. Based on this comparison, conclusions are drawn as to the quality of the translation. Objections to this model, which may lead to overly literal translations and a false sense of security, have led to the development of the team model known as TRAPD (Harkness, 2003). TRAPD was developed for the European Social Survey (ESS), the flagship in the social sciences for the advancement of crossnational survey methods. 6 The model has been adopted or adapted in many forms in other surveys. TRAPD stands for translation, review, adjudication, pretesting, and documentation. In its pure form, parallel translations are done by two skilled practitioners who work independently from each other (T). The translators then meet with survey and subject-matter experts to reconcile their versions and arrive at a final review version (R).",
            "score": 0.4620355125850396,
            "section_title": "Cross-national survey methodology and questionnaire translation: A brief overview",
            "char_start_offset": 7797,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 773
                },
                {
                    "start": 776,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1350
                },
                {
                    "start": 1351,
                    "end": 1436
                },
                {
                    "start": 1437,
                    "end": 1618
                },
                {
                    "start": 1619,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2177
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 173,
                    "matchedPaperCorpusId": "140347452"
                },
                {
                    "start": 656,
                    "end": 699,
                    "matchedPaperCorpusId": "70522622"
                },
                {
                    "start": 699,
                    "end": 730,
                    "matchedPaperCorpusId": "153405367"
                },
                {
                    "start": 730,
                    "end": 758,
                    "matchedPaperCorpusId": "59470178"
                },
                {
                    "start": 758,
                    "end": 772,
                    "matchedPaperCorpusId": "153797897"
                },
                {
                    "start": 887,
                    "end": 910,
                    "matchedPaperCorpusId": "148235851"
                },
                {
                    "start": 1072,
                    "end": 1087,
                    "matchedPaperCorpusId": "146188546"
                },
                {
                    "start": 1601,
                    "end": 1617,
                    "matchedPaperCorpusId": "242348327"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12432861328125
        },
        {
            "corpus_id": "265213090",
            "title": "How Vocabulary Sharing Facilitates Multilingualism in LLaMA?",
            "text": "Languages that fall into this quadrant demonstrate a \"selfless\" characteristic. Training based on the data from these languages does not necessarily improve, and may even decrease their performance. Inter-estingly, it can lead to performance enhancements in other languages. We will conduct a thorough examination of the underlying causes of this phenomenon and propose potential solutions. \n\nInterpretation for bilingual performance decline: The model transitions from an error type that is easy to score to a less score-friendly error type. The primary error for LLaMA is \"source copy\", which simply duplicates the source sentence as the translation. This error often leads to moderate scores when there are names, numbers, and punctuation in the translation tasks. However, after tuning, the main error shifts to \"oscillatory hallucination\" (Li et al., 2023), a state where the model becomes stuck in a specific translation state and generates repeated n-grams until it reaches the maximum length. This error makes it challenging to earn the score of spBLEU. Therefore, the performance of the fine-tuned model is lower than that of the original model. \n\nInterpretation for multilingual performance improvement: Those languages' vocabulary encompasses the majority of English tokens. We estimate the linguistics of these languages on the Flores-101 benchmark, a multilingual parallel corpus translated by professional translators through a controlled process. For an altruistic language, \n\nLG we first employ LLaMA's tokenizer to segment the words in both the LG and English data from Flores. This allows us to compile the sets of tokens that belong to the LG language, denoted as S LG , and the English language, denoted as S En . Finally, we calculate the ratios of the size of S LG S En to the size of S LG and the size of S En respectively. Intriguingly, as shown in Figure 4, we discovered that most tokenized results used in these languages exhibit a high degree of consistency with English. \n\nPractice Guidance: Full FT with a minimal dataset can effectively enhance bilingual per- Table 3: Performance comparison of bilingual and multilingual models. In full fine-tuning, multilingual models improve with more languages. However, in embedding fine-tuning, language quantity doesn't significantly affect performance.",
            "score": 0.46199387199223996,
            "section_title": "Altruistic Quadrant",
            "char_start_offset": 19984,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 79
                },
                {
                    "start": 80,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1285
                },
                {
                    "start": 1286,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1846
                },
                {
                    "start": 1847,
                    "end": 1999
                },
                {
                    "start": 2002,
                    "end": 2160
                },
                {
                    "start": 2161,
                    "end": 2230
                },
                {
                    "start": 2231,
                    "end": 2325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03338623046875
        },
        {
            "corpus_id": "265043227",
            "title": "Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for Bias Evaluation in Machine Translation",
            "text": "To conclude our study, we highlighted the need for contextualising NMT bias evaluation for non-English source languages, especially for languages that capture gender-related information in different forms. We demonstrated this using Hindi as a source language by creating evaluation benchmarks for HI-EN Machine Translation and comparing various state-of-the-art translation systems. In future, we plan to extend our evaluation to more languages and use natural sentences for evaluation without following a particular template. We are also looking forward to developing evaluation methods that are more inclusive of all gender identities.",
            "score": 0.4619692063246001,
            "section_title": "Conclusion and Future Work",
            "char_start_offset": 14380,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 638
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2607421875
        },
        {
            "corpus_id": "252873067",
            "title": "DATScore: Evaluating Translation with Data Augmented Translations",
            "text": "We compare the performance of our metric against BLEU and three other reference-based unsupervised metrics: BERTScore5 , MoverScore6 and BARTScore7 (detailed in Subsection 2.1 and Section 3), using their official implementations. Experimental results are reported in Table 1 and 2. Following their original settings, we use different underlying language models for each baseline metric. For BERTScore and MoverScore, RoBERTa-Large (RL; Liu et al., 2019) and Bert-Base (BB) are used respectively when we evaluate to-English translations, and mBERT (Devlin et al., 2019) for from-English translations. In the case of BARTScore, we use a BART-Large (BL) checkpoint (finetuned on CNNDM (See et al., 2017) and (Hu et al., 2019) datasets) for evaluating to-English translations, and an mBART-50 model (Escolano et al., 2021) for from-English translations. \n\nOverall, results show that, on average, across all language pairs, DATScore significantly outperforms all 4 baseline metrics under their original model settings (rows 1a-1d and 2a-2d). Specifically, with respect to the best performing baseline BERTScore (row 1b and 2b), our metric provides a performance boost of 0.7 for to-English case and of 9.8 for from-English case on WMT17 dataset in Table 1, and achieves a gain of 0.7 and of 7.3 respectively on WMT18 dataset in Table 2. These averaging results demonstrate the superiority and applicability of DATScore in evaluating general machine translations of many languages. Moreover, it is interesting to note that our improvement is much more significant in from-English case, which makes DATScore particularly well-suited to evaluate hypothesis translations in non-English languages, often with low resource. We hypothesize that this is due to the inconsistency of underlying language models. The baselines adopt a monolingual model for evaluating English, but a multilingual one for non-English languages. However, DATScore uses a single multilingual M2M-100 model for both cases.",
            "score": 0.4618836284518963,
            "section_title": "Main results",
            "char_start_offset": 15800,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 849
                },
                {
                    "start": 852,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1712
                },
                {
                    "start": 1713,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1910
                },
                {
                    "start": 1911,
                    "end": 1985
                }
            ],
            "ref_mentions": [
                {
                    "start": 547,
                    "end": 568,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 682,
                    "end": 700,
                    "matchedPaperCorpusId": "8314118"
                },
                {
                    "start": 705,
                    "end": 722,
                    "matchedPaperCorpusId": "207984304"
                },
                {
                    "start": 795,
                    "end": 818,
                    "matchedPaperCorpusId": "215754272"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "277271948",
            "title": "PM4Bench: A Parallel Multilingual Multi-Modal Multi-task Benchmark for Large Vision Language Model",
            "text": "Existing multilingual LLM benchmarks typically translate English datasets (Shi et al., 2022;Hasan et al., 2021) into other languages. Recent efforts like P-MMEval (Zhang et al., 2024) and Bench-MAX (Huang et al., 2025) use parallel corpora to fairly assess LLMs' cross-lingual capabilities, stripping away cultural biases to focus on fundamental language abilities. Safety in multilingual settings is equally critical, with studies showing non-English prompts are more likely to jailbreak LLMs (Jain et al., 2024;de Wynter et al., 2024;Deng et al., 2023;Xu et al., 2023). However, similar research in LVLMs remains limited.",
            "score": 0.4618312073508508,
            "section_title": "Multilingual Benchmark",
            "char_start_offset": 4359,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 623
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06964111328125
        },
        {
            "corpus_id": "258187051",
            "title": "UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining",
            "text": "In selecting evaluation tasks, we aim to satisfy several key properties. First, tasks should be linguistically diverse, covering a range of languages from distinct families and regions, including both high-and low-resource languages. Second, tasks should be free of language bias. For example, the task training data and evaluation metrics should be well-balanced across languages. We also avoid benchmarks where English plays a special role, including datasets constructed in English and translated post-hoc to other languages, as well as zero-shot transfer tasks where English is the sole source language. Finally, to the degree possible, benchmarks should be realistic, such that performing better on the benchmark gives us confidence that a model will do better on actual tasks facing language technology users. This is in contrast to \"intermediate structure\" tasks such as part-of-speech tagging. \n\nTyDi QA (Clark et al., 2020) is a multilingual question-answering benchmark covering a range of typologically diverse languages. Questions are written from scratch by native speakers in each language, ensuring culturally relevant content and the absence of \"translationese\". We use the \"GoldP\" task, which covers 9 languages. To evaluate candidate models, we use the \"in-language multitask\" setting (Hu et al., 2020)-fine-tuning on a mixture of all available languages, and evaluating in each language separately. To maximize per-language performance, we select per-language checkpoints based on the validation performance, and report validation metrics, as no test set is provided. \n\nThe WMT21 shared task on large-scale multilingual machine translation (Wenzek et al., 2021) tests the ability of single model to translate across many languages. We focus on the \"small track\" tasks, each testing translation between 6 languages, in all 30 combinations. As with TyDi QA, we finetune a single multilingual model on the mixture of all tasks and select per-language-pair checkpoints based on the test set performance. 3 To reduce heavy English bias, we restrict training data to 1,000 examples per language pair by randomly subsampling from the training data.",
            "score": 0.4617907409251726,
            "section_title": "EVALUATION TASKS",
            "char_start_offset": 13982,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 901
                },
                {
                    "start": 904,
                    "end": 1032
                },
                {
                    "start": 1033,
                    "end": 1178
                },
                {
                    "start": 1179,
                    "end": 1229
                },
                {
                    "start": 1230,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1586
                },
                {
                    "start": 1589,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2160
                }
            ],
            "ref_mentions": [
                {
                    "start": 912,
                    "end": 931,
                    "matchedPaperCorpusId": "212657414"
                },
                {
                    "start": 1303,
                    "end": 1319,
                    "matchedPaperCorpusId": "214641214"
                },
                {
                    "start": 1659,
                    "end": 1680,
                    "matchedPaperCorpusId": "245148999"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23876953125
        },
        {
            "corpus_id": "274234962",
            "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages",
            "text": "Cultural QA Generation. ALM-bench dataset covers different question types. To generate high-quality cultural questions, we utilize GPT-4o to create QA pairs based on the provided images and their metadata. Notably, images and QAs are not shared across languages in the cultural set. \n\nFor each image, we generate two MCQs, two short questions, a long question, and a true/false question in English. \n\nWe instruct GPT-4o to emphasize the cultural concepts depicted in each image, generate questions that require a visual understanding of an image, and not perpetuate bias and stereotypes. To mitigate the randomness in multiple choice and true/false, we ensure that these questions can also be answered when converted to open-ended questions [37]. We also evaluate with shuffled orders, predicting the entire cho- sen option rather than just the leading alphabet. The detailed instructions are presented in Sec. I (suppl. material.) Filtration and Translation Errors. Given the QA pairs, we translate them to remaining 99 languages using GPT4o, followed by manual correction from native speakers. Similar to image filtering, native speakers assess translation quality and make detailed corrections to provide culturally grounded answers focusing on core cultural aspects. They also discard redundant and culturally irrelevant QAs. We note several challenges in GPT-4o's translations across 100 languages. Specifically, GPT-4o demonstrates difficulty in generating culturally grounded QA pairs across diverse languages. To analyze these issues, mistakes from the GPT-4o model in translations are categorized into four types: semantic error, cultural error, language error, and grammatical error. We sample 57 question-answer pairs from 51 randomly selected languages and plotted error distribution in Fig. 5. Notably, GPT-4o encounters more issues with semantic and grammatical accuracy when translating into different languages. We show examples of such errors in",
            "score": 0.46157607264255207,
            "section_title": "Data Collection and Annotation",
            "char_start_offset": 11340,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 74
                },
                {
                    "start": 75,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 746
                },
                {
                    "start": 747,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 920
                },
                {
                    "start": 921,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1270
                },
                {
                    "start": 1271,
                    "end": 1329
                },
                {
                    "start": 1330,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1517
                },
                {
                    "start": 1518,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1927
                },
                {
                    "start": 1928,
                    "end": 1962
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.136962890625
        },
        {
            "corpus_id": "275357770",
            "title": "On The Origin of Cultural Biases in Language Models: From Pre-training Data to Linguistic Phenomena",
            "text": "Language Models (LMs) have been shown to exhibit a strong preference towards entities associated with Western culture when operating in non-Western languages. In this paper, we aim to uncover the origins of entity-related cultural biases in LMs by analyzing several contributing factors, including the representation of entities in pre-training data and the impact of variations in linguistic phenomena across languages. We introduce CAMeL-2, a parallel Arabic-English benchmark of 58,086 entities associated with Arab and Western cultures and 367 masked natural contexts for entities. Our evaluations using CAMeL-2 reveal reduced performance gaps between cultures by LMs when tested in English compared to Arabic. We find that LMs struggle in Arabic with entities that appear at high frequencies in pre-training, where entities can hold multiple word senses. This also extends to entities that exhibit high lexical overlap with languages that are not Arabic but use the Arabic script. Further, we show how frequency-based tokenization leads to this issue in LMs, which gets worse with larger Arabic vocabularies. We will make CAMeL-2 available at: https://github.com/tareknaous/camel2",
            "score": 0.46120439506412736,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53466796875
        },
        {
            "corpus_id": "270711451",
            "title": "CaLMQA: Exploring culturally specific long-form question answering across 23 languages",
            "text": "Large language models (LLMs) are used for long-form question answering (LFQA), which requires them to generate paragraph-length answers to complex questions. While LFQA has been well-studied in English, this research has not been extended to other languages. To bridge this gap, we introduce CaLMQA, a collection of 1.5K complex culturally specific questions spanning 23 languages and 51 culturally agnostic questions translated from English into 22 other languages. We define culturally specific questions as those uniquely or more likely to be asked by people from cultures associated with the question's language. We collect naturally-occurring questions from community web forums and hire native speakers to write questions to cover under-resourced, rarely-studied languages such as Fijian and Kirundi. Our dataset contains diverse, complex questions that reflect cultural topics (e.g. traditions, laws, news) and the language usage of native speakers. We automatically evaluate a suite of open- and closed-source models on CaLMQA by detecting incorrect language and token repetitions in answers, and observe that the quality of LLM-generated answers degrades significantly for some low-resource languages. Lastly, we perform human evaluation on a subset of models and languages. Manual evaluation reveals that model performance is significantly worse for culturally specific questions than for culturally agnostic questions. Our findings highlight the need for further research in non-English LFQA and provide an evaluation framework.",
            "score": 0.4611865268325795,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281494140625
        },
        {
            "corpus_id": "270737791",
            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
            "text": "The evaluation of Large Language Models (LLMs) is a key element in their continuous improvement process and many benchmarks have been developed to assess the performance of LLMs in different tasks and topics. As LLMs become adopted worldwide, evaluating them in languages other than English is increasingly important. However, most LLM benchmarks are simply translated using an automated tool and then run in the target language. This means that the results depend not only on the LLM performance in that language but also on the quality of the translation. In this paper, we consider the case of the well-known Massive Multitask Language Understanding (MMLU) benchmark. Selected categories of the benchmark are translated into Spanish using Azure Translator and ChatGPT4 and run on ChatGPT4. Next, the results are processed to identify the test items that produce different answers in Spanish and English. Those are then analyzed manually to understand if the automatic translation caused the change. The results show that a significant fraction of the failing items can be attributed to mistakes in the translation of the benchmark. These results make a strong case for improving benchmarks in languages other than English by at least revising the translations of the items and preferably by adapting the tests to the target language by experts.",
            "score": 0.4611458002652646,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.38916015625
        },
        {
            "corpus_id": "247951931",
            "title": "PaLM: Scaling Language Modeling with Pathways",
            "text": "3, our fairness analyses are narrowly scoped and do not provide an exhaustive account of the various potential risks. For instance, we focus on biases along gender, race, ethnicity and religion axes, but perform the analysis only on the English language data and model outputs. We have not looked into biases along other axes of social disparities, such as sexual orientation (Dixon et al., 2018) or disabilities (Hutchinson et al., 2020), or biases that are salient in non-Western socio-cultural contexts, such as caste (Sambasivan et al., 2021). Hence, it is important to perform fairness analysis along the axes of disparities that are relevant to the target application domain and socio-cultural contexts, in order to meaningfully assess the potential risks.\n\nSimilarly, while our analyses focus on the biases in data and models, the downstream harms they cause will depend on how the models are used in practice (Blodgett et al., 2020;Hutchinson et al., 2020). For instance, while toxic content in training data may seem undesirable in the abstract, if PaLM is used to detect toxic content (e.g., for online content moderation), it is arguably important that the pre-trained model has been exposed to such content.\n\nBeyond the representational bias analysis, it is important to note that the language capabilities of PaLM are likely constrained by the limitations of language present in the training data and evaluation benchmarks. As Raji et al. (2021) point out, benchmark evaluations are often deficient in construct validity, where they may not capture the full complexity of language understanding capabilities and gaps, and may not live up to their claims of general applicability. While the benchmark evaluations we present are useful to assess scientific progress in this area, there are gaps in what they intend to measure and what they actually measure, and similar levels of performance are not guaranteed across different real-world application contexts.\n\nWhile PaLM yields impressive multilingual capabilities on the benchmarks we evaluated against, the majority of our benchmarks (including all of fairness evaluations) are in the English language. More robust evaluations of performance and potential biases in non-English languages are required before PaLM may be used in those languages. Similarly, web pages included in the",
            "score": 0.4608730851516225,
            "section_title": "Ethical Considerations",
            "char_start_offset": 125916,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 376,
                    "end": 396,
                    "matchedPaperCorpusId": "54997157"
                },
                {
                    "start": 413,
                    "end": 438,
                    "matchedPaperCorpusId": "218487466"
                },
                {
                    "start": 521,
                    "end": 546,
                    "matchedPaperCorpusId": "231698728"
                },
                {
                    "start": 917,
                    "end": 940,
                    "matchedPaperCorpusId": "218971825"
                },
                {
                    "start": 940,
                    "end": 964,
                    "matchedPaperCorpusId": "218487466"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.314453125
        },
        {
            "corpus_id": "266903052",
            "title": "Convergences and Divergences between Automatic Assessment and Human Evaluation: Insights from Comparing ChatGPT-Generated Translation and Neural Machine Translation",
            "text": "Such inconsistencies may arise from the divergent constructs of translation quality. In other words, human raters may conceptualize translation quality differently from what can be quantified by automated metrics, and there may be significant dimensions within human evaluation that are not captured by automatic scores (Chung, 2020). \n\nUnderstanding the disparity in how humans and automated systems perceive the construct of translation quality is the crucial first step to leverage their respective advantages. Ideally, given the esteemed status of human evaluation as the gold standard, it is essential to identify the distinctive dimensions inherent in human assessment and incorporate them into existing automated metrics. \n\nThe present study thus aims to address this issue by offering preliminary insights and potential solutions through empirical explorations. We conducted automatic assessment and human evaluation on translation outputs generated by three advanced NMT systems and ChatGPT. Special attention was paid to discern the dimensions that distinguish human-assigned scores from automatic scores Our investigation was conducted on a customized corpus consisting of 6,878 Spokesperson's Remarks. These diplomatic texts possess linguistic complexity and contextual intricacies, making them an ideal testing ground to evaluate how different machine translators handle the nuances and subtleties of language (Jiang et al., 2023). By concentrating on this specific register, we aim to identify dimensions of translation quality that extend beyond accuracy, encompassing dimensions such as cultural awareness and textual coherence. Our ultimate objective is to make automatic assessment more human-like and fine-grained, and to develop a TQA method that exhibits enhanced validity, reliability, and applicability. This endeavor is particularly crucial in light of the emerging challenges posed by the continual advancements in large language models and NMT systems. \n\nWe constructed our major research questions (RQs) as follows: \n\n1  (Brown et al., 2020;Chowdhery et al., 2023;Wei et al., 2022a,b;Wang et al., 2022), and several studies have explored the influence of prompting strategies on the translation performance of LLMs (Jiao et al., 2023;Hendy et al., 2023;Wang et al., 2023;Peng et al., 2023;Lu et al., 2023;Chen et al., 2023;He et al., 2023).",
            "score": 0.4606962098340904,
            "section_title": "Introduction",
            "char_start_offset": 4181,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 334
                },
                {
                    "start": 337,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 728
                },
                {
                    "start": 731,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1644
                },
                {
                    "start": 1645,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 1978
                },
                {
                    "start": 1981,
                    "end": 2042
                },
                {
                    "start": 2045,
                    "end": 2367
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 333,
                    "matchedPaperCorpusId": "218667104"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08135986328125
        },
        {
            "corpus_id": "275471651",
            "title": "Lessons From Red Teaming 100 Generative AI Products",
            "text": "Most AI research is conducted in Western cultural contexts, and modern language models use predominantly English pre-training data, performance benchmarks, and safety evaluations [1,14]. Nonetheless, non-English tokens in large-scale text corpora often give rise to multilingual capabilities [5], and model developers are increasingly training LLMs with enhanced abilities in non-English languages, including Microsoft. Recently, AIRT tested the multilingual Phi-3.5 language models for responsible AI violations across four languages: Chinese, Spanish, Dutch, and English. Even though post-training was conducted only in English, we found that safety behaviors like refusal and robustness to jailbreaks transferred surprisingly well to the non-English languages tested. Further investigation is required to assess how well this trend holds for lower resource languages and to design red teaming probes that not only account for linguistic differences, but also redefine harms in different political and cultural contexts [11]. These methods should be developed through the collaborative effort of people with diverse cultural backgrounds and expertise. \n\nEmotional intelligence. Finally, the human element of AI red teaming is perhaps most evident in answering questions about AI safety that require emotional intelligence, such as: \"how might this model response be interpreted in different contexts?\" and \"do these outputs make me feel uncomfortable?\" Ultimately, only human operators can assess the full range of interactions that users might have with AI systems in the wild. Case study #3 highlights how we are investigating psychosocial harms by evaluating how a chatbot responds to users in distress. \n\nIn order to make these assessments, red teamers may be exposed to disproportionate amounts of unsettling and disturbing AI-generated content. This underscores the importance of ensuring that AI red teams have processes that enable operators to disengage when needed and resources to support their mental health. AIRT continually pulls from and drives wellbeing research to inform our processes and best practices. \n\nLesson 6: Responsible AI harms are pervasive but difficult to measure Many of the human aspects of AI red teaming discussed above apply most directly to RAI impacts.",
            "score": 0.4606167496928047,
            "section_title": "Lesson 5: The human element of AI red teaming is crucial",
            "char_start_offset": 26039,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1153
                },
                {
                    "start": 1156,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1403
                },
                {
                    "start": 1404,
                    "end": 1454
                },
                {
                    "start": 1455,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1708
                },
                {
                    "start": 1711,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 2022
                },
                {
                    "start": 2023,
                    "end": 2124
                },
                {
                    "start": 2127,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 295,
                    "matchedPaperCorpusId": "252780005"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1253662109375
        },
        {
            "corpus_id": "276250064",
            "title": "SeaExam and SeaBench: Benchmarking LLMs with Local Multilingual Questions in Southeast Asia",
            "text": "For example, as shown in Figure 1, we visualize the distribution of objects in questions collected from local usage queries versus those translated from English. Compared to local usage queries, translated questions based on English exhibit more of an American context, e.g., involving the place \"Hawaii\". It shows that translated questions cover only a small portion of the entities in local usage queries, indicating a significant divergence in the query context. \n\nConsidering the scarcity of such effective multilingual benchmarks, this paper introduces two new benchmarks, SeaExam and SeaBench. These benchmarks are specifically designed to address the unique application scenarios and cultural contexts of Southeast Asian (SEA) countries, which often differ significantly from western-centric datasets. Following the design principles of two widely used English-based datasets, MMLU and MT-bench, we do not simply translate the original English questions but incorporate real-world usage scenarios from SEA natives into the content -allowing us to measure a model's adaptability in multilingual application scenarios. Specifically, SeaExam is a multitask exam dataset sourced from real exams in SEA countries that cover a wide range of subjects including local history, geography, and literature. SeaBench, following MT-Bench's approach, focuses on multi-turn instruction-following tasks spanning ten task categories. It incorporates scenarios and instructions that are commonly encountered in SEA cultures and daily life. \n\nOur experimental analysis quantitatively demonstrates that, 1) Compared to the translated benchmarks MMLU and MT-bench, our SeaExam and SeaBench benchmarks include questions that are more aligned with the daily usage of regional languages (Section 3.1). 2) Furthermore, using SeaExam and SeaBench, we are able to more effectively discern the capabilities of models in real-world multilingual applications (Section 3.2.1). Further analysis reveals that 3) While multiplechoice questions in exam datasets can objectively measure model capabilities, open-ended questions are more effective in highlighting differences in model performance across various languages (Section 3.2.2 and Section 3.2.3).",
            "score": 0.46034084988099944,
            "section_title": "Object embedding distribution",
            "char_start_offset": 2913,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 465
                },
                {
                    "start": 468,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1952
                },
                {
                    "start": 1953,
                    "end": 2226
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "271771141",
            "title": "Language discrepancies in the performance of generative artificial intelligence models: an examination of infectious disease queries in English and Arabic",
            "text": "Third, the study did not account for the various Arabic dialects, focusing only on the Standard Arabic. Future research could expand on this particular issue in light of the previous evidence showing potential variability in dialectical performance [94,100]. Fourth, in this study, we adhered to pre-established consensus key responses to maintain objectivity upon expert assessment of the AI-generated content. However, this approach limited our ability to capture dynamic consensus that could emerge from direct raters' interactions. The observed discrepancies in expert raters' agreement, especially in the evaluations of AI-generated content in English, suggest that linguistic complexity and the subjective nature of certain CLEAR components impacted the consistency of assessments. The notably low agreement on the Relevance of content in English might reflect broader issues in interpreting relevance across different medical contexts, where the raters may hold divergent views based on their backgrounds and expertise. Future studies could benefit from refined guidelines for these components, potentially incorporating more structured and detailed criteria to aid raters in achieving higher consistency. Fifth, in translating the queries from English to Arabic, the study employed a simplified practical approach where two bilingual authors independently translated the queries. This expedient method did not follow the rigorous, standardized procedures recommended for cross-cultural healthcare research, such as those outlined by Sousa and Rojjanasrirat [101]. Consequently, this might have introduced variations in the semantic equivalence of the queries across languages, potentially affecting the reliability and validity of the responses. Future research should consider implementing a more structured translation methodology, including validation by a panel of linguistic and subject matter experts. Finally, future studies can benefit from including a broader range of queries involving not only infectious disease topics to achieve a more comprehensive understanding of AI performance in diverse health and linguistic contexts. Addressing these limitations in future studies can help to advance the collective understanding of multilingual generative AI applications and to enhance the generative AI tools' reliability and equity in global healthcare settings.",
            "score": 0.45990278043143834,
            "section_title": "Discussion",
            "char_start_offset": 26683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1212
                },
                {
                    "start": 1213,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2145
                },
                {
                    "start": 2146,
                    "end": 2378
                }
            ],
            "ref_mentions": [
                {
                    "start": 253,
                    "end": 257,
                    "matchedPaperCorpusId": "267302989"
                },
                {
                    "start": 1565,
                    "end": 1570,
                    "matchedPaperCorpusId": "1341509"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2130126953125
        },
        {
            "corpus_id": "257206042",
            "title": "Fairness in Language Models Beyond English: Gaps and Challenges",
            "text": "With language models becoming increasingly ubiquitous, it has become essential to address their inequitable treatment of diverse demographic groups and factors. Most research on evaluating and mitigating fairness harms has been concentrated on English, while multilingual models and non-English languages have received comparatively little attention. In this paper, we survey different aspects of fairness in languages beyond English and multilingual contexts. This paper presents a survey of fairness in multilingual and non-English contexts, highlighting the shortcomings of current research and the difficulties faced by methods designed for English. We contend that the multitude of diverse cultures and languages across the world makes it infeasible to achieve comprehensive coverage in terms of constructing fairness datasets. Thus, the measurement and mitigation of biases must evolve beyond the current dataset-driven practices that are narrowly focused on specific dimensions and types of biases and, therefore, impossible to scale across languages and cultures.",
            "score": 0.4591574455066133,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1805419921875
        },
        {
            "corpus_id": "273532712",
            "title": "Dialectal and Low Resource Machine Translation for Aromanian",
            "text": "Two human evaluators were involved in this study. Both were native Aromanian speakers residing in Romania, so they can speak both Romanian and Aromanian. We do not evaluate for English here. The sentences given for evaluation contain both human and machine translations in equal proportion and so each annotator evaluates a total of 80 samples. For machine translation, we use our NLLB 600M fine-tuned model. \n\nAnnotators are instructed to assign direct assessment scores (Graham et al., 2013) from 1 to 10 to evaluate the quality of each translation (1 being the lowest score, 10 being the highest). This scale is culturally motivated by the fact that it is familiar to the study participants because it is used in Romanian schools at all levels of post-primary school. \n\nThey assigned a score taking into account three categories: fluency, style, and meaning (logical sense). Fluency refers to how grammatically correct the sentence is. Style denotes how likely it is for the speaker to build the sentence they are evaluating. Meaning alludes to the logical sense of the sentence. \n\nThe evaluation was carried out bilingually for all language pairs. The results can be seen in Table 5. The outcome is dubious, as it may appear that machine translation is rated better than human translation. Manually analyzing the samples, we observe that oftentimes, the human translations are more creative, whilst the model tends to keep the structure of the original sentence mot-a-mot. In addition, the small number of human evaluators together with the small sample size renders the human evaluation inconclusive.",
            "score": 0.45898065808758676,
            "section_title": "Human Evaluation",
            "char_start_offset": 20693,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1082
                },
                {
                    "start": 1085,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1476
                },
                {
                    "start": 1477,
                    "end": 1605
                }
            ],
            "ref_mentions": [
                {
                    "start": 472,
                    "end": 493,
                    "matchedPaperCorpusId": "1128384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05340576171875
        },
        {
            "corpus_id": "274822728",
            "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation",
            "text": "In this work we have included bias and toxicity benchmark datasets that were largely created around English-centric prompts and biases, and are therefore also mostly translations of English datasets. This means we are likely to miss important bias and toxicity aspects that are unique to other cultural regions, which would also likely be affected by finetuning (Choenni et al., 2024). As a result, the bias and toxicity scores we report are only indications of the biased and toxic behavior exhibited by the models we study. Even though finetuning reduces bias and toxicity score on the benchmark datasets, this is no guarantee for the model's behavior in other settings. \n\nExpanse has benefited from improvements to the training procedure, such as data arbitrage (Odumakinde et al., 2024), multilingual preference training (Dang et al., 2024), multilingual safety tuning (Aakanksha et al., 2024), and model merging (Aakanksha et al., 2024). Again, we use the Aya Expanse model with 8B parameters. \n\nGemma 2 (Google, 2024) are a family of LLMs that were primarily trained on English data, including web documents, code, and mathematical text. \n\nThis training data was filtered, to make sure no personal information or other sensitive, harmful or illegal content was included. We use the 9B model that was trained on 8 trillion tokens, and the 2B model that was trained on 2 trillion tokens. \n\nLlama 3 (AI@Meta, 2024a) are a family of LLMs that were primarily trained on English data, though over 5% of the 15 trillion pretraining tokens consist of non-English data covering 30 languages. After this pretraining stage, Llama 3 has gone through supervised fine-tuning, rejection sampling, proximal policy optimization (PPO) and direct preference optimization (DPO). We use the version with 8B parameters. \n\nLlama 3.1 (AI@Meta, 2024b) are a family of LLMs trained on more multilingual data compared to Llama 3, and instruction-tuned on 8 languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.",
            "score": 0.4584939204384544,
            "section_title": "Ethical Considerations",
            "char_start_offset": 29819,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 385
                },
                {
                    "start": 386,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1143
                },
                {
                    "start": 1146,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1391
                },
                {
                    "start": 1394,
                    "end": 1588
                },
                {
                    "start": 1589,
                    "end": 1764
                },
                {
                    "start": 1765,
                    "end": 1803
                },
                {
                    "start": 1806,
                    "end": 2018
                }
            ],
            "ref_mentions": [
                {
                    "start": 362,
                    "end": 384,
                    "matchedPaperCorpusId": "272336905"
                },
                {
                    "start": 825,
                    "end": 844,
                    "matchedPaperCorpusId": "270924100"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.19873046875
        },
        {
            "corpus_id": "267759768",
            "title": "CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models",
            "text": "Specifically, models usually perform poorly in a new \"programming language\" environment that requires the model to understand restricted actions.As for summarization tasks, models could misinterpret the instruction, eg.models sometimes consider the instruction \"modify the input into a more friendly expression to non-native speakers\" as a Chinese-English translation task and might provide redundant explanations even if not required by the instructions and hence will cause large semantic distances to the golden reference.We point out that Chinese-commented code corpora and parallel translation data of Chinese and other languages are still scarce resources, which might lead to their poor performance on CIF-Bench's code and translation categories.Additionally, we assume that Chinese and English bilingual LLMs, although a major branch of multilingual LLM, do not signifi-cantly benefit LLMs' capacity to deal with minorlanguage-related tasks.Part of the tasks in CIF-Bench's summarization category are very challenging, combining counterfactual reasoning and empathy estimation (i.e., task 125 and task 131 referring to Appendix A.1). Thereby, the bilingual LLMs' poor performance on CIF-Bench's summarization category is understandable.Detailed category-based scores on the public split are available in Table 13 in Appendix B for further analysis.\n\nLanguage Transferability.We select the public split to investigate LLM language transferability in instruction-following.In the CIF-Bench public split, a set of 70 tasks from SNI (Wang et al., 2022b) are used as representative samples of English NLP tasks equipped with directly translated input-output pairs in Chinese.We select the top-5 performing models on the public split to show the performance comparison between SNI and our 37 original curated Chinese tasks in Table 5.Although these models maintain instruction-following capability when encountering the translated SNI data, they generally perform worse on tasks newly created in Chinese without a corresponding \"copy\" in English, which yields an average score decrement of 2.2%.\n\nData Contamination Does Exist.As mentioned in \u00a73, we evaluate the model performances on the public split with half of the input-output pairs in the single instruction setting, with which we can conveniently probe the benchmark data contamination issue of the LLMs.",
            "score": 0.4583004719410595,
            "section_title": "Experiments",
            "char_start_offset": 17261,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 145,
                    "end": 219
                },
                {
                    "start": 219,
                    "end": 525
                },
                {
                    "start": 525,
                    "end": 753
                },
                {
                    "start": 753,
                    "end": 949
                },
                {
                    "start": 949,
                    "end": 1244
                },
                {
                    "start": 1244,
                    "end": 1356
                },
                {
                    "start": 1358,
                    "end": 1383
                },
                {
                    "start": 1383,
                    "end": 1479
                },
                {
                    "start": 1479,
                    "end": 1678
                },
                {
                    "start": 1678,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 2097
                },
                {
                    "start": 2099,
                    "end": 2129
                },
                {
                    "start": 2129,
                    "end": 2363
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12249755859375
        },
        {
            "corpus_id": "277066532",
            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
            "text": "As large language models (LLMs) rapidly advance, evaluating their performance is critical. LLMs are trained on multilingual data, but their reasoning abilities are mainly evaluated using English datasets. Hence, robust evaluation frameworks are needed using high-quality non-English datasets, especially low-resource languages (LRLs). This study evaluates eight state-of-the-art (SOTA) LLMs on Latvian and Giriama using a Massive Multitask Language Understanding (MMLU) subset curated with native speakers for linguistic and cultural relevance. Giriama is benchmarked for the first time. Our evaluation shows that OpenAI's o1 model outperforms others across all languages, scoring 92.8% in English, 88.8% in Latvian, and 70.8% in Giriama on 0-shot tasks. Mistral-large (35.6%) and Llama-70B IT (41%) have weak performance, on both Latvian and Giriama. Our results underscore the need for localized benchmarks and human evaluations in advancing cultural AI contextualization.",
            "score": 0.458248218389,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1400146484375
        },
        {
            "corpus_id": "273403947",
            "title": "A Dutch Financial Large Language Model",
            "text": "One major issue in the current state of financial LLMs is the lack of models for languages other than English. Because English is a globally prominent language for international finance as well as machine learning research, it has been the focus for the majority of the existing work on financial LLMs. However, the comparative lack of models available in other languages represents a significant knowledge gap, preventing an in-depth understanding of localized financial contexts and nuances. The development of LLMs in these other languages is dependent on the availability of sufficient datasets and appropriate evaluation methods. Although extensive datasets are available for English, there is a notable scarcity of such datasets for other languages [19]. \n\nThe development of financial LLMs in languages other than English is still at an early stage. Notable examples include the Mengzi-fin [36] and BBT-FinT5 [14] models, both uniquely designed for the Chinese language. In addition to non-English languagespecific models, multilingual financial LLMs, such as FinMA-ES [35] and ICE-PIXIU [10], have been developed, which have been fine-tuned on Spanish and English data and Chinese and English data, respectively. These models attempt to capture the linguistic nuances of the financial domain in various languages. \n\nLanguage-specific financial benchmarks are a crucial factor for evaluating the impact and effectiveness of these financial LLMs in diversified linguistic contexts [19]. The gap between English and other languages in the domain of financial language models is the absence of financial benchmarks tailored to those languages. A worthy mention is a newly established benchmark for Japanese financial LLMs, although it does not accompany a specified LLM [9]. \n\nTable 1 compares the different financial LLMs available today across several parameters, including language compatibility. The addition of Chinese-specific models and multilingual models indicate a diversification in language applicability beyond English. For a complete analysis of the existing financial LLMs, see [19].",
            "score": 0.4582269588371243,
            "section_title": "The Scarcity of Non-English Financial LLMs",
            "char_start_offset": 6284,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 111,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 760
                },
                {
                    "start": 763,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1321
                },
                {
                    "start": 1324,
                    "end": 1492
                },
                {
                    "start": 1493,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1778
                },
                {
                    "start": 1781,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2036
                },
                {
                    "start": 2037,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 1095,
                    "end": 1099,
                    "matchedPaperCorpusId": "268357052"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06878662109375
        },
        {
            "corpus_id": "274610534",
            "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
            "text": "The proliferation of LLMs capable of generating fluent and plausible responses has significantly complicated the task of assessing their knowledge and reasoning abilities (Li et al., 2024). To address this challenge, researchers have developed a range of benchmarks designed to assess diverse capabilities of models such as GLUE (Wang et al., 2018) and MMLU (Hendrycks et al., 2020). Although these benchmarks are primarily in English, when addressing multilingual issues, current evaluation practices often depend on translations of those datasets originally in English (Shi et al., 2022). However, a critical issue arises as their inherent Western cultural bias makes them unsuitable and inappropriate for evaluating LLMs across a variety of cultures and languages. Therefore, there has been a notable trend toward the development of culturally specific benchmarks that cater to local contexts such as IndiBias (Sahoo et al., 2024) for Indian culture, Heron-Bench (Inoue et al., 2024) for Japanese culture, and CCPM (Li et al., 2021) and ChID (Zheng et al, 2019) for Chinese culture. However, comparable research in Korea is still relatively scarce. \n\nTo bridge this resource gap, we developed KULTURE Bench, which includes 3584 instances across three datasets. This benchmark not only evaluates cultural knowledge but also includes datasets that inherently feature idioms and poetry, enriching the linguistic elements with deep cultural significance. Additionally, it thoroughly assesses language models' cultural comprehension and reasoning abilities across word, sentence, and paragraph levels. \n\nWe conducted evaluations on models trained with various primary corpora, including Clova X (Korean), GPT-4 and GPT-3.5 Turbo (English), and Tiangong (Chinese). The evaluation results show that, overall, the native Korean model, Clova X, performed the best, followed by GPT-4. However, even the highest-performing Clova X achieves only a 69.12% accuracy on the dataset containing idiomatic expressions, and the best result for the dataset featuring poetry is a mere 45.7% accuracy by GPT-4, which suggests that LLMs still face challenges when processing texts embedded with deep cultural and historical contexts.",
            "score": 0.45789745186400654,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 1085
                },
                {
                    "start": 1086,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1599
                },
                {
                    "start": 1602,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 171,
                    "end": 188,
                    "matchedPaperCorpusId": "259164635"
                },
                {
                    "start": 913,
                    "end": 933,
                    "matchedPaperCorpusId": "268793592"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.287353515625
        },
        {
            "corpus_id": "201741133",
            "title": "Findings of the 2019 Conference on Machine Translation (WMT19)",
            "text": "In previous year's evaluation translated segments for all language pairs were evaluated independent of the wider document context. However, since recent MT evaluations address the question of comparison of system and human performance, evaluation within document context has become more relevant (L\u00e4ubli et al., 2018;Toral et al., 2018). Therefore, for a selection of language pairs, human evaluation was carried out within the document context. We denote the two options \"+DC\" (with document context) and \"\u2212DC\" (without document context) in the following. Additionally in past years, test data included text that was created in the opposite direction to testing, in order to achieve a larger test set with limited resources. Inclusion of test data has been shown to introduce inaccuracies in evaluations particularly in terms of BLEU scores however  and for this reason, this year we only test systems on data that was originally written in the source language.\n\nIn previous years we have employed only monolingual human evaluation (denoted \"M\" in the following for official results. Last year we trialled source-based evaluation for English to Czech translation, i.e. a bilingual configuration (\"B\") in which the human assessor is shown the source input and system output only (with no reference English to German assessment from the human evaluation campaign. The annotator is presented with the machine translation output document randomly selected from competing systems (anonymized) and is asked to rate the translation on a sliding scale. translation shown). This approach has the advantage of freeing up the human-generated reference translation so that it can be included in the evaluation as another system and provide an estimate of human performance. Since we would like to restrict human assessors to only evaluate translation into their native language, we restricted bilingual/source-based evaluation to evaluation of translation for out-of-English language pairs. This is especially relevant since we have a large group of volunteer human assessors with native language fluency in non-English languages and high fluency in English, while we generally lack the reverse, native English speakers with high fluency in non-English languages. A summary of the human evaluation configurations run this year in the news task is provided in Table 7, where configurations that correspond to official results are highlighted in bold.\n\nThe style of official evaluation used in the past recent years",
            "score": 0.4573851739011799,
            "section_title": "Styles of Direct Assessment Tested in WMT19",
            "char_start_offset": 42906,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 296,
                    "end": 317,
                    "matchedPaperCorpusId": "49746614"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0830078125
        },
        {
            "corpus_id": "259129374",
            "title": "Evaluating the Social Impact of Generative AI Systems in Systems and Society",
            "text": "imagery included in datasets as well as decisions made about that data, including filtering and reward modeling, will impact how the model performs for different groups or categories of concepts associated with groups. Generative image models for example, may output varying quality generations when producing different concepts, with quality referring to photorealism, aesthetic quality, and conceptual richness [170].\n\nEvaluating model generations across subpopulation languages, accents, and similar topics using the same evaluation criteria as the highest performing language or accent can illustrate areas where there is disparate performance and can help document areas for further model development and mitigation work.\n\nLimitations Similar limitations that lead to disparate system performance contribute to disparate attention to evaluations for different groups. Performance evaluations for similar tasks in non-English languages will vary by the amount of resourcing for a given language. More spoken and digitized languages may have more evaluations than lower-resource languages.",
            "score": 0.4572051942675581,
            "section_title": "Disparate Performance",
            "char_start_offset": 24815,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.060516357421875
        },
        {
            "corpus_id": "273404060",
            "title": "HEALTH-PARIKSHA: Assessing RAG Models for Health Chatbots in Real-World Multilingual Settings",
            "text": "Large Language Models (LLMs) have demonstrated impressive proficiency across various domains. Nonetheless, their full spectrum of capabilities and limitations remains unclear, resulting in unpredictable performance on certain tasks. Additionally, there is now a wide selection of LLMs available. Therefore, evaluation has become crucial for comprehending the internal mechanisms of LLMs and for comparing them against each other. \n\nDespite the importance of evaluation, significant challenges still persist. Many widely-used benchmarks for assessing LLMs are contaminated (Ahuja et al., 2024;Oren et al., 2024;Xu et al., 2024), meaning that they often appear in LLM training data. Some of these benchmarks were originally created for conventional Natural Language Processing tasks and may not fully represent current practical applications of LLMs (Conneau et al., 2018;Pan et al., 2017). Recently, there has been growing interest in assessing LLMs within multilingual and multicultural contexts (Ahuja et al., 2023(Ahuja et al., , 2024;;Faisal et al., 2024;Watts et al., 2024;Chiu et al., 2024). Traditionally, these benchmarks were developed by translating English versions into various languages. However, due to the loss of linguistic and cultural context during translation, new benchmarks specific to different languages and cultures are now being created. However, such benchmarks are few in number, and several of the older ones are contaminated in training data (Ahuja et al., 2024;Oren et al., 2024). Thus, there is a need for new benchmarks that can test the abilities of models in real-world multilingual settings. \n\nLLMs are employed in various fields, including critical areas like healthcare. Jin et al. (2024) translate an English healthcare dataset into Spanish, Chinese, and Hindi, and demonstrate that performance declines in these languages compared to English. This highlights the necessity of examining LLMs more thoroughly in multilingual contexts for these important uses. \n\nIn this study, we conduct the first comprehensive assessment of multilingual models within a realworld healthcare context.",
            "score": 0.4569497186058091,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 93
                },
                {
                    "start": 94,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 429
                },
                {
                    "start": 432,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1996
                },
                {
                    "start": 1999,
                    "end": 2121
                }
            ],
            "ref_mentions": [
                {
                    "start": 572,
                    "end": 592,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 592,
                    "end": 610,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 870,
                    "end": 887,
                    "matchedPaperCorpusId": "29939583"
                },
                {
                    "start": 996,
                    "end": 1015,
                    "matchedPaperCorpusId": "257663467"
                },
                {
                    "start": 1015,
                    "end": 1038,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 1471,
                    "end": 1491,
                    "matchedPaperCorpusId": "265150423"
                },
                {
                    "start": 1491,
                    "end": 1509,
                    "matchedPaperCorpusId": "264490730"
                },
                {
                    "start": 1708,
                    "end": 1725,
                    "matchedPaperCorpusId": "264405758"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.281005859375
        },
        {
            "corpus_id": "268856712",
            "title": "HyperCLOVA X Technical Report",
            "text": "Numerous benchmarks have been proposed to objectively evaluate the capabilities of LLMs along various dimensions of quality.In this section, we present a detailed analysis of HyperCLOVA X's performance on a core set of benchmarks.\n\nBenchmark Design.A primary constraint in the advances of multilingual language models has been the absence of thorough evaluation frameworks for languages other than English (\u00dcst\u00fcn et al., 2024).\n\nCompetence in a particular language involves more than just linguistic proficiency; it also requires a profound understanding of the cultural and societal nuances unique to its speakers.To evaluate the bilingual and general capabilities of our models, we systematically utilize widely recognized English and Korean benchmarks sourced both our in-house and externally.Given that core competencies like reasoning, world knowledge, and mathematics transcend language, a significant portion of these benchmarks is conducted in English to assess language-neutral skills.On the other hand, to gauge the model's adeptness at incorporating various dimensions of intelligence in answering language-specific questions and addressing cultural nuances, we utilize two detailed benchmark categories tailored to each language.\n\nFor assessing proficiency in Korean, our benchmarks, unlike their machine-translated equivalents (Conneau et al., 2018;Achiam et al., 2023), are either meticulously crafted by experts or curated from existing well-recognized work.These benchmarks include region-specific questions, such as those found in KoBigBench (KBB), a comprehensive Korean benchmark built from an internal effort, and the Korean-specific question set within KMMLU (Son et al., 2024), ensuring a rigorous evaluation of the model's understanding of Korean cultural and societal contexts.Further information on the Korean benchmarks can be found in Section 3.1.\n\nBaselines.HyperCLOVA X is a uniquely designed set of LLMs with inherent proficiency in both Korean and English, setting them apart from other models in the field and lacking a directly comparable counterpart.To provide a comprehensive view of their diverse capabilities, we compare HyperCLOVA X with Korean-focused LLMs to showcase their fluency in Korean and with general foundational models to highlight the language-agnostic core competencies.",
            "score": 0.45690254892059295,
            "section_title": "Core Benchmarks",
            "char_start_offset": 15405,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 124,
                    "end": 230
                },
                {
                    "start": 232,
                    "end": 249
                },
                {
                    "start": 249,
                    "end": 427
                },
                {
                    "start": 429,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 796
                },
                {
                    "start": 796,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1241
                },
                {
                    "start": 1243,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1874
                },
                {
                    "start": 1876,
                    "end": 1886
                },
                {
                    "start": 1886,
                    "end": 2084
                },
                {
                    "start": 2084,
                    "end": 2322
                }
            ],
            "ref_mentions": [
                {
                    "start": 1340,
                    "end": 1362,
                    "matchedPaperCorpusId": "52271711"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1502685546875
        },
        {
            "corpus_id": "270845705",
            "title": "Voices Unheard: NLP Resources and Models for Yor\u00f9b\u00e1 Regional Dialects",
            "text": "A limitation of our work is the robustness of the metrics we use for evaluation.While all of these metrics are standard for all of the tasks, we acknowledge that model-based metrics like AfriCOMET (Wang et al., 2024) could be biased towards standard dialects that their models have been trained on.Exploring model-based metrics that facilitate robust evaluations on dialectal tasks remains a challenge for future work (Faisal et al., 2024).\n\nAdditionally, the text portion of our dataset is translated from the standard dialect into English and the non-standard dialects.We acknowledge that this could introduce translation artifacts known as translationese (Volansky et al., 2015) that are not present in the source dialect.However, we believe that the benefits of our dataset outweighs the potential risks of these artifacts.",
            "score": 0.45685411351300453,
            "section_title": "Limitations",
            "char_start_offset": 26863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 80,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 440
                },
                {
                    "start": 442,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 725
                },
                {
                    "start": 725,
                    "end": 827
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 215,
                    "matchedPaperCorpusId": "269042892"
                },
                {
                    "start": 658,
                    "end": 681,
                    "matchedPaperCorpusId": "5133943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11456298828125
        },
        {
            "corpus_id": "259360395",
            "title": "A Survey on Evaluation of Large Language Models",
            "text": "While English is the predominant language, many LLMs are trained on mixed-language training data. The combination of multilingual data indeed helps LLMs gain the ability to process inputs and generate responses in different languages, making them widely adopted and accepted across the globe. However, due to the relatively recent emergence of this technology, LLMs are primarily evaluated on English data, leading to a potential oversight of evaluating their multilingual performance. To address this, several articles have provided comprehensive, open, and independent evaluations of LLMs' performance on various NLP tasks in different non-English languages. These evaluations offer valuable insights for future research and applications. \n\nAbdelali et al. [1] evaluated the performance of ChatGPT in standard Arabic NLP tasks and observed that ChatGPT exhibits lower performance compared to SOTA models in the zero-shot setting for most tasks. Ahuja et al. [2], Bang et al. [6], Lai et al. [100], Zhang et al. [250] utilized a greater number of languages across multiple datasets, encompassing a wider range of tasks, and conducted a more comprehensive evaluation of LLMs, including BLOOM, Vicuna, Claude, ChatGPT, and GPT-4. The results indicated that these LLMs perform poorly when it came to non-Latin languages and languages with limited resources. Despite translating the input to English and using it as the query, generative LLMs still displays subpar performance across tasks and languages compared to SOTA models [2]. Furthermore, Bang et al. [6] highlighted that ChatGPT still faces a limitation in translating sentences written in non-Latin script languages with rich linguistic resources. The aforementioned demonstrates that there are numerous challenges and ample opportunities for enhancement in multilingual tasks for LLMs. Future research should prioritize achieving multilingual balance and addressing the challenges faced by non-Latin languages and low-resource languages, with the aim of better supporting users worldwide. At the same time, attention should be paid to the impartiality and neutrality of the language in order to mitigate any potential biases, including English bias or other biases, that could impact multilingual applications. \n\n3.1.5 Factuality.",
            "score": 0.4567384147937828,
            "section_title": "Multilingual tasks.",
            "char_start_offset": 27661,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 740
                },
                {
                    "start": 743,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2267
                },
                {
                    "start": 2270,
                    "end": 2275
                },
                {
                    "start": 2276,
                    "end": 2287
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08526611328125
        },
        {
            "corpus_id": "264146033",
            "title": "Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance",
            "text": "This observation highlights that zero-shot performance is much more prone to variations compared to the performance of supervised directions. This raises the question: What factors contribute to the significant variations observed in zero-shot performance? \n\nExploring the Role of Non-English-Centric Systems Training with non-English language pairs has shown promise in improving zero-shot performance (Fan et al., 2021). To delve deeper into this aspect, we evaluate m2m100 models directly without further finetuning on our benchmark test set because our goal is to investigate whether the high variations in the zero-shot performance phenomenon hold for non-English-centric models. \n\nOur analysis consists of English-centric (54), supervised (546), and zero-shot (860) directions, which are determined by the training settings of m2m100. The results in Table 2 yield two important observations. Firstly, significant performance gaps exist between supervised and zero-shot directions, suggesting that the challenges of zero-shot translation persist even in non-English-centric systems. More importantly, our finding of considerable variations in zero-shot NMT also holds for non-English-centric systems.",
            "score": 0.45659486256607756,
            "section_title": "Variations in Zero-Shot NMT",
            "char_start_offset": 15937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 684
                },
                {
                    "start": 687,
                    "end": 840
                },
                {
                    "start": 841,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1087
                },
                {
                    "start": 1088,
                    "end": 1205
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 421,
                    "matchedPaperCorpusId": "224814118"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "52179151",
            "title": "Assessing gender bias in machine translation: a case study with Google Translate",
            "text": "In this paper, we have provided preliminary evidence that statistical translation tools such as Google Translate can exhibit gender biases and a strong tendency towards male defaults. Although implicit, these biases possibly stem from the real world data which is used to train them, and in this context possibly provide a window into the way our society talks (and writes) about women in the workplace. In this paper, we suggest that and test the hypothesis that statistical translation tools can be probed to yield insights about stereotypical gender roles in our society -or at least in their training data. By translating professional-related sentences such as \"He/She is an engineer\" from gender neutral languages such as Hungarian and Chinese into English, we were able to collect statistics about the asymmetry between female and male pronominal genders in the translation outputs. Our results show that male defaults are not only prominent but exaggerated in fields suggested to be troubled with gender stereotypes, such as STEM (Science, Technology, Engineering and Mathematics) jobs. And because Google Translate typically uses English as a lingua franca to translate between other languages (e.g. Chinese \u2192 English \u2192 Portuguese) (Google, 2017;Boitet, Blanchon, Seligman, & Bellynck, 2010), our findings possibly extend to translations between gender neutral languages and non-gender neutral languages (apart from English) in general, although we have not tested this hypothesis. \n\nAlthough not conclusive, our results seem to suggest that this phenomenon extends beyond the scope of the workplace, with the proportion of female pronouns varying significantly according to adjectives used to describe a person. Adjectives such as Shy and Desirable are translated with a larger proportion of female pronouns, while Guilty and Cruel are almost exclusively translated with male ones. Different languages also seemingly have a significant impact in machine gender bias, with Hungarian exhibiting a better equilibrium between male and female pronouns than for example Chinese. Some languages such as Yoruba and Basque were found to translate sentences with gender neutral pronouns very often, although this is the exception rather than the rule and Basque also exhibits a high frequency of phrases for which we could not automatically extract a gender pronoun. \n\nTo solidify our results, we ran our pronominal gender translation statistics against the U.S. Bureau of Labor Statistics data on the frequency of women participation for each job position.",
            "score": 0.4563022666184507,
            "section_title": "Conclusions",
            "char_start_offset": 29865,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1207
                },
                {
                    "start": 1208,
                    "end": 1489
                },
                {
                    "start": 1492,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1890
                },
                {
                    "start": 1891,
                    "end": 2081
                },
                {
                    "start": 2082,
                    "end": 2365
                },
                {
                    "start": 2368,
                    "end": 2556
                }
            ],
            "ref_mentions": [
                {
                    "start": 1254,
                    "end": 1299,
                    "matchedPaperCorpusId": "1113613"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0531005859375
        },
        {
            "corpus_id": "271309919",
            "title": "CoVoSwitch: Machine Translation of Synthetic Code-Switched Text Based on Intonation Units",
            "text": "Models.We use the HuggingFace pre-trained model checkpoints facebook/m2m100_418M and facebook/nllb-200-distilled-600M for the M2M-100 418M and NLLB-200 600M models.These two models were chosen for their exceptional multilingual capabilities, with M2M-100 intended for non-English centric translation and NLLB-200 designed to improve translation performance in low-resource languages.Both support all languages covered by our synthetic dataset.\n\nTranslation Settings.We experiment with four translation settings for each of the English and non-English language pairs.First is csw\u2192En, in which code-switched text is translated into English.This setting was examined in previous research (Nguyen et al., 2023;Xu and Yvon, 2021), but we also experiment with csw\u2192X to analyze any performance gaps that may arise by setting target language for translation differently.We compare these two codeswitching translation settings to two monolingual translation settings, X\u2192En and En\u2192X, where X is a non-English language and En is English.Baselines.Our baselines are twofold.First, we compare code-switching translations with monolingual translations and interpret deltas from monolingual baselines as the gains or losses from introducing code-switching units.We set our second baseline in consideration of our synthetic code-switched inputs.Because synthetic code-switched inputs already contain segments from reference texts, evaluation scores for these may be higher than translations of solely monolingual texts.In light of this, we consider deltas from raw code-switched inputs the performance of systems in translating code-switched text.Evaluation Metrics.We measure the performance of translation models with the following automatic metrics: chrF++ (Popovi\u0107, 2017) at the character level, spBLEU (Goyal et al., 2022) at the languageagnostic subword level tokenized through Sentence-Piece (Kudo and Richardson, 2018), and COMET (Rei et al., 2020) at the detokenized representation level.spBLEU and chrF++ measure similarity between reference translation and system translation, while COMET predicts human judgments of system translations based on a neural model.",
            "score": 0.4554621051357062,
            "section_title": "Machine Translation Experimental Setup",
            "char_start_offset": 9425,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 7
                },
                {
                    "start": 7,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 383
                },
                {
                    "start": 383,
                    "end": 443
                },
                {
                    "start": 445,
                    "end": 466
                },
                {
                    "start": 466,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 862
                },
                {
                    "start": 862,
                    "end": 1026
                },
                {
                    "start": 1026,
                    "end": 1036
                },
                {
                    "start": 1036,
                    "end": 1062
                },
                {
                    "start": 1062,
                    "end": 1247
                },
                {
                    "start": 1247,
                    "end": 1329
                },
                {
                    "start": 1329,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1631
                },
                {
                    "start": 1631,
                    "end": 1650
                },
                {
                    "start": 1650,
                    "end": 1981
                },
                {
                    "start": 1981,
                    "end": 2156
                }
            ],
            "ref_mentions": [
                {
                    "start": 685,
                    "end": 706,
                    "matchedPaperCorpusId": "259858880"
                },
                {
                    "start": 706,
                    "end": 724,
                    "matchedPaperCorpusId": "234357574"
                },
                {
                    "start": 1744,
                    "end": 1759,
                    "matchedPaperCorpusId": "12942757"
                },
                {
                    "start": 1791,
                    "end": 1811,
                    "matchedPaperCorpusId": "235358129"
                },
                {
                    "start": 1883,
                    "end": 1910,
                    "matchedPaperCorpusId": "52051958"
                },
                {
                    "start": 1922,
                    "end": 1940,
                    "matchedPaperCorpusId": "221819581"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.057586669921875
        },
        {
            "corpus_id": "222310410",
            "title": "Multilingual Argument Mining: Datasets and Analysis",
            "text": "translation step increases prediction run-time, and on our data also tends to perform worse.\n\nWe examine two text sources for performance evaluation on non-English texts. The first one is a \"pseudo test\" set -an automatic translation of an English evaluation set for a task. While such texts can be easily generated, it is not clear how well they represent \"real\" texts, authored by humans. Hence, we also examine human-authored texts, in several non-English languages, collected via crowdsourcing specifically for this work. Both datasets are released as part of this work. 1 When translating the evaluation set, either automatically or by a human translator, one would like to assume that the initial label of the English text is maintained after translation. While this is often the case, we show that this assumption becomes more dubious as the argument mining task becomes more complex and subjective, as well as when the original labels are not clearly agreed upon.\n\nIn summary, the main contributions of this paper are: (1) a comparative analysis of the translatetrain approach on three central argument mining tasks using different subsets of languages, showing that training on more data helps, but that, in some cases, training on related languages is sufficient; (2) multilingual benchmark datasets for the three tasks;\n\n(3) an analysis of the three tasks, showing how well labels are preserved across translation, and the impact that has on the success of the translate-train approach.",
            "score": 0.4554498803136629,
            "section_title": "Introduction",
            "char_start_offset": 2085,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30224609375
        },
        {
            "corpus_id": "269033069",
            "title": "Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian Language?",
            "text": "Several benchmarks have been developed to assess the performance of LLMs.One of the most significant benchmarks is MMLU (Hendrycks et al., 2021), which evaluates language models for answering multiple-choice questions in 57 different tasks, but only in English.M3Exam (Zhang Table 2: Comparison of various features of Persian and English benchmarks.As descriptive answers and trapped questions were not defined for reading comprehension benchmarks, we marked those fields with a hyphen (-).Desc.Ans.: Descriptive Answer, Diff.Lev.: Difficulty Level, Eng: English, Chi: Chinese, orig.: original non-translated question, tran.: translated question, MCQA: Multiple-Choice Question Answering, FB: Fill in the Blank, RC: Reading Comprehension, SA: Sentiment Analysis, TE: Textual Entailment, QP: Question Paraphrasing, MT: Machine Translation., edu.stages: educational stages et al., 2023) introduces a multilingual, multimodal, and multilevel benchmark for evaluating LLMs including more than 12K multiple-choice questions from 9 languages (excluding Persian) at three educational stages.AGIEval (Zhong et al., 2023) is another benchmark that assesses the performance of LLMs on human-centric standardized exams in English and Chinese languages to measure their ability in human-level tasks.\n\nThere have been a few benchmarks built to assess language models on the Persian language, including ParSQUAD (Abadani et al., 2021), PersianQA (Ayoubi, 2021), ParsiNLU (Khashabi et al., 2021), and PQuAD (Darvishi et al., 2023), in which some of their features are compared in Table 2. ParSQUAD, PersianQA, and PQuAD present extractive datasets where models are asked to extract answers from given paragraphs.While this task can gauge models' reading comprehension skills, it may not effectively evaluate general capabilities and inherent knowledge of models.ParsiNLU evaluates language models based on 14500 questions from six language understanding tasks, including multiple-choice QA (MCQA), sentiment analysis, and more (Table 2).",
            "score": 0.45529785091523967,
            "section_title": "Evaluation of LLMs",
            "char_start_offset": 2696,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 73,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 349
                },
                {
                    "start": 349,
                    "end": 490
                },
                {
                    "start": 490,
                    "end": 495
                },
                {
                    "start": 495,
                    "end": 526
                },
                {
                    "start": 526,
                    "end": 844
                },
                {
                    "start": 844,
                    "end": 1084
                },
                {
                    "start": 1084,
                    "end": 1287
                },
                {
                    "start": 1289,
                    "end": 1697
                },
                {
                    "start": 1697,
                    "end": 1847
                },
                {
                    "start": 1847,
                    "end": 2022
                }
            ],
            "ref_mentions": [
                {
                    "start": 1398,
                    "end": 1420,
                    "matchedPaperCorpusId": "235308420"
                },
                {
                    "start": 1492,
                    "end": 1515,
                    "matchedPaperCorpusId": "246823191"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03656005859375
        },
        {
            "corpus_id": "247625205",
            "title": "What Language Model to Train if You Have One Million GPU Hours?",
            "text": "No 43.46 Yes 42.24  (2021). First row is the reported XGLM results, and the second is our reproduction of their results to validate our multilingual evaluation setup. Last two rows show that our multilingual model matches the XGLM results. \n\nEnglish-only evaluation. We first evaluate our multilingual model on the same set of English benchmarks we have used previously, in Table 6. \n\nMultilinguality significantly lowers accuracy on the English benchmark, which is in line with the results from Lin et al. ( 2021). \n\nMultilingual evaluation. Zero-shot multilingual evaluation is more challenging to setup because it requires writing new prompts for each new language. Therefore, instead of manually writing prompts for each language, we follow the strategy proposed by Lin et al. ( 2021), using English prompts for non-English examples-this can be viewed as cross-lingual zero-shot generalization. They validated this strategy by demonstrating its ability to achieve zero-shot performance on par with (and sometimes even better than) humanwritten language-specific prompts. This strategy also demonstrates cross-lingual abilities. \n\nWe evaluate on XNLI (Conneau et al., 2018), a multilingual NLI dataset that covers 8 of the languages we use for training. Our evaluation is different from the zero-shot evaluation of the XTREME benchmark (Hu et al., 2020). XTREME first finetunes the model on the English training data of each downstream task, then evaluates it on the non-English dataset, attempting cross-lingual generalization. Our evaluation avoids any finetuning, and instead relies entirely on zero-shot generalization. Results. Language-specific scaling laws. To explore how scale influences multilinguality, we train a wider range of models (i.e. 0.3-6B parameters) on a larger corpus of more than 300B tokens of text drawn from a variety of languages (Lauren\u00e7on et al., 2022).",
            "score": 0.45528181999374684,
            "section_title": "Embedding Norm Average EAI Results",
            "char_start_offset": 15670,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 27
                },
                {
                    "start": 28,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 515
                },
                {
                    "start": 518,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1131
                },
                {
                    "start": 1134,
                    "end": 1256
                },
                {
                    "start": 1257,
                    "end": 1357
                },
                {
                    "start": 1358,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1626
                },
                {
                    "start": 1627,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1886
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11181640625
        },
        {
            "corpus_id": "269626686",
            "title": "Understanding the Capabilities and Limitations of Large Language Models for Cultural Commonsense",
            "text": "We also see that Falcon and LLAMA lack the instruction-following capability in Farsi and Swahili.This decrease in performance in languages other than English suggests that commonsense knowledge is \"lost\" (becomes inaccessible) when queried in these other languages, thus indirectly diminishing the value that LLMs can have for speakers of these languages.Besides, asking questions in the language spoken in a given country does not necessarily help as we expect; instead, in most cases, there is a significant benefit from asking questions about the cultural commonsense of a country in English.\n\nTo delve deeper into the effect of language on the performance achieved by LLMs on this task, Table 3 shows the performance obtained with one model (GPT-3.5-turbo)when prompted with all five languages.For all countries, we see a clear benefit obtained by interaction in English.Also, on average, interactions in Hindi lead to the worst performance, followed by Swahili as the second worst.Our results suggest that even the cultural relevance of the task does not fully mitigate performance disparities, where LLMs persist in exhibiting lower performance in non-English languages as they do on culture-agnostic tasks (Lai et al., 2023).The results of the multilingual-optimized models also agree with this finding as shown in the Appendix.",
            "score": 0.45526234387630865,
            "section_title": "Cultural Commonsense Question Answering",
            "char_start_offset": 15921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 355
                },
                {
                    "start": 355,
                    "end": 595
                },
                {
                    "start": 597,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 798
                },
                {
                    "start": 798,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 986
                },
                {
                    "start": 986,
                    "end": 1232
                },
                {
                    "start": 1232,
                    "end": 1335
                }
            ],
            "ref_mentions": [
                {
                    "start": 1213,
                    "end": 1231,
                    "matchedPaperCorpusId": "258079179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06561279296875
        },
        {
            "corpus_id": "270562911",
            "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?",
            "text": "Despite Section 4.4 suggesting that it is more critical to have the domain and the knowledge of the native language in instructions, it is an unrealistic setting since it employs native data. This is difficult to obtain especially for under-served languages, so it is hard to avoid machine-translated data. We, therefore, investigate techniques that can apply better regularization during instruction tuning to reduce the negative impact of the translated data. This also represents an effort to pursue a more generalizable finding. \n\nA lower learning rate Our first inspiration is drawn from Chirkova and Nikoulina (2024), whose experiments showed that English instruction-tuned models display remarkably different levels of crosslingual transfer when only changing the learning rate-a smaller one leads to better instruction following in zero-shot languages. This means that it is possible to teach a base model a desired instructionresponse style without even touching on the content or language. In this case, the undesirable properties in translated data could be mitigated. Following this, we run another set of experiments with the learning rate reduced from 10 \u22124 to 10 \u22126 . \n\nMultilingualism Another exploration is multilingual instruction tuning, which could prevent a model from overfitting to a single language. In addition to Spanish, Russian, and Chinese which we evaluate, we also add another five languages-Arabic (ar), German (de), Finnish (fi), Irish (ga), and Hindi (hi)-into the multilingual pot. For the native multilingual data, we simply down-sample all languages in the Aya dataset to a size of 241 (the size of the German split in Aya, which is the smallest among the eight languages), leading to a total size of 1928. For the translated data in each language, we randomly select 241 instances from English and translate them (different data instances for different languages). This simulates a multilingual instruction set derived from translating English resources. \n\nSetup For each of our previous data-model combinations, we now have two variants. Due to the space constraint, we only display results from larger models in the main text for the follow- ing benchmarks: TyDi QA, CMMLU, XQuAD, MSGM, MT-MMLU, and open-ended question answering.",
            "score": 0.45520623937124904,
            "section_title": "Can we bridge the gap?",
            "char_start_offset": 21923,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 532
                },
                {
                    "start": 535,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1079
                },
                {
                    "start": 1080,
                    "end": 1182
                },
                {
                    "start": 1185,
                    "end": 1323
                },
                {
                    "start": 1324,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 1992
                },
                {
                    "start": 1995,
                    "end": 2076
                },
                {
                    "start": 2077,
                    "end": 2270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0885009765625
        },
        {
            "corpus_id": "270619619",
            "title": "Learn and Unlearn in Multilingual LLMs",
            "text": "While large language models (LLMs) demonstrate promising success in various domains, from natural language understanding to creative content generation, their broad applications raise safety concerns for their ability to generate misleading, offensive, or otherwise harmful content (Shen et al., 2024a;Qi et al., 2023;Huang et al., 2023b), impacting millions worldwide, spanning all languages and cultural contexts. \n\nDespite extensive research and development dedicated to improving the safety of LLMs (Zhang et al., 2023;Ge et al., 2023), the majority of these efforts have been centered on English tasks (Eldan and Russinovich, 2023;Wang et al., 2023). These English-centric approaches often overlook the complexities and challenges presented by the multilingual settings (Wu et al., 2023;Wang et al., 2024). Consequently, LLMs are less reliable and more susceptible to producing harmful content in non-English environments (Shen et al., 2024a), highlighting a significant gap in the current safety frameworks. \n\nOne of the main reasons that LLMs produce problematic content is their training on contaminated datasets. Harmful contents often slip through during training (Golchin and Surdeanu, 2024;Sainz et al., 2023), especially in non-English texts, where filtering mechanisms frequently fail. This oversight leads to the widespread dissemination of misinformation, harm, and bias, which in turn undermines the reliability of LLMs. \n\nIn this paper, we simulate a practical scenario where harmful contents from various language sources exist in pretraining data. We investigate how these harmful contents spread across different languages within multilingual LLMs and how prompts in various languages can trigger the generation. With the multilingual dimension complicating the issue, we evaluate the effectiveness of unlearning across languages. \n\nOur findings are threefold: \n\n\u2022 Fake information from all language sources propagates within multilingual LLMs. \n\n\u2022 Standard unlearning methods are largely insufficient and can lead to deceptive conclusions when the harmful data is non-English. \n\n\u2022 Only grounding harmful data in both English and the original language will effectively eliminate fake responses.",
            "score": 0.4551461720103974,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 415
                },
                {
                    "start": 418,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 811
                },
                {
                    "start": 812,
                    "end": 1013
                },
                {
                    "start": 1016,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1299
                },
                {
                    "start": 1300,
                    "end": 1437
                },
                {
                    "start": 1440,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1851
                },
                {
                    "start": 1854,
                    "end": 1881
                },
                {
                    "start": 1884,
                    "end": 1965
                },
                {
                    "start": 1968,
                    "end": 2098
                },
                {
                    "start": 2101,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07720947265625
        },
        {
            "corpus_id": "54882786",
            "title": "Assessing the English Translation of BFSU\u2019s New Motto",
            "text": "The discrepancy between (abstract) linguistic structures and concrete real-world objectives explains one of the most frequent criticisms leveled at functionalist approaches: lack of (reliable) empirical data to prove that what the theorist-evaluator thinks is adequate solutions are really adequate, or inadequate solutions really inadequate. As H\u00f6nig mused: \"The problem with any TQA [\u2026] is that it tacitly implies an assessment of a supposed reader's reaction. However, the empirical basis of the reader's putative reaction is often unclear\" (1998, p. 29). Despite recent efforts to generate empirical evidence by means of interviews and questionnaires in one recent study and through think-aloud protocols in another (Nord, 2012, p. 33), the reliability and sufficiency of such methodologies are still open to question. \n\nMore important to the task at hand, in terms of translation into English, the ultimate validation in the form of expected effect-actualized effect comparison looks increasingly elusive, because any effort in gauging receptors' response is infinitely complicated by the fact of English as a lingua franca and the subsequent question of how to assess and reflect the audience heterogeneity of an English translation in evaluative methodology. In other words, now that English is the global language in almost all spheres of human engagement commerce, politics, education, etc, the potential readership of an English translation is no longer restricted to native speakers. If we use the \"three circles model\" originally discussed by Kachru (1986), an English translation, like any other English texts, is to be read by the Inner circle (communities for which English is first or the dominant language of social interactions), Outer circle (former colonies where English has been institutionalized for intra-communal use, like HK, Singapore...) and Expanding circle of English speakers (non post-colonial communities who nonetheless uses English as a foreign language for communication with people who do not speak their language). How to scientifically represent this diverse and elusive group in empirical data is a question that functionalist-oriented researchers have not begun to address. \n\nIn contrast, \"functional equivalence,\" like other types of equivalence proposed by equivalence theoreticians, is clearly linguistically defined, entailing close analysis and comparison of ST and TT linguistic elements.",
            "score": 0.4549848286019832,
            "section_title": "A Theoretical Prequel: TQA Literature",
            "char_start_offset": 8031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 2052
                },
                {
                    "start": 2053,
                    "end": 2214
                },
                {
                    "start": 2217,
                    "end": 2435
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01554107666015625
        },
        {
            "corpus_id": "268512999",
            "title": "Lost in Translation? Translation Errors and Challenges for Fair Assessment of Text-to-Image Models on Multilingual Concepts",
            "text": "Trivially, human annotators for every language would remove false-negative mistranslations from future benchmarks, but there's a trade-off between easy scalability and certainty of correctness.\n\nOur work incorporates human efforts of both native and proficient but non-native language speakers to propose and resolve translation error candidates caused by the machine translation pipeline in the original CoCo-CroLa benchmark.This could potentially bring human biases into the nuance of factors such as words' choices, introducing less culturally neural expressions as a result.\n\nThe assumption of translatability that underlies CCCL in general is a challenge.As a practical usebased test of functional fairness, using heuristics and only common everyday objects that can be reasonably assumed universal is acceptable, but more linguistic and even philosophical work is needed to really motivate fairness across languages and cultures when underlying assumptions differ.",
            "score": 0.45491957750185513,
            "section_title": "Limitations",
            "char_start_offset": 13651,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 195,
                    "end": 426
                },
                {
                    "start": 426,
                    "end": 578
                },
                {
                    "start": 580,
                    "end": 660
                },
                {
                    "start": 660,
                    "end": 970
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.203125
        },
        {
            "corpus_id": "276903136",
            "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Enhancement Protocol",
            "text": "Our examination of the current AI4SE benchmarking environment leads us to a series of standardized insights about the domain, which we outline below. \n\n1) Limited Programming Language Variety: There is a strong tendency for new benchmarks to follow the norms of the field which revolve around well-known languages like Python, Java, or C++; With this, more niche languages such as Haskell, Visual Basic, and Julia. Although projects like the MultiPL-E [6] series of automatically translated benchmarks help reduce this disparity, a lack of benchmarks focused on native lowresource languages remains. 2) Limited Natural Language Variety: Similar to programming languages, most benchmarks focus mainly on English, with Chinese being the next most common. Although resources like MicrosoftDocs [173] include languages like Norwegian, Danish, and Latvian, evaluation datasets still lack sufficient natural language diversity, restricting the insights into how these models perform in different languages. 3) Limited Task Diversity: Over the years, tasks like Text2Code have been subjected to numerous evaluation benchmarks, whereas tasks like domain-specific code generation and memorization have been largely overlooked; This, highlights an imbalance in benchmark availability. 4) Real-World Applicability Issues: Benchmarks traditionally centered on isolated tasks such as function-level code generation, which limited the practicality of their results in real-world scenarios. Recently, this has shifted towards more comprehensive software engineering and project-level tasks to enhance the real-world relevance of benchmark results. With tools such as BenchScout, users can select more relevant benchmarks and gain deeper insights into model performance for their specific needs. Furthermore, our results show the significant impact of integrating the BENCHFRAME framework and highlight the value of peer-reviewed, validated benchmarks. With many state-of-the-art models tested on similar benchmarks, both industry and academia should adapt their evaluation methods to ensure robust results. The refinement of benchmarks across various AI4SE tasks will be critical for guiding future research and ensuring that these models can perform effectively. Although this study highlights the issue of data leakage in current benchmarks, it remains true that HumanEvalNext would also be affected by this issue.",
            "score": 0.454886203948274,
            "section_title": "A. Implications",
            "char_start_offset": 44014,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 152,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 1000
                },
                {
                    "start": 1001,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1779
                },
                {
                    "start": 1780,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2248
                },
                {
                    "start": 2249,
                    "end": 2401
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06488037109375
        },
        {
            "corpus_id": "273508009",
            "title": "JMMMU: A Japanese Massive Multi-discipline Multimodal Understanding Benchmark for Culture-aware Evaluation",
            "text": "In recent years, large language models (LLMs) have revolutionized the field of language processing (Chen et al., 2023a;vic, 2023;Touvron et al., 2023;Wei et al., 2023). Building on the success of LLMs, large multimodal models (LMMs) have demonstrated remarkable performance across tasks Figure 1: Overview of the JMMMU dataset. JMMMU includes 720 culture-agnostic (translation-based) questions and 600 culture-specific (newly created) questions, totaling 1,320 questions, thus expanding the existing cultureaware Japanese benchmark (Inoue et al., 2024b) by over 10 times. JMMMU serves as a diagnostic tool for assessing both Japanese cultural understanding and culture-agnostic language understanding capability. serves as a diagnostic tool for model developers, providing valuable feedback for future improvements. \n\nEvaluating 16 open-source LMMs and three advanced proprietary LMMs on JMMMU, our key findings are summarized as follows: \n\n\u2022 Overall performance is up to 58.6%, leaving great room for improvement in the utility of the Japanese context. \n\n\u2022 The CA subset reveals that most models perform worse when asked in Japanese than in English (up to 8.6%), even when the question asks exactly the same content. This appleto-apple comparison clearly indicates that the utility in non-English languages is falling behind in current LMMs. \n\n\u2022 The CS subset reveals that models trained on Japanese datasets perform the best among open-source models, suggesting that such finetuning effectively contributes to incorporating Japanese cultural knowledge into the models. \n\n\u2022 Combining both subsets, we reveal a significant discrepancy among the state-of-the-art proprietary models. While they perform similarly on English benchmarks and even on culture-agnostic questions in Japanese, their performances are significantly different on CS subset. This finding is particularly alarming, as it indicates that evaluation exclusively on a translation-based benchmark could risk overestimation of an LMM's multilingual capabil-ity without truly understanding the context of the individual cultures. \n\nOur findings indicate that English-centered performance evaluation may lead to biased development, neglecting non-English languages.",
            "score": 0.4547138568280156,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 712
                },
                {
                    "start": 713,
                    "end": 815
                },
                {
                    "start": 818,
                    "end": 938
                },
                {
                    "start": 941,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1342
                },
                {
                    "start": 1345,
                    "end": 1570
                },
                {
                    "start": 1573,
                    "end": 1681
                },
                {
                    "start": 1682,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 2092
                },
                {
                    "start": 2095,
                    "end": 2227
                }
            ],
            "ref_mentions": [
                {
                    "start": 532,
                    "end": 552,
                    "matchedPaperCorpusId": "269043145"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.172119140625
        },
        {
            "corpus_id": "268041734",
            "title": "Exploring Multilingual Concepts of Human Values in Large Language Models: Is Value Alignment Consistent, Transferable and Controllable across Languages?",
            "text": "We translated the aforementioned human value datasets from English into 15 non-English languages using Google Translate. These languages belong to various language families, including Indo-European (Catalan, French, Indonesian, Portuguese, Spanish), Niger-Congo (Chichewa, Swahili), Dravidian (Tamil, Telugu), Uralic (Finnish, Hungarian), Sino-Tibetan (Chinese), Japonic (Japanese), Koreanic (Korean) and Austro-Asiatic (Vietnamese). The impact of translation quality on our results is discussed in Appendix C. \n\nOur experiments involved three multilingual LLM families, including the LLaMA2-chat series (7B, 13B, 70B) (Touvron et al., 2023), Qwenchat series (1B8, 7B, 14B) (Bai et al., 2023) and BLOOMZ series (560M, 1B7, 7B1) (Scao et al., 2022). Appendix D provides detailed language distributions of their pre-training data. Notably, not all selected languages are included in the pretraining data of these model families. Specifically, both LLaMA2 and BLOOMZ cover 12 of these languages, though their selections do not fully overlap. In contrast, Qwen's technical report only mentions the inclusion of English and Chinese. For the multilingual concept recognition task, we consider all   16 languages, regardless of the model series, while other tasks explore only the languages covered in the pre-training data. \n\n4.2 Q1: Do LLMs Encode Concepts Representing Human Values in Multiple Languages? \n\nFigure 1 illustrates the multilingual concept recognition accuracy of the three LLM families, averaged across all value concepts. We first observe that all three models achieve notable accuracy across all represented languages and even the smallest models surpass \u03c4 = 65% accuracy in them. It's important to note that the accuracy of 65% is a conservative statistic and represents a lower bound, derived from the smallest model (BLOOMZ-560M) on the poorest-performing language (ny, accounting for only 0.00007% in pre-training data). However, results from larger models are significantly higher.",
            "score": 0.45463246346000985,
            "section_title": "Examined Languages and LLMs",
            "char_start_offset": 14145,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 433
                },
                {
                    "start": 434,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1127
                },
                {
                    "start": 1128,
                    "end": 1317
                },
                {
                    "start": 1320,
                    "end": 1400
                },
                {
                    "start": 1403,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 1998
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07135009765625
        },
        {
            "corpus_id": "276408022",
            "title": "M-ABSA: A Multilingual Dataset for Aspect-Based Sentiment Analysis",
            "text": "Main Results. This indicates that introducing complex categories7 as sentiment elements presents a greater challenge for ABSA, as also shown by Zhang et al. (2021b). \n\n2) On average, the performance of English exceeds that of the other languages. German (de) and Dutch (nl) achieve the highest scores of all languages other than English, with averages of 33.99% and 31.85% on the TASD task respectively. It is worth noting that German, Dutch, and English all belong to the West Germanic group of languages, which share structural and vocabulary similarities. In contrast, Swahili (sw), a Bantu language with relatively limited linguistic resources and distinct grammatical structures compared to more widely studied lan-guages, emerges as the most challenging language, with an average performance of 15.96%. This suggests the model has lower transferability when handling languages with fewer resources, particularly those from non-Indo-European language families. \n\nImpact of Source Language. We conduct additional experiments using five typologically diverse non-English source languages (zh, ko, ar, ru, fr), each with a different script, to examine their impact on TASD performance on the Restaurant dataset. In some cases, selecting a non-English source improves performance compared to English. As shown in Figure 3(a), cross-lingual transfer benefits when the target language is semantically close to the source (e.g., Chinese-to-Japanese outperforms English-to-Japanese), likely due to biases in crosslingual models, which are predominantly trained on English. Using non-English sources helps mitigate this bias. However, Arabic presents a challenge, possibly due to its significant linguistic and typological differences from the other languages. \n\nWe use the t-SNE algorithm to visualize aspect term representations in 2-dimensional Euclidean space for six languages from the Restaurant dataset in Figure 3(b), revealing two clusters: (1) Chinese, Japanese, and Korean and (2) French, English, and German. These clusters align well with typological similarities in their respective language families. This visualization also aligns with the empirical results from 3(a) that transferring from linguistically similar languages can enhance performance.",
            "score": 0.45453555697103365,
            "section_title": "Cross-Lingual Transfer Results",
            "char_start_offset": 19834,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 808
                },
                {
                    "start": 809,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1621
                },
                {
                    "start": 1622,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2016
                },
                {
                    "start": 2017,
                    "end": 2111
                },
                {
                    "start": 2112,
                    "end": 2260
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 164,
                    "matchedPaperCorpusId": "236460053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07196044921875
        },
        {
            "corpus_id": "267413041",
            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
            "text": "This work studies the problem of translation irregularities in evaluation sets of multilingual benchmarks like XNLI that are created by translating English into multiple target languages. We find that the translation sets of low-resource languages like Urdu, Swahili exhibit most inconsistencies while translations of high-resource languages like French, German are more immune to this problem. We suggest an effective way to check the quality of human translations by comparing performance with machine translations, and show how the cross-lingual transfer estimates can significantly vary with improved translations. \n\nFor tasks that have output labels directly corresponding to the input text (e.g., sequence labeling tasks like POS-tagging, question answering, etc.), it would be trickier to use our technique since translations could change the word order and subsequently affect the output labels as well. \n\nWe highlight the problem of the cross-lingual transfer gap for low-resource languages being mischaracterized due to poor performance on these languages stemming from poor-quality translations and not necessarily because the model has difficulty with the given target languages. We do not offer a solution to deal with translation errors. Rather, we ask for additional checks when collecting translations for low-resource languages. \n\nWe identify that the existing translation datasets for low-resource languages in XNLI have inconsistencies. While we did not create manuallycorrected versions of these translation sets, we will be releasing the machine-translated text from English to these target languages upon publication.",
            "score": 0.45439051333492736,
            "section_title": "Conclusions",
            "char_start_offset": 13897,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5107421875
        },
        {
            "corpus_id": "273502644",
            "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
            "text": "Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.",
            "score": 0.45423673154283395,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.244873046875
        },
        {
            "corpus_id": "259858965",
            "title": "FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models",
            "text": "We have introduced FORK, a dataset to measure culinary cultural bias in commonsense models. Confirming our hypothesis, we find that models default to US cultural contexts in underspecified questions, and perform markedly better on implicit and explicit questions about US culture than non-US. A likely source of bias is the English, USproduced texts which models are pretrained on. We believe the results support our hypothesis that English Language Models LMS trained on texts (many of which are produced for a US audience) would reflect US (or broadly Western) cultural assumptions. We hypothesize that the Underspecified setting had the lowest \"accuracy\" for non-US countries because the experimental design forced the model to choose between US and non-US interpretations of the same question. For Implicit and Explicit settings, we speculate that the non-US accuracy is generally higher for Explicit than Implicit because the former made it easier for models to determine the cultural setting. \n\nPotential mitigation techniques to eliminate such biases may involve better curation of training data, training separate models for different cultural contexts, training models to better recognize cultural cues or ask for clarification in ambiguous settings, among many other possibilities. We believe this is an open research question, and we hope this paper will inspire future research to address it. \n\nThe topic of cultural bias is vast, and we choose a narrow scope to avoid biting off more than we can chew. Future work will explore strategies for cultural awareness of commonsense models, analysis of cultural assumptions in non-English models, and analysis of other aspects of culture beyond the culinary.",
            "score": 0.4541784888275685,
            "section_title": "Conclusion",
            "char_start_offset": 13481,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 998
                },
                {
                    "start": 1001,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1404
                },
                {
                    "start": 1407,
                    "end": 1514
                },
                {
                    "start": 1515,
                    "end": 1714
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.115966796875
        },
        {
            "corpus_id": "270521667",
            "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
            "text": "The findings highlight the importance of carefully evaluating performance across individual languages and considering regional and linguistic factors in LLM development.\n\nMEGA (Ahuja et al., 2023) is the first comprehensive benchmarking of generative LLMs on multilingual NLP tasks, covering 16 datasets across 70 typologically diverse languages.The authors compare the performance of generative LLMs like GPT-3.5, GPT-4, and BLOOMZ to state-of-the-art non-autoregressive models.They find a significant performance gap between English and non-English languages, especially low-resource languages with non-Latin scripts, for which fine-tuned models often outperform LLMs.The authors found that while GPT-4 narrows this gap it does not close it.The authors also explore different prompting strategies, finding that translating test data to English before inputting it to the model often improves performance for low-resource languages.Factors like tokenizer quality and amount of pre-training data partially explain performance trends across languages.The paper presents a framework for evaluating generative multi-lingual LLMs and provides directions for future research.\n\nNicholas and Bhatia (2023) talk about LLMs being increasingly used for content analysis tasks, but perform far better in English than in the world's other 7,000+ languages.Efforts to address this imbalance through multilingual LLMs, which are trained on text from multiple languages, raise their own concerns.Multilingual LLMs still rely disproportionately on English data, fail to capture the nuances of local contexts, and exhibit inconsistent performance across languages.Despite their limitations, the authors find that companies are already deploying multilingual LLMs for content moderation and other applications.To mitigate potential harms, companies should be transparent about their use of LLMs and provide adequate channels for human review.Researchers and funders should support language-specific NLP communities to improve LLM performance in non-English languages.Governments should exercise caution in using LLMs for high-stakes decision-making and convene stakeholders to establish norms and guardrails around their development and deployment.",
            "score": 0.4540373290301589,
            "section_title": "Multi-lingual performance evaluation",
            "char_start_offset": 42963,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 171,
                    "end": 346
                },
                {
                    "start": 346,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 670
                },
                {
                    "start": 670,
                    "end": 743
                },
                {
                    "start": 743,
                    "end": 933
                },
                {
                    "start": 933,
                    "end": 1050
                },
                {
                    "start": 1050,
                    "end": 1170
                },
                {
                    "start": 1172,
                    "end": 1344
                },
                {
                    "start": 1344,
                    "end": 1481
                },
                {
                    "start": 1481,
                    "end": 1647
                },
                {
                    "start": 1647,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1924
                },
                {
                    "start": 1924,
                    "end": 2049
                },
                {
                    "start": 2049,
                    "end": 2230
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "257663467"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10247802734375
        },
        {
            "corpus_id": "53247198",
            "title": "Findings of the 2018 Conference on Machine Translation (WMT18)",
            "text": "Before making any statements about \"machine translation outperforming humans\" or \"machinehuman parity in translation\", it is important to consider the following points:\n\n\u2022 The alternate HIT structure applied in this 291 version of DA has not been tested thoroughly enough to be certain of high reliability. For example, as described in Section 3.3.2, forcing all translations of a given source segment to be assessed by the same human judge within the same HIT could cause individual DA ratings to become highly relative as opposed to the aim of DA ratings to be as close as possible to absolute judgments of translation quality. Furthermore, an additional bias that could cause problems for this HIT structure is one associated with a past evaluation method, relative ranking. When evaluating competing translations of the same source that are situated in close proximity within a HIT, annotators may be primed by high (or low) quality outputs resulting in overly severe (or lenient) judgments for subsequent translations of the same source segment (Bojar et al., 2011).\n\n\u2022 While standard monolingual DA employs annotators only required to be speakers of a single language, source-based DA requires fluency in two languages and it is not known the degree to which varying levels of native language fluency in at least one language may negatively impact the reliability of DA rankings in the case of bilingual annotators.\n\n\u2022 It is likely that the quality of reference translations can vary and this could potentially impact the reliability of human performance estimates in source-based DA. Although reference-based DA assumes high quality reference translations, in the unfortunate case of problematic references, the overall rankings are unlikely to suffer to any large degree in terms of the reliability of system rankings, since all competing systems are likely to suffer equally from any lack of quality in reference translations.\n\nHowever, in the adapted source-based version of DA, the effect of low quality reference translations is quite different. Firstly, since assessment involves comparison of MT outputs with the source, genuine participating systems will not suffer from the fact reference translations are low quality, since references are not involved in their evaluation. On the other hand, human performance estimates certainly will, as a drop in reference qual-ity is indeed highly likely to negatively impact the placement of human performance estimates in system rankings. The reliability of comparisons with human performance with source-based DA is therefore highly dependent on",
            "score": 0.45349314920528294,
            "section_title": "Considerations as to Human Parity",
            "char_start_offset": 51408,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 1050,
                    "end": 1070,
                    "matchedPaperCorpusId": "6395516"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053985595703125
        },
        {
            "paperId": "0cdd8569a99263ba638a6f1d42fbfaa7eebd59b4",
            "corpusId": 279075008,
            "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain",
            "venue": "",
            "year": 2025,
            "referenceCount": 27,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.00250, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2364747084",
                    "name": "Mohammad Javad Ranjbar Kalahroodi"
                },
                {
                    "authorId": "2364745981",
                    "name": "Amirhossein Sheikholselami"
                },
                {
                    "authorId": "2364748645",
                    "name": "Sepehr Karimi"
                },
                {
                    "authorId": "2364746914",
                    "name": "Sepideh Ranjbar Kalahroodi"
                },
                {
                    "authorId": "3054779",
                    "name": "Heshaam Faili"
                },
                {
                    "authorId": "2887988",
                    "name": "A. Shakery"
                }
            ],
            "abstract": "Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale, expert-validated dataset of multiple-choice Persian medical questions, designed to evaluate LLMs across both Persian and English. We benchmark over 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 35.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, Persian responses are sometimes more accurate due to cultural and clinical contextual cues. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating multilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be accessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA",
            "corpus_id": "279075008",
            "text": "Large Language Models (LLMs) have achieved remarkable performance on a wide range of NLP benchmarks, often surpassing human-level accuracy. However, their reliability in high-stakes domains such as medicine, particularly in low-resource languages, remains underexplored. In this work, we introduce PersianMedQA, a large-scale, expert-validated dataset of multiple-choice Persian medical questions, designed to evaluate LLMs across both Persian and English. We benchmark over 40 state-of-the-art models, including general-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and chain-of-thought (CoT) settings. Our results show that closed-source general models (e.g., GPT-4.1) consistently outperform all other categories, achieving 83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models such as Dorna underperform significantly (e.g., 35.9% in Persian), often struggling with both instruction-following and domain reasoning. We also analyze the impact of translation, showing that while English performance is generally higher, Persian responses are sometimes more accurate due to cultural and clinical contextual cues. Finally, we demonstrate that model size alone is insufficient for robust performance without strong domain or language adaptation. PersianMedQA provides a foundation for evaluating multilingual and culturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be accessed at: https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.1424560546875
        },
        {
            "paperId": "f8d9e15c2daca9767e981812fe6d12fbe4d414fc",
            "corpusId": 276987062,
            "title": "Scientific Evidence for Clinical Text Summarization Using Large Language Models: Scoping Review",
            "venue": "Journal of Medical Internet Research",
            "year": 2024,
            "referenceCount": 62,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC12123242, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2349926856",
                    "name": "Lydie Bednarczyk"
                },
                {
                    "authorId": "1754345513",
                    "name": "Daniel Reichenpfader"
                },
                {
                    "authorId": "1413826879",
                    "name": "C. Gaudet-Blavignac"
                },
                {
                    "authorId": "2349928994",
                    "name": "Amon Kenna Ette"
                },
                {
                    "authorId": "2120713582",
                    "name": "Jamil Zaghir"
                },
                {
                    "authorId": "2316827958",
                    "name": "Yuanyuan Zheng"
                },
                {
                    "authorId": "2316700729",
                    "name": "Adel Bensahla"
                },
                {
                    "authorId": "1753725063",
                    "name": "Mina Bjelogrlic"
                },
                {
                    "authorId": "2246383579",
                    "name": "Christian Lovis"
                }
            ],
            "abstract": "Background Information overload in electronic health records requires effective solutions to alleviate clinicians\u2019 administrative tasks. Automatically summarizing clinical text has gained significant attention with the rise of large language models. While individual studies show optimism, a structured overview of the research landscape is lacking. Objective This study aims to present the current state of the art on clinical text summarization using large language models, evaluate the level of evidence in existing research and assess the applicability of performance findings in clinical settings. Methods This scoping review complied with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. Literature published between January 1, 2019, and June 18, 2024, was identified from 5 databases: PubMed, Embase, Web of Science, IEEE Xplore, and ACM Digital Library. Studies were excluded if they did not describe transformer-based models, did not focus on clinical text summarization, did not engage with free-text data, were not original research, were nonretrievable, were not peer-reviewed, or were not in English, French, Spanish, or German. Data related to study context and characteristics, scope of research, and evaluation methodologies were systematically collected and analyzed by 3 authors independently. Results A total of 30 original studies were included in the analysis. All used observational retrospective designs, mainly using real patient data (n=28, 93%). The research landscape demonstrated a narrow research focus, often centered on summarizing radiology reports (n=17, 57%), primarily involving data from the intensive care unit (n=15, 50%) of US-based institutions (n=19, 73%), in English (n=26, 87%). This focus aligned with the frequent reliance on the open-source Medical Information Mart for Intensive Care dataset (n=15, 50%). Summarization methodologies predominantly involved abstractive approaches (n=17, 57%) on single-document inputs (n=4, 13%) with unstructured data (n=13, 43%), yet reporting on methodological details remained inconsistent across studies. Model selection involved both open-source models (n=26, 87%) and proprietary models (n=7, 23%). Evaluation frameworks were highly heterogeneous. All studies conducted internal validation, but external validation (n=2, 7%), failure analysis (n=6, 20%), and patient safety risks analysis (n=1, 3%) were infrequent, and none reported bias assessment. Most studies used both automated metrics and human evaluation (n=16, 53%), while 10 (33%) used only automated metrics, and 4 (13%) only human evaluation. Conclusions Key barriers hinder the translation of current research into trustworthy, clinically valid applications. Current research remains exploratory and limited in scope, with many applications yet to be explored. Performance assessments often lack reliability, and clinical impact evaluations are insufficient raising concerns about model utility, safety, fairness, and data privacy. Advancing the field requires more robust evaluation frameworks, a broader research scope, and a stronger focus on real-world applicability.",
            "corpus_id": "276987062",
            "text": "Background Information overload in electronic health records requires effective solutions to alleviate clinicians\u2019 administrative tasks. Automatically summarizing clinical text has gained significant attention with the rise of large language models. While individual studies show optimism, a structured overview of the research landscape is lacking. Objective This study aims to present the current state of the art on clinical text summarization using large language models, evaluate the level of evidence in existing research and assess the applicability of performance findings in clinical settings. Methods This scoping review complied with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. Literature published between January 1, 2019, and June 18, 2024, was identified from 5 databases: PubMed, Embase, Web of Science, IEEE Xplore, and ACM Digital Library. Studies were excluded if they did not describe transformer-based models, did not focus on clinical text summarization, did not engage with free-text data, were not original research, were nonretrievable, were not peer-reviewed, or were not in English, French, Spanish, or German. Data related to study context and characteristics, scope of research, and evaluation methodologies were systematically collected and analyzed by 3 authors independently. Results A total of 30 original studies were included in the analysis. All used observational retrospective designs, mainly using real patient data (n=28, 93%). The research landscape demonstrated a narrow research focus, often centered on summarizing radiology reports (n=17, 57%), primarily involving data from the intensive care unit (n=15, 50%) of US-based institutions (n=19, 73%), in English (n=26, 87%). This focus aligned with the frequent reliance on the open-source Medical Information Mart for Intensive Care dataset (n=15, 50%). Summarization methodologies predominantly involved abstractive approaches (n=17, 57%) on single-document inputs (n=4, 13%) with unstructured data (n=13, 43%), yet reporting on methodological details remained inconsistent across studies. Model selection involved both open-source models (n=26, 87%) and proprietary models (n=7, 23%). Evaluation frameworks were highly heterogeneous. All studies conducted internal validation, but external validation (n=2, 7%), failure analysis (n=6, 20%), and patient safety risks analysis (n=1, 3%) were infrequent, and none reported bias assessment. Most studies used both automated metrics and human evaluation (n=16, 53%), while 10 (33%) used only automated metrics, and 4 (13%) only human evaluation. Conclusions Key barriers hinder the translation of current research into trustworthy, clinically valid applications. Current research remains exploratory and limited in scope, with many applications yet to be explored. Performance assessments often lack reliability, and clinical impact evaluations are insufficient raising concerns about model utility, safety, fairness, and data privacy. Advancing the field requires more robust evaluation frameworks, a broader research scope, and a stronger focus on real-world applicability.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.028656005859375
        },
        {
            "paperId": "f4c1e6998e0bafbb441c6a3ca971669b2b7b3bd6",
            "corpusId": 62971899,
            "title": "Teaching by Principles: An Interactive Approach to Language Pedagogy",
            "venue": "",
            "year": 1995,
            "referenceCount": 21,
            "citationCount": 7231,
            "influentialCitationCount": 827,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2307/415773?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2307/415773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325711303",
                    "name": "Douglas Brown"
                }
            ],
            "abstract": "PART I. FOUNDATIONS FOR CLASSROOM PRACTICE Chapter 1 Getting Started A Classroom Observation Beneath the Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 2 A \"Methodical\" History of Language Teaching Approach, Method, and Technique Changing Winds and Shifting Sands The Grammar Translation Method Gouin and the Series Method The Direct Method The Audiolingual Method Cognitive Code Learning \"Designer\" Methods of the Spirited 1970s Community Language Learning Suggestopedia The Silent Way Total Physical Response The Natural Approach Functional Syllabuses Topics for Discussion, Action, and Research For Your Further Reading Chapter 3 The Post-Method Era: Toward Informed Approaches The Dysfunction of the Theory-Practice Dichotomy An Enlightened, Eclectic Approach Communicative Language Teaching Task-Based Language Teaching Learner-Centered Instruction Cooperative Learning Interactive Learning Whole Language Education Content-Based Instruction Other Candidates for CLT Approaches Topics for Discussion, Action, and Research For Your Further Reading Chapter 4 Teaching by Principles Cognitive Principles Automaticity Meaningful Learning The Anticipation of Reward Intrinsic Motivation Strategic Investment Autonomy Socioaffective Principles Language Ego Willingness to Communicate The Language-Culture Connection Linguistic Principles The Native Language Effect Interlanguage Communicative Competence Topics for Discussion, Action, and Research For Your Further Reading Chapter 5 Intrinsic Motivation in the Classroom Defining Motivation A Behavioral Definition Cognitive Definitions Intrinsic and Extrinsic Motivation Intrinsic Motivation in Education Intrinsic Motivation in the Second Language Classroom Topics for Discussion, Action, and Research For Your Further Reading PART II. CONTEXTS OF LEARNING AND TEACHING Chapter 6 Teaching Across Age Levels Teaching Children Intellectual Development Attention Span Sensory Input Affective Factors Authentic, Meaningful Language Teaching Adults Teaching Teens Topics for Discussion, Action, and Research For Your Further Reading Chapter 7 Teaching Across Proficiency Levels Defining Proficiency Levels Teaching Beginning Levels Teaching Intermediate Levels Teaching Advanced Levels Topics for Discussion, Action, and Research For Your Further Reading Chapter 8 Sociocultural, Political, and Institutional Contexts Cultural Contexts Second and Foreign Language Contexts English as an International Language Non-native English-speaking Teachers Language Policy Issues Institutional Contexts Elementary and Secondary Schools Post-Secondary and Adult Education Topics for Discussion, Action, and Research For Your Further Reading PART III. DESIGNING AND IMPLEMENTING CLASSROOM LESSONS Chapter 9 Curriculum Design An Overview of the Curriculum Design Process Situation Analysis Needs Analysis Problematizing Specifying Goals Conceptualizing a Course Syllabus Selecting Textbooks, Materials, and Resources Assessment Program Evaluation Topics for Discussion, Action, and Research For Your Further Reading Chapter 10 Lesson Planning Format of a Lesson Plan Guidelines for Lesson Planning Sample Lesson Plan Topics for Discussion, Action, and Research For Your Further Reading Chapter 11 Techniques and Materials Techniques Redefined Categorizing Techniques: A Bit of History A Taxonomy of Techniques Textbooks Other Texts and Visual Aids Topics for Discussion, Action, and Research For Your Further Reading Chapter 12 Technology in the Classroom Non-Computer-Based Technology Computer Assisted Language Learning (CALL) CALL, CMC, TMLL, or What? Principles and Benefits of CALL Uses of CALL in the Language Classroom Topics for Discussion, Action, and Research Some Useful Web Resources For Your Further Reading Chapter 13 Initiating Interaction in the Classroom What is Interaction? Interactive Principles Roles of the Interactive Teacher Foreign Language Interaction Analysis Questioning Strategies for Interactive Learning Topics for Discussion, Action, and Research For Your Further Reading Chapter 14 Sustaining Interaction through Group Work Advantages of Group Work Excuses for Avoiding Group Work Implementing Group Work in Your Classroom Classroom Language Selecting Appropriate Group Techniques Planning Group Work Monitoring the Task Debriefing Topics for Discussion, Action, and Research For Your Further Reading Chapter 15 Classroom Management The Physical Environment of the Classroom Your Voice and Body Language Unplanned Teaching: Midstream Lesson Changes Teaching Under Adverse Circumstances Teachers' Roles and Styles Creating a Positive Classroom Climate Topics for Discussion, Action, and Research For Your Further Reading Chapter 16 Strategies-Based Instruction Strategic Investment Good Language Learners Styles of Successful Language Learning Awareness and Action Developing Student Self-awareness of Styles How to Teach Strategies in the Classroom \"Packaged\" Models of SBI Topics for Discussion, Action, and Research For Your Further Reading PART IV. TEACHING LANGUAGE SKILLS Chapter 17 Integrating the Four Skills Why Integrated Skills? Models of Skills Integration Content-Based Instruction Task-Based Language Teaching Theme-Based Instruction Experiential Learning The Episode Hypothesis An Integrated Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 18 Teaching Listening Listening Comprehension in Pedagogical Research An Interactive Model of Listening Comprehension Types of Spoken Language What Makes Listening Difficult? Microskills and Macroskills of Listening Types of Classroom Listening Performance Principles for Teaching Listening Skills Listening Techniques from Beginning to Advanced Assessing Listening in the Classroom On Understanding the terms Assessment and Test Assessing Types of Listening and Micro- and Macroskills Topics for Discussion, Action, and Research For Your Further Reading Chapter 19 Teaching Speaking Oral Communication Skills in Pedagogical Research Types of Spoken Language What Makes Speaking Difficult? Micro- and Macroskills of Oral Communication Types of Classroom Speaking Performance Principles for Teaching Speaking Skills Teaching Conversation Teaching Pronunciation Focus on Form and Error Treatment Assessing Speaking in the Classroom Item Types and Tasks for Assessing Speaking Topics for Discussion, Action, and Research For Your Further Reading Chapter 20 Teaching Reading Research on Reading a Second Language Genres of Written Language Characteristics of Written Language Micro- and Macroskills for Reading Comprehension Strategies for Reading Comprehension Types of Classroom Reading Performance Principles for Teaching Reading Skills Two Reading Lessons Assessing Reading Topics for Discussion, Action, and Research For Your Further Reading Chapter 21 Teaching Writing Research on Second Language Writing Types of Written Language Characteristics of Written Language: A Writer's View Micro- and Macroskills for Writing Types of Classroom Writing Performance Principles for Teaching Writing Skills Assessing Writing Evaluation Checklists Writing Assessment Tasks Topics for Discussion, Action, and Research For Your Further Reading Chapter 22 Form-Focused Instruction The Place of Grammar To Teach or Not to Teach Grammar Issues About How to Teach Grammar Grammar Techniques Grammar Sequencing in Textbooks and Curricula Current Issues in Vocabulary Teaching Principles for Teaching Vocabulary Topics for Discussion, Action, and Research For Your Further Reading PART V. ASSESSING LANGUAGE SKILLS Chapter 23 Language Assessment: Principles and Issues Testing, Assessing, and Teaching Principles of Language Assessment Practicality Reliability Validity Content Validity Face Validity Construct Validity Authenticity Washback Kinds of Tests Historical Developments in Language Assessment Current Issues in Language Assessment Topics for Discussion, Action, and Research For Your Further Reading Chapter 24 Classroom-Based Assessment Norm-Referenced and Criterion-Referenced Tests Some Practical Steps to Test Construction Turning Existing Tests into more Effective Procedures Alternatives in Assessment Scrutinizing the Alternatives Topics for Discussion, Action, and Research For Your Further Reading PART VI. LIFELONG LEARNING Chapter 25 Teacher Development Peak Performers The \"Good\" Language Teacher Classroom Observation Classroom-Based \"Action\" Research Teacher Collaboration: Learning from each other Journals, Teaching Portfolios, Reading, and Writing Topics for Discussion, Action, and Research For Your Further Reading Chapter 26 Teachers for Social Responsibility Critical Pedagogy Hot Topics in the Language Classroom Moral Dilemmas and Moral Imperatives Agents for Change Topics for Discussion, Action, and Research For Your Further Reading Bibliography Name Index Subject Index",
            "corpus_id": "62971899",
            "text": "PART I. FOUNDATIONS FOR CLASSROOM PRACTICE Chapter 1 Getting Started A Classroom Observation Beneath the Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 2 A \"Methodical\" History of Language Teaching Approach, Method, and Technique Changing Winds and Shifting Sands The Grammar Translation Method Gouin and the Series Method The Direct Method The Audiolingual Method Cognitive Code Learning \"Designer\" Methods of the Spirited 1970s Community Language Learning Suggestopedia The Silent Way Total Physical Response The Natural Approach Functional Syllabuses Topics for Discussion, Action, and Research For Your Further Reading Chapter 3 The Post-Method Era: Toward Informed Approaches The Dysfunction of the Theory-Practice Dichotomy An Enlightened, Eclectic Approach Communicative Language Teaching Task-Based Language Teaching Learner-Centered Instruction Cooperative Learning Interactive Learning Whole Language Education Content-Based Instruction Other Candidates for CLT Approaches Topics for Discussion, Action, and Research For Your Further Reading Chapter 4 Teaching by Principles Cognitive Principles Automaticity Meaningful Learning The Anticipation of Reward Intrinsic Motivation Strategic Investment Autonomy Socioaffective Principles Language Ego Willingness to Communicate The Language-Culture Connection Linguistic Principles The Native Language Effect Interlanguage Communicative Competence Topics for Discussion, Action, and Research For Your Further Reading Chapter 5 Intrinsic Motivation in the Classroom Defining Motivation A Behavioral Definition Cognitive Definitions Intrinsic and Extrinsic Motivation Intrinsic Motivation in Education Intrinsic Motivation in the Second Language Classroom Topics for Discussion, Action, and Research For Your Further Reading PART II. CONTEXTS OF LEARNING AND TEACHING Chapter 6 Teaching Across Age Levels Teaching Children Intellectual Development Attention Span Sensory Input Affective Factors Authentic, Meaningful Language Teaching Adults Teaching Teens Topics for Discussion, Action, and Research For Your Further Reading Chapter 7 Teaching Across Proficiency Levels Defining Proficiency Levels Teaching Beginning Levels Teaching Intermediate Levels Teaching Advanced Levels Topics for Discussion, Action, and Research For Your Further Reading Chapter 8 Sociocultural, Political, and Institutional Contexts Cultural Contexts Second and Foreign Language Contexts English as an International Language Non-native English-speaking Teachers Language Policy Issues Institutional Contexts Elementary and Secondary Schools Post-Secondary and Adult Education Topics for Discussion, Action, and Research For Your Further Reading PART III. DESIGNING AND IMPLEMENTING CLASSROOM LESSONS Chapter 9 Curriculum Design An Overview of the Curriculum Design Process Situation Analysis Needs Analysis Problematizing Specifying Goals Conceptualizing a Course Syllabus Selecting Textbooks, Materials, and Resources Assessment Program Evaluation Topics for Discussion, Action, and Research For Your Further Reading Chapter 10 Lesson Planning Format of a Lesson Plan Guidelines for Lesson Planning Sample Lesson Plan Topics for Discussion, Action, and Research For Your Further Reading Chapter 11 Techniques and Materials Techniques Redefined Categorizing Techniques: A Bit of History A Taxonomy of Techniques Textbooks Other Texts and Visual Aids Topics for Discussion, Action, and Research For Your Further Reading Chapter 12 Technology in the Classroom Non-Computer-Based Technology Computer Assisted Language Learning (CALL) CALL, CMC, TMLL, or What? Principles and Benefits of CALL Uses of CALL in the Language Classroom Topics for Discussion, Action, and Research Some Useful Web Resources For Your Further Reading Chapter 13 Initiating Interaction in the Classroom What is Interaction? Interactive Principles Roles of the Interactive Teacher Foreign Language Interaction Analysis Questioning Strategies for Interactive Learning Topics for Discussion, Action, and Research For Your Further Reading Chapter 14 Sustaining Interaction through Group Work Advantages of Group Work Excuses for Avoiding Group Work Implementing Group Work in Your Classroom Classroom Language Selecting Appropriate Group Techniques Planning Group Work Monitoring the Task Debriefing Topics for Discussion, Action, and Research For Your Further Reading Chapter 15 Classroom Management The Physical Environment of the Classroom Your Voice and Body Language Unplanned Teaching: Midstream Lesson Changes Teaching Under Adverse Circumstances Teachers' Roles and Styles Creating a Positive Classroom Climate Topics for Discussion, Action, and Research For Your Further Reading Chapter 16 Strategies-Based Instruction Strategic Investment Good Language Learners Styles of Successful Language Learning Awareness and Action Developing Student Self-awareness of Styles How to Teach Strategies in the Classroom \"Packaged\" Models of SBI Topics for Discussion, Action, and Research For Your Further Reading PART IV. TEACHING LANGUAGE SKILLS Chapter 17 Integrating the Four Skills Why Integrated Skills? Models of Skills Integration Content-Based Instruction Task-Based Language Teaching Theme-Based Instruction Experiential Learning The Episode Hypothesis An Integrated Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 18 Teaching Listening Listening Comprehension in Pedagogical Research An Interactive Model of Listening Comprehension Types of Spoken Language What Makes Listening Difficult? Microskills and Macroskills of Listening Types of Classroom Listening Performance Principles for Teaching Listening Skills Listening Techniques from Beginning to Advanced Assessing Listening in the Classroom On Understanding the terms Assessment and Test Assessing Types of Listening and Micro- and Macroskills Topics for Discussion, Action, and Research For Your Further Reading Chapter 19 Teaching Speaking Oral Communication Skills in Pedagogical Research Types of Spoken Language What Makes Speaking Difficult? Micro- and Macroskills of Oral Communication Types of Classroom Speaking Performance Principles for Teaching Speaking Skills Teaching Conversation Teaching Pronunciation Focus on Form and Error Treatment Assessing Speaking in the Classroom Item Types and Tasks for Assessing Speaking Topics for Discussion, Action, and Research For Your Further Reading Chapter 20 Teaching Reading Research on Reading a Second Language Genres of Written Language Characteristics of Written Language Micro- and Macroskills for Reading Comprehension Strategies for Reading Comprehension Types of Classroom Reading Performance Principles for Teaching Reading Skills Two Reading Lessons Assessing Reading Topics for Discussion, Action, and Research For Your Further Reading Chapter 21 Teaching Writing Research on Second Language Writing Types of Written Language Characteristics of Written Language: A Writer's View Micro- and Macroskills for Writing Types of Classroom Writing Performance Principles for Teaching Writing Skills Assessing Writing Evaluation Checklists Writing Assessment Tasks Topics for Discussion, Action, and Research For Your Further Reading Chapter 22 Form-Focused Instruction The Place of Grammar To Teach or Not to Teach Grammar Issues About How to Teach Grammar Grammar Techniques Grammar Sequencing in Textbooks and Curricula Current Issues in Vocabulary Teaching Principles for Teaching Vocabulary Topics for Discussion, Action, and Research For Your Further Reading PART V. ASSESSING LANGUAGE SKILLS Chapter 23 Language Assessment: Principles and Issues Testing, Assessing, and Teaching Principles of Language Assessment Practicality Reliability Validity Content Validity Face Validity Construct Validity Authenticity Washback Kinds of Tests Historical Developments in Language Assessment Current Issues in Language Assessment Topics for Discussion, Action, and Research For Your Further Reading Chapter 24 Classroom-Based Assessment Norm-Referenced and Criterion-Referenced Tests Some Practical Steps to Test Construction Turning Existing Tests into more Effective Procedures Alternatives in Assessment Scrutinizing the Alternatives Topics for Discussion, Action, and Research For Your Further Reading PART VI. LIFELONG LEARNING Chapter 25 Teacher Development Peak Performers The \"Good\" Language Teacher Classroom Observation Classroom-Based \"Action\" Research Teacher Collaboration: Learning from each other Journals, Teaching Portfolios, Reading, and Writing Topics for Discussion, Action, and Research For Your Further Reading Chapter 26 Teachers for Social Responsibility Critical Pedagogy Hot Topics in the Language Classroom Moral Dilemmas and Moral Imperatives Agents for Change Topics for Discussion, Action, and Research For Your Further Reading Bibliography Name Index Subject Index",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0274810791015625
        },
        {
            "paperId": "13e0556d066ec6456f3b5cddc47d724f78ac6c98",
            "corpusId": 237374960,
            "title": "Bilingual Aphasia Test in Urdu and its Clinical Implications",
            "venue": "Pakistan Journal of Medical Sciences",
            "year": 2021,
            "referenceCount": 4,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://pjms.org.pk/index.php/pjms/article/download/4572/1001",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://pmc.ncbi.nlm.nih.gov/articles/PMC8377929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2112424816",
                    "name": "Nadir Ali"
                },
                {
                    "authorId": "40438671",
                    "name": "M. Rafi"
                }
            ],
            "abstract": "Dear Editor, A language is core to express identity, thoughts, and feelings. Aphasia alters language performance on all levels of linguistic skills.1 It is essential to mention the significance of language assessment of persons who have lost their language skills due to aphasia in Pakistan, where a majority of the population is bilingual. Bilingual aphasia is a frequent phenomenon in clinical settings. People practice different languages in different communicative contexts. The language used at work may be different from that at home or in their social sphere. In a bilingual context, all languages collectively contribute to everyday life. To evaluate the real impact of aphasia in routine life, assessing the linguistic deficit in each language is necessary. The effectiveness of treatment intervention depends upon the administration of a valid and reliable test.2 In bilingual aphasics, symptoms can be varied and could encompass all the languages that he/ she speaks. It is necessary to assess the patient\u2019s linguistic ability with each of the languages that was spoken in his/her premorbid circumstances with the help of a cross-linguistically equivalent test. Therefore, its assessment and treatment may become more challenging as compared to monolingual aphasics. Thus, in a progressively bilingual or multilingual Pakistan, the impact of stroke on such individuals\u2019 communication abilities is tremendous.3 Most developed and validated aphasia batteries are in the English language (e.g., Boston Diagnostic Aphasia Examination and Western Aphasia Battery). In Pakistan, the options left for clinicians for assessment of language in bilingual aphasics is to either use institutionallyorganized, non-standardized translations of batteries originally constructed for native English speakers, considering their specific socio-cultural and linguistic context, or to use the Urdu version of the Bilingual Aphasia Test (BAT-Urdu), which is a structural and cultural adaptation to the idiosyncrasies of the Urdu language. However, BAT-Urdu has not been standardized and normed on patients with aphasia. Hence, the test has not been clinically validated in the bilingual population by obtaining psychometric data (either not obtained or published in English). Consequently, it prevents easy access for the larger audience of linguistic aphasiology researchers from evaluating the reliability and validity of the test. BAT, which is considered an equivalent test to assess the differential linguistic impairment in bilinguals, is adapted culturally and linguistically to many languages. It is the most-used clinical tool and research instrument for assessing residual linguistic performance in aphasics, comprised of three parts. Part-A of the test evaluates the bilingual background of the patients and their families. Part-B measures their performance in different modalities of language through various comprehension and expression tasks. Part-C examines the patient\u2019s ability to translate the content of one of his/her languages to another. Despite its general flexibility, its invalidity in the Pakistani context hinders the reliability of the diagnosis.4 Considering the lack of clinical data and the unavailability of a valid assessment test for Urdu/ 1. Dr. Nadir Ali, MD. 2. Dr. Muhammad Shaban Rafi, PhD. 1-2: Department of Linguistics and Communications, Institute of Liberal Arts University of Management and Technology, Lahore, Pakistan.",
            "corpus_id": "237374960",
            "text": "Dear Editor, A language is core to express identity, thoughts, and feelings. Aphasia alters language performance on all levels of linguistic skills.1 It is essential to mention the significance of language assessment of persons who have lost their language skills due to aphasia in Pakistan, where a majority of the population is bilingual. Bilingual aphasia is a frequent phenomenon in clinical settings. People practice different languages in different communicative contexts. The language used at work may be different from that at home or in their social sphere. In a bilingual context, all languages collectively contribute to everyday life. To evaluate the real impact of aphasia in routine life, assessing the linguistic deficit in each language is necessary. The effectiveness of treatment intervention depends upon the administration of a valid and reliable test.2 In bilingual aphasics, symptoms can be varied and could encompass all the languages that he/ she speaks. It is necessary to assess the patient\u2019s linguistic ability with each of the languages that was spoken in his/her premorbid circumstances with the help of a cross-linguistically equivalent test. Therefore, its assessment and treatment may become more challenging as compared to monolingual aphasics. Thus, in a progressively bilingual or multilingual Pakistan, the impact of stroke on such individuals\u2019 communication abilities is tremendous.3 Most developed and validated aphasia batteries are in the English language (e.g., Boston Diagnostic Aphasia Examination and Western Aphasia Battery). In Pakistan, the options left for clinicians for assessment of language in bilingual aphasics is to either use institutionallyorganized, non-standardized translations of batteries originally constructed for native English speakers, considering their specific socio-cultural and linguistic context, or to use the Urdu version of the Bilingual Aphasia Test (BAT-Urdu), which is a structural and cultural adaptation to the idiosyncrasies of the Urdu language. However, BAT-Urdu has not been standardized and normed on patients with aphasia. Hence, the test has not been clinically validated in the bilingual population by obtaining psychometric data (either not obtained or published in English). Consequently, it prevents easy access for the larger audience of linguistic aphasiology researchers from evaluating the reliability and validity of the test. BAT, which is considered an equivalent test to assess the differential linguistic impairment in bilinguals, is adapted culturally and linguistically to many languages. It is the most-used clinical tool and research instrument for assessing residual linguistic performance in aphasics, comprised of three parts. Part-A of the test evaluates the bilingual background of the patients and their families. Part-B measures their performance in different modalities of language through various comprehension and expression tasks. Part-C examines the patient\u2019s ability to translate the content of one of his/her languages to another. Despite its general flexibility, its invalidity in the Pakistani context hinders the reliability of the diagnosis.4 Considering the lack of clinical data and the unavailability of a valid assessment test for Urdu/ 1. Dr. Nadir Ali, MD. 2. Dr. Muhammad Shaban Rafi, PhD. 1-2: Department of Linguistics and Communications, Institute of Liberal Arts University of Management and Technology, Lahore, Pakistan.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.039703369140625
        },
        {
            "paperId": "844778f17f7c85b80114b5cc62216171a1b6a45b",
            "corpusId": 145507809,
            "title": "Teaching by Principles: An Interactive Approach to Language Pedagogy",
            "venue": "",
            "year": 1995,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2307/415773?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2307/415773, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325625638",
                    "name": "By H. Douglas"
                },
                {
                    "authorId": "2325596655",
                    "name": "Brown"
                },
                {
                    "authorId": "2325542300",
                    "name": "Heekyeong Lee"
                }
            ],
            "abstract": "PART I. FOUNDATIONS FOR CLASSROOM PRACTICE Chapter 1 Getting Started A Classroom Observation Beneath the Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 2 A \"Methodical\" History of Language Teaching Approach, Method, and Technique Changing Winds and Shifting Sands The Grammar Translation Method Gouin and the Series Method The Direct Method The Audiolingual Method Cognitive Code Learning \"Designer\" Methods of the Spirited 1970s Community Language Learning Suggestopedia The Silent Way Total Physical Response The Natural Approach Functional Syllabuses Topics for Discussion, Action, and Research For Your Further Reading Chapter 3 The Post-Method Era: Toward Informed Approaches The Dysfunction of the Theory-Practice Dichotomy An Enlightened, Eclectic Approach Communicative Language Teaching Task-Based Language Teaching Learner-Centered Instruction Cooperative Learning Interactive Learning Whole Language Education Content-Based Instruction Other Candidates for CLT Approaches Topics for Discussion, Action, and Research For Your Further Reading Chapter 4 Teaching by Principles Cognitive Principles Automaticity Meaningful Learning The Anticipation of Reward Intrinsic Motivation Strategic Investment Autonomy Socioaffective Principles Language Ego Willingness to Communicate The Language-Culture Connection Linguistic Principles The Native Language Effect Interlanguage Communicative Competence Topics for Discussion, Action, and Research For Your Further Reading Chapter 5 Intrinsic Motivation in the Classroom Defining Motivation A Behavioral Definition Cognitive Definitions Intrinsic and Extrinsic Motivation Intrinsic Motivation in Education Intrinsic Motivation in the Second Language Classroom Topics for Discussion, Action, and Research For Your Further Reading PART II. CONTEXTS OF LEARNING AND TEACHING Chapter 6 Teaching Across Age Levels Teaching Children Intellectual Development Attention Span Sensory Input Affective Factors Authentic, Meaningful Language Teaching Adults Teaching Teens Topics for Discussion, Action, and Research For Your Further Reading Chapter 7 Teaching Across Proficiency Levels Defining Proficiency Levels Teaching Beginning Levels Teaching Intermediate Levels Teaching Advanced Levels Topics for Discussion, Action, and Research For Your Further Reading Chapter 8 Sociocultural, Political, and Institutional Contexts Cultural Contexts Second and Foreign Language Contexts English as an International Language Non-native English-speaking Teachers Language Policy Issues Institutional Contexts Elementary and Secondary Schools Post-Secondary and Adult Education Topics for Discussion, Action, and Research For Your Further Reading PART III. DESIGNING AND IMPLEMENTING CLASSROOM LESSONS Chapter 9 Curriculum Design An Overview of the Curriculum Design Process Situation Analysis Needs Analysis Problematizing Specifying Goals Conceptualizing a Course Syllabus Selecting Textbooks, Materials, and Resources Assessment Program Evaluation Topics for Discussion, Action, and Research For Your Further Reading Chapter 10 Lesson Planning Format of a Lesson Plan Guidelines for Lesson Planning Sample Lesson Plan Topics for Discussion, Action, and Research For Your Further Reading Chapter 11 Techniques and Materials Techniques Redefined Categorizing Techniques: A Bit of History A Taxonomy of Techniques Textbooks Other Texts and Visual Aids Topics for Discussion, Action, and Research For Your Further Reading Chapter 12 Technology in the Classroom Non-Computer-Based Technology Computer Assisted Language Learning (CALL) CALL, CMC, TMLL, or What? Principles and Benefits of CALL Uses of CALL in the Language Classroom Topics for Discussion, Action, and Research Some Useful Web Resources For Your Further Reading Chapter 13 Initiating Interaction in the Classroom What is Interaction? Interactive Principles Roles of the Interactive Teacher Foreign Language Interaction Analysis Questioning Strategies for Interactive Learning Topics for Discussion, Action, and Research For Your Further Reading Chapter 14 Sustaining Interaction through Group Work Advantages of Group Work Excuses for Avoiding Group Work Implementing Group Work in Your Classroom Classroom Language Selecting Appropriate Group Techniques Planning Group Work Monitoring the Task Debriefing Topics for Discussion, Action, and Research For Your Further Reading Chapter 15 Classroom Management The Physical Environment of the Classroom Your Voice and Body Language Unplanned Teaching: Midstream Lesson Changes Teaching Under Adverse Circumstances Teachers' Roles and Styles Creating a Positive Classroom Climate Topics for Discussion, Action, and Research For Your Further Reading Chapter 16 Strategies-Based Instruction Strategic Investment Good Language Learners Styles of Successful Language Learning Awareness and Action Developing Student Self-awareness of Styles How to Teach Strategies in the Classroom \"Packaged\" Models of SBI Topics for Discussion, Action, and Research For Your Further Reading PART IV. TEACHING LANGUAGE SKILLS Chapter 17 Integrating the Four Skills Why Integrated Skills? Models of Skills Integration Content-Based Instruction Task-Based Language Teaching Theme-Based Instruction Experiential Learning The Episode Hypothesis An Integrated Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 18 Teaching Listening Listening Comprehension in Pedagogical Research An Interactive Model of Listening Comprehension Types of Spoken Language What Makes Listening Difficult? Microskills and Macroskills of Listening Types of Classroom Listening Performance Principles for Teaching Listening Skills Listening Techniques from Beginning to Advanced Assessing Listening in the Classroom On Understanding the terms Assessment and Test Assessing Types of Listening and Micro- and Macroskills Topics for Discussion, Action, and Research For Your Further Reading Chapter 19 Teaching Speaking Oral Communication Skills in Pedagogical Research Types of Spoken Language What Makes Speaking Difficult? Micro- and Macroskills of Oral Communication Types of Classroom Speaking Performance Principles for Teaching Speaking Skills Teaching Conversation Teaching Pronunciation Focus on Form and Error Treatment Assessing Speaking in the Classroom Item Types and Tasks for Assessing Speaking Topics for Discussion, Action, and Research For Your Further Reading Chapter 20 Teaching Reading Research on Reading a Second Language Genres of Written Language Characteristics of Written Language Micro- and Macroskills for Reading Comprehension Strategies for Reading Comprehension Types of Classroom Reading Performance Principles for Teaching Reading Skills Two Reading Lessons Assessing Reading Topics for Discussion, Action, and Research For Your Further Reading Chapter 21 Teaching Writing Research on Second Language Writing Types of Written Language Characteristics of Written Language: A Writer's View Micro- and Macroskills for Writing Types of Classroom Writing Performance Principles for Teaching Writing Skills Assessing Writing Evaluation Checklists Writing Assessment Tasks Topics for Discussion, Action, and Research For Your Further Reading Chapter 22 Form-Focused Instruction The Place of Grammar To Teach or Not to Teach Grammar Issues About How to Teach Grammar Grammar Techniques Grammar Sequencing in Textbooks and Curricula Current Issues in Vocabulary Teaching Principles for Teaching Vocabulary Topics for Discussion, Action, and Research For Your Further Reading PART V. ASSESSING LANGUAGE SKILLS Chapter 23 Language Assessment: Principles and Issues Testing, Assessing, and Teaching Principles of Language Assessment Practicality Reliability Validity Content Validity Face Validity Construct Validity Authenticity Washback Kinds of Tests Historical Developments in Language Assessment Current Issues in Language Assessment Topics for Discussion, Action, and Research For Your Further Reading Chapter 24 Classroom-Based Assessment Norm-Referenced and Criterion-Referenced Tests Some Practical Steps to Test Construction Turning Existing Tests into more Effective Procedures Alternatives in Assessment Scrutinizing the Alternatives Topics for Discussion, Action, and Research For Your Further Reading PART VI. LIFELONG LEARNING Chapter 25 Teacher Development Peak Performers The \"Good\" Language Teacher Classroom Observation Classroom-Based \"Action\" Research Teacher Collaboration: Learning from each other Journals, Teaching Portfolios, Reading, and Writing Topics for Discussion, Action, and Research For Your Further Reading Chapter 26 Teachers for Social Responsibility Critical Pedagogy Hot Topics in the Language Classroom Moral Dilemmas and Moral Imperatives Agents for Change Topics for Discussion, Action, and Research For Your Further Reading Bibliography Name Index Subject Index",
            "corpus_id": "145507809",
            "text": "PART I. FOUNDATIONS FOR CLASSROOM PRACTICE Chapter 1 Getting Started A Classroom Observation Beneath the Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 2 A \"Methodical\" History of Language Teaching Approach, Method, and Technique Changing Winds and Shifting Sands The Grammar Translation Method Gouin and the Series Method The Direct Method The Audiolingual Method Cognitive Code Learning \"Designer\" Methods of the Spirited 1970s Community Language Learning Suggestopedia The Silent Way Total Physical Response The Natural Approach Functional Syllabuses Topics for Discussion, Action, and Research For Your Further Reading Chapter 3 The Post-Method Era: Toward Informed Approaches The Dysfunction of the Theory-Practice Dichotomy An Enlightened, Eclectic Approach Communicative Language Teaching Task-Based Language Teaching Learner-Centered Instruction Cooperative Learning Interactive Learning Whole Language Education Content-Based Instruction Other Candidates for CLT Approaches Topics for Discussion, Action, and Research For Your Further Reading Chapter 4 Teaching by Principles Cognitive Principles Automaticity Meaningful Learning The Anticipation of Reward Intrinsic Motivation Strategic Investment Autonomy Socioaffective Principles Language Ego Willingness to Communicate The Language-Culture Connection Linguistic Principles The Native Language Effect Interlanguage Communicative Competence Topics for Discussion, Action, and Research For Your Further Reading Chapter 5 Intrinsic Motivation in the Classroom Defining Motivation A Behavioral Definition Cognitive Definitions Intrinsic and Extrinsic Motivation Intrinsic Motivation in Education Intrinsic Motivation in the Second Language Classroom Topics for Discussion, Action, and Research For Your Further Reading PART II. CONTEXTS OF LEARNING AND TEACHING Chapter 6 Teaching Across Age Levels Teaching Children Intellectual Development Attention Span Sensory Input Affective Factors Authentic, Meaningful Language Teaching Adults Teaching Teens Topics for Discussion, Action, and Research For Your Further Reading Chapter 7 Teaching Across Proficiency Levels Defining Proficiency Levels Teaching Beginning Levels Teaching Intermediate Levels Teaching Advanced Levels Topics for Discussion, Action, and Research For Your Further Reading Chapter 8 Sociocultural, Political, and Institutional Contexts Cultural Contexts Second and Foreign Language Contexts English as an International Language Non-native English-speaking Teachers Language Policy Issues Institutional Contexts Elementary and Secondary Schools Post-Secondary and Adult Education Topics for Discussion, Action, and Research For Your Further Reading PART III. DESIGNING AND IMPLEMENTING CLASSROOM LESSONS Chapter 9 Curriculum Design An Overview of the Curriculum Design Process Situation Analysis Needs Analysis Problematizing Specifying Goals Conceptualizing a Course Syllabus Selecting Textbooks, Materials, and Resources Assessment Program Evaluation Topics for Discussion, Action, and Research For Your Further Reading Chapter 10 Lesson Planning Format of a Lesson Plan Guidelines for Lesson Planning Sample Lesson Plan Topics for Discussion, Action, and Research For Your Further Reading Chapter 11 Techniques and Materials Techniques Redefined Categorizing Techniques: A Bit of History A Taxonomy of Techniques Textbooks Other Texts and Visual Aids Topics for Discussion, Action, and Research For Your Further Reading Chapter 12 Technology in the Classroom Non-Computer-Based Technology Computer Assisted Language Learning (CALL) CALL, CMC, TMLL, or What? Principles and Benefits of CALL Uses of CALL in the Language Classroom Topics for Discussion, Action, and Research Some Useful Web Resources For Your Further Reading Chapter 13 Initiating Interaction in the Classroom What is Interaction? Interactive Principles Roles of the Interactive Teacher Foreign Language Interaction Analysis Questioning Strategies for Interactive Learning Topics for Discussion, Action, and Research For Your Further Reading Chapter 14 Sustaining Interaction through Group Work Advantages of Group Work Excuses for Avoiding Group Work Implementing Group Work in Your Classroom Classroom Language Selecting Appropriate Group Techniques Planning Group Work Monitoring the Task Debriefing Topics for Discussion, Action, and Research For Your Further Reading Chapter 15 Classroom Management The Physical Environment of the Classroom Your Voice and Body Language Unplanned Teaching: Midstream Lesson Changes Teaching Under Adverse Circumstances Teachers' Roles and Styles Creating a Positive Classroom Climate Topics for Discussion, Action, and Research For Your Further Reading Chapter 16 Strategies-Based Instruction Strategic Investment Good Language Learners Styles of Successful Language Learning Awareness and Action Developing Student Self-awareness of Styles How to Teach Strategies in the Classroom \"Packaged\" Models of SBI Topics for Discussion, Action, and Research For Your Further Reading PART IV. TEACHING LANGUAGE SKILLS Chapter 17 Integrating the Four Skills Why Integrated Skills? Models of Skills Integration Content-Based Instruction Task-Based Language Teaching Theme-Based Instruction Experiential Learning The Episode Hypothesis An Integrated Lesson Topics for Discussion, Action, and Research For Your Further Reading Chapter 18 Teaching Listening Listening Comprehension in Pedagogical Research An Interactive Model of Listening Comprehension Types of Spoken Language What Makes Listening Difficult? Microskills and Macroskills of Listening Types of Classroom Listening Performance Principles for Teaching Listening Skills Listening Techniques from Beginning to Advanced Assessing Listening in the Classroom On Understanding the terms Assessment and Test Assessing Types of Listening and Micro- and Macroskills Topics for Discussion, Action, and Research For Your Further Reading Chapter 19 Teaching Speaking Oral Communication Skills in Pedagogical Research Types of Spoken Language What Makes Speaking Difficult? Micro- and Macroskills of Oral Communication Types of Classroom Speaking Performance Principles for Teaching Speaking Skills Teaching Conversation Teaching Pronunciation Focus on Form and Error Treatment Assessing Speaking in the Classroom Item Types and Tasks for Assessing Speaking Topics for Discussion, Action, and Research For Your Further Reading Chapter 20 Teaching Reading Research on Reading a Second Language Genres of Written Language Characteristics of Written Language Micro- and Macroskills for Reading Comprehension Strategies for Reading Comprehension Types of Classroom Reading Performance Principles for Teaching Reading Skills Two Reading Lessons Assessing Reading Topics for Discussion, Action, and Research For Your Further Reading Chapter 21 Teaching Writing Research on Second Language Writing Types of Written Language Characteristics of Written Language: A Writer's View Micro- and Macroskills for Writing Types of Classroom Writing Performance Principles for Teaching Writing Skills Assessing Writing Evaluation Checklists Writing Assessment Tasks Topics for Discussion, Action, and Research For Your Further Reading Chapter 22 Form-Focused Instruction The Place of Grammar To Teach or Not to Teach Grammar Issues About How to Teach Grammar Grammar Techniques Grammar Sequencing in Textbooks and Curricula Current Issues in Vocabulary Teaching Principles for Teaching Vocabulary Topics for Discussion, Action, and Research For Your Further Reading PART V. ASSESSING LANGUAGE SKILLS Chapter 23 Language Assessment: Principles and Issues Testing, Assessing, and Teaching Principles of Language Assessment Practicality Reliability Validity Content Validity Face Validity Construct Validity Authenticity Washback Kinds of Tests Historical Developments in Language Assessment Current Issues in Language Assessment Topics for Discussion, Action, and Research For Your Further Reading Chapter 24 Classroom-Based Assessment Norm-Referenced and Criterion-Referenced Tests Some Practical Steps to Test Construction Turning Existing Tests into more Effective Procedures Alternatives in Assessment Scrutinizing the Alternatives Topics for Discussion, Action, and Research For Your Further Reading PART VI. LIFELONG LEARNING Chapter 25 Teacher Development Peak Performers The \"Good\" Language Teacher Classroom Observation Classroom-Based \"Action\" Research Teacher Collaboration: Learning from each other Journals, Teaching Portfolios, Reading, and Writing Topics for Discussion, Action, and Research For Your Further Reading Chapter 26 Teachers for Social Responsibility Critical Pedagogy Hot Topics in the Language Classroom Moral Dilemmas and Moral Imperatives Agents for Change Topics for Discussion, Action, and Research For Your Further Reading Bibliography Name Index Subject Index",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0274810791015625
        },
        {
            "paperId": "500c275354f1d9b9b18473318292a13e6f85afe8",
            "corpusId": 145350852,
            "title": "Levelling the playing field : an investigation into the translation of academic literacy tests",
            "venue": "",
            "year": 2011,
            "referenceCount": 0,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.4314/JLT.V45I1.71804?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.4314/JLT.V45I1.71804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "40370911",
                    "name": "T. V. Dyk"
                },
                {
                    "authorId": "144797069",
                    "name": "A. V. Rensburg"
                },
                {
                    "authorId": "68981909",
                    "name": "F. Marais"
                }
            ],
            "abstract": "It is widely accepted that low levels of proficiency in the languages of learning and teaching (in this article, academic language proficiency refers to academic literacy and the terms will be used interchangeably), affect through-put rates negatively. This unsettling trend is confirmed by local and international literature, and can possibly be attributed to the language curriculum in secondary education that does not prepare students adequately for the higher-order language-thinking skills they need for study at university. By this we refer to Bloom\u2019s taxonomy, especially the three higher-order skills of analysing, synthesising and evaluating, and the way language is used for these purposes. In order to address this problem, and as part of language-planning initiatives, some faculties at Stellenbosch University introduced the integration of academic literacy courses into the first-year curriculum. These courses are fully creditbearing and a system of continuous assessment was adopted. Semester tests form part of this assessment process, and led to the investigation done for this paper. Since both Afrikaans- and English-speaking students register for the same academic literacy module it is imperative that outcomes and assessments should be on the same level. However, the aggregate on the Afrikaans semester tests have continuously been lower than on the English test. The aggregate for the Afrikaans tests was, furthermore, on par with the weighted average for all other first-year courses, which was not always the case with the English tests. After an initial investigation, it was concluded that the Englishspeaking students were not necessarily academically stronger than their Afrikaans counterparts, but it seemed likely that the problem lay with the tests themselves. A first notion was that academic and spoken English are closer than academic and spoken Afrikaans. It was also possible that the level of difficulty of the English test was substantially lower than that of the Afrikaans test. It should, however, be noted that both the Afrikaans and English tests produced excellent reliability coefficients (alpha above 0.88) and most items discriminated adequately. A possible solution to the benchmarking problem was to translate the Afrikaans test into English. The translation framework, adopted for this study, was Nord\u2019s functionalist model. This paper will elaborate on the translation procedure, and the variance in students\u2019 performance on the translated version compared to previous administrations. Preliminary conclusions on bias in translated tests and the success and feasibility of such procedures are drawn. Daar word allerwee aanvaar dat lae vaardigheidsvlakke in die tale van onderrig en leer (in hierdie artikel verwys akademiese taalvaardigheid na akademiese geletterdheid en die twee terme word afwisselend met mekaar gebruik) deurvloeikoerse benadeel. Hierdie onrusbarende tendens word deur plaaslike en internasionale literatuur bevestig, en kan moontlik daaraan toegeskryf word dat die taalkurrikulum in hoerskoolonderrig nie daarin slaag om studente genoegsaam vir die hoerorde-taaldenkvaardighede van universiteitstudie toe te rus nie. Met hoerorde-taaldenkvaardighede word verwys na Bloom se taksonomie, en spesifiek die drie hoerorde-vaardighede van analise, sintese en evaluering, en die wyse waarop taal vir hierdie doel aangewend word. Om hierdie probleem te hanteer, en as deel van taalbeplanningsinisiatiewe, het sommige fakulteite aan die Universiteit Stellenbosch kursusse in Akademiese Geletterdheid by die eerstejaarskurrikulum begin integreer. Hierdie kursusse is ten volle kredietdraend en volg \u2018n stelsel van voortgesette assessering. Semestertoetse maak deel uit van hierdie assesseringsproses en het tot die ondersoek vir hierdie navorsingstuk aanleiding gegee. Aangesien sowel Afrikaans- as Engelssprekende studente vir dieselfde module in Akademiese Geletterdheid registreer, is dit noodsaaklik dat uitkomste en assessering op gelyke vlak le. Tog was die totale punt vir die Afrikaanse semestertoets deurgaans laer as die vir die Engelse toets. Voorts was die totale punt vir die Afrikaanse toets in lyn met die geweegde gemiddelde vir alle ander eerstejaarskursusse, wat weer nie altyd die geval met die Engelse toets was nie. Na \u2018n aanvanklike ondersoek is afgelei dat die Engelssprekende studente nie noodwendig akademies sterker as hul Afrikaanse ewekniee is nie, maar dat die probleem waarskynlik by die toetse self le. \u2018n Eerste moontlikheid was dat akademiese en gesproke Engels nader aan mekaar is as akademiese en gesproke Afrikaans. Tweedens kon die moeilikheidsgraad van die Engelse toets ook aansienlik laer wees as die van die Afrikaanse toets. Dit is egter belangrik om daarop te let dat die Afrikaanse en Engelse toetse albei uitstekende betroubaarheidskoeffisiente (alfa bo 0.88) sowel as merendeels goeie diskrimineringskoeffisiente opgelewer het. \u2018n Moontlike oplossing vir die probleem van rigpuntstelling was om die Afrikaanse toets in Engels te vertaal. Die vertaalraamwerk wat vir hierdie studie aanvaar is, was Nord se funksionalistiese model. Hierdie navorsingstuk wei uit oor die vertaalprosedure sowel as die wisseling in studenteprestasie in die vertaalde toets in vergelyking met vorige toetse. Voorlopige gevolgtrekkings word ook gemaak oor sydigheid in vertaalde toetse, en die sukses en uitvoerbaarheid van sodanige prosedures. Keywords: academic literacy test; functionalist translation approach; back-translation; adaptation of tests The article is in English.",
            "corpus_id": "145350852",
            "text": "It is widely accepted that low levels of proficiency in the languages of learning and teaching (in this article, academic language proficiency refers to academic literacy and the terms will be used interchangeably), affect through-put rates negatively. This unsettling trend is confirmed by local and international literature, and can possibly be attributed to the language curriculum in secondary education that does not prepare students adequately for the higher-order language-thinking skills they need for study at university. By this we refer to Bloom\u2019s taxonomy, especially the three higher-order skills of analysing, synthesising and evaluating, and the way language is used for these purposes. In order to address this problem, and as part of language-planning initiatives, some faculties at Stellenbosch University introduced the integration of academic literacy courses into the first-year curriculum. These courses are fully creditbearing and a system of continuous assessment was adopted. Semester tests form part of this assessment process, and led to the investigation done for this paper. Since both Afrikaans- and English-speaking students register for the same academic literacy module it is imperative that outcomes and assessments should be on the same level. However, the aggregate on the Afrikaans semester tests have continuously been lower than on the English test. The aggregate for the Afrikaans tests was, furthermore, on par with the weighted average for all other first-year courses, which was not always the case with the English tests. After an initial investigation, it was concluded that the Englishspeaking students were not necessarily academically stronger than their Afrikaans counterparts, but it seemed likely that the problem lay with the tests themselves. A first notion was that academic and spoken English are closer than academic and spoken Afrikaans. It was also possible that the level of difficulty of the English test was substantially lower than that of the Afrikaans test. It should, however, be noted that both the Afrikaans and English tests produced excellent reliability coefficients (alpha above 0.88) and most items discriminated adequately. A possible solution to the benchmarking problem was to translate the Afrikaans test into English. The translation framework, adopted for this study, was Nord\u2019s functionalist model. This paper will elaborate on the translation procedure, and the variance in students\u2019 performance on the translated version compared to previous administrations. Preliminary conclusions on bias in translated tests and the success and feasibility of such procedures are drawn. Daar word allerwee aanvaar dat lae vaardigheidsvlakke in die tale van onderrig en leer (in hierdie artikel verwys akademiese taalvaardigheid na akademiese geletterdheid en die twee terme word afwisselend met mekaar gebruik) deurvloeikoerse benadeel. Hierdie onrusbarende tendens word deur plaaslike en internasionale literatuur bevestig, en kan moontlik daaraan toegeskryf word dat die taalkurrikulum in hoerskoolonderrig nie daarin slaag om studente genoegsaam vir die hoerorde-taaldenkvaardighede van universiteitstudie toe te rus nie. Met hoerorde-taaldenkvaardighede word verwys na Bloom se taksonomie, en spesifiek die drie hoerorde-vaardighede van analise, sintese en evaluering, en die wyse waarop taal vir hierdie doel aangewend word. Om hierdie probleem te hanteer, en as deel van taalbeplanningsinisiatiewe, het sommige fakulteite aan die Universiteit Stellenbosch kursusse in Akademiese Geletterdheid by die eerstejaarskurrikulum begin integreer. Hierdie kursusse is ten volle kredietdraend en volg \u2018n stelsel van voortgesette assessering. Semestertoetse maak deel uit van hierdie assesseringsproses en het tot die ondersoek vir hierdie navorsingstuk aanleiding gegee. Aangesien sowel Afrikaans- as Engelssprekende studente vir dieselfde module in Akademiese Geletterdheid registreer, is dit noodsaaklik dat uitkomste en assessering op gelyke vlak le. Tog was die totale punt vir die Afrikaanse semestertoets deurgaans laer as die vir die Engelse toets. Voorts was die totale punt vir die Afrikaanse toets in lyn met die geweegde gemiddelde vir alle ander eerstejaarskursusse, wat weer nie altyd die geval met die Engelse toets was nie. Na \u2018n aanvanklike ondersoek is afgelei dat die Engelssprekende studente nie noodwendig akademies sterker as hul Afrikaanse ewekniee is nie, maar dat die probleem waarskynlik by die toetse self le. \u2018n Eerste moontlikheid was dat akademiese en gesproke Engels nader aan mekaar is as akademiese en gesproke Afrikaans. Tweedens kon die moeilikheidsgraad van die Engelse toets ook aansienlik laer wees as die van die Afrikaanse toets. Dit is egter belangrik om daarop te let dat die Afrikaanse en Engelse toetse albei uitstekende betroubaarheidskoeffisiente (alfa bo 0.88) sowel as merendeels goeie diskrimineringskoeffisiente opgelewer het. \u2018n Moontlike oplossing vir die probleem van rigpuntstelling was om die Afrikaanse toets in Engels te vertaal. Die vertaalraamwerk wat vir hierdie studie aanvaar is, was Nord se funksionalistiese model. Hierdie navorsingstuk wei uit oor die vertaalprosedure sowel as die wisseling in studenteprestasie in die vertaalde toets in vergelyking met vorige toetse. Voorlopige gevolgtrekkings word ook gemaak oor sydigheid in vertaalde toetse, en die sukses en uitvoerbaarheid van sodanige prosedures. Keywords: academic literacy test; functionalist translation approach; back-translation; adaptation of tests The article is in English.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0802001953125
        },
        {
            "paperId": "3b212070acb5b2d98ccad8e9f566d764e188ee5d",
            "corpusId": 275021593,
            "title": "EDITORIAL: Evaluating Generative AI-Based Models in Healthcare",
            "venue": "Jordan Medical Journal",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.35516/jmj.v58i4.3809?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.35516/jmj.v58i4.3809, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1390173448",
                    "name": "M. Al-Akhras"
                }
            ],
            "abstract": "Since their release, generative AI models are increasingly used in many domains. Healthcare is one of the major domains where the incorporation of generative AI models in general and with focus on ChatGPT as the primary leading tool could be particularly transformative to the sector. \nRecognizing the urgency and significance of this research area, the Jordan Medical Journal (JMJ) has launched this special issue titled \"Evaluating Generative AI-Based Models in Healthcare\". This special issue aims to address existing challenges and highlights novel perspectives in this rapidly evolving field to advance the collective understanding of generative AI\u2019s role in healthcare. This special issue highlights key prioritized areas in the assessment of ChatGPT utilization in healthcare. \n\u00a0 \nThe leading article entitled \u201cEnvisioning the Future of ChatGPT in Healthcare: Insights and Recommendations from a Systematic Identification of Influential Research and a Call for Papers\u201d by Sallam et al. [1] paved the way for the call for papers of the remaining articles. The authors conducted a systematic search on Scopus, Web of Science, and Google Scholar to identify the top ChatGPT-related published records in healthcare with top articles denoted to be influential based on the citation count. This study [1] was based on a previous bibliometric analysis that identified the top ten ChatGPT-related healthcare published records over the period of a year (November 2022\u2013November 2023) [2]. \nIn the current study [1] the authors identified a total of 22 different articles discussing the use of ChatGPT in the healthcare sector. The authors defined future research areas regarding the utilization of ChatGPT in healthcare education, practice, and research based on the insights and recommendations from the top influential published records. Several important themes were identified by authors based on their search, including improving healthcare education, improving the efficiency of clinical processes, addressing ethical concerns, supporting research tasks, mitigating model\u2019s bias due to the possible bias in the training data, improving patient education and engagement, and developing standardized assessment protocols for ChatGPT utility in healthcare. \n\u00a0 \nIn the second article [3] entitled \u201cEnsuring Security and Privacy in Healthcare Systems: A Review Exploring Challenges, Solutions, Future Trends\u201d by Bala et al. the authors discussed the importance of ensuring security and offering privacy in healthcare systems in the context of AI integration in such systems. The authors discussed the belief that healthcare systems should emphasize providing medical attention to patients and pay less attention to the security aspects of the system. This was due to the historical misconception that healthcare systems would not be targeted by attackers. \nThe authors identified several factors contributing to the increasing vulnerabilities of the healthcare systems: (1) the increased use of interconnected technologies with their inherited vulnerabilities, (2) continuous monitoring of patients' well-being beyond the clinical setting leading to increased use of extra medical devices heightening the vulnerability of data breaches, (3) The increased use of mobile consumer electronics, such as cell phones making more challenging to safeguard health data from the risks associated with multipurpose gadgets, (4) Many healthcare organizations continue to employ outdated systems in various sectors, such as Windows XP, which has not received recent updates. \nThe authors discussed: (1) The detailed architecture of typical healthcare system components, (2) existing cyberattacks in the healthcare sector proposing solutions to combat these attacks. (3) \u00a0the research community's challenges in ensuring security and privacy in healthcare systems. (4) The importance of AI in protecting medical data, protecting privacy, and monitoring patients. (5) The Open challenges and future research directions for addressing security and privacy issues in health systems. \nThe authors also emphasized that the integration of generative AI in healthcare introduces a range of cybersecurity challenges and ethical considerations. If not properly secured AI systems can be exploited in cyberattacks [4] or be manipulated to generate false data or misleading medical predictions, which could have severe negative consequences for patient care and privacy [5]. The proliferation of AI-driven technologies increases the surface area for potential breaches, as every point in an AI system\u2014from data input to model training and output\u2014can be a vulnerability [6]. Data bias and reliability were another source of concern raised by the authors. AI systems are only as good as the data they are trained on. If training data is flawed or biased, this can lead to inaccurate medical predictions and treatments, exacerbating health disparities [7]. \nIn the third article [8] entitled \u201cEvaluation of Artificial Intelligence-Based Chatbot Responses to Common Dermatological Queries\u201d the authors aimed to investigate the utility of conversational AI models in addressing diagnostic challenges and treatment recommendations for common dermatological ailments. \nA dataset of 22 cases vignettes of dermatological conditions was compiled, each case accompanied by three specific queries. These cases were presented to four distinct conversational AI models - ChatGPT 3.5, Google Gemini, Microsoft Copilot (GPT 4), and Perplexity.ai and responses were saved and evaluated by two expert dermatologists (>5 years experience post-graduation) independently using a 5-point Likert scale ranging from highly accurate (= 5) to inaccurate (= 1). \nThis study [8] pointed to the efforts of utilizing generative AI potential in dermatological healthcare education [9, 10]. The results indicate that the 4 used generative AI systems may produce different accuracy results to the same question due to their variations in underlying architecture, domain-specific fine-tuning, and intended application [11, 12, 13]. The authors pointed to several possible reasons, such as the specific algorithm, training data, and linguistic nuances employed [14, 15]. The study [8] findings underscore the limitations of the used generative models in accurate disease diagnosis. The authors recommended that the programs may be used as supplementary resources rather than primary diagnostic tools. \nIn the fourth article [16] entitled \u201cEnhancing Security and Privacy in Healthcare with Generative Artificial Intelligence-Based Detection and Mitigation of Data Poisoning Attacks Software\u201d the authors proposed stopping poisoning attacks by mixing federated learning, homomorphic encryption, and auto encoder-based anomaly identification without changing the original data. \nIn the fifth article [17] entitled \u201cEvaluating ChatGPT's Role in Assessing Turkey's Medical Tourism Trends,\u201d the authors evaluated the performance of ChatGPT as an expert in the field of medical tourism and to assess the current and future states of medical tourism in Turkey. \u00a0The authors developed a questionnaire based on a study entitled \u201cBridging the gap of bibliometric analysis: the evolution, current state, and future directions of tourism research using ChatGPT\u201d [18]. Questions were evaluated by 8 experts of different relevant backgrounds. Thirteen questions were asked to ChatGPT 4-o and questions were statistically analyzed. The reporting was conducted using the CLEAR tool and the METRICS checklist, which are prepared for artificial intelligence studies [19, 20]. \nThe authors [17] reported that the general answers to ChatGPT 4-o were accurate, informative, and helpful in providing a good overview of medical tourism in Turkey. However, the responses lacked details, did not provide evidence-based information, and did not always address the nuances and cultural/social impacts of medical tourism. ChatGPT 4-o views the future of Turkey\u2019s medical tourism as shiny. \nIn the sixth article [21] entitled \u201cCross-Linguistic Evaluation of Generative AI Models for Diabetes and Endocrine Queries\u201d the authors aimed to evaluate the performance of two genAI models, ChatGPT and Microsoft Copilot, in addressing endocrine and metabolic disorders queries in English and Arabic. Comparing responses generated in English and Arabic aimed to identify potential disparities in accuracy, relevance, and completeness. \nDuring the evaluation, the METRICS checklist [19, 20] was used to evaluate the study. ChatGPT-4o outperformed Microsoft Copilot in all CLEAR components, but notable language-based disparities were evident. Addressing these limitations is crucial to ensure equitable access to endocrine care for non-English-speaking patients. \nIn the seventh (last) article [22] entitled \u201cThe Future of Pediatric Care: AI and ML as Catalysts for Change in Genetic Syndrome Management\u201d The authors thoroughly investigate the potential impact of Artificial Intelligence (AI) and Machine Learning (ML) in pediatric healthcare and education, with a focus on genetic syndromes. Exploring several disorders highlights the significance of AI and ML in improving diagnostic accuracy, tailoring treatment approaches, and transforming educational methods. This review emphasizes the effect of incorporating AI and ML into pediatric practices for genomic analysis and adaptive education. These technologies help enhance comprehension of genetic disorders, allowing for personalized interventions customized to each child's specific requirements. The review suggests that combining AI and ML significantly enhances diagnostic accuracy, treatment effectiveness, and educational results, thereby establishing higher benchmarks for pediatric care. \nIn conclusion, the JMJ special issue published 7 articles that cover several aspects of the use of generative AI in healthcare education, practice, and research. The leading article, based on a systematic search process, paved the way to highlight k",
            "corpus_id": "275021593",
            "text": "Since their release, generative AI models are increasingly used in many domains. Healthcare is one of the major domains where the incorporation of generative AI models in general and with focus on ChatGPT as the primary leading tool could be particularly transformative to the sector. \nRecognizing the urgency and significance of this research area, the Jordan Medical Journal (JMJ) has launched this special issue titled \"Evaluating Generative AI-Based Models in Healthcare\". This special issue aims to address existing challenges and highlights novel perspectives in this rapidly evolving field to advance the collective understanding of generative AI\u2019s role in healthcare. This special issue highlights key prioritized areas in the assessment of ChatGPT utilization in healthcare. \n\u00a0 \nThe leading article entitled \u201cEnvisioning the Future of ChatGPT in Healthcare: Insights and Recommendations from a Systematic Identification of Influential Research and a Call for Papers\u201d by Sallam et al. [1] paved the way for the call for papers of the remaining articles. The authors conducted a systematic search on Scopus, Web of Science, and Google Scholar to identify the top ChatGPT-related published records in healthcare with top articles denoted to be influential based on the citation count. This study [1] was based on a previous bibliometric analysis that identified the top ten ChatGPT-related healthcare published records over the period of a year (November 2022\u2013November 2023) [2]. \nIn the current study [1] the authors identified a total of 22 different articles discussing the use of ChatGPT in the healthcare sector. The authors defined future research areas regarding the utilization of ChatGPT in healthcare education, practice, and research based on the insights and recommendations from the top influential published records. Several important themes were identified by authors based on their search, including improving healthcare education, improving the efficiency of clinical processes, addressing ethical concerns, supporting research tasks, mitigating model\u2019s bias due to the possible bias in the training data, improving patient education and engagement, and developing standardized assessment protocols for ChatGPT utility in healthcare. \n\u00a0 \nIn the second article [3] entitled \u201cEnsuring Security and Privacy in Healthcare Systems: A Review Exploring Challenges, Solutions, Future Trends\u201d by Bala et al. the authors discussed the importance of ensuring security and offering privacy in healthcare systems in the context of AI integration in such systems. The authors discussed the belief that healthcare systems should emphasize providing medical attention to patients and pay less attention to the security aspects of the system. This was due to the historical misconception that healthcare systems would not be targeted by attackers. \nThe authors identified several factors contributing to the increasing vulnerabilities of the healthcare systems: (1) the increased use of interconnected technologies with their inherited vulnerabilities, (2) continuous monitoring of patients' well-being beyond the clinical setting leading to increased use of extra medical devices heightening the vulnerability of data breaches, (3) The increased use of mobile consumer electronics, such as cell phones making more challenging to safeguard health data from the risks associated with multipurpose gadgets, (4) Many healthcare organizations continue to employ outdated systems in various sectors, such as Windows XP, which has not received recent updates. \nThe authors discussed: (1) The detailed architecture of typical healthcare system components, (2) existing cyberattacks in the healthcare sector proposing solutions to combat these attacks. (3) \u00a0the research community's challenges in ensuring security and privacy in healthcare systems. (4) The importance of AI in protecting medical data, protecting privacy, and monitoring patients. (5) The Open challenges and future research directions for addressing security and privacy issues in health systems. \nThe authors also emphasized that the integration of generative AI in healthcare introduces a range of cybersecurity challenges and ethical considerations. If not properly secured AI systems can be exploited in cyberattacks [4] or be manipulated to generate false data or misleading medical predictions, which could have severe negative consequences for patient care and privacy [5]. The proliferation of AI-driven technologies increases the surface area for potential breaches, as every point in an AI system\u2014from data input to model training and output\u2014can be a vulnerability [6]. Data bias and reliability were another source of concern raised by the authors. AI systems are only as good as the data they are trained on. If training data is flawed or biased, this can lead to inaccurate medical predictions and treatments, exacerbating health disparities [7]. \nIn the third article [8] entitled \u201cEvaluation of Artificial Intelligence-Based Chatbot Responses to Common Dermatological Queries\u201d the authors aimed to investigate the utility of conversational AI models in addressing diagnostic challenges and treatment recommendations for common dermatological ailments. \nA dataset of 22 cases vignettes of dermatological conditions was compiled, each case accompanied by three specific queries. These cases were presented to four distinct conversational AI models - ChatGPT 3.5, Google Gemini, Microsoft Copilot (GPT 4), and Perplexity.ai and responses were saved and evaluated by two expert dermatologists (>5 years experience post-graduation) independently using a 5-point Likert scale ranging from highly accurate (= 5) to inaccurate (= 1). \nThis study [8] pointed to the efforts of utilizing generative AI potential in dermatological healthcare education [9, 10]. The results indicate that the 4 used generative AI systems may produce different accuracy results to the same question due to their variations in underlying architecture, domain-specific fine-tuning, and intended application [11, 12, 13]. The authors pointed to several possible reasons, such as the specific algorithm, training data, and linguistic nuances employed [14, 15]. The study [8] findings underscore the limitations of the used generative models in accurate disease diagnosis. The authors recommended that the programs may be used as supplementary resources rather than primary diagnostic tools. \nIn the fourth article [16] entitled \u201cEnhancing Security and Privacy in Healthcare with Generative Artificial Intelligence-Based Detection and Mitigation of Data Poisoning Attacks Software\u201d the authors proposed stopping poisoning attacks by mixing federated learning, homomorphic encryption, and auto encoder-based anomaly identification without changing the original data. \nIn the fifth article [17] entitled \u201cEvaluating ChatGPT's Role in Assessing Turkey's Medical Tourism Trends,\u201d the authors evaluated the performance of ChatGPT as an expert in the field of medical tourism and to assess the current and future states of medical tourism in Turkey. \u00a0The authors developed a questionnaire based on a study entitled \u201cBridging the gap of bibliometric analysis: the evolution, current state, and future directions of tourism research using ChatGPT\u201d [18]. Questions were evaluated by 8 experts of different relevant backgrounds. Thirteen questions were asked to ChatGPT 4-o and questions were statistically analyzed. The reporting was conducted using the CLEAR tool and the METRICS checklist, which are prepared for artificial intelligence studies [19, 20]. \nThe authors [17] reported that the general answers to ChatGPT 4-o were accurate, informative, and helpful in providing a good overview of medical tourism in Turkey. However, the responses lacked details, did not provide evidence-based information, and did not always address the nuances and cultural/social impacts of medical tourism. ChatGPT 4-o views the future of Turkey\u2019s medical tourism as shiny. \nIn the sixth article [21] entitled \u201cCross-Linguistic Evaluation of Generative AI Models for Diabetes and Endocrine Queries\u201d the authors aimed to evaluate the performance of two genAI models, ChatGPT and Microsoft Copilot, in addressing endocrine and metabolic disorders queries in English and Arabic. Comparing responses generated in English and Arabic aimed to identify potential disparities in accuracy, relevance, and completeness. \nDuring the evaluation, the METRICS checklist [19, 20] was used to evaluate the study. ChatGPT-4o outperformed Microsoft Copilot in all CLEAR components, but notable language-based disparities were evident. Addressing these limitations is crucial to ensure equitable access to endocrine care for non-English-speaking patients. \nIn the seventh (last) article [22] entitled \u201cThe Future of Pediatric Care: AI and ML as Catalysts for Change in Genetic Syndrome Management\u201d The authors thoroughly investigate the potential impact of Artificial Intelligence (AI) and Machine Learning (ML) in pediatric healthcare and education, with a focus on genetic syndromes. Exploring several disorders highlights the significance of AI and ML in improving diagnostic accuracy, tailoring treatment approaches, and transforming educational methods. This review emphasizes the effect of incorporating AI and ML into pediatric practices for genomic analysis and adaptive education. These technologies help enhance comprehension of genetic disorders, allowing for personalized interventions customized to each child's specific requirements. The review suggests that combining AI and ML significantly enhances diagnostic accuracy, treatment effectiveness, and educational results, thereby establishing higher benchmarks for pediatric care. \nIn conclusion, the JMJ special issue published 7 articles that cover several aspects of the use of generative AI in healthcare education, practice, and research. The leading article, based on a systematic search process, paved the way to highlight k",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.023193359375
        }
    ],
    "quotes": {
        "cost": 0.09395700000000001,
        "quotes": [
            {
                "idx": 0,
                "key": "[215548041 | Artetxe et al. | 2020 | Citations: 120]",
                "snippets": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Discussion",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 1089,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 66
                            },
                            {
                                "start": 67,
                                "end": 142
                            },
                            {
                                "start": 143,
                                "end": 425
                            },
                            {
                                "start": 426,
                                "end": 620
                            },
                            {
                                "start": 621,
                                "end": 793
                            },
                            {
                                "start": 794,
                                "end": 1089
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[247594499 | Hershcovich et al. | 2022 | Citations: 182]",
                "snippets": "When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[253885863 | Milios et al. | 2022 | Citations: 8]",
                "snippets": "In this paper, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this paper, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[259108559 | Keleg et al. | 2023 | Citations: 11]",
                "snippets": "While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks....results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models....The quality of the template might degrade after automatically translating it from English...Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias....Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 962,
                        "end": 1147,
                        "sentence_offsets": [
                            {
                                "start": 962,
                                "end": 1144
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks"
                    },
                    {
                        "quote": ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 527,
                        "end": 616,
                        "sentence_offsets": [
                            {
                                "start": 527,
                                "end": 779
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".The quality of the template might degrade after automatically translating it from English"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 783,
                        "end": 912,
                        "sentence_offsets": [
                            {
                                "start": 780,
                                "end": 910
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias"
                    },
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1065,
                        "end": 1240,
                        "sentence_offsets": [
                            {
                                "start": 1065,
                                "end": 1240
                            }
                        ],
                        "ref_mentions": [],
                        "quote": ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[267413041 | Agrawal et al. | 2024 | Citations: 3]",
                "snippets": "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268819377 | Xu et al. | 2024 | Citations: 43]",
                "snippets": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[247626152 | Talat1 et al. | 2022 | Citations: 102]": "Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."
                },
                "metadata": [
                    {
                        "section_title": "B. Bias Benchmark",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 295,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 148
                            },
                            {
                                "start": 149,
                                "end": 295
                            }
                        ],
                        "ref_mentions": [
                            "247626152"
                        ],
                        "quote": "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022)."
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[270380088 | Etxaniz et al. | 2024 | Citations: 11]",
                "snippets": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 569,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 267
                            },
                            {
                                "start": 268,
                                "end": 406
                            },
                            {
                                "start": 407,
                                "end": 569
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[270737791 | Plaza et al. | 2024 | Citations: 8]",
                "snippets": "This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 117,
                        "end": 1066,
                        "sentence_offsets": [
                            {
                                "start": 117,
                                "end": 259
                            },
                            {
                                "start": 259,
                                "end": 341
                            },
                            {
                                "start": 341,
                                "end": 379
                            },
                            {
                                "start": 379,
                                "end": 853
                            },
                            {
                                "start": 853,
                                "end": 1066
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[274130807 | Guo et al. | 2024 | Citations: 22]",
                "snippets": "\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A.2.5 Machine Translation",
                        "pdf_hash": "",
                        "start": 638,
                        "end": 1665,
                        "sentence_offsets": [
                            {
                                "start": 638,
                                "end": 836
                            },
                            {
                                "start": 837,
                                "end": 1020
                            },
                            {
                                "start": 1021,
                                "end": 1180
                            },
                            {
                                "start": 1183,
                                "end": 1345
                            },
                            {
                                "start": 1346,
                                "end": 1510
                            },
                            {
                                "start": 1511,
                                "end": 1665
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016)."
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[274464561 | Singh et al. | 2024 | Citations: 34]",
                "snippets": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 607,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[274610534 | Wang et al. | 2024 | Citations: 2]",
                "snippets": "Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 212,
                        "end": 388,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[276106987 | Magnini et al. | 2025 | Citations: 1]",
                "snippets": "\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[221516475 | Hendrycks et al. | 2020 | Citations: 4568]": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 638,
                        "end": 1924,
                        "sentence_offsets": [
                            {
                                "start": 638,
                                "end": 664
                            },
                            {
                                "start": 665,
                                "end": 793
                            },
                            {
                                "start": 794,
                                "end": 882
                            },
                            {
                                "start": 883,
                                "end": 1044
                            },
                            {
                                "start": 1045,
                                "end": 1226
                            },
                            {
                                "start": 1227,
                                "end": 1237
                            },
                            {
                                "start": 1238,
                                "end": 1386
                            },
                            {
                                "start": 1387,
                                "end": 1570
                            },
                            {
                                "start": 1571,
                                "end": 1653
                            },
                            {
                                "start": 1654,
                                "end": 1662
                            },
                            {
                                "start": 1663,
                                "end": 1809
                            },
                            {
                                "start": 1810,
                                "end": 1924
                            }
                        ],
                        "ref_mentions": [
                            "275357255",
                            "221516475"
                        ],
                        "quote": "\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[276421738 | Barth et al. | 2025 | Citations: 0]",
                "snippets": "The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures....Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[265445838 | Tao et al. | 2023 | Citations: 87]": "Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 106,
                        "end": 589,
                        "sentence_offsets": [
                            {
                                "start": 106,
                                "end": 278
                            },
                            {
                                "start": 279,
                                "end": 451
                            },
                            {
                                "start": 452,
                                "end": 587
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures"
                    },
                    {
                        "section_title": "Challenge 3: Cultural Bias",
                        "pdf_hash": "",
                        "start": 126,
                        "end": 1269,
                        "sentence_offsets": [
                            {
                                "start": 61,
                                "end": 150
                            },
                            {
                                "start": 151,
                                "end": 328
                            },
                            {
                                "start": 329,
                                "end": 444
                            },
                            {
                                "start": 445,
                                "end": 525
                            },
                            {
                                "start": 526,
                                "end": 607
                            },
                            {
                                "start": 608,
                                "end": 820
                            },
                            {
                                "start": 821,
                                "end": 943
                            },
                            {
                                "start": 946,
                                "end": 1018
                            },
                            {
                                "start": 1019,
                                "end": 1193
                            },
                            {
                                "start": 1194,
                                "end": 1304
                            }
                        ],
                        "ref_mentions": [
                            "265445838"
                        ],
                        "quote": ".Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[276580130 | Liu et al. | 2025 | Citations: 0]",
                "snippets": "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Factor",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 590,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 67
                            },
                            {
                                "start": 68,
                                "end": 187
                            },
                            {
                                "start": 188,
                                "end": 387
                            },
                            {
                                "start": 388,
                                "end": 590
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[277066532 | Etori et al. | 2025 | Citations: 0]",
                "snippets": "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 282,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 136
                            },
                            {
                                "start": 137,
                                "end": 282
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023)."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.019899,
        "cot": "For this query about cultural biases and language artifacts in translated benchmarks, I need to organize the quotes into meaningful dimensions that will help answer the question comprehensively.\n\n1. \"Introduction/Background\" - This section should provide context about the practice of translating English benchmarks and why this is an important issue. This will be in synthesis format to give the reader a coherent understanding of the landscape before diving into specifics.\n\n2. \"Types of Cultural Biases in Translated Benchmarks\" - This dimension will identify and explain the specific cultural biases that emerge when English benchmarks are translated. This should be in synthesis format to provide a cohesive analysis of the various cultural biases mentioned across the quotes.\n\n3. \"Translation Artifacts and Inconsistencies\" - This section will focus specifically on language artifacts and inconsistencies introduced during translation processes. Synthesis format is appropriate here to connect the various technical issues that can arise.\n\n4. \"Impact on Model Performance and Evaluation Reliability\" - This dimension will address how these biases and artifacts affect the assessment of multilingual models. Synthesis format will help connect the various effects into a coherent explanation.\n\n5. \"Proposed Solutions and Recommendations\" - This final section should collect the recommendations and potential solutions mentioned across the quotes. List format is appropriate here to clearly enumerate the different approaches that researchers have suggested.\n\nI'll now map each quote to these dimensions:",
        "plan": {
            "Introduction/Background (synthesis)": [
                6,
                10
            ],
            "Types of Cultural Biases in Translated Benchmarks (synthesis)": [
                1,
                2,
                3,
                8,
                9,
                11,
                12,
                13,
                14
            ],
            "Translation Artifacts and Inconsistencies (synthesis)": [
                0,
                4,
                5,
                7
            ],
            "Impact on Model Performance and Evaluation Reliability (synthesis)": [
                3,
                4,
                7,
                9,
                11,
                13
            ],
            "Proposed Solutions and Recommendations (list)": [
                0,
                1,
                3,
                5,
                12
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background",
                "tldr": "Natural language processing (NLP) evaluation has historically centered on English, with multilingual benchmarks typically created through translation from English sources. This approach introduces cultural biases and representational issues that can skew evaluation results and impact our understanding of model capabilities across different languages. (2 sources)",
                "text": "\nMultilingual evaluation in natural language processing has been predominantly English-centric, with most benchmarks originating in English and then being translated to other languages. Popular multilingual datasets like XNLI, XQUAD, MLQA, and Belebele follow this translation-based approach, where English content serves as the source material. <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> While this methodology facilitates valuable comparisons across languages through parallel content, it introduces significant challenges to fair and accurate evaluation.\n\nThe translation process itself is not culturally neutral. When English evaluation benchmarks are translated into non-English languages, they carry forward Western cultural assumptions, contexts, and references that may not resonate or function equivalently in target languages and cultures. <Paper corpusId=\"274610534\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> These embedded biases can impact how models perform on translated benchmarks, potentially providing an inaccurate picture of a model's capabilities in non-English languages.\n\nBeyond cultural representation issues, translation introduces structural artifacts that reflect the source language rather than natural expressions in the target language. These translation artifacts affect experimental conclusions by reflecting the origin culture and linguistic patterns rather than authentic target language usage. <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> This can lead to evaluation results that don't accurately represent how models would perform on content originally created in those languages.",
                "citations": [
                    {
                        "id": "(Etxaniz et al., 2024)",
                        "snippets": [
                            "Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."
                        ],
                        "paper": {
                            "corpus_id": 270380088,
                            "title": "BertaQA: How Much Do Language Models Know About Local Culture?",
                            "authors": [
                                {
                                    "authorId": "2226458991",
                                    "name": "Julen Etxaniz"
                                },
                                {
                                    "authorId": "2481918",
                                    "name": "Gorka Azkune"
                                },
                                {
                                    "authorId": "2260104163",
                                    "name": "A. Soroa"
                                },
                                {
                                    "authorId": "2251043402",
                                    "name": "Oier L\u00f3pez de Lacalle"
                                },
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 11
                        },
                        "score": 0.525390625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures."
                        ],
                        "paper": {
                            "corpus_id": 274610534,
                            "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context",
                            "authors": [
                                {
                                    "authorId": "2334689696",
                                    "name": "Xiaonan Wang"
                                },
                                {
                                    "authorId": "2334573099",
                                    "name": "Jinyoung Yeo"
                                },
                                {
                                    "authorId": "2334827754",
                                    "name": "Joon-Ho Lim"
                                },
                                {
                                    "authorId": "2334743749",
                                    "name": "Hansaem Kim"
                                }
                            ],
                            "year": 2024,
                            "venue": "Pacific Asia Conference on Language, Information and Computation",
                            "n_citations": 2
                        },
                        "score": 0.5400390625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Types of Cultural Biases in Translated Benchmarks",
                "tldr": "Translated benchmarks contain multiple forms of cultural bias including Western-centric content representation, cultural nuance loss during translation, and representation biases toward dominant cultures, making them unreliable indicators of model performance across different languages and cultural contexts. (10 sources)",
                "text": "\nTranslating English benchmarks into other languages introduces several distinct types of cultural biases that significantly impact evaluation validity. One of the most pervasive issues is content bias, where benchmarks remain rooted in Anglo-American cultural contexts despite being translated. The MMLU benchmark, for example, contains numerous questions about specific Anglo-American cultural facts that models trained primarily on non-English data would struggle to answer correctly <Paper corpusId=\"276106987\" paperTitle=\"(Magnini et al., 2025)\" isShortName></Paper> <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. This creates an inherent disadvantage for evaluating models in non-English languages and cultures <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nRepresentation bias constitutes another significant concern. Datasets like mLAMA demonstrate a clear bias toward Western countries and facts, creating fairness issues when probing multilingual models <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>. This occurs because many benchmarks are built from English Wikipedia or Western-centric knowledge sources, resulting in evaluation sets that disproportionately feature Western cultural references <Paper corpusId=\"277066532\" paperTitle=\"(Etori et al., 2025)\" isShortName></Paper>.\n\nCultural nuance loss represents a third critical bias type. Culturally specific idioms, proverbs, and colloquial expressions often lose their meaning or are mistranslated when models lack cultural context <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. For instance, English idioms like \"raining cats and dogs\" might be translated literally into other languages, creating confusing or meaningless phrases in the target culture rather than finding culturally equivalent expressions <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nTranslation processes frequently demonstrate bias toward dominant cultures. Machine translation systems may favor translations that align with Western cultural norms even when source text belongs to non-Western cultures <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. This creates situations where culturally significant concepts are diluted or misrepresented in translated benchmarks <Paper corpusId=\"253885863\" paperTitle=\"(Milios et al., 2022)\" isShortName></Paper>.\n\nSocial and behavioral differences across cultures create additional bias challenges. When translating datasets into other languages, decisions must be made about modifying items to make them intelligible in the target culture <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. While some approaches attempt to automatically flag and remove culturally specific examples, these methods cannot capture all instances of cultural over-specificity, such as references to sports teams or region-specific foods <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>.\n\nThe mechanisms for addressing bias may themselves perpetuate inequality. Research shows that reasoning approaches like Chain-of-Thought may be more aligned with Western fairness norms while reinforcing cultural specificity in non-English languages <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This indicates an imbalance in multilingual fairness mechanisms, where bias mitigation efforts developed primarily for English-speaking cultures leave non-Western biases more embedded <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThese cultural biases collectively undermine the effectiveness of translated benchmarks as global evaluation tools <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. They stem not only from language differences but also from the cultural knowledge required to interpret questions correctly, reducing the practical utility of translated datasets like MMLU for comprehensive multilingual evaluation <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Magnini et al., 2025)",
                        "snippets": [
                            "\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results."
                        ],
                        "paper": {
                            "corpus_id": 276106987,
                            "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
                            "authors": [
                                {
                                    "authorId": "2338997440",
                                    "name": "Bernardo Magnini"
                                },
                                {
                                    "authorId": "36024018",
                                    "name": "Roberto Zanoli"
                                },
                                {
                                    "authorId": "2343747257",
                                    "name": "Michele Resta"
                                },
                                {
                                    "authorId": "2343745825",
                                    "name": "Martin Cimmino"
                                },
                                {
                                    "authorId": "2343739878",
                                    "name": "Paolo Albano"
                                },
                                {
                                    "authorId": "2277456934",
                                    "name": "Marco Madeddu"
                                },
                                {
                                    "authorId": "2337691090",
                                    "name": "Viviana Patti"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.5693359375
                    },
                    {
                        "id": "(Hendrycks et al., 2020)",
                        "snippets": [
                            "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                        ],
                        "paper": {
                            "corpus_id": 221516475,
                            "title": "Measuring Massive Multitask Language Understanding",
                            "authors": [
                                {
                                    "authorId": "3422872",
                                    "name": "Dan Hendrycks"
                                },
                                {
                                    "authorId": "90909974",
                                    "name": "Collin Burns"
                                },
                                {
                                    "authorId": "104444594",
                                    "name": "Steven Basart"
                                },
                                {
                                    "authorId": "1380103052",
                                    "name": "Andy Zou"
                                },
                                {
                                    "authorId": "16787428",
                                    "name": "Mantas Mazeika"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "5164568",
                                    "name": "J. Steinhardt"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4568
                        },
                        "score": 0
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures",
                            ".Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.63134765625
                    },
                    {
                        "id": "(Keleg et al., 2023)",
                        "snippets": [
                            "While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks",
                            ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models",
                            ".The quality of the template might degrade after automatically translating it from English",
                            "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias",
                            ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."
                        ],
                        "paper": {
                            "corpus_id": 259108559,
                            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "1725417862",
                                    "name": "Amr Keleg"
                                },
                                {
                                    "authorId": "1745226",
                                    "name": "Walid Magdy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 11
                        },
                        "score": 0.53173828125
                    },
                    {
                        "id": "(Etori et al., 2025)",
                        "snippets": [
                            "However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023)."
                        ],
                        "paper": {
                            "corpus_id": 277066532,
                            "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama",
                            "authors": [
                                {
                                    "authorId": "1742219452",
                                    "name": "Naome A. Etori"
                                },
                                {
                                    "authorId": "2350516308",
                                    "name": "Kevin Lu"
                                },
                                {
                                    "authorId": "2350516959",
                                    "name": "Randu Karisa"
                                },
                                {
                                    "authorId": "2322446945",
                                    "name": "Arturs Kanepajs"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.62890625
                    },
                    {
                        "id": "(Guo et al., 2024)",
                        "snippets": [
                            "\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016)."
                        ],
                        "paper": {
                            "corpus_id": 274130807,
                            "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
                            "authors": [
                                {
                                    "authorId": "2331613833",
                                    "name": "Yufei Guo"
                                },
                                {
                                    "authorId": "2331371661",
                                    "name": "Muzhe Guo"
                                },
                                {
                                    "authorId": "2273060735",
                                    "name": "Juntao Su"
                                },
                                {
                                    "authorId": "2331371753",
                                    "name": "Zhou Yang"
                                },
                                {
                                    "authorId": "2331357933",
                                    "name": "Mengqiu Zhu"
                                },
                                {
                                    "authorId": "2331365682",
                                    "name": "Hongfei Li"
                                },
                                {
                                    "authorId": "1455277182",
                                    "name": "Mengyang Qiu"
                                },
                                {
                                    "authorId": "2331377869",
                                    "name": "Shuo Shuo Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 22
                        },
                        "score": 0.57275390625
                    },
                    {
                        "id": "(Milios et al., 2022)",
                        "snippets": [
                            "In this paper, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it."
                        ],
                        "paper": {
                            "corpus_id": 253885863,
                            "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages",
                            "authors": [
                                {
                                    "authorId": "1516303035",
                                    "name": "Aristides Milios"
                                },
                                {
                                    "authorId": "2101317786",
                                    "name": "Parishad BehnamGhader"
                                }
                            ],
                            "year": 2022,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.59521484375
                    },
                    {
                        "id": "(Hershcovich et al., 2022)",
                        "snippets": [
                            "When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process."
                        ],
                        "paper": {
                            "corpus_id": 247594499,
                            "title": "Challenges and Strategies in Cross-Cultural NLP",
                            "authors": [
                                {
                                    "authorId": "2064295987",
                                    "name": "Daniel Hershcovich"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "49568895",
                                    "name": "Heather Lent"
                                },
                                {
                                    "authorId": "3295381",
                                    "name": "Miryam de Lhoneux"
                                },
                                {
                                    "authorId": "30671790",
                                    "name": "Mostafa Abdou"
                                },
                                {
                                    "authorId": "6547490",
                                    "name": "Stephanie Brandl"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "2093582149",
                                    "name": "Laura Cabello Piqueras"
                                },
                                {
                                    "authorId": "2125376289",
                                    "name": "Ilias Chalkidis"
                                },
                                {
                                    "authorId": "1717462692",
                                    "name": "Ruixiang Cui"
                                },
                                {
                                    "authorId": "50110151",
                                    "name": "Constanza Fierro"
                                },
                                {
                                    "authorId": "82259306",
                                    "name": "Katerina Margatina"
                                },
                                {
                                    "authorId": "1660797358",
                                    "name": "Phillip Rust"
                                },
                                {
                                    "authorId": "1700187",
                                    "name": "Anders S\u00f8gaard"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 182
                        },
                        "score": 0.5400390625
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems."
                        ],
                        "paper": {
                            "corpus_id": 276580130,
                            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
                            "authors": [
                                {
                                    "authorId": "4289746",
                                    "name": "Qianying Liu"
                                },
                                {
                                    "authorId": "2347161903",
                                    "name": "Katrina Qiyao Wang"
                                },
                                {
                                    "authorId": "2342687540",
                                    "name": "Fei Cheng"
                                },
                                {
                                    "authorId": "1795664",
                                    "name": "S. Kurohashi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.50634765625
                    },
                    {
                        "id": "(Singh et al., 2024)",
                        "snippets": [
                            "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges."
                        ],
                        "paper": {
                            "corpus_id": 274464561,
                            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
                            "authors": [
                                {
                                    "authorId": "2283844788",
                                    "name": "Shivalika Singh"
                                },
                                {
                                    "authorId": "1910588458",
                                    "name": "Angelika Romanou"
                                },
                                {
                                    "authorId": "2080941785",
                                    "name": "Cl\u00e9mentine Fourrier"
                                },
                                {
                                    "authorId": "2518906",
                                    "name": "David Ifeoluwa Adelani"
                                },
                                {
                                    "authorId": "119076558",
                                    "name": "Jian Gang Ngui"
                                },
                                {
                                    "authorId": "1403849795",
                                    "name": "Daniel Vila-Suero"
                                },
                                {
                                    "authorId": "1596821065",
                                    "name": "Peerat Limkonchotiwat"
                                },
                                {
                                    "authorId": "1396188646",
                                    "name": "Kelly Marchisio"
                                },
                                {
                                    "authorId": "2140097897",
                                    "name": "Wei Qi Leong"
                                },
                                {
                                    "authorId": "2239100505",
                                    "name": "Yosephine Susanto"
                                },
                                {
                                    "authorId": "2352918897",
                                    "name": "Raymond Ng"
                                },
                                {
                                    "authorId": "2283848744",
                                    "name": "Shayne Longpre"
                                },
                                {
                                    "authorId": "2309005865",
                                    "name": "Wei-Yin Ko"
                                },
                                {
                                    "authorId": "2303318993",
                                    "name": "Madeline Smith"
                                },
                                {
                                    "authorId": "2284866282",
                                    "name": "Antoine Bosselut"
                                },
                                {
                                    "authorId": "2320522443",
                                    "name": "Alice Oh"
                                },
                                {
                                    "authorId": "2334310309",
                                    "name": "Andr\u00e9 F. T. Martins"
                                },
                                {
                                    "authorId": "2283849613",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2290769562",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "2268314228",
                                    "name": "Enzo Ferrante"
                                },
                                {
                                    "authorId": "2818759",
                                    "name": "Marzieh Fadaee"
                                },
                                {
                                    "authorId": "2445273",
                                    "name": "B. Ermi\u015f"
                                },
                                {
                                    "authorId": "2257040307",
                                    "name": "Sara Hooker"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.921875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Translation Artifacts and Inconsistencies",
                "tldr": "Translation processes introduce systematic artifacts that compromise benchmark integrity, including inconsistencies between source and target languages that disproportionately affect low-resource languages. These artifacts not only reflect translation quality issues but also create evaluation challenges where benchmarks measure both model performance and translation tool effectiveness. (5 sources)",
                "text": "\nBeyond cultural biases, the translation process itself creates technical artifacts that significantly impact benchmark validity. These translation artifacts manifest as structural inconsistencies between source and target languages that can fundamentally alter evaluation outcomes. Researchers note that these artifacts don't necessarily indicate flaws exclusively in translated content\u2014original datasets may contain their own artifacts that translation either changes or potentially mitigates <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nTranslation inconsistencies disproportionately affect low-resource languages, creating an uneven evaluation landscape. In the XNLI benchmark, researchers identified significant performance gaps between human-translated and machine-translated target text versions, with these gaps serving as indicators of translation errors. This finding was confirmed through manual verification for Hindi and Urdu, where reannotated human-translated test instances showed poor agreement with the original English labels they were meant to preserve <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nThe evaluation methodology compounds these issues, particularly for multilingual benchmarks. Most multilingual assessments rely on automatic translation tools to convert English benchmarks into target languages due to the volume of questions. Projects like Okapi use ChatGPT for translating benchmarks, while GPT-4 evaluations used Azure Translator <Paper corpusId=\"270737791\" paperTitle=\"(Plaza et al., 2024)\" isShortName></Paper>. This practice means that evaluations in non-English languages effectively measure both the language model's performance and the translation tool's quality simultaneously, making it difficult to isolate genuine model capabilities.\n\nThese translation artifacts create methodological challenges for effective cross-lingual evaluation. Researchers recommend several approaches to address these issues, including using original annotations in all languages, starting with non-English annotations and translating to other languages, or conducting document-level translation to minimize inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Additionally, when designing multilingual bias benchmarks, considering various cultural contexts becomes crucial for developing culturally diverse datasets <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.6298828125
                    },
                    {
                        "id": "(Agrawal et al., 2024)",
                        "snippets": [
                            "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."
                        ],
                        "paper": {
                            "corpus_id": 267413041,
                            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2282542641",
                                    "name": "Ashish Agrawal"
                                },
                                {
                                    "authorId": "2187454108",
                                    "name": "Barah Fazili"
                                },
                                {
                                    "authorId": "144859542",
                                    "name": "P. Jyothi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.5244140625
                    },
                    {
                        "id": "(Plaza et al., 2024)",
                        "snippets": [
                            "This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results."
                        ],
                        "paper": {
                            "corpus_id": 270737791,
                            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
                            "authors": [
                                {
                                    "authorId": "2308274656",
                                    "name": "Irene Plaza"
                                },
                                {
                                    "authorId": "2293315903",
                                    "name": "Nina Melero"
                                },
                                {
                                    "authorId": "2308274509",
                                    "name": "C. Pozo"
                                },
                                {
                                    "authorId": "2230852635",
                                    "name": "Javier Conde"
                                },
                                {
                                    "authorId": "2243081043",
                                    "name": "Pedro Reviriego"
                                },
                                {
                                    "authorId": "2308274000",
                                    "name": "Marina Mayor-Rocher"
                                },
                                {
                                    "authorId": "2176184513",
                                    "name": "Mar\u00eda Grandury"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.54248046875
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 268819377,
                            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                            "authors": [
                                {
                                    "authorId": "2257136845",
                                    "name": "Yuemei Xu"
                                },
                                {
                                    "authorId": "2258334185",
                                    "name": "Ling Hu"
                                },
                                {
                                    "authorId": "2294513520",
                                    "name": "Jiayi Zhao"
                                },
                                {
                                    "authorId": "2294361104",
                                    "name": "Zihan Qiu"
                                },
                                {
                                    "authorId": "2294363807",
                                    "name": "Yuqi Ye"
                                },
                                {
                                    "authorId": "2294933103",
                                    "name": "Hanwen Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Comput. Sci.",
                            "n_citations": 43
                        },
                        "score": 0.63427734375
                    },
                    {
                        "id": "(Talat1 et al., 2022)",
                        "snippets": [
                            "Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."
                        ],
                        "paper": {
                            "corpus_id": 247626152,
                            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
                            "authors": [
                                {
                                    "authorId": "2165041549",
                                    "name": "Zeerak Talat1"
                                },
                                {
                                    "authorId": "1692256",
                                    "name": "Aur\u00e9lie N\u00e9v\u00e9ol"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "1879591269",
                                    "name": "Manan Dey"
                                },
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "2993731",
                                    "name": "A. Luccioni"
                                },
                                {
                                    "authorId": "153528116",
                                    "name": "Maraim Masoud"
                                },
                                {
                                    "authorId": "49501003",
                                    "name": "Margaret Mitchell"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "1409842673",
                                    "name": "S. Sharma"
                                },
                                {
                                    "authorId": "1677386832",
                                    "name": "Arjun Subramonian"
                                },
                                {
                                    "authorId": "2112211652",
                                    "name": "Jaesung Tae"
                                },
                                {
                                    "authorId": "145814654",
                                    "name": "Samson Tan"
                                },
                                {
                                    "authorId": "70209311",
                                    "name": "D. Tunuguntla"
                                },
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                }
                            ],
                            "year": 2022,
                            "venue": "BIGSCIENCE",
                            "n_citations": 102
                        },
                        "score": 0.475341796875
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Impact on Model Performance and Evaluation Reliability",
                "tldr": "Translation artifacts and cultural biases significantly skew performance measurements in multilingual evaluations, creating an illusion of language capability gaps that may actually reflect translation quality issues. These distortions particularly disadvantage low-resource languages and models trained primarily on non-English data, leading to unreliable assessments of true multilingual capabilities. (7 sources)",
                "text": "\nThe translation artifacts and cultural biases in multilingual benchmarks substantially impact model performance measurements and evaluation reliability in several interconnected ways. Most fundamentally, these issues create artificial performance disparities that may not reflect true language capabilities. Research shows that using English prompts to recall facts from multilingual models typically yields significantly better and more consistent performance than using non-English prompts, suggesting that evaluation results are skewed by translation quality rather than model competence <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>.\n\nThis distortion is particularly pronounced for low-resource languages. Analysis of the XNLI benchmark demonstrates that translation inconsistencies disproportionately affect languages with fewer resources, creating an uneven evaluation landscape that disadvantages certain languages systematically <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. This finding is supported by manual reannotation efforts in Hindi and Urdu, which revealed poor agreement between human-translated test instances and their original English labels, indicating fundamental translation problems rather than model capability issues <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nThe evaluation methodology itself compounds these reliability problems. When benchmarks in languages other than English are created through automatic translation tools, the evaluation results inadvertently measure both the language model's performance and the translation tool's quality simultaneously <Paper corpusId=\"270737791\" paperTitle=\"(Plaza et al., 2024)\" isShortName></Paper>. This dual measurement makes it nearly impossible to isolate genuine model capabilities from translation artifacts, creating significant uncertainty about actual multilingual performance.\n\nCultural knowledge requirements further undermine evaluation reliability. Many benchmarks contain questions that require specific cultural knowledge, such as those in MMLU that reference Anglo-American facts <Paper corpusId=\"276106987\" paperTitle=\"(Magnini et al., 2025)\" isShortName></Paper> <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. Models trained primarily on non-English data would naturally struggle with these culturally specific questions, leading to artificially lowered performance scores that reflect cultural knowledge gaps rather than language understanding deficiencies <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.\n\nEven bias mitigation strategies demonstrate uneven effectiveness across languages. Research indicates that reasoning approaches like Chain-of-Thought may align well with Western fairness norms while potentially reinforcing cultural specificity in non-English languages <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This creates an imbalance where bias mitigation efforts developed primarily for English-speaking cultures leave non-Western biases more deeply embedded, further complicating reliable cross-cultural performance assessment <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThese combined factors\u2014translation artifacts, cultural knowledge requirements, and inconsistent bias mitigation\u2014significantly reduce the reliability of multilingual benchmarks as accurate performance indicators. The result is an evaluation landscape where performance gaps between languages may reflect translation quality and cultural alignment more than actual model capabilities <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Keleg et al., 2023)",
                        "snippets": [
                            "While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks",
                            ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models",
                            ".The quality of the template might degrade after automatically translating it from English",
                            "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias",
                            ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."
                        ],
                        "paper": {
                            "corpus_id": 259108559,
                            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "1725417862",
                                    "name": "Amr Keleg"
                                },
                                {
                                    "authorId": "1745226",
                                    "name": "Walid Magdy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 11
                        },
                        "score": 0.53173828125
                    },
                    {
                        "id": "(Agrawal et al., 2024)",
                        "snippets": [
                            "We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."
                        ],
                        "paper": {
                            "corpus_id": 267413041,
                            "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning",
                            "authors": [
                                {
                                    "authorId": "2282542641",
                                    "name": "Ashish Agrawal"
                                },
                                {
                                    "authorId": "2187454108",
                                    "name": "Barah Fazili"
                                },
                                {
                                    "authorId": "144859542",
                                    "name": "P. Jyothi"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
                            "n_citations": 3
                        },
                        "score": 0.5244140625
                    },
                    {
                        "id": "(Plaza et al., 2024)",
                        "snippets": [
                            "This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results."
                        ],
                        "paper": {
                            "corpus_id": 270737791,
                            "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?",
                            "authors": [
                                {
                                    "authorId": "2308274656",
                                    "name": "Irene Plaza"
                                },
                                {
                                    "authorId": "2293315903",
                                    "name": "Nina Melero"
                                },
                                {
                                    "authorId": "2308274509",
                                    "name": "C. Pozo"
                                },
                                {
                                    "authorId": "2230852635",
                                    "name": "Javier Conde"
                                },
                                {
                                    "authorId": "2243081043",
                                    "name": "Pedro Reviriego"
                                },
                                {
                                    "authorId": "2308274000",
                                    "name": "Marina Mayor-Rocher"
                                },
                                {
                                    "authorId": "2176184513",
                                    "name": "Mar\u00eda Grandury"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 8
                        },
                        "score": 0.54248046875
                    },
                    {
                        "id": "(Magnini et al., 2025)",
                        "snippets": [
                            "\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results."
                        ],
                        "paper": {
                            "corpus_id": 276106987,
                            "title": "Evalita-LLM: Benchmarking Large Language Models on Italian",
                            "authors": [
                                {
                                    "authorId": "2338997440",
                                    "name": "Bernardo Magnini"
                                },
                                {
                                    "authorId": "36024018",
                                    "name": "Roberto Zanoli"
                                },
                                {
                                    "authorId": "2343747257",
                                    "name": "Michele Resta"
                                },
                                {
                                    "authorId": "2343745825",
                                    "name": "Martin Cimmino"
                                },
                                {
                                    "authorId": "2343739878",
                                    "name": "Paolo Albano"
                                },
                                {
                                    "authorId": "2277456934",
                                    "name": "Marco Madeddu"
                                },
                                {
                                    "authorId": "2337691090",
                                    "name": "Viviana Patti"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.5693359375
                    },
                    {
                        "id": "(Hendrycks et al., 2020)",
                        "snippets": [
                            "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."
                        ],
                        "paper": {
                            "corpus_id": 221516475,
                            "title": "Measuring Massive Multitask Language Understanding",
                            "authors": [
                                {
                                    "authorId": "3422872",
                                    "name": "Dan Hendrycks"
                                },
                                {
                                    "authorId": "90909974",
                                    "name": "Collin Burns"
                                },
                                {
                                    "authorId": "104444594",
                                    "name": "Steven Basart"
                                },
                                {
                                    "authorId": "1380103052",
                                    "name": "Andy Zou"
                                },
                                {
                                    "authorId": "16787428",
                                    "name": "Mantas Mazeika"
                                },
                                {
                                    "authorId": "143711382",
                                    "name": "D. Song"
                                },
                                {
                                    "authorId": "5164568",
                                    "name": "J. Steinhardt"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 4568
                        },
                        "score": 0
                    },
                    {
                        "id": "(Singh et al., 2024)",
                        "snippets": [
                            "Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges."
                        ],
                        "paper": {
                            "corpus_id": 274464561,
                            "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation",
                            "authors": [
                                {
                                    "authorId": "2283844788",
                                    "name": "Shivalika Singh"
                                },
                                {
                                    "authorId": "1910588458",
                                    "name": "Angelika Romanou"
                                },
                                {
                                    "authorId": "2080941785",
                                    "name": "Cl\u00e9mentine Fourrier"
                                },
                                {
                                    "authorId": "2518906",
                                    "name": "David Ifeoluwa Adelani"
                                },
                                {
                                    "authorId": "119076558",
                                    "name": "Jian Gang Ngui"
                                },
                                {
                                    "authorId": "1403849795",
                                    "name": "Daniel Vila-Suero"
                                },
                                {
                                    "authorId": "1596821065",
                                    "name": "Peerat Limkonchotiwat"
                                },
                                {
                                    "authorId": "1396188646",
                                    "name": "Kelly Marchisio"
                                },
                                {
                                    "authorId": "2140097897",
                                    "name": "Wei Qi Leong"
                                },
                                {
                                    "authorId": "2239100505",
                                    "name": "Yosephine Susanto"
                                },
                                {
                                    "authorId": "2352918897",
                                    "name": "Raymond Ng"
                                },
                                {
                                    "authorId": "2283848744",
                                    "name": "Shayne Longpre"
                                },
                                {
                                    "authorId": "2309005865",
                                    "name": "Wei-Yin Ko"
                                },
                                {
                                    "authorId": "2303318993",
                                    "name": "Madeline Smith"
                                },
                                {
                                    "authorId": "2284866282",
                                    "name": "Antoine Bosselut"
                                },
                                {
                                    "authorId": "2320522443",
                                    "name": "Alice Oh"
                                },
                                {
                                    "authorId": "2334310309",
                                    "name": "Andr\u00e9 F. T. Martins"
                                },
                                {
                                    "authorId": "2283849613",
                                    "name": "Leshem Choshen"
                                },
                                {
                                    "authorId": "2290769562",
                                    "name": "Daphne Ippolito"
                                },
                                {
                                    "authorId": "2268314228",
                                    "name": "Enzo Ferrante"
                                },
                                {
                                    "authorId": "2818759",
                                    "name": "Marzieh Fadaee"
                                },
                                {
                                    "authorId": "2445273",
                                    "name": "B. Ermi\u015f"
                                },
                                {
                                    "authorId": "2257040307",
                                    "name": "Sara Hooker"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 34
                        },
                        "score": 0.921875
                    },
                    {
                        "id": "(Liu et al., 2025)",
                        "snippets": [
                            "English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems."
                        ],
                        "paper": {
                            "corpus_id": 276580130,
                            "title": "Assessing Large Language Models in Agentic Multilingual National Bias",
                            "authors": [
                                {
                                    "authorId": "4289746",
                                    "name": "Qianying Liu"
                                },
                                {
                                    "authorId": "2347161903",
                                    "name": "Katrina Qiyao Wang"
                                },
                                {
                                    "authorId": "2342687540",
                                    "name": "Fei Cheng"
                                },
                                {
                                    "authorId": "1795664",
                                    "name": "S. Kurohashi"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.50634765625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Proposed Solutions and Recommendations",
                "tldr": "Researchers propose several approaches to address translation biases in multilingual benchmarks, including using original non-English content, implementing careful documentation, and employing local or regional resources rather than translations. (7 sources)",
                "text": "\n## Improving Benchmark Design\n- **Develop consistent multilingual test sets** through one of three approaches: (1) using original annotations in all languages, (2) starting with non-English annotations and translating to other languages, or (3) when translating from English, doing so at the document level to minimize inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n \n- **Create benchmarks from regional resources** rather than translating English content. The Include benchmark demonstrates this approach by using local exam sources that naturally contain questions about local history, culture, and regional knowledge instead of translating culturally biased benchmarks <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\n- **Engage professional and community annotators** to evaluate cultural biases in benchmarks. Global-MMLU improved quality by having annotators label samples as culturally-sensitive or culturally-agnostic, helping verify translations and evaluate cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\n## Addressing Cultural Representation\n- **Consider various cultural contexts** when designing multilingual bias benchmarks to develop culturally diverse datasets <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Expand targets of bias beyond gender** and address cultural differences that exist between languages when developing multilingual bias evaluation frameworks <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Implement cultural prompting** as a control strategy to increase cultural alignment for different countries and territories, which has been shown to improve cultural alignment in model outputs for 71-81% of countries in later GPT models <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.\n\n## Mitigating Translation Artifacts\n- **Automatically flag and remove culturally specific examples** from datasets, though this approach has limitations in capturing all instances of cultural over-specificity (like references to sports teams or region-specific foods) <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>.\n\n- **Address representational bias** in fact-based benchmarks by ensuring balanced sampling rather than randomly selecting facts that might favor Western cultures <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>.\n\n- **Increase transparency through documentation** of the translation process and potential biases to better understand limitations of multilingual benchmarks <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Conduct ongoing evaluation** of cultural bias in generative AI outputs to continuously improve multilingual performance assessment <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Artetxe et al., 2020)",
                        "snippets": [
                            "Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."
                        ],
                        "paper": {
                            "corpus_id": 215548041,
                            "title": "Translation Artifacts in Cross-lingual Transfer Learning",
                            "authors": [
                                {
                                    "authorId": "2347956",
                                    "name": "Mikel Artetxe"
                                },
                                {
                                    "authorId": "2064469779",
                                    "name": "Gorka Labaka"
                                },
                                {
                                    "authorId": "1733049",
                                    "name": "Eneko Agirre"
                                }
                            ],
                            "year": 2020,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 120
                        },
                        "score": 0.6298828125
                    },
                    {
                        "id": "(Barth et al., 2025)",
                        "snippets": [
                            "The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures",
                            ".Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."
                        ],
                        "paper": {
                            "corpus_id": 276421738,
                            "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges",
                            "authors": [
                                {
                                    "authorId": "2325726142",
                                    "name": "Fabio Barth"
                                },
                                {
                                    "authorId": "2302558975",
                                    "name": "Georg Rehm"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.63134765625
                    },
                    {
                        "id": "(Xu et al., 2024)",
                        "snippets": [
                            "Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 268819377,
                            "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
                            "authors": [
                                {
                                    "authorId": "2257136845",
                                    "name": "Yuemei Xu"
                                },
                                {
                                    "authorId": "2258334185",
                                    "name": "Ling Hu"
                                },
                                {
                                    "authorId": "2294513520",
                                    "name": "Jiayi Zhao"
                                },
                                {
                                    "authorId": "2294361104",
                                    "name": "Zihan Qiu"
                                },
                                {
                                    "authorId": "2294363807",
                                    "name": "Yuqi Ye"
                                },
                                {
                                    "authorId": "2294933103",
                                    "name": "Hanwen Gu"
                                }
                            ],
                            "year": 2024,
                            "venue": "Frontiers Comput. Sci.",
                            "n_citations": 43
                        },
                        "score": 0.63427734375
                    },
                    {
                        "id": "(Talat1 et al., 2022)",
                        "snippets": [
                            "Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."
                        ],
                        "paper": {
                            "corpus_id": 247626152,
                            "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
                            "authors": [
                                {
                                    "authorId": "2165041549",
                                    "name": "Zeerak Talat1"
                                },
                                {
                                    "authorId": "1692256",
                                    "name": "Aur\u00e9lie N\u00e9v\u00e9ol"
                                },
                                {
                                    "authorId": "103476203",
                                    "name": "Stella Biderman"
                                },
                                {
                                    "authorId": "2029314697",
                                    "name": "Miruna Clinciu"
                                },
                                {
                                    "authorId": "1879591269",
                                    "name": "Manan Dey"
                                },
                                {
                                    "authorId": "29909347",
                                    "name": "S. Longpre"
                                },
                                {
                                    "authorId": "2993731",
                                    "name": "A. Luccioni"
                                },
                                {
                                    "authorId": "153528116",
                                    "name": "Maraim Masoud"
                                },
                                {
                                    "authorId": "49501003",
                                    "name": "Margaret Mitchell"
                                },
                                {
                                    "authorId": "9215251",
                                    "name": "Dragomir R. Radev"
                                },
                                {
                                    "authorId": "1409842673",
                                    "name": "S. Sharma"
                                },
                                {
                                    "authorId": "1677386832",
                                    "name": "Arjun Subramonian"
                                },
                                {
                                    "authorId": "2112211652",
                                    "name": "Jaesung Tae"
                                },
                                {
                                    "authorId": "145814654",
                                    "name": "Samson Tan"
                                },
                                {
                                    "authorId": "70209311",
                                    "name": "D. Tunuguntla"
                                },
                                {
                                    "authorId": "1986356851",
                                    "name": "Oskar van der Wal"
                                }
                            ],
                            "year": 2022,
                            "venue": "BIGSCIENCE",
                            "n_citations": 102
                        },
                        "score": 0.475341796875
                    },
                    {
                        "id": "(Tao et al., 2023)",
                        "snippets": [
                            "Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."
                        ],
                        "paper": {
                            "corpus_id": 265445838,
                            "title": "Cultural bias and cultural alignment of large language models",
                            "authors": [
                                {
                                    "authorId": "2105711243",
                                    "name": "Yan Tao"
                                },
                                {
                                    "authorId": "2269470454",
                                    "name": "Olga Viberg"
                                },
                                {
                                    "authorId": "2255317189",
                                    "name": "Ryan S. Baker"
                                },
                                {
                                    "authorId": "2246853606",
                                    "name": "Ren\u00e9 F. Kizilcec"
                                }
                            ],
                            "year": 2023,
                            "venue": "PNAS Nexus",
                            "n_citations": 87
                        },
                        "score": 0
                    },
                    {
                        "id": "(Hershcovich et al., 2022)",
                        "snippets": [
                            "When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process."
                        ],
                        "paper": {
                            "corpus_id": 247594499,
                            "title": "Challenges and Strategies in Cross-Cultural NLP",
                            "authors": [
                                {
                                    "authorId": "2064295987",
                                    "name": "Daniel Hershcovich"
                                },
                                {
                                    "authorId": "37922370",
                                    "name": "Stella Frank"
                                },
                                {
                                    "authorId": "49568895",
                                    "name": "Heather Lent"
                                },
                                {
                                    "authorId": "3295381",
                                    "name": "Miryam de Lhoneux"
                                },
                                {
                                    "authorId": "30671790",
                                    "name": "Mostafa Abdou"
                                },
                                {
                                    "authorId": "6547490",
                                    "name": "Stephanie Brandl"
                                },
                                {
                                    "authorId": "83574123",
                                    "name": "Emanuele Bugliarello"
                                },
                                {
                                    "authorId": "2093582149",
                                    "name": "Laura Cabello Piqueras"
                                },
                                {
                                    "authorId": "2125376289",
                                    "name": "Ilias Chalkidis"
                                },
                                {
                                    "authorId": "1717462692",
                                    "name": "Ruixiang Cui"
                                },
                                {
                                    "authorId": "50110151",
                                    "name": "Constanza Fierro"
                                },
                                {
                                    "authorId": "82259306",
                                    "name": "Katerina Margatina"
                                },
                                {
                                    "authorId": "1660797358",
                                    "name": "Phillip Rust"
                                },
                                {
                                    "authorId": "1700187",
                                    "name": "Anders S\u00f8gaard"
                                }
                            ],
                            "year": 2022,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 182
                        },
                        "score": 0.5400390625
                    },
                    {
                        "id": "(Keleg et al., 2023)",
                        "snippets": [
                            "While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks",
                            ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models",
                            ".The quality of the template might degrade after automatically translating it from English",
                            "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias",
                            ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."
                        ],
                        "paper": {
                            "corpus_id": 259108559,
                            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
                            "authors": [
                                {
                                    "authorId": "1725417862",
                                    "name": "Amr Keleg"
                                },
                                {
                                    "authorId": "1745226",
                                    "name": "Walid Magdy"
                                }
                            ],
                            "year": 2023,
                            "venue": "Annual Meeting of the Association for Computational Linguistics",
                            "n_citations": 11
                        },
                        "score": 0.53173828125
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.104268
    }
}