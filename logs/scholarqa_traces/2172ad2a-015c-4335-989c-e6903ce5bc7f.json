{
    "query": "What recent research compares Retrieval-Augmented Generation (RAG) with fine-tuning for improving generative AI models?",
    "user_id": "lib_user",
    "task_id": "2172ad2a-015c-4335-989c-e6903ce5bc7f",
    "timestamp": "2025-06-23T21:31:30.765597",
    "n_retrieval": 256,
    "n_retrieved": 268,
    "n_candidates": 47,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.437667,
    "decomposed_query": {
        "rewritten_query": "Comparison between Retrieval-Augmented Generation (RAG) and fine-tuning for improving generative AI models.",
        "keyword_query": "Retrieval-Augmented Generation RAG fine-tuning generative AI models comparison",
        "search_filters": {
            "year": "2022-2025",
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.010008,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.10497, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2316485338",
                    "name": "Mohammad Baqar"
                },
                {
                    "authorId": "69923048",
                    "name": "Rajat Khanda"
                }
            ],
            "abstract": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "corpus_id": 276408784,
            "sentences": [
                {
                    "corpus_id": "276408784",
                    "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
                    "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
                    "score": 0.6521914294777227,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.986328125
                }
            ],
            "relevance_judgement": 0.986328125,
            "relevance_judgment_input_expanded": "# Title: Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA\n# Venue: arXiv.org\n# Authors: Mohammad Baqar, Rajat Khanda\n## Abstract\nRecent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.\n",
            "reference_string": "[276408784 | Baqar et al. | 2025 | Citations: 1]"
        },
        {
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 44,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.04528, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2186740325",
                    "name": "Aleksander Ficek"
                },
                {
                    "authorId": "2266881428",
                    "name": "Jiaqi Zeng"
                },
                {
                    "authorId": "2787022",
                    "name": "Oleksii Kuchaiev"
                }
            ],
            "abstract": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.",
            "corpus_id": 271039066,
            "sentences": [
                {
                    "corpus_id": "271039066",
                    "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
                    "text": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
                    "score": 0.5438752896203215,
                    "section_title": "Conclusion",
                    "char_start_offset": 13407,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 161
                        },
                        {
                            "start": 162,
                            "end": 321
                        },
                        {
                            "start": 322,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 551
                        },
                        {
                            "start": 552,
                            "end": 669
                        },
                        {
                            "start": 670,
                            "end": 843
                        },
                        {
                            "start": 844,
                            "end": 968
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97802734375
                }
            ],
            "relevance_judgement": 0.97802734375,
            "relevance_judgment_input_expanded": "# Title: GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Aleksander Ficek, Jiaqi Zeng, Oleksii Kuchaiev\n## Abstract\nParameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT. Additionally, our study indicates that 8B parameter models strike an optimal balance between cost and performance and P-tuning lags behind other PEFT techniques. We further provide a comparative analysis of between applying PEFT to Instruction-tuned RETRO model and base RETRO model. This work presents the first comprehensive comparison of various PEFT methods integrated with RAG, applied to both GPT and RETRO models, highlighting their relative performance.\n## Conclusion\nThis study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
            "reference_string": "[271039066 | Ficek et al. | 2024 | Citations: 1]"
        },
        {
            "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
            "venue": "International Conference on Learning Representations",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 11,
            "influential_citation_count": 2,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13509, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2261354998",
                    "name": "Xinze Li"
                },
                {
                    "authorId": "2124028252",
                    "name": "Senkun Mei"
                },
                {
                    "authorId": "49047064",
                    "name": "Zhenghao Liu"
                },
                {
                    "authorId": "2277242040",
                    "name": "Yukun Yan"
                },
                {
                    "authorId": "2267033597",
                    "name": "Shuo Wang"
                },
                {
                    "authorId": "2314785970",
                    "name": "Shi Yu"
                },
                {
                    "authorId": "1633538428",
                    "name": "Zheni Zeng"
                },
                {
                    "authorId": "2327546188",
                    "name": "Hao Chen"
                },
                {
                    "authorId": "2204644192",
                    "name": "Ge Yu"
                },
                {
                    "authorId": "2290295914",
                    "name": "Zhiyuan Liu"
                },
                {
                    "authorId": "2273551430",
                    "name": "Maosong Sun"
                },
                {
                    "authorId": "2139787803",
                    "name": "Chenyan Xiong"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
            "corpus_id": 273403480,
            "sentences": [
                {
                    "corpus_id": "273403480",
                    "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
                    "text": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
                    "score": 0.5479337769593946,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.97314453125
                }
            ],
            "relevance_judgement": 0.97314453125,
            "relevance_judgment_input_expanded": "# Title: RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards\n# Venue: International Conference on Learning Representations\n# Authors: Xinze Li, Senkun Mei, Zhenghao Liu, Yukun Yan, Shuo Wang, Shi Yu, Zheni Zeng, Hao Chen, Ge Yu, Zhiyuan Liu, Maosong Sun, Chenyan Xiong\n## Abstract\nRetrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.\n",
            "reference_string": "[273403480 | Li et al. | 2024 | Citations: 11]"
        },
        {
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 106,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.03780, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2324796381",
                    "name": "Thang Nguyen"
                },
                {
                    "authorId": "2324790937",
                    "name": "Peter Chin"
                },
                {
                    "authorId": "2324792268",
                    "name": "Yu-Wing Tai"
                }
            ],
            "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "corpus_id": 273186680,
            "sentences": [
                {
                    "corpus_id": "273186680",
                    "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
                    "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
                    "score": 0.5367619902863696,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96728515625
                }
            ],
            "relevance_judgement": 0.96728515625,
            "relevance_judgment_input_expanded": "# Title: Reward-RAG: Enhancing RAG with Reward Driven Supervision\n# Venue: arXiv.org\n# Authors: Thang Nguyen, Peter Chin, Yu-Wing Tai\n## Abstract\nIn this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.\n",
            "reference_string": "[273186680 | Nguyen et al. | 2024 | Citations: 5]"
        },
        {
            "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 10,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284077584",
                    "name": "R\u00f3bert Lakatos"
                },
                {
                    "authorId": "2279978930",
                    "name": "P. Pollner"
                },
                {
                    "authorId": "2260742529",
                    "name": "Andr\u00e1s Hajdu"
                },
                {
                    "authorId": "2243157601",
                    "name": "Tam\u00e1s Jo\u00f3"
                }
            ],
            "abstract": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
            "corpus_id": 268510325,
            "sentences": [],
            "relevance_judgement": 0.9658203125,
            "relevance_judgment_input_expanded": "# Title: Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems\n# Venue: Machine Learning and Knowledge Extraction\n# Authors: R\u00f3bert Lakatos, P. Pollner, Andr\u00e1s Hajdu, Tam\u00e1s Jo\u00f3\n## Abstract\nGenerative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.\n",
            "reference_string": "[268510325 | Lakatos et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 127,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07688, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2270181751",
                    "name": "Zilun Zhang"
                },
                {
                    "authorId": "2174678931",
                    "name": "Haozhan Shen"
                },
                {
                    "authorId": "8200875",
                    "name": "Tiancheng Zhao"
                },
                {
                    "authorId": "2330774884",
                    "name": "Yuhao Wang"
                },
                {
                    "authorId": "2330612748",
                    "name": "Bin Chen"
                },
                {
                    "authorId": "2149196373",
                    "name": "Yuxiang Cai"
                },
                {
                    "authorId": "2093090552",
                    "name": "Yongheng Shang"
                },
                {
                    "authorId": "2111612160",
                    "name": "Jianwei Yin"
                }
            ],
            "abstract": "Ultra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG",
            "corpus_id": 273969615,
            "sentences": [
                {
                    "corpus_id": "273969615",
                    "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                    "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
                    "score": 0.7907323114508069,
                    "section_title": "C. Retrieval-Augmented Generation",
                    "char_start_offset": 65622,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 143
                        },
                        {
                            "start": 144,
                            "end": 309
                        },
                        {
                            "start": 310,
                            "end": 479
                        },
                        {
                            "start": 480,
                            "end": 635
                        },
                        {
                            "start": 636,
                            "end": 865
                        },
                        {
                            "start": 868,
                            "end": 1056
                        },
                        {
                            "start": 1057,
                            "end": 1239
                        },
                        {
                            "start": 1240,
                            "end": 1397
                        },
                        {
                            "start": 1398,
                            "end": 1544
                        },
                        {
                            "start": 1545,
                            "end": 1708
                        },
                        {
                            "start": 1709,
                            "end": 1938
                        },
                        {
                            "start": 1939,
                            "end": 2057
                        },
                        {
                            "start": 2058,
                            "end": 2263
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 358,
                            "end": 362,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1264,
                            "end": 1268,
                            "matchedPaperCorpusId": "254877046"
                        },
                        {
                            "start": 1424,
                            "end": 1429,
                            "matchedPaperCorpusId": "258841283"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.96533203125
                }
            ],
            "relevance_judgement": 0.96533203125,
            "relevance_judgment_input_expanded": "# Title: Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG\n# Venue: arXiv.org\n# Authors: Zilun Zhang, Haozhan Shen, Tiancheng Zhao, Yuhao Wang, Bin Chen, Yuxiang Cai, Yongheng Shang, Jianwei Yin\n## Abstract\nUltra High Resolution (UHR) remote sensing imagery (RSI) (e.g. 100,000 $\\times$ 100,000 pixels or more) poses a significant challenge for current Remote Sensing Multimodal Large Language Models (RSMLLMs). If choose to resize the UHR image to standard input image size, the extensive spatial and contextual information that UHR images contain will be neglected. Otherwise, the original size of these images often exceeds the token limits of standard RSMLLMs, making it difficult to process the entire image and capture long-range dependencies to answer the query based on the abundant visual context. In this paper, we introduce ImageRAG for RS, a training-free framework to address the complexities of analyzing UHR remote sensing imagery. By transforming UHR remote sensing image analysis task to image's long context selection task, we design an innovative image contextual retrieval mechanism based on the Retrieval-Augmented Generation (RAG) technique, denoted as ImageRAG. ImageRAG's core innovation lies in its ability to selectively retrieve and focus on the most relevant portions of the UHR image as visual contexts that pertain to a given query. Fast path and slow path are proposed in this framework to handle this task efficiently and effectively. ImageRAG allows RSMLLMs to manage extensive context and spatial information from UHR RSI, ensuring the analysis is both accurate and efficient. Codebase will be released in https://github.com/om-ai-lab/ImageRAG\n## C. Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "reference_string": "[273969615 | Zhang et al. | 2024 | Citations: 3]"
        },
        {
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 53,
            "citation_count": 91,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.08406, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "34938986",
                    "name": "M. A. D. L. Balaguer"
                },
                {
                    "authorId": "3456623",
                    "name": "Vinamra Benara"
                },
                {
                    "authorId": "2279752416",
                    "name": "Renato Luiz de Freitas Cunha"
                },
                {
                    "authorId": "2279750954",
                    "name": "Roberto de M. Estevao Filho"
                },
                {
                    "authorId": "2279749685",
                    "name": "Todd Hendry"
                },
                {
                    "authorId": "2279750514",
                    "name": "Daniel Holstein"
                },
                {
                    "authorId": "2279752770",
                    "name": "Jennifer Marsman"
                },
                {
                    "authorId": "2279750706",
                    "name": "Nick Mecklenburg"
                },
                {
                    "authorId": "145707932",
                    "name": "S. Malvar"
                },
                {
                    "authorId": "2256989583",
                    "name": "Leonardo Nunes"
                },
                {
                    "authorId": "2279548480",
                    "name": "Rafael Padilha"
                },
                {
                    "authorId": "2279750745",
                    "name": "Morris Sharp"
                },
                {
                    "authorId": "2257019569",
                    "name": "B. Silva"
                },
                {
                    "authorId": "2279667352",
                    "name": "Swati Sharma"
                },
                {
                    "authorId": "2257349985",
                    "name": "Vijay Aski"
                },
                {
                    "authorId": "2256993742",
                    "name": "Ranveer Chandra"
                }
            ],
            "abstract": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.",
            "corpus_id": 267027552,
            "sentences": [
                {
                    "corpus_id": "267027552",
                    "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
                    "text": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.",
                    "score": 0.5425869738792914,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9619140625
                }
            ],
            "relevance_judgement": 0.9619140625,
            "relevance_judgment_input_expanded": "# Title: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture\n# Venue: arXiv.org\n# Authors: M. A. D. L. Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto de M. Estevao Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick Mecklenburg, S. Malvar, Leonardo Nunes, Rafael Padilha, Morris Sharp, B. Silva, Swati Sharma, Vijay Aski, Ranveer Chandra\n## Abstract\nThere are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.\n",
            "reference_string": "[267027552 | Balaguer et al. | 2024 | Citations: 91]"
        },
        {
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "venue": "AAAI Conference on Artificial Intelligence",
            "year": 2024,
            "reference_count": 65,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.19670, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1900406",
                    "name": "Yutao Zhu"
                },
                {
                    "authorId": "2187935160",
                    "name": "Zhaoheng Huang"
                },
                {
                    "authorId": "1897235",
                    "name": "Zhicheng Dou"
                },
                {
                    "authorId": "2186578511",
                    "name": "Ji-Rong Wen"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
            "corpus_id": 270123034,
            "sentences": [
                {
                    "corpus_id": "270123034",
                    "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
                    "text": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
                    "score": 0.5990589911070195,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.958984375
                }
            ],
            "relevance_judgement": 0.958984375,
            "relevance_judgment_input_expanded": "# Title: One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models\n# Venue: AAAI Conference on Artificial Intelligence\n# Authors: Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen\n## Abstract\nRetrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.\n",
            "reference_string": "[270123034 | Zhu et al. | 2024 | Citations: 6]"
        },
        {
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 0,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.18365, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2275628230",
                    "name": "Yiteng Tu"
                },
                {
                    "authorId": "2147219374",
                    "name": "Weihang Su"
                },
                {
                    "authorId": "2290870875",
                    "name": "Yujia Zhou"
                },
                {
                    "authorId": "2260835922",
                    "name": "Yiqun Liu"
                },
                {
                    "authorId": "2256982003",
                    "name": "Qingyao Ai"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "corpus_id": 275993994,
            "sentences": [
                {
                    "corpus_id": "275993994",
                    "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                    "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
                    "score": 0.6889308377302972,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95849609375
                }
            ],
            "relevance_judgement": 0.95849609375,
            "relevance_judgment_input_expanded": "# Title: RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects\n# Venue: arXiv.org\n# Authors: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai\n## Abstract\nRetrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.\n",
            "reference_string": "[275993994 | Tu et al. | 2025 | Citations: 6]"
        },
        {
            "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2024,
            "reference_count": 11,
            "citation_count": 12,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/make6040116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/make6040116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327432431",
                    "name": "Arunabh Bora"
                },
                {
                    "authorId": "1806041",
                    "name": "H. Cuay\u00e1huitl"
                }
            ],
            "abstract": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
            "corpus_id": 273566432,
            "sentences": [],
            "relevance_judgement": 0.9541015625,
            "relevance_judgment_input_expanded": "# Title: Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications\n# Venue: Machine Learning and Knowledge Extraction\n# Authors: Arunabh Bora, H. Cuay\u00e1huitl\n## Abstract\nArtificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.\n",
            "reference_string": "[273566432 | Bora et al. | 2024 | Citations: 12]"
        },
        {
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 32,
            "citation_count": 9,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.13397, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2189018699",
                    "name": "Sefika Efeoglu"
                },
                {
                    "authorId": "2259621860",
                    "name": "Adrian Paschke"
                }
            ],
            "abstract": "Information Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks. This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.",
            "corpus_id": 269292881,
            "sentences": [
                {
                    "corpus_id": "269292881",
                    "title": "Retrieval-Augmented Generation-based Relation Extraction",
                    "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
                    "score": 0.592613626857869,
                    "section_title": "Retrieval-Augmented Generation",
                    "char_start_offset": 8393,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 136
                        },
                        {
                            "start": 136,
                            "end": 318
                        },
                        {
                            "start": 318,
                            "end": 515
                        },
                        {
                            "start": 515,
                            "end": 591
                        },
                        {
                            "start": 591,
                            "end": 768
                        },
                        {
                            "start": 768,
                            "end": 1031
                        },
                        {
                            "start": 1033,
                            "end": 1207
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 511,
                            "end": 514,
                            "matchedPaperCorpusId": "218869575"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9521484375
                }
            ],
            "relevance_judgement": 0.9521484375,
            "relevance_judgment_input_expanded": "# Title: Retrieval-Augmented Generation-based Relation Extraction\n# Venue: arXiv.org\n# Authors: Sefika Efeoglu, Adrian Paschke\n## Abstract\nInformation Extraction (IE) is a transformative process that converts unstructured text data into a structured format by employing entity and relation extraction (RE) methodologies. The identification of the relation between a pair of entities plays a crucial role within this framework. Despite the existence of various techniques for relation extraction, their efficacy heavily relies on access to labeled data and substantial computational resources. In addressing these challenges, Large Language Models (LLMs) emerge as promising solutions; however, they might return hallucinating responses due to their own training data. To overcome these limitations, Retrieved-Augmented Generation-based Relation Extraction (RAG4RE) in this work is proposed, offering a pathway to enhance the performance of relation extraction tasks. This work evaluated the effectiveness of our RAG4RE approach utilizing different LLMs. Through the utilization of established benchmarks, such as TACRED, TACREV, Re-TACRED, and SemEval RE datasets, our aim is to comprehensively evaluate the efficacy of our RAG4RE approach. In particularly, we leverage prominent LLMs including Flan T5, Llama2, and Mistral in our investigation. The results of our study demonstrate that our RAG4RE approach surpasses performance of traditional RE approaches based solely on LLMs, particularly evident in the TACRED dataset and its variations. Furthermore, our approach exhibits remarkable performance compared to previous RE methodologies across both TACRED and TACREV datasets, underscoring its efficacy and potential for advancing RE tasks in natural language processing.\n## Retrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "reference_string": "[269292881 | Efeoglu et al. | 2024 | Citations: 9]"
        },
        {
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "venue": "",
            "year": 2025,
            "reference_count": 31,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.10792, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362089035",
                    "name": "Zhan Peng Lee"
                },
                {
                    "authorId": "2362188632",
                    "name": "Andre Lin"
                },
                {
                    "authorId": "2363425126",
                    "name": "Calvin Tan"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.",
            "corpus_id": 278714952,
            "sentences": [
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
                    "score": 0.6170570174488293,
                    "section_title": "Methodology",
                    "char_start_offset": 8999,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 403
                        },
                        {
                            "start": 404,
                            "end": 632
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.95068359375
                },
                {
                    "corpus_id": "278714952",
                    "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                    "text": "In this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
                    "score": 0.6498784246360466,
                    "section_title": "Conclusion",
                    "char_start_offset": 23325,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 167
                        },
                        {
                            "start": 168,
                            "end": 364
                        },
                        {
                            "start": 367,
                            "end": 491
                        },
                        {
                            "start": 492,
                            "end": 594
                        },
                        {
                            "start": 595,
                            "end": 737
                        },
                        {
                            "start": 740,
                            "end": 906
                        },
                        {
                            "start": 907,
                            "end": 1143
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.939453125
                }
            ],
            "relevance_judgement": 0.95068359375,
            "relevance_judgment_input_expanded": "# Title: Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation\n# Venue: \n# Authors: Zhan Peng Lee, Andre Lin, Calvin Tan\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.\n## Methodology\nWe introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.\n\n## Conclusion\nIn this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
            "reference_string": "[278714952 | Lee et al. | 2025 | Citations: 0]"
        },
        {
            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
            "venue": "",
            "year": 2024,
            "reference_count": 0,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.14881, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327003851",
                    "name": "Jianfa Chen"
                },
                {
                    "authorId": "2326992786",
                    "name": "Emily Shen"
                },
                {
                    "authorId": "2297187181",
                    "name": "Trupti Bavalatti"
                },
                {
                    "authorId": "2327028660",
                    "name": "Xiaowen Lin"
                },
                {
                    "authorId": "2326986310",
                    "name": "Yongkai Wang"
                },
                {
                    "authorId": "2327158340",
                    "name": "Shuming Hu"
                },
                {
                    "authorId": "2322094813",
                    "name": "Harihar Subramanyam"
                },
                {
                    "authorId": "2149726609",
                    "name": "Ksheeraj Sai Vepuri"
                },
                {
                    "authorId": "2327303021",
                    "name": "Ming Jiang"
                },
                {
                    "authorId": "2327505613",
                    "name": "Ji Qi"
                },
                {
                    "authorId": "2287762612",
                    "name": "Li Chen"
                },
                {
                    "authorId": "2326964342",
                    "name": "Nan Jiang"
                },
                {
                    "authorId": "2287848816",
                    "name": "Ankit Jain"
                }
            ],
            "abstract": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
            "corpus_id": 273502659,
            "sentences": [
                {
                    "corpus_id": "273502659",
                    "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
                    "text": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
                    "score": 0.586287732910191,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9482421875
                }
            ],
            "relevance_judgement": 0.9482421875,
            "relevance_judgment_input_expanded": "# Title: Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation\n# Venue: \n# Authors: Jianfa Chen, Emily Shen, Trupti Bavalatti, Xiaowen Lin, Yongkai Wang, Shuming Hu, Harihar Subramanyam, Ksheeraj Sai Vepuri, Ming Jiang, Ji Qi, Li Chen, Nan Jiang, Ankit Jain\n## Abstract\nRobust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.\n",
            "reference_string": "[273502659 | Chen et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "venue": "SIGIR-AP",
            "year": 2024,
            "reference_count": 66,
            "citation_count": 37,
            "influential_citation_count": 2,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3673791.3698415",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.01432, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2165569122",
                    "name": "Heydar Soudani"
                },
                {
                    "authorId": "1713134",
                    "name": "E. Kanoulas"
                },
                {
                    "authorId": "1951737",
                    "name": "Faegheh Hasibi"
                }
            ],
            "abstract": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
            "corpus_id": 268248396,
            "sentences": [
                {
                    "corpus_id": "268248396",
                    "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
                    "text": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
                    "score": 0.5522335976859882,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.947265625
                }
            ],
            "relevance_judgement": 0.947265625,
            "relevance_judgment_input_expanded": "# Title: Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge\n# Venue: SIGIR-AP\n# Authors: Heydar Soudani, E. Kanoulas, Faegheh Hasibi\n## Abstract\nLanguage Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.\n",
            "reference_string": "[268248396 | Soudani et al. | 2024 | Citations: 37]"
        },
        {
            "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2412.11050, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309429494",
                    "name": "Yujin Wang"
                },
                {
                    "authorId": "2310858751",
                    "name": "Quanfeng Liu"
                },
                {
                    "authorId": "1934959729",
                    "name": "Jiaqi Fan"
                },
                {
                    "authorId": "9451675",
                    "name": "Jinlong Hong"
                },
                {
                    "authorId": "2309098",
                    "name": "Hongqing Chu"
                },
                {
                    "authorId": "30849412",
                    "name": "Mengjian Tian"
                },
                {
                    "authorId": "2292395467",
                    "name": "Bingzhao Gao"
                },
                {
                    "authorId": "2238391457",
                    "name": "Hong Chen"
                }
            ],
            "abstract": "Understanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, a cross-modal alignment training method for embedding models with hard and semi-hard negative mining, and a fast querying and retrieval pipeline based on K-Means clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving.",
            "corpus_id": 274776379,
            "sentences": [
                {
                    "corpus_id": "274776379",
                    "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
                    "text": "The core idea of RAG technology is to introduce an external retrieval module that dynamically retrieves relevant information during the generation process, thereby improving the performance of generative models. In visual-linguistic tasks, RAG effectively compensates for the limitations of scarce knowledge by combining external knowledge bases. The model is not only able to extract information from images but also retrieves supplementary knowledge via the retrieval mechanism, thereby improving the quality and accuracy of the generated output. Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities. \n\nBuilding upon this, Shao et al. [40] further explore the application of RAG in VQA tasks. They propose that by integration the retrieval mechanism into pre-trained VLMs, model performance in complex reasoning tasks could be significantly enhanced. Furthermore,Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance. Meanwhile, Zheng et al. [42] point out that RAG technology not only enhances the model's generative capabilities but also improves its flexibility and adaptability in handling complex multimodal tasks, especially when dealing with tasks lacking sufficient annotations or background knowledge. \n\nAs RAG technology continues to deepen its application across various tasks, the key challenge, especially in opendomain VQA tasks, lies in how to dynamically retrieve relevant background knowledge through the retrieval mechanism to improve the model's reasoning accuracy [43]. Although RAG can provide more contextual information, optimizing the retrieval and generation processes, as well as handling the vast amounts of potential external knowledge, remains a current research challenge.",
                    "score": 0.567900283373747,
                    "section_title": "A. RAG Technologies",
                    "char_start_offset": 7565,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 211
                        },
                        {
                            "start": 212,
                            "end": 346
                        },
                        {
                            "start": 347,
                            "end": 548
                        },
                        {
                            "start": 549,
                            "end": 783
                        },
                        {
                            "start": 784,
                            "end": 1106
                        },
                        {
                            "start": 1109,
                            "end": 1198
                        },
                        {
                            "start": 1199,
                            "end": 1356
                        },
                        {
                            "start": 1357,
                            "end": 1598
                        },
                        {
                            "start": 1599,
                            "end": 1898
                        },
                        {
                            "start": 1899,
                            "end": 2191
                        },
                        {
                            "start": 2194,
                            "end": 2470
                        },
                        {
                            "start": 2471,
                            "end": 2683
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94384765625
                }
            ],
            "relevance_judgement": 0.94384765625,
            "relevance_judgment_input_expanded": "# Title: RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models\n# Venue: arXiv.org\n# Authors: Yujin Wang, Quanfeng Liu, Jiaqi Fan, Jinlong Hong, Hongqing Chu, Mengjian Tian, Bingzhao Gao, Hong Chen\n## Abstract\nUnderstanding and addressing corner cases is essential for ensuring the safety and reliability of autonomous driving systems. Vision-language models (VLMs) play a crucial role in enhancing scenario comprehension, yet they face significant challenges, such as hallucination and insufficient real-world grounding, which compromise their performance in critical driving scenarios. In this work, RAC3, a novel framework designed to enhance the performance of VLMs in corner case comprehension, is proposed. RAC3 integrates a frequency-spatial fusion (FSF) image encoder, a cross-modal alignment training method for embedding models with hard and semi-hard negative mining, and a fast querying and retrieval pipeline based on K-Means clustering and hierarchical navigable small world (HNSW) indexing. A multimodal chain-of-thought (CoT) prompting strategy to guide analogical reasoning and reduce hallucinations during inference is introduced. Moreover, an update mechanism is integrated into RAC3 to ensure continual learning within the framework. Extensive experiments on the CODA and nuScenes datasets demonstrate that RAC3 significantly improves corner case comprehension across multiple downstream tasks. Compared to prior state-of-the-art methods, RAC3 achieves the highest final score of 74.46 on the CODA-LM benchmark and shows consistent performance gains when integrated with end-to-end frameworks like DriveLM. These results demonstrate the effectiveness of retrieval-augmented strategies and cross-modal alignment for safer and more interpretable autonomous driving.\n## A. RAG Technologies\nThe core idea of RAG technology is to introduce an external retrieval module that dynamically retrieves relevant information during the generation process, thereby improving the performance of generative models. In visual-linguistic tasks, RAG effectively compensates for the limitations of scarce knowledge by combining external knowledge bases. The model is not only able to extract information from images but also retrieves supplementary knowledge via the retrieval mechanism, thereby improving the quality and accuracy of the generated output. Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities. \n\nBuilding upon this, Shao et al. [40] further explore the application of RAG in VQA tasks. They propose that by integration the retrieval mechanism into pre-trained VLMs, model performance in complex reasoning tasks could be significantly enhanced. Furthermore,Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance. Meanwhile, Zheng et al. [42] point out that RAG technology not only enhances the model's generative capabilities but also improves its flexibility and adaptability in handling complex multimodal tasks, especially when dealing with tasks lacking sufficient annotations or background knowledge. \n\nAs RAG technology continues to deepen its application across various tasks, the key challenge, especially in opendomain VQA tasks, lies in how to dynamically retrieve relevant background knowledge through the retrieval mechanism to improve the model's reasoning accuracy [43]. Although RAG can provide more contextual information, optimizing the retrieval and generation processes, as well as handling the vast amounts of potential external knowledge, remains a current research challenge.",
            "reference_string": "[274776379 | Wang et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "venue": "Natural Language Processing",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/D7B259BCD35586E04358DF06006E0A85/S2977042424000530a.pdf/div-class-title-maximizing-rag-efficiency-a-comparative-analysis-of-rag-methods-div.pdf",
                "status": "HYBRID",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1017/nlp.2024.53?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1017/nlp.2024.53, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2328607527",
                    "name": "Tolga \u015eakar"
                },
                {
                    "authorId": "2328604444",
                    "name": "Hakan Emekci"
                }
            ],
            "abstract": "\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "corpus_id": 273749074,
            "sentences": [
                {
                    "corpus_id": "273749074",
                    "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
                    "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
                    "score": 0.6321452844167325,
                    "section_title": "abstract",
                    "char_start_offset": 2,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: Maximizing RAG efficiency: A comparative analysis of RAG methods\n# Venue: Natural Language Processing\n# Authors: Tolga \u015eakar, Hakan Emekci\n## Abstract\n\n This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.\n",
            "reference_string": "[273749074 | Sakar et al. | 2024 | Citations: 10]"
        },
        {
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 62,
            "citation_count": 1,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.01281, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2203079037",
                    "name": "Sakhinana Sagar Srinivas"
                },
                {
                    "authorId": "2139833562",
                    "name": "Venkataramana Runkana"
                }
            ],
            "abstract": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "corpus_id": 277501853,
            "sentences": [
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "Retrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.",
                    "score": 0.590427867288926,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 364
                        },
                        {
                            "start": 365,
                            "end": 539
                        },
                        {
                            "start": 540,
                            "end": 700
                        },
                        {
                            "start": 701,
                            "end": 1059
                        },
                        {
                            "start": 1060,
                            "end": 1430
                        },
                        {
                            "start": 1431,
                            "end": 1694
                        },
                        {
                            "start": 1695,
                            "end": 1857
                        },
                        {
                            "start": 1858,
                            "end": 2041
                        },
                        {
                            "start": 2042,
                            "end": 2299
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 57,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 578,
                            "end": 599,
                            "matchedPaperCorpusId": "268510197"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.94287109375
                },
                {
                    "corpus_id": "277501853",
                    "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                    "text": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
                    "score": 0.5815907827902453,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.927734375
                }
            ],
            "relevance_judgement": 0.94287109375,
            "relevance_judgment_input_expanded": "# Title: Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding\n# Venue: arXiv.org\n# Authors: Sakhinana Sagar Srinivas, Venkataramana Runkana\n## Abstract\nWe present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.\n## Introduction\nRetrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.",
            "reference_string": "[277501853 | Srinivas et al. | 2025 | Citations: 1]"
        },
        {
            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.11929, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2341534946",
                    "name": "Peter Devine"
                }
            ],
            "abstract": "Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.",
            "corpus_id": 275788867,
            "sentences": [
                {
                    "corpus_id": "275788867",
                    "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                    "text": "Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.",
                    "score": 0.587325053927875,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9384765625
                },
                {
                    "corpus_id": "275788867",
                    "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                    "text": "Retrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
                    "score": 0.5364287834315216,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 289
                        },
                        {
                            "start": 290,
                            "end": 459
                        },
                        {
                            "start": 460,
                            "end": 611
                        },
                        {
                            "start": 614,
                            "end": 929
                        },
                        {
                            "start": 932,
                            "end": 1199
                        },
                        {
                            "start": 1200,
                            "end": 1333
                        },
                        {
                            "start": 1336,
                            "end": 1463
                        },
                        {
                            "start": 1464,
                            "end": 1640
                        },
                        {
                            "start": 1643,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 2002
                        },
                        {
                            "start": 2005,
                            "end": 2211
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 268,
                            "end": 288,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 708,
                            "end": 726,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 726,
                            "end": 743,
                            "matchedPaperCorpusId": "256459451"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.93798828125
                }
            ],
            "relevance_judgement": 0.9384765625,
            "relevance_judgment_input_expanded": "# Title: ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation\n# Venue: arXiv.org\n# Authors: Peter Devine\n## Abstract\nRetrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.\n## Introduction\nRetrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
            "reference_string": "[275788867 | Devine | 2025 | Citations: 0]"
        },
        {
            "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models",
            "venue": "International Conference on AI-ML-Systems",
            "year": 2024,
            "reference_count": 23,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2410.18344",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327775708",
                    "name": "Fengchen Liu"
                },
                {
                    "authorId": "2327510830",
                    "name": "Jordan Jung"
                },
                {
                    "authorId": "2327335509",
                    "name": "Wei Feinstein"
                },
                {
                    "authorId": "2327336236",
                    "name": "Jeff DAmbrogia"
                },
                {
                    "authorId": "2327334401",
                    "name": "Gary Jung"
                }
            ],
            "abstract": "This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.",
            "corpus_id": 273549218,
            "sentences": [],
            "relevance_judgement": 0.93505859375,
            "relevance_judgment_input_expanded": "# Title: Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models\n# Venue: International Conference on AI-ML-Systems\n# Authors: Fengchen Liu, Jordan Jung, Wei Feinstein, Jeff DAmbrogia, Gary Jung\n## Abstract\nThis paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.\n",
            "reference_string": "[273549218 | Liu et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 2,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11201, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279752649",
                    "name": "Scott Barnett"
                },
                {
                    "authorId": "2279020735",
                    "name": "Zach Brannelly"
                },
                {
                    "authorId": "2266469333",
                    "name": "Stefanus Kurniawan"
                },
                {
                    "authorId": "2307101480",
                    "name": "Sheng Wong"
                }
            ],
            "abstract": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "corpus_id": 270560495,
            "sentences": [
                {
                    "corpus_id": "270560495",
                    "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                    "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
                    "score": 0.5492588884370492,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9296875
                }
            ],
            "relevance_judgement": 0.9296875,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models\n# Venue: arXiv.org\n# Authors: Scott Barnett, Zach Brannelly, Stefanus Kurniawan, Sheng Wong\n## Abstract\nLarge Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.\n",
            "reference_string": "[270560495 | Barnett et al. | 2024 | Citations: 2]"
        },
        {
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 41,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.17137, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357698631",
                    "name": "Chanhee Park"
                },
                {
                    "authorId": "2101318920",
                    "name": "Hyeonseok Moon"
                },
                {
                    "authorId": "2115195904",
                    "name": "Chanjun Park"
                },
                {
                    "authorId": "83056580",
                    "name": "Heu-Jeoung Lim"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.",
            "corpus_id": 278033562,
            "sentences": [
                {
                    "corpus_id": "278033562",
                    "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
                    "text": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.",
                    "score": 0.5618744986981116,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92626953125
                }
            ],
            "relevance_judgement": 0.92626953125,
            "relevance_judgment_input_expanded": "# Title: MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Chanhee Park, Hyeonseok Moon, Chanjun Park, Heu-Jeoung Lim\n## Abstract\nRetrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.\n",
            "reference_string": "[278033562 | Park et al. | 2025 | Citations: 1]"
        },
        {
            "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.09699, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2325887387",
                    "name": "Xinxi Chen"
                },
                {
                    "authorId": "2284477218",
                    "name": "Li Wang"
                },
                {
                    "authorId": "2325928099",
                    "name": "Wei Wu"
                },
                {
                    "authorId": "2312876232",
                    "name": "Qizhi Tang"
                },
                {
                    "authorId": "2325900000",
                    "name": "Yiyao Liu"
                }
            ],
            "abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
            "corpus_id": 273346023,
            "sentences": [
                {
                    "corpus_id": "273346023",
                    "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
                    "text": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
                    "score": 0.5381896053819161,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.923828125
                }
            ],
            "relevance_judgement": 0.923828125,
            "relevance_judgment_input_expanded": "# Title: Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG\n# Venue: arXiv.org\n# Authors: Xinxi Chen, Li Wang, Wei Wu, Qizhi Tang, Yiyao Liu\n## Abstract\nHallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.\n",
            "reference_string": "[273346023 | Chen et al. | 2024 | Citations: 6]"
        },
        {
            "title": "ARAGOG: Advanced RAG Output Grading",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.01037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2294361167",
                    "name": "Matouvs Eibich"
                },
                {
                    "authorId": "2294361283",
                    "name": "Shivay Nagpal"
                },
                {
                    "authorId": "2294362877",
                    "name": "Alexander Fred-Ojala"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.",
            "corpus_id": 268819923,
            "sentences": [
                {
                    "corpus_id": "268819923",
                    "title": "ARAGOG: Advanced RAG Output Grading",
                    "text": "Large Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
                    "score": 0.5707729010328457,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 180
                        },
                        {
                            "start": 180,
                            "end": 268
                        },
                        {
                            "start": 268,
                            "end": 502
                        },
                        {
                            "start": 502,
                            "end": 696
                        },
                        {
                            "start": 696,
                            "end": 754
                        },
                        {
                            "start": 756,
                            "end": 1033
                        },
                        {
                            "start": 1033,
                            "end": 1171
                        },
                        {
                            "start": 1171,
                            "end": 1328
                        },
                        {
                            "start": 1328,
                            "end": 1556
                        },
                        {
                            "start": 1558,
                            "end": 1673
                        },
                        {
                            "start": 1673,
                            "end": 1854
                        },
                        {
                            "start": 1854,
                            "end": 2066
                        },
                        {
                            "start": 2068,
                            "end": 2169
                        },
                        {
                            "start": 2169,
                            "end": 2347
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: ARAGOG: Advanced RAG Output Grading\n# Venue: arXiv.org\n# Authors: Matouvs Eibich, Shivay Nagpal, Alexander Fred-Ojala\n## Abstract\nRetrieval-Augmented Generation (RAG) is essential for integrating external knowledge into Large Language Model (LLM) outputs. While the literature on RAG is growing, it primarily focuses on systematic reviews and comparisons of new state-of-the-art (SoTA) techniques against their predecessors, with a gap in extensive experimental comparisons. This study begins to address this gap by assessing various RAG methods' impacts on retrieval precision and answer similarity. We found that Hypothetical Document Embedding (HyDE) and LLM reranking significantly enhance retrieval precision. However, Maximal Marginal Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a baseline Naive RAG system, and Multi-query approaches underperformed. Sentence Window Retrieval emerged as the most effective for retrieval precision, despite its variable performance on answer similarity. The study confirms the potential of the Document Summary Index as a competent retrieval approach. All resources related to this research are publicly accessible for further investigation through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We welcome the community to further this exploratory study in RAG systems.\n## Introduction\nLarge Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
            "reference_string": "[268819923 | Eibich et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 44,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.08356, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2256382094",
                    "name": "Kushagra Bhushan"
                },
                {
                    "authorId": "1392630568",
                    "name": "Yatin Nandwani"
                },
                {
                    "authorId": "2345003462",
                    "name": "Dinesh Khandelwal"
                },
                {
                    "authorId": "2320314900",
                    "name": "Sonam Gupta"
                },
                {
                    "authorId": "2345005348",
                    "name": "Gaurav Pandey"
                },
                {
                    "authorId": "1916865",
                    "name": "Dinesh Raghu"
                },
                {
                    "authorId": "2243011716",
                    "name": "Sachindra Joshi"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.",
            "corpus_id": 276287820,
            "sentences": [
                {
                    "corpus_id": "276287820",
                    "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
                    "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
                    "score": 0.5452360836095633,
                    "section_title": "Related Work",
                    "char_start_offset": 5971,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 232
                        },
                        {
                            "start": 233,
                            "end": 829
                        },
                        {
                            "start": 830,
                            "end": 1058
                        },
                        {
                            "start": 1059,
                            "end": 1158
                        },
                        {
                            "start": 1159,
                            "end": 1288
                        },
                        {
                            "start": 1291,
                            "end": 1434
                        },
                        {
                            "start": 1435,
                            "end": 1593
                        },
                        {
                            "start": 1594,
                            "end": 1677
                        },
                        {
                            "start": 1680,
                            "end": 1854
                        },
                        {
                            "start": 1855,
                            "end": 1980
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.92138671875
                }
            ],
            "relevance_judgement": 0.92138671875,
            "relevance_judgment_input_expanded": "# Title: Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, Sachindra Joshi\n## Abstract\nRetrieval-Augmented Generation (RAG) has emerged as a prominent method for incorporating domain knowledge into Large Language Models (LLMs). While RAG enhances response relevance by incorporating retrieved domain knowledge in the context, retrieval errors can still lead to hallucinations and incorrect answers. To recover from retriever failures, domain knowledge is injected by fine-tuning the model to generate the correct response, even in the case of retrieval errors. However, we observe that without systematic knowledge augmentation, fine-tuned LLMs may memorize new information but still fail to extract relevant domain knowledge, leading to poor performance. In this work, we present a novel framework that significantly enhances the fine-tuning process by augmenting the training data in two ways -- context augmentation and knowledge paraphrasing. In context augmentation, we create multiple training samples for a given QA pair by varying the relevance of the retrieved information, teaching the model when to ignore and when to rely on retrieved content. In knowledge paraphrasing, we fine-tune with multiple answers to the same question, enabling LLMs to better internalize specialized knowledge. To mitigate catastrophic forgetting due to fine-tuning, we add a domain-specific identifier to a question and also utilize a replay buffer containing general QA pairs. Experimental results demonstrate the efficacy of our method over existing techniques, achieving up to 10\\% relative gain in token-level recall while preserving the LLM's generalization capabilities.\n## Related Work\nRetrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "reference_string": "[276287820 | Bhushan et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 10,
            "citation_count": 20,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2402.01722, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2279813822",
                    "name": "Liang Zhang"
                },
                {
                    "authorId": "2279831793",
                    "name": "Katherine Jijo"
                },
                {
                    "authorId": "2282528163",
                    "name": "Spurthi Setty"
                },
                {
                    "authorId": "2279830841",
                    "name": "Eden Chung"
                },
                {
                    "authorId": "2282539958",
                    "name": "Fatima Javid"
                },
                {
                    "authorId": "2279830757",
                    "name": "Natan Vidra"
                },
                {
                    "authorId": "2279838243",
                    "name": "Thomas Clifford"
                }
            ],
            "abstract": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
            "corpus_id": 267412954,
            "sentences": [
                {
                    "corpus_id": "267412954",
                    "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                    "text": "To enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
                    "score": 0.5836235929319684,
                    "section_title": "Process and Work",
                    "char_start_offset": 9452,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 145
                        },
                        {
                            "start": 146,
                            "end": 285
                        },
                        {
                            "start": 286,
                            "end": 369
                        },
                        {
                            "start": 370,
                            "end": 526
                        },
                        {
                            "start": 527,
                            "end": 734
                        },
                        {
                            "start": 737,
                            "end": 820
                        },
                        {
                            "start": 821,
                            "end": 949
                        },
                        {
                            "start": 950,
                            "end": 1107
                        },
                        {
                            "start": 1108,
                            "end": 1208
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91943359375
                }
            ],
            "relevance_judgement": 0.91943359375,
            "relevance_judgment_input_expanded": "# Title: Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately\n# Venue: arXiv.org\n# Authors: Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, Thomas Clifford\n## Abstract\nLarge Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.\n## Process and Work\nTo enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
            "reference_string": "[267412954 | Zhang et al. | 2024 | Citations: 20]"
        },
        {
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 78,
            "citation_count": 61,
            "influential_citation_count": 6,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01219, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273537815",
                    "name": "Xiaohua Wang"
                },
                {
                    "authorId": "2308276345",
                    "name": "Zhenghua Wang"
                },
                {
                    "authorId": "2292070745",
                    "name": "Xuan Gao"
                },
                {
                    "authorId": "2308226671",
                    "name": "Feiran Zhang"
                },
                {
                    "authorId": "2308043953",
                    "name": "Yixin Wu"
                },
                {
                    "authorId": "2308044030",
                    "name": "Zhibo Xu"
                },
                {
                    "authorId": "2308036711",
                    "name": "Tianyuan Shi"
                },
                {
                    "authorId": "2309182278",
                    "name": "Zhengyuan Wang"
                },
                {
                    "authorId": "2309656885",
                    "name": "Shizheng Li"
                },
                {
                    "authorId": "2309176521",
                    "name": "Qi Qian"
                },
                {
                    "authorId": "2292032843",
                    "name": "Ruicheng Yin"
                },
                {
                    "authorId": "2220896023",
                    "name": "Changze Lv"
                },
                {
                    "authorId": "2257315404",
                    "name": "Xiaoqing Zheng"
                },
                {
                    "authorId": "2257129987",
                    "name": "Xuanjing Huang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.",
            "corpus_id": 270870251,
            "sentences": [
                {
                    "corpus_id": "270870251",
                    "title": "Searching for Best Practices in Retrieval-Augmented Generation",
                    "text": "The contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".",
                    "score": 0.593719397638711,
                    "section_title": "Retrieval Source",
                    "char_start_offset": 4117,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 47
                        },
                        {
                            "start": 49,
                            "end": 208
                        },
                        {
                            "start": 210,
                            "end": 465
                        },
                        {
                            "start": 465,
                            "end": 711
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9150390625
                }
            ],
            "relevance_judgement": 0.9150390625,
            "relevance_judgment_input_expanded": "# Title: Searching for Best Practices in Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang\n## Abstract\nRetrieval-augmented generation (RAG) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many RAG approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a RAG workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing RAG approaches and their potential combinations to identify optimal RAG practices. Through extensive experiments, we suggest several strategies for deploying RAG that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a \u201cretrieval as generation\u201d strategy.\n## Retrieval Source\nThe contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".",
            "reference_string": "[270870251 | Wang et al. | 2024 | Citations: 61]"
        },
        {
            "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 80,
            "citation_count": 14,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.01102, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2309172052",
                    "name": "David Rau"
                },
                {
                    "authorId": "2290801744",
                    "name": "Herv'e D'ejean"
                },
                {
                    "authorId": "2258716783",
                    "name": "Nadezhda Chirkova"
                },
                {
                    "authorId": "1630412772",
                    "name": "Thibault Formal"
                },
                {
                    "authorId": "2146514461",
                    "name": "Shuai Wang"
                },
                {
                    "authorId": "2841761",
                    "name": "Vassilina Nikoulina"
                },
                {
                    "authorId": "2207074",
                    "name": "S. Clinchant"
                }
            ],
            "abstract": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
            "corpus_id": 270870632,
            "sentences": [
                {
                    "corpus_id": "270870632",
                    "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
                    "score": 0.5861531053324742,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.91064453125
                }
            ],
            "relevance_judgement": 0.91064453125,
            "relevance_judgment_input_expanded": "# Title: BERGEN: A Benchmarking Library for Retrieval-Augmented Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: David Rau, Herv'e D'ejean, Nadezhda Chirkova, Thibault Formal, Shuai Wang, Vassilina Nikoulina, S. Clinchant\n## Abstract\nRetrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.\n",
            "reference_string": "[270870632 | Rau et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "venue": "IEEE India Conference",
            "year": 2024,
            "reference_count": 17,
            "citation_count": 14,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2406.11424",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.11424, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2356633197",
                    "name": "Gautam Balakrishnan"
                },
                {
                    "authorId": "33856997",
                    "name": "A. Purwar"
                }
            ],
            "abstract": "This paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frame-works using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.",
            "corpus_id": 270560505,
            "sentences": [
                {
                    "corpus_id": "270560505",
                    "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
                    "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
                    "score": 0.7344670845843272,
                    "section_title": "INTRODUCTION",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 217
                        },
                        {
                            "start": 217,
                            "end": 514
                        },
                        {
                            "start": 514,
                            "end": 615
                        },
                        {
                            "start": 615,
                            "end": 779
                        },
                        {
                            "start": 779,
                            "end": 1040
                        },
                        {
                            "start": 1040,
                            "end": 1414
                        },
                        {
                            "start": 1414,
                            "end": 1672
                        },
                        {
                            "start": 1672,
                            "end": 1817
                        },
                        {
                            "start": 1817,
                            "end": 2026
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90966796875
                }
            ],
            "relevance_judgement": 0.90966796875,
            "relevance_judgment_input_expanded": "# Title: Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability\n# Venue: IEEE India Conference\n# Authors: Gautam Balakrishnan, A. Purwar\n## Abstract\nThis paper presents an analysis of open-source large language models (LLMs) and their application in Retrieval-Augmented Generation (RAG) tasks, specific for enterprise-specific data sets scraped from their websites. With the increasing reliance on LLMs in natural language processing, it is crucial to evaluate their performance, accessibility, and integration within specific organizational contexts. This study examines various open-source LLMs, explores their integration into RAG frame-works using enterprise-specific data, and assesses the performance of different open-source embeddings in enhancing the retrieval and generation process. Our findings indicate that open-source LLMs, combined with effective embedding techniques, can significantly improve the accuracy and efficiency of RAG systems, offering a viable alternative to proprietary solutions for enterprises.\n## INTRODUCTION\nThe rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "reference_string": "[270560505 | Balakrishnan et al. | 2024 | Citations: 14]"
        },
        {
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "venue": "Conference on Multimedia Information Processing and Retrieval",
            "year": 2024,
            "reference_count": 14,
            "citation_count": 57,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.07220, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003089508",
                    "name": "Kunal Sawarkar"
                },
                {
                    "authorId": "2295990127",
                    "name": "Abhilasha Mangal"
                },
                {
                    "authorId": "2295990033",
                    "name": "S. R. Solanki"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "corpus_id": 269043117,
            "sentences": [
                {
                    "corpus_id": "269043117",
                    "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
                    "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
                    "score": 0.5462770767312055,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90576171875
                }
            ],
            "relevance_judgement": 0.90576171875,
            "relevance_judgment_input_expanded": "# Title: Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers\n# Venue: Conference on Multimedia Information Processing and Retrieval\n# Authors: Kunal Sawarkar, Abhilasha Mangal, S. R. Solanki\n## Abstract\nRetrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.\n",
            "reference_string": "[269043117 | Sawarkar et al. | 2024 | Citations: 57]"
        },
        {
            "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 51,
            "citation_count": 47,
            "influential_citation_count": 5,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.12457, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2170749149",
                    "name": "Chao Jin"
                },
                {
                    "authorId": "2182609505",
                    "name": "Zili Zhang"
                },
                {
                    "authorId": "2297730218",
                    "name": "Xuanlin Jiang"
                },
                {
                    "authorId": "2297499793",
                    "name": "Fangyue Liu"
                },
                {
                    "authorId": "2305828489",
                    "name": "Xin Liu"
                },
                {
                    "authorId": "2237080638",
                    "name": "Xuanzhe Liu"
                },
                {
                    "authorId": "2182349318",
                    "name": "Xin Jin"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.",
            "corpus_id": 269283058,
            "sentences": [
                {
                    "corpus_id": "269283058",
                    "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
                    "text": "Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) and machine learning, combining LLMs with the vast information accessible in external knowledge databases.Specifically, RAG is employed to enhance the generative models' ability to produce more accurate, relevant, and contextually rich responses by dynamically retrieving information from a corpus during the generation process.This hybrid approach combines the strengths of two major strands: the deep contextual understanding of LLMs and the precision of knowledge database retrieval.Recent work [1,8,22,27,37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.The RAG framework has since been applied across various tasks, including question answering [39], content creation [24], and even code generation [33,43], showcasing its versatility and promise.As shown in Figure 1, RAG operates on a two-step workflow: retrieval and generation, integrating offline preparation with real-time processing for enhanced performance.Initially, in its offline phase, RAG transforms the external knowledge sources, such as documents, into high-dimensional vectors using advanced embedding models.RAG then indexes these vectors into a specialized vector database designed for efficient retrieval.Upon receiving a user request, RAG first accesses this vector database to conduct a vector similarity search, retrieving the documents that best match the request based on their semantic content.Following this, RAG combines the content of these retrieved documents with the original user request, creating an augmented request.This augmented request is then provided to an LLM, which leverages the combined information to generate a response that is more informed and contextually rich.\n\nIn an RAG workflow, the retrieval step is mainly performed on CPUs, while the generation step is executed on GPUs.From a system perspective, the end-to-end performance of RAG is affected by both the retrieval or generation steps.The retrieval time is mainly determined by the vector database's scale, and the generation time is decided by the model size and the sequence length.Our subsequent characterization will identify RAG's performance bottleneck and highlight potential areas for optimization.",
                    "score": 0.7066959519371433,
                    "section_title": "Background",
                    "char_start_offset": 6804,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 230
                        },
                        {
                            "start": 230,
                            "end": 452
                        },
                        {
                            "start": 452,
                            "end": 610
                        },
                        {
                            "start": 610,
                            "end": 778
                        },
                        {
                            "start": 778,
                            "end": 972
                        },
                        {
                            "start": 972,
                            "end": 1140
                        },
                        {
                            "start": 1140,
                            "end": 1301
                        },
                        {
                            "start": 1301,
                            "end": 1400
                        },
                        {
                            "start": 1400,
                            "end": 1595
                        },
                        {
                            "start": 1595,
                            "end": 1727
                        },
                        {
                            "start": 1727,
                            "end": 1886
                        },
                        {
                            "start": 1888,
                            "end": 2002
                        },
                        {
                            "start": 2002,
                            "end": 2117
                        },
                        {
                            "start": 2117,
                            "end": 2266
                        },
                        {
                            "start": 2266,
                            "end": 2388
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 625,
                            "end": 627,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 630,
                            "end": 633,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 928,
                            "end": 931,
                            "matchedPaperCorpusId": "247255943"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9052734375
                }
            ],
            "relevance_judgement": 0.9052734375,
            "relevance_judgment_input_expanded": "# Title: RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation\n# Venue: arXiv.org\n# Authors: Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, Xin Jin\n## Abstract\nRetrieval-Augmented Generation (RAG) has shown significant improvements in various natural language processing tasks by integrating the strengths of large language models (LLMs) and external knowledge databases. However, RAG introduces long sequence generation and leads to high computation and memory costs. We propose RAGCache, a novel multilevel dynamic caching system tailored for RAG. Our analysis benchmarks current RAG systems, pinpointing the performance bottleneck (i.e., long sequence due to knowledge injection) and optimization opportunities (i.e., caching knowledge's intermediate states). Based on these insights, we design RAGCache, which organizes the intermediate states of retrieved knowledge in a knowledge tree and caches them in the GPU and host memory hierarchy. RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency. We implement RAGCache and evaluate it on vLLM, a state-of-the-art LLM inference system and Faiss, a state-of-the-art vector database. The experimental results show that RAGCache reduces the time to first token (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to vLLM integrated with Faiss.\n## Background\nRetrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) and machine learning, combining LLMs with the vast information accessible in external knowledge databases.Specifically, RAG is employed to enhance the generative models' ability to produce more accurate, relevant, and contextually rich responses by dynamically retrieving information from a corpus during the generation process.This hybrid approach combines the strengths of two major strands: the deep contextual understanding of LLMs and the precision of knowledge database retrieval.Recent work [1,8,22,27,37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.The RAG framework has since been applied across various tasks, including question answering [39], content creation [24], and even code generation [33,43], showcasing its versatility and promise.As shown in Figure 1, RAG operates on a two-step workflow: retrieval and generation, integrating offline preparation with real-time processing for enhanced performance.Initially, in its offline phase, RAG transforms the external knowledge sources, such as documents, into high-dimensional vectors using advanced embedding models.RAG then indexes these vectors into a specialized vector database designed for efficient retrieval.Upon receiving a user request, RAG first accesses this vector database to conduct a vector similarity search, retrieving the documents that best match the request based on their semantic content.Following this, RAG combines the content of these retrieved documents with the original user request, creating an augmented request.This augmented request is then provided to an LLM, which leverages the combined information to generate a response that is more informed and contextually rich.\n\nIn an RAG workflow, the retrieval step is mainly performed on CPUs, while the generation step is executed on GPUs.From a system perspective, the end-to-end performance of RAG is affected by both the retrieval or generation steps.The retrieval time is mainly determined by the vector database's scale, and the generation time is decided by the model size and the sequence length.Our subsequent characterization will identify RAG's performance bottleneck and highlight potential areas for optimization.",
            "reference_string": "[269283058 | Jin et al. | 2024 | Citations: 47]"
        },
        {
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 37,
            "citation_count": 11,
            "influential_citation_count": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.14979, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2187857206",
                    "name": "Yuanjie Lyu"
                },
                {
                    "authorId": "2307915655",
                    "name": "Zihan Niu"
                },
                {
                    "authorId": "2202470155",
                    "name": "Zheyong Xie"
                },
                {
                    "authorId": "2260850374",
                    "name": "Chao Zhang"
                },
                {
                    "authorId": "2277237058",
                    "name": "Tong Xu"
                },
                {
                    "authorId": "2308313519",
                    "name": "Yang Wang"
                },
                {
                    "authorId": "2265580543",
                    "name": "Enhong Chen"
                }
            ],
            "abstract": "Despite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.",
            "corpus_id": 270688152,
            "sentences": [
                {
                    "corpus_id": "270688152",
                    "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
                    "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
                    "score": 0.5597676364512983,
                    "section_title": "Related Work",
                    "char_start_offset": 7176,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 32,
                            "end": 222
                        },
                        {
                            "start": 223,
                            "end": 333
                        },
                        {
                            "start": 334,
                            "end": 549
                        },
                        {
                            "start": 550,
                            "end": 718
                        },
                        {
                            "start": 719,
                            "end": 834
                        },
                        {
                            "start": 835,
                            "end": 1165
                        },
                        {
                            "start": 1166,
                            "end": 1292
                        },
                        {
                            "start": 1293,
                            "end": 1387
                        },
                        {
                            "start": 1388,
                            "end": 1533
                        },
                        {
                            "start": 1536,
                            "end": 1568
                        },
                        {
                            "start": 1569,
                            "end": 1742
                        },
                        {
                            "start": 1743,
                            "end": 1830
                        },
                        {
                            "start": 1831,
                            "end": 2055
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 69,
                            "end": 88,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 334,
                            "end": 351,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 530,
                            "end": 548,
                            "matchedPaperCorpusId": "264350686"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9052734375
                }
            ],
            "relevance_judgement": 0.9052734375,
            "relevance_judgment_input_expanded": "# Title: Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, Enhong Chen\n## Abstract\nDespite the significant progress of large language models (LLMs) in various tasks, they often produce factual errors due to their limited internal knowledge. Retrieval-Augmented Generation (RAG), which enhances LLMs with external knowledge sources, offers a promising solution. However, these methods can be misled by irrelevant paragraphs in retrieved documents. Due to the inherent uncertainty in LLM generation, inputting the entire document may introduce off-topic information, causing the model to deviate from the central topic and affecting the relevance of the generated content. To address these issues, we propose the Retrieve-Plan-Generation (RPG) framework. RPG generates plan tokens to guide subsequent generation in the plan stage. In the answer stage, the model selects relevant fine-grained paragraphs based on the plan and uses them for further answer generation. This plan-answer process is repeated iteratively until completion, enhancing generation relevance by focusing on specific topics. To implement this framework efficiently, we utilize a simple but effective multi-task prompt-tuning method, enabling the existing LLMs to handle both planning and answering. We comprehensively compare RPG with baselines across 5 knowledge-intensive generation tasks, demonstrating the effectiveness of our approach.\n## Related Work\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
            "reference_string": "[270688152 | Lyu et al. | 2024 | Citations: 11]"
        },
        {
            "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "reference_count": 98,
            "citation_count": 5,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320837964",
                    "name": "Shiyue Zhang"
                },
                {
                    "authorId": "1478928280",
                    "name": "Mark Dredze"
                },
                {
                    "authorId": "2348274137",
                    "name": "AI Bloomberg"
                },
                {
                    "authorId": "2347353861",
                    "name": "Marinka Zitnik"
                },
                {
                    "authorId": "2279159644",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2289005170",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2287863492",
                    "name": "James Zou"
                },
                {
                    "authorId": "2228505567",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2320145734",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2288643686",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2285893376",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2309202279",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2279062891",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2324891509",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2281975152",
                    "name": "Joaquin Vanschoren"
                },
                {
                    "authorId": "2319671705",
                    "name": "John C. Mitchell"
                },
                {
                    "authorId": "2348273681",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2348317878",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2289604896",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2348517655",
                    "name": "Lifang He"
                },
                {
                    "authorId": "2319173963",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "2347147641",
                    "name": "Michael Backes"
                },
                {
                    "authorId": "2347575422",
                    "name": "Aaron Hurst"
                },
                {
                    "authorId": "1977806",
                    "name": "Adam Lerer"
                },
                {
                    "authorId": "1964382",
                    "name": "Adam P. Goucher"
                },
                {
                    "authorId": "2275245529",
                    "name": "Adam Perelman"
                },
                {
                    "authorId": "2261024614",
                    "name": "Aditya Ramesh"
                },
                {
                    "authorId": "2293772267",
                    "name": "Aidan Clark"
                },
                {
                    "authorId": "2348273516",
                    "name": "AJ Os-trow"
                },
                {
                    "authorId": "2275245663",
                    "name": "Akila Welihinda"
                },
                {
                    "authorId": "2347575420",
                    "name": "Alan Hayes"
                },
                {
                    "authorId": "38909097",
                    "name": "Alec Radford"
                },
                {
                    "authorId": "41016275",
                    "name": "Alon Jacovi"
                },
                {
                    "authorId": "2336876301",
                    "name": "Andrew Wang"
                },
                {
                    "authorId": "114577307",
                    "name": "Chris Alberti"
                },
                {
                    "authorId": "2218112465",
                    "name": "Connie Tao"
                },
                {
                    "authorId": "2336819290",
                    "name": "Jon Lipovetz"
                },
                {
                    "authorId": "2275180557",
                    "name": "Kate Olszewska"
                },
                {
                    "authorId": "2336820230",
                    "name": "Lukas Haas"
                },
                {
                    "authorId": "2336829596",
                    "name": "Michelle Liu"
                },
                {
                    "authorId": "2336820056",
                    "name": "Nate Keating"
                },
                {
                    "authorId": "2728564",
                    "name": "Adam Bloniarz"
                },
                {
                    "authorId": "2275186597",
                    "name": "Carl Saroufim"
                },
                {
                    "authorId": "2336816667",
                    "name": "Corey Fry"
                },
                {
                    "authorId": "2307471502",
                    "name": "Dror Marcus"
                },
                {
                    "authorId": "2771709",
                    "name": "Doron Kukliansky"
                },
                {
                    "authorId": "2348274256",
                    "name": "Gau-rav Singh Tomar"
                },
                {
                    "authorId": "2336819438",
                    "name": "James Swirhun"
                },
                {
                    "authorId": "2260046021",
                    "name": "J. Xing"
                },
                {
                    "authorId": "2336829198",
                    "name": "Lily Wang"
                },
                {
                    "authorId": "2275187445",
                    "name": "Madhu Gurumurthy"
                },
                {
                    "authorId": "2336819607",
                    "name": "Michael Aaron"
                },
                {
                    "authorId": "2311700543",
                    "name": "Moran Ambar"
                },
                {
                    "authorId": "2203789378",
                    "name": "Rachana Fellinger"
                },
                {
                    "authorId": "2337079313",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2336277273",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "35540270",
                    "name": "S. Goldshtein"
                },
                {
                    "authorId": "2348305792",
                    "name": "Dipanjan Das. 2025"
                },
                {
                    "authorId": "2204647912",
                    "name": "Neel Jain"
                },
                {
                    "authorId": "102604362",
                    "name": "Avi Schwarzschild"
                },
                {
                    "authorId": "123191916",
                    "name": "Yuxin Wen"
                },
                {
                    "authorId": "2306783462",
                    "name": "Gowthami Somepalli"
                },
                {
                    "authorId": "2237425585",
                    "name": "John Kirchenbauer"
                },
                {
                    "authorId": "143923090",
                    "name": "Ping-yeh Chiang"
                },
                {
                    "authorId": "121592562",
                    "name": "Micah Goldblum"
                },
                {
                    "authorId": "2056290221",
                    "name": "Aniruddha Saha"
                },
                {
                    "authorId": "2284863781",
                    "name": "Jonas Geiping"
                },
                {
                    "authorId": "2348276774",
                    "name": "Tom Goldstein. 2023"
                },
                {
                    "authorId": "2273548793",
                    "name": "Jiaming Ji"
                },
                {
                    "authorId": "2210950163",
                    "name": "Mickel Liu"
                },
                {
                    "authorId": "2260610683",
                    "name": "Josef Dai"
                },
                {
                    "authorId": "2190800297",
                    "name": "Xuehai Pan"
                },
                {
                    "authorId": "2347919703",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "14548852",
                    "name": "Juntao Dai"
                },
                {
                    "authorId": "2263267720",
                    "name": "Tianyi Qiu"
                },
                {
                    "authorId": "2222521695",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "152705071",
                    "name": "Borong Zhang"
                },
                {
                    "authorId": "2263298625",
                    "name": "Hantao Lou"
                },
                {
                    "authorId": "2263734134",
                    "name": "Kaile Wang"
                },
                {
                    "authorId": "2341723291",
                    "name": "Ya Duan"
                }
            ],
            "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.",
            "corpus_id": 276743362,
            "sentences": [],
            "relevance_judgement": 0.904296875,
            "relevance_judgment_input_expanded": "# Title: RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models\n# Venue: North American Chapter of the Association for Computational Linguistics\n# Authors: Shiyue Zhang, Mark Dredze, AI Bloomberg, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John C. Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Os-trow, Akila Welihinda, Alan Hayes, Alec Radford, Alon Jacovi, Andrew Wang, Chris Alberti, Connie Tao, Jon Lipovetz, Kate Olszewska, Lukas Haas, Michelle Liu, Nate Keating, Adam Bloniarz, Carl Saroufim, Corey Fry, Dror Marcus, Doron Kukliansky, Gau-rav Singh Tomar, James Swirhun, J. Xing, Lily Wang, Madhu Gurumurthy, Michael Aaron, Moran Ambar, Rachana Fellinger, Rui Wang, Zizhao Zhang, S. Goldshtein, Dipanjan Das. 2025, Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, Tom Goldstein. 2023, Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Juntao Dai, Tianyi Qiu, Bo Chen, Borong Zhang, Hantao Lou, Kaile Wang, Ya Duan\n## Abstract\nEfforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.\n",
            "reference_string": "[276743362 | Zhang et al. | 2025 | Citations: 5]"
        },
        {
            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
            "venue": "International Symposium on Chinese Spoken Language Processing",
            "year": 2024,
            "reference_count": 21,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.15569, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2312343839",
                    "name": "Yuetong Zhao"
                },
                {
                    "authorId": "2312344958",
                    "name": "Hongyu Cao"
                },
                {
                    "authorId": "2312340862",
                    "name": "Xianyu Zhao"
                },
                {
                    "authorId": "2243267608",
                    "name": "Zhijian Ou"
                }
            ],
            "abstract": "Since the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.",
            "corpus_id": 271329121,
            "sentences": [
                {
                    "corpus_id": "271329121",
                    "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
                    "text": "In recent years, with the rapid development of human-computer dialogue, a key technology in this field, generative dialogue models [1,2], has shown great potential and wide application prospects. From the early sequence-to-sequence (Seq2Seq) [3] architecture to recent innovations based on the Transformer [4] model with attention mechanisms, more advanced models are constantly emerging. However, generative dialogue models still confront significant challenges in accuracy, consistency, coherence, security, and resource efficiency. Enhancing their performance is a critical issue that demands attention. \n\nTo tackle more complex and diverse NLP tasks, the chainof-thought (CoT) method [5,6,7,8] has been proposed. Chainof thought breaks down complex reasoning tasks into multiple intermediate steps that are computed sequentially to obtain the final result. It not only improves the logical consistency of the model's responses but also enhances user interaction experiences. However, recent studies have shown that chainof-thought prompting method requires models of \u02dc100 billion parameters to fully release their reasoning ability [5], and thus will have a significant demands on computational resources. \n\nRetrieval Augmented Generation (RAG) [9] is also a promising method to improve the performance of generative dialogue models [10,11,12,13]. Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge. \n\nThis paper studies a method that combines the chain-ofthought with retrieval augmented generation for Supervised Fine-Tuning (SFT) small-scale models to optimize their performance in reasoning tasks, which is called RAFT (Retrieval Augmented Fine-Tuning) [14].",
                    "score": 0.5631880170149608,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 195
                        },
                        {
                            "start": 196,
                            "end": 388
                        },
                        {
                            "start": 389,
                            "end": 534
                        },
                        {
                            "start": 535,
                            "end": 606
                        },
                        {
                            "start": 609,
                            "end": 716
                        },
                        {
                            "start": 717,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 978
                        },
                        {
                            "start": 979,
                            "end": 1209
                        },
                        {
                            "start": 1212,
                            "end": 1351
                        },
                        {
                            "start": 1352,
                            "end": 1510
                        },
                        {
                            "start": 1511,
                            "end": 1707
                        },
                        {
                            "start": 1708,
                            "end": 1752
                        },
                        {
                            "start": 1753,
                            "end": 1955
                        },
                        {
                            "start": 1956,
                            "end": 2095
                        },
                        {
                            "start": 2098,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 131,
                            "end": 134,
                            "matchedPaperCorpusId": "208267760"
                        },
                        {
                            "start": 134,
                            "end": 136,
                            "matchedPaperCorpusId": "252907770"
                        },
                        {
                            "start": 242,
                            "end": 245,
                            "matchedPaperCorpusId": "7961699"
                        },
                        {
                            "start": 306,
                            "end": 309,
                            "matchedPaperCorpusId": "13756489"
                        },
                        {
                            "start": 688,
                            "end": 691,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 691,
                            "end": 693,
                            "matchedPaperCorpusId": "252917648"
                        },
                        {
                            "start": 695,
                            "end": 697,
                            "matchedPaperCorpusId": "252762395"
                        },
                        {
                            "start": 1136,
                            "end": 1139,
                            "matchedPaperCorpusId": "246411621"
                        },
                        {
                            "start": 1249,
                            "end": 1252,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 1337,
                            "end": 1341,
                            "matchedPaperCorpusId": "251371732"
                        },
                        {
                            "start": 1341,
                            "end": 1344,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 1344,
                            "end": 1347,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 1347,
                            "end": 1350,
                            "matchedPaperCorpusId": "207870430"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90380859375
                }
            ],
            "relevance_judgement": 0.90380859375,
            "relevance_judgment_input_expanded": "# Title: An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought\n# Venue: International Symposium on Chinese Spoken Language Processing\n# Authors: Yuetong Zhao, Hongyu Cao, Xianyu Zhao, Zhijian Ou\n## Abstract\nSince the launch of ChatGPT at the end of 2022, generative dialogue models represented by ChatGPT have quickly become widely used. As user expectations increase, enhancing the capability of generative dialogue models to solve complex problems has become a focal point of current research. This paper delves into the effectiveness of the RAFT (Retrieval Aug-mented Fine-Tuning) method in improving the performance of Generative dialogue models. RAFT combines chain-of-thought with model supervised fine-tuning (SFT) and retrieval augmented generation (RAG), which significantly enhanced the model's information extraction and logical reasoning abilities. We evaluated the RAFT method across multiple datasets and analysed its performance in various reasoning tasks, including long-form QA and short-form QA tasks, tasks in both Chinese and English, and supportive and comparison reasoning tasks. Notably, it addresses the gaps in previous research regarding long-form QA tasks and Chinese datasets. Moreover, we also evaluate the benefit of the chain-of-thought (CoT) in the RAFT method. This work offers valuable insights for studies focused on enhancing the performance of generative dialogue models.\n## Introduction\nIn recent years, with the rapid development of human-computer dialogue, a key technology in this field, generative dialogue models [1,2], has shown great potential and wide application prospects. From the early sequence-to-sequence (Seq2Seq) [3] architecture to recent innovations based on the Transformer [4] model with attention mechanisms, more advanced models are constantly emerging. However, generative dialogue models still confront significant challenges in accuracy, consistency, coherence, security, and resource efficiency. Enhancing their performance is a critical issue that demands attention. \n\nTo tackle more complex and diverse NLP tasks, the chainof-thought (CoT) method [5,6,7,8] has been proposed. Chainof thought breaks down complex reasoning tasks into multiple intermediate steps that are computed sequentially to obtain the final result. It not only improves the logical consistency of the model's responses but also enhances user interaction experiences. However, recent studies have shown that chainof-thought prompting method requires models of \u02dc100 billion parameters to fully release their reasoning ability [5], and thus will have a significant demands on computational resources. \n\nRetrieval Augmented Generation (RAG) [9] is also a promising method to improve the performance of generative dialogue models [10,11,12,13]. Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge. \n\nThis paper studies a method that combines the chain-ofthought with retrieval augmented generation for Supervised Fine-Tuning (SFT) small-scale models to optimize their performance in reasoning tasks, which is called RAFT (Retrieval Augmented Fine-Tuning) [14].",
            "reference_string": "[271329121 | Zhao et al. | 2024 | Citations: 4]"
        },
        {
            "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 42,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.13847, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2343693016",
                    "name": "Feiyuan Zhang"
                },
                {
                    "authorId": "2346899749",
                    "name": "Dezhi Zhu"
                },
                {
                    "authorId": "2346111453",
                    "name": "James Ming"
                },
                {
                    "authorId": "2297644069",
                    "name": "Yilun Jin"
                },
                {
                    "authorId": "2064208321",
                    "name": "Di Chai"
                },
                {
                    "authorId": "2145494870",
                    "name": "Liu Yang"
                },
                {
                    "authorId": "2308447281",
                    "name": "Han Tian"
                },
                {
                    "authorId": "2346645031",
                    "name": "Zhaoxin Fan"
                },
                {
                    "authorId": "2343460791",
                    "name": "Kai Chen"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) systems have shown substantial benefits in applications such as question answering and multi-turn dialogue \\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging static knowledge bases, often overlook the potential of dynamic historical information in ongoing conversations. To bridge this gap, we introduce DH-RAG, a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that utilize both long-term memory and immediate historical context in conversational responses \\citep{stafford1987conversational}. DH-RAG is structured around two principal components: a History-Learning based Query Reconstruction Module, designed to generate effective queries by synthesizing current and prior interactions, and a Dynamic History Information Updating Module, which continually refreshes historical context throughout the dialogue. The center of DH-RAG is a Dynamic Historical Information database, which is further refined by three strategies within the Query Reconstruction Module: Historical Query Clustering, Hierarchical Matching, and Chain of Thought Tracking. Experimental evaluations show that DH-RAG significantly surpasses conventional models on several benchmarks, enhancing response relevance, coherence, and dialogue quality.",
            "corpus_id": 276449952,
            "sentences": [
                {
                    "corpus_id": "276449952",
                    "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
                    "text": "Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. [22] introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval [10,36] and enhancing generation capabilities [2,17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal. Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.",
                    "score": 0.6139031918937866,
                    "section_title": "Related Work",
                    "char_start_offset": 3838,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 36
                        },
                        {
                            "start": 39,
                            "end": 236
                        },
                        {
                            "start": 237,
                            "end": 368
                        },
                        {
                            "start": 369,
                            "end": 523
                        },
                        {
                            "start": 524,
                            "end": 695
                        },
                        {
                            "start": 696,
                            "end": 905
                        },
                        {
                            "start": 906,
                            "end": 1076
                        },
                        {
                            "start": 1077,
                            "end": 1300
                        },
                        {
                            "start": 1301,
                            "end": 1568
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 237,
                            "end": 241,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 470,
                            "end": 474,
                            "matchedPaperCorpusId": "258479968"
                        },
                        {
                            "start": 516,
                            "end": 519,
                            "matchedPaperCorpusId": "252186384"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.90234375
                }
            ],
            "relevance_judgement": 0.90234375,
            "relevance_judgment_input_expanded": "# Title: DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue\n# Venue: arXiv.org\n# Authors: Feiyuan Zhang, Dezhi Zhu, James Ming, Yilun Jin, Di Chai, Liu Yang, Han Tian, Zhaoxin Fan, Kai Chen\n## Abstract\nRetrieval-Augmented Generation (RAG) systems have shown substantial benefits in applications such as question answering and multi-turn dialogue \\citep{lewis2020retrieval}. However, traditional RAG methods, while leveraging static knowledge bases, often overlook the potential of dynamic historical information in ongoing conversations. To bridge this gap, we introduce DH-RAG, a Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue. DH-RAG is inspired by human cognitive processes that utilize both long-term memory and immediate historical context in conversational responses \\citep{stafford1987conversational}. DH-RAG is structured around two principal components: a History-Learning based Query Reconstruction Module, designed to generate effective queries by synthesizing current and prior interactions, and a Dynamic History Information Updating Module, which continually refreshes historical context throughout the dialogue. The center of DH-RAG is a Dynamic Historical Information database, which is further refined by three strategies within the Query Reconstruction Module: Historical Query Clustering, Hierarchical Matching, and Chain of Thought Tracking. Experimental evaluations show that DH-RAG significantly surpasses conventional models on several benchmarks, enhancing response relevance, coherence, and dialogue quality.\n## Related Work\nRetrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. [22] introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval [10,36] and enhancing generation capabilities [2,17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal. Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.",
            "reference_string": "[276449952 | Zhang et al. | 2025 | Citations: 2]"
        },
        {
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "venue": "",
            "year": 2024,
            "reference_count": 48,
            "citation_count": 6,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2401.10733, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2280063225",
                    "name": "Ran Elgedawy"
                },
                {
                    "authorId": "2274464131",
                    "name": "Ioana Danciu"
                },
                {
                    "authorId": "1387927897",
                    "name": "Maria Mahbub"
                },
                {
                    "authorId": "2149506151",
                    "name": "Sudarshan Srinivasan"
                }
            ],
            "abstract": "Electronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.",
            "corpus_id": 267061013,
            "sentences": [
                {
                    "corpus_id": "267061013",
                    "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
                    "text": "Retrieval Augmented Generation (RAG) has emerged as a transformative technique across diverse domains, revolutionizing information processing and decision-making processes. \n\nIn a work by Datta et al. [16], the authors propose two models, MAKG (Medical Appropriateness Knowledge Graph) and RAG-GPT (Retrieval Augmented Generation -Generative Pretrained Transformer). MAKG functions as an autonomous coarse-grained medical-inappropriateness vigilance model for payers and regulators, while RAG-GPT operates as a fine-grained LLM with human-in-the-loop for assessing medical appropriateness and inappropriateness. \n\nAnother work by Lewis et al. [34] introduces RAG models that combine parametric and non-parametric memory components to enhance sequence-to-sequence (seq2seq) models. By endowing pre-trained, parametric-memory generation models with a non-parametric memory, RAG models achieve state-of-the-art results in open-domain extractive question answering and knowledge-intensive generation tasks. \n\nIn the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
                    "score": 0.5925615719454909,
                    "section_title": "Retrieval Augmented Generation (RAG) in Various Contexts",
                    "char_start_offset": 11665,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 172
                        },
                        {
                            "start": 175,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 611
                        },
                        {
                            "start": 614,
                            "end": 780
                        },
                        {
                            "start": 781,
                            "end": 1002
                        },
                        {
                            "start": 1005,
                            "end": 1196
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 201,
                            "end": 205,
                            "matchedPaperCorpusId": "266126075"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.9013671875
                }
            ],
            "relevance_judgement": 0.9013671875,
            "relevance_judgment_input_expanded": "# Title: Dynamic Q&A of Clinical Documents with Large Language Models\n# Venue: \n# Authors: Ran Elgedawy, Ioana Danciu, Maria Mahbub, Sudarshan Srinivasan\n## Abstract\nElectronic health records (EHRs) house crucial patient data in clinical notes. As these notes grow in volume and complexity, manual extraction becomes challenging. This work introduces a natural language interface using large language models (LLMs) for dynamic question-answering on clinical notes. Our chatbot, powered by Langchain and transformer-based LLMs, allows users to query in natural language, receiving relevant answers from clinical notes. Experiments, utilizing various embedding models and advanced LLMs, show Wizard Vicuna's superior accuracy, albeit with high compute demands. Model optimization, including weight quantization, improves latency by approximately 48 times. Promising results indicate potential, yet challenges such as model hallucinations and limited diverse medical case evaluations remain. Addressing these gaps is crucial for unlocking the value in clinical notes and advancing AI-driven clinical decision-making.\n## Retrieval Augmented Generation (RAG) in Various Contexts\nRetrieval Augmented Generation (RAG) has emerged as a transformative technique across diverse domains, revolutionizing information processing and decision-making processes. \n\nIn a work by Datta et al. [16], the authors propose two models, MAKG (Medical Appropriateness Knowledge Graph) and RAG-GPT (Retrieval Augmented Generation -Generative Pretrained Transformer). MAKG functions as an autonomous coarse-grained medical-inappropriateness vigilance model for payers and regulators, while RAG-GPT operates as a fine-grained LLM with human-in-the-loop for assessing medical appropriateness and inappropriateness. \n\nAnother work by Lewis et al. [34] introduces RAG models that combine parametric and non-parametric memory components to enhance sequence-to-sequence (seq2seq) models. By endowing pre-trained, parametric-memory generation models with a non-parametric memory, RAG models achieve state-of-the-art results in open-domain extractive question answering and knowledge-intensive generation tasks. \n\nIn the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
            "reference_string": "[267061013 | Elgedawy et al. | 2024 | Citations: 6]"
        },
        {
            "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
            "venue": "Applied Informatics",
            "year": 2024,
            "reference_count": 27,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12890, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326450664",
                    "name": "Ambuje Gupta"
                },
                {
                    "authorId": "2326298544",
                    "name": "Mrinal Rawat"
                },
                {
                    "authorId": "2306165476",
                    "name": "A. Stolcke"
                },
                {
                    "authorId": "2326296747",
                    "name": "Roberto Pieraccini"
                }
            ],
            "abstract": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.",
            "corpus_id": 273403870,
            "sentences": [
                {
                    "corpus_id": "273403870",
                    "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
                    "text": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.",
                    "score": 0.608204555063612,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.900390625
                }
            ],
            "relevance_judgement": 0.900390625,
            "relevance_judgment_input_expanded": "# Title: REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models\n# Venue: Applied Informatics\n# Authors: Ambuje Gupta, Mrinal Rawat, A. Stolcke, Roberto Pieraccini\n## Abstract\nRetrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.\n",
            "reference_string": "[273403870 | Gupta et al. | 2024 | Citations: 1]"
        },
        {
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.16088, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3333168",
                    "name": "Ali Anaissi"
                },
                {
                    "authorId": "3069261",
                    "name": "Ali Braytee"
                },
                {
                    "authorId": "1992906806",
                    "name": "Junaid Akram"
                }
            ],
            "abstract": "We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.",
            "corpus_id": 273501949,
            "sentences": [
                {
                    "corpus_id": "273501949",
                    "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                    "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
                    "score": 0.5647296189373872,
                    "section_title": "II. RELATED WORK",
                    "char_start_offset": 3739,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 243
                        },
                        {
                            "start": 244,
                            "end": 366
                        },
                        {
                            "start": 367,
                            "end": 530
                        },
                        {
                            "start": 531,
                            "end": 694
                        },
                        {
                            "start": 695,
                            "end": 858
                        },
                        {
                            "start": 859,
                            "end": 1025
                        },
                        {
                            "start": 1026,
                            "end": 1163
                        },
                        {
                            "start": 1166,
                            "end": 1336
                        },
                        {
                            "start": 1337,
                            "end": 1498
                        },
                        {
                            "start": 1499,
                            "end": 1690
                        },
                        {
                            "start": 1691,
                            "end": 1826
                        },
                        {
                            "start": 1827,
                            "end": 2030
                        },
                        {
                            "start": 2031,
                            "end": 2165
                        },
                        {
                            "start": 2166,
                            "end": 2358
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.8994140625
                }
            ],
            "relevance_judgement": 0.8994140625,
            "relevance_judgment_input_expanded": "# Title: Fine-Tuning LLMs for Reliable Medical Question-Answering Services\n# Venue: 2024 IEEE International Conference on Data Mining Workshops (ICDMW)\n# Authors: Ali Anaissi, Ali Braytee, Junaid Akram\n## Abstract\nWe present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.\n## II. RELATED WORK\nRetrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "reference_string": "[273501949 | Anaissi et al. | 2024 | Citations: 3]"
        },
        {
            "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
            "venue": "SIGIR-AP",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://dl.acm.org/doi/pdf/10.1145/3673791.3698435",
                "status": "BRONZE",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.20598, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2259065706",
                    "name": "Zihan Wang"
                },
                {
                    "authorId": "1380224383",
                    "name": "Xuri Ge"
                },
                {
                    "authorId": "2286309062",
                    "name": "Joemon M. Jose"
                },
                {
                    "authorId": "2327995614",
                    "name": "Haitao Yu"
                },
                {
                    "authorId": "2311314421",
                    "name": "Weizhi Ma"
                },
                {
                    "authorId": "2260895127",
                    "name": "Zhaochun Ren"
                },
                {
                    "authorId": "2294565066",
                    "name": "Xin Xin"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.",
            "corpus_id": 273654156,
            "sentences": [
                {
                    "corpus_id": "273654156",
                    "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
                    "text": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.",
                    "score": 0.6216307702437097,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89892578125
                }
            ],
            "relevance_judgement": 0.89892578125,
            "relevance_judgment_input_expanded": "# Title: R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation\n# Venue: SIGIR-AP\n# Authors: Zihan Wang, Xuri Ge, Joemon M. Jose, Haitao Yu, Weizhi Ma, Zhaochun Ren, Xin Xin\n## Abstract\nRetrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.\n",
            "reference_string": "[273654156 | Wang et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 30,
            "citation_count": 3,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2408.04645, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2315811033",
                    "name": "Sebastian Kahl"
                },
                {
                    "authorId": "2320842831",
                    "name": "Felix L\u00f6ffler"
                },
                {
                    "authorId": "2315810380",
                    "name": "Martin Maciol"
                },
                {
                    "authorId": "2315810328",
                    "name": "Fabian Ridder"
                },
                {
                    "authorId": "2315811128",
                    "name": "Marius Schmitz"
                },
                {
                    "authorId": "2315812881",
                    "name": "Jennifer Spanagel"
                },
                {
                    "authorId": "2315811156",
                    "name": "Jens Wienkamp"
                },
                {
                    "authorId": "1397379404",
                    "name": "Christopher Burgahn"
                },
                {
                    "authorId": "1913256",
                    "name": "M. Schilling"
                }
            ],
            "abstract": "This study evaluates the performance of Large Language Models (LLMs) as an Artificial Intelligence-based tutor for a university course. In particular, different advanced techniques are utilized, such as prompt engineering, Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the different models and applied techniques using common similarity metrics like BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of helpfulness and trustworthiness. Our findings indicate that RAG combined with prompt engineering significantly enhances model responses and produces better factual answers. In the context of education, RAG appears as an ideal technique as it is based on enriching the input of the model with additional information and material which usually is already present for a university course. Fine-tuning, on the other hand, can produce quite small, still strong expert models, but poses the danger of overfitting. Our study further asks how we measure performance of LLMs and how well current measurements represent correctness or relevance? We find high correlation on similarity metrics and a bias of most of these metrics towards shorter responses. Overall, our research points to both the potential and challenges of integrating LLMs in educational settings, suggesting a need for balanced training approaches and advanced evaluation frameworks.",
            "corpus_id": 271843111,
            "sentences": [
                {
                    "corpus_id": "271843111",
                    "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
                    "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
                    "score": 0.5889574887408178,
                    "section_title": "Discussion and Conclusion",
                    "char_start_offset": 42602,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 229
                        },
                        {
                            "start": 230,
                            "end": 418
                        },
                        {
                            "start": 419,
                            "end": 618
                        },
                        {
                            "start": 619,
                            "end": 791
                        },
                        {
                            "start": 792,
                            "end": 922
                        },
                        {
                            "start": 923,
                            "end": 1103
                        },
                        {
                            "start": 1104,
                            "end": 1361
                        },
                        {
                            "start": 1362,
                            "end": 1417
                        },
                        {
                            "start": 1420,
                            "end": 1482
                        },
                        {
                            "start": 1483,
                            "end": 1551
                        },
                        {
                            "start": 1552,
                            "end": 1757
                        },
                        {
                            "start": 1758,
                            "end": 1838
                        },
                        {
                            "start": 1839,
                            "end": 1914
                        },
                        {
                            "start": 1915,
                            "end": 2077
                        },
                        {
                            "start": 2078,
                            "end": 2190
                        },
                        {
                            "start": 2191,
                            "end": 2254
                        },
                        {
                            "start": 2255,
                            "end": 2388
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 595,
                            "end": 617,
                            "matchedPaperCorpusId": "233240939"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.896484375
                }
            ],
            "relevance_judgement": 0.896484375,
            "relevance_judgment_input_expanded": "# Title: Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course\n# Venue: arXiv.org\n# Authors: Sebastian Kahl, Felix L\u00f6ffler, Martin Maciol, Fabian Ridder, Marius Schmitz, Jennifer Spanagel, Jens Wienkamp, Christopher Burgahn, M. Schilling\n## Abstract\nThis study evaluates the performance of Large Language Models (LLMs) as an Artificial Intelligence-based tutor for a university course. In particular, different advanced techniques are utilized, such as prompt engineering, Retrieval-Augmented-Generation (RAG), and fine-tuning. We assessed the different models and applied techniques using common similarity metrics like BLEU-4, ROUGE, and BERTScore, complemented by a small human evaluation of helpfulness and trustworthiness. Our findings indicate that RAG combined with prompt engineering significantly enhances model responses and produces better factual answers. In the context of education, RAG appears as an ideal technique as it is based on enriching the input of the model with additional information and material which usually is already present for a university course. Fine-tuning, on the other hand, can produce quite small, still strong expert models, but poses the danger of overfitting. Our study further asks how we measure performance of LLMs and how well current measurements represent correctness or relevance? We find high correlation on similarity metrics and a bias of most of these metrics towards shorter responses. Overall, our research points to both the potential and challenges of integrating LLMs in educational settings, suggesting a need for balanced training approaches and advanced evaluation frameworks.\n## Discussion and Conclusion\nThe evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "reference_string": "[271843111 | Kahl et al. | 2024 | Citations: 3]"
        },
        {
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 22,
            "citation_count": 5,
            "influential_citation_count": 3,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.12248, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2326248349",
                    "name": "Jintao Liu"
                },
                {
                    "authorId": "2058085406",
                    "name": "Ruixue Ding"
                },
                {
                    "authorId": "2326165573",
                    "name": "Linhao Zhang"
                },
                {
                    "authorId": "2326115683",
                    "name": "Pengjun Xie"
                },
                {
                    "authorId": "2326114691",
                    "name": "Fie Huang"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.",
            "corpus_id": 273375021,
            "sentences": [
                {
                    "corpus_id": "273375021",
                    "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
                    "text": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
                    "score": 0.5519352495436166,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 381
                        },
                        {
                            "start": 382,
                            "end": 600
                        },
                        {
                            "start": 601,
                            "end": 714
                        },
                        {
                            "start": 715,
                            "end": 856
                        },
                        {
                            "start": 857,
                            "end": 956
                        },
                        {
                            "start": 957,
                            "end": 1094
                        },
                        {
                            "start": 1095,
                            "end": 1248
                        },
                        {
                            "start": 1249,
                            "end": 1428
                        },
                        {
                            "start": 1429,
                            "end": 1559
                        },
                        {
                            "start": 1562,
                            "end": 1741
                        },
                        {
                            "start": 1742,
                            "end": 1946
                        },
                        {
                            "start": 1947,
                            "end": 2027
                        },
                        {
                            "start": 2028,
                            "end": 2201
                        },
                        {
                            "start": 2202,
                            "end": 2392
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 359,
                            "end": 380,
                            "matchedPaperCorpusId": "251371732"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.896484375
                }
            ],
            "relevance_judgement": 0.896484375,
            "relevance_judgment_input_expanded": "# Title: CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity\n# Venue: arXiv.org\n# Authors: Jintao Liu, Ruixue Ding, Linhao Zhang, Pengjun Xie, Fie Huang\n## Abstract\nRetrieval-Augmented Generation (RAG) aims to enhance large language models (LLMs) to generate more accurate and reliable answers with the help of the retrieved context from external knowledge sources, thereby reducing the incidence of hallucinations. Despite the advancements, evaluating these systems remains a crucial research area due to the following issues: (1) Limited data diversity: The insufficient diversity of knowledge sources and query types constrains the applicability of RAG systems; (2) Obscure problems location: Existing evaluation methods have difficulty in locating the stage of the RAG pipeline where problems occur; (3) Unstable retrieval evaluation: These methods often fail to effectively assess retrieval performance, particularly when the chunking strategy changes. To tackle these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline, including chunking, retrieval, reranking, and generation. To effectively evaluate the first three phases, we introduce multi-granularity keywords, including coarse-grained and fine-grained keywords, to assess the retrieved context instead of relying on the annotation of golden chunks. Moreover, we release a holistic benchmark dataset tailored for diverse data scenarios covering a wide range of document formats and query types. We demonstrate the utility of the CoFE-RAG framework by conducting experiments to evaluate each stage of RAG systems. Our evaluation method provides unique insights into the effectiveness of RAG systems in handling diverse data scenarios, offering a more nuanced understanding of their capabilities and limitations.\n## Introduction\nIn recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
            "reference_string": "[273375021 | Liu et al. | 2024 | Citations: 5]"
        },
        {
            "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 34,
            "citation_count": 18,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.07590, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2303957265",
                    "name": "Songshuo Lu"
                },
                {
                    "authorId": "2325461964",
                    "name": "Hua Wang"
                },
                {
                    "authorId": "2325157287",
                    "name": "Yutian Rong"
                },
                {
                    "authorId": "2325197565",
                    "name": "Zhi Chen"
                },
                {
                    "authorId": "2304014129",
                    "name": "Yaohua Tang"
                }
            ],
            "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
            "corpus_id": 273233795,
            "sentences": [
                {
                    "corpus_id": "273233795",
                    "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
                    "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has achieved significant progress in natural language processing by integrating large language models (LLMs) with external knowledge databases. This integration enhances the ability of generative models to produce accurate, relevant, and context-rich responses. Recent studies (Borgeaud et al., 2022;Jiang et al., 2024;Trivedi et al., 2022;Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2023;Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc. However, as a relative new research topic, the current RAG systems still suffer from some drawbacks, among which low performance and long latency are the most prominent ones. Addressing these problems would effectively make RAG more applicable to latency-sensitive LLM tasks. \n\nAs illustrated in Figure 1a, the workflow of a naive RAG system comprises two steps: retrieval and generation, combining offline preparation with online processing to enhance performance. In the offline phase, RAG utilizes embedding models such as BGE (Chen et al., 2024a)) and GTE (Li et al., 2023) to convert external knowledge sources (e.g., document chunks) into high-dimensional vectors, which are then indexed into a specialized vector database. Upon receiving a user request, RAG first accesses this vector database to perform a similarity search, retrieving documents that best match the request based on semantic content. Subsequently, RAG integrates the content of these retrieved documents with the original user request to form an augmented query, which is input into the LLM to generate a more informative and contextually relevant response (Topsakal & Akinci, 2023). Researchers have proposed various methods to optimize the performance of RAG systems. Some approaches modify the attention computation mechanism to reduce computational complexity (Wang et al., 2020;Choromanski et al., 2020;Monteiro et al., 2024).",
                    "score": 0.5679316998234354,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4843,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 201
                        },
                        {
                            "start": 202,
                            "end": 319
                        },
                        {
                            "start": 320,
                            "end": 756
                        },
                        {
                            "start": 757,
                            "end": 931
                        },
                        {
                            "start": 932,
                            "end": 1032
                        },
                        {
                            "start": 1035,
                            "end": 1222
                        },
                        {
                            "start": 1223,
                            "end": 1486
                        },
                        {
                            "start": 1487,
                            "end": 1665
                        },
                        {
                            "start": 1666,
                            "end": 1915
                        },
                        {
                            "start": 1916,
                            "end": 2001
                        },
                        {
                            "start": 2002,
                            "end": 2163
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 37,
                            "end": 57,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 335,
                            "end": 358,
                            "matchedPaperCorpusId": "244954723"
                        },
                        {
                            "start": 398,
                            "end": 415,
                            "matchedPaperCorpusId": "256459451"
                        },
                        {
                            "start": 626,
                            "end": 653,
                            "matchedPaperCorpusId": "252735056"
                        },
                        {
                            "start": 1889,
                            "end": 1914,
                            "matchedPaperCorpusId": "260223847"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89599609375
                }
            ],
            "relevance_judgement": 0.89599609375,
            "relevance_judgment_input_expanded": "# Title: TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text\n# Venue: arXiv.org\n# Authors: Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, Yaohua Tang\n## Abstract\nCurrent Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.\n## RELATED WORK\nRetrieval-Augmented Generation (RAG) (Lewis et al., 2020) has achieved significant progress in natural language processing by integrating large language models (LLMs) with external knowledge databases. This integration enhances the ability of generative models to produce accurate, relevant, and context-rich responses. Recent studies (Borgeaud et al., 2022;Jiang et al., 2024;Trivedi et al., 2022;Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2023;Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc. However, as a relative new research topic, the current RAG systems still suffer from some drawbacks, among which low performance and long latency are the most prominent ones. Addressing these problems would effectively make RAG more applicable to latency-sensitive LLM tasks. \n\nAs illustrated in Figure 1a, the workflow of a naive RAG system comprises two steps: retrieval and generation, combining offline preparation with online processing to enhance performance. In the offline phase, RAG utilizes embedding models such as BGE (Chen et al., 2024a)) and GTE (Li et al., 2023) to convert external knowledge sources (e.g., document chunks) into high-dimensional vectors, which are then indexed into a specialized vector database. Upon receiving a user request, RAG first accesses this vector database to perform a similarity search, retrieving documents that best match the request based on semantic content. Subsequently, RAG integrates the content of these retrieved documents with the original user request to form an augmented query, which is input into the LLM to generate a more informative and contextually relevant response (Topsakal & Akinci, 2023). Researchers have proposed various methods to optimize the performance of RAG systems. Some approaches modify the attention computation mechanism to reduce computational complexity (Wang et al., 2020;Choromanski et al., 2020;Monteiro et al., 2024).",
            "reference_string": "[273233795 | Lu et al. | 2024 | Citations: 18]"
        },
        {
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 41,
            "citation_count": 2,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.07021, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2330357180",
                    "name": "Ziwei Liu"
                },
                {
                    "authorId": "2290883726",
                    "name": "Liangyin Zhang"
                },
                {
                    "authorId": "2342500515",
                    "name": "Qian Li"
                },
                {
                    "authorId": "2333407955",
                    "name": "Jianghua Wu"
                },
                {
                    "authorId": "2330382982",
                    "name": "Guangxu Zhu"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "corpus_id": 273962778,
            "sentences": [
                {
                    "corpus_id": "273962778",
                    "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
                    "text": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
                    "score": 0.6002065861164355,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89501953125
                }
            ],
            "relevance_judgement": 0.89501953125,
            "relevance_judgment_input_expanded": "# Title: Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation\n# Venue: arXiv.org\n# Authors: Ziwei Liu, Liangyin Zhang, Qian Li, Jianghua Wu, Guangxu Zhu\n## Abstract\nRetrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.\n",
            "reference_string": "[273962778 | Liu et al. | 2024 | Citations: 2]"
        },
        {
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "venue": "Electronics",
            "year": 2024,
            "reference_count": 29,
            "citation_count": 4,
            "influential_citation_count": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.3390/electronics13234643",
                "status": "GOLD",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/electronics13234643?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/electronics13234643, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2332684729",
                    "name": "Chengyuan Yao"
                },
                {
                    "authorId": "2275134724",
                    "name": "Satoshi Fujita"
                }
            ],
            "abstract": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
            "corpus_id": 274283400,
            "sentences": [
                {
                    "corpus_id": "274283400",
                    "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
                    "text": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
                    "score": 0.5621391142177752,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89306640625
                }
            ],
            "relevance_judgement": 0.89306640625,
            "relevance_judgment_input_expanded": "# Title: Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags\n# Venue: Electronics\n# Authors: Chengyuan Yao, Satoshi Fujita\n## Abstract\nWhile retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.\n",
            "reference_string": "[274283400 | Yao et al. | 2024 | Citations: 4]"
        },
        {
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 159,
            "citation_count": 10,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2404.09022, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2296715370",
                    "name": "Benjue Weng"
                }
            ],
            "abstract": "With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.",
            "corpus_id": 269149041,
            "sentences": [
                {
                    "corpus_id": "269149041",
                    "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
                    "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
                    "score": 0.6046512278008267,
                    "section_title": "XIII. RAG-MEMORY-FINETUNING",
                    "char_start_offset": 145958,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 116
                        },
                        {
                            "start": 116,
                            "end": 216
                        },
                        {
                            "start": 216,
                            "end": 453
                        },
                        {
                            "start": 453,
                            "end": 528
                        },
                        {
                            "start": 530,
                            "end": 781
                        },
                        {
                            "start": 783,
                            "end": 979
                        },
                        {
                            "start": 981,
                            "end": 1179
                        },
                        {
                            "start": 1181,
                            "end": 1547
                        },
                        {
                            "start": 1549,
                            "end": 1721
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1296,
                            "end": 1301,
                            "matchedPaperCorpusId": "252735056"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.892578125
                }
            ],
            "relevance_judgement": 0.892578125,
            "relevance_judgment_input_expanded": "# Title: Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies\n# Venue: arXiv.org\n# Authors: Benjue Weng\n## Abstract\nWith the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.\n## XIII. RAG-MEMORY-FINETUNING\nFor every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "reference_string": "[269149041 | Weng | 2024 | Citations: 10]"
        },
        {
            "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
            "venue": "",
            "year": 2024,
            "reference_count": 24,
            "citation_count": 0,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.13258, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "89919188",
                    "name": "Xiangci Li"
                },
                {
                    "authorId": "2284862335",
                    "name": "Jessica Ouyang"
                }
            ],
            "abstract": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
            "corpus_id": 273403839,
            "sentences": [
                {
                    "corpus_id": "273403839",
                    "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
                    "text": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
                    "score": 0.585065160751746,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.89013671875
                }
            ],
            "relevance_judgement": 0.89013671875,
            "relevance_judgment_input_expanded": "# Title: How Does Knowledge Selection Help Retrieval Augmented Generation?\n# Venue: \n# Authors: Xiangci Li, Jessica Ouyang\n## Abstract\nRetrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.\n",
            "reference_string": "[273403839 | Li et al. | 2024 | Citations: 0]"
        },
        {
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "venue": "arXiv.org",
            "year": 2024,
            "reference_count": 74,
            "citation_count": 99,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.07437, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2301195826",
                    "name": "Hao Yu"
                },
                {
                    "authorId": "2301156375",
                    "name": "Aoran Gan"
                },
                {
                    "authorId": "2263584690",
                    "name": "Kai Zhang"
                },
                {
                    "authorId": "66187823",
                    "name": "Shiwei Tong"
                },
                {
                    "authorId": "2301169021",
                    "name": "Qi Liu"
                },
                {
                    "authorId": "2301158163",
                    "name": "Zhaofeng Liu"
                }
            ],
            "abstract": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
            "corpus_id": 269758033,
            "sentences": [
                {
                    "corpus_id": "269758033",
                    "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
                    "text": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
                    "score": 0.6779528149945756,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.88916015625
                }
            ],
            "relevance_judgement": 0.88916015625,
            "relevance_judgment_input_expanded": "# Title: Evaluation of Retrieval-Augmented Generation: A Survey\n# Venue: arXiv.org\n# Authors: Hao Yu, Aoran Gan, Kai Zhang, Shiwei Tong, Qi Liu, Zhaofeng Liu\n## Abstract\nRetrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.\n",
            "reference_string": "[269758033 | Yu et al. | 2024 | Citations: 99]"
        },
        {
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "venue": "International Conference on Learning Representations",
            "year": 2023,
            "reference_count": 57,
            "citation_count": 780,
            "influential_citation_count": 120,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2310.11511, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "35584853",
                    "name": "Akari Asai"
                },
                {
                    "authorId": "7806955",
                    "name": "Zeqiu Wu"
                },
                {
                    "authorId": "1705260",
                    "name": "Yizhong Wang"
                },
                {
                    "authorId": "2707234",
                    "name": "Avirup Sil"
                },
                {
                    "authorId": "2548384",
                    "name": "Hannaneh Hajishirzi"
                }
            ],
            "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
            "corpus_id": 264288947,
            "sentences": [
                {
                    "corpus_id": "264288947",
                    "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
                    "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
                    "score": 0.6518244326388962,
                    "section_title": "RELATED WORK",
                    "char_start_offset": 4683,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 31
                        },
                        {
                            "start": 32,
                            "end": 296
                        },
                        {
                            "start": 297,
                            "end": 535
                        },
                        {
                            "start": 536,
                            "end": 777
                        },
                        {
                            "start": 778,
                            "end": 1019
                        },
                        {
                            "start": 1020,
                            "end": 1265
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 130,
                            "end": 148,
                            "matchedPaperCorpusId": "211204736"
                        },
                        {
                            "start": 148,
                            "end": 167,
                            "matchedPaperCorpusId": "218869575"
                        },
                        {
                            "start": 881,
                            "end": 902,
                            "matchedPaperCorpusId": "254877603"
                        },
                        {
                            "start": 937,
                            "end": 955,
                            "matchedPaperCorpusId": "256459776"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.888671875
                }
            ],
            "relevance_judgement": 0.888671875,
            "relevance_judgment_input_expanded": "# Title: Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n# Venue: International Conference on Learning Representations\n# Authors: Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi\n## Abstract\nDespite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.\n## RELATED WORK\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
            "reference_string": "[264288947 | Asai et al. | 2023 | Citations: 780]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "273969615",
            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
            "text": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 [96], enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation. \n\nThe effectiveness of RAG systems heavily depends on the quality and relevance of the retrieved knowledge, which directly influences the accuracy and factual grounding of generated content. To enhance retrieval efficiency and overcome the limitations of traditional methods, several advancements have been proposed, particularly for zero-shot and few-shot retrieval tasks. Techniques such as HyDE [99] and REINA [100] utilize LLMs to generate hypothetical documents, improving retrieval performance without requiring labeled data. The Rewrite-Retrieve-Read [101] framework introduces a query rewriting step, allowing the input query to be better aligned with retrieval modules. By using reinforcement learning to adapt queries, R3 enhances retrieval quality, improving performance in open-domain and multiple-choice question answering tasks. Promptagator [102] demonstrates the effectiveness of few-shot learning in dense retrieval, utilizing LLMs to generate synthetic training data from minimal examples, surpassing models trained on large-scale datasets like MS MARCO. This underscores the viability of few-shot learning and LLM-generated synthetic data in resource-constrained settings. To bridge the preference gap between retrievers and LLMs, Zixuan Ke et al. [103] introduce the BGM framework, which employs a sequence-to-sequence model to align retrieved information with LLM preferences.",
            "score": 0.7907323114508069,
            "section_title": "C. Retrieval-Augmented Generation",
            "char_start_offset": 65622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1708
                },
                {
                    "start": 1709,
                    "end": 1938
                },
                {
                    "start": 1939,
                    "end": 2057
                },
                {
                    "start": 2058,
                    "end": 2263
                }
            ],
            "ref_mentions": [
                {
                    "start": 358,
                    "end": 362,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "254877046"
                },
                {
                    "start": 1424,
                    "end": 1429,
                    "matchedPaperCorpusId": "258841283"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96533203125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging hybrid architecture designed to address the limitations of pure generative models. RAG integrates two key components: (i) a retrieval mechanism, which retrieves relevant documents or information from an external knowledge source, and (ii) a generation module, which processes this information to generate human-like text (Lewis et al. 2020). This combination allows RAG models to not only generate fluent text but also ground their outputs in real-world, up-to-date data. \n\nThe retrieval module in RAG typically leverages dense vector representations to identify relevant documents from large datasets, such as Wikipedia or proprietary databases. Once retrieved, these documents are passed to the generative module, often built using transformer-based architectures, to generate responses grounded in the retrieved knowledge. This methodology helps mitigate the hallucination problem and ensures that the generated text is more factual and contextually appropriate (Thakur et al. 2021). Over the period, RAG models have seen applications in various domains, including open-domain question answering (Karpukhin et al., 2020), conversational agents (Liu et al. 2021), and personalized recommendations. \n\nFigure 2: A basic flow of the RAG system along with its component",
            "score": 0.7879733598235182,
            "section_title": "Overview of Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 2068,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 393
                },
                {
                    "start": 394,
                    "end": 523
                },
                {
                    "start": 526,
                    "end": 698
                },
                {
                    "start": 699,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1319
                }
            ],
            "ref_mentions": [
                {
                    "start": 373,
                    "end": 392,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.865234375
        },
        {
            "corpus_id": "277104989",
            "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
            "text": "Retrieval-augmented generation (RAG) can be used for knowledge-intensive NLP tasks [14]. RAG is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from predetermined external sources. \n\nLLMs have some limitations, such as; presenting false information when they do not have the answer, presenting out-of-date or generic information, and creating a response from non-authoritative sources in train data. RAG addresses these challenges by redirecting the LLM to retrieve relevant information from authoritative, predetermined knowledge sources. This way, LLM can use the retrieved data as context for generating a response that is more relevant, accurate, and useful in various contexts. RAG applications potentially provide user transparency by revealing the sources of the retrieved data, offering insight into how the LLM generates its responses.",
            "score": 0.7834922847901545,
            "section_title": "Vector Databases and RAG as a Solution",
            "char_start_offset": 5123,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 732
                },
                {
                    "start": 733,
                    "end": 894
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52734375
        },
        {
            "corpus_id": "276949546",
            "title": "From Code Generation to Software Testing: AI Copilot With Context-Based Retrieval-Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG) combines retrieval mechanisms with generative models to enhance output accuracy and contextual relevanceLewis et al. [2020]. Originally developed for natural language processing, RAG has shown promise in software engineering for tasks like bug detectionPerera et al. [2020]. By integrating relevant code snippets or bug reports, RAG provides essential context to Large Language Models, improving the precision of generated solutions. Our approach leverages RAG to refine both programming and testing processes, ensuring AI-generated outputs are highly informed and effective.",
            "score": 0.778261574009338,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 5027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 311
                },
                {
                    "start": 312,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84814453125
        },
        {
            "corpus_id": "278367823",
            "title": "ZeroSearch: Incentivize the Search Capability of LLMs without Searching",
            "text": "Retrieval-augmented generation (RAG) enhances generation performance by integrating relevant external knowledge into the generation pipeline. Early research primarily adopted prompt-based approaches, guiding LLMs through processes such as query generation, query decomposition, and multi-turn information retrieval [44,28,43,15,33,22]. Despite their effectiveness, these methods often require intricate prompt engineering and impose substantial demands on the model's reasoning capabilities. To improve efficiency and reduce dependency on strong black-box LLMs, subsequent work has proposed supervised fine-tuning strategies for smaller LLMs. For instance, Self-RAG [1] employs a self-reflection mechanism, iteratively refining model outputs through predicted reflection tokens. RetroLLM [24] integrates retrieval and generation by enabling the model to directly generate fine-grained evidence from the corpus via constrained decoding. Recent advances also include test-time scaling techniques [25,14,47,13], notably Monte Carlo Tree Search (MCTS), which dynamically expands the search space during inference. For example, RAG-star [13] integrates retrieved information into a tree-based reasoning process, while AirRAG [5] employs MCTS to activate intrinsic reasoning capabilities and expand the solution space. Despite promising results, these approaches introduce significant computational overhead, limiting their practical applicability.",
            "score": 0.7621318306131982,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 6269,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 642
                },
                {
                    "start": 643,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 935
                },
                {
                    "start": 936,
                    "end": 1109
                },
                {
                    "start": 1110,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1442
                }
            ],
            "ref_mentions": [
                {
                    "start": 325,
                    "end": 328,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 666,
                    "end": 669,
                    "matchedPaperCorpusId": "264288947"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) [34] efficiently enhances the performance of generative language models through integrating information retrieval techniques.It addresses a critical challenge faced by standalone generative language models: the tendency to produce responses that, while plausible, may not be grounded in facts.By retrieving relevant information from external sources, RAG significantly reduces the incidence of hallucinations [23] or factually incorrect outputs, thereby improving the content's reliability and richness.[73] This fusion of retrieval and generation capabilities enables the creation of responses that are not only contextually appropriate but also informed by the most current and accurate information available, making RAG a development in the pursuit of more intelligent and versatile language models [73,64].\n\nNumerous studies of RAG systems have emerged from various perspectives since the advent of Large Language Models (LLMs) [55,45,59,42,41,69,16].The RAG system comprises two primary components: Retrieval and Generation.The retrieval component aims to extract relevant information from various external knowledge sources.It involves two main phases, indexing and searching.Indexing organizes documents to facilitate efficient retrieval, using either inverted indexes for sparse retrieval or dense vector encoding for dense retrieval [16,12,28].The searching component utilizes these indexes to fetch relevant documents on the user's query, often incorporating the optional rerankers [4,39,6,52] to refine the ranking of the retrieved documents.The generation component utilizes the retrieved content and question query to formulate coherent and contextually relevant responses with the prompting and inferencing phases.As the \"Emerging\" ability [59] of LLMs and the breakthrough in aligning human commands [42], LLMs are the best performance choices model for the generation stage.Prompting methods like Chain of Thought (CoT) [60], Tree of Thgouht [65], Rephrase and Respond (RaR) [8] guide better generation results.In the inferencing step, LLMs interpret the prompted input to generate accurate and in-depth responses that align with the query's intent and integrate the extracted information [35,9] without further finetuning, such as fully finetuning [16,1,67,68] or LoRA [21].",
            "score": 0.7467752046289905,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 162,
                    "end": 330
                },
                {
                    "start": 330,
                    "end": 540
                },
                {
                    "start": 540,
                    "end": 847
                },
                {
                    "start": 849,
                    "end": 992
                },
                {
                    "start": 992,
                    "end": 1066
                },
                {
                    "start": 1066,
                    "end": 1167
                },
                {
                    "start": 1167,
                    "end": 1219
                },
                {
                    "start": 1219,
                    "end": 1390
                },
                {
                    "start": 1390,
                    "end": 1590
                },
                {
                    "start": 1590,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1927
                },
                {
                    "start": 1927,
                    "end": 2064
                },
                {
                    "start": 2064,
                    "end": 2328
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 839,
                    "end": 843,
                    "matchedPaperCorpusId": "264426669"
                },
                {
                    "start": 973,
                    "end": 976,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "267069204",
            "title": "Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented Generation in Niche Domains, Exemplified by Korean Medicine",
            "text": "Retrieval-Augmented Generation (RAG) models combine a generative model with an information retrieval function, designed to overcome the inherent constraints of generative models.(1) They integrate the robustness of a large language model (LLM) with the relevance and up-to-dateness of external information sources, resulting in responses that are not only natural and human-like but also the latest, accurate, and contextually relevant to the query. (1)(2)(3)(4) The interaction of the two modules (retrieval and generation) enables responses that would not be achievable with either module alone, making RAG more than just the sum of its components. This approach represents a significant milestone in the field of generative models by enabling the induction of high-quality responses in less-explored domains at a low expense. (5,6) In the conventional RAG operation, the initial step involves converting input queries into vector embeddings, which are then used to retrieve relevant data from the vectorized database. Following this, the generative part of RAG utilizes the retrieved external data for producing contextually rich responses. (7) Thus, both the embedding and generative models are considered crucial factors in the performance of RAG, directly affecting the retrieval process.(8) However, in niche domains, the performance of generic LLM-based embedding models appears suboptimal compared to their effectiveness in more general fields. The lack of specialized training data in these domains results in embeddings that do not adequately capture the nuances and specificity of the domain (9), leading to less accurate and contextually relevant information retrieval. Despite the evident presence of these functional limitations, they have not been much identified through experiments, therefore the optimality of the conventional LLM-based vector embedding RAG methods for niche domains has remained in obscurity. Researchers have been aware of these shortcomings of LLMs and have explored supplementary processes such as fine-tuning to improve the performance. (8,(10)(11)(12) However, the cost of fine-tuning, especially when it involves adjusting the entire or majority of parameters in LLM, has rapidly become expensive, thereby increasing the demand for alternative solutions.",
            "score": 0.7454913251786341,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 650
                },
                {
                    "start": 651,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1453
                },
                {
                    "start": 1454,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1929
                },
                {
                    "start": 1930,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1604,
                    "end": 1607,
                    "matchedPaperCorpusId": "227277273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.80859375
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "The rapid advancements in natural language processing (NLP) have led to the development of sophisticated large language models (LLMs) that excel in tasks such as text generation, summarization, and question answering.Among these advancements, Retrieval-Augmented Generation (RAG) has emerged as a promising approach for the retrieval-based systems with generative models to produce highly accurate and contextually relevant outputs.The concept of Retrieval-Augmented Generation (RAG) was introduced by Lewis et al.In their seminar 2020 paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" [20].In their research, Lewis et al. present a method that combines retrieval-based and generative models to enhance the performance of knowledge-intensive tasks.By integrating non-parametric memory (retrieved documents) with parametric memory (the generative model's internal parameters), RAG models achieve superior accuracy and flexibility in tasks such as open-domain question answering and abstract question answering.Karpukhin et al. (2020) developed dense passage retrieval for open-domain question answering, which significantly boosts retrieval accuracy by using dense vector representations and a neural retriever [18].More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].There has been efficient ways to improve the retrieval process such as the Keyword Augmented Retrieval (KAR), which integrates keyword generation using transformer models with document metadata to identify the right context quickly and cost-effectively [23].Also, approach to handle sparse information where classical RAG using hybrid retriever fails to generate correct answers have been reported [17].More recent work by Tay et al. (2023) on the UL2 model and studies on ColBERT by Khattab and Zaharia (2020) have further pushed the boundaries of retrieval and generation synergies in RAG frameworks [19] [25].",
            "score": 0.7344670845843272,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 217,
                    "end": 514
                },
                {
                    "start": 514,
                    "end": 615
                },
                {
                    "start": 615,
                    "end": 779
                },
                {
                    "start": 779,
                    "end": 1040
                },
                {
                    "start": 1040,
                    "end": 1414
                },
                {
                    "start": 1414,
                    "end": 1672
                },
                {
                    "start": 1672,
                    "end": 1817
                },
                {
                    "start": 1817,
                    "end": 2026
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90966796875
        },
        {
            "corpus_id": "278171164",
            "title": "ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code",
            "text": "To overcome the limitations of LLMs in domain-specific tasks, retrieval-augmented generation (RAG) has emerged as a widely adopted technique in LLM-based generation frameworks. RAG enhances the accuracy and domain knowledge of LLMs by incorporating external knowledge sources into the generation process. Instead of relying solely on the internal parameters of the model, RAG enables the LLM to look up relevant information from a designated knowledge base before generating outputs. The key step of the RAG is the retrieval step, where a retriever module queries a database to fetch domain-specific documents or code snippets. The retrieved contents are then integrated into the model's prompt during generation. For instance, ChipNeMo [2] incorporates RAG by retrieving relevant textual passages to enrich prompts for Verilog code generation, supplemented with further parameter fine-tuning to improve output quality. AutoVCoder [17] enhances the retrieval process by fine-tuning the retriever itself using contrastive learning to boost relevance and diversity in the retrieved content. \n\nHowever, current RAG approaches often place significant emphasis on optimizing the retriever, which introduces additional complexity through separate data preparation and model fine-tuning. This underscores the need for a more lightweight yet effective RAG strategy, particularly in simplifying retriever construction without compromising performance.",
            "score": 0.7307359442733056,
            "section_title": "C. Retrieval-Augmented Generation in LLM for EDA",
            "char_start_offset": 12580,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1442
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83203125
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Despite significant advancements in generative models, AIGC still grapples with challenges like outdated knowledge, lack of long-tail knowledge [27], and risks of leaking private training data [28]. Retrieval-Augmented Generation (RAG) aims to mitigate these issues with its flexible data repository [29]. The retrievable knowledge acts as non-parametric memory, which is easily updatable, accommodates extensive long-tail knowledge, and can encode confidential data. Moreover, retrieval can lower generation costs. RAG can reduce the size of large models [30], support long contexts [31], and eliminate certain generation steps [32]. \n\nA typical RAG process is depicted in Fig. 1. Given an input query, the retriever identifies relevant data sources, and the retrieved information interacts with the generator to improve the generation process. There are several foundational paradigms (foundations in short) according to how the retrieved results augment the generation: they can serve as augmented input to the generator [33], [34]; they can join at the middle stage of generation as latent representations [35], [36]; they can contribute to the final generation results in the form of logits [37], [38]; they can even influence or omit certain generation steps [32], [39]. Additionally, researchers have proposed various enhancements to improve the foundational RAG process. These methods encompass specific optimizations for individual components as well as holistic enhancements aimed Fig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results and ultimately produces outcomes of various modalities. at the entire pipeline. \n\nIn addition, while the concept of RAG initially emerged in text-to-text generation [34], this technique has also found applications across various domains, including codes [40]- [42], audios [43], [44], images [45]- [47], videos [48], [49], 3D [50], [51], knowledge [52]- [54], and AI for science [55], [56].",
            "score": 0.7117752242915707,
            "section_title": "I. INTRODUCTION A. Background",
            "char_start_offset": 1995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 467
                },
                {
                    "start": 468,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 634
                },
                {
                    "start": 637,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1825
                },
                {
                    "start": 1828,
                    "end": 2136
                }
            ],
            "ref_mentions": [
                {
                    "start": 144,
                    "end": 148,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 193,
                    "end": 197,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1024,
                    "end": 1028,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1110,
                    "end": 1114,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1116,
                    "end": 1120,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1202,
                    "end": 1206,
                    "matchedPaperCorpusId": "237452184"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.873046875
        },
        {
            "corpus_id": "269283058",
            "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of natural language processing (NLP) and machine learning, combining LLMs with the vast information accessible in external knowledge databases.Specifically, RAG is employed to enhance the generative models' ability to produce more accurate, relevant, and contextually rich responses by dynamically retrieving information from a corpus during the generation process.This hybrid approach combines the strengths of two major strands: the deep contextual understanding of LLMs and the precision of knowledge database retrieval.Recent work [1,8,22,27,37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.The RAG framework has since been applied across various tasks, including question answering [39], content creation [24], and even code generation [33,43], showcasing its versatility and promise.As shown in Figure 1, RAG operates on a two-step workflow: retrieval and generation, integrating offline preparation with real-time processing for enhanced performance.Initially, in its offline phase, RAG transforms the external knowledge sources, such as documents, into high-dimensional vectors using advanced embedding models.RAG then indexes these vectors into a specialized vector database designed for efficient retrieval.Upon receiving a user request, RAG first accesses this vector database to conduct a vector similarity search, retrieving the documents that best match the request based on their semantic content.Following this, RAG combines the content of these retrieved documents with the original user request, creating an augmented request.This augmented request is then provided to an LLM, which leverages the combined information to generate a response that is more informed and contextually rich.\n\nIn an RAG workflow, the retrieval step is mainly performed on CPUs, while the generation step is executed on GPUs.From a system perspective, the end-to-end performance of RAG is affected by both the retrieval or generation steps.The retrieval time is mainly determined by the vector database's scale, and the generation time is decided by the model size and the sequence length.Our subsequent characterization will identify RAG's performance bottleneck and highlight potential areas for optimization.",
            "score": 0.7066959519371433,
            "section_title": "Background",
            "char_start_offset": 6804,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 230,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 610
                },
                {
                    "start": 610,
                    "end": 778
                },
                {
                    "start": 778,
                    "end": 972
                },
                {
                    "start": 972,
                    "end": 1140
                },
                {
                    "start": 1140,
                    "end": 1301
                },
                {
                    "start": 1301,
                    "end": 1400
                },
                {
                    "start": 1400,
                    "end": 1595
                },
                {
                    "start": 1595,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1886
                },
                {
                    "start": 1888,
                    "end": 2002
                },
                {
                    "start": 2002,
                    "end": 2117
                },
                {
                    "start": 2117,
                    "end": 2266
                },
                {
                    "start": 2266,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 625,
                    "end": 627,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 630,
                    "end": 633,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 928,
                    "end": 931,
                    "matchedPaperCorpusId": "247255943"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "278026850",
            "title": "Beyond the model: Key differentiators in large language models and multi-agent services",
            "text": "The rapid evolution of generative AI has led to a saturation point where numerous industry and open-source Large Language Models (LLMs) exhibit similar quality levels [1,2]. If LLMs are no longer the competitive edge, what drives the advantage in AI services? The actual value in generative AI lies not in the models themselves but in the ancillary components that enhance and support these models. \n\nModel Reliability can be enhanced by integrating human-generated data through human-in-the-loop approaches to reach desired outcomes. Techniques like Retrieval-Augmented Generation (RAG) help reduce AI hallucinations and lower computational costs by dynamically retrieving information, enabling AI to provide more accurate responses without frequent retraining [5].",
            "score": 0.7048278258875543,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 398
                },
                {
                    "start": 401,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 766
                }
            ],
            "ref_mentions": [
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "238222421"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.480712890625
        },
        {
            "corpus_id": "270062508",
            "title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions",
            "text": "Retrieval-Augmented Generation.We review the literature of Retrieval-Augmented Generation (RAG) in terms of (1) Naive RAG, (2) Advanced RAG, and (3) Modular RAG.For (1), Naive RAG follows a standard process including indexing, retrieval, and generation (Ma et al., 2023).However, its quality faces significant challenges such as low precision, hallucination, and redundancy during the process.For (2), Advanced RAG is further developed to overcome the shortcomings of Naive RAG.Specifically, during the indexing stage, the objective is to enhance the quality of the indexed content by optimizing data embedding (Li et al., 2023).During the retrieval stage, the focus is on identifying the appropriate context by calculating the similarity between the query and chunks, where the techniques involve fine-tuning embedding models (Xiao et al., 2023), or learning dynamic embeddings for different context (Karpukhin et al., 2020).During the generation stage, it merges the retrieved context with the query as an input into large language models (LLMs), where it addresses challenges posed by context window limits with re-ranking the most relevant content (Jiang et al., 2023b;Zhuang et al., 2023), or compressing prompts (Litman et al., 2020;Xu et al., 2023).In addition, Self-RAG (Asai et al., 2023) is proposed to identify whether retrieval is necessary, or the retrieved context is relevant, which helps language models to produce meaningful generation (Asai et al., 2023).For (3), Modular RAG diverges from the traditional Naive RAG structure by incorporating external modules to further enhance the performance, including search module (Wang et al., 2023a), memory module (Wang et al., 2022;Cheng et al., 2023b), tuning module (Lin et al., 2023), and task adapter (Cheng et al., 2023a;Dai et al., 2023).",
            "score": 0.7046048689027977,
            "section_title": "Related Work",
            "char_start_offset": 6288,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 161
                },
                {
                    "start": 161,
                    "end": 271
                },
                {
                    "start": 271,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 478
                },
                {
                    "start": 478,
                    "end": 629
                },
                {
                    "start": 629,
                    "end": 926
                },
                {
                    "start": 926,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1473
                },
                {
                    "start": 1473,
                    "end": 1805
                }
            ],
            "ref_mentions": [
                {
                    "start": 611,
                    "end": 628,
                    "matchedPaperCorpusId": "258987900"
                },
                {
                    "start": 901,
                    "end": 925,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1152,
                    "end": 1173,
                    "matchedPaperCorpusId": "263830701"
                },
                {
                    "start": 1173,
                    "end": 1193,
                    "matchedPaperCorpusId": "264406035"
                },
                {
                    "start": 1218,
                    "end": 1239,
                    "matchedPaperCorpusId": "214641123"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8330078125
        },
        {
            "corpus_id": "274166497",
            "title": "Enhancing Translation Quality: A Comparative Study of Fine-Tuning and Prompt Engineering in Dialog-Oriented Machine Translation Systems. Insights from the MULTITAN-GML Team",
            "text": "6 Further Research 6.1 Retrieval-Augmented Generation (RAG) \n\nThe database serves as a vital resource for addressing the challenges posed by rare or complex structures that may not be well-represented in translation models (Gao et al., 2024). Retrievalaugmented generation (RAG) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources. Future improvements could involve aug-",
            "score": 0.6994010246009515,
            "section_title": "Automatic Post-editing vs. Prompt Engineering",
            "char_start_offset": 9845,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 62,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 441
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76123046875
        },
        {
            "corpus_id": "272955388",
            "title": "Application of RAG Model Based on Retrieval Enhanced Generation Technique in Complex Query Processing",
            "text": "Compared with traditional generative models, RAG models not only rely on pre-trained language models, but also dynamically acquire the most relevant knowledge to the query, which gives them an obvious advantage when facing diverse and high-complexity queries. Retrieval-Augmented Generation (RAG) models are employed in various applications, including question-answering systems, dialogue systems, information retrieval, and knowledge graph construction. They are particularly effective for tasks that require the integration of extensive background information to produce accurate responses.",
            "score": 0.6972474360249229,
            "section_title": "Overview of the RAG model and its retrieval enhancement generation technique",
            "char_start_offset": 4522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 592
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.66015625
        },
        {
            "corpus_id": "277150624",
            "title": "DroidTTP: Mapping Android Applications with TTP for Cyber Threat Intelligence",
            "text": "Retrieval-Augmented Generation (RAG) [24] represents a fusion of information retrieval and language generation technologies. This approach enhances AI language models by connecting them with external knowledge sources that enable more informed and accurate responses. Unlike traditional language models that rely solely on their training data, RAG actively draws upon current information when generating responses. \n\nThe RAG workflow is illustrated in Figure 1. At its core, RAG functions through a two-stage process. The first stage involves information retrieval, where the system searches through external knowledge sources to find content relevant to the current query. This works by converting both the user's question and the available reference documents into mathematical representations called embeddings. It then scans similarly encoded documents in its knowledge base, using mathematical comparison of embeddings to identify the most semantically relevant information. \n\nThe second stage leverages an LLM's generative capabilities. Rather than relying solely on its pre-trained knowledge, the model receives both the user's query and the retrieved relevant information (context) from the vector database. This allows the LLM to craft responses that incorporate specific, factual details from the retrieved sources while maintaining natural language fluency. The result is more accurate and contextually appropriate than what could be achieved by either retrieval or generation alone. This combination of dynamic knowledge access and sophisticated language generation represents a significant leap forward in AI's ability to provide precise, contextually enriched responses.",
            "score": 0.6918100207996633,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 16615,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 414
                },
                {
                    "start": 417,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 517
                },
                {
                    "start": 518,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 814
                },
                {
                    "start": 815,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1042
                },
                {
                    "start": 1043,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1684
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.464599609375
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
            "score": 0.6907010345322081,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83740234375
        },
        {
            "corpus_id": "273798011",
            "title": "Unified Generative and Discriminative Training for Multi-modal Large Language Models",
            "text": "Early models for retrieval primarily focused on word representations [16,64,74], with minimal generative capabilities. Some recent works have endeavored to fine-tune generative pre-trained LLMs to generate discriminative embeddings, albeit at the expense of compromising the model's original generative capabilities [44,70,65,63,24,71]. GRIT [66] integrates generative and discriminative tasks in NLP and demonstrates mutual benefits between them. However, its training cost is prohibitively high compared to individual tasks. Moreover, due to its specialized attention mechanism, the model can only be trained from scratch. \n\nRetrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) [25,69], which harnesses the advanced inference capabilities of LLMs along with external knowledge, has the potential to significantly mitigate issues related to long-tail entities and reduce the occurrence of hallucina-tory responses [29,36,101,77,90,92,97]. Recently, there have also been related studies in the multimodal domain attempting to utilize retrieval augmentation [93,96]. These methods typically require an additional retrieval module (e.g., CLIP), leading to component optimization challenges where the overall model performance is affected by the performance of the retrieval model, as well as concerns regarding the compatibility between the retrieval model and the MLLMs. Furthermore, retrieval modules like CLIP struggle to handle compositional or fine-grained scenarios, posing certain challenges for retrieval.",
            "score": 0.6894820889045895,
            "section_title": "Related Work",
            "char_start_offset": 8945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 447
                },
                {
                    "start": 448,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 624
                },
                {
                    "start": 627,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1527
                }
            ],
            "ref_mentions": [
                {
                    "start": 73,
                    "end": 76,
                    "matchedPaperCorpusId": "16447573"
                },
                {
                    "start": 931,
                    "end": 935,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 935,
                    "end": 938,
                    "matchedPaperCorpusId": "261033863"
                },
                {
                    "start": 951,
                    "end": 954,
                    "matchedPaperCorpusId": "265445443"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7705078125
        },
        {
            "corpus_id": "275993994",
            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
            "text": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.",
            "score": 0.6889308377302972,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95849609375
        },
        {
            "corpus_id": "273963542",
            "title": "Leveraging Retrieval-Augmented Generation for Persian University Knowledge Retrieval",
            "text": "Retrieval-Augmented Generation (RAG) is a novel paradigm that enhances the performance of large language models by incorporating information retrieval processes into the generation mechanism. This approach aims to",
            "score": 0.6885702739945361,
            "section_title": "A. Introduction to Retrieval-Augmented Generation",
            "char_start_offset": 3196,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 192,
                    "end": 213
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.4765625
        },
        {
            "corpus_id": "269757277",
            "title": "ERAGent: Enhancing Retrieval-Augmented Language Models with Improved Accuracy, Efficiency, and Personalization",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) leverages a retriever that provides substantial external information for improving the generated output of LLMs.This strategy utilizes knowledge in a parameter-free manner, circumvents the high training costs of LLMs' parameterized knowledge.Furthermore, it alleviates the hallucination issues in LLMs, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework (Chen et al., 2017), which marked the initial phase of integrating retrieval mechanisms with Language Models (LMs) through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG underwent evolution with the introduction of Dense Passage Retrieval (Karpukhin et al., 2020), and further advancements in RAG (Lewis et al., 2020) and REALM (Ram et al., 2023).These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation components.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs).Exemplary developments such as RE-PLUG (Shi et al., 2023) and IC-RALM (Ram et al., 2023) demonstrate the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models are capable of following complex instructions, understanding retrieved information, and utilizing limited demonstrations to generate high-quality responses.",
            "score": 0.6866268451918736,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 6872,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 170,
                    "end": 300
                },
                {
                    "start": 300,
                    "end": 445
                },
                {
                    "start": 445,
                    "end": 674
                },
                {
                    "start": 674,
                    "end": 869
                },
                {
                    "start": 869,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1100
                },
                {
                    "start": 1100,
                    "end": 1280
                },
                {
                    "start": 1280,
                    "end": 1449
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 839,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79833984375
        },
        {
            "corpus_id": "269758033",
            "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
            "text": "Retrieval-Augmented Generation (RAG) has recently gained traction in natural language processing. Numerous studies and real-world applications are leveraging its ability to enhance generative models through external information retrieval. Evaluating these RAG systems, however, poses unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. To better understand these challenges, we conduct A Unified Evaluation Process of RAG (Auepora) and aim to provide a comprehensive overview of the evaluation and benchmarks of RAG systems. Specifically, we examine and compare several quantifiable metrics of the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the current RAG benchmarks, encompassing the possible output and ground truth pairs. We then analyze the various datasets and metrics, discuss the limitations of current benchmarks, and suggest potential directions to advance the field of RAG benchmarks.",
            "score": 0.6779528149945756,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88916015625
        },
        {
            "corpus_id": "276576280",
            "title": "Balancing Content Size in RAG-Text2SQL System",
            "text": "Retrieval-Augmented Generation (RAG) introduces an information retrieval process that enhances the generative model's accuracy and robustness by fetching relevant objects from external data stores. This integration allows RAG systems to dynamically incorporate up-to-date and domain-specific knowledge, significantly improving their performance, particularly in knowledge-intensive tasks (Zhao et al. [2024b]). \n\nA retrieval mechanism is embedded into the model pipeline in an RAG system. It fetches contextually relevant information from an external knowledge base or document corpus based on the user query. The retrieved content is then combined with the original query and passed to a generative model, which uses this enriched context to produce its output. This approach represents a transformative shift in Generative AI, creating more transparent (\"glass-box\") models that excel in accuracy and reliability, especially in domains requiring precise information (Khan et al. [2024]). RAG systems also mitigate the need for frequent retraining of large models, reducing both computational and financial costs. This adaptability makes RAG particularly appealing for enterprise applications, where maintaining up-to-date models is essential. \n\nWe designed our RAG system using the following components: \n\n1. Framework: We utilized LangChain (Chase [2022]), a robust framework that simplifies the development of advanced applications integrating language models. Its modular design allows seamless integration of RAG components. 2. Embeddings: Semantic search in RAG systems relies on vector embeddings. For our implementation, we used all-MiniLM-L12-v2, a sentence-transformer model capable of converting textual data into fixed-size embeddings. This model is ideal for clustering and semantic search tasks and has demonstrated superior performance among open-source embedding models (Aperdannier et al. [2024]). 3. Vector Store: Efficient storage and querying of embeddings are essential for the RAG pipeline. We employed FAISS (Facebook AI Similarity Search) (Douze et al. [2024]), an open-source library optimized for fast and lightweight similarity searches. FAISS retrieves relevant document chunks during query processing with high precision.",
            "score": 0.6757993754087436,
            "section_title": "Description of RAG system",
            "char_start_offset": 18265,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 410
                },
                {
                    "start": 413,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 762
                },
                {
                    "start": 763,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1244
                },
                {
                    "start": 1247,
                    "end": 1305
                },
                {
                    "start": 1308,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1605
                },
                {
                    "start": 1606,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 1887,
                    "end": 1913,
                    "matchedPaperCorpusId": "276934091"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "273963552",
            "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text Classification with Large LLMs",
            "text": "Retrieval Augmented Generation (RAG) [9], a method first introduced in 2020, gained popularity after the emergence of LLM. RAG offers benefits such as associating answers with professional knowledge without undergoing training, thereby enhancing the answer quality and reducing hallucinations. This is achieved by first retrieving relevant information from external knowledge, mainly through embedding models, and subsequently combining the retrieved information into the input prompt of LLMs to generate answers. There are a number of works aiming at improving RAG pipeline from various aspects such as query rewrite, document rerank, and post-retrieval processing [10]. The RAG framework proved to be highly suitable for text classification tasks. In this regard, [11] utilized retrieval-enhanced LLMs to generate text classification datasetes under zero-shot condition. Moreover, [12] proposed a Retrieval-Augmented framework to alleviate poor generalization issues existed in text classification taskes. \n\nHowever, for methods like RAG or CoT that do not entail fine-tuning, the outputs might not always comply with the instructions in the prompt, while fine-tuning enables adjustments of the behaviors of LLMs, such as adhering to specific writing styles [10]. At the same time, adding content to the prompt and generating reasoning prior to answering leads to increased computation costs. For instance, as of July 2024, for GPT-4-o, it is $5 per 1M input and $15 per 1M output, and the cost for text-embedding-3-large is $0.13 .This can be depleted rapidly when we have a large volume of data and if we aim to apply CoT and RAG to every data point. In production scenarios where there is a massive amount of data and the answer is required as soon as possible, a smaller yet quicker specialized model that directly outputs the answer is much preferred. While [13] reveals that LLMs can improve themselves through the utilization of CoT prompting and self-consistency to generate high-quality answers from unlabeled data and fine-tuning based on these generated answers. Studies such as [14] have further demonstrated that fine-tuning a model with fewer parameters through knowledge distillation from the CoT outputs of larger LLMs results in improved task performance on datasets like GSM8K.",
            "score": 0.6705003291275935,
            "section_title": "Introduction",
            "char_start_offset": 2349,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 513
                },
                {
                    "start": 514,
                    "end": 671
                },
                {
                    "start": 672,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1007
                },
                {
                    "start": 1010,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1858
                },
                {
                    "start": 1859,
                    "end": 2075
                },
                {
                    "start": 2076,
                    "end": 2297
                }
            ],
            "ref_mentions": [
                {
                    "start": 1865,
                    "end": 1869,
                    "matchedPaperCorpusId": "253080328"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "270764659",
            "title": "Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation",
            "text": "To address the misalignment between different components of retrieval-augmented generation (RAG) and improve overall generation performance, we propose the DPA-RAG framework, which is illustrated in Figure 2. In general, DPA-RAG improves traditional RAG architecture in two main aspects: (1) we fine-tune a preference-aligned reranker between the retriever and the LLM to selectively filter out knowledge that aligns with LLMs' knowledge preferences ( \u00a73.3); and (2) we design a self-alignment mechanism that fine-tunes LLMs to better recognize and utilize knowledge consistent with their reasoning preferences ( \u00a73.4).To acquire the LLM's preference knowledge, we devise a three-step construction method, motivated by our preliminary analysis of how different types of retrieved documents affect RAG performance ( \u00a73.2).Below, we will first introduce the task definition ( \u00a73.1) and then we delve into the specifics of our approach.",
            "score": 0.6683860266740645,
            "section_title": "Methodology",
            "char_start_offset": 8100,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 619
                },
                {
                    "start": 619,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 933
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74658203125
        },
        {
            "corpus_id": "274283400",
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "text": "LLMs have known weaknesses, such as producing answers that do not match or contradict the given context. This issue is particularly pronounced when addressing domain-specific or highly specialized queries. To mitigate these weaknesses, a technique called retrieval-augmented generation (RAG) was proposed in 2020 [1,2]. The core idea of RAG is to integrate data obtained from querying external knowledge sources into the generation process. By leveraging the accuracy and specificity of knowledge from external sources, RAG enhances the generative process, improving the ability to provide highly relevant and real-time responses to queries. This section provides an overview of RAG models and reviews existing research aimed at enhancing their performance.",
            "score": 0.6683043267534963,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 6316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 641
                },
                {
                    "start": 642,
                    "end": 757
                }
            ],
            "ref_mentions": [
                {
                    "start": 313,
                    "end": 316,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 316,
                    "end": 318,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.861328125
        },
        {
            "corpus_id": "276576207",
            "title": "A Taxonomy of Generative AI in HEOR: Concepts, Emerging Applications, and Advanced Tools - An ISPOR Working Group Report.",
            "text": "This section examines several more advanced techniques to enhance generative AI performance in HEOR. Strategies like prompt engineering and retrieval-augmented generation (RAG) improve accuracy, factuality, and comprehensiveness. Model fine-tuning and domain-specific FMs ensure contextually relevant outputs for specialized tasks. These methods, applicable across use cases, help address key challenges in accuracy and reliability. Table 2 summarizes key approaches to enhance the quality of generative AI outputs in HEOR.",
            "score": 0.6678463564107304,
            "section_title": "Techniques to improve the use of generative AI in HEOR",
            "char_start_offset": 14533,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 432
                },
                {
                    "start": 433,
                    "end": 523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64111328125
        },
        {
            "corpus_id": "268032903",
            "title": "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)",
            "text": "2.1 Retrieval-Augmented Generation (RAG) Retrieval-augmented generation (RAG), first introduced by Lewis et al. (2020), has emerged as one of the most popular approaches to enhance the generation ability of LLMs (Liu, 2022;Chase, 2022;Van Veen et al., 2023;Ram et al., 2023;Shi et al., 2023). This synergy markedly boosts the output's accuracy and relevance (Gao et al., 2023), mitigating essential issues commonly referred to as \"hal-lucinations\" of LLMs (Shuster et al., 2021). One of RAG's distinctive features is its flexible architecture, allowing for the seamless interchange or update of its three core components: the dataset, the retriever, and the LLM. This flexibility means that adjustments to any of these elements can be made without necessitating re-training or fine-tuning of the entire system (Shao et al., 2023;Cheng et al., 2023). These unique advantages have positioned RAG as a favored approach for a range of practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants (Panagoulias et al., 2024).",
            "score": 0.6662878302251304,
            "section_title": "Related Work",
            "char_start_offset": 4018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 292
                },
                {
                    "start": 293,
                    "end": 479
                },
                {
                    "start": 480,
                    "end": 662
                },
                {
                    "start": 663,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 118,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1075,
                    "matchedPaperCorpusId": "267112617"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61279296875
        },
        {
            "corpus_id": "270703323",
            "title": "Context-augmented Retrieval: A Novel Framework for Fast Information Retrieval based Response Generation using Large Language Model",
            "text": "With the launch of OpenAI's ChatGPT, Large Language Models (LLMs) such as the GPT and LLAMA series have garnered significant attention.These LLMs can perform various impressive tasks, including language translation, content summarization, and other functions that typically require human intelligence.In certain aspects, these models can sometimes outperform humans.However, LLMs face challenges such as factual hallucination, knowledge obsolescence, and a lack of domain-specific expertise [29][15] [35].Two primary solutions to address these problems are fine-tuning LLMs on domain-specific data and using Retrieval-Augmented Generation (RAG).Based on previous research, RAG is considered the better option [13].\n\nRetrieval-augmented generation (RAG) is an advanced technique in NLP that combines retrieval-based and generative models to improve the quality and accuracy of responses, especially in applications requiring precise information retrieval within a specific knowledge domain (e.g., a book or other provided sources for factual correctness) [14].\n\nTraditional generative models like the GPT series and the LLAMA series generate responses based solely on the input they receive.This can sometimes lead to inaccuracies or hallucinations where the model produces irrelevant information [8][34] [30].In contrast, retrieval-based models focus on fetching relevant information from a pre-existing database, ensuring factual correctness.While generative models can provide faster answers, incorrect information can be more damaging than no information at all.RAG bridges these two approaches.It employs a retrieval mechanism to fetch relevant documents or passages from a large database based on the input query.These retrieved documents provide factual grounding for response.The generative model takes these documents and generates a contextually appropriate response.A typical RAG workflow is depicted in Figure 1.",
            "score": 0.6656910253899009,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 135,
                    "end": 301
                },
                {
                    "start": 301,
                    "end": 366
                },
                {
                    "start": 366,
                    "end": 505
                },
                {
                    "start": 505,
                    "end": 645
                },
                {
                    "start": 645,
                    "end": 714
                },
                {
                    "start": 716,
                    "end": 1059
                },
                {
                    "start": 1061,
                    "end": 1190
                },
                {
                    "start": 1190,
                    "end": 1309
                },
                {
                    "start": 1309,
                    "end": 1443
                },
                {
                    "start": 1443,
                    "end": 1565
                },
                {
                    "start": 1565,
                    "end": 1598
                },
                {
                    "start": 1598,
                    "end": 1718
                },
                {
                    "start": 1718,
                    "end": 1783
                },
                {
                    "start": 1783,
                    "end": 1876
                },
                {
                    "start": 1876,
                    "end": 1923
                }
            ],
            "ref_mentions": [
                {
                    "start": 1054,
                    "end": 1058,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "275757440",
            "title": "Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations",
            "text": "\u2022 Prompt engineering: creating and adapting prompts (input) to instruct AI models to generate specific output. \n\n\u2022 Retrieval Augmented Generation (RAG): A method in natural language processing that combines a generative model with a retrieval system to improve response accuracy. The retrieval system finds relevant information, which the generative model uses to produce more contextually accurate and factual outputs. -Example: DALL-E generating images from text descriptions.",
            "score": 0.6654268142727503,
            "section_title": "Glossary",
            "char_start_offset": 29583,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 110
                },
                {
                    "start": 113,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 419
                },
                {
                    "start": 420,
                    "end": 478
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.39501953125
        },
        {
            "corpus_id": "267061186",
            "title": "Understanding Biases in ChatGPT-based Recommender Systems: Provider Fairness, Temporal Stability, and Recency",
            "text": "Moreover, they discuss the application of retrieval-augmented generation (RAG) techniques, which combine information retrieval with generative modeling to produce contextually relevant recommendations. Overall, these insights underscore the significant advancements and ongoing challenges in utilizing generative models for RS.",
            "score": 0.6632857636318475,
            "section_title": "Leveraging Pre-trained LMs and Prompting for Recommender Systems",
            "char_start_offset": 23909,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 327
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.57177734375
        },
        {
            "corpus_id": "272524830",
            "title": "LLMs Will Always Hallucinate, and We Need to Live With This",
            "text": "Fine-tuning improves how large language models (LLMs) perform on specific tasks, but it does not always ensure that the information they produce is factually accurate. Retrieval-Augmented Generation (RAG) addresses this by combining the strengths of language models with information retrieval systems, allowing the model to generate content based on accurate, up-to-date information. \n\nIn simple terms, RAG works like this: \n\nHere, y is the output, G is the language model, x is the input, and R(x) is the relevant information retrieved from an external knowledge base.",
            "score": 0.6595801766397112,
            "section_title": "Retrieval-Augmented Generation (RAG): Bridging Knowledge Gaps",
            "char_start_offset": 12455,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 383
                },
                {
                    "start": 386,
                    "end": 423
                },
                {
                    "start": 426,
                    "end": 569
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.56591796875
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) represents a significant advancement in artificial intelligence combining a retrieval phase with a generative phase, with the latter typically being powered by large language models (LLMs). The current common practices in RAG involve using\"instructed\"LLMs, which are fine-tuned with supervised training to enhance their ability to follow instructions and are aligned with human preferences using state-of-the-art techniques. Contrary to popular belief, our study demonstrates that base models outperform their instructed counterparts in RAG tasks by 20% on average under our experimental settings. This finding challenges the prevailing assumptions about the superiority of instructed LLMs in RAG applications. Further investigations reveal a more nuanced situation, questioning fundamental aspects of RAG and suggesting the need for broader discussions on the topic; or, as Fromm would have it,\"Seldom is a glance at the statistics enough to understand the meaning of the figures\".",
            "score": 0.6571438934987788,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.830078125
        },
        {
            "corpus_id": "276576207",
            "title": "A Taxonomy of Generative AI in HEOR: Concepts, Emerging Applications, and Advanced Tools - An ISPOR Working Group Report.",
            "text": "Retrieval-Augmented Generation (RAG) is a sophisticated method that combines the broad knowledge base of FMs with precise, domain-specific data retrieval 73 . Conceptually a RAG system retrieves more up-to-date information, or task specific information from external knowledge or data sources than the FM was pre-trained on 74 . For example, a generative AI solution can employ RAG to verify facts by accessing external databases or websites in realtime, ensuring responses not only draw from a vast internal dataset but are also cross verified with the latest external references 75 . This capability significantly enhances the accuracy and reliability of the information provided, especially in rapidly evolving fields where the FM might not have been trained on the most up to date data 74,76 . Unless carefully managed, a limitation of RAG could be its lack of ability in handling conflicting information retrieval which can detrimentally affect RAG's output quality and is an active area of research 73,76,77 .",
            "score": 0.6570392231407244,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 20067,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 158
                },
                {
                    "start": 159,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 585
                },
                {
                    "start": 586,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1015
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64306640625
        },
        {
            "corpus_id": "270521566",
            "title": "HIRO: Hierarchical Information Retrieval Optimization",
            "text": "Retrieval-Augmented Generation (RAG) [8], enhances Large Language Models (LLMs) by integrating real-world data from external databases, thus establishing Retrieval Augmented Language Models (RALMs). This approach ensures generated content is both contextually relevant and factually accurate, addressing the issue of LLMs producing plausible but incorrect \"hallucinated\" in-formation [16]. By enhancing the accuracy and reliability of AI-generated text, RAG heralds a new era of trustworthy machine-generated communications. It's widespread use in information retrieval and question-answering showcases its utility. However, challenges remain, particularly in scalability and efficiency when dealing with large datasets and complex structures. These issues highlight the need for ongoing research in optimization techniques, an area where Hierarchical Information Retrieval shows potential.",
            "score": 0.655547582227742,
            "section_title": "Related Work",
            "char_start_offset": 3331,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 199,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 890
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74267578125
        },
        {
            "corpus_id": "268513072",
            "title": "Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI",
            "text": "This method modifies only a small portion of the model's weights, reducing the number of parameters that need to be updated during fine-tuning.By focusing on these adaptable components, LoRA efficiently updates the model, maintaining performance while significantly lowering computational demands and memory usage.\n\nImproving the performance of Retrieval Augmented Generation (RAG) involves several strategic enhancements across data preparation, indexing, and query handling.To reduce computational time, we can explore various index types for better context retrieval.Additionally, we can also transform queries to better match the retrieval context.Each of these tactics aims at refining the interaction between the LLM and the data, ensuring more accurate, relevant, and efficient generation outcomes [20].",
            "score": 0.6524768576362944,
            "section_title": "B. Challenges in the Generative Models",
            "char_start_offset": 7772,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 314
                },
                {
                    "start": 316,
                    "end": 476
                },
                {
                    "start": 476,
                    "end": 570
                },
                {
                    "start": 570,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 810
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.49267578125
        },
        {
            "corpus_id": "276408784",
            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
            "text": "Recent advancements in Generative AI have significantly improved the efficiency and adaptability of natural language processing (NLP) systems, particularly through Retrieval-Augmented Generation (RAG), Low-Rank Adaptation (LoRA), and Weight-Decomposed Low-Rank Adaptation (DoRA). RAG integrates external knowledge to enhance factual consistency in generative outputs, while LoRA enables parameter-efficient fine-tuning of large language models (LLMs). DoRA further refines this process by optimizing fine-tuning through adaptive parameter ranking and domain-aware weight adjustments, improving learning efficiency while maintaining inference performance. This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision. These insights provide practical guidance for deploying AI-driven generative systems in accuracy-critical domains such as healthcare, finance, and legal services, ensuring scalability, reliability, and optimal performance in dynamic environments.",
            "score": 0.6521914294777227,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.986328125
        },
        {
            "corpus_id": "264288947",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020;Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b). While prior work often retrieves only once at the beginning, Jiang et al. (2023) propose to adaptively retrieve passages for generation on top of a proprietary LLM or Schick et al. (2023) train an LM to generate API calls for named entities. Yet, the improved task performance of such approaches often comes at the expense of runtime efficiency (Mallen et al., 2023), robustness to irrelevant context (Shi et al., 2023), and lack of attributions (Liu et al., 2023a;Gao et al., 2023). We introduce a method to train an arbitrary LM to learn to use retrieval on-demand for diverse instruction-following queries and introduce controlled generation guided by reflections tokens to further improve generation quality and attributions.",
            "score": 0.6518244326388962,
            "section_title": "RELATED WORK",
            "char_start_offset": 4683,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1265
                }
            ],
            "ref_mentions": [
                {
                    "start": 130,
                    "end": 148,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 148,
                    "end": 167,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 881,
                    "end": 902,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 937,
                    "end": 955,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.888671875
        },
        {
            "corpus_id": "276249863",
            "title": "LM2: Large Memory Models",
            "text": "Retrieval-Augmented Generation (RAG) Retrieval-Augmented Generation (RAG) Lewis et al. ( 2020) is a popular solution for language models to handle large amounts of text. The core architecture of RAG comprises a retriever module that identifies relevant information from a knowledge base, ensuring that the input to the generative model remains within the token limit while filtering out irrelevant noise, thereby improving efficiency and response quality. While Retrieval-Augmented Generation (RAG) has proven effective for many tasks, it struggles with some complicated tasks like multi-hop question-answering Mavi et al. (2024), which require retrieving and reasoning over multiple interconnected pieces of evidence.",
            "score": 0.650183693254397,
            "section_title": "Related Work",
            "char_start_offset": 22501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 718
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70263671875
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "In this work, we present Finetune-RAG, a simple yet effective method for reducing hallucination in Retrieval-Augmented Generation (RAG) through supervised fine-tuning. Rather than focusing on retrieval quality, Finetune-RAG trains the generation model to rely solely on factual context while ignoring misleading information, with no architectural changes required. \n\nWe constructed a diverse training set and evaluate using Bench-RAG, a technique that leverages GPT-4o as an automatic judge. Results show substantial gains in factual accuracy while preserving helpfulness, relevance, and depth. Ablation studies further reveal that prompt structure subtly impacts robustness, with less structured formats sometimes aiding discrimination. \n\nDespite its simplicity, Finetune-RAG demonstrates that generation-stage fine-tuning can meaningfully improve hallucination resistance in noisy retrieval environments. We release our code, dataset, and checkpoints to support further research in this direction, and highlight future extensions including multi-document training, joint retrieval-generation optimization, and adaptation to multimodal tasks.",
            "score": 0.6498784246360466,
            "section_title": "Conclusion",
            "char_start_offset": 23325,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 168,
                    "end": 364
                },
                {
                    "start": 367,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 737
                },
                {
                    "start": 740,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1143
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.939453125
        },
        {
            "corpus_id": "272367349",
            "title": "Evaluating ChatGPT on Nuclear Domain-Specific Data",
            "text": "Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by grounding their responses in external knowledge sources. This approach is",
            "score": 0.6475767798984428,
            "section_title": "Retrieval-Augmented Generation (RAG): A Background",
            "char_start_offset": 4164,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 191
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.352783203125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) has undergone significant evolution, with extensive research dedicated to improving retrieval effectiveness and enhancing coherent generation to minimize hallucinations. From its early iterations to recent advancements, RAG has been instrumental in integrating external knowledge into Large Language Models (LLMs), thereby boosting accuracy and reliability. In particular, recent domain-specific work has showcased RAG's potential in specialized areas such as legal, medical, and low-resource language applications, highlighting its adaptability and scope. However, despite these advances, this paper identifies clear gaps that remain unresolved. Challenges such as the integration of ambiguous or unstructured information, effective handling of domain-specific contexts, and the high computational overhead of complex retrieval tasks still persist. These limitations constrain the broader applicability of RAG systems, particularly in diverse and dynamic real-world environments. The future research directions outlined in this paper-ranging from improving retrieval mechanisms to enhancing context management and ensuring scalability-will serve as a critical guide for the next phase of innovation in this space. By addressing these gaps, the next generation of RAG models has the potential to drive more reliable, efficient, and domain-adaptable LLM systems, further pushing the boundaries of what is possible in retrieval-augmented AI applications.",
            "score": 0.646398266093392,
            "section_title": "Conclusion",
            "char_start_offset": 38729,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1489
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "274965531",
            "title": "XRAG: eXamining the Core - Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.",
            "score": 0.6459923413389017,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8076171875
        },
        {
            "corpus_id": "277272240",
            "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
            "text": "Retrieval-augmented generation (RAG) is a transformative technique in generative AI, particularly in natural language processing (NLP) and recommendation systems. It improves content quality by integrating external, up-to-date information from knowledge sources [1]. Despite the impressive performance of large language models (LLMs), challenges like hallucinations, outdated knowledge, and lack of domainspecific expertise remain [1]. RAG addresses these issues by supplying LLMs with relevant, retrieved factual information, enhancing model outputs. \n\nRAG works by using a retriever to extract relevant knowledge from external databases [12], which is then combined with the model's input to provide enriched context [13]. This approach is efficient, requiring minimal adaptation and often no additional training [2]. Recent studies highlight RAG's potential, not only for knowledge-intensive tasks but also for a broad range of language-based applications [2], enabling more accurate and up-to-date outputs. \n\nWhile traditional RAG pipelines are text-based, real-world knowledge is often multimodal, in the form of images, videos, and 3D models. This creates challenges when applying RAG to computer vision (CV). In CV, visual understanding tasks like object identification [14,15], anomaly detection [16], and segmentation [17] require integrating external knowledge to improve accuracy [13]. Similarly, visual generation tasks, such as transforming textual descriptions into realistic images, can benefit from external knowledge like scene layouts, object relationships, and temporal dynamics in videos [15]. \n\nGiven the complexity of visual data, RAG can significantly improve model performance. For example, scene generation models can benefit from knowledge about object interactions and spatial relationships, while image classification models can enhance accuracy by retrieving up-to-date visual references. By integrating external knowledge, RAG enhances both visual understanding and generation, helping overcome inherent challenges in vision tasks. As shown in Figure 1, recent research has started exploring RAG's integration into CV, aiming to improve both understanding and generation. While large visionlanguage models (LVLMs) have shown promise, they still face challenges with image generalization and understanding [18].",
            "score": 0.645197065305399,
            "section_title": "A. Background",
            "char_start_offset": 2182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 266
                },
                {
                    "start": 267,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 551
                },
                {
                    "start": 554,
                    "end": 724
                },
                {
                    "start": 725,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 1010
                },
                {
                    "start": 1013,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1396
                },
                {
                    "start": 1397,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1917
                },
                {
                    "start": 1918,
                    "end": 2061
                },
                {
                    "start": 2062,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 639,
                    "end": 643,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 815,
                    "end": 818,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 959,
                    "end": 962,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1277,
                    "end": 1281,
                    "matchedPaperCorpusId": "206594692"
                },
                {
                    "start": 1281,
                    "end": 1284,
                    "matchedPaperCorpusId": "268531117"
                },
                {
                    "start": 1608,
                    "end": 1612,
                    "matchedPaperCorpusId": "268531117"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88818359375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "Ensuring the accuracy of responses generated by Large Language Models (LLMs) such as Chat-GPT [13] and LLaMA [14] is essential.However, simply enlarging model size does not fundamentally address the issue of hallucinations [15,16], especially in knowledge-intensive tasks and specialized domains.Retrieval-augmented generation (RAG) addresses these challenges by retrieving relevant documents from external knowledge bases, providing accurate, real-time, domain-specific context to LLMs [6].Previous works have optimized the RAG pipeline through query and retrieval transformations, enhancing retriever performance, and fine-tuning both the retriever and generator.These optimizations improve the interaction between input queries, retrieval mechanisms, and generation processes, ensuring the accuracy and relevance of responses.",
            "score": 0.6451405011637366,
            "section_title": "Related Work",
            "char_start_offset": 4844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 127,
                    "end": 296
                },
                {
                    "start": 296,
                    "end": 491
                },
                {
                    "start": 491,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 829
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 230,
                    "matchedPaperCorpusId": "266164171"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8291015625
        },
        {
            "corpus_id": "266741703",
            "title": "Enhancing Multilingual Information Retrieval in Mixed Human Resources Environments: A RAG Model Implementation for Multicultural Enterprise",
            "text": "Retrieval-Augmented Generation (RAG) models have shown promise in bridging the gap between retrieval-based and generative models, offering a hybrid approach to information retrieval. Lee et al. introduced RAG, demonstrating its effectiveness in improving pre-trained language models by incorporating information retrieval components [5]. This approach aligns with our goal of enhancing information retrieval in a mixed human resources environment.",
            "score": 0.6443782924660443,
            "section_title": "Retrieval-Augmented Generation (RAG) Models",
            "char_start_offset": 5206,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 183,
                    "end": 337
                },
                {
                    "start": 338,
                    "end": 447
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This paper focuses on Retrieval-Augmented Generation (RAG) system for black-box Large Language Models (LLMs), namely black-box RAG.In this section, we first give the definition of RAG and subsequently introduce the black-box RAG.\n\nRetrieval-Augmented Generation (RAG).Given a natural language question , an external knowledge corpus W and a generative language model M, a RAG system aims to help M generate more accurate and informative responses to  using a retrieval model R, which effectively retrieves relevant documents D = ( 1 ,  2 ,  3 , ...) from W. The form of introducing external knowledge to the language model varies, including modifying attention weights during generation, incorporating it into input prompts, or using it in post-calibration of the model output.Moreover, existing RAG methods typically require joint fine-tuning of the retriever and the language model (e.g.Atlas [15], REALM [9]).However, joint fine-tuning is unaffordable in amount of practical scenarios due to the extremely large parameter scale of LLMs.In these scenarios, we can alternatively treat an LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it with a RAG system, namely black-box RAG.Next, we introduce the definition of the black-box RAG.\n\nRetrieval-Augmented Generation System for Black-box LLM (Black-box RAG).A RAG system for blackbox LLM aims to enhance the generation capability of the black-box LLM M  by retrieving external knowledge without updating the LLM parameters.While the parameters of the black-box LLM M  are frozen, the parameters of the retrieval model R are learnable.Thus, the RAG system for black-box LLM only optimizes R to improve overall system performance, without modifying M  .Moreover, existing black-box RAG systems typically inject the retrieved documents D into M  by constructing an input prompt that concatenates the question  and documents D, which leverages the powerful in-context learning capabilities of the LLM.",
            "score": 0.6439273452860144,
            "section_title": "Problem Formulation",
            "char_start_offset": 14673,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 229
                },
                {
                    "start": 231,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 777
                },
                {
                    "start": 777,
                    "end": 889
                },
                {
                    "start": 889,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1204
                },
                {
                    "start": 1204,
                    "end": 1259
                },
                {
                    "start": 1261,
                    "end": 1333
                },
                {
                    "start": 1333,
                    "end": 1498
                },
                {
                    "start": 1498,
                    "end": 1609
                },
                {
                    "start": 1609,
                    "end": 1726
                },
                {
                    "start": 1726,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 907,
                    "end": 910,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "270869499",
            "title": "First Place Solution of 2023 Global Artificial Intelligence Technology Innovation Competition Track 1",
            "text": "Retrieval Augmented Generation (RAG) technology [2,6,7,9,15] in the field of natural language processing represents an innovative breakthrough.Traditional NLP techniques primarily rely on large language models, but their accuracy and depth may be limited when dealing with complex queries that require extensive background knowledge.To overcome this limitation, RAG combines conventional information retrieval methods with modern generative language models, aiming to enhance the model's text generation capabilities by incorporating external knowledge sources.The core principle is to integrate retrieval and generation techniques, allowing the model to access and utilize a vast amount of external information before generating text.RAG excels in addressing knowledge-intensive NLP tasks such as question answering, fact verification, and more.In recent years, RAG systems have evolved from a primary stage to an advanced stage, and then to a modular stage, to improve performance, cost-effectiveness, and   crease the difficulty of the pre-training task, gradually increasing the proportion of masking as the number of epochs increased.Specifically, we set an initial mask proportion of 0.3, and after every 10 epochs of pre-training, we perform fine-tuning of the downstream task.If the performance of the fine-tuning is lower than the previous one, we increase the mask proportion by 0.05 and continue with pre-training.Ultimately, we increase the number of pre-training epochs to 140, which significantly improves the text generation performance of the downstream task.For the construction of the training set with retrieval knowledge, we use D(Description) as the query and calculate the similarity with the key of each key-value pair in the knowledge base (e.g., vector inner product, L2 distance, or cosine similarity).If the similarity is larger than the threshold k, we call it an effective retrieval.We retrieve this key-value pair and concatenate the value to the end of the query as the new training sample corresponding to the query.For the val set and test set, we use the same retrieval method to construct the val set and test set with retrieval knowledge.Retrieval Iterations.For the first retrieval augmentation, the embeddings of key-value pairs are computed using a model trained on a training set without a knowledge base.",
            "score": 0.6395037248973108,
            "section_title": "Retrieval Augmentation in NLP",
            "char_start_offset": 3769,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 143,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 561
                },
                {
                    "start": 561,
                    "end": 735
                },
                {
                    "start": 735,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 1139
                },
                {
                    "start": 1139,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1425
                },
                {
                    "start": 1425,
                    "end": 1575
                },
                {
                    "start": 1575,
                    "end": 1828
                },
                {
                    "start": 1828,
                    "end": 1912
                },
                {
                    "start": 1912,
                    "end": 2048
                },
                {
                    "start": 2048,
                    "end": 2174
                },
                {
                    "start": 2174,
                    "end": 2195
                },
                {
                    "start": 2195,
                    "end": 2345
                }
            ],
            "ref_mentions": [
                {
                    "start": 48,
                    "end": 51,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 51,
                    "end": 53,
                    "matchedPaperCorpusId": "264426178"
                },
                {
                    "start": 53,
                    "end": 55,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 55,
                    "end": 57,
                    "matchedPaperCorpusId": "267053546"
                },
                {
                    "start": 57,
                    "end": 60,
                    "matchedPaperCorpusId": "247058346"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6220703125
        },
        {
            "corpus_id": "270357334",
            "title": "RAG Does Not Work for Enterprises",
            "text": "Retrieval-Augmented Generation (RAG) is an emerging paradigm that combines the strengths of pre-trained language models with external knowledge retrieval to enhance the accuracy, consistency, and contextual relevance of generated outputs [ Lewis et al., 2020 ].In a typical RAG architecture, a retriever component first selects the most relevant documents or passages based on the input query, and then a generator component conditions on both the query and the retrieved content to produce a final output [ Izacard and Grave, 2021 ].RAG has shown significant promise in improving the factual accuracy, consistency, and contextual awareness of generative models across a wide range of applications, such as question answering, dialogue systems, and content creation [ Zhao et al., 2024 ].However, implementing RAG effectively in real-world, enterprise settings poses several challenges, which this paper aims to address.",
            "score": 0.6385238971957157,
            "section_title": "Background",
            "char_start_offset": 5003,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 261
                },
                {
                    "start": 261,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 788
                },
                {
                    "start": 788,
                    "end": 920
                }
            ],
            "ref_mentions": [
                {
                    "start": 506,
                    "end": 533,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 766,
                    "end": 787,
                    "matchedPaperCorpusId": "220302360"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63134765625
        },
        {
            "corpus_id": "268091298",
            "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
            "text": "Advancements in model algorithms, the growth of foundational models, and access to high-quality datasets have propelled the evolution of Artificial Intelligence Generated Content (AIGC). Despite its notable successes, AIGC still faces hurdles such as updating knowledge, handling long-tail data, mitigating data leakage, and managing high training and inference costs. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances the generation process by retrieving relevant objects from available data stores, leading to higher accuracy and better robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator, distilling the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Github: https://github.com/PKU-DAIR/RAG-Survey.",
            "score": 0.6371806348411719,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84619140625
        },
        {
            "corpus_id": "277940421",
            "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm",
            "text": "Retrieval augmented generation (RAG) combines the generative power of LLMs with external knowledge retrieval systems to produce more accurate and contextually informed outputs [14]. This augmented approach addresses key limitations of stand-alone generative models, such as hallucination and factual inaccuracy, by incorporating relevant information retrieved from structured databases, corpora, or the web during the generation process. The process typically involves two main steps. The first is to identify and retrieve relevant documents or information from alternative sources. The second passes these texts into the generative model as additional context. The model integrates this information with its internal knowledge to produce responses that more strongly influenced by the new knowledge. \n\nBy integrating external information sources with its inherent generative abilities, RAG can substantially reduce the risk of generating misleading information. The retrieved context acts as a dynamic, evidence-based grounding layer, enhancing the reliability of the outputs produced by the generative model. Incorporating verifiable information from external sources helps mitigate the phenomenon of hallucination that is common in large language models. The retrieved content can also provide richer context for generating nuanced responses. \n\nRetrieval augmented generation has found applications across a range of domains: conversational AI, in enhancing chatbot responses by grounding them in current events or factual databases; question answering -improving the precision of responses in systems that need to answer fact-based queries; content creation, by supporting writers and researchers by providing dynamically retrieved background information that complements the creative process. It is the ability to focus the LLMs onto specific areas that presents an interesting opportunity for educationalists and technology designers alike.",
            "score": 0.6368795910953013,
            "section_title": "Focussing LLMs",
            "char_start_offset": 9318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 800
                },
                {
                    "start": 803,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1110
                },
                {
                    "start": 1111,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1345
                },
                {
                    "start": 1348,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1946
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 180,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74951171875
        },
        {
            "corpus_id": "277151020",
            "title": "Privacy-Aware RAG: Secure and Isolated Knowledge Retrieval",
            "text": "Retrieval-Augmented Generation (RAG), initially introduced by [35], has rapidly become one of the most prominent methodologies for enhancing the generative capabilities of Large Language Models (LLMs) [11,45,47,51]. This approach significantly improves the accuracy and relevance of generated outputs by mitigating common issues such as \"hallucinations\" in LLMs [20,50]. One of RAG's distinctive features is its flexible architecture, enabling the interchange or update of its core components-the dataset, retriever, and LLM-without necessitating retraining or fine-tuning of the entire system [13,46]. Consequently, RAG has been widely adopted across various practical applications, including personal chatbots and specialized domain experts like medical diagnostic assistants [41]. \n\nThe increasing attention towards LLMs, both in industry and academia, underscores their remarkable ability to facilitate convincing linguistic interactions with humans [29,30,36,60]. However, adapting these models to new knowledge not available at training time poses significant challenges. For instance, in real-world scenarios involving virtual assistants [15,21,33], the knowledge base or tasks may evolve over time, requiring model adaptation through fine-tuning processes [2,17,58]. This can lead to catastrophic forgetting, where previously acquired knowledge is lost [38]. Alternatively, new knowledge can be appended to the input prompt via in-context learning (ICL) without altering the model parameters [6,19,37,53,55], a principle that underpins RAG systems. \n\nIn the context of RAG, a typical system comprises four principal components [45]: (i) a text embedder function , which maps textual information into a high-dimensional embedding space; (ii) a storage mechanism, often referred to as a vector store, that memorizes texts and their embedded representations; (iii) a similarity function, such as cosine similarity, used to evaluate the similarity between pairs of embedded text vectors; and (iv) a generative model, denoted as function  , typically an LLM, that produces output text based on input prompts and retrieved information.",
            "score": 0.6350709117543745,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 11697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1274
                },
                {
                    "start": 1275,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1556
                },
                {
                    "start": 1559,
                    "end": 2137
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 205,
                    "end": 208,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 211,
                    "end": 214,
                    "matchedPaperCorpusId": "261822526"
                },
                {
                    "start": 594,
                    "end": 598,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 778,
                    "end": 782,
                    "matchedPaperCorpusId": "267112617"
                },
                {
                    "start": 961,
                    "end": 964,
                    "matchedPaperCorpusId": "220961531"
                },
                {
                    "start": 1149,
                    "end": 1152,
                    "matchedPaperCorpusId": "269914863"
                },
                {
                    "start": 1152,
                    "end": 1155,
                    "matchedPaperCorpusId": "257445349"
                },
                {
                    "start": 1500,
                    "end": 1503,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.81103515625
        },
        {
            "corpus_id": "270560978",
            "title": "Vul-RAG: Enhancing LLM-based Vulnerability Detection via Knowledge-level RAG",
            "text": "Retrieval-Augmented Generation (RAG) is a general paradigm which enhances LLMs by including relevant information retrieved from external databases into the input [24].RAG typically consists of three phases: indexing, retrieval, and generation.First, the indexing phase constructs external databases and their retrieval index from external data sources; Second, given a user query, the retrieval system then utilizes these index to fetch the relevant document chunks as context.Third, the retrieved context is then integrated into the input prompt for LLMs, and LLMs then generate the final output based on the augmented inputs.RAG has been widely used in various domains [25][26][27][28].For example, RAG has been specialized to software engineering tasks such as code generation [27,28], which retrieves the similar code from the code base and augments the prompt with the retrieved code for model inference.",
            "score": 0.6344573941606622,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 10204,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 167
                },
                {
                    "start": 167,
                    "end": 243
                },
                {
                    "start": 243,
                    "end": 477
                },
                {
                    "start": 477,
                    "end": 627
                },
                {
                    "start": 627,
                    "end": 688
                },
                {
                    "start": 688,
                    "end": 909
                }
            ],
            "ref_mentions": [
                {
                    "start": 671,
                    "end": 675,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 675,
                    "end": 679,
                    "matchedPaperCorpusId": "253097741"
                },
                {
                    "start": 679,
                    "end": 683,
                    "matchedPaperCorpusId": "257663528"
                },
                {
                    "start": 683,
                    "end": 687,
                    "matchedPaperCorpusId": "247450969"
                },
                {
                    "start": 780,
                    "end": 784,
                    "matchedPaperCorpusId": "257663528"
                },
                {
                    "start": 784,
                    "end": 787,
                    "matchedPaperCorpusId": "247450969"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.587890625
        },
        {
            "corpus_id": "273749074",
            "title": "Maximizing RAG efficiency: A comparative analysis of RAG methods",
            "text": "This paper addresses the optimization of retrieval-augmented generation (RAG) processes by exploring various methodologies, including advanced RAG methods. The research, driven by the need to enhance RAG processes as highlighted by recent studies, involved a grid-search optimization of 23,625 iterations. We evaluated multiple RAG methods across different vectorstores, embedding models, and large language models, using cross-domain datasets and contextual compression filters. The findings emphasize the importance of balancing context quality with similarity-based ranking methods, as well as understanding tradeoffs between similarity scores, token usage, runtime, and hardware utilization. Additionally, contextual compression filters were found to be crucial for efficient hardware utilization and reduced token consumption, despite the evident impacts on similarity scores, which may be acceptable depending on specific use cases and RAG methods.",
            "score": 0.6321452844167325,
            "section_title": "abstract",
            "char_start_offset": 2,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "276667096",
            "title": "Exploring RAG Solutions for a Specific Language: Albanian",
            "text": "Retrieval-augmented generation (RAG) is an advanced method for enhancing the output of generative AI models by incorporating factual information from external resources [4]. This approach addresses a key limitation of large language models (LLMs): their inability to access real-time or domain-specific knowledge without additional training. \n\nLLMs, much like neural networks in the human brain, function based on their parameters, which represent patterns in language usage derived from extensive training datasets. These parameters allow LLMs to generate coherent responses quickly and efficiently, making them well-suited for general-purpose queries. However, they often fall short when tasked with providing detailed or up-to-date information on specialized topics. For example, while an LLM can summarize general knowledge, it struggles to answer context-specific questions about a newly provided document. \n\nRAG solves this problem by enabling LLMs to reference external sources dynamically during query processing. Much like footnotes in a research paper, these sources can be cited, ensuring transparency and trust. By offering verifiable facts and reducing errors-often called \"hallucinations\" in AI-the RAG approach improves the reliability and accuracy of responses. \n\nOne notable advantage of RAG is its efficiency. Instead of retraining a model with additional datasets, RAG allows developers to connect external resources to LLMs seamlessly. This lightweight implementation is not only faster but also more cost-effective, often requiring minimal code to integrate new data sources. \n\nRAG also expands the potential of AI applications across industries. For instance, integrating a medical database with an LLM could create a valuable assistant for healthcare professionals, while linking financial data could support analysts in making informed decisions. Similarly, businesses can enhance customer support, training, and productivity by converting internal documents, logs, or multimedia content into accessible knowledge bases. The concept of retrieval-augmented techniques can be traced [5] (Fig. 1).",
            "score": 0.6320515675890765,
            "section_title": "RAG solutions",
            "char_start_offset": 3203,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 341
                },
                {
                    "start": 344,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1596
                },
                {
                    "start": 1599,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2118
                }
            ],
            "ref_mentions": [
                {
                    "start": 2105,
                    "end": 2108,
                    "matchedPaperCorpusId": "267733899"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64013671875
        },
        {
            "corpus_id": "271924151",
            "title": "Hierarchical Retrieval-Augmented Generation Model with Rethink for Multi-hop Question Answering",
            "text": "Retrieval-Augmented Generation (RAG) is a current research hotspot for Multi-hop QA tasks [10]. It can integrate the internal knowledge of the model with the external knowledge retrieved. LLM can retrieve external content through RAG to expand their knowledge base, thereby improving their ability to generate accurate and contextually relevant responses. Historically, various studies attempt to adapt the use of generative models to improve their performance. For instance, REPLUG [17] uses different retrieved content to generate corresponding answers and then combine them. Self-Rag [2] fine-tunes a generation model to simultaneously produce answers along with relevance, support, and usefulness scores. Concurrently, several methods for multi-hop QA emphasize the content and timing of retrieval. Self-Ask [15] lets the model generate sub-questions and queries, and continuously alternate between retrieval and generation. PROMPTAGATOR [4], Take a step back [32] focus on abstracting high-level concepts and utilizing LLMs for prompt-based query generation. Additionally, the confidence-based method, FLARE [8], generates queries using low-confidence tokens. However, most studies directly feed the retrieval content into the generation model, ignoring the evaluation and processing of the retrieval content. Unlike them, HiRAG highlights the importance of verifying retrieved content and adjusts the retriever to enhance the relevance of results when the quality of the retrieved information is subpar.",
            "score": 0.6318742367777304,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7386,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 187
                },
                {
                    "start": 188,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 802
                },
                {
                    "start": 803,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1314
                },
                {
                    "start": 1315,
                    "end": 1509
                }
            ],
            "ref_mentions": [
                {
                    "start": 90,
                    "end": 94,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85546875
        },
        {
            "corpus_id": "269137180",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "text": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "score": 0.6286765717479403,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77685546875
        },
        {
            "corpus_id": "269502216",
            "title": "On the Evaluation of Machine-Generated Reports",
            "text": "Retrieval-Augmented Generation.Early retrieval augmented generation systems have been evaluated using task-specific metrics on end-to-end tasks.For example, in the context of question answering, exact match and  1 metrics have been used [30,41].For summarization, ROUGE and BERTScore on reference summaries are common [26].These approaches have two limitations: they only measure ability to complete end tasks, and thus cannot assess intermediate stages or evaluate generation across multiple dimensions; and they are not well-suited to capture failures that can be introduced by current generative models [27].\n\nMore recently, techniques have proposed to more holistically evaluate RAG systems.Gienapp et al. [25] introduce a theoretical framework for evaluating ad hoc generative retrieval.Chen et al. [11] focus on robustness of RAG systems against various perturbations.Thakur et al. [82] benchmark hallucinations and the ability of RAG systems to identify relevant information for 18 languages.Others have introduced benchmarks to measure the ability of RAG systems to provide citations [6,23,53,90].While not specifically  designed for RAG applications, metrics designed to evaluate factuality (e.g., FactScore [58]) or faithful manipulation of long inputs (e.g., BooookScore [10]) can complement application-specific evaluation frameworks.\n\nMost approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance).Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73].Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74].",
            "score": 0.6273387695234373,
            "section_title": "3.2.4",
            "char_start_offset": 19611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 144
                },
                {
                    "start": 144,
                    "end": 245
                },
                {
                    "start": 245,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 611
                },
                {
                    "start": 613,
                    "end": 695
                },
                {
                    "start": 695,
                    "end": 792
                },
                {
                    "start": 792,
                    "end": 874
                },
                {
                    "start": 874,
                    "end": 999
                },
                {
                    "start": 999,
                    "end": 1105
                },
                {
                    "start": 1105,
                    "end": 1346
                },
                {
                    "start": 1348,
                    "end": 1527
                },
                {
                    "start": 1527,
                    "end": 1657
                },
                {
                    "start": 1657,
                    "end": 1777
                }
            ],
            "ref_mentions": [
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 318,
                    "end": 322,
                    "matchedPaperCorpusId": "258865156"
                },
                {
                    "start": 1101,
                    "end": 1104,
                    "matchedPaperCorpusId": "258587884"
                },
                {
                    "start": 1772,
                    "end": 1776,
                    "matchedPaperCorpusId": "238207962"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "Retrieval-Augmented Generation (RAG) combines LLMs with a retrieval mechanism that allows the model to access external data sources in real-time, making it suitable for scenarios requiring up-to-date or frequently changing information. RAG systems are valuable for handling vast knowledge bases, where embedding all the information directly into the model would be impractical or impossible. \n\nAdvantages: RAG is ideal for applications that require access to dynamic information, ensuring responses are grounded in real-time data and minimizing hallucinations. It also provides transparency, as the source of the retrieved information can be linked directly. \n\nDrawbacks: RAG requires complex infrastructure, including vector databases and effective retrieval pipelines, and can be resource-intensive during inference.",
            "score": 0.6266058143161592,
            "section_title": "RAG: Dynamic Information and Large Knowledge Bases",
            "char_start_offset": 4977,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 818
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.436767578125
        },
        {
            "corpus_id": "270688478",
            "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is an innovative approach that enhances the capabilities of Large Language Models (LLMs) by integrating retrieval mechanisms into the generative process.At its core, RAG operates by retrieving relevant information from a vast corpus of data and then generating coherent and contextually enriched responses based on this retrieved information.This dual process not only improves the accuracy and relevance of the generated content but also addresses some of the inherent limitations of standalone generative models, such as hallucinations (Huang et al., 2023) and context drift (Wang et al., 2022).The significance of RAG in natural language processing and artificial intelligence cannot be overstated.As the demand for more sophisticated and context-aware AI systems grows, the ability to generate information that is both accurate and contextually relevant becomes crucial (Gao et al., 2024).RAG achieves this by leveraging the vast amount of information available, ensuring that the outputs of the models are informed by up-todate and contextually appropriate data.This has profound implications for various applications, including conversational AI, information retrieval, and automated content generation (Shuster et al., 2021;Wang et al., 2024).Furthermore, RAG represents a paradigm shift in how we think about and utilize LLMs.Instead of relying solely on the generative power of these models, RAG harnesses the complementary strengths of retrieval systems.This synergy enables the creation of AI systems that are not only more knowledgeable but also more reliable and versatile in their applications (Izacard and Grave, 2021;Zhu et al., 2024).\n\nLLMs are the key component in RAG systems.They are initially pre-trained on the task of next token prediction (Radford et al., 2018), where the LLM learns to predict the next word in a sequence based on the context provided by the preceding words.This extensive pre-training phase involves processing vast amounts of text data, enabling the model to acquire a broad understanding of language, syntax, semantics, and general knowledge.We call this the \"base\" version.Following this pre-training phase, LLMs typically undergo two stages of refinement to enhance their performance and usability, whose output we call the \"instruct\".",
            "score": 0.6263927421517209,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 400
                },
                {
                    "start": 400,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 759
                },
                {
                    "start": 759,
                    "end": 951
                },
                {
                    "start": 951,
                    "end": 1125
                },
                {
                    "start": 1125,
                    "end": 1308
                },
                {
                    "start": 1308,
                    "end": 1392
                },
                {
                    "start": 1392,
                    "end": 1522
                },
                {
                    "start": 1522,
                    "end": 1709
                },
                {
                    "start": 1711,
                    "end": 1753
                },
                {
                    "start": 1753,
                    "end": 1958
                },
                {
                    "start": 1958,
                    "end": 2145
                },
                {
                    "start": 2145,
                    "end": 2177
                },
                {
                    "start": 2177,
                    "end": 2340
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78515625
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "Generative artificial intelligence (AI) has brought revolutionary innovations in various fields, including medicine. However, it also exhibits limitations. In response, retrieval-augmented generation (RAG) provides a potential solution, enabling models to generate more accurate contents by leveraging the retrieval of external knowledge. With the rapid advancement of generative AI, RAG can pave the way for connecting this transformative technology with medical applications and is expected to bring innovations in equity, reliability, and personalization to health care.",
            "score": 0.62518637970364,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6806640625
        },
        {
            "corpus_id": "271244968",
            "title": "NinjaLLM: Fast, Scalable and Cost-effective RAG using Amazon SageMaker and AWS Trainium and Inferentia2",
            "text": "Retrieval-augmented generation (RAG) techniques are widely used today to retrieve and present information in a conversational format. This paper presents a set of enhancements to traditional RAG techniques, focusing on large language models (LLMs) fine-tuned and hosted on AWS Trainium and Inferentia2 AI chips via SageMaker. These chips are characterized by their elasticity, affordability, and efficient performance for AI compute tasks. Besides enabling deployment on these chips, this work aims to improve tool usage, add citation capabilities, and mitigate the risks of hallucinations and unsafe responses due to context bias. We benchmark our RAG system's performance on the Natural Questions and HotPotQA datasets, achieving an accuracy of 62% and 59% respectively, exceeding other models such as DBRX and Mixtral Instruct.",
            "score": 0.6250698106385985,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.734375
        },
        {
            "corpus_id": "276774736",
            "title": "Explainable Depression Detection in Clinical Interviews with Personalized Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) enhances language models (LMs) by incorporating retrieved text passages into the input, leading to significant improvements in knowledge-intensive tasks (Guu et al., 2020;Lewis et al., 2020)  instruction-tuning LMs with a fixed number of retrieved passages or jointly pre-training a retriever and LM followed by few-shot fine-tuning (Luo et al., 2023;Izacard et al., 2022). Some approaches adaptively retrieve passages during generation (Jiang et al., 2023), while others, like Schick et al. (2023), train LMs to generate API calls for named entities. However, these improvements often come with trade-offs in runtime efficiency, robustness, and contextual relevance (Mallen et al., 2023;Shi et al., 2023). To address these challenges, recent work introduces methods like SELF-RAG, which enables on-demand retrieval and filters out irrelevant passages through self-reflection, enhancing robustness and control (Lin et al., 2024;Yoran et al., 2024). SELF-RAG (Asai et al., 2023) also evaluates the factuality and quality of the generated output without relying on external models during inference, making it more efficient and customizable. Additionally, other concurrent RAG methods, such as LATS (Zhou et al., 2023), explore ways to improve retrieval for specific tasks like question answering through tree search. \n\n3 Method",
            "score": 0.6233893235930679,
            "section_title": "Retrievel Augementated Generation",
            "char_start_offset": 6546,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 743
                },
                {
                    "start": 744,
                    "end": 985
                },
                {
                    "start": 986,
                    "end": 1176
                },
                {
                    "start": 1177,
                    "end": 1352
                },
                {
                    "start": 1355,
                    "end": 1363
                }
            ],
            "ref_mentions": [
                {
                    "start": 190,
                    "end": 208,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 208,
                    "end": 227,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 474,
                    "end": 494,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 515,
                    "end": 535,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 704,
                    "end": 725,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 725,
                    "end": 742,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 947,
                    "end": 965,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 965,
                    "end": 984,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "273229050",
            "title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG",
            "text": "This paper investigates the impact of increasing the number of retrieved passages on the performance of long-context LLMs in retrieval-augmented generation (RAG) systems. Contrary to expectations, we observe that performance initially improve but then degrade as more passages are included. This phenomenon is attributed to the detrimental influence of retrieved \"hard negatives\". To mitigate this issue, we propose and evaluate three solutions: training-free retrieval reordering, RAG-specific implicit LLM fine-tuning, and RAG-oriented LLM fine-tuning with intermediate reasoning. A systematic analysis of the training-based methods explores the effects of data distribution, retriever for training, and training context length. Interesting future directions include exploring (automated) position optimization with more advanced retrieval ordering methods, and fine-tuning the LLMs for RAG with more fine-grained and multi-step reasoning chains. (2) Contriever is more similar to BM25, while bge is more similar to e5 (since their curves are closer respectively).",
            "score": 0.6225284965031941,
            "section_title": "Conclusions",
            "char_start_offset": 29954,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 948
                },
                {
                    "start": 949,
                    "end": 1066
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "270062436",
            "title": "SynthAI: A Multi Agent Generative AI Framework for Automated Modular HLS Design Generation",
            "text": "Another technique we use in SynthAI is Retrieval Augmented Generation (RAG) [11]- [13], a method that augments LLMs knowledge by incorporating external knowledge retrieval into their generative process. In practice, RAG involves constructing a vector database from semantic data. This database acts as a search tool for the LLM, enabling it to supplement its inherent knowledge (embedded in its parameters) with external, task-specific information. RAG allows us to integrate diverse resources like textbooks and coding best practices and other domain-specific knowledge.",
            "score": 0.6217566627180848,
            "section_title": "C. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 7784,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 448
                },
                {
                    "start": 449,
                    "end": 571
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.625
        },
        {
            "corpus_id": "273654156",
            "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications. However, with the comprehensive application of RAG, more and more problems and limitations have been identified, thus urgently requiring further fundamental exploration to improve current RAG frameworks. This workshop aims to explore in depth how to conduct refined and reliable RAG for downstream AI tasks. To this end, we propose to organize the first R3AG workshop at SIGIR-AP 2024 to call for participants to re-examine and formulate the basic principles and practical implementation of refined and reliable RAG. The workshop serves as a platform for both academia and industry researchers to conduct discussions, share insights, and foster research to build the next generation of RAG systems. Participants will engage in discussions and presentations focusing on fundamental challenges, cutting-edge research, and potential pathways to improve RAG. At the end of the workshop, we aim to have a clearer understanding of how to improve the reliability and applicability of RAG with more robust information retrieval and language generation.",
            "score": 0.6216307702437097,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89892578125
        },
        {
            "corpus_id": "277621182",
            "title": "Don't Lag, RAG: Training-Free Adversarial Detection Using RAG",
            "text": "Retrieval-Augmented Generation (RAG) integrates external knowledge into a generative model to improve both its generative capacity and semantic coherence. Formally, given a query Q, the model retrieves the top-k most relevant documents or embeddings R k from a database D: \n\nwhere S(\u2022, \u2022) is a similarity function. The query Q is then combined with R k within a generative function: \n\nIn our approach, this retrieval phase facilitates access to known adversarial patches, thereby enabling a more robust generative reasoning process. By incorporating historical data on diverse attack patterns, RAG-based defenses can dynamically adapt to novel threats while sustaining high efficacy against existing adversaries.",
            "score": 0.6204637049146788,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 9168,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 272
                },
                {
                    "start": 275,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 532
                },
                {
                    "start": 533,
                    "end": 712
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "266999736",
            "title": "Reinforcement Learning for Optimizing RAG for Domain Chatbots",
            "text": "With recent advancements in Generative AI and LLMs, the Retrieval-Augmented Generation (RAG) (Lewis et al. 2021) approach has emerged as the preferred strategy for contex-tual question answering. RAG pipeline consists of a retrieval model followed by an LLM to generate the answer. Different approaches have been experimented with to improve components of the RAG in terms of accuracy and minimize hallucinations in answer generation. Khatry et al. (kha 2023) proposed a low-rank residual adaptation approach with the pretrained embedding model to improve the retrieval model. It was shown to lead to improved task-specific retrieval as compared to a general-purpose embeddings-based baseline. Instead of using an interleaved retrieval and generation, Shao et al. (Shao et al. 2023) proposed an iterative retrieval and generation approach where the current model output acts as an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Li et al. (Li et al. 2022) extensively surveys recent RAG-based approaches. \n\nRL has been experimented with to improve RAG. Bacciu et al. (Bacciu et al. 2023) propose an RL-based approach to train an efficient retriever model to search for relevant information in an arbitrarily large database. Once this set of relevant data has been retrieved, it is forwarded to the API-based LLM to generate the answer. In particular, the authors show that RL helps reduce hallucinations by minimizing the number of damaging documents returned by the retriever. Self-RAG (Asai et al. 2023) trains a single LLM that adaptively retrieves passages and generates and reflects on retrieved passages and their generations using reflection tokens. In our work, we assume we do not have access to the gradients of the retrieval model and LLM. We only train a policy model that resides external to RAG. \n\nGPT-4 is observed to provide human-level accuracy for automated evaluation tasks. Hack at al. (Hackl et al. 2023) investigated the consistency of feedback ratings generated by GPT-4 across multiple iterations, time spans, and stylistic variations.",
            "score": 0.6186304501305446,
            "section_title": "Related works",
            "char_start_offset": 5694,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 281
                },
                {
                    "start": 282,
                    "end": 434
                },
                {
                    "start": 435,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1083
                },
                {
                    "start": 1086,
                    "end": 1131
                },
                {
                    "start": 1132,
                    "end": 1302
                },
                {
                    "start": 1303,
                    "end": 1414
                },
                {
                    "start": 1415,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1735
                },
                {
                    "start": 1736,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 1888
                },
                {
                    "start": 1891,
                    "end": 1972
                },
                {
                    "start": 1973,
                    "end": 2138
                }
            ],
            "ref_mentions": [
                {
                    "start": 764,
                    "end": 782,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8818359375
        },
        {
            "corpus_id": "278714952",
            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
            "text": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response.",
            "score": 0.6170570174488293,
            "section_title": "Methodology",
            "char_start_offset": 8999,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 403
                },
                {
                    "start": 404,
                    "end": 632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.95068359375
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "Generative artificial intelligence (AI) has recently attracted widespread attention across various fields, including the GPT 1,2 and LLaMA [3][4][5] series for text generation; DALL-E 6 for image generation; as well as Sora 7 for video generation.In medicine, generative AI holds tremendous potential for applications in consulting, diagnosis, treatment, management, and education 8,9 .Additionally, the utilization of generative AI could enhance the quality of health services for patients while alleviating the workload for clinicians [9][10][11] .\n\nDespite this, we must consider the inherent limitations of generative AI models, which include susceptibility to biases from pre-training data 12 , lack of transparency, the potential to generate incorrect content, difficulty in maintaining up-to-date knowledge, among others 8 .For instance, large language models were shown to generate biased responses by adopting outdated race-based equations to estimate renal function 13 .In the process of image generation, biases related to gender, skin tone, and geo-cultural factors have been observed 14 .Similarly, for downstream tasks such as question answering and text summarization, the generated content is often factually inconsistent and lacks evidence for verification 15 .Moreover, due to their static knowledge and inability to access external data, generative AI models are unable to provide up to date clinical advice for physicians or effective personalized health management for patients 16 .\n\nIn tackling these challenges, retrieval-augmented generation (RAG) may provide a solution 17,18 .By providing models access to external data, RAG is capable of enhancing the accuracy of generated content.Specifically, a typical RAG framework consists of three parts (Figure 1): indexing, retrieval, and generation.In the indexing stage, external data is split into chunks, encoded into vectors, and stored into a vector database.In the retrieval stage, the user's query is encoded into a vector representation, and then the most relevant information is retrieved through similarity calculations between the query and the information in the vector database.In the generation stage, both the user's query and the retrieved relevant information are prompted to the model to generate content.",
            "score": 0.6153680652838543,
            "section_title": "Main Text",
            "char_start_offset": 12,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 386
                },
                {
                    "start": 386,
                    "end": 550
                },
                {
                    "start": 552,
                    "end": 831
                },
                {
                    "start": 831,
                    "end": 980
                },
                {
                    "start": 980,
                    "end": 1101
                },
                {
                    "start": 1101,
                    "end": 1278
                },
                {
                    "start": 1278,
                    "end": 1503
                },
                {
                    "start": 1505,
                    "end": 1602
                },
                {
                    "start": 1602,
                    "end": 1709
                },
                {
                    "start": 1709,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1934
                },
                {
                    "start": 1934,
                    "end": 2161
                },
                {
                    "start": 2161,
                    "end": 2293
                }
            ],
            "ref_mentions": [
                {
                    "start": 381,
                    "end": 383,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 383,
                    "end": 384,
                    "matchedPaperCorpusId": "260230800"
                },
                {
                    "start": 537,
                    "end": 540,
                    "matchedPaperCorpusId": "260230800"
                },
                {
                    "start": 540,
                    "end": 544,
                    "matchedPaperCorpusId": "268854975"
                },
                {
                    "start": 828,
                    "end": 829,
                    "matchedPaperCorpusId": "259947046"
                },
                {
                    "start": 976,
                    "end": 978,
                    "matchedPaperCorpusId": "264378028"
                },
                {
                    "start": 1499,
                    "end": 1501,
                    "matchedPaperCorpusId": "269348048"
                },
                {
                    "start": 1595,
                    "end": 1598,
                    "matchedPaperCorpusId": "269327466"
                },
                {
                    "start": 1598,
                    "end": 1600,
                    "matchedPaperCorpusId": "258740478"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69140625
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "Large language models (LLMs) excel at generating human like responses, but base AI models can't keep up with the constantly evolving information within dynamic sectors. They rely on static training data, leading to outdated or incomplete answers. Thus they often lack transparency and accuracy in high stakes enhancing the model's ability to respond. Fig. 1: Architecture of Retrieval Augmented Generation(RAG) system.",
            "score": 0.6151337662698266,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 246
                },
                {
                    "start": 247,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 418
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.24072265625
        },
        {
            "corpus_id": "272463832",
            "title": "VERA: Validation and Evaluation of Retrieval-Augmented Systems",
            "text": "The increasing use of Retrieval-Augmented Generation (RAG) systems in various applications necessitates stringent protocols to ensure RAG systems accuracy, safety, and alignment with user intentions. In this paper, we introduce VERA (Validation and Evaluation of Retrieval-Augmented Systems), a framework designed to enhance the transparency and reliability of outputs from large language models (LLMs) that utilize retrieved information. VERA improves the way we evaluate RAG systems in two important ways: (1) it introduces a cross-encoder based mechanism that encompasses a set of multidimensional metrics into a single comprehensive ranking score, addressing the challenge of prioritizing individual metrics, and (2) it employs Bootstrap statistics on LLM-based metrics across the document repository to establish confidence bounds, ensuring the repositorys topical coverage and improving the overall reliability of retrieval systems. Through several use cases, we demonstrate how VERA can strengthen decision-making processes and trust in AI applications. Our findings not only contribute to the theoretical understanding of LLM-based RAG evaluation metric but also promote the practical implementation of responsible AI systems, marking a significant advancement in the development of reliable and transparent generative AI technologies.",
            "score": 0.6148659032410473,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "277272240",
            "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
            "text": "Retrieval-augmented generation (RAG) has emerged as a pivotal technique in artificial intelligence (AI), particularly in enhancing the capabilities of large language models (LLMs) by enabling access to external, reliable, and up-to-date knowledge sources. In the context of AI-Generated Content (AIGC), RAG has proven invaluable by augmenting model outputs with supplementary, relevant information, thus improving their quality. Recently, the potential of RAG has extended beyond natural language processing, with emerging methods integrating retrieval-augmented strategies into the computer vision (CV) domain. These approaches aim to address the limitations of relying solely on internal model knowledge by incorporating authoritative external knowledge bases, thereby improving both the understanding and generation capabilities of vision models. This survey provides a comprehensive review of the current state of retrieval-augmented techniques in CV, focusing on two main areas: (I) visual understanding and (II) visual generation. In the realm of visual understanding, we systematically review tasks ranging from basic image recognition to complex applications such as medical report generation and multimodal question answering. For visual content generation, we examine the application of RAG in tasks related to image, video, and 3D generation. Furthermore, we explore recent advancements in RAG for embodied AI, with a particular focus on applications in planning, task execution, multimodal perception, interaction, and specialized domains. Given that the integration of retrieval-augmented techniques in CV is still in its early stages, we also highlight the key limitations of current approaches and propose future research directions to drive the development of this promising area.",
            "score": 0.6147529533070781,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "270285974",
            "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
            "text": "Retrieval-Augmented Generation (RAG) [212] is a machine learning technique that combines the strengths of retrievalbased and generative models to enhance the quality and diversity of generated text.This approach has gained significant attention in NLP tasks, particularly in areas like conversational AI, question-answering systems, and text summarization.\n\n1) General Algorithm: In general, a typical RAG system should consist of the following components:\n\n\u2022 Retrieval Component: The system starts by retrieving relevant information from a large database or corpus.This could involve indexing and efficiently searching through past conversations, documents, or web pages based on the input query.Techniques like TF-IDF [213], BM25 [214], or more advanced retrieval methods can be used for this component.\u2022 Generation Component: Once the relevant information is retrieved, a generative model (e.g., GPT-3, T5, or BERT) can use this information as context to generate a response or output [215].The generation process is augmented by conditioning the model on the retrieved data, allowing it to generate more informed, contextually accurate, and diverse responses rather than generating from scratch.\u2022 Component Integration: The integration ways of these two components can vary [212].Firstly, the retrieved information might be concatenated with the input prompt and directly fed into the generator.Others might use a more sophisticated fusion mechanism, where the retrieval and generation models interact in multiple steps, refining the context and the generated output iteratively.By leveraging external knowledge, the generated text of RAG is more likely to be contextually appropriate and accurate.Retrieval of varied sources can introduce more diversity in the generated outputs, reducing the likelihood of repetitive or generic responses [216].Retrieval models can quickly narrow down the scope of information needed, which can make the generation process more efficient compared to exploring the entire knowledge space.Incorporating specific retrieved information can provide more control over the content and tone of the generated text, aligning it better with user expectations or specific requirements.\n\n2) Specific RAG Algorithm: Clinfo.ai[51].Clinfo.ai is an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature.The information retrieval and summarization tasks are applied to evaluate the retrieval-augmented LLM systems.\n\nAlmanac [52].",
            "score": 0.6146131990687451,
            "section_title": "D. Retrieval-Augmented Generation for Med-LLMs",
            "char_start_offset": 58486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 356
                },
                {
                    "start": 358,
                    "end": 456
                },
                {
                    "start": 458,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 805
                },
                {
                    "start": 805,
                    "end": 994
                },
                {
                    "start": 994,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1284
                },
                {
                    "start": 1284,
                    "end": 1399
                },
                {
                    "start": 1399,
                    "end": 1583
                },
                {
                    "start": 1583,
                    "end": 1702
                },
                {
                    "start": 1702,
                    "end": 1850
                },
                {
                    "start": 1850,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2212
                },
                {
                    "start": 2214,
                    "end": 2250
                },
                {
                    "start": 2250,
                    "end": 2255
                },
                {
                    "start": 2255,
                    "end": 2375
                },
                {
                    "start": 2375,
                    "end": 2485
                },
                {
                    "start": 2487,
                    "end": 2500
                }
            ],
            "ref_mentions": [
                {
                    "start": 720,
                    "end": 725,
                    "matchedPaperCorpusId": "14638345"
                },
                {
                    "start": 732,
                    "end": 737,
                    "matchedPaperCorpusId": "207220720"
                },
                {
                    "start": 1844,
                    "end": 1849,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2250,
                    "end": 2254,
                    "matchedPaperCorpusId": "264487188"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6328125
        },
        {
            "corpus_id": "261531346",
            "title": "A Study on the Implementation of Generative AI Services Using an Enterprise Data-Based LLM Application Architecture",
            "text": "This study presents a method for implementing generative AI services by utilizing the Large Language Models (LLM) application architecture. With recent advancements in generative AI technology, LLMs have gained prominence across various domains. In this context, the research addresses the challenge of information scarcity and proposes specific remedies by harnessing LLM capabilities. The investigation delves into strategies for mitigating the issue of inadequate data, offering tailored solutions. The study delves into the efficacy of employing fine-tuning techniques and direct document integration to alleviate data insufficiency. A significant contribution of this work is the development of a Retrieval-Augmented Generation (RAG) model, which tackles the aforementioned challenges. The RAG model is carefully designed to enhance information storage and retrieval processes, ensuring improved content generation. The research elucidates the key phases of the information storage and retrieval methodology underpinned by the RAG model. A comprehensive analysis of these steps is undertaken, emphasizing their significance in addressing the scarcity of data. The study highlights the efficacy of the proposed method, showcasing its applicability through illustrative instances. By implementing the RAG model for information storage and retrieval, the research not only contributes to a deeper comprehension of generative AI technology but also facilitates its practical usability within enterprises utilizing LLMs. This work holds substantial value in advancing the field of generative AI, offering insights into enhancing data-driven content generation and fostering active utilization of LLM-based services within corporate settings.",
            "score": 0.6143236415574201,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85693359375
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
            "score": 0.6140395893529735,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "276449952",
            "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
            "text": "Retrieval-Augmented Generation (RAG) \n\nRetrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. [22] introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval [10,36] and enhancing generation capabilities [2,17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself. Despite these significant advancements, the challenge of seamlessly integrating dynamic historical context in RAG models for multi-turn dialogues remains an elusive goal. Though achieve remarkable progress, most existing approaches continue to depend predominantly on static knowledge bases and do not adequately address the need to capture the evolving contextual nuances within conversations. This gap propels the development of DH-RAG in this paper, aimed at more effectively incorporating both static external knowledge and the transient context prevalent in ongoing dialogues, thereby enhancing the quality and coherence of multi-turn dialogue interactions.",
            "score": 0.6139031918937866,
            "section_title": "Related Work",
            "char_start_offset": 3838,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1076
                },
                {
                    "start": 1077,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1568
                }
            ],
            "ref_mentions": [
                {
                    "start": 237,
                    "end": 241,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 470,
                    "end": 474,
                    "matchedPaperCorpusId": "258479968"
                },
                {
                    "start": 516,
                    "end": 519,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90234375
        },
        {
            "corpus_id": "259064131",
            "title": "TimelineQA: A Benchmark for Question Answering over Timelines",
            "text": "Furthermore, by fine-tuning the retrievers on the episodic data, we get a significant boost in performance for both extractive and rag setups, indicating that the QA systems do not generalize well to episodic data, and that improving retrieval is crucial to getting good performance from these models, particularly for RAG. After fine-tuning, the generative model performance still lags behind the extractive setup.",
            "score": 0.6137607124408488,
            "section_title": "Atomic QA",
            "char_start_offset": 27036,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 323
                },
                {
                    "start": 324,
                    "end": 415
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.775390625
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG), a cutting-edge AI technique, has achieved remarkable success across various applications, including recommendation, molecule generation, protein representation, and software engineering, owing to the potent capabilities of retrieval in providing supplementary information to enhance generation performance.Recently, increasing efforts have been made to alleviate the limitations of large language models (LLMs), such as hallucination and out-of-date internal knowledge, by leveraging retrieval to provide the latest auxiliary information and teaching LLMs to harness the retrieved external knowledge.With the rapid advancements in retrieval-augmented large language models (RA-LLMs), there is a pressing need for a comprehensive and systematic overview.To bridge this gap, in this paper, we comprehensively review the RA-LLMs from the perspectives of morel architecture, training strategy, and application area, providing researchers with an in-depth understanding.Moreover, since the studies of RA-LLMs are still in the early stage, we also discuss the current limitations and several potential research directions for future research.",
            "score": 0.6124046707140443,
            "section_title": "CONCLUSION",
            "char_start_offset": 60360,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 344
                },
                {
                    "start": 344,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 791
                },
                {
                    "start": 791,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69482421875
        },
        {
            "corpus_id": "266977237",
            "title": "WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge",
            "text": "Retrieval-Augmented Generation (RAG) enhances Language Models by incorporating retrieved text, significantly improving performance in knowledgebased tasks, applicable in both fine-tuned and offthe-shelf scenarios (Gao et al., 2023;Gupta et al., 2024). Traditional RAG (Lewis et al., 2020), also known as Naive RAG, incorporates retrieval content to aid generation but faces key challenges: 1) varying retrieval quality, 2) generation of responses prone to inaccuracies, and 3) difficulties in coherently integrating retrieved-context with current tasks. To overcome the limitations of Naive RAG, advanced methods introduce more contextually rich information during inference. The DSP frame-  work (Khattab et al., 2022) facilitates an intricate exchange between frozen LMs and retrieval models, improving context richness, while PKG (Luo et al., 2023) allows LLMs to retrieve relevant information for complex tasks without altering their parameters. The working mechanism of WisdoM is similar to RAG, but different at the following aspects: \u2460 WisdoM utilizes LVLM to generate world knowledge to provide coherent and accurate context rather than retrieval, \u2461 WisdoM incorporates a contextual fusion mechanism to diminish noise within the context. For additional experimental analysis and discussion, please refer to \u00a7 5.3.",
            "score": 0.6122173779812426,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1245
                },
                {
                    "start": 1246,
                    "end": 1321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "276782318",
            "title": "From Architectural Sketch to Conceptual Representation: Using Structure-Aware Diffusion Model to Generate Renderings of School Buildings",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) provides an efficient approach to guide and improve the generation process using relevant information from an external database without training network parameters. In image generation tasks, RAG enhances the performance of generative models by improving the quality and accuracy of image synthesis (Sheynin et al., 2022;Golatkar et al., 2024). The core of RAG is its ability to extract relevant information from a large external database, guiding the generation process and greatly enhancing both the quality and accuracy of image generation. Our work employs RAG to convert a rough input sketch into a detailed sketch. The RAG process enhances rough input sketches by retrieving and integrating architectural components, such as windows and doors, ensuring realistic representations during the conceptual design stage.",
            "score": 0.6118652170961784,
            "section_title": "RETRIEVAL-AUGMENTED GENERATION",
            "char_start_offset": 5112,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 878
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 379,
                    "end": 401,
                    "matchedPaperCorpusId": "268732879"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at: https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
            "score": 0.6095255525481961,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74365234375
        },
        {
            "corpus_id": "271710111",
            "title": "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation",
            "text": "Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.",
            "score": 0.6090856031197531,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85302734375
        },
        {
            "corpus_id": "273403870",
            "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
            "text": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data. To address these challenges, we propose REFINE, a novel technique that generates synthetic data from available documents and then uses a model fusion approach to fine-tune embeddings for improved retrieval performance in new domains, while preserving out-of-domain capability. We conducted experiments on the two public datasets: SQUAD and RAG-12000 and a proprietary TOURISM dataset. Results demonstrate that even the standard fine-tuning with the proposed data augmentation technique outperforms the vanilla pretrained model. Furthermore, when combined with model fusion, the proposed approach achieves superior performance, with a 5.76% improvement in recall on the TOURISM dataset, and 6.58 % and 0.32% enhancement on SQUAD and RAG-12000 respectively.",
            "score": 0.608204555063612,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.900390625
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "F.1. Retrieval-Augmented Generation (RAG) \n\nAdvances in Retrieval-Augmented Generation (RAG) continue to extend the capabilities of Large Language Models (LLMs) in domain adaptation, efficiency, and longcontext reasoning. RAFT (Zhang et al., 2024c) improves factual accuracy by fine-tuning models to ignore irrelevant retrievals and cite only the most pertinent sources. CoRAG (Wang et al., 2025) enhances multi-hop reasoning through iterative retrieval, refining queries based on intermediate results rather than relying on a single retrieval step. DRAGIN (Su et al.) introduces dynamic retrieval by detecting real-time information needs using model uncertainty and self-attention cues, enabling context-sensitive query formulation during generation. RAPID (Chen et al.,  (Qian et al., 2024) integrates external retrieval with a cognitive memory system, recording episodic interactions and distilling them into semantic memory to improve retrieval relevance and consistency. Speculative RAG (Wang et al., 2024c) reduces latency and enhances comprehension by generating draft responses using a small model and verifying them with a larger model. CAG (Chan et al., 2024) addresses retrieval latency by preloading cached documents into extended context windows, bypassing real-time retrieval altogether. Parametric RAG (Su et al., 2025) replaces input-context retrieval with document parameterization, temporarily updating LLM weights during inference to embed external knowledge directly, thereby streamlining the retrieve-update-generate process.",
            "score": 0.6079287916881226,
            "section_title": "F. Related Work",
            "char_start_offset": 138273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 4
                },
                {
                    "start": 5,
                    "end": 41
                },
                {
                    "start": 44,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1301
                },
                {
                    "start": 1302,
                    "end": 1546
                }
            ],
            "ref_mentions": [
                {
                    "start": 227,
                    "end": 248,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "276408622",
            "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights",
            "text": "Retrieval-Augmented Generation (RAG) has been widely applied to enhance the performance of Large Language Models (LLMs) by retrieving relevant information from external sources, addressing the limitation of LLMs' restricted context windows, improving factual accuracy, and mitigating hallucinations (Fan et al., 2024;Gao et al., 2023). Most RAG systems primarily process text data by first splitting it into chunks (Finardi et al., 2024). When a query is received, RAG retrieves relevant chunks either through lexical search (Ram et al., 2023) or by computing semantic similarity (Karpukhin et al., 2020), embeddings both the query and text chunks into a shared vector space. Advanced techniques, such as pre-retrieval processing (Ma et al., 2023;Zheng et al., 2023a) and post-retrieval processing (Dong et al., 2024;Xu et al., 2023), as well as fine-tuning strategies (Li et al., 2023), have further enhanced RAG's effectiveness across various domains, including QA) (Yan et al., 2024), dialogue generation (Izacard et al., 2023), and text summarization (Jiang et al., 2023). \n\nSeveral studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;Chen et al., 2024;Es et al., 2023), such as multi-hop question answering (Tang and Yang, 2024), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks.",
            "score": 0.607153566759425,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 1076
                },
                {
                    "start": 1079,
                    "end": 1368
                },
                {
                    "start": 1369,
                    "end": 1508
                }
            ],
            "ref_mentions": [
                {
                    "start": 299,
                    "end": 317,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 525,
                    "end": 543,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1008,
                    "end": 1030,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1181,
                    "end": 1199,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1254,
                    "end": 1275,
                    "matchedPaperCorpusId": "263152125"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83837890625
        },
        {
            "corpus_id": "269484468",
            "title": "How Can I Improve? Using GPT to Highlight the Desired and Undesired Parts of Open-ended Responses",
            "text": "Additionally, while the prompting approach offers flexibility in testing different prompts to quickly gauge the model's capabilities on our task, its effectiveness heavily depends on the quality of the prompt design.As observed during our prompt engineering phase, inadequate prompts can lead to misleading outputs.\n\nOn the other hand, fine-tuning allows for deeper model customization by adjusting internal parameters to closely align with our task in identifying the components of praises from tutor responses, often resulting in superior performance measured by M-IOU scores, as observed in our study.Finetuning enables the GPT model to deeply integrate new knowledge and adjust its existing knowledge, better fitting the task requirements of identifying components of effort-and outcome-based praise.Despite these advantages, fine-tuning requires a substantial amount of relevant and high-quality data and significant computational resources.The data must be carefully annotated to guide the model effectively toward the desired behavior, which present a significant limitation if such data is scarce or difficult to collect.Additionally, finetuning involves updating the weights of a neural network based on a specific dataset, a process that can be resourceintensive and requires access to powerful hardware, especially for larger models.\n\nTo address some of these challenges and further enhance our highlighted feedback system, we are considering the integration of Retrieval-Augmented Generation (RAG).RAG combines the strengths of both retrieval and generation models to improve the performance of language models on specific tasks [35].RAG could enhance the performance of prompting LLMs by dynamically incorporating relevant external information into responses, providing more informed and contextually accurate outputs (e.g., [20]).Additionally, RAG can be integrated with the fine-tuning approach for providing highlighted feedback, potentially improving the model's accuracy in highlighting components of praise.This integration aims to create a model that not only leverages external data through RAG but also adapts more finely to specialized tasks through fine-tuning, demonstrating superior performance in contextually rich and dynamic environments.",
            "score": 0.6068597387112078,
            "section_title": "DISCUSSION",
            "char_start_offset": 42315,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 315
                },
                {
                    "start": 317,
                    "end": 604
                },
                {
                    "start": 604,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1344
                },
                {
                    "start": 1346,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1646
                },
                {
                    "start": 1646,
                    "end": 1844
                },
                {
                    "start": 1844,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2267
                }
            ],
            "ref_mentions": [
                {
                    "start": 1641,
                    "end": 1645,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69189453125
        },
        {
            "corpus_id": "269293007",
            "title": "A Survey on Efficient Inference for Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) [29] aims to improve the quality of LLMs' responses by incorporating external knowledge sources. RAG can be also viewed as a technique to improve the inference efficiency when handling a large amount of data. Instead of merging all information into an excessively long prompt, RAG only adds relevant retrieved information to the original prompt, ensuring that the model receives necessary information while reducing prompt length significantly. FLARE [30] uses predictions of upcoming sentences to proactively decide when and what information to retrieve. REPLUG [31] treats the LLM as a black box and augments it with a tuneable retrieval model. It prepends retrieved documents to the input for the frozen black-box LLM, and further utilizes the LLM to supervise the retrieval model. Self-RAG [32] enhances LLM's quality and factuality through retrieval and self-reflection. It introduces reflection tokens to make the LLM controllable during the inference phase.",
            "score": 0.6062885885869695,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 20663,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 592
                },
                {
                    "start": 593,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 821
                },
                {
                    "start": 822,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1001
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.716796875
        },
        {
            "corpus_id": "270560221",
            "title": "Retrieval-Augmented Feature Generation for Domain-Specific Classification",
            "text": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is a technique that integrates the information retrieval capabilities with the generative language models to enhance the performance of the final output.RAG effectively assists LLMs with tasks requiring extensive and specific domain knowledge (Zhang et al., 2024a;Huang and Huang, 2024).Given a domain-specific task, the LLMs can access a large external library, which might be a set of documents or knowledge related to the task (Hu and Lu, 2024).Then, the LLMs generate a query according to the task information and use it for searching based on the similarity between the query and the candidate documents.This approach helps the LLMs to produce responses that are more accurate, domain-relevant, and reduce hallucinations (Yang et al., 2024).However, the vanilla results of RAG might not be used for direct feature generation which requires additional processing to align with the existing features (Li et al., 2024).This is the main difference between the goal of general RAG and our proposed framework for feature generation.",
            "score": 0.6059017031720221,
            "section_title": "Retrieval Augmented Generation for Large Language Models",
            "char_start_offset": 7072,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 211,
                    "end": 345
                },
                {
                    "start": 345,
                    "end": 506
                },
                {
                    "start": 506,
                    "end": 667
                },
                {
                    "start": 667,
                    "end": 804
                },
                {
                    "start": 804,
                    "end": 979
                },
                {
                    "start": 979,
                    "end": 1089
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 784,
                    "end": 803,
                    "matchedPaperCorpusId": "259262077"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5302734375
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.If retrieval is not required,the model predicts the next output segment,as it does in a standard LM.If retrieval is needed,the model generates: a critique token to evaluate the retrieved passage's relevance,the next response segment,and a critique token to evaluate if the information in the response segment is supported by the passage.Finally,a new critique token evaluates the overall utility of the response.\n\n\"REPLUG: Retrieval-Augmented Black-Box Language Models [115]:\" This research explores how to optimize retrieval results in black-box language models,such as those that only expose APIs without revealing embeddings,using a retrieval-augmented approach.\n\n\"Atlas: Few-shot Learning with Retrieval Augmented Language Models [45]:\" The paper discusses methods for joint training of retrievers and language models,especially for knowledge-intensive tasks.\n\n\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end,an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG,fine-tuning,and their combina-",
            "score": 0.6046512278008267,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 145958,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 116,
                    "end": 216
                },
                {
                    "start": 216,
                    "end": 453
                },
                {
                    "start": 453,
                    "end": 528
                },
                {
                    "start": 530,
                    "end": 781
                },
                {
                    "start": 783,
                    "end": 979
                },
                {
                    "start": 981,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1547
                },
                {
                    "start": 1549,
                    "end": 1721
                }
            ],
            "ref_mentions": [
                {
                    "start": 1296,
                    "end": 1301,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.892578125
        },
        {
            "corpus_id": "273502291",
            "title": "Developing Retrieval Augmented Generation (RAG) based LLM Systems from PDFs: An Experience Report",
            "text": "Choosing between fine-tuning, using Retrieval Augmented Generation (RAG), or base models can be a challenging decision for practitioners. Each approach offers distinct advantages depending on the context and constraints of the use case. This section aims to outline the scenarios in which each method is most effective, providing a decision framework to guide practitioners in selecting the appropriate strategy.",
            "score": 0.6046456261008755,
            "section_title": "When to Use RAG: Considerations for Practitioners",
            "char_start_offset": 3141,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 138,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 412
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68359375
        },
        {
            "corpus_id": "271244634",
            "title": "Exploring Advanced Large Language Models with LLMsuite",
            "text": "Despite advancements in training, tuning, and alignment techniques, large language models (LLMs) like ChatGPT and Gemini face inherent challenges that cannot be addressed by training alone.One significant issue is the temporal limitation of their knowledge base.This knowledge cutoff renders the model's information outdated.LLMs also struggle with complex mathematical tasks.When prompted to perform arithmetic operations, models may provide incorrect results as they attempt to predict the next token rather than execute precise calculations.Furthermore, LLMs are prone to hallucinations, generating plausible but incorrect information.To overcome these limitations, integrating external data sources and applications at inference time proves essential.Retrieval Augmented Generation (RAG) [23] is a framework designed to connect LLMs to external data, thus updating their knowledge base and improving accuracy without costly retraining.By accessing current information from external databases, vector stores, or APIs, models can generate more relevant and precise responses.RAG involves a retriever component that encodes user queries and searches an external data source for relevant information.This augmented query is then processed by the LLM to produce a more accurate completion.For example, in legal applications, RAG can enhance discovery phases by querying a corpus of legal documents, thus retrieving pertinent information efficiently.Figure 1 illustrates the architecture of an LLM-powered application using LangChain, demonstrating how various components such as tools, agents, and memory modules are integrated to facilitate complex task execution.This framework not only updates the model's knowledge but also mitigates hallucinations by grounding responses in external data.Implementing RAG requires careful handling of context window limitations and efficient data retrieval using techniques like vector stores, which facilitate rapid semantic searches.In Figure 2, we illustrate the Retrieval-Augmented Generation (RAG) framework, detailing its components and operational workflow to enhance the accuracy and reliability of generative AI models.\n\nIn conclusion, while LLMs face challenges like knowledge cutoffs, mathematical errors, and hallucinations, integrating external data sources through frameworks like RAG [23] can significantly enhance their performance, making them more reliable and useful in various applications (see slides at Tutorial LLMs Part 1).",
            "score": 0.6043709283020372,
            "section_title": "Retrieval-Augmented Generation (RAG) Framework",
            "char_start_offset": 9052,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 262
                },
                {
                    "start": 262,
                    "end": 325
                },
                {
                    "start": 325,
                    "end": 376
                },
                {
                    "start": 376,
                    "end": 544
                },
                {
                    "start": 544,
                    "end": 638
                },
                {
                    "start": 638,
                    "end": 755
                },
                {
                    "start": 755,
                    "end": 939
                },
                {
                    "start": 939,
                    "end": 1077
                },
                {
                    "start": 1077,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1448
                },
                {
                    "start": 1448,
                    "end": 1664
                },
                {
                    "start": 1664,
                    "end": 1792
                },
                {
                    "start": 1792,
                    "end": 1972
                },
                {
                    "start": 1972,
                    "end": 2165
                },
                {
                    "start": 2167,
                    "end": 2484
                }
            ],
            "ref_mentions": [
                {
                    "start": 792,
                    "end": 796,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 2336,
                    "end": 2340,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.417724609375
        },
        {
            "corpus_id": "271915646",
            "title": "Xinyu: An Efficient LLM-based System for Commentary Generation",
            "text": "The domain of Natural Language Processing (NLP) has witnessed substantial progress [14,21,30,31,42], especially through the advent of Large Language Models (LLMs) [2,17,18,27,35]. These models show exceptional text generation proficiency, yielding high fluency and readability outputs [32,39]. Their ability to adapt to downstream tasks with minimal in-context examples is particularly noteworthy. To further augment the efficacy of LLMs in downstream tasks, two main methods have been identified: supervised fine-Tuning (SFT) and retrieval augmented generation (RAG). \n\nSupervised Fine-Tuning (SFT) entails the adaptation of an LLM to a specific downstream task. This process refines the model's parameters to align with the data distribution and task requirements, ensuring the model's behavior mirrors human behavior within the given domain. The topic of SFT has been extensively explored in numerous research. Ouyang et al. [18] pioneered the introduction of supervised fine-tuning and reinforcement learning to align language models with human intent. Zhou et al. [41] compiled a dataset of merely 1K examples for SFT, demonstrating that the success of SFT depends on the quality and diversity of data. \n\nRetrieval Augmented Generation (RAG) amalgamates LLMs with content retrieved from external databases. This approach offers a promising solution to the challenges encountered by LLMs, such as hallucination, outdated knowledge, and untraceable reasoning processes. The conventional RAG process encompasses indexing, retrieval, and generation [9,15]. RAG has been further enhanced by a range of innovative techniques: fine-tuning retrieval models to obtain precise semantic representations [11,28,33], reformulating queries to align with the semantic space of queries and documents [8,20,29], fine-tuning LLMs to harmonize the output of the retriever with the LLM's preference [10,22,34]. \n\nIn our work, we leverage the advances of both SFT and RAG to enhance the performance of the Xinyu.",
            "score": 0.6021700607511368,
            "section_title": "RELATED WORK 2.1 Large Language Models",
            "char_start_offset": 5295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 568
                },
                {
                    "start": 571,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 844
                },
                {
                    "start": 845,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1472
                },
                {
                    "start": 1473,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1895
                },
                {
                    "start": 1898,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 83,
                    "end": 87,
                    "matchedPaperCorpusId": "259950027"
                },
                {
                    "start": 90,
                    "end": 93,
                    "matchedPaperCorpusId": "226262321"
                },
                {
                    "start": 93,
                    "end": 96,
                    "matchedPaperCorpusId": "259858754"
                },
                {
                    "start": 96,
                    "end": 99,
                    "matchedPaperCorpusId": "254998782"
                },
                {
                    "start": 169,
                    "end": 172,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8447265625
        },
        {
            "corpus_id": "276249310",
            "title": "Optimizing Knowledge Integration in Retrieval-Augmented Generation with Self-Selection",
            "text": "In this section, we elaborate on the proposed Self-Selection framework for enhanced Retrieval-Augmented Generation (RAG). Before explaining our method, we first revisit two preliminary concepts, i.e. Large Language Model (LLM) and Retrieval-Augmented Generation (RAG). Then, we present the formulation of our Self-Selection framework with detailed notations. In order to strengthen the capabilities of the LLM in accurately generating and selecting responses, we develop a novel Self-Selection-RGP method, which is essentially fining tuning the LLM over a newly built Retrieval-Generation Preference (RGP) dataset.",
            "score": 0.6021093879530466,
            "section_title": "SELF-SELECTION FRAMEWORK",
            "char_start_offset": 6140,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 614
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.833984375
        },
        {
            "corpus_id": "270357690",
            "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
            "text": "Large Language Models (LLMs) transformed many machine learning tasks using in-context learning abilities.They achieved such accuracy by leveraging an increasing number of parameters, which in recent models have grown to hundreds of billions, making LLM training expensive in terms of both time and resources.It also comes with the danger of leaking confidential data into model weights [28,33,40].Additionally, continuous training through fine-tuning is necessary to keep LLMs up-to-date.Even using the newest data, LLMs display an ongoing problem of hallucinations [13,38,44] by providing factually incorrect information.Retrieval Augmented Generation (RAG) was proposed [11,18] in order to address these issues as well as others and make LLMs more trustworthy.\n\nThe key idea behind RAG is to enhance the generative model's capabilities by integrating a retrieval system that can fetch relevant documents or passages from a large corpus of data.In this setting, when a query is received, the retrieval system first identifies and retrieves pertinent information, which is fed into the generative model's context for a more accurate and relevant response.Instead of the model storing information within its weights, RAG effectively leverages external knowledge, reducing Despite all these advances, we observe that no existing RAG scheme or evaluation methodology explicitly targets an important class of problems that come with a high degree of multi-aspectuality.These are problems that require combining several (potentially many) significantly different aspects in a single query.As a simple illustrative example of such a query, consider the question \"What car did Alexander the Great drive?\", and assume that the queried model has not been trained on history.When using RAG, to answer this question accurately, one would retrieve two documents, one describing Alexander the Great and one outlining the history of car manufacturing.However, the embeddings of these two documents could be far away from each other in the embedding space.At the same time, such queries are common in different industry settings, as indicated by extensive discussions with our industry collaborators.Imagine a chemical processing plant experiencing an equipment accident.One could use an LLM to find the accident cause, which might require the retrieval of multiple, potentially confidential documents to provide the necessary context.",
            "score": 0.601337095725446,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 105,
                    "end": 308
                },
                {
                    "start": 308,
                    "end": 397
                },
                {
                    "start": 397,
                    "end": 488
                },
                {
                    "start": 488,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 762
                },
                {
                    "start": 764,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1155
                },
                {
                    "start": 1155,
                    "end": 1465
                },
                {
                    "start": 1465,
                    "end": 1584
                },
                {
                    "start": 1584,
                    "end": 1765
                },
                {
                    "start": 1765,
                    "end": 1937
                },
                {
                    "start": 1937,
                    "end": 2041
                },
                {
                    "start": 2041,
                    "end": 2185
                },
                {
                    "start": 2185,
                    "end": 2256
                },
                {
                    "start": 2256,
                    "end": 2420
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 390,
                    "matchedPaperCorpusId": "263311025"
                },
                {
                    "start": 676,
                    "end": 679,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6123046875
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability. However, directly applying LLM to RAG systems presents challenges. This may cause feature locality problems as massive parametric knowledge can hinder effective usage of global information across the corpus; for example, an LLM-based retriever often inputs document summaries instead of full documents. Moreover, various pre-trained tasks in LLMs introduce variance, further weakening performance as a retriever. To address these issues, we propose a novel two-stage fine-tuning architecture called Invar-RAG. In the retrieval stage, an LLM-based retriever is constructed by integrating LoRA-based representation learning to tackle feature locality issues. To enhance retrieval performance, we develop two patterns (invariant and variant patterns) and an invariance loss to reduce LLM variance. In the generation stage, a refined fine-tuning method is employed to improve LLM accuracy in generating answers based on retrieved information. Experimental results show that Invar-RAG significantly outperforms existing baselines across three open-domain question answering (ODQA) datasets. Code is available in the Supplementary Material for reproducibility.",
            "score": 0.6002065861164355,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89501953125
        },
        {
            "corpus_id": "265610005",
            "title": "Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models",
            "text": "Retrieval-augmented generation (RAG) for LLMs represents an important advancement in enhancing the generative capabilities of models by integrating external knowledge sources. Early works such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) introduced the foundational methodology for incorporating external documents into the generation process. Subsequent research expanded the retrieval-augmented paradigm to multi-modal contexts. Such as MuRAG (Chen et al., 2022) and REVEAL (Hu et al., 2022), which augment language generation with both textual and visual information from external sources. Furthermore, recent studies such as FiD-Light (Hofst\u00e4tter et al., 2022) and REPLUG (Shi et al., 2023) have focused on improving the efficiency and effectiveness of retrieval-augmented systems as well as exploring in-context retrieval-augmented mechanisms (Ram et al., 2023;de Jong et al., 2023).",
            "score": 0.5997674549268095,
            "section_title": "Retrieval-Augmented Generation for LLMs",
            "char_start_offset": 3981,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 900
                }
            ],
            "ref_mentions": [
                {
                    "start": 488,
                    "end": 505,
                    "matchedPaperCorpusId": "254564204"
                },
                {
                    "start": 651,
                    "end": 676,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 860,
                    "end": 878,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8779296875
        },
        {
            "corpus_id": "270123034",
            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
            "text": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters. This limitation poses challenges in practical applications, especially when LLMs are already deployed, as parameter adjustments may affect their original functionality. To address this, we propose a novel method that involves learning scalable and pluggable virtual tokens for RAG. By maintaining the LLMs' original parameters and fine-tuning only the embeddings of these pluggable tokens, our approach not only enhances LLMs' performance but also preserves their general generation capabilities. Furthermore, we design several training strategies to improve the scalability, flexibility, and generalizability of our method. Comprehensive experiments across 12 question-answering tasks demonstrate the superiority of our approach.",
            "score": 0.5990589911070195,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.958984375
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases.",
            "score": 0.5985315922263181,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83251953125
        },
        {
            "corpus_id": "272689170",
            "title": "SFR-RAG: Towards Contextually Faithful LLMs",
            "text": "Retrieval Augmented Generation (RAG), a paradigm that integrates external contextual information with large language models (LLMs) to enhance factual accuracy and relevance, has emerged as a pivotal area in generative AI. The LLMs used in RAG applications are required to faithfully and completely comprehend the provided context and users' questions, avoid hallucination, handle unanswerable, counterfactual or otherwise low-quality and irrelevant contexts, perform complex multi-hop reasoning and produce reliable citations. In this paper, we introduce SFR-RAG, a small LLM that is instruction-tuned with an emphasis on context-grounded generation and hallucination minimization. We also present ContextualBench, a new evaluation framework compiling multiple popular and diverse RAG benchmarks, such as HotpotQA and TriviaQA, with consistent RAG settings to ensure reproducibility and consistency in model assessments. Experimental results demonstrate that our SFR-RAG-9B model outperforms leading baselines such as Command-R+ (104B) and GPT-4o, achieving state-of-the-art results in 3 out of 7 benchmarks in ContextualBench with significantly fewer parameters. The model is also shown to be resilient to alteration in the contextual information and behave appropriately when relevant context is removed. Additionally, the SFR-RAG model maintains competitive performance in general instruction-following tasks and function-calling capabilities.",
            "score": 0.5983201983303227,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84033203125
        },
        {
            "corpus_id": "273185794",
            "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) improves language model performance by incorporating relevant knowledge from external sources (Guu et al., 2020;Karpukhin et al., 2020;Lewis et al., 2020). In contrast to na\u00efve RAG, optimizing the retrieval stage can effectively enhance context relevance and improve generation performance (Jiang et al., 2023;Lin et al., 2024;Ma et al., 2023;Sarthi et al., 2024;Shi et al., 2024;Trivedi et al., 2023). An example is REPLUG, in which Shi et al. (2024) leverage LLM as supervision to learn a dense retriever model. In addition, encoding documents can increase knowledge retrieval and improve generation capabilities (Borgeaud et al., 2022;Izacard and Grave, 2021;Izacard et al., 2023;Khandelwal et al., 2019). For instance, Izacard and Grave (2021) leverages fusion-in-decoder architecture to encode multiple question-passage pairs while maintaining the model efficiency. Alternatively, selectively utilizing knowledge from the documents improves the robustness of LLMs against irrelevant context (Yan et al., 2024;Yoran et al., 2024;Yu et al., 2023;Yue et al., 2024;Zhang et al., 2024). For example, RAFT proposes to train language models with negative documents to improve generation quality and relevance (Zhang et al., 2024). Concurrent to our work, long-document retrieval and datastore scaling are proposed to optimize RAG performance (Jiang et al., 2024;Shao et al., 2024). Despite such progress, inference scaling remains under-explored for long-context RAG methods. As such, we investigate how variations in inference computation impact RAG performance, with the goal of optimizing test-time compute allocation.",
            "score": 0.597955416530801,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 9246,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 550
                },
                {
                    "start": 551,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1656
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 149,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 149,
                    "end": 172,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 172,
                    "end": 191,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 327,
                    "end": 347,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 347,
                    "end": 364,
                    "matchedPaperCorpusId": "263605962"
                },
                {
                    "start": 364,
                    "end": 380,
                    "matchedPaperCorpusId": "258841283"
                },
                {
                    "start": 380,
                    "end": 400,
                    "matchedPaperCorpusId": "267334785"
                },
                {
                    "start": 400,
                    "end": 417,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 417,
                    "end": 438,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 471,
                    "end": 488,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 652,
                    "end": 675,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 675,
                    "end": 699,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 699,
                    "end": 720,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 720,
                    "end": 744,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1051,
                    "end": 1070,
                    "matchedPaperCorpusId": "263608822"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75341796875
        },
        {
            "corpus_id": "277321530",
            "title": "What to Retrieve for Effective Retrieval-Augmented Code Generation? An Empirical Study and Beyond",
            "text": "Retrieval-augmented generation (RAG) enhances generative models by integrating external knowledge [25]. Traditional models, while coherent, often suffer from factual inaccuracies and knowledge gaps due to their reliance on fixed memory [49,57]. RAG addresses these limitations by incorporating a retrieval mechanism that fetches relevant information from large-scale knowledge sources, such as databases or document corpora, enabling more accurate and contextually grounded outputs. \n\nThe RAG framework includes a retriever, which ranks relevant information based on the query, and a generator, which conditions its output on both the query and retrieved content, ensuring linguistic fluency while incorporating external knowledge. This adaptability allows the model to stay current without expensive retraining. \n\nRAG has been successfully applied in domains requiring factual accuracy and knowledge grounding, including question answering [2,54], scientific text generation [14], and automated code synthesis [40]. By bridging retrieval-based and generative models, RAG advances the development of more reliable, context-aware AI systems. \n\nThe RAG process can be formulated as follows: \n\nwhere  represents the user query, { 1 , . . . ,   } denotes the candidate knowledge set, and  is the output generated by the language model. The function  (\u2022) retrieves relevant candidates from the knowledge base based on the given query, while  (\u2022) \n\ncombines the retrieved information with the original query and feeds it into the language model for response generation. Repository-level code generation is particularly well-suited for the RAG framework. Software repositories contain extensive historical code, library functions, API calls, and contextual information, making it impractical to rely solely on user queries for code generation. Moreover, due to the maximum input length constraints of LLMs, it is infeasible to import an entire repository into the model. RAG overcomes these limitations by retrieving relevant code snippets from the repository, enriching the model's context and improving the accuracy and coherence of generated code. Additionally, large codebases often contain implementations of similar functions. By retrieving and reusing high-quality existing code snippets, RAG ensures consistency in coding style and interface design while reducing redundancy and potential errors. Consequently, most LLM-based approaches for repository-level code generation incorporate RAG to enhance performance.",
            "score": 0.5972223657943008,
            "section_title": "Background 2.1 Retrieval-Augmented Generation",
            "char_start_offset": 6037,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 103
                },
                {
                    "start": 104,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 482
                },
                {
                    "start": 485,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 812
                },
                {
                    "start": 815,
                    "end": 1016
                },
                {
                    "start": 1017,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1188
                },
                {
                    "start": 1191,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1440
                },
                {
                    "start": 1443,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1963
                },
                {
                    "start": 1964,
                    "end": 2143
                },
                {
                    "start": 2144,
                    "end": 2225
                },
                {
                    "start": 2226,
                    "end": 2397
                },
                {
                    "start": 2398,
                    "end": 2514
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 102,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 976,
                    "end": 980,
                    "matchedPaperCorpusId": "273850363"
                },
                {
                    "start": 1011,
                    "end": 1015,
                    "matchedPaperCorpusId": "274895273"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8427734375
        },
        {
            "corpus_id": "269149146",
            "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "text": "Retrieval-Augmented Generation (RAG) [68] is a framework that combines the capabilities of large language models (LLMs) with external knowledge sources through a retrieval mechanism.Unlike traditional language models that generate text based solely on the text's internal representation, RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses.The theoretical principles underpinning RAG stem from the need to enhance language models with the ability to access and utilize external, structured knowledge [69].This is in response to the limitations of traditional LLMs that rely solely on their pre-trained parameters for knowledge, which can be outdated or incomplete.The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68].The incorporation of RAG into the workflow of LLMs, therefore, presents a sophisticated approach to augmenting the model's knowledge base beyond its pretraining, specifically tailored to the demands of the evolving nature of studies in SLRs.By allowing the model to access an external corpus of domain-specific literature the context within which the LLM operates, it attains the ability to be enriched while a critical countermeasure to the model's propensity for generating plausible yet factually incorrect information -hallucination -is mitigated.In the realm of SLRs, where the precision of synthesized knowledge is paramount, RAG's ability to draws upon relevant information from a targeted corpus helps ensure that the generative outputs of LLMs are anchored in verifiable data.\n\n2.4 Advancing LLMs for Specialized Domains: From Pretraining to Fine-Tuning\n\nIn training LLMs, they initially undergo pretraining on extensive, diverse datasets, acquiring a foundational grasp of both language and knowledge.",
            "score": 0.5939012112696964,
            "section_title": "RAG for Enhanced Factual Accuracy in SLRs",
            "char_start_offset": 18167,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 182,
                    "end": 432
                },
                {
                    "start": 432,
                    "end": 654
                },
                {
                    "start": 654,
                    "end": 819
                },
                {
                    "start": 819,
                    "end": 978
                },
                {
                    "start": 978,
                    "end": 1288
                },
                {
                    "start": 1288,
                    "end": 1529
                },
                {
                    "start": 1529,
                    "end": 1839
                },
                {
                    "start": 1839,
                    "end": 2073
                },
                {
                    "start": 2075,
                    "end": 2150
                },
                {
                    "start": 2152,
                    "end": 2299
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73486328125
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "The contributions of this study are three-fold:\n\n\u2022 Through extensive experimentation, we thoroughly investigated existing RAG approaches and their combinations to identify and recommend optimal RAG practices.\n\n\u2022 We introduce a comprehensive framework of evaluation metrics and corresponding datasets to comprehensively assess the performance of retrieval-augmented generation models, covering general, specialized (or domain-specific), and RAG-related capabilities.\u2022 We demonstrate that the integration of multimodal retrieval techniques can substantially improve question-answering capabilities on visual inputs and speed up the generation of multimodal content through a strategy of \"retrieval as generation\".",
            "score": 0.593719397638711,
            "section_title": "Retrieval Source",
            "char_start_offset": 4117,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 47
                },
                {
                    "start": 49,
                    "end": 208
                },
                {
                    "start": 210,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 711
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9150390625
        },
        {
            "corpus_id": "269292881",
            "title": "Retrieval-Augmented Generation-based Relation Extraction",
            "text": "Retrieval-Augmented Generation (RAG) for large language models can be classified into two categories: i) naive RAG and ii) advanced RAG.Naive RAG has basic steps: retrieve, augmentation, and generation, while the advanced version includes a post-processing step before sending the retrieved information to a user [24].The concept of RAG has been suggested as a way to minimize the undesired alterations in Language Models (LLMs) when conversational systems built on LLMs generate arbitrary responses to a query [7].RAG is an example of open-book exams which are applied to the usage of LLMs.The retriever mechanism in RAG finds an example of the user query (prompt), and then the user query is regenerated along with the example by the data-augmentation module in RAG.Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.\n\nIn this work, we introduce a Retrieval-Augmented Generation-based Relation Extraction (RAG4RE) approach to identify the relationship between a pair of entities in a sentence.",
            "score": 0.592613626857869,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 8393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 318
                },
                {
                    "start": 318,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 591
                },
                {
                    "start": 591,
                    "end": 768
                },
                {
                    "start": 768,
                    "end": 1031
                },
                {
                    "start": 1033,
                    "end": 1207
                }
            ],
            "ref_mentions": [
                {
                    "start": 511,
                    "end": 514,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9521484375
        },
        {
            "corpus_id": "276742664",
            "title": "Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for RAG-Equipped LLM",
            "text": "Shortly after the introduction of pre-trained language models [13], Large Language Models (LLMs) [5] have significantly advanced natural language processing, excelling in tasks like translation and summarization [9,81]. However, they often struggle with factual accuracy, generating outdated or incorrect information due to reliance on learned patterns. To address these challenges, the Retrieval-Augmented Generation (RAG) framework [31] is introduced. RAG enhances the generative capabilities of LLMs by incorporating a retrieval mechanism that accesses relevant information from external knowledge bases [32]. This two-step process retrieves relevant documents based on the input query and uses them to inform response generation. By integrating retrieval and generation, RAG enhances factual accuracy and enriches content with current, contextually relevant information [27]. RAG has shown promising results in applications like questionanswering and conversational agents, setting a new standard for combining retrieval and generative techniques. The integration of vector databases with RAG holds significant potential for improving the efficiency and effectiveness of information retrieval alongside LLMs [52].",
            "score": 0.5925943891458385,
            "section_title": "RELATED WORK 2.1 Retrieval Augmented Generation",
            "char_start_offset": 6333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 353
                },
                {
                    "start": 354,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1217
                }
            ],
            "ref_mentions": [
                {
                    "start": 62,
                    "end": 66,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "259360395"
                },
                {
                    "start": 434,
                    "end": 438,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 874,
                    "end": 878,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1212,
                    "end": 1216,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7958984375
        },
        {
            "corpus_id": "267061013",
            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a transformative technique across diverse domains, revolutionizing information processing and decision-making processes. \n\nIn a work by Datta et al. [16], the authors propose two models, MAKG (Medical Appropriateness Knowledge Graph) and RAG-GPT (Retrieval Augmented Generation -Generative Pretrained Transformer). MAKG functions as an autonomous coarse-grained medical-inappropriateness vigilance model for payers and regulators, while RAG-GPT operates as a fine-grained LLM with human-in-the-loop for assessing medical appropriateness and inappropriateness. \n\nAnother work by Lewis et al. [34] introduces RAG models that combine parametric and non-parametric memory components to enhance sequence-to-sequence (seq2seq) models. By endowing pre-trained, parametric-memory generation models with a non-parametric memory, RAG models achieve state-of-the-art results in open-domain extractive question answering and knowledge-intensive generation tasks. \n\nIn the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
            "score": 0.5925615719454909,
            "section_title": "Retrieval Augmented Generation (RAG) in Various Contexts",
            "char_start_offset": 11665,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 175,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 1002
                },
                {
                    "start": 1005,
                    "end": 1196
                }
            ],
            "ref_mentions": [
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "266126075"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9013671875
        },
        {
            "corpus_id": "267412330",
            "title": "C-RAG: Certified Generation Risks for Retrieval-Augmented Language Models",
            "text": "Retrieval augmented generation (RAG) is a framework for improving the generation quality of LLMs via retrieving relevant information from the external knowledge base and grounding the model on the information for conditional generations. Biencoder retrieval methods (Lewis et al., 2020;Karpukhin et al., 2020;Xiong et al., 2020) leverage two encoders to map the query text and candidate texts into the embedding space and retrieve candidate texts with high embedding similarity to the query text embedding. End-to-end retrieval methods (Tay et al., 2022;Wang et al., 2022b;Kishore et al., 2023) train a model to map the query text to the id of relevant candidate documents directly. Another line of work (Luo et al., 2023;Gou et al., 2023) leverages external tools such as LLMs to retrieve relevant documents via prompting design. Although RAG demonstrates impressive capacities, the theoretical analysis of retrieval models for LLM generations is limited. Basu et al. analyze the retrieval model of constrained function class from a statistical perspective, but the results cannot be generalized to the self-attention transformers. In this work, we provide the first analysis of how RAG enhances the generation quality and mitigate generation risks of self-attention transformers. \n\nConformal prediction is a statistical tool to construct the prediction set with guaranteed prediction coverage (Vovk et al., 1999;2005;Lei et al., 2013;Yang & Kuchibhotla, 2021;Kang et al., 2023;2024a), assuming that the data is exchangeable. However, conformal prediction can only provide guarantees for the regression and classification tasks and is not directly applicable to the generation tasks, which are more relevant for LLMs.",
            "score": 0.5921697663292881,
            "section_title": "B. Additional related work",
            "char_start_offset": 35780,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1132
                },
                {
                    "start": 1133,
                    "end": 1281
                },
                {
                    "start": 1284,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1718
                }
            ],
            "ref_mentions": [
                {
                    "start": 266,
                    "end": 286,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 536,
                    "end": 554,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 554,
                    "end": 573,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 573,
                    "end": 594,
                    "matchedPaperCorpusId": "259991824"
                },
                {
                    "start": 722,
                    "end": 739,
                    "matchedPaperCorpusId": "52010032"
                },
                {
                    "start": 1419,
                    "end": 1436,
                    "matchedPaperCorpusId": "17499892"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.69384765625
        },
        {
            "corpus_id": "272828029",
            "title": "Retrieval Augmented Generation-Based Incident Resolution Recommendation System for IT Support",
            "text": "Clients wishing to implement generative AI in the domain of IT Support and AIOps face two critical issues: domain coverage and model size constraints due to model choice limitations. Clients might choose to not use larger proprietary models such as GPT-4 due to cost and privacy concerns and so are limited to smaller models with potentially less domain coverage that do not generalize to the client's domain. Retrieval augmented generation is a common solution that addresses both of these issues: a retrieval system first retrieves the necessary domain knowledge which a smaller generative model leverages as context for generation. We present a system developed for a client in the IT Support domain for support case solution recommendation that combines retrieval augmented generation (RAG) for answer generation with an encoder-only model for classification and a generative large language model for query generation. We cover architecture details, data collection and annotation, development journey and preliminary validations, expected final deployment process and evaluation plans, and finally lessons learned.",
            "score": 0.5912299581591889,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6201171875
        },
        {
            "corpus_id": "270285886",
            "title": "XL-HeadTags: Leveraging Multimodal Retrieval Augmentation for the Multilingual Generation of News Headlines and Tags",
            "text": "Retrieval-Augmented Generation (RAG) represents a pivotal advancement in Natural Language Generation (NLG), addressing the issue of neural models' limited contextual understanding.Traditional neural models often falter when the input lacks comprehensive information for generating accurate outputs, particularly in complex realworld applications (Yu et al., 2022).To bridge this gap, KNNLM (Khandelwal et al., 2020) introduced a technique for augmenting language models with examples retrieved from a training text dataset, enhancing contextual relevance.Building on this, RETRO (Borgeaud et al., 2021) leveraged a vastly expanded text corpus, enabling models with a smaller footprint to achieve performance on par with GPT-3 (Brown et al., 2020).\n\nModels such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020b) incorporate Wikipedia passages as external knowledge bases, significantly boosting their efficacy in tasks like Question Answering.REALM focuses on encoding information through masked language modeling, whereas RAG employs an encoder-decoder structure for generative language tasks.\n\nExpanding on these concepts, MuRAG (Chen et al., 2022) stands out by integrating multimodal knowledge sources, encompassing both visual and textual data.This innovation extends the capabilities of knowledge-enhanced text generation, catering to the nuanced demands of intricate information landscapes.-w/I (K=15) 31.37 (-0.09) 12.63 (-0.10) 28.13 (-0.02) 8.58 (-0.17) 24.40 (-0.21) 0.71 (0.00) 70.84 (-0.03)Table 8: Headline Generation Evaluation.Selected Content (Only Important Sentences).The best results compared to their respective baseline models are marked in bold, and \u2206 gains are shown in round brackets and highlighted with green and red colors.",
            "score": 0.5908028048375673,
            "section_title": "G.4 Retrieval-Augmented Generation",
            "char_start_offset": 38590,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 364
                },
                {
                    "start": 364,
                    "end": 555
                },
                {
                    "start": 555,
                    "end": 747
                },
                {
                    "start": 749,
                    "end": 950
                },
                {
                    "start": 950,
                    "end": 1101
                },
                {
                    "start": 1103,
                    "end": 1256
                },
                {
                    "start": 1256,
                    "end": 1404
                },
                {
                    "start": 1404,
                    "end": 1510
                },
                {
                    "start": 1510,
                    "end": 1550
                },
                {
                    "start": 1550,
                    "end": 1594
                },
                {
                    "start": 1594,
                    "end": 1758
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 363,
                    "matchedPaperCorpusId": "222272210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76806640625
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "Retrieval-Augmented Generation (RAG, (Lewis et al., 2020;Su et al.;Wang et al., 2025)) has gained significant interest in Natural Language Processing for enhancing large language models (LLMs) on knowledge-intensive tasks through external information retrieval, with applications across search engines, conversational agents, chatbots, and many other applications. RAG addresses key LLM limitations, including hallucinations, outdated information, and insufficient domain-specific knowledge, particularly in open-domain question answering. Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering). Building on advancements in LLM training methodologies, DeepSeek has enhanced its AI models, notably DeepSeek-R1 (Liu et al., 2024;Guo et al., 2025;Shao et al., 2024), by implementing Group Relative Policy Optimization (GRPO), an advanced reinforcement learning algorithm that improves training efficiency and model performance beyond traditional supervised fine-tuning. GRPO reduces computational overhead by eliminating the value function, using group-based advantage estimation for simplified reward computation, lowering memory usage, and integrating Kullback-Leibler (KL) divergence regularization for stable, efficient training. It outperforms standard Rejection Sampling Fine-Tuning (RFT), which relies on offline sampling, and Online RFT, which dynamically samples from an evolving policy. GRPO also supports process supervision (GRPO+PS), providing step-by-step feedback for improved reasoning, surpassing outcome supervision (GRPO+OS), which evaluates only final answers. Addressing the limitations of static retrieval in traditional RAG, DRAGIN (Dynamic Retrieval-Augmented Generation based on Information Needs, (Su et al.)) is an advanced framework that dynamically determines when and what to retrieve during text generation.",
            "score": 0.590427867288926,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 1059
                },
                {
                    "start": 1060,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1857
                },
                {
                    "start": 1858,
                    "end": 2041
                },
                {
                    "start": 2042,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 578,
                    "end": 599,
                    "matchedPaperCorpusId": "268510197"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94287109375
        },
        {
            "corpus_id": "277596427",
            "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented Generation with Reranker KV-Cache Reuse",
            "text": "Retrieval-Augmented Generation (RAG) (Ram et al., 2023;Lewis et al., 2021;Asai et al., 2023;Khandelwal et al., 2020;Jin et al., 2024a;Shao et al., 2024) is the process of optimizing the output of a large language model and it references an authoritative knowledge base outside of its training data sources before generating a response. This paradigm has gained traction for tasks requiring factual accuracy and up-to-date information, such as question answering, summarization, and dialogue generation. \n\nFigure 1: Classic RAG Workflow: The query is embedded and used to retrieve top-K documents. Then the reranker selects the most relevant ones which are combined with the query to generate the final response. \n\nTraditional RAG pipelines rely on retrievers which depend on embedding representations and cosine similarity to fetch relevant documents (Robertson & Zaragoza, 2009;Reimers & Gurevych, 2019;Wang et al., 2024). While this approach is straightforward, it often struggles to achieve optimal results in more complex scenarios. To solve the problem, advancements have introduced reranker mechanisms that refine the retrieved documents to improve relevance and contextuality before generation. These rerankers, often transformer-based, significantly boost the quality of generated content by ensuring the most pertinent documents are prioritized. \n\nEarly Rerankers were predominantly trained on encoder-only models such as BERT (Devlin et al., 2019) or XLM-RoBERTa (Conneau et al., 2020), leveraging their strong encoding capabilities to improve retrieval precision. However, recent advancements have demonstrated the growing dominance of decoder-based rerankers which capitalize on the powerful generative language capabilities of modern decoder models (Chen et al., 2024;Ma et al., 2023;Pradeep et al., 2023). By fine-tuning these decoder-based models on tasks originally designed for encoder-only rerankers, they achieve significant gains in performance, benefiting from both their inherent generative power and the fine-grained contextual understanding acquired during fine-tuning.",
            "score": 0.5903666824203555,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 3985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 335
                },
                {
                    "start": 336,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 711
                },
                {
                    "start": 714,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1036
                },
                {
                    "start": 1037,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1354
                },
                {
                    "start": 1357,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 2093
                }
            ],
            "ref_mentions": [
                {
                    "start": 879,
                    "end": 904,
                    "matchedPaperCorpusId": "201646309"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.875
        },
        {
            "corpus_id": "273812427",
            "title": "Enhancing Question Answering Precision with Optimized Vector Retrieval and Instructions",
            "text": "Accurately answering queries utilizing large-scale datasets has become an important task in the fast evolving filed of natural language processing (NLP). Recent advancements in machine learning and artificial intelligence have led to the development of sophisticated models that can parse, follow, and respond to natural language queries with unprecedented accuracy. Among these, the Retrieval-Augmented Generation (RAG) architecture represents a significant leap forward, combining the strengths of information (vector) retrieval and generative models for QA applications [7]. This paper introduces a novel methodology that leverages the RAG architecture, aiming to enhance QA precision through optimized vector retrieval and improved context construction.",
            "score": 0.5902206034248831,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 757
                }
            ],
            "ref_mentions": [
                {
                    "start": 573,
                    "end": 576,
                    "matchedPaperCorpusId": "252735056"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73388671875
        },
        {
            "corpus_id": "270380174",
            "title": "Natural Language Interaction with a Household Electricity Knowledge-based Digital Twin",
            "text": "Retrieval Augmented Generation (RAG) is a technique that combines the capabilities of retrieval-based and generationbased models to improve information generation and accuracy. As depicted in Figure 1, in RAG, the LLM produces and answer based on three elements: a question, a query and context. The question is typically provided by a human in natural language. It is mapped by a transformation block to a query as depicted by steps 1 and 2 in Figure 1. The transformation step may embed the question and the embedding subsequently represents the query. It can also transform the question from natural language to an alternative query language such as SQL or SPARQL. The query resulting from the transformation block is then sent to a retrieval system that fetches relevant documents or pieces of information from a large dataset as represented by steps 3 and 4 in the figure. This system can use a vector database when the query is an embedding, and relational database in case of an SQL query or a knowledge graph in the case of a SPARQL query. The information retrieved by the system (i.e. embeddings, records, triples) is referred to as context that is send together with the question and query to the LLM that generates the answer as illustrated in steps 5 and 6. With this process, the generative model produces a coherent and contextually accurate response. This approach leverages the extensive factual knowledge in the retrieval database and the linguistic and contextual abilities of generative models. \n\nIn general, RAG is considered useful because it enhances the accuracy and reliability of generated content by grounding it in real, retrieved data, reducing the likelihood of generating hallucinations or incorrect information [18]. This makes RAG particularly valuable in applications requiring precise and factual responses, such as question answering, customer support, Fig. 2. Household Electricity Knowledge-based Digital Twin Visualization [12]. and content creation, where both relevance and correctness are critical. By combining retrieval and generation, RAG achieves a more robust and informed response generation process.",
            "score": 0.5899395993805712,
            "section_title": "B. Background on RAG",
            "char_start_offset": 7482,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 295
                },
                {
                    "start": 296,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 877
                },
                {
                    "start": 878,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1513
                },
                {
                    "start": 1516,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1966
                },
                {
                    "start": 1967,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2147
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5556640625
        },
        {
            "corpus_id": "271843111",
            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
            "text": "The evaluation of a Large Language Model-based tutoring system for a University robotics course highlighted several insights into the application of advanced LLM techniques and the resulting performance in an educational setting. First, our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations [Shuster et al., 2021]. Furthermore, even though our human evaluation is currently restricted to two test subjects, their answers already point out that added references increases trustworthiness. Therefore, RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first. As further advantages, in our experience RAG is quite straight forward to realize, in particular in a course setting in which well-curated background material is readily available. Furthermore, from a teaching point of view a tutor should stick to PREPRINT the lecture material, e.g., when going over a concept the tutoring system should carefully choose examples and ideally stick-or at least start-with the ones provided in the lecture. This should positively affect the learning of students. \n\nFine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG. As an explanation, fine-tuning aims to specialize a model to a specific task and a specific type of interaction. A fine-tuned model might loose some of its general flexibility. As a consequence, when interacting very differently with the model, the model might produce worse results or even behave erratically.",
            "score": 0.5889574887408178,
            "section_title": "Discussion and Conclusion",
            "char_start_offset": 42602,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 791
                },
                {
                    "start": 792,
                    "end": 922
                },
                {
                    "start": 923,
                    "end": 1103
                },
                {
                    "start": 1104,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1417
                },
                {
                    "start": 1420,
                    "end": 1482
                },
                {
                    "start": 1483,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2254
                },
                {
                    "start": 2255,
                    "end": 2388
                }
            ],
            "ref_mentions": [
                {
                    "start": 595,
                    "end": 617,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "273323961",
            "title": "A Methodology for Evaluating RAG Systems: A Case Study On Configuration Dependency Validation",
            "text": "Retrieval-augmented generation (RAG) is an emerging novel approach, driven by recent advancements in large language models (LLMs), which have enhanced their capabilities in various software engineering tasks, such as code generation [6,49], configuration validation [26], database tuning [16,25], and program repair [10,22,48]. Despite these advancements, LLMs still face challenges, such as hallucination [20,58], outdated data [45,57], and untraceable reasoning [14,23], which undermine their ability to provide accurate and reliable outputs. RAG offers a promising solution to these challenges by integrating techniques from information retrieval with the generative capabilities of LLMs. This enables the delivery of contextually relevant, up-to-date, and factually accurate information to an LLM, effectively mitigating its limitations. \n\nTypically, a RAG system comprises of an ingestion part where data is preprocess, embedded, and stored as context documents in a vector database including meta-data, a retrieval part where context documents are retrieved and ranked according to their relevance to a query, and a query part where a prompt with a query is combined with the retrieval results and sent to the LLM. So, RAG is not a single technology, but an umbrella of different components, design decisions, and domain-specific adaptations. It can be composed from different embedding models, search types, filters, and re-rankers. Developers must define which data sources are relevant and how to preprocess them prior to ingestion into a vector database. Moreover, how to prompt the LLM, how to rank which contextual information is most beneficial, and whether and how to provide the LLM with examples to the query at hand, often requires multiple evaluation and refinement steps [2]. The final RAG system is, thus, a product of several design decisions and iterations based on empirical evidence about the RAG's effectiveness collected on the way. \n\nSuch an iterative, and empirical-driven development process requires a sound research methodology when a RAG system is not only built but also evaluated in a sound way.",
            "score": 0.5888045981972709,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 841
                },
                {
                    "start": 844,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1439
                },
                {
                    "start": 1440,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1794
                },
                {
                    "start": 1795,
                    "end": 1958
                },
                {
                    "start": 1961,
                    "end": 2129
                }
            ],
            "ref_mentions": [
                {
                    "start": 236,
                    "end": 239,
                    "matchedPaperCorpusId": "247158549"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "255372224"
                },
                {
                    "start": 320,
                    "end": 323,
                    "matchedPaperCorpusId": "251765058"
                },
                {
                    "start": 323,
                    "end": 326,
                    "matchedPaperCorpusId": "259860439"
                },
                {
                    "start": 406,
                    "end": 410,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 464,
                    "end": 468,
                    "matchedPaperCorpusId": "254877753"
                },
                {
                    "start": 1790,
                    "end": 1793,
                    "matchedPaperCorpusId": "266933076"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75390625
        },
        {
            "corpus_id": "269149041",
            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
            "text": "RAG-Memory-Finetuning is a technique to optimize the performance of large language models (LLMs) by combining Retrieval Augmented Generation (RAG) with fine-tuning.This approach aims to connect LLMs to external knowledge sources 56 AgentLM-70B:https://huggingface.co/THUDM/agentlm-70B 57 AgentLM-7B:https://huggingface.co/THUDM/agentlm-7b 58 AgentLM-7B:https://huggingface.co/THUDM/agentlm-13B 59 AgentTuning Github:https://github.com/THUDM/AgentTuningNavigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies 2023-12-20 through a retrieval mechanism and combine it with generative capabilities to search and integrate relevant information from knowledge bases.The goal of RAG-Memory-Finetuning is to improve the consistency and reliability of the output and reduce hallucination issues.Specifically,the RAG technique is implemented through the following steps:\n\n1) Chunking an external domain-specific knowledge base into small documents,each about 150 words.2) Creating embeddings using a pre-trained model and storing document vectors in a vector database.3) When an input query is passed to the LLM,the most relevant information is retrieved from the external database using metrics such as cosine similarity and combined with the LLM as additional context.4) This external context and the input prompt are passed together to the text generator to produce the output response.Responses generated by RAG are more factual,specific,and diverse.The parameter knowledge provided by traditional finetuning techniques is static,while RAG allows us to bypass retraining and obtain up-to-date information through retrievalbased generation to produce reliable outputs.M gen predicts IsUse given x, y t 12: end if through self-reflection and also addresses the hallucination issues present in large models.\n\nAlgorithm 2 present an overview of SELF-RAG [4] at inference.For every x and preceding generation y < t,the model decodes a retrieval token to evaluate the utility of retrieval.",
            "score": 0.588560440055355,
            "section_title": "XIII. RAG-MEMORY-FINETUNING",
            "char_start_offset": 144027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 164,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 730
                },
                {
                    "start": 730,
                    "end": 856
                },
                {
                    "start": 856,
                    "end": 930
                },
                {
                    "start": 932,
                    "end": 1029
                },
                {
                    "start": 1029,
                    "end": 1128
                },
                {
                    "start": 1128,
                    "end": 1330
                },
                {
                    "start": 1330,
                    "end": 1449
                },
                {
                    "start": 1449,
                    "end": 1514
                },
                {
                    "start": 1514,
                    "end": 1731
                },
                {
                    "start": 1731,
                    "end": 1868
                },
                {
                    "start": 1870,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2047
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7978515625
        },
        {
            "corpus_id": "265594594",
            "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)",
            "text": "Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input. \n\nThe advantages of RAG are multi-fold. Firstly, it bolsters the performance by grounding LLMs with factual, up-todate information from external knowledge repositories. Furthermore, RAG maintains contextual relevance in responses, contributing to a more engaging user experience in conversational AI applications. Its scalability is noteworthy, as RAG models seamlessly handle copious volumes of information, proving invaluable for data-intensive tasks. Additionally, the adaptability of RAG models allows fine-tuning for specific applications [2], rendering them versatile across diverse data and use cases. Customizability is another hallmark, permitting RAG models to specialize in particular domains or subjects through customization and fine-tuning on specific knowledge bases. Due to the importance of such a framework for enterprises, extensive research is currently being pursued to discover new algorithms and techniques to enhance the performance of such models bounded by the context-window limitations of LLMs. Although there is ongoing research to expand the window size for LLM to be able to ingest more data in the prompt, the use of techniques like RAG is still of great practical importance, not only on homogeneous unstructured data but also on heterogeneous data [3]. \n\nIn principle, at the heart of the information retrieval module is the semantic embedding module which converts a piece of text, whether a query or a context text chunk to a numeric feature vector that embodies all semantic features of the text. The development of word and sentence embeddings is a relatively recent area of research in natural language processing (NLP) and information retrieval. \n\nMost of the semantic models are English language-centred; however, in recent years, Multilingual embedding models were released [4].",
            "score": 0.588326774815126,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 363
                },
                {
                    "start": 364,
                    "end": 571
                },
                {
                    "start": 574,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1858
                },
                {
                    "start": 1861,
                    "end": 2105
                },
                {
                    "start": 2106,
                    "end": 2257
                },
                {
                    "start": 2260,
                    "end": 2392
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "273403982",
            "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
            "text": "Retrieval-augmented generation (RAG) is an advanced hybrid model architecture that augments natural language generation (NLG) with external retrieval mechanisms to enhance the model's knowledge base. Traditional large language models (LLMs) such as GPT-3 and BERT, which are pre-trained on vast corpora, rely entirely on their internal representations of knowledge, making them susceptible to issues like hallucinations-where the models generate plausible but incorrect information. These models cannot efficiently update their knowledge bases without retraining, making them less practical for dynamic, knowledge-intensive tasks like open-domain question answering and fact verification (Brown, T., et al. 2020). To overcome these limitations, the paper (Lewis et al. 2020) proposed the RAG architecture, which retrieves real-time, relevant external documents to ground the generated text in factual information. \n\nThe RAG model incorporates two key components: \n\n1. Retriever: This retrieves the most relevant documents from a corpus using techniques such as dense passage retrieval (DPR) (Karpukhin et. al. 2020) or traditional BM25 algorithms. 2. Generator: It synthesizes the retrieved documents into coherent, contextually relevant responses. \n\nRAG's strength lies in its ability to leverage external knowledge dynamically, allowing it to outperform generative models like GPT-3 and knowledge-grounded systems like BERT, which rely on static datasets. \n\nIn open-domain question answering, RAG has been demonstrated to be highly effective, consistently retrieving relevant information and improving the factual accuracy of the generated responses (Guu, K., et al. 2020). In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues",
            "score": 0.5883017057325931,
            "section_title": "Overview of RAG Models",
            "char_start_offset": 10564,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 200,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 913
                },
                {
                    "start": 916,
                    "end": 962
                },
                {
                    "start": 965,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1248
                },
                {
                    "start": 1251,
                    "end": 1457
                },
                {
                    "start": 1460,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 2056
                },
                {
                    "start": 2057,
                    "end": 2239
                }
            ],
            "ref_mentions": [
                {
                    "start": 755,
                    "end": 773,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "271218596",
            "title": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems",
            "text": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [21,14].In response, Retriever-Augmented Generation (RAG) systems [21,20] are becoming increasingly popular in user-facing dialogue applications [35].Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response.The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [21].\n\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1,31,7].In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks.However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs.With numerous commercial and open-source generative LLMs readily available 1 and many variable parameters in the RAG system design (Figure 1), tuning an optimal system for a particular RAG application involves iterative evaluation of multiple configurations.This motivates the need for automated RAG evaluation solutions.\n\nIn response, automated RAG evaluation systems like RAGAS [9] and TruLens [37] have emerged.These systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG evaluation metrics.However, the lack of unified RAG benchmarks makes it difficult to compare approaches against each other.Each new study designs a new dataset, often employing LLMs as generators and labelers [9,33,4], which renders them irreproducible.A few benchmarks like RGB [4], AttributionBench [22] and RAGTruth [41] have been proposed recently, but they are small in size and target a disjoint set of labels.The exact RAG evaluation criteria also vary from study to study.",
            "score": 0.588135125515964,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 191
                },
                {
                    "start": 191,
                    "end": 333
                },
                {
                    "start": 333,
                    "end": 589
                },
                {
                    "start": 589,
                    "end": 730
                },
                {
                    "start": 732,
                    "end": 905
                },
                {
                    "start": 905,
                    "end": 1049
                },
                {
                    "start": 1049,
                    "end": 1200
                },
                {
                    "start": 1200,
                    "end": 1458
                },
                {
                    "start": 1458,
                    "end": 1521
                },
                {
                    "start": 1523,
                    "end": 1614
                },
                {
                    "start": 1614,
                    "end": 1723
                },
                {
                    "start": 1723,
                    "end": 1827
                },
                {
                    "start": 1827,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2120
                },
                {
                    "start": 2120,
                    "end": 2184
                }
            ],
            "ref_mentions": [
                {
                    "start": 183,
                    "end": 187,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 249,
                    "end": 253,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 253,
                    "end": 256,
                    "matchedPaperCorpusId": "173990818"
                },
                {
                    "start": 328,
                    "end": 332,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 725,
                    "end": 729,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "235898896"
                },
                {
                    "start": 902,
                    "end": 904,
                    "matchedPaperCorpusId": "258947803"
                },
                {
                    "start": 1580,
                    "end": 1583,
                    "matchedPaperCorpusId": "263152733"
                },
                {
                    "start": 1913,
                    "end": 1916,
                    "matchedPaperCorpusId": "263152733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "275788867",
            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
            "text": "Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains. We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.",
            "score": 0.587325053927875,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9384765625
        },
        {
            "corpus_id": "273502659",
            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
            "text": "Robust content moderation classifiers are essential for the safety of Generative AI systems. In this task, differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without context or explanation. Scaling risk discovery and mitigation through continuous model fine-tuning is also slow, challenging and costly, preventing developers from being able to respond quickly and effectively to emergent harms. We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies. Our findings also suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.",
            "score": 0.586287732910191,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9482421875
        },
        {
            "corpus_id": "270870632",
            "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation allows to enhance Large Language Models with external knowledge. In response to the recent popularity of generative LLMs, many RAG approaches have been proposed, which involve an intricate number of different configurations such as evaluation datasets, collections, metrics, retrievers, and LLMs. Inconsistent benchmarking poses a major challenge in comparing approaches and understanding the impact of each component in the pipeline. In this work, we study best practices that lay the groundwork for a systematic evaluation of RAG and present BERGEN, an end-to-end library for reproducible research standardizing RAG experiments. In an extensive study focusing on QA, we benchmark different state-of-the-art retrievers, rerankers, and LLMs. Additionally, we analyze existing RAG metrics and datasets. Our open-source library BERGEN is available under \\url{https://github.com/naver/bergen}.",
            "score": 0.5861531053324742,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91064453125
        },
        {
            "corpus_id": "273403839",
            "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
            "text": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear. This paper empirically analyzes how knowledge selection influences downstream generation performance in RAG systems. By simulating different retrieval and selection conditions through a controlled mixture of gold and distractor knowledge, we assess the impact of these factors on generation outcomes. Our findings indicate that the downstream generator model's capability, as well as the complexity of the task and dataset, significantly influence the impact of knowledge selection on the overall RAG system performance. In typical scenarios, improving the knowledge recall score is key to enhancing generation outcomes, with the knowledge selector providing limited benefit when a strong generator model is used on clear, well-defined tasks. For weaker generator models or more ambiguous tasks and datasets, the knowledge F1 score becomes a critical factor, and the knowledge selector plays a more prominent role in improving overall performance.",
            "score": 0.585065160751746,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89013671875
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "This joint training helps the generation model better utilize the retrieved information, and improves model synergy and generalization performance.However, this approach becomes impractical when the generation module is a large language model, which can have billions of parameters.On one hand, fine-tuning the full LLM is often infeasible due to the massive computational resources required; on the other hand, many existing LLMs are only accessible via APIs [30,31] and cannot be fine-tuned.\n\nTo overcome the infeasibility of fine-tuning LLMs in RAG, black-box RAG, which alternatively regards a LLM as a black-box (i.e., freeze the parameters of the LLM) and augment it without fine-tuning, has achieved success in knowledgeintensive tasks and gained much attention.Existing black-box RAG methods [36,44,53,54] typically fine-tune the retriever only based on LLMs' preferences (e.g., whether LLMs can give correct answer with the retrieved documents) and concatenate all the retrieved documents as the input, which suffers both effectiveness and efficiency issues.Only considering LLMs' preferences in retrieval causes ignorance of factual information, which can degenerate the effectiveness of RAG for it may mislead the retriever.As demonstrated in Figure 1, the LLM can answer correctly with the retrieved documents, but the documents themselves do not actually contain relevant factual information for the given question.For example, Q1 asks the location of the State Hermitage Museum; however, the retrieved document Fig. 2. The overview of FIT-RAG provides information about the Museum of Moscow.Although the LLM can give the correct answer, the retrieved document is actually unnecessary.If these unnecessary documents is used to reward the retriever, they can mislead the retriever.Besides, concatenating all the retrieved documents as the input causes waste of tokens, which can introduce excessive unnecessary tokens and hurts the efficiency of RAG.",
            "score": 0.5848627950035735,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2033,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 147,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 493
                },
                {
                    "start": 495,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 1067
                },
                {
                    "start": 1067,
                    "end": 1235
                },
                {
                    "start": 1235,
                    "end": 1428
                },
                {
                    "start": 1428,
                    "end": 1605
                },
                {
                    "start": 1605,
                    "end": 1698
                },
                {
                    "start": 1698,
                    "end": 1793
                },
                {
                    "start": 1793,
                    "end": 1962
                }
            ],
            "ref_mentions": [
                {
                    "start": 804,
                    "end": 807,
                    "matchedPaperCorpusId": "263828724"
                },
                {
                    "start": 810,
                    "end": 813,
                    "matchedPaperCorpusId": "258960666"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.50390625
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "To enhance the accuracy of question and answer (Q&A) tasks, our approach included supervised finetuning on the zero-shot model for GPT-3.5 Turbo. This is needed because zero shot Retrieval-Augmented Generation (RAG) oftentimes retrieves irrelevant information or irrelevant embeddings. Subsequently, we utilized reprompting on GPT4All and Llama2 for further refinement. This process included feeding in example prompts with given questions, evidence text, and answers to guide the model in learning specific response patterns. Additionally, we benchmarked the fine-tuning process by incorporating the Retrieval-Augmented Generation (RAG) technique for Q&A tasks, thereby enhancing the evaluation and overall performance of the models. \n\nThere is an absence in standardized methodologies for evaluating model performance. We assessed the zero-shot model (LLM without fine-tuning) and subsequently evaluated the model with fine-tuning and reprompting. The approach involved iterative accuracy evaluation, incorporating fine-tuning or reprompting into models such as GPT-3.5 Turbo, GPT4All, Llama2, and Claude. The evaluation metrics used were cosine similarity and Rouge-L, measuring accuracy in each iteration",
            "score": 0.5836235929319684,
            "section_title": "Process and Work",
            "char_start_offset": 9452,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 734
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1107
                },
                {
                    "start": 1108,
                    "end": 1208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.91943359375
        },
        {
            "corpus_id": "270371381",
            "title": "TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models",
            "text": "In this section, we mainly study the influence of prompts on the retrieval and generation performance in knowledge-intensive scenarios. Toward this end, we first let SEED-LLaMA, Gemini Pro, and GPT-4o to explain the raw query with knowledge concepts from 6 domains. Subsequently, we concatenate the raw prompt and expanded ones to form new long text prompts and feed them to SEED-LLaMA. \n\nThe results for text-to-image generation and retrieval are listed in Tab. 11 and Tab. 12, respectively. We can see that prompt/query expansion with strong LLMs could promote both generation and retrieval performance. Meanwhile, weak LLMs may introduce false explanations and do harm to the generation and retrieval performance. As shown in Fig. 8, our models has the advantages over SDXL in the knowledge-intensive domain, accurately retrieving the correct results. However, decision errors still occur. We leave further exploration of the decision strategy for future work. \n\nIn Fig. 10, we compare our models with current Text-to-Image baseline models such as Emu2, DreamLLM, and GILL, which can autonomously decide between retrieval and generation. Our models are consistently retrieving the correct images in the knowledge-intensive domain. In this domain, Emu2, DreamLLM, and GILL fail to generate closely matching images, highlighting the limitations of current MLLMs. \n\nWe further explored two scenarios: Augmented Generation for Better Retrieval and Augmented Retrieval for Better Generation. In the Augmented Generation for Better Retrieval scenario, we first use the MLLM's capability to generate an image before performing image retrieval. The generated image, along with the retrieval prompt, is then used as input for the retrieval process. As shown in Fig. 11, generating an image beforehand improves the model's retrieval performance. \n\nIn the Augmented Retrieval for Better Generation scenario, we leverage our model's generative retrieval capabilities to perform an image retrieval before generating an image. The retrieved image, along with the generation prompt, is then used as input for the generation process, similar to Retrieval-Augmented Generation (RAG). As shown in Fig. 12, performing image retrieval beforehand improves the stability and quality of the generated images. \n\nOne major limitation of the CLIP model for retrieval is its limited context length.",
            "score": 0.5817997916229496,
            "section_title": "D.1.2 PROMPT/QUERY EXTENSION",
            "char_start_offset": 39554,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 854
                },
                {
                    "start": 855,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1363
                },
                {
                    "start": 1366,
                    "end": 1489
                },
                {
                    "start": 1490,
                    "end": 1639
                },
                {
                    "start": 1640,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1838
                },
                {
                    "start": 1841,
                    "end": 2015
                },
                {
                    "start": 2016,
                    "end": 2169
                },
                {
                    "start": 2170,
                    "end": 2288
                },
                {
                    "start": 2291,
                    "end": 2374
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53369140625
        },
        {
            "corpus_id": "277501853",
            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
            "text": "We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.",
            "score": 0.5815907827902453,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.927734375
        },
        {
            "corpus_id": "266435559",
            "title": "RealGen: Retrieval Augmented Generation for Controllable Traffic Scenarios",
            "text": "Retrieval Augmented Generation (RAG) [8,25], which enhances the generative process by querying related information from external databases, represents great potential in the domain of large language models [37]. In contrast to conventional models that memorize all knowledge within their parameters, as shown in Figure 1(a), RAG models, shown in Figure 1(b), learn to generate comprehensive outputs by retrieving pertinent knowledge from a database, based on the input provided. A notable aspect of RAG is the ability of the database to undergo updates even after the model has been trained, allowing continuous improvement and adaptation. This flexible framework offers possibilities for controllable scenario generation by using appropriate template scenarios as input and facilitating the generation that is not only realistic but also aligned with specific training and evaluation requirements. \n\nIn this work, we present RealGen, a retrieval augmented generation framework for generating traffic scenarios. This framework, as shown in Figure 2, begins with the training of an encoder through contrastive self-supervised learning [42] to allow the retrieval process to query similar scenarios in a latent embedding space. Leveraging this latent representation, we subsequently train a generative model that combines retrieved scenarios to create novel scenarios. \n\nThe key contributions of this paper are summarized below. \n\n-We develop a novel contrastive autoencoder model to extract scenario embeddings as latent representations, which can be used for a wide range of downstream tasks. -We propose the first retrieval augmented generation framework using the latent representation tailored for controllable driving scenario generation. -We validate our framework through qualitative and quantitative metrics, demonstrating strong flexibility and controllability of generated scenarios. \n\n2 Related Work",
            "score": 0.5814534364394461,
            "section_title": "Introduction",
            "char_start_offset": 2478,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 898
                },
                {
                    "start": 901,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1426
                },
                {
                    "start": 1429,
                    "end": 1592
                },
                {
                    "start": 1593,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1892
                },
                {
                    "start": 1895,
                    "end": 1909
                }
            ],
            "ref_mentions": [
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84130859375
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "In this section, we analyze the efficacy of the two-stage fine-tuning in the Invar-RAG architecture, including the retrieval stage (LLM-aligned Retrieval with Invariance Loss) and the generation stage. We design three variants: (1)w/o representation learning: this variant uses the coarse text representation mapped by small language model (MiniLM-v26 ) to calculate the relevance score and adopt the same generation fine-tuning method in Sec. \u2022 With the representation learning method, LLM-based retrieval contributes to improving the retrieval and corresponding generation performance. \u2022 Invariance loss significantly boosts our designed Invar-RAG by making the prediction rely more on invariant patterns. \u2022 Generative fine-tuning is crucial for enhancing LLM's capability of giving predictions based on retrieved information. Moreover, it shows the effectiveness of the two-stage fine-tuning for a single LLM. \n\nFor ablation results on other two datasets (NQ and PopQA), please found them in Appendix E.",
            "score": 0.5799320322460919,
            "section_title": "Ablation Study",
            "char_start_offset": 19303,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 912
                },
                {
                    "start": 915,
                    "end": 1006
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73193359375
        },
        {
            "corpus_id": "263835211",
            "title": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and Domain-Specificity",
            "text": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach to address certain limitations inherent to standalone LLMs, such as outdated information and the inability to memorize (Chase, 2022;Liu, 2022). These challenges are elaborated upon in Sec 4.2.1. Yet, while RAG offers solutions to some issues, it introduces its own set of challenges, including the potential for insufficient information and the misinterpretation of related data, as detailed in Sec 4.2.2. This subsection delves into various strategies devised to mitigate these challenges. Within the realm of retrieval-augmented generation, enhancement techniques can be broadly categorized into several pivotal areas: \n\n(1) The Normal Setting of Utilizing Retrieved Text for Generations (Sec 5.2.1): \n\n(2) Interactive Retrieval and Generation (Sec 5.2.2): Examples here include the integration of Chain-of-Thoughts steps into query retrieval (He et al., 2022) and the use of an LLMbased agent framework that taps into external knowledge APIs (Yao et al., 2023a). \n\n(3) Adapting LLMs to the RAG Setting (Sec 5.2.3): This involves methods like the one proposed by Peng et al. (2023), which combines a fixed LLM with a plug-and-play retrieval module. Another notable approach is REPLUG (Shi et al., 2023), a retrieval-augmented framework that treats the LLM as a black box and fine-tunes retrieval models using language modeling scores. \n\n(4) Retrieving from Additional Knowledge Bases (Sec 5.2.5 and Sec 5.2.4): This category includes methods that retrieve from external parametric memories (Chen et al., 2023a) or knowledge graphs (Zhang et al., 2023d) to enhance the model's knowledge base.",
            "score": 0.579566464729041,
            "section_title": "On Retrieval-Augmented Generation",
            "char_start_offset": 107043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 272
                },
                {
                    "start": 273,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 698
                },
                {
                    "start": 701,
                    "end": 780
                },
                {
                    "start": 783,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1414
                },
                {
                    "start": 1417,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1671
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 210,
                    "matchedPaperCorpusId": "253107178"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.74609375
        },
        {
            "corpus_id": "272753572",
            "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
            "text": "RAG (Retrieval-Augmented Generation) is an effective strategy for enhancing model knowledge by retrieving relevant information from external sources (Fan et al., 2024). RAG has been leveraged in various scenarios including knowledge-intensive question answering (Borgeaud et al., 2022;Guu et al., 2020), machine translation (He et al., 2021), and hallucination elimination (B\u00e9chard & Ayala, 2024). Current works has focused on improving specific aspects of RAG. RG-RAG (Chan et al., 2024) proposes to refine the query for retrieval by decomposition and disambiguation. Self-RAG (Asai et al., 2023) incorporates the self-reflection of LLM to enhance the generation quality. The AI search engine could be viewed as a form of RAG with the Internet serving as the external knowledge source. Recently, MindSearch (Chen et al., 2024c) proposes an AI search engine framework to simulate the human minds in web information seeking. Meanwhile, multiple benchmarks of RAG (Yang et al., 2024b;Chen et al., 2024b) have been introduced to comprehensively evaluate a RAG system. However, both the current AI search engine and RAG benchmark are limited to the text-only setting, leaving the multimodal search engine and evaluation largely unexplored. To bridge this gap, we introduce MMSEARCH-ENGINE and MMSEARCH, a multimodal AI search engine pipeline and dataset designed to evaluate various multimodal scenarios.",
            "score": 0.5787424703827213,
            "section_title": "A RELATED WORK",
            "char_start_offset": 36563,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 168
                },
                {
                    "start": 169,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1400
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 167,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 262,
                    "end": 285,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 285,
                    "end": 302,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 324,
                    "end": 341,
                    "matchedPaperCorpusId": "236460076"
                },
                {
                    "start": 808,
                    "end": 828,
                    "matchedPaperCorpusId": "269362546"
                },
                {
                    "start": 982,
                    "end": 1001,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77978515625
        },
        {
            "corpus_id": "267301416",
            "title": "The Power of Noise: Redefining Retrieval for RAG Systems",
            "text": "Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first comprehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages included in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever's highest-scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to 35%. These results highlight the need to investigate the appropriate strategies when integrating retrieval with LLMs, thereby laying the groundwork for future research in this area.",
            "score": 0.5782462111574993,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "268509926",
            "title": "DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time Information Needs of Large Language Models",
            "text": "In recent years, large language models (LLMs) have made significant advancements across various natural language processing (NLP) tasks, quickly becoming a critical element in numerous AI applications (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a;Scao et al., 2022;Zhang et al., 2022). Despite their impressive capabilities, these models often produce text that seems coherent and plausible but factually incorrect, a problem commonly known as hallucination (Maynez et al., 2020;Zhou et al., 2020;Liu et al., 2021;Ji et al., 2023;Su et al., 2024). \n\nTo mitigate this issue, Retrieval-Augmented Generation (RAG) has emerged as a prominent solution. RAG enhances LLMs by retrieving and incorporating relevant information from external databases into the LLMs' inputs. It has demonstrated superior effectiveness across numerous NLP challenges (Khandelwal et al., 2019;Borgeaud et al., 2022;Lewis et al., 2020;Guu et al., 2020;Izacard and Grave, 2020;Jiang et al., 2022;Shi et al., 2023). Traditional methods of RAG typically rely on single-round retrieval, using the LLM's initial input to retrieve relevant information from external corpora. While this method is effective for straightforward tasks, it tends to fall short for complex multi-step tasks and long-form generation tasks (Jiang et al., 2023). In contrast, dynamic RAG (Trivedi et al., 2022;Borgeaud et al., 2022;Ram et al., 2023;Jiang et al., 2023) performs multiple times of retrieval during the generation process of LLMs. It includes two steps: identifying the optimal moment to activate the retrieval module (deciding when to retrieve), and crafting the appropriate query once retrieval is triggered (determining what to retrieve). Depending on when and what to retrieve, a variety types of methods have been proposed in this direction.",
            "score": 0.5773432225086983,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1003
                },
                {
                    "start": 1004,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1503
                },
                {
                    "start": 1504,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1819
                }
            ],
            "ref_mentions": [
                {
                    "start": 533,
                    "end": 549,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 884,
                    "end": 906,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 906,
                    "end": 925,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 925,
                    "end": 942,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1369,
                    "end": 1391,
                    "matchedPaperCorpusId": "244954723"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76171875
        },
        {
            "corpus_id": "273229293",
            "title": "Retrieving, Rethinking and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation",
            "text": "In this paper, we introduce a novel retrieval augmented generation method, CoV-RAG. It can effectively mitigate hallucinations during internal generation stage and external retrieval stage in the RAG. Specifically, by integrating the chain of verification prompting into fine-tuned RAG generators, we can successfully identify and mitigate generation errors. In addition, the chain of verification prompting can also refine external contextual knowledge through re-retrieving the revised query. We conduct a various experiments to assess the effectiveness of CoV-RAG over different language model backbones. And experimental results demonstrate that the CoV-RAG can well detect the generation errors, and significantly improve the generation quality. Looking ahead, CoV-RAG paves the way for further research in refining knowledge augmentation strategies, contributing to the improvement of reliability and accuracy of RAG.",
            "score": 0.5771692211366124,
            "section_title": "Conclusion",
            "char_start_offset": 20200,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 83
                },
                {
                    "start": 84,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 494
                },
                {
                    "start": 495,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 750
                },
                {
                    "start": 751,
                    "end": 923
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) [127] has emerged as a key approach that integrates information retrieval with generative models to enhance natural language processing tasks. By leveraging external knowledge sources, RAG systems can generate more accurate and contextually relevant outputs, addressing complex challenges in areas like question answering [119], summarization [85], and open-domain dialogue. In recent years, a variety of RAG methods have been proposed, ranging from basic retrieval-augmented models to more advanced architectures incorporating multi-hop [190] reasoning and memory-augmented techniques [67]. These developments have highlighted the potential of RAG to improve the performance of NLP systems by dynamically combining retrieval and generation in a unified framework. 1 https://github.com/USTCAGI/Awesome-Papers-Retrieval-Augmented-Generation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \n\nRAG models augment traditional language models by incorporating external knowledge sources, such as documents, databases, or structured data [93,103]., during the generation process. Unlike conventional models that rely solely on pre-trained parameters, RAG systems dynamically retrieve relevant information at generation time, allowing them to produce more informed and contextually accurate outputs. This approach addresses key limitations of traditional language models, such as their inability to access real-time or domain-specific knowledge, and mitigates the challenge of handling out-of-vocabulary or rare entities. For example, in question answering tasks [62,192], RAG models retrieve relevant passages from large corpora to generate more precise and informative answers, while in summarization [85,172], they leverage external documents to provide richer and more comprehensive summaries.",
            "score": 0.5767797729161924,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1462
                },
                {
                    "start": 1465,
                    "end": 1616
                },
                {
                    "start": 1617,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2364
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 42,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 359,
                    "end": 364,
                    "matchedPaperCorpusId": "86611921"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "226965071"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "267412954",
            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
            "text": "Large Language Models (LLMs) generate responses to questions; however, their effectiveness is often hindered by sub-optimal quality of answers and occasional failures to provide accurate responses to questions. To address these challenges, a fine-tuning process is employed, involving feedback and examples to refine models. The objective is to enhance AI models through continuous feedback loops, utilizing metrics such as cosine similarity, LLM evaluation and Rouge-L scores to evaluate the models. Leveraging LLMs like GPT-3.5, GPT4ALL, and LLaMA2, and Claude, this approach is benchmarked on financial datasets, including the FinanceBench and RAG Instruct Benchmark Tester Dataset, illustrating the necessity of fine-tuning. The results showcase the capability of fine-tuned models to surpass the accuracy of zero-shot LLMs, providing superior question and answering capabilities. Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
            "score": 0.5761339457712238,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87744140625
        },
        {
            "corpus_id": "278368151",
            "title": "AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities",
            "text": "Retrieval-Augmented Generation (RAG) [19] offers a compelling alternative to the resource-intensive retraining and fine-tuning tasks by enhancing pre-trained models with external, domain-specific data. Instead of modifying the model's internal parameters, RAG integrates a retrieval mechanism that accesses upto-date and relevant information from external sources, thereby enriching the model's responses for specialized tasks. This approach not only minimizes computational demands but also ensures that the model remains adaptive and context-aware, making it an attractive solution for applications requiring continuous updates and precision in domain-specific outputs.",
            "score": 0.5758763397795322,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 7031,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 671
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.78076171875
        },
        {
            "corpus_id": "271909360",
            "title": "Reconciling Methodological Paradigms: Employing Large Language Models as Novice Qualitative Research Assistants in Talent Management Research",
            "text": "Retrieval Augmented Generation (RAG) \n\nIn our LLM based approaches, we experiment with four methods -zero-shot prompting, few-shot prompting, chain-of-thought reasoning, and Retrieval Augmented Generation based Question Answering. In zero-shot prompting we provide a single prompt to the model. In few-shot prompting, we provide a set of topics and anecdotes to the model as examples. In the chain of thought (COT) approach, we provide a set of instructions for the model to follow. Finally, for Retrieval Augmented Generation (RAG) we provide context and questions to the model, from which it extracts information. \n\nZero-shot prompts are simple instructions or tasks given to an LLM that have not been specifically trained on that task. It serves as a baseline because it demonstrates the model's fundamental ability to understand and respond to prompts based solely on its pre-training [19]. In few-shot prompting, a small set of examples illustrating the desired outcome are manually selected and provided to the LLM. These examples allow the model to understand the tasks at hand and generate similar results [5]. Chain-of-thought prompting provides a set of intermediate steps to guide the LLM to mimic human-like reasoning. This significantly improves the capability of the LLM to understand complex reasoning and generate better topics [36]. Retrieval-augmented generation (RAG) combines the capabilities of an LLM with a retrieval system to source and integrate additional information into its responses [23]. This effort provides contextually richer and ultimately more accurate outputs. We do this by providing all the interview transcripts to the LLM as a custom knowledge base. Two considerations helped the RAG approach outperform the other approaches: 4.1.1 Focused Analysis: In our approach, LLM searches the knowledge base to find and retrieve parts of documents that are most relevant to the question in the query. This narrows the focus to the most relevant information and ensures attention to critical topics and nuances.",
            "score": 0.5755561814031769,
            "section_title": "Thematic analysis enhanced through",
            "char_start_offset": 13846,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 36
                },
                {
                    "start": 39,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 615
                },
                {
                    "start": 618,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 894
                },
                {
                    "start": 895,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1349
                },
                {
                    "start": 1350,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1597
                },
                {
                    "start": 1598,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1772
                },
                {
                    "start": 1773,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2042
                }
            ],
            "ref_mentions": [
                {
                    "start": 1513,
                    "end": 1517,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.499267578125
        },
        {
            "corpus_id": "276421594",
            "title": "RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models",
            "text": "Retrieval augmented generation. Initially proposed to enhance the generative quality of NLP models by retrieving related information from external sources Khandelwal et al. (2019); Lewis et al. (2020); Guu et al. (2020), RAG has been extended to utilize local cohorts in the training data to facilitate image synthesis. For instance, rather than directly outputting the synthesized sample, Casanova et al. (2021) compute the average of the sample's nearest neighbors in the training data. Blattmann et al. (2022) use an external image dataset to provide enhanced conditional guidance, augmenting the text prompt during the text-to-image generation. Zhang et al. (2023) explore RAG to accelerate the inference process of a diffusion model by reusing pre-computed sample trajectories as surrogates for skipping intermediate sampling steps. However, existing work primarily focuses on employing RAG in the inference stage to improve generative quality or efficiency. \n\nx 0 < l a t e x i t s h a 1 _ b a s e 6 4 = \" W e n j 5 + I z / w + E a l g p 3 j X J y",
            "score": 0.5737423210408836,
            "section_title": "RELATED WORK",
            "char_start_offset": 4681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 648
                },
                {
                    "start": 649,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1053
                }
            ],
            "ref_mentions": [
                {
                    "start": 181,
                    "end": 200,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 202,
                    "end": 219,
                    "matchedPaperCorpusId": "211204736"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7861328125
        },
        {
            "corpus_id": "265067168",
            "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a promising strategy to mitigate hallucinations in LLMs. As concerns around LLM hallucinations have intensified, RAG has increasingly come under the spotlight, paving the way for a range of commercial applications, such as Perplexity 2 , YOU.com 3 and New Bing 4 . By retrieving evidence from external knowledge bases, RAG enables LLMs to be equipped with up-to-date knowledge and generate responses conditioning on relevant evidence. However, despite its advantages, RAG also suffers from hallucinations. One notable issue is the potential for error accumulation within the RAG pipeline. Irrelevant evidence can be propagated into the generation phase, possibly tainting the output (Li et al., 2023a;Shi et al., 2023a;Cho et al., 2023;Xu et al., 2023). Another concern lies in the arena of generative retrievals, which occasionally suffer from citation inaccuracies (Rashkin et al.;Liu et al., 2023f;Yue et al., 2023;Gao et al., 2023a;Chen et al., 2023a). While citations aim to offer a traceable path to the information's source for validation purposes, errors in this domain can lead users astray. Furthermore, existing RAG may suffer from a trade-off between diversity and factuality (Liu et al., 2023f) which poses a new challenge in terms of the need for diversity.",
            "score": 0.5733165427927828,
            "section_title": "Hallucination in Retrieval Augmented Generation",
            "char_start_offset": 97090,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 108
                },
                {
                    "start": 109,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1323
                }
            ],
            "ref_mentions": [
                {
                    "start": 735,
                    "end": 753,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 753,
                    "end": 771,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 771,
                    "end": 788,
                    "matchedPaperCorpusId": "264490648"
                },
                {
                    "start": 970,
                    "end": 988,
                    "matchedPaperCorpusId": "254247260"
                },
                {
                    "start": 988,
                    "end": 1007,
                    "matchedPaperCorpusId": "258865339"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2274169921875
        },
        {
            "corpus_id": "272911196",
            "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments",
            "text": "We release all models and generated datasets to facilitate further study on HuggingFace1 . In Section 2, we present some related work on RAG, fine tuning methods for RAG including RAFT, and PEFT techniques such as LoRA. In Section 3, we introduce our compute-efficient RAFT method, CRAFT, in detail. In Section 4, we present the experiment setup. In Section 5 we report the results, and in Section 6 we conclude the paper. \n\nRetrieval Augmented Generation (RAG) RAG (Lewis et al., 2021) enhances LLMs by retrieving relevant document chunks from external knowledge bases through semantic similarity calculations. This method mitigates the generation of factually incorrect content by referencing external knowledge rather than relying solely on knowledge the model learned during training, thereby improving the relevancy of the generated text while reducing \"hallucinations\". Despite its advantages, RAG faces challenges, particularly with domain-specific or knowledge-intensive tasks, particularly when handling queries beyond the scope of its retrieved data (Zhang et al., 2023), though at a lesser extent when compared to non-retrieval-augmented LLMs. Other major challenges with RAG includes requiring a high-performing retriever model to produce representative embeddings from the document chunks and retrieval system that balances scale and accuracy. Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024). RAG excels in dynamic environments by offering real-time knowledge updates and effective utilization of external knowledge sources with high interpretability. However, it comes with higher latency and the possibility of added noise from extraneous contexts. \n\nFine tuning for RAG Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning.",
            "score": 0.572773370384072,
            "section_title": "Introduction",
            "char_start_offset": 4339,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 299
                },
                {
                    "start": 300,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 422
                },
                {
                    "start": 425,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1154
                },
                {
                    "start": 1155,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1495
                },
                {
                    "start": 1496,
                    "end": 1654
                },
                {
                    "start": 1655,
                    "end": 1753
                },
                {
                    "start": 1756,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 2090
                },
                {
                    "start": 2091,
                    "end": 2235
                },
                {
                    "start": 2236,
                    "end": 2383
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82080078125
        },
        {
            "corpus_id": "277104989",
            "title": "A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM",
            "text": "Different use cases for Generative AI and RAG are available in the literature. The use of vector-based databases also finds a place in the literature. LLMs have been shown to experience hallucinations in their responses to instruction-following tasks. Fine-tuning with new data may not prevent hallucinations and may be costly. Therefore, RAG was introduced to provide factual information and adapt to new information [17]. RAG uses a vector database. When a query is made to RAG, the most relevant data from this database is selected. This data is included in the prompt for LLMs. This process can combine existing information without requiring constant fine-tuning and provides contextbased information, increasing the clarity and relevance of responses. RAG provides the LLM with the ability to gain a focused understanding vis-\u00e0-vis a query. There are studies in the literature where RAG is used in NLP tasks [22][23][24][25]. Pal et al. sought to create unique and enjoyable music using Generative AI. When provided with a starting bar of music, the initial Discriminatory network, incorporating Support Vector Machines and Neural Nets, selects a note or chord to guide the subsequent bar. Building upon this chosen chord or note, a Generative Net, which includes Generative Pretrained Transformers (GPT-2) and LSTMs, then generates the entire bar of music. Their innovative two-step approach aims to closely emulate the process of real music composition to enhance the authenticity of the generated music [26]. \n\nKulkarni et al. proposed a Retrieval-Augmented Generator (RAG) approach for constructing a chatbot capable of addressing user queries using Frequently Asked Questions (FAQ) data. The chatbot employs an open API-based paid ChatGPT model as a Language Model (LLM), recognizing the potential to optimize LLM token usage and cost by leveraging previously retrieved context for specific query patterns. The authors employ Reinforcement Learning (RL) to optimize the number of LLM tokens within the constraints of a fixed retrieval model and LLM. The proposed RL-based optimization, coupled with a similarity threshold, achieves notable cost savings with a slight enhancement in accuracy.",
            "score": 0.572193660119488,
            "section_title": "Related Works",
            "char_start_offset": 9365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 78
                },
                {
                    "start": 79,
                    "end": 150
                },
                {
                    "start": 151,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 451
                },
                {
                    "start": 452,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1697
                },
                {
                    "start": 1698,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2059
                },
                {
                    "start": 2060,
                    "end": 2201
                }
            ],
            "ref_mentions": [
                {
                    "start": 418,
                    "end": 422,
                    "matchedPaperCorpusId": "225930035"
                },
                {
                    "start": 913,
                    "end": 917,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 925,
                    "end": 929,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 1511,
                    "end": 1515,
                    "matchedPaperCorpusId": "241327053"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.55078125
        },
        {
            "corpus_id": "277814037",
            "title": "AI-driven FMEA: integration of large language models for faster and more accurate risk analysis",
            "text": "Generative AI refers to a subset of AI models that are able to generate new data instances that are similar to the training data. Unlike traditional AI systems that perform classification or prediction tasks based on existing data, generative models create new content, such as text, images or music, by learning the underlying patterns and structures in the training data. Techniques such as Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs) and transformer-based models are widely used in generative AI. These models have a wide range of applications, including content creation, data augmentation and simulation of environments for training other AI systems (Kingma & Welling 2013;Goodfellow et al. 2014). \n\nLLMs are a type of generative AI specifically designed to understand and generate human language. These models are trained on large amounts of text data and can perform a variety of tasks such as translation, summarization, question answering and text completion. Examples of LLMs include OpenAI's GPT-4, Google's Gemini and the open-source GPT-Neo model. They work by predicting the next word in a sequence so that they can produce coherent and contextually relevant text based on the input received (Brown et al. 2020;Rae et al. 2022). \n\nThere are several techniques to work with LLMs, including prompting, finetuning and Retrieval-Augmented Generation (RAG). Here you will find a brief explanation of each technique: a) Prompt Engineering Prompt engineering involves the creation of effective prompts to elicit the desired responses from LLMs. Since LLMs respond to the input text provided, the way a prompt is structured can significantly affect the output of the model. Effective prompt engineering can improve the accuracy and relevance of the content generated. Techniques include using specific keywords, providing detailed context and iteratively refining prompts based on the model's responses (Liu et al. 2021). b) Retrieval-Augmented Generation RAG combines the strengths of retrieval-based and generative models to improve the accuracy and relevance of AI-generated content. In a RAG system, a retrieval component first searches a large corpus of documents to find the most relevant information based on the input query. This information is then passed to a generative model that produces a coherent and contextually appropriate response.",
            "score": 0.5712440265796439,
            "section_title": "Generative AI and LLM",
            "char_start_offset": 9341,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 527
                },
                {
                    "start": 528,
                    "end": 730
                },
                {
                    "start": 733,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1088
                },
                {
                    "start": 1089,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2120
                },
                {
                    "start": 2121,
                    "end": 2266
                },
                {
                    "start": 2267,
                    "end": 2384
                }
            ],
            "ref_mentions": [
                {
                    "start": 706,
                    "end": 729,
                    "matchedPaperCorpusId": "10319744"
                },
                {
                    "start": 1234,
                    "end": 1253,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.64453125
        },
        {
            "corpus_id": "273532677",
            "title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large Language Models to Specialized Domains",
            "text": "Retrieval-augmented generation (RAG) (Lewis et al., 2020;Gao et al., 2023;Guti\u00e9rrez et al., 2024;Asai et al., 2024) is a powerful technique that enhances large language models (LLMs) for various knowledge-intensive tasks such as question answering (QA) by incorporating external knowledge sources. This method not only customizes responses to handle long-tail knowledge but also avoids the need for costly model retraining (Ovadia et al., 2023). Additionally, RAG helps reduce the issue of LLM hallucination by ensuring responses are grounded in relevant evidence (Shuster et al., 2021), thereby improving the overall accuracy and reliability of LLM outputs. \n\n* Work done during an internship at Amazon. \n\nWhile extensive research has focused on developing effective (Asai et al., 2024;Lin et al., 2024;Liu et al., 2024) and efficient (Xu et al., 2024a) RAG systems for general-domain QA tasks, adapting RAG to specialized domains for LLMs poses significant challenges. These models often struggle with distribution shifts and fail to accurately extract information from domain-specific contexts (Miller et al., 2020;Liu et al., 2022). Moreover, directly using black-box LLMs (OpenAI, 2023;Anthropic, 2023;Wang et al., 2023b) in specialized domains raises concerns about privacy when dealing with sensitive proprietary data. It is essential to finetune LLMs on domain-relevant QA tasks to unlock the full potential of LLM-based RAG systems in specialized domains. \n\nDespite the critical need for domain-specific finetuning, the primary challenge lies in the acquisition of high-quality fine-tuning data towards RAG applications. Prior works rely on continuous pretraining (Chen et al., 2023;Zhang et al., 2024a) on specialized corpora or fine-tuning on domain-specific instruction-tuning data (Wu et al., 2024;Wadden et al., 2024). However, the mismatch between these general-purpose tasks and domain-specific QA hinders their effectiveness.",
            "score": 0.5708909407750408,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 297
                },
                {
                    "start": 298,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1136
                },
                {
                    "start": 1137,
                    "end": 1325
                },
                {
                    "start": 1326,
                    "end": 1464
                },
                {
                    "start": 1467,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 97,
                    "end": 115,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 564,
                    "end": 586,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 768,
                    "end": 787,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1097,
                    "end": 1118,
                    "matchedPaperCorpusId": "216867120"
                },
                {
                    "start": 1118,
                    "end": 1135,
                    "matchedPaperCorpusId": "237417170"
                },
                {
                    "start": 1794,
                    "end": 1811,
                    "matchedPaperCorpusId": "269136910"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8662109375
        },
        {
            "corpus_id": "268819923",
            "title": "ARAGOG: Advanced RAG Output Grading",
            "text": "Large Language Models (LLMs) have significantly advanced the field of natural language processing, enabling a wide range of applications from text generation to question answering.However, integrating dynamic, external information remains a challenge for these models.Retrieval Augmented Generation (RAG) techniques address this limitation by incorporating external knowledge sources into the generation process, thus enhancing the models' ability to produce contextually relevant and informed outputs.This integration of retrieval mechanisms with generative models is a key development in improving the performance and versatility of LLMs, facilitating more accurate and context-aware responses.See Figure 1 for an overview of the standard RAG workflow.\n\nDespite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios.\n\nThe focus of this investigation is a spectrum of advanced RAG techniques aimed at optimizing the retrieval process.These techniques can be categorized into several areas: To evaluate the RAG techniques, this study leverages two metrics: Retrieval Precision and Answer Similarity (Tonic AI, 2023).Retrieval Precision measures the relevance of the retrieved context to the question asked, while Answer Similarity assesses how closely the system's answers align with reference responses, on a scale from 0 to 5.\n\nFigure 1: A high-level overview of the workflow within a Retrieval-Augmented Generation (RAG) system.This process diagram shows how a user query is processed by the system to retrieve relevant documents from a database and how these documents inform the generation of a response.",
            "score": 0.5707729010328457,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 180
                },
                {
                    "start": 180,
                    "end": 268
                },
                {
                    "start": 268,
                    "end": 502
                },
                {
                    "start": 502,
                    "end": 696
                },
                {
                    "start": 696,
                    "end": 754
                },
                {
                    "start": 756,
                    "end": 1033
                },
                {
                    "start": 1033,
                    "end": 1171
                },
                {
                    "start": 1171,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1556
                },
                {
                    "start": 1558,
                    "end": 1673
                },
                {
                    "start": 1673,
                    "end": 1854
                },
                {
                    "start": 1854,
                    "end": 2066
                },
                {
                    "start": 2068,
                    "end": 2169
                },
                {
                    "start": 2169,
                    "end": 2347
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "271270616",
            "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
            "text": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
            "score": 0.5701683400973736,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6533203125
        },
        {
            "corpus_id": "270562128",
            "title": "Retrieval-Augmented Generation for Generative Artificial Intelligence in Medicine",
            "text": "In the generation stage, both the user's query and the retrieved relevant information are prompted to the model to generate content.Compared to fine-tuning a model for a specialized task, RAG has been shown to improve accuracy for knowledge-intensive tasks 19 , and is a more flexible paradigm for model updates.\n\nIn this comment, we emphasize the significance of RAG in the era of generative AI, particularly its potential applications within medicine and healthcare.We primarily analyze the breakthroughs that RAG could bring to medicine from three perspectives: equity, reliability, and personalization (Figure 2).Additionally, we explore the limitations of RAG in medical application scenarios.",
            "score": 0.5694598271973799,
            "section_title": "Main Text",
            "char_start_offset": 2173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 132
                },
                {
                    "start": 132,
                    "end": 312
                },
                {
                    "start": 314,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 617
                },
                {
                    "start": 617,
                    "end": 698
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88330078125
        },
        {
            "corpus_id": "273233795",
            "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) has achieved significant progress in natural language processing by integrating large language models (LLMs) with external knowledge databases. This integration enhances the ability of generative models to produce accurate, relevant, and context-rich responses. Recent studies (Borgeaud et al., 2022;Jiang et al., 2024;Trivedi et al., 2022;Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2023;Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc. However, as a relative new research topic, the current RAG systems still suffer from some drawbacks, among which low performance and long latency are the most prominent ones. Addressing these problems would effectively make RAG more applicable to latency-sensitive LLM tasks. \n\nAs illustrated in Figure 1a, the workflow of a naive RAG system comprises two steps: retrieval and generation, combining offline preparation with online processing to enhance performance. In the offline phase, RAG utilizes embedding models such as BGE (Chen et al., 2024a)) and GTE (Li et al., 2023) to convert external knowledge sources (e.g., document chunks) into high-dimensional vectors, which are then indexed into a specialized vector database. Upon receiving a user request, RAG first accesses this vector database to perform a similarity search, retrieving documents that best match the request based on semantic content. Subsequently, RAG integrates the content of these retrieved documents with the original user request to form an augmented query, which is input into the LLM to generate a more informative and contextually relevant response (Topsakal & Akinci, 2023). Researchers have proposed various methods to optimize the performance of RAG systems. Some approaches modify the attention computation mechanism to reduce computational complexity (Wang et al., 2020;Choromanski et al., 2020;Monteiro et al., 2024).",
            "score": 0.5679316998234354,
            "section_title": "RELATED WORK",
            "char_start_offset": 4843,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 319
                },
                {
                    "start": 320,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1032
                },
                {
                    "start": 1035,
                    "end": 1222
                },
                {
                    "start": 1223,
                    "end": 1486
                },
                {
                    "start": 1487,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2163
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 335,
                    "end": 358,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 398,
                    "end": 415,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 626,
                    "end": 653,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1889,
                    "end": 1914,
                    "matchedPaperCorpusId": "260223847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89599609375
        },
        {
            "corpus_id": "274776379",
            "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
            "text": "The core idea of RAG technology is to introduce an external retrieval module that dynamically retrieves relevant information during the generation process, thereby improving the performance of generative models. In visual-linguistic tasks, RAG effectively compensates for the limitations of scarce knowledge by combining external knowledge bases. The model is not only able to extract information from images but also retrieves supplementary knowledge via the retrieval mechanism, thereby improving the quality and accuracy of the generated output. Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities. \n\nBuilding upon this, Shao et al. [40] further explore the application of RAG in VQA tasks. They propose that by integration the retrieval mechanism into pre-trained VLMs, model performance in complex reasoning tasks could be significantly enhanced. Furthermore,Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance. Meanwhile, Zheng et al. [42] point out that RAG technology not only enhances the model's generative capabilities but also improves its flexibility and adaptability in handling complex multimodal tasks, especially when dealing with tasks lacking sufficient annotations or background knowledge. \n\nAs RAG technology continues to deepen its application across various tasks, the key challenge, especially in opendomain VQA tasks, lies in how to dynamically retrieve relevant background knowledge through the retrieval mechanism to improve the model's reasoning accuracy [43]. Although RAG can provide more contextual information, optimizing the retrieval and generation processes, as well as handling the vast amounts of potential external knowledge, remains a current research challenge.",
            "score": 0.567900283373747,
            "section_title": "A. RAG Technologies",
            "char_start_offset": 7565,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 1106
                },
                {
                    "start": 1109,
                    "end": 1198
                },
                {
                    "start": 1199,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1598
                },
                {
                    "start": 1599,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2191
                },
                {
                    "start": 2194,
                    "end": 2470
                },
                {
                    "start": 2471,
                    "end": 2683
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.94384765625
        },
        {
            "corpus_id": "271534082",
            "title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph",
            "text": "The RAG (Retrieval-Augmented Generation) model combines retrieval and generation to produce answers by integrating document retrieval and generation models (Lewis et al., 2020). To generate an answer to a question, the model first retrieves relevant documents and then uses them to produce the response. This process helps in generating accurate answers to questions. The RAG model can handle various types of questions effectively, even when there is a lack of specific domain knowledge. Consequently, it enhances the accuracy and consistency of information compared to traditional generative models. \n\nThe RAG model consists of two main stages: \n\n\u2022 Retrieval Stage: Information relevant to the given question is retrieved through a search engine. \u2022 Generation Stage: Answers are generated based on the retrieved information.",
            "score": 0.5660969635132108,
            "section_title": "Overview of the RAG Model",
            "char_start_offset": 4778,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 601
                },
                {
                    "start": 604,
                    "end": 646
                },
                {
                    "start": 649,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 826
                }
            ],
            "ref_mentions": [
                {
                    "start": 156,
                    "end": 176,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7431640625
        },
        {
            "corpus_id": "275357908",
            "title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
            "text": "As more and more Generative AI (GenAI) applications are integrated into real-world production systems, Retrieval-Augmented Generation (RAG) has been adopted in industry as a common technique to improve the output of Large Language Models (LLMs). RAG alleviates inherent LLM pitfalls such as propensity to hallucinate, generating outdated knowledge, and lack of traceability to data sources (Fan et al., 2024;Gao et al., 2024). \n\nIntroducing a retrieval step into the generation process introduces, however, several practical challenges. While an LLM with a large number of parameters, such as GPT-4 (OpenAI et al., 2024), can be prompted to work with any kind of input and generate any kind of textual output, the retriever needs to be small, fast, and perform well with data sources that tend to be domain-specific. : Given an ecosystem of RAG applications, how do we build a retriever that can adapt to a specific domain and to a variety of retrieval tasks? \n\nOff-the-shelf retrievers of different sizes are available to AI practitioners. Embedding services such as Voyage1 perform well on open-source benchmarks but they do not necessarily generalize to the kind of data seen in real-world settings, especially when this data is structured and comes from existing databases. \n\nAnother practical challenge is achieving scalability and generalization across different GenAI use cases that depend on retrieval. A crucial advantage of LLMs compared to traditional machine learning models is that they can generalize to a myriad of tasks due to vast amounts of pretraining data and instruction fine-tuning (Wei et al., 2022;Zhang et al., 2024a;Ouyang et al., 2022). But if the retriever does not perform well and fast across many retrieval tasks, the downstream generation will be negatively affected. \n\nThe problem we are trying to solve is then: how to adapt the retrieval step to a specific domain and to a variety of retrieval tasks? In this work, we are not interested in the choice of LLM, assuming that improvements in the retrieved results translate into improvements in the downstream generation task.",
            "score": 0.5654537813903815,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 245
                },
                {
                    "start": 246,
                    "end": 426
                },
                {
                    "start": 429,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1410
                },
                {
                    "start": 1411,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1799
                },
                {
                    "start": 1802,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2108
                }
            ],
            "ref_mentions": [
                {
                    "start": 390,
                    "end": 408,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1604,
                    "end": 1622,
                    "matchedPaperCorpusId": "237416585"
                },
                {
                    "start": 1642,
                    "end": 1662,
                    "matchedPaperCorpusId": "246426909"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85791015625
        },
        {
            "corpus_id": "273501949",
            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
            "text": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]. These models, equipped with extensive training datasets and substantial parameters, have led advancements in these fields. However, they face persistent challenges such as the potential for outdated or incorrect information and difficulties with real-time updates [8], [10], [12], [17]. The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches. \n\nSELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]. Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23].",
            "score": 0.5647296189373872,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 3739,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 530
                },
                {
                    "start": 531,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 858
                },
                {
                    "start": 859,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1163
                },
                {
                    "start": 1166,
                    "end": 1336
                },
                {
                    "start": 1337,
                    "end": 1498
                },
                {
                    "start": 1499,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1826
                },
                {
                    "start": 1827,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2165
                },
                {
                    "start": 2166,
                    "end": 2358
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8994140625
        },
        {
            "corpus_id": "273950321",
            "title": "IntellBot: Retrieval Augmented LLM Chatbot for Cyber Threat Knowledge Delivery",
            "text": "Retrieval Augmented Generation (RAG) models, when combined with LLMs, significantly enhance chatbot accuracy and relevance. RAG integrates retrieval-based and generation-based approaches to derive precise responses from documents. In RAG, a user query is first processed by a retriever model, which searches a database or document corpus using techniques like TF-IDF, BM25, or dense embeddings (e.g., BERT) to find top relevant documents. These documents are then formatted to provide context for a generative model, such as GPT, which combines the query with the retrieved information to generate a coherent, fact-based response.",
            "score": 0.564725060432991,
            "section_title": "B. RAG Model",
            "char_start_offset": 6759,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 630
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67236328125
        },
        {
            "corpus_id": "273346841",
            "title": "Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning",
            "text": "Large Language Models (LLMs) have demonstrated exceptional capabilities, achieving stateof-the-art performance on various tasks (Brown, 2020;Chowdhery et al., 2023;Bubeck et al., 2023). Despite their success, they still face notable challenges, particularly in generating hallucinations and dealing with outdated knowledge (Gao et al., 2023). Retrieval-Augmented Generation (RAG) has emerged as a promising approach to mitigate these issues (Peng et al., 2023;Ren et al., 2023;Lewis et al., 2020). By integrating external information relevant to a specific query, RAG enhances the generative process with supplementary non-parametric knowledge. However, typical RAG * Equal contribution. \u2020 Corresponding author. frameworks lack effective control mechanisms for managing internal and external knowledge (Li et al., 2023a), which presents two main challenges: First, the conflicts between external knowledge and the internal memory of LLMs can, in certain cases, prevent the model from effectively following external evidence to produce accurate responses (Wu et al., 2024b;Li et al., 2023a). Secondly, the inherent imperfections of retrieval mechanisms mean that the retrieved contexts might include irrelevant noises (Creswell et al.), which can mislead the LLMs, leading to degraded performance (Fang et al., 2024;Xu et al., 2024). \n\nTo address the above issues, some approaches optimize knowledge selection in the RAG process through carefully designed prompts (Zhou et al., 2023). However, such methods do not fundamentally improve LLMs' ability to integrate external knowledge, leading to suboptimal outcomes in certain situations. Other methods focus on altering the behavioral patterns of LLMs through training techniques such as instruction tuning (Li et al., 2023a;Fang et al., 2024;Xu et al., 2024). Nevertheless, they lack differentiation in supervisory signals, leading to significant learning variance. On one hand, an excessive emphasis on adhering to context can lead models to pay attention to noisy information. On the other hand, prioritizing resistance to noise might cause models to overlook critical evidence present within the context (Wu et al., 2024a).",
            "score": 0.5645555075739688,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1332
                },
                {
                    "start": 1335,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1914
                },
                {
                    "start": 1915,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2175
                }
            ],
            "ref_mentions": [
                {
                    "start": 141,
                    "end": 164,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 477,
                    "end": 496,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 802,
                    "end": 820,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1072,
                    "end": 1089,
                    "matchedPaperCorpusId": "253420654"
                },
                {
                    "start": 1755,
                    "end": 1773,
                    "matchedPaperCorpusId": "253420654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.73583984375
        },
        {
            "corpus_id": "268819582",
            "title": "RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) represents an effective strategy for enhancing a model's capability to leverage non-parametric knowledge by retrieving external data resources, rather than relying solely on its intrinsic parametric knowledge during generation.This paradigm has garnered widespread attention across both industry and academia, proving its efficacy in a variety of scenarios such as question answering (Lewis et al., 2020), code generation (Zhou et al., 2022), alignment with human values (Xu et al., 2023a) and reducing hallucinations (Shuster et al., 2021).Recent advancements in RAG systems can generally be categorized into two areas: improvements in either the retrieval component or the generation component of the system.",
            "score": 0.5642473185802919,
            "section_title": "Related Works",
            "char_start_offset": 19385,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 578
                },
                {
                    "start": 578,
                    "end": 747
                }
            ],
            "ref_mentions": [
                {
                    "start": 421,
                    "end": 441,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86474609375
        },
        {
            "corpus_id": "273662368",
            "title": "LLM Robustness Against Misinformation in Biomedical Question Answering",
            "text": "In recent years, generative artificial intelligence (AI) has become increasingly ubiquitous, with a plethora of tools that allow even non-experts to easily access and use AI systems (for example, for natural language generation, image or video generation, etc.). Now, a wide range of individuals and organizations can almost effortlessly incorporate the capabilities of advanced AI into their workflows and products. However, this ease of access has also led to an increase in the misuse of generative AI, such as producing fake images or generating misinformation (i.e., the information that appears plausible but is factually incorrect) that often lands on the Internet. \n\nOne problem with incorrect synthetic information on the web is when the next generation of AI models (for example, large language models) is trained on this data. For instance, the work by Shumailov et al. [1] introduced a concept of a \"model collapse\" that is described as irreversible defects in the model trained on synthetic content leading to partial or complete disappearance of the original content distribution. Thus, (factually) incorrect content present in the training data may lead over time to the situation that AI models would \"memorize\" only the erroneous information. \n\nAnother problem can arise when web search is used to steer answer generation with a large language model using a retrieval-augmented generation approach [2]. The strengths of RAG-based approaches are that they allow: (1) to increase the LLM-generated answer accuracy by complementing or overriding the model's parametric (i.e., internal) knowledge with non-parametric (i.e., external) knowledge and (2) to provide the answer's provenance information, for example, by referencing retrieved information sources. However, the RAG bottleneck is its susceptibility to irrelevant context [3] or misinformation, i.e., incorrect evidence information [4], during the model prompting. \n\nIn this work, we investigate to what extent LLMs can be deceived by other LLMs into generating an incorrect answer in the RAG scenario in the domain of biomedical question answering. However, we do not address the retrieval aspects and focus only on the augmented generation (AG) part.",
            "score": 0.5639207166101243,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 672
                },
                {
                    "start": 675,
                    "end": 837
                },
                {
                    "start": 838,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1259
                },
                {
                    "start": 1262,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1771
                },
                {
                    "start": 1772,
                    "end": 1936
                },
                {
                    "start": 1939,
                    "end": 2121
                },
                {
                    "start": 2122,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 881,
                    "end": 884,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1415,
                    "end": 1418,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1479,
                    "end": 1482,
                    "matchedPaperCorpusId": "271448069"
                },
                {
                    "start": 1661,
                    "end": 1664,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1844,
                    "end": 1847,
                    "matchedPaperCorpusId": "256459776"
                },
                {
                    "start": 1904,
                    "end": 1907,
                    "matchedPaperCorpusId": "266163801"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.353515625
        },
        {
            "corpus_id": "271329121",
            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
            "text": "In recent years, with the rapid development of human-computer dialogue, a key technology in this field, generative dialogue models [1,2], has shown great potential and wide application prospects. From the early sequence-to-sequence (Seq2Seq) [3] architecture to recent innovations based on the Transformer [4] model with attention mechanisms, more advanced models are constantly emerging. However, generative dialogue models still confront significant challenges in accuracy, consistency, coherence, security, and resource efficiency. Enhancing their performance is a critical issue that demands attention. \n\nTo tackle more complex and diverse NLP tasks, the chainof-thought (CoT) method [5,6,7,8] has been proposed. Chainof thought breaks down complex reasoning tasks into multiple intermediate steps that are computed sequentially to obtain the final result. It not only improves the logical consistency of the model's responses but also enhances user interaction experiences. However, recent studies have shown that chainof-thought prompting method requires models of \u02dc100 billion parameters to fully release their reasoning ability [5], and thus will have a significant demands on computational resources. \n\nRetrieval Augmented Generation (RAG) [9] is also a promising method to improve the performance of generative dialogue models [10,11,12,13]. Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge. \n\nThis paper studies a method that combines the chain-ofthought with retrieval augmented generation for Supervised Fine-Tuning (SFT) small-scale models to optimize their performance in reasoning tasks, which is called RAFT (Retrieval Augmented Fine-Tuning) [14].",
            "score": 0.5631880170149608,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 196,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 716
                },
                {
                    "start": 717,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 978
                },
                {
                    "start": 979,
                    "end": 1209
                },
                {
                    "start": 1212,
                    "end": 1351
                },
                {
                    "start": 1352,
                    "end": 1510
                },
                {
                    "start": 1511,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1955
                },
                {
                    "start": 1956,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2358
                }
            ],
            "ref_mentions": [
                {
                    "start": 131,
                    "end": 134,
                    "matchedPaperCorpusId": "208267760"
                },
                {
                    "start": 134,
                    "end": 136,
                    "matchedPaperCorpusId": "252907770"
                },
                {
                    "start": 242,
                    "end": 245,
                    "matchedPaperCorpusId": "7961699"
                },
                {
                    "start": 306,
                    "end": 309,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 688,
                    "end": 691,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 691,
                    "end": 693,
                    "matchedPaperCorpusId": "252917648"
                },
                {
                    "start": 695,
                    "end": 697,
                    "matchedPaperCorpusId": "252762395"
                },
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1249,
                    "end": 1252,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1337,
                    "end": 1341,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1341,
                    "end": 1344,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1344,
                    "end": 1347,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1347,
                    "end": 1350,
                    "matchedPaperCorpusId": "207870430"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90380859375
        },
        {
            "corpus_id": "269293655",
            "title": "Evaluating Retrieval Quality in Retrieval-Augmented Generation",
            "text": "Retrieval-augmented generation (RAG) has emerged as a prominent approach in natural language processing, combining the strengths of retrieval and generation models [35], with use cases in decreasing hallucination [1,29], knowledge-grounding [9,16,34], and personalization [25,26].Evaluating RAG systems is important as it ensures the effectiveness of integrating retrieval-based methods with generative models [10,23].Traditionally, RAG evaluation has primarily relied on end-to-end assessment, which entails comparing the generated output with one or more ground truth references [20].While this is crucial, it presents several limitations, especially, for evaluating retrieval models in RAG systems.\n\nFirst, end-to-end evaluation lacks transparency regarding which retrieved document contributed to the generated output, hindering interpretability of the system's behavior.Secondly, it is resourceintensive, consuming significant time and computational power, particularly when dealing with a large set of retrieval results consumed by the LLM.To process long input sequences resulting from the utilization of all retrieved documents by the LLM, GPUs with substantial memory capacities are essential for end-to-end evaluation.Moreover, many ranking systems rely on interleaving (i.e., replacing one or more documents in the result list) for evaluation and optimization, which further complicates the evaluation, as slight variations in retrieval results necessitate re-computation of the RAG pipeline.Finally, optimizing ranking models often requires document-level feedback, such as user clicks [3,6].However, endto-end evaluation only provides list-level feedback for the retrieval results.That said, this paper studies retrieval evaluation in RAG.\n\nHuman annotations can be a potential solution for evaluating retrieval models in RAG, however, accurate annotations are often challenging and costly to obtain.More recently, with the emergence of large language models (LLMs) and their advanced capabilities in reasoning and text comprehension, they have been utilized to annotate documents for retrieval evaluation [10,23].Nevertheless, these approaches predominantly evaluate the retriever in RAG systems based on human preferences, whereas the primary objective of the retrieval model in RAG is to serve the LLM that leverages the retrieved results [35].",
            "score": 0.5630610345811973,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 280,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 701
                },
                {
                    "start": 703,
                    "end": 875
                },
                {
                    "start": 875,
                    "end": 1046
                },
                {
                    "start": 1046,
                    "end": 1228
                },
                {
                    "start": 1228,
                    "end": 1503
                },
                {
                    "start": 1503,
                    "end": 1604
                },
                {
                    "start": 1604,
                    "end": 1694
                },
                {
                    "start": 1694,
                    "end": 1752
                },
                {
                    "start": 1754,
                    "end": 1913
                },
                {
                    "start": 1913,
                    "end": 2127
                },
                {
                    "start": 2127,
                    "end": 2360
                }
            ],
            "ref_mentions": [
                {
                    "start": 164,
                    "end": 168,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 216,
                    "end": 219,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 241,
                    "end": 244,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 244,
                    "end": 247,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 247,
                    "end": 250,
                    "matchedPaperCorpusId": "269605438"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 581,
                    "end": 585,
                    "matchedPaperCorpusId": "221507798"
                },
                {
                    "start": 1598,
                    "end": 1601,
                    "matchedPaperCorpusId": "258212955"
                },
                {
                    "start": 1601,
                    "end": 1603,
                    "matchedPaperCorpusId": "2979013"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.736328125
        },
        {
            "corpus_id": "278000491",
            "title": "Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models",
            "text": "Large Language Model (LLM)-based tools are increasingly proposed as cognitive assistants in domains requiring human reasoning, such as education. However, these systems are prone to generating plausible yet incorrect or biased information [3]. Retrieval-Augmented Generation (RAG) has emerged as a key approach to mitigating these hallucinations by grounding responses in external knowledge sources, thereby enhancing reliability and contextual relevance. Since the foundational RAG framework was introduced, research has made significant strides in improving retrieval accuracy and addressing misinformation-related challenges [12]. This paper was presented at the 2025 ACM Workshop on Human-AI Interaction for Augmented Reasoning (AIREASONING-2025-01). This is the authors' version for arXiv.",
            "score": 0.5625730228539434,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 794
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "271114654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53271484375
        },
        {
            "corpus_id": "270703345",
            "title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long Context Utilization",
            "text": "Retrieval augmented generation.While LLMs exhibit strong capabilities (Gemini Team, 2023;  5. OpenAI, 2022;Touvron et al., 2023), their knowledge is inherently limited in its pretraining data, and they are observed to struggle in handling knowledge intensive tasks (Petroni et al., 2020).To tackle this, retrieval augmented generation (RAG) is an effective framework that retrieves relevant information from external knowledge sources to aid and ground language models' generation (Lewis et al., 2020;Khandelwal et al., 2020;Borgeaud et al., 2021;Izacard and Grave, 2021;Izacard et al., 2022b).\n\nAlthough RAG has powered many recent language model applications from questionanswering (Izacard and Grave, 2021) to automatic task completion (Shen et al., 2023), recent work show that LLMs tend to lost-in-the-middle, significantly hindering the full potential of RAG (Liu et al., 2023).In this work, we take a step further to understand the lost-in-the-middle problem from the viewpoint of attention bias.Moreover, we propose a remedy through attention calibration, which improves upon existing RAG frameworks.",
            "score": 0.5623372285056206,
            "section_title": "Related work",
            "char_start_offset": 21148,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 31,
                    "end": 288
                },
                {
                    "start": 288,
                    "end": 594
                },
                {
                    "start": 596,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 1003
                },
                {
                    "start": 1003,
                    "end": 1108
                }
            ],
            "ref_mentions": [
                {
                    "start": 481,
                    "end": 501,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 501,
                    "end": 525,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 547,
                    "end": 571,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 684,
                    "end": 709,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 739,
                    "end": 758,
                    "matchedPaperCorpusId": "257833781"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.673828125
        },
        {
            "corpus_id": "270620574",
            "title": "R^2AG: Incorporating Retrieval Information into Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) (Lewis et al., 2020) significantly enhances the capabilities of large language models (LLMs) by integrating external, non-parametric knowledge provided by retrievers. In RAG framework, the retriever locates and looks up useful documents based on a given query, and then the LLM interacts with these retrieved results to generate a response. The coordination of retrieval and generation achieves impressive performance without additional training. Especially in domain-specific and knowledge-intensive Figure 1: A comparison between RAG and R 2 AG. R 2 AG employs a trainable R 2 -Former to bridge the semantic gap between retrievers and LLMs. Optionally, LLMs can be fine-tuned to understand the retrieval information further. \n\ntasks, RAG offers real-time knowledge with high interpretability to LLMs, effectively mitigating the hallucination problem (Mallen et al., 2023). \n\nHowever, there exists a semantic gap between retrievers and LLMs due to their vastly different training objectives and architectures (BehnamGhader et al., 2022). Specifically, retrievers, typically encoder architecture, are designed to retrieve the most relevant documents for a query (Zhu et al., 2023b). Conversely, LLMs, generally decoder architecture, are expected to answer questions based on their inherent knowledge or given documents. However, the interaction between retrievers and LLMs in RAG primarily relies on simple text concatenation (BehnamGhader et al., 2022). This poor communication strategy will lead to several challenges for LLMs. Externally, it is hard for LLMs to utilize more information from retrievers in separate processes. In RAG, the retrieved documents that only preserve sequential relationships are unidirectionally delivered to LLMs, and LLMs do not fully understand why retrievers provide the documents. \n\nParticularly, low-quality documents inevitably appear in retrieved results (Barnett et al., 2024), but LLMs have to accept this noise passively. Internally, it is hard for LLMs to handle all of the retrieved documents with their inherent knowledge. LLMs must process all the results and assess which documents are important, impacting their ability to generate accurate answers (Wu et al., 2024).",
            "score": 0.5623098065117533,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 203
                },
                {
                    "start": 204,
                    "end": 377
                },
                {
                    "start": 378,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 763
                },
                {
                    "start": 766,
                    "end": 911
                },
                {
                    "start": 914,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1665
                },
                {
                    "start": 1666,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1999
                },
                {
                    "start": 2000,
                    "end": 2103
                },
                {
                    "start": 2104,
                    "end": 2251
                }
            ],
            "ref_mentions": [
                {
                    "start": 889,
                    "end": 910,
                    "matchedPaperCorpusId": "254877603"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6796875
        },
        {
            "corpus_id": "271213436",
            "title": "Revolutionizing Bridge Operation and maintenance with LLM-based Agents: An Overview of Applications and Insights",
            "text": "Retrieval Enhanced Generation (RAG) is a technique to improve the question and answer quality and interaction capabilities of generative AI by utilizing additional data resources without changing the parameters of Large Language Models. The workflow of RAG includes loading external documents, document segmentation, content vectorization, data retrieval, and finally answer generation (Lewis et al. 2020). However, a limitation of RAG is that it may lead to retrieval failure of valid results for the case of semantically identical but differently worded questions, a problem that decreases as the quality of the embedded model improves. \n\nCoT significantly improves the performance of Large Language Models by guiding them to progressively engage in the process of decomposing a complex question into a series of sub-problems and solving them sequentially (Fujita, Onaga, & Kano 2024). A generic template for the CoT technique should contain the question, reasoning process, and the answer as three core components to guide the reasoning and generation process of the model.",
            "score": 0.562235830169063,
            "section_title": "RAG and CoT",
            "char_start_offset": 32921,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 406
                },
                {
                    "start": 407,
                    "end": 638
                },
                {
                    "start": 641,
                    "end": 887
                },
                {
                    "start": 888,
                    "end": 1076
                }
            ],
            "ref_mentions": [
                {
                    "start": 386,
                    "end": 404,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 858,
                    "end": 886,
                    "matchedPaperCorpusId": "270227159"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6689453125
        },
        {
            "corpus_id": "268732879",
            "title": "CPR: Retrieval Augmented Generation for Copyright Protection",
            "text": "Retrieval Augmented Generation Retrival Augmented Generation (RAG) methods have been successfully applied to large language models (LLMs) [24,31,42,45,54].RAG has been shown to outperform even LLMs trained jointly on the training set and the retrievable data pool.RAG have also been explored for image synthesis [2,7,9,50,53,67,69].However, rather than reusing existing models, current methods require training of retrieval-specific architectures which -unlike the standard text-to-image diffusion models [46,47,49,68] -can be prompted with several retrieved images along with conditional information, such as text, as inputs.Instead, we explore RAG using more generic pretrained text-to-image diffusion models.[2,50,69] train a diffusion-based image retrieval model that can be prompted with latent image embeddings, while [9,67] use autoregressive generative models inspired from LLMs.\n\nImage Manipulation Several recent works [4,18,25,35,40,41,51,63] have provided methods for image manipulation, editing by either fine-tuning or changing the crossattention values at inference.With an appropriate retrieval function, and database such methods can be used to perform retrieval augmented generation by merging diffusion scores [12,23,38].However, the manipulation methods significantly lower inference speed.Instead, we opt to use the unCLIP model [47] to generate a backward flow using the retrieved images, and merge it with the flow generated by a base text-to-image diffusion model at inference [12,23,38,49].\n\nPrivacy Recent works [5,6,56,57] have shown that such models are able to memorize their training data.This raises several privacy challenges, including:\n\nUnlearning: Machine unlearning [3,20] enables users to delete their data from the weights of trained models [17,43].[3,14,19,23,34,39] provide training methods which makes unlearning efficient, for example by breaking the dataset into multiple shards and training separate models on each, followed by ensembling at inference.",
            "score": 0.5621887173760538,
            "section_title": "Related Works",
            "char_start_offset": 4266,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 155,
                    "end": 264
                },
                {
                    "start": 264,
                    "end": 332
                },
                {
                    "start": 332,
                    "end": 626
                },
                {
                    "start": 626,
                    "end": 711
                },
                {
                    "start": 711,
                    "end": 887
                },
                {
                    "start": 889,
                    "end": 1081
                },
                {
                    "start": 1081,
                    "end": 1240
                },
                {
                    "start": 1240,
                    "end": 1310
                },
                {
                    "start": 1310,
                    "end": 1515
                },
                {
                    "start": 1517,
                    "end": 1619
                },
                {
                    "start": 1619,
                    "end": 1669
                },
                {
                    "start": 1671,
                    "end": 1787
                },
                {
                    "start": 1787,
                    "end": 1996
                }
            ],
            "ref_mentions": [
                {
                    "start": 312,
                    "end": 315,
                    "matchedPaperCorpusId": "248377386"
                },
                {
                    "start": 505,
                    "end": 509,
                    "matchedPaperCorpusId": "232035663"
                },
                {
                    "start": 512,
                    "end": 515,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 711,
                    "end": 714,
                    "matchedPaperCorpusId": "248377386"
                },
                {
                    "start": 929,
                    "end": 932,
                    "matchedPaperCorpusId": "253581213"
                },
                {
                    "start": 938,
                    "end": 941,
                    "matchedPaperCorpusId": "254408780"
                },
                {
                    "start": 941,
                    "end": 944,
                    "matchedPaperCorpusId": "260125230"
                },
                {
                    "start": 947,
                    "end": 950,
                    "matchedPaperCorpusId": "251800180"
                },
                {
                    "start": 1229,
                    "end": 1233,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 1236,
                    "end": 1239,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "257078922"
                },
                {
                    "start": 1508,
                    "end": 1511,
                    "matchedPaperCorpusId": "249375227"
                },
                {
                    "start": 1511,
                    "end": 1514,
                    "matchedPaperCorpusId": "245335280"
                },
                {
                    "start": 1538,
                    "end": 1541,
                    "matchedPaperCorpusId": "229156229"
                },
                {
                    "start": 1541,
                    "end": 1543,
                    "matchedPaperCorpusId": "256389993"
                },
                {
                    "start": 1543,
                    "end": 1546,
                    "matchedPaperCorpusId": "254366634"
                },
                {
                    "start": 1705,
                    "end": 1708,
                    "matchedPaperCorpusId": "207863297"
                },
                {
                    "start": 1779,
                    "end": 1783,
                    "matchedPaperCorpusId": "216504112"
                },
                {
                    "start": 1783,
                    "end": 1786,
                    "matchedPaperCorpusId": "226968241"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6396484375
        },
        {
            "corpus_id": "274283400",
            "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags",
            "text": "While retrieval-augmented generation (RAG) enhances large language models (LLMs), it also introduces challenges that can impact accuracy and performance. In practice, RAG can obscure the intrinsic strengths of LLMs. Firstly, LLMs may become too reliant on external retrieval, underutilizing their own knowledge and reasoning, which can diminish responsiveness. Secondly, RAG may introduce irrelevant or low-quality data, adding noise that disrupts generation, especially with complex tasks. This paper proposes an RAG framework that uses reflective tags to manage retrieval, evaluating documents in parallel and applying the chain-of-thought (CoT) technique for step-by-step generation. The model selects the highest quality content for final output. The key contributions are as follows: (1) reducing hallucinations by focusing on high-scoring documents; (2) improving real-time performance through efficient retrieval; and (3) mitigating negative effects by filtering out irrelevant information using parallel generation and reflective tagging. These innovations aim to optimize RAG for more reliable, high-quality results.",
            "score": 0.5621391142177752,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.89306640625
        },
        {
            "corpus_id": "271720020",
            "title": "Creating a Taxonomy for Retrieval Augmented Generation Applications",
            "text": "The rise of new modes of interaction with artificial intelligence (AI) has significantly increased its popularity and broadened its applicability. Despite these advancements, the conceptual integration of AI in organizational settings remains limited, with a notable lack of systematic application (Uba et al. 2023). Among various AI applications, Retrieval-Augmented Generation (RAG) stands out due to its potential to transform information retrieval and content generation (Shuster et al. 2021). This was particularly evident following the public unveiling of OpenAI's models like ChatGPT in November 2022, where generative AI (genAI) has garnered much attention in both academic (B\u00f6hmann et al. 2023;Wessel et al. 2023) and industry sectors (McGrath 2024;McKinsey 2023). Recent studies on the potential of genAI, which largely rely on large language model (LLM) systems, ranging from automatization (Engel et al. 2023) to improving knowledge work (Anthony et al., 2023;Dell'Acqua et al., 2023) to creating novel business models (Kanbach et al. 2023). However, LLMs are not without flaws. In recent studies, Large Language Models (LLMs) have been identified to have several core limitations. These include a tendency to generate incorrect or misleading information (hallucinations) (Blom 2010), poor arithmetic capabilities, a lack of interpretative power, the high costs associated with model revisions, limitations in handling less popular or low-resource concepts and entities, and an inability to reference sources accurately (Barnett et al. 2024;Soudani et al. 2024;Zhao et al. 2024). Several approaches have been developed to mitigate the limitations of, while retrieval augmented generation (RAG) is as of now deemed as most promising (Gao et al. 2024). RAG primarily enhances LLMs by incorporating contextual information during the retrieval process, significantly improving the generated content's accuracy and consistency. Consequently, RAG improves LLM tasks and applications in various ways, as evidenced by recent studies (Asai et al. 2023;Jiang et al. 2023;Martino et al. 2023).",
            "score": 0.5619812660024349,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 146
                },
                {
                    "start": 147,
                    "end": 316
                },
                {
                    "start": 317,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1591
                },
                {
                    "start": 1592,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2094
                }
            ],
            "ref_mentions": [
                {
                    "start": 298,
                    "end": 314,
                    "matchedPaperCorpusId": "256902156"
                },
                {
                    "start": 475,
                    "end": 496,
                    "matchedPaperCorpusId": "233240939"
                },
                {
                    "start": 703,
                    "end": 721,
                    "matchedPaperCorpusId": "266784043"
                },
                {
                    "start": 744,
                    "end": 758,
                    "matchedPaperCorpusId": "268272501"
                },
                {
                    "start": 902,
                    "end": 921,
                    "matchedPaperCorpusId": "259476254"
                },
                {
                    "start": 950,
                    "end": 972,
                    "matchedPaperCorpusId": "255732846"
                },
                {
                    "start": 972,
                    "end": 996,
                    "matchedPaperCorpusId": "195208606"
                },
                {
                    "start": 1031,
                    "end": 1052,
                    "matchedPaperCorpusId": "261849160"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.611328125
        },
        {
            "corpus_id": "278033562",
            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
            "text": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.",
            "score": 0.5618744986981116,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92626953125
        },
        {
            "corpus_id": "271212697",
            "title": "Mitigating Interpretation Bias in Rock Records with Large Language Models: Insights from Paleoenvironmental Analysis",
            "text": "The generation module serves as the primary functional component within this system, receiving outputs from the data retrieval module and invoking the LLM module to summarize and condense information, ultimately generating conclusions.\n\nRetrieval-augmented generation (RAG) represents an innovative approach amalgamating retrieval-based methods with generative models, typically Language Models (LMs), to enrich the quality and relevance of generated text (Ciuc\u0103 et al., 2023;Gao et al., 2024).This article's expert question and answer system embodies the concept of RAG.In this paradigm, LLMs retrieve input questions (segmented into subqueries via CoT) from the data retrieval module to acquire information for enhancing generation.Subsequently, LLMs reconsider both the acquired information and original questions.During the reconsideration phase, prompt engineering, leveraging task description, input data, contextual information, and prompt style, enriches and standardizes LLM output (Zhao et al., 2023), enabling LLM to produce answers meeting predefined standards, akin to those of domain experts or proficient students.\n\nIn the realm of RAG research, advanced and modular RAG methodologies are evolving from the original or naive RAG approach (Gao et al., 2024).Advanced RAG incorporates additional processing stages pre-and post-retrieval.Pre-retrieval processing concentrates on optimizing data indexing via various methods, including data granularity refinement, index structure optimization, and metadata incorporation to enhance retrieval content quality.Postretrieval processing involves reranking and prompt compression, with embedding playing a crucial role.Optimization strategies encompass fine-tuning embedding or employing dynamic embedding methods.Embracing the design principles of Advanced RAG, we enhance the pre-retrieval stage with query planning and expansion, while bolstering post-retrieval stages with reranking and summarization techniques.",
            "score": 0.5613100876580677,
            "section_title": "Generation module",
            "char_start_offset": 18381,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 237,
                    "end": 494
                },
                {
                    "start": 494,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 734
                },
                {
                    "start": 734,
                    "end": 817
                },
                {
                    "start": 817,
                    "end": 1129
                },
                {
                    "start": 1131,
                    "end": 1272
                },
                {
                    "start": 1272,
                    "end": 1350
                },
                {
                    "start": 1350,
                    "end": 1570
                },
                {
                    "start": 1570,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1771
                },
                {
                    "start": 1771,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.65869140625
        },
        {
            "corpus_id": "277468036",
            "title": "Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization",
            "text": "Selective retrieval improves retrieval-augmented generation (RAG) by reducing distractions from low-quality retrievals and improving efficiency. However, existing approaches under-utilize the inherent knowledge of large language models (LLMs), leading to suboptimal retrieval decisions and degraded generation performance. To bridge this gap, we propose Self-Routing RAG (SR-RAG), a novel framework that binds selective retrieval with knowledge verbalization. SR-RAG enables an LLM to dynamically decide between external retrieval and verbalizing its own parametric knowledge. To this end, we design a multi-task objective that jointly optimizes an LLM on knowledge source selection, knowledge verbalization, and response generation. We further introduce dynamic knowledge source inference via nearest neighbor search to improve the accuracy of knowledge source decision under domain shifts. Fine-tuning three LLMs with SR-RAG significantly improves both their response accuracy and inference latency. Compared to the strongest selective retrieval baseline, SR-RAG reduces retrievals by 29% while improving the performance by 5.1%.",
            "score": 0.5612208118985413,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83642578125
        },
        {
            "corpus_id": "269148483",
            "title": "Introducing Super RAGs in Mistral 8x7B-v1",
            "text": "Retrieval-Augmented Generation (RAG) systems have emerged as pivotal tools in enhancing Language Model (LM) capabilities by incorporating external knowledge sources.Notably, RAG systems play a crucial role in mitigating inaccuracies inherent in LMs and facilitating real-time knowledge updates [6].These systems operate by retrieving pertinent information from extensive corpora, subsequently empowering the LM to produce outputs characterized by enhanced relevance and accuracy.However, the efficacy of RAGs hinges significantly upon the quality of the retrieved documents and their contextual relevance to the specific query under consideration [18].Thus, optimizing the selection and integration of retrieved documents is imperative to maximize the utility and effectiveness of RAG systems in augmenting LM performance.",
            "score": 0.5607741434800113,
            "section_title": "A. Background",
            "char_start_offset": 1137,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 298
                },
                {
                    "start": 298,
                    "end": 479
                },
                {
                    "start": 479,
                    "end": 652
                },
                {
                    "start": 652,
                    "end": 822
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.52294921875
        },
        {
            "corpus_id": "269188036",
            "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
            "text": "To provide clarity and structure, this paper is organized as follows: Section 2 outlines the overall RAG workflow, dividing the methodologies into pre-retrieval, retrieval, post-retrieval, and generation phases. Sections 3 through 6 explore the core techniques within each phase. Section 7 focuses on the evaluation methodologies for RAG. Section 8 summarizes the reviewed studies, detailing the retrievers and generators used, while Section 9 discusses challenges and future research directions, extending beyond text-based studies to include multimodal data applications. The paper concludes with Section 10. \n\nOther related surveys provide valuable insights into the evolving RAG landscape from different angles. Gao et al. [38] identified three key stages in RAG development: pre-training enhancement, inference, and fine-tuning. Zhao et al. [162] focused on the diverse applications of RAG, including text, code, image, and video generation, emphasizing augmented intelligence in generative tasks. Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications. \n\nIn this paper, we aim to offer a comprehensive and unified framework for understanding RAG from an information retrieval (IR) perspective, identifying key challenges and areas for improvement. We delve into the core technologies that drive RAG, assessing their effectiveness in addressing retrieval and generation tasks. Additionally, this survey introduces the evaluation methods employed in RAG research, highlights current limitations, and proposes promising avenues for future exploration.",
            "score": 0.5602354438806636,
            "section_title": "Introduction",
            "char_start_offset": 2330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 610
                },
                {
                    "start": 613,
                    "end": 715
                },
                {
                    "start": 716,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 1002
                },
                {
                    "start": 1003,
                    "end": 1212
                },
                {
                    "start": 1215,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1535
                },
                {
                    "start": 1536,
                    "end": 1708
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88720703125
        },
        {
            "corpus_id": "278535570",
            "title": "Patchwork: A Unified Framework for RAG Serving",
            "text": "Retrieval-Augmented Generation (RAG) [36] is a technique that enhances the capabilities of a Large Language Model (LLM) by supplementing it with an external, indexed knowledge base. In a typical RAG setup, a user query triggers the retrieval of relevant documents or data from this external source. The retrieved content is incorporated into a prompt and passed to the LLM (or another generative model, such as a multi-modal model) to generate a response. This augmentation improves accuracy and mitigates hallucinations. Unlike standard LLM serving [4,34,47], RAG systems involve multiple interconnected components. In this paper, we refer to the ordered sequence of these components as the RAG pipeline. Figure 2 illustrates a schematic of typical components a query might go through in a RAG pipeline and their configuration parameters. \n\nProblem statement. The goal of a RAG serving system is to allocate resources to different pipeline components (which includes replicating some of them) and partition work (across replicas) so as to maximize throughput and minimize SLO violations for requests while operating under bounded RAG pipeline resources (e.g., CPUs, GPUs, and memory). Achieving this goal is challenging for three reasons -evolution, heterogeneity, and unpredictability -which we discuss in the following subsections.",
            "score": 0.5600089873277656,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 7027,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1334
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "220128068"
                },
                {
                    "start": 553,
                    "end": 556,
                    "matchedPaperCorpusId": "261697361"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.439697265625
        },
        {
            "corpus_id": "270371535",
            "title": "Retrieval & Fine-Tuning for In-Context Tabular Models",
            "text": "The idea of pre-training a model on corpora of text prior to fine-tuning has been explored in the Natural Language Processing domain for both classification and generation tasks [14,27,42].Later iterations refined this idea to train a model and use its in-context learning abilities for new tasks [10].This elicited research into prompt engineering to determine what to actually put in a model's context [39,51].Similar to prompt engineering, to better utilize the model's context, one can search for similar examples from a corpora and use them to facilitate the task; this is known as Retrieval-Augmented Generation (RAG) [31] in the generative context.Other variants of the idea include training jointly with retrieval [22,8] and augmenting the output of the model with kNN via interpolating [29].These ideas are analogous to our approach of (i) fine-tuning and retrieving jointly, and (ii) disjoint kNN and fine-tuning in our ablations, respectively.LLMs have also been directly applied to tabular data [16,25,18] however, due to the pre-training of these foundation models on large text corpora, there is the possibility of data leakage, which causes concern with evaluations [7].Note that this is not the case with TabPFN as it has been trained on synthetic data.",
            "score": 0.5599876491891531,
            "section_title": "Links with LLMs",
            "char_start_offset": 17004,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 189,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 412
                },
                {
                    "start": 412,
                    "end": 655
                },
                {
                    "start": 655,
                    "end": 800
                },
                {
                    "start": 800,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1185
                },
                {
                    "start": 1185,
                    "end": 1269
                }
            ],
            "ref_mentions": [
                {
                    "start": 178,
                    "end": 182,
                    "matchedPaperCorpusId": "7138078"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 408,
                    "end": 411,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 624,
                    "end": 628,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 722,
                    "end": 726,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 726,
                    "end": 728,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1007,
                    "end": 1011,
                    "matchedPaperCorpusId": "249642722"
                },
                {
                    "start": 1011,
                    "end": 1014,
                    "matchedPaperCorpusId": "252992811"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70703125
        },
        {
            "corpus_id": "270688152",
            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
            "text": "Retrieval-Augmented Generation. Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; where plan and answer tasks use the same example data, different loss functions, and train two task-specific prompts simultaneously. The right shows the inference process, where the plan-answer process is repeated iteratively until completion. Guu et al., 2020) enhances LLMs by retrieving relevant passages, thereby improving both the quality and accuracy of generated content, particularly in knowledge-intensive tasks (Shen et al., 2023;Chen et al., 2023). Early works (Es et al., 2023;Lyu et al., 2024) chose to retrieve once, incorporating a fixed number of retrieved passages with a query into LLMs to generate a response. Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts. However, existing approaches may not take full advantage of the planning capabilities of LLMs. Additionally, these methods may struggle to extract relevant content from retrieved passages and are easily influenced by irrelevant information. \n\nParameter-Efficient Fine-Tuning. Despite the powerful generative capabilities of LLMs, fine-tuning them requires substantial computational resources (Lester et al., 2021;Ding et al., 2022;Liu et al., 2023). To achieve more efficient fine-tuning, parameter-efficient tuning methods have emerged. These methods either fine-tune a small portion of the model parameters or introduce additional learnable parameters without fine-tuning the model itself (Hu et al., 2021;Liu et al., 2021;Ding et al., 2022;Wang et al., 2023).",
            "score": 0.5597676364512983,
            "section_title": "Related Work",
            "char_start_offset": 7176,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1292
                },
                {
                    "start": 1293,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1568
                },
                {
                    "start": 1569,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1830
                },
                {
                    "start": 1831,
                    "end": 2055
                }
            ],
            "ref_mentions": [
                {
                    "start": 69,
                    "end": 88,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 334,
                    "end": 351,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 530,
                    "end": 548,
                    "matchedPaperCorpusId": "264350686"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9052734375
        },
        {
            "corpus_id": "270560505",
            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
            "text": "This step involves incorporating LLMs to enhance the generative component of the Retrieval-Augmented Generation (RAG) system.For this study, we utilized open-source LLMs provided by Perplexity, integrating them into the langchain framework through a custom wrapper function (git link to wrapper code) .This integration was done by adapting the approach detailed in this github repository [8][9].",
            "score": 0.5590101799912068,
            "section_title": "LLM Integration",
            "char_start_offset": 9860,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 302
                },
                {
                    "start": 302,
                    "end": 395
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5791015625
        },
        {
            "corpus_id": "269214364",
            "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study",
            "text": "AI-powered question-answering (Q&A) systems have emerged as important tools, alongside established search technologies, to enable quick access to relevant information and knowledge from large digital sources that are complex and time-consuming for humans to navigate.Advancements in large language models (LLMs) have revolutionized the field of Q&A, with models like GPT-3 (Brown et al. 2020), BERT (Devlin et al. 2018), and RoBERTa (Liu et al. 2019) demonstrating remarkable abilities in understanding and generating human-like text.However, the effectiveness of such models in handling domain-specific questions that require specialized knowledge is limited.\n\nRetrieval-augmented generation (RAG) techniques, which combine information retrieval and generative models (Lewis et al. 2021), have shown promise in boosting the quality of LLM output in Q&A tasks.RAG systems leverage the strengths of both retrieval and generation components to provide contextually relevant and informative responses.While there is a lack of established quantification of RAG accuracy, early findings suggest that generic RAG does not perform well in complex domains such as finance.In one instance, RAG based on generic LLMs such as GPT-4-Turbo fails to answer 81% of the questions derived from Securities and Exchange Commission (SEC) financial filings (Islam et al. 2023).\n\nThe underperformance of generic LLMs and RAG in domain-specific Q&A has motivated us to research into and create methods to adapt and extend such models and techniques.In this paper, we describe and quantify the gains in accuracy from two major methods: model fine-tuning and iterative reasoning.\n\nFine-tuning is a way to adapt language models to specific domains and tasks (Devlin et al. 2018;Liu et al. 2019) by training them on domain-specific data and having them capture the nuances and intricacies of a particular field.In a typical RAG workflow, there are two principal models that can be considered for fine-tuning: the Embedding Model, whose tasks are indexing the information in the corpus and retrieving information relevant to the posed question, and the Generative Model, whose task is synthesizing an answer.",
            "score": 0.5587918636306158,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 267
                },
                {
                    "start": 267,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 660
                },
                {
                    "start": 662,
                    "end": 860
                },
                {
                    "start": 860,
                    "end": 998
                },
                {
                    "start": 998,
                    "end": 1164
                },
                {
                    "start": 1164,
                    "end": 1356
                },
                {
                    "start": 1358,
                    "end": 1526
                },
                {
                    "start": 1526,
                    "end": 1654
                },
                {
                    "start": 1656,
                    "end": 1884
                },
                {
                    "start": 1884,
                    "end": 2180
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88525390625
        },
        {
            "corpus_id": "272831924",
            "title": "IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through Semantic Comprehension in Retrieval-Augmented Generation Scenarios",
            "text": "The field of Retrieval-Augmented Generation (RAG) has gained significant attention, especially in addressing the limitations of Large Language models (LLMs) in providing accurate and contextually relevant information. This section reviews notable works in this domain and situates our contribution within the existing research. \n\nBenchmarking in RAG Chen et al. developed the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs on four abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. Their findings highlight the need for nuanced evaluation metrics to improve RAG capabilities, as LLMs showed weaknesses in negative rejection, information integration, and handling false information (Chen et al., 2024b) . However, RGB primarily focuses on robustness aspects and does not provide comprehensive coverage of different retrieval tasks, which is crucial for real-world RAG applications.",
            "score": 0.5585385263552101,
            "section_title": "Related Work",
            "char_start_offset": 2956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 951
                }
            ],
            "ref_mentions": [
                {
                    "start": 752,
                    "end": 772,
                    "matchedPaperCorpusId": "261530434"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8681640625
        },
        {
            "corpus_id": "265050661",
            "title": "Evaluating Generative Ad Hoc Information Retrieval",
            "text": "'Generative retrieval' or 'generative IR' are umbrella terms for a diversity of approaches that use generative models to solve retrieval tasks. 5 Following Arora et al. [6], Figure 3 categorizes these approaches into generation-augmented retrieval (GAR) and retrievalaugmented generation (RAG). Notably, GAR approaches create traditional list SERPs, while RAG approaches generate text SERPs. \n\nIn GAR approaches, generative models are used to enhance the traditional search architecture at indexing time or at query time. At indexing time, generative models can be used for augmenting documents [37,44,78,94,152] with confabulated or hallucinated content, or for replacing the standard indexing process with what are commonly termed 'differentiable indices' by, for instance, generating document identifiers like page titles [20,31,125], URLs [153], or (structured) string identifiers [124,128,148,151]. At query time, generative models can be used for augmenting queries [2,77], or for modeling relevance by, for instance, generating parts of existing documents from the query and retrieving the documents by string matching [11], by predicting a (re-)ranking directly [123], or by using special tokens as relevance signal [76,93,99,150].",
            "score": 0.558092305117348,
            "section_title": "A Taxonomy of Generative Retrieval",
            "char_start_offset": 11570,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 521
                },
                {
                    "start": 522,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1239
                }
            ],
            "ref_mentions": [
                {
                    "start": 599,
                    "end": 602,
                    "matchedPaperCorpusId": "255545874"
                },
                {
                    "start": 602,
                    "end": 605,
                    "matchedPaperCorpusId": "216641912"
                },
                {
                    "start": 825,
                    "end": 829,
                    "matchedPaperCorpusId": "248118757"
                },
                {
                    "start": 829,
                    "end": 832,
                    "matchedPaperCorpusId": "222125277"
                },
                {
                    "start": 843,
                    "end": 848,
                    "matchedPaperCorpusId": "258714822"
                },
                {
                    "start": 885,
                    "end": 890,
                    "matchedPaperCorpusId": "246863488"
                },
                {
                    "start": 890,
                    "end": 894,
                    "matchedPaperCorpusId": "249395549"
                },
                {
                    "start": 894,
                    "end": 898,
                    "matchedPaperCorpusId": "255879096"
                },
                {
                    "start": 972,
                    "end": 975,
                    "matchedPaperCorpusId": "259123956"
                },
                {
                    "start": 1126,
                    "end": 1130,
                    "matchedPaperCorpusId": "248366293"
                },
                {
                    "start": 1170,
                    "end": 1175,
                    "matchedPaperCorpusId": "258212638"
                },
                {
                    "start": 1228,
                    "end": 1231,
                    "matchedPaperCorpusId": "212725651"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60693359375
        },
        {
            "corpus_id": "273850363",
            "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation",
            "text": "We will select five different generative models to conduct comparative experiments with the RAG model based on graph structure optimization proposed in this paper (labeled as Ours). By comparing their performance on the same dataset, we can verify the superiority of our model. The evaluation indicators used in the comparative experiments include Quality, Knowledge Consistency (KC), and Reasoning Capability(RC). 1, the performance of each model in the three indicators of Quality, Knowledge Consistency, and Reasoning Capability varies. It can be observed that traditional generative models such as BART and T5 performed relatively weakly in the experiment, especially in terms of knowledge consistency and reasoning capability. BART's quality score is 0.74, KC score is 0.65, and reasoning capability is 0.68, reflecting that although its generated text is fluent, it cannot provide sufficient external support in knowledge-intensive tasks. The T5 model has a slight improvement in the three indicators, especially in reasoning capability (0.72), which shows that the T5 model can improve its generalization ability by unifying task processing, but it is still not enough to fully handle complex knowledge reasoning tasks. \n\nIn contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks. \n\nFinally, the graph-based RAG optimization model (ours) proposed in this paper performs well in all indicators, with a quality score of 0.90, knowledge consistency of 0.85, and reasoning ability of 0.91, which is significantly better than other models. This result verifies the effectiveness of our introduction of graph structure information.",
            "score": 0.5580204996260831,
            "section_title": "B. Experimental Results",
            "char_start_offset": 11942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 414
                },
                {
                    "start": 415,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 731
                },
                {
                    "start": 732,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1226
                },
                {
                    "start": 1229,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2315
                },
                {
                    "start": 2316,
                    "end": 2406
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8798828125
        },
        {
            "corpus_id": "277994166",
            "title": "CiteFix: Enhancing RAG Accuracy Through Post-Processing Citation Correction",
            "text": "Recent advancements in AI infrastructure and methodologies have enabled training Large Language Models (LLMs) over internet-scale data. These models demonstrate impressive competence in answering a wide range of general queries. However, when applied to specialized domains such as addressing questions based on internal company documents, off-the-shelf LLMs exhibit significant limitations. They often lack access to latest information, have difficulty interpreting domain specific language, struggle with source attribution, are prone to hallucinations (Ji et al., 2023), and are prone to overly broad responses. \n\nTo overcome these challenges, two broad strategies have emerged. The first involves fine-tuning LLMs on domain-specific data. However, this approach is not only resource-intensive and requires frequent updates, but also risks unintended consequences such as catastrophic forgetting, where the model loses previously acquired general knowledge, thereby increasing the overall system complexity. The second, often more practical method is Retrieval-Augmented Generation (RAG). RAG is a process that combines information retrieval with text generation. It typically involves the following steps: (1) indexing a knowledge base of relevant information, (2) using a retrieval system to find content specifically relevant to a given user query, (3) providing the user query and the retrieved content to an LLM, instructing it to generate a response based on the retrieved content. RAG offers numerous benefits, including real-time access to up-to-date information, improved token generation (Khandelwal et al., 2019), reduced hallucinations, better source attribution (Gao et al., 2023a;Hsu et al., 2024) and overall superior response generation (Shuster et al., 2021;B\u00e9chard and Ayala, 2024). Additionally, RAG tends to be more cost-effective and transparent than full model fine-tuning. Examples of RAG-based products include Perplexity.ai (Perplexity AI, 2024), bing search, GPT Search etc. \n\nDespite enabling a novel information retrieval experience for users, RAG systems today face key limitations. Table 1 illustrates results of a Subject Matter Expert based auditing of a RAG based system. Shown is a metric \"Relative Mean Question Level Accuracy\", which captures relevancy of cited chunks, correctness and completeness of the answer(Sec.",
            "score": 0.5577326865351231,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 135
                },
                {
                    "start": 136,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 391
                },
                {
                    "start": 392,
                    "end": 614
                },
                {
                    "start": 617,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1803
                },
                {
                    "start": 1804,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2003
                },
                {
                    "start": 2006,
                    "end": 2114
                },
                {
                    "start": 2115,
                    "end": 2207
                },
                {
                    "start": 2208,
                    "end": 2356
                }
            ],
            "ref_mentions": [
                {
                    "start": 555,
                    "end": 572,
                    "matchedPaperCorpusId": "246652372"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.88232421875
        },
        {
            "corpus_id": "268358441",
            "title": "PipeRAG: Fast Retrieval-Augmented Generation via Algorithm-System Co-design",
            "text": "Retrieval-augmented generation presents both opportunities and efficiency challenges, due to the significant over-when retrieving from large databases.We propose PipeRAG, a novel RAG approach that improves generation efficiency by adopting pipeline parallelism, allowing flexible retrieval intervals, and dynamically adjusting retrieval quality via performance modeling.PipeRAG achieves up to 2.6\u00d7 speedup over RETRO without compromising generation quality.This not only establishes a solid foundation for integrating pipeline parallelism in future RAG systems but also showcasing future research opportunities in optimizing RAG through algorithm-system co-design.",
            "score": 0.5563567420402609,
            "section_title": "Conclusion",
            "char_start_offset": 31442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 151,
                    "end": 370
                },
                {
                    "start": 370,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79296875
        },
        {
            "corpus_id": "272689170",
            "title": "SFR-RAG: Towards Contextually Faithful LLMs",
            "text": "Retrieval Augmented Generation (RAG) has recently garnered significant attention as one of the most prominent areas of research in generative AI [53,54], driven by the latest advancements in foundational large language models (LLMs) [4,39,29,30,40,9,14,2]. RAG frameworks are wellsuited for solving knowledge-dependent problems or questions, where external contextual information is provided and the generated answer is expected to be factually grounded on the contextual cues. In practice, the RAG setup is designed such that a generator LLM works in tandem with a knowledge retriever. The retriever [26,46,5,21] is tasked with retrieving passages relevant to a given query from a database of documents (potentially the entire internet). The LLM interacts with users, formulates queries for the retriever to gather knowledge, and finally answers users' questions. To retrieve the most accurate context information, the retriever typically relies on an embedding model [28,26,5,21], and optionally employs a re-ranker to get a refined list of context documents [24]. Recent research has also led to the development of more sophisticated RAG frameworks [1,13,22,47,16,45] that involve multiple inference steps to improve the reliability of answers. Figure 1: Our SFR-RAG-9B model exhibits strong overall performance on an ContextualBench, our comprehensive evaluation suite of seven contextual tasks under a standardized setup. Notably, SFR-RAG achieves state-of-the-art performance on three of seven tasks, with extremely competitive performance on the rest, despite having far fewer parameters than competitive baselines. \n\nIn this work, we focus our efforts on the generator LLM component of the RAG framework. \n\nTraditional general-purpose LLMs trained for chat often struggle when directly applied to the RAG framework. This can be attributed to several potential factors, including: \n\n\u2022 The knowledge in the context obtained from the retriever might conflict with the training data used for the LLM. \n\n\u2022 The LLM is not trained to deal with conflicting or redundant facts from the retriever. \n\n\u2022 In scenarios where the retrieved knowledge is insufficient, the LLMs revert to answering questions based on its training data.",
            "score": 0.5559291544377689,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 477
                },
                {
                    "start": 478,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 738
                },
                {
                    "start": 739,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1712
                },
                {
                    "start": 1715,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 2004
                },
                {
                    "start": 2007,
                    "end": 2095
                },
                {
                    "start": 2098,
                    "end": 2226
                }
            ],
            "ref_mentions": [
                {
                    "start": 149,
                    "end": 152,
                    "matchedPaperCorpusId": "257632157"
                },
                {
                    "start": 233,
                    "end": 236,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1155,
                    "end": 1158,
                    "matchedPaperCorpusId": "226236740"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83349609375
        },
        {
            "corpus_id": "270219294",
            "title": "BadRAG: Identifying Vulnerabilities in Retrieval Augmented Generation of Large Language Models",
            "text": "Large Language Models (LLMs) are constrained by outdated information and a tendency to generate incorrect data, commonly referred to as\"hallucinations.\"Retrieval-Augmented Generation (RAG) addresses these limitations by combining the strengths of retrieval-based methods and generative models. This approach involves retrieving relevant information from a large, up-to-date dataset and using it to enhance the generation process, leading to more accurate and contextually appropriate responses. Despite its benefits, RAG introduces a new attack surface for LLMs, particularly because RAG databases are often sourced from public data, such as the web. In this paper, we propose \\TrojRAG{} to identify the vulnerabilities and attacks on retrieval parts (RAG database) and their indirect attacks on generative parts (LLMs). Specifically, we identify that poisoning several customized content passages could achieve a retrieval backdoor, where the retrieval works well for clean queries but always returns customized poisoned adversarial queries. Triggers and poisoned passages can be highly customized to implement various attacks. For example, a trigger could be a semantic group like\"The Republican Party, Donald Trump, etc.\"Adversarial passages can be tailored to different contents, not only linked to the triggers but also used to indirectly attack generative LLMs without modifying them. These attacks can include denial-of-service attacks on RAG and semantic steering attacks on LLM generations conditioned by the triggers. Our experiments demonstrate that by just poisoning 10 adversarial passages can induce 98.2\\% success rate to retrieve the adversarial passages. Then, these passages can increase the reject ratio of RAG-based GPT-4 from 0.01\\% to 74.6\\% or increase the rate of negative responses from 0.22\\% to 72\\% for targeted queries.",
            "score": 0.5558863801022681,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "278284698",
            "title": "Leveraging long context in retrieval augmented language models for medical question answering",
            "text": "Despite the potential of LLMs, the deployment of LLMs in healthcare faces significant safety threats. LLMs struggle to generate accurate and up-to-date responses on current topics, due to outdated knowledge, lack of domain-specific expertise, or hallucination [13][14][15][16][17] . \n\nRetrieval-Augmented Generation (RAG) is a pivotal innovation to enhance the quality and relevance of responses in LLMs [18][19][20][21] . Typically, a RAG system consists of a retrieval module and a generative module. When a user query is provided as input, the system first uses the retrieval module to fetch relevant documents or data snippets by searching through external data sources. Next, the generative module takes the retrieved information as input and produces a response to the user query. With the help of the retrieval module, the generative module can provide more accurate and factual answers without the need for continual training or fine-tuning. As such, RAG poses a promising direction for applications requiring high factual accuracy and specificity 14,22 . \n\nHowever, prompting LLMs with contextual information has trade-offs. On the one hand, providing contextual information enhances the model's ability to perform the downstream tasks by augmenting LLMs with external domain-specific knowledge that is under-represented in their pretraining data. On the other hand, the input of LLMs is bounded by the limit of their context windows. Even though recently released models can process an increasing number of tokens, the increased amount of content to reason over can still hinder model performance 23 . The quality of RAG completion also depends on the retrieval results, such as the density or positions of queryrelevant information 14,22,[24][25][26] . As retrieval systems are still imperfect, it is inevitable to retrieve information irrelevant to the user query 14 . \n\nA recent study reports an issue of \"lost-in-the-middle\", i.e., the position of key information in the LLM context impacts the quality of the model completions 24 . This issue occurs when a lengthy context of information is retrieved, and the highly relevant information is not ranked at the top or bottom of the retrieval results.",
            "score": 0.5541796158569899,
            "section_title": "Check for updates",
            "char_start_offset": 2198,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 282
                },
                {
                    "start": 285,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 674
                },
                {
                    "start": 675,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1063
                },
                {
                    "start": 1066,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1356
                },
                {
                    "start": 1357,
                    "end": 1443
                },
                {
                    "start": 1444,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1880
                },
                {
                    "start": 1883,
                    "end": 2046
                },
                {
                    "start": 2047,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 260,
                    "end": 264,
                    "matchedPaperCorpusId": "224706057"
                },
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 268,
                    "end": 272,
                    "matchedPaperCorpusId": "233231373"
                },
                {
                    "start": 272,
                    "end": 276,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 276,
                    "end": 280,
                    "matchedPaperCorpusId": "267301644"
                },
                {
                    "start": 404,
                    "end": 408,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 408,
                    "end": 412,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 412,
                    "end": 416,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 416,
                    "end": 420,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1056,
                    "end": 1059,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1743,
                    "end": 1746,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 1749,
                    "end": 1753,
                    "matchedPaperCorpusId": "259360665"
                },
                {
                    "start": 1753,
                    "end": 1757,
                    "matchedPaperCorpusId": "263830692"
                },
                {
                    "start": 1876,
                    "end": 1878,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 2042,
                    "end": 2044,
                    "matchedPaperCorpusId": "259360665"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.53955078125
        },
        {
            "corpus_id": "273962778",
            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
            "text": "Over the past decade, large language models (LLMs) have demonstrated promising capability in processing natural language [1]. Owing to the vast amount of knowledge encoded in their internal parameters, LLMs such as GPT [3] and LLaMa [9] have demonstrated remarkable performance on various downstream tasks, including Open-domain Question Answering (ODQA) [38], Reading Comprehension [7]. However, the fixed parametric knowledge of LLMs has hindered the further applications of LLMs and made them prone to errors (hallucination [7] and factual errors [2]). To overcome the limitations of parametric knowledge, one promising approach is Retrieval-Augmented Generation (RAG) [2,15]. Compared to relying solely on parametric knowledge, RAG enables LLMs to use retrievers to access relevant information from external knowledge sources, enhancing their question-answering abilities. Among the two components of RAG, current methods primarily focus on optimizing the retriever to return more relevant documents due to the high cost of finetuning and black-box LLM APIs. Previous retrievers leveraged deep learning technology (e.g., dense retrieval [37]) to encode the text representations from the lexical space into the high-dimensional latent space, allowing them to model more complex semantic relationships between queries and corpora. However, the separation between the retriever and generation model has hindered their full integration, limiting their compatibility in downstream applications []. Some advanced RAG systems, such as RA-DIT [18], have adopted joint training mechanisms that fine-tune both the retriever and the generation model for better alignment. However, this approach is impractical due to the need for frequent fine-tuning and fails to utilize the LLMs' semantic understanding capabilities during the retrieval stage. Consequently, generative retrieval (GR), also known as LLM-based retrieval, leverages the parametric memory of generative models to directly generate document identifiers (DocIDs) [16], which has aroused much attention. By memorizing the documents as the parametric knowledge of LLM, this kind of method breaks the limitations of traditional IR in terms of document granularity and simple relevance matching [23], offering more flexibility and creativity, thus better meeting practical needs.",
            "score": 0.5540844288009255,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 126,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 1062
                },
                {
                    "start": 1063,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1496
                },
                {
                    "start": 1497,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2058
                },
                {
                    "start": 2059,
                    "end": 2331
                }
            ],
            "ref_mentions": [
                {
                    "start": 355,
                    "end": 359,
                    "matchedPaperCorpusId": "230433817"
                },
                {
                    "start": 675,
                    "end": 678,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1141,
                    "end": 1145,
                    "matchedPaperCorpusId": "254044526"
                },
                {
                    "start": 2019,
                    "end": 2023,
                    "matchedPaperCorpusId": "269303210"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "275906944",
            "title": "Chain-of-Retrieval Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) integrates information retrieval techniques with generative models to enhance the quality and factual accuracy of generated content [18,19]. By equipping LLMs with the ability to browse the web [24], RAG systems can access real-time data, thereby providing responses that are both up-to-date and grounded. The relevance and quality of the retrieved information are pivotal for the efficacy of RAG systems. A substantial body of recent research has concentrated on developing better general-purpose text embeddings [16,33]. Nevertheless, text embeddings frequently face limitations in addressing complex queries due to their reliance on fixed-size vector representations for efficiency purposes. \n\nTo mitigate this constraint, contemporary research has extended the conventional paradigm of a single retrieval step followed by generation, advancing to multi-step iterative retrieval and generation [5]. FLARE [12] prompts an LLM to actively determine when and what to retrieve during the generation process. ITER-RETGEN [28] proposes to interleave retrieval-augmented generation with generation-augmented retrieval, demonstrating enhancements in multi-hop QA tasks. Similarly, IRCoT [31] employs a chain-of-thought methodology, which recursively refines the reasoning thought for subsequent retrieval steps. Self-RAG [1] empowers LLMs to adaptively retrieve, generate, and critique through self-reflection, thus improving factual accuracy and citation precision in open-domain QA and long-form generation tasks. Auto-RAG [38] utilizes heuristic rules and exact answer matching to construct intermediate retrieval steps, yet its performance remains significantly below that of state-of-the-art models. In this study, rather than exclusively on few-shot prompting or distillation from proprietary models, we propose a novel approach to explicitly train LLMs to iteratively retrieve and reason over relevant information. \n\nScaling Test-time Compute Instead of prompting LLMs to directly generate the final answer, Chainof-Thought (CoT) [34] demonstrates that letting the model to think step by step can drastically improve the performance on mathematical reasoning tasks. Tree-of-Thought (ToT) [37] extends the idea of CoT by adopting a tree structure, allowing the model to explore the search space more comprehensively.",
            "score": 0.5540159118994841,
            "section_title": "Related Work",
            "char_start_offset": 4679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 342
                },
                {
                    "start": 343,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 559
                },
                {
                    "start": 560,
                    "end": 731
                },
                {
                    "start": 734,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1043
                },
                {
                    "start": 1044,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1547
                },
                {
                    "start": 1548,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1953
                },
                {
                    "start": 1956,
                    "end": 2204
                },
                {
                    "start": 2205,
                    "end": 2354
                }
            ],
            "ref_mentions": [
                {
                    "start": 169,
                    "end": 173,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 551,
                    "end": 555,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 945,
                    "end": 949,
                    "matchedPaperCorpusId": "258615731"
                },
                {
                    "start": 1056,
                    "end": 1060,
                    "matchedPaperCorpusId": "258866037"
                },
                {
                    "start": 1219,
                    "end": 1223,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 1353,
                    "end": 1356,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 2069,
                    "end": 2073,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.83154296875
        },
        {
            "corpus_id": "269303017",
            "title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering",
            "text": "The Retrieval-Augmented Generation (RAG) system typically retrieves additional knowledge from specific corpora, such as Wikipedia, to alleviate the hallucination problem of Large Language Models (LLMs), thereby significantly enhancing the performance of LLMs in various tasks (Lewis et al., 2020;Guu et al., 2020;Ram et al., 2023).\n\nEarly research on RAG typically employs a onestep retrieval approach, which is ineffective in addressing composite problems.To tackle composite problems, Self-Ask (Press et al., 2023) poses sub-questions before answering the main question, optimizing complex composite problems through multiple retrievals.IRCoT (Trivedi et al., 2023) triggers retrieval on each sentence of the CoT.ITER-RETGEN (Shao et al., 2023) connects the complete CoT reasoning steps generated in the previous turn with the original question for the next turn's generation query.However, these methods all adopt a chain-like structure for reasoning.If an error occurs at any step in the reasoning path, it could potentially cause the reasoning path to deviate.",
            "score": 0.5538693413295419,
            "section_title": "Retrieval-Augmentation for Complex Problems",
            "char_start_offset": 6422,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 331
                },
                {
                    "start": 333,
                    "end": 457
                },
                {
                    "start": 457,
                    "end": 715
                },
                {
                    "start": 715,
                    "end": 884
                },
                {
                    "start": 884,
                    "end": 954
                },
                {
                    "start": 954,
                    "end": 1065
                }
            ],
            "ref_mentions": [
                {
                    "start": 276,
                    "end": 296,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 296,
                    "end": 313,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 313,
                    "end": 330,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 496,
                    "end": 516,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 645,
                    "end": 667,
                    "matchedPaperCorpusId": "254877499"
                },
                {
                    "start": 727,
                    "end": 746,
                    "matchedPaperCorpusId": "258866037"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60791015625
        },
        {
            "corpus_id": "277672691",
            "title": "Large Language Models and Their Applications in Drug Discovery and Development: A Primer",
            "text": "In scientific applications, the accuracy and relevance of information generated by LLMs are crucial for making advancements in research and informing decision-making. However, it is well known that LLMs can often confabulate (or hallucinate) information [11] if they are not grounded on facts and/or data relevant to the application area of interest. Thus, methods to further improve the accuracy and provide citations for the generated information are crucial to build confidence and trust in LLM applications. \n\nRetrieval-augmented generation (RAG) [25] is a widely used approach that combines two powerful techniques: retrieval and generation. First, information pertinent to the application of interest, such as documents (e.g., scientific papers, manuals, model codes, etc.), is collected and preprocessed by splitting it into smaller chunks, such as paragraphs, which are subsequently converted to embeddings (i.e., numeric vector representations) and stored in a vector database that the LLM can utilize. When a user query is received by the LLM, it is converted to the same embedding space, and a similarity search is performed within the vector database to identify the most relevant information. The original query is then augmented with the retrieved information as additional context, which is then passed to the LLM to generate the response. It has been shown [26] that RAG can significantly enhance the accuracy of LLMs as compared to CoT prompting. \n\nFine-tuning a pretrained LLM on domain-specific data to adapt it to a specific task is another way to improve performance [14]. In the context of chemical text mining, it has been shown that fine-tuning can improve the model's performance substantially as compared to prompting [27]. \n\nIn the context of using LLM for medical question answering, instruction fine-tuning on the PaLM2 model gave rise to the Med-PaLM 2 that demonstrated substantial improvement in performance on multiple benchmarks [28]. While offering performance improvements, fine-tuning over a large set of model parameters can entail a high computational burden [11].",
            "score": 0.5538360748721698,
            "section_title": "| Improving Accuracy and Relevance",
            "char_start_offset": 10468,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 166
                },
                {
                    "start": 167,
                    "end": 350
                },
                {
                    "start": 351,
                    "end": 511
                },
                {
                    "start": 514,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 1011
                },
                {
                    "start": 1012,
                    "end": 1205
                },
                {
                    "start": 1206,
                    "end": 1354
                },
                {
                    "start": 1355,
                    "end": 1463
                },
                {
                    "start": 1466,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2103
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 258,
                    "matchedPaperCorpusId": "269358226"
                },
                {
                    "start": 1744,
                    "end": 1748,
                    "matchedPaperCorpusId": "270351696"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.72607421875
        },
        {
            "corpus_id": "277066612",
            "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning",
            "text": "Retrieval-augmented generation (RAG; [7,11,24]) relies on retrieval and generation models that work together to retrieve and integrate external contexts effectively for answering questions or generating content. While previous works have made significant progress in improving these systems by optimizing the retriever and designing re-ranking models [30,6,27], challenges persist when it comes to retrieving relevant real-world contexts that require deep reasoning [21]. Furthermore, prior studies have shown that generation models also face limitations to effectively synthesizing information from multiple documents due to limitations in their reasoning capabilities [25]. \n\nIn this work, we tackle the aforementioned challenges by training reasoning language models (RLMs) capable of performing multi-hop reasoning across a greater number of retrieved documents. Prior research has focused on optimizing the retrieval and re-ranking components of RAG with the goal of presenting a small subset of documents to the answer generation model by maximizing metrics such as recall@5. However, a model that can effectively differentiate between relevant and irrelevant contexts given a longer list of retrieved passages would lessen the burden on retrieval models by increasing recall and be more adept at using the information in the retrieved contexts [8]. \n\nBuilding upon on the recent success of reinforcement learning (RL) post-training techniques in mathematics and coding [26,28], we apply Group Relative Policy Optimization (GRPO) [19] with simple, rule-based rewards in the RAG scenario and show that RAG-RL achieves state-of-theart performance on both HotpotQA and MuSiQue compared to prevous SOTA generative reader models. Furthermore, our comprehensive evaluation demonstrates that RAG-RL performs well both in scenarios with numerous distractor passages and when restricted to only gold documents, mirroring the settings of using a weaker retrieval model or a more advanced retriever and/or re-ranking system. Here, \"gold\" documents refers to the set of documents from which the answer to a given question can be deduced, while \"distractor\" documents are those that do not contain relevant information. \n\nWe also investigate how to achieve better model performance when using GRPO through training models in different curriculum learning settings.",
            "score": 0.5534950074274266,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 471
                },
                {
                    "start": 472,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1355
                },
                {
                    "start": 1358,
                    "end": 1730
                },
                {
                    "start": 1731,
                    "end": 2019
                },
                {
                    "start": 2020,
                    "end": 2212
                },
                {
                    "start": 2215,
                    "end": 2357
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 40,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 40,
                    "end": 43,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 43,
                    "end": 46,
                    "matchedPaperCorpusId": "268032752"
                },
                {
                    "start": 1351,
                    "end": 1354,
                    "matchedPaperCorpusId": "273229050"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87646484375
        },
        {
            "corpus_id": "270870632",
            "title": "BERGEN: A Benchmarking Library for Retrieval-Augmented Generation",
            "text": "We provide comprehensive ablations on the impact of retrieval quality on generation.We study modern SoTA retrievers -including models from the MTEB benchmark which have been fine-tuned on datasets like NQ.Table 6 lists all the models we consider, and Table 7 present the retrieval performance alongside the generation quality (with and without re-ranking respectively).Overall, we observe that SoTA models from MTEB achieve better performance in both aspects.These results are somewhat expected, as fine-tuning ranking models on the target collection improves ranking quality and therefore the relevance of input contexts.However, it does not measure the \"zero-shot\" performance of the RAG pipeline -especially given the inability of learned retrievers to generalize to out-of-domain collections [77].In the meantime, re-ranking closes the gap between approaches.",
            "score": 0.5532523845370767,
            "section_title": "D. Retrieval Analysis",
            "char_start_offset": 27235,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 369
                },
                {
                    "start": 369,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 622
                },
                {
                    "start": 622,
                    "end": 801
                },
                {
                    "start": 801,
                    "end": 863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.82958984375
        },
        {
            "corpus_id": "277740883",
            "title": "DRAFT-ing Architectural Design Decisions using LLMs",
            "text": "Retrieval-Augmented Generation (RAG) introduced by Patrick et al. [13] is a hybrid approach that combines the generative capabilities of LLMs with an external retrieval mechanism. This hybrid approach enhances the contextual accuracy and factual reliability of generated responses by retrieving relevant information from an external knowledge base. RAG has been successfully employed in various applications, including question-answering systems and document summarization. Multiple studies have shown complex Retrieval architectures such as Knowledge Graphs [20], Re-ranking [21], etc. improving the performance of RAG models. \n\nIn this study we use Retrieval-Augmented Fewshot Generation, which is a combination of fewshot prompting and RAG [22]. Rather than relying on a static, predefined set of few-shot exemplars, this approach retrieves contextually similar examples from a structured knowledge base, such as a vector database (VDB). This method improves the model's ability to generate context-aware and semantically relevant responses. Here is a breakdown of how it works: \n\nEmbedding Representation: Textual data is transformed into high-dimensional vector representations, known as embeddings, which capture the semantic meaning of the content. These embeddings sre generated using an embedding function, which is a pretrained LLM such as BERT [16]. \n\nFormally, a given context C is converted into an embedding v C (of dimension d) using an embedding function: \n\nVector Database (VDB) Construction: A vector database (VDB) is a specialized type of database designed to store representations of data, such as sentences or documents, in the form of embeddings. A VDB performs efficient similarity searches by comparing the vector representations of queries with those of the data stored in the VDB, enabling quick retrieval of relevant or similar data. \n\nHere, given a dataset of context-decision pairs {(C i , D i )}, each context C i is converted into its embedding v C i and stored along with its corresponding {(C i , D i )} pair, forming the vector database: \n\nRetrieval mechanism: When a query is received, the top-k most similar documents are retrieved from the vector database. \n\nWhen a new Decision Context C is provided, its embedding is computed as:",
            "score": 0.5531008436437934,
            "section_title": "Retrieval-Augmented Few-shot Generation",
            "char_start_offset": 10745,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 348
                },
                {
                    "start": 349,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 627
                },
                {
                    "start": 630,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1081
                },
                {
                    "start": 1084,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1360
                },
                {
                    "start": 1363,
                    "end": 1471
                },
                {
                    "start": 1474,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2072
                },
                {
                    "start": 2075,
                    "end": 2194
                },
                {
                    "start": 2197,
                    "end": 2269
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.77392578125
        },
        {
            "corpus_id": "269605438",
            "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization",
            "text": "Most machine learning systems, including large generative models, are self-contained systems, with both knowledge and reasoning encoded in model parameters.However, these models do not work effectively for tasks that require knowledge grounding [46], especially in case of non-stationary data where new information is actively being produced [47,52].As suggested by Zamani et al. [52], this issue can be addressed when machine learning systems are being enhanced with the capability of retrieving stored content.For example, in retrieval-augmented generation (RAG), as a special case of retrieval-enhanced machine learning (REML) [52], systems consume the responses provided by one or more retrieval models for the purpose of (text) generation [21,22].RAG models demonstrate substantial promise across various applications, including open-domain question answering [16,21,53], fact verification [44], dialogue systems [5,42,48], and personalized generation [36,37].\n\nMany prior studies on RAG use off-the-shelf retrieval models.For instance, Nakano et al. [25] used APIs from a commercial search engine for text generation.Glass et al. [9], on the other hand, used a term matching retrieval model.Neural ranking models trained based on human annotated data have also been used in the literature [12,21].There also exist methods that only optimize the retrieval model and keep the language model parameters frozen [40].A research direction in this area argues that optimizing retrieval models in RAG should depend on the downstream language model that consumes the retrieval results.This is also motivated by the findings presented by Salemi and Zamani [38] on evaluating retrieval quality in RAG systems.There exist solutions based on knowledge distillation [13] or end-to-end optimization based on some simplifying assumptions [35].One of these assumptions is marginalization via top  approximation [10,21].",
            "score": 0.5526389671786422,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 350
                },
                {
                    "start": 350,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 752
                },
                {
                    "start": 752,
                    "end": 965
                },
                {
                    "start": 967,
                    "end": 1028
                },
                {
                    "start": 1028,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1197
                },
                {
                    "start": 1197,
                    "end": 1303
                },
                {
                    "start": 1303,
                    "end": 1418
                },
                {
                    "start": 1418,
                    "end": 1582
                },
                {
                    "start": 1582,
                    "end": 1704
                },
                {
                    "start": 1704,
                    "end": 1833
                },
                {
                    "start": 1833,
                    "end": 1908
                }
            ],
            "ref_mentions": [
                {
                    "start": 346,
                    "end": 349,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 380,
                    "end": 384,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 630,
                    "end": 634,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 744,
                    "end": 748,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 869,
                    "end": 872,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 921,
                    "end": 924,
                    "matchedPaperCorpusId": "51879945"
                },
                {
                    "start": 924,
                    "end": 927,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 957,
                    "end": 961,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 1136,
                    "end": 1139,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1295,
                    "end": 1299,
                    "matchedPaperCorpusId": "252568176"
                },
                {
                    "start": 1299,
                    "end": 1302,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1652,
                    "end": 1656,
                    "matchedPaperCorpusId": "269293655"
                },
                {
                    "start": 1828,
                    "end": 1832,
                    "matchedPaperCorpusId": "230437591"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8046875
        },
        {
            "corpus_id": "268248396",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "text": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge. Additionally, the success of both RAG and FT approaches is amplified by improving retrieval and data augmentation techniques. Fine tuning, while beneficial for small LMs, requires extensive resources. To address this issue, we propose the new Stimulus RAG approach that surpasses the effectiveness of fine tuning based approaches, thereby eliminating the need for the costly data augmentation and fine tuning step for enriching LMs with less popular factual knowledge.",
            "score": 0.5522335976859882,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.947265625
        },
        {
            "corpus_id": "271744975",
            "title": "A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel Chatbot Use Case",
            "text": "Furthermore, it incorporates paged optimizers to handle memory spikes. The second method implemented was a novel method called Retrieval Augmented Fine Tuning (RAFT), a training procedure for domain-specific Retrieval Augmented Generation (RAG), which can adapt pretrained LLMs like LLaMa 2 and Mistral for RAG in specific domains, such as ours in travel. [9] RAG is a text generation method for outsourcing relevant information, from a knowledge base, or a large corpus of relevant, factual and quality information, to supply an LLM with contextual clues for producing factual responses. Then, an RLHF training pipeline will be done for domain-specific LLM curation, aligned with human preferences, with reward model training. [10] II. RELATED WORK Through a comprehensive review of literature and existing research papers, we build an understanding for state-or-theart techniques and approaches aimed to achieve competitive performances. New innovations are expressed in different variations for enhancing large language models. Table II summarizes the selection of models, objective addressed, unique approach, performance results and ultimate findings. A survey of existing solutions show that initiatives have been developed to overcome shortcoming identified with public general-purpose and pretrained LLM. Some of the feature performance issues garnered from literature review are potential risks of hallucination, underdeveloped retrieval approaches, and inefficiencies in the use of computational resources. \n\nZhang et. al introduced Retrieval Augmented fine-tuning (RAFT) as a novel training strategy for fine-tuning LLMs to better perform on RAG tasks. The key concept is data augmentation to generate \"question, answer, document\" triplets before fine-tuning. This is done by generating realistic questions paired with elaborate chain of thought answering scheme and purposefully including relevant and irrelevant context documents. Through a chain of thought with the distractor documents, the model learns to extract the correct information from the entire chunk of context through reasoning, ignoring the distractors. RAFT operates by training the model to disregard any retrieved documents that do not contribute to answering a given question, thereby eliminating distractions. The optimal ratio of oracle to distractor documents during training varies across datasets, but including some distractors improves generalization. Finally, during RAG, RAFT retrieves the top-k documents from the database. With RAFT, Zhang et.",
            "score": 0.5520726866900704,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 4319,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 70
                },
                {
                    "start": 71,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 939
                },
                {
                    "start": 940,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1516
                },
                {
                    "start": 1519,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1943
                },
                {
                    "start": 1944,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2292
                },
                {
                    "start": 2293,
                    "end": 2440
                },
                {
                    "start": 2441,
                    "end": 2515
                },
                {
                    "start": 2516,
                    "end": 2536
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7724609375
        },
        {
            "corpus_id": "278481422",
            "title": "VR-RAG: Open-vocabulary Species Recognition with RAG-Assisted Large Multi-Modal Models",
            "text": "Retrieval Augmented Generation (RAG). RAG has emerged as a powerful paradigm that enhances large generative models by incorporating external knowledge through retrieval systems [15,17,20,21,32,47]. [22] proposed a modular framework where a retriever and generator are jointly optimized. Subsequent works [16] refined retrievalaugmented architectures by improving the fusion of retrieved content. [3,24] have expanded RAG's applicability to tasks like long-form reasoning and multi-hop QA while addressing challenges like hallucination mitigation. Towards the more vision-centric side, works like MuRAG [6] employ an image-text memory bank, retrieving top-k candidates via inner-product similarity. MIRAGE [44] leverages a CLIP-based [32] encoder to train a retriever. RE-VEAL [14] extends RAG to multi-modal reasoning, using graphs to link entities across text and images for long-form generation. In this work, we propose VR-RAG, a two-stage framework that combines multiple vision encoders to improve on each one of them, followed by a re-ranker module to refine the top k selected candidates. Our results show the effectiveness of VR-RAG over existing methods.",
            "score": 0.5519382036255998,
            "section_title": "Related Work",
            "char_start_offset": 8188,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 767
                },
                {
                    "start": 768,
                    "end": 897
                },
                {
                    "start": 898,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1163
                }
            ],
            "ref_mentions": [
                {
                    "start": 184,
                    "end": 187,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 198,
                    "end": 202,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 304,
                    "end": 308,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 399,
                    "end": 402,
                    "matchedPaperCorpusId": "254877603"
                },
                {
                    "start": 705,
                    "end": 709,
                    "matchedPaperCorpusId": "273323967"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.859375
        },
        {
            "corpus_id": "273375021",
            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
            "text": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al. 2023). With the development of this technology, comprehensively evaluating all stages of RAG systems becomes increasingly important as it offers cally factual queries, wherein the answers usually consist of specific entities. This narrows their applicability and hampers their ability to handle more complex analytical or tutorial queries. (2) Obscure problems location: Most previous methods predominantly evaluated the end-to-end results without performing step-by-step analysis. The RAG process can be divided into several stages: chunking, retrieval, reranking, and generation. By solely assessing the final generated outcomes, it becomes challenging to identify problems at specific stages within the RAG pipeline. Such approaches would result in poor interpretability and low optimization efficiency, hindering the ability to refine individual components effectively. (3) Unstable retrieval evaluation: Previous methods evaluate the retrieval stage relying on the annotation of golden chunks with metrics such as Mean Reciprocal Rank and Hit Rate. Annotating all chunks is a tedious and labor-intensive process, and relabeling is required when the chunking strategy is modified. \n\nTo systematically address these challenges, we propose a Comprehensive Full-chain Evaluation (CoFE-RAG) framework to facilitate thorough evaluation across the entire RAG pipeline. We introduce multi-granularity keywords to effectively assess the chunking, retrieval, and reranking phases of RAG systems, which aims to address the dependency on golden chunk annotations for evaluation. The multigranularity keywords encompass coarse-grained and finegrained keywords. Specifically, coarse-grained keywords are the most representative and relevant words extracted from the query and context, serving as initial indicators for chunk relevance. Fine-grained keywords are formulated as a set of lists, where each list corresponds to an information point extracted from the context, providing detailed references for answering the query.",
            "score": 0.5519352495436166,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 956
                },
                {
                    "start": 957,
                    "end": 1094
                },
                {
                    "start": 1095,
                    "end": 1248
                },
                {
                    "start": 1249,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2201
                },
                {
                    "start": 2202,
                    "end": 2392
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 380,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.896484375
        },
        {
            "corpus_id": "276543876",
            "title": "Geographic Named Entity Matching and Evaluation Recommendation Using Multi-Objective Tasks: A Study Integrating a Large Language Model (LLM) and Retrieval-Augmented Generation (RAG)",
            "text": "Common deep learning models include LSTM and GRU [44,45]. With the development of large language model [46][47][48] technology, the Retrieval-Augmented Generation (RAG) framework was proposed initially by Patrick Lewis et al. [49] to address the limitations of large language models in handling domainspecific issues. Gao et al. reintroduced the RAG approach to mitigate the 'hallucination' phenomenon of large language models. Various RAG styles have since been proposed, generally categorized into Naive RAG, Advanced RAG, and Modular RAG, each focusing on different retrieval, generation, and enhancement techniques [50]. This framework enhances prediction capabilities by retrieving relevant information from a vast number of documents during the question-answering or text-generation stage and incorporating this information into the response. For example, Lin et al. [51] used the Retrieval-Augmented Dual Instruction Tuning (RA-DIT) method to enable pre-trained large language models with retrieval capabilities, significantly improving their performance in knowledge-intensive tasks. Wang-Chiew Tan et al. [52] innovated the RAG model architecture to improve the efficiency and accuracy of query answering tasks, while Lin et al. [53] enhanced retrieval performance and answer quality in visual question-answering tasks through fine-grained multimodal retrieval. Inspired by these developments, we propose an RAG-based address recommendation framework that uses new addresses and related information as an external knowledge base, combined with matching addresses from Geographical Named Entity Matching Models, to conduct address recommendation tasks.",
            "score": 0.5516967347186996,
            "section_title": "Geographical Text Recommendation Based on Retrieval-Augmented Generation",
            "char_start_offset": 10342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 57
                },
                {
                    "start": 58,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 1091
                },
                {
                    "start": 1092,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1660
                }
            ],
            "ref_mentions": [
                {
                    "start": 49,
                    "end": 53,
                    "matchedPaperCorpusId": "199510402"
                },
                {
                    "start": 53,
                    "end": 56,
                    "matchedPaperCorpusId": "233699286"
                },
                {
                    "start": 226,
                    "end": 230,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1238,
                    "end": 1242,
                    "matchedPaperCorpusId": "263310932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8486328125
        },
        {
            "corpus_id": "277065966",
            "title": "A Survey on the Optimization of Large Language Model-based Agents",
            "text": "RAG combines LLM with external information retrieval systems to enhance the relevance and accuracy of generated outputs. By retrieving relevant documents from external sources, RAG allows LLMs to address the knowledge constraints inherent in models. The evolution of RAG methods has been marked by significant advancements in retrieval and generation integration [44]. Early, Naive RAG methods focus on directly retrieving relevant documents to augment the generative process, improving the quality of responses in tasks requiring factual knowledge. To address the challenges of Naive RAG, Advanced RAG is introduced, refining the retrieval process by incorporating more effective ranking, filtering, and document selection strategies. Subsequently, Modular RAG introduces a modular framework that optimizes the retrieval and generative components independently. This modular approach enables task-specific optimizations, allowing for more flexibility and scalability in applications across different domains [8,193]. These advancements in RAG highlight its potential to enhance LLMs by enabling dynamic access to external knowledge, making them more adaptable and capable of addressing complex tasks in real-world scenarios.",
            "score": 0.551641975948997,
            "section_title": "LLM-based RAG",
            "char_start_offset": 10969,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 249
                },
                {
                    "start": 250,
                    "end": 368
                },
                {
                    "start": 369,
                    "end": 549
                },
                {
                    "start": 550,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 1017
                },
                {
                    "start": 1018,
                    "end": 1225
                }
            ],
            "ref_mentions": [
                {
                    "start": 1009,
                    "end": 1012,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1012,
                    "end": 1016,
                    "matchedPaperCorpusId": "252408513"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.744140625
        },
        {
            "corpus_id": "271533386",
            "title": "Introducing a new hyper-parameter for RAG: Context Window Utilization",
            "text": "This paper introduces a new hyper-parameter for Retrieval-Augmented Generation (RAG) systems called Context Window Utilization. RAG systems enhance generative models by incorporating relevant information retrieved from external knowledge bases, improving the factual accuracy and contextual relevance of generated responses. The size of the text chunks retrieved and processed is a critical factor influencing RAG performance. This study aims to identify the optimal chunk size that maximizes answer generation quality. Through systematic experimentation, we analyze the effects of varying chunk sizes on the efficiency and effectiveness of RAG frameworks. Our findings reveal that an optimal chunk size balances the trade-off between providing sufficient context and minimizing irrelevant information. These insights are crucial for enhancing the design and implementation of RAG systems, underscoring the importance of selecting an appropriate chunk size to achieve superior performance.",
            "score": 0.551280651473507,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87890625
        },
        {
            "corpus_id": "268554288",
            "title": "FIT-RAG: Black-Box RAG with Factual Information and Token Reduction",
            "text": "Recently, Large Language Models (LLMs) have grown rapidly in scale and capabilities.Early language models like BERT [8] and T5 [33], show strong performance on natural language understanding and generation tasks.These early successes spur further expansion of LLMs to even larger scales.Models such as InstructGPT [31], LLama [42], OPT [55] and BLOOM [46] processes parameters of tens or even hundreds of billions.This substantial increase in scale brings about a significant enhancement in the model's capacity.Recent models like GPT-4 [30], which possesses an even larger scale, showcases a level of sophistication that approaches human-like proficiency.However, even the strongest GPT-4 model suffers from hallucinations and factual errors as the knowledge stored in the parameters is limited and easy to be out-of-date.To address these issues, a possible solution is Retrieval-Augmented Generation (RAG), which augments LLMs with external knowledge.Traditional RAG frameworks often target models with white-box settings, which may not be accessible in many scenarios since fine-tuning the full LLM requires massive computational resources and many LLMs can only be accessed through APIs [30,31].Therefore, we need to investigate RAG systems tailored for LLMs under black-box settings.",
            "score": 0.5512322785694672,
            "section_title": "Large Language Models",
            "char_start_offset": 7362,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 84,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 287
                },
                {
                    "start": 287,
                    "end": 414
                },
                {
                    "start": 414,
                    "end": 512
                },
                {
                    "start": 512,
                    "end": 823
                },
                {
                    "start": 823,
                    "end": 953
                },
                {
                    "start": 953,
                    "end": 1199
                },
                {
                    "start": 1199,
                    "end": 1288
                }
            ],
            "ref_mentions": [
                {
                    "start": 116,
                    "end": 119,
                    "matchedPaperCorpusId": "52967399"
                },
                {
                    "start": 127,
                    "end": 131,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.63818359375
        },
        {
            "corpus_id": "277150654",
            "title": "Parameters vs. Context: Fine-Grained Control of Knowledge Reliance in Language Models",
            "text": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020;Santhanam et al., 2021;Gao et al., 2023;Fan et al., 2024) has become a widely adopted technique for various applications, as it effectively integrates external knowledge with the powerful generative capabilities of Large Language Models (LLMs) (Achiam et al., 2023;Grattafiori et al., 2024) to produce accurate responses. However, potential knowledge conflicts (Xu et al., 2024a;Xie et al., 2023;Shi et al., 2025) between the external context and the model's internal parameters pose significant challenges to the reliability of RAG-generated outputs, often leading to hallucinations (Huang et al., 2023;Tonmoy et al., 2024). \n\nThere exists an inherent trade-off between the factuality of model parameters and the fidelity of externally retrieved context (Bi et al., 2024d). Enhancing the model's internal factuality (Chuang et al., 2023;Li et al., 2024a;Zhang et al., 2024b) may become unreliable as the model becomes outdated, while excessive dependence on retrieved context (Zhou et al., 2023;Shi et al., 2024) can be problematic due to the quality limitations of the retrieved information. I remember that Elon Musk acquired Twitter and soon rebranded it as X, implementing significant changes to it. \n\nParameters or Context? Which should I believe? \n\nBad para or good context Good para or bad context In this paper, we argue that efficient control of knowledge reliance is crucial for the effective deployment of RAG systems. Existing alignment to factuality (Tian et al., 2023;Lin et al., 2024a) or context faithfulness (Bi et al., 2024a;Huang et al., 2025a) are unidirectional and uncontrollable, lacking the flexibility for bidirectional adjustment. The degree of reliance on internal parameters versus external context should be customizable to adapt to varying RAG scenarios, such as differences in model capabilities or retrieval quality.",
            "score": 0.5507763147467226,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 682
                },
                {
                    "start": 685,
                    "end": 831
                },
                {
                    "start": 832,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1261
                },
                {
                    "start": 1264,
                    "end": 1286
                },
                {
                    "start": 1287,
                    "end": 1310
                },
                {
                    "start": 1313,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1906
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 57,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 97,
                    "end": 114,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 436,
                    "end": 453,
                    "matchedPaperCorpusId": "263610324"
                },
                {
                    "start": 453,
                    "end": 470,
                    "matchedPaperCorpusId": "270737698"
                },
                {
                    "start": 1053,
                    "end": 1070,
                    "matchedPaperCorpusId": "258866080"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.68896484375
        },
        {
            "corpus_id": "277272240",
            "title": "Retrieval Augmented Generation and Understanding in Vision: A Survey and New Outlook",
            "text": "In this survey, we have presented a comprehensive review of retrieval-augmented generation (RAG) techniques in the context of computer vision (CV). We explored the integration of RAG into visual understanding, visual generation, and embodied vision, highlighting the significant potential for enhancing tasks such as object recognition, scene parsing, and 3D content generation. By leveraging external knowledge, RAG improves model performance, addressing challenges related to knowledge limitations, outdated information, and domain-specific expertise. We have identified key advancements in RAG for CV, including its application in multimodal understanding and its role in improving the efficiency and quality of visual generation tasks. Despite its potential, RAG in CV still faces challenges, such as retrieval efficiency, modality alignment, and computational cost, which need to be addressed for broader adoption. Our work also outlines future research directions, particularly in real-time retrieval optimization, crossmodal fusion, and the integration of RAG into embodied AI, 3D content generation, and robotics. This survey serves as a foundation for future exploration of retrieval-augmented techniques in computer vision, offering valuable insights into how these methods can be applied to enhance real-world decision-making and model adaptability. We hope our work encourages further research that will continue to push the boundaries of RAG's applications in CV, leading to more robust, intelligent, and adaptable vision systems.",
            "score": 0.5504996737136737,
            "section_title": "VI. Conclusion",
            "char_start_offset": 53717,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 553
                },
                {
                    "start": 554,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1543
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8369140625
        },
        {
            "corpus_id": "277596113",
            "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System",
            "text": "Recent developments in Generative AI and large language models have made the approach of Retrieval-Augmented Generation, introduced by [Lewis et al., 2020], a favored one for answering complex questions with context. RAG combines a retrieval model that finds relevant data with the LLMs that use these data to generate responses, reducing the risk of generating inaccurate or unsupported information [Yang et al., 2019]. Despite the strengths, RAG still faces challenges, such as retrieving irrelevant or low-quality information that can lead to errors in the final response [Gao et al., 2023]. For instance, irrelevant information might skew the generation of answers, particularly in domains where the stakes are very high such as in medical or legal applications [Pipitone and Alami, 2024] Improving the retrieval component of the RAG is an important way to ensure that the RAG responses are accurate and contextually relevant [Sawarkar et al., 2024] [Mohsin et al., 2025a]. In this respect, hybrid retrieval methods provide a compelling example of improvement, combining dense and sparse retrieval techniques [Mandikal and Mooney, 2024]. The dense retrieval techniques use embeddings to capture semantic similarity, while the sparse retrieval methods ensure that keyword matching is exact. For instance, dense embeddings will find semantically similar phrases like \"climate change impact\" and \"global warming effects,\" whereas sparse embeddings will find exact matches for terms like \"CO2 emissions.\" The combination returns a balanced approach to the retrieval of relevant but precise data to improve the quality of the generated outputs. In addition, domain-specific retrievers trained on domain-specific datasets result in better performance [Chen et al., 2024a] [Mohsin et al., 2025b]. For example, in health care, domain-specific retrievers prove to be good at retrieving disease-specific studies to provide informa-tion very close to the needs in the specific domain. \n\nResearchers have developed various methods to offer improvements.",
            "score": 0.5503295777115453,
            "section_title": "Related Work",
            "char_start_offset": 7143,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1793
                },
                {
                    "start": 1794,
                    "end": 1977
                },
                {
                    "start": 1980,
                    "end": 2045
                }
            ],
            "ref_mentions": [
                {
                    "start": 930,
                    "end": 952,
                    "matchedPaperCorpusId": "269043117"
                },
                {
                    "start": 952,
                    "end": 976,
                    "matchedPaperCorpusId": "266844268"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79638671875
        },
        {
            "corpus_id": "270045808",
            "title": "RE-Adapt: Reverse Engineered Adaptation of Large Language Models",
            "text": "Retrieval-augmented generation (RAG) Lewis et al. ( 2020) is a popular alternative for utilizing new data with instruction-tuned models.Instead of altering the model directly, RAG maintains a database of all text and retrieves relevant documents to include in the prompt as context.This begs the question, is RE-Adapt still beneficial if the new data is already available via RAG?\n\nTable 3: QA performance when using RAG with BM25 and (Oracle) retrievers.\n\nretriever is unrealistic in practice, it allows us to further isolate the benefit of combining RAG with fine-tuning by eliminating any impact from imperfect retrieval.\n\nThe RAG results are shown in Table 3. Again we see significant improvements when using RE-Adapt and LoRE-Adapt even in this RAG setting where the model should already have access to the relevant information needed to answer the questions.The BM-25 search retrieved the correct document with approximately 73% accuracy across models.Using RE-Adapt to incorporate the data outside of RAG alleviates the shortcomings of the retriever.However, RE-Adapt also improved results when using the oracle, suggesting that adding domain knowledge with an adapter also reduces incorrect interpretations of the context retrieved via RAG.",
            "score": 0.550322480283482,
            "section_title": "RE-Adapt with RAG",
            "char_start_offset": 23044,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 282
                },
                {
                    "start": 282,
                    "end": 380
                },
                {
                    "start": 382,
                    "end": 455
                },
                {
                    "start": 457,
                    "end": 624
                },
                {
                    "start": 626,
                    "end": 864
                },
                {
                    "start": 864,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1057
                },
                {
                    "start": 1057,
                    "end": 1248
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.884765625
        },
        {
            "corpus_id": "270560495",
            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
            "text": "Large Language Models (LLMs) have the unique capability to understand and generate human-like text from input queries. When fine-tuned, these models show enhanced performance on domain-specific queries. OpenAI highlights the process of fine-tuning, stating:\"To fine-tune a model, you are required to provide at least 10 examples. We typically see clear improvements from fine-tuning on 50 to 100 training examples, but the right number varies greatly based on the exact use case.\"This study extends this concept to the integration of LLMs within Retrieval-Augmented Generation (RAG) pipelines, which aim to improve accuracy and relevance by leveraging external corpus data for information retrieval. However, RAG's promise of delivering optimal responses often falls short in complex query scenarios. This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI. This study highlights the need for vigorous investigation and validation of fine-tuned models for domain-specific tasks.",
            "score": 0.5492588884370492,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9296875
        },
        {
            "corpus_id": "269740933",
            "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
            "text": "As one of the most fundamental data mining techniques, retrieval aims to understand the input query and extract relevant information from external data sources [24,30,67,140].It has found extensive application in various fields [8,28,106,179], such as search, question answering, and recommender systems.For instance, search engines (e.g., Google, Bing, and Baidu) are the most successful applications of retrieval in the industry; they can filter and retrieve the most relevant web pages or documents that can match a user's query [19,179], enabling users to find the desired information effectively.Meanwhile, retrieval models, through effective data maintenance in external databases, can provide faithful and timely external knowledge, thereby serving vital functions in various knowledge-intensive tasks.Due to their powerful capacities, retrieval techniques have been successfully incorporated into advanced generative models in the era of AI-Generated Content (AIGC) [77,132,163].Notably, the integration of retrieval models with language models has given rise to Retrieval-Augmented Generation (RAG) [74], which has emerged as one of the most representative techniques in the field of generative AI, aiming to enhance the quality of the generated text content with retrieved information [6,74,77].\n\nTo advance generation models and enhance the generated results, RAG incorporates information or knowledge from external data sources, which serves as supplementary for the input query or the generated output [62,103].Specifically, RAG first invokes the retriever to search and extract the relevant documents from external databases, which are then leveraged as the context to enhance the generation process [54].In practice, RAG techniques are feasible and efficient to apply in various generation tasks with simple adaptation of the retrieval component, requiring minimal or even no additional training [117].Recent studies have demonstrated the great potential of RAG not only for knowledge-intensive tasks such as the Open-domain Question Answering (OpenQA) [6,46,109,133], but also for general language tasks [48,62,170], and various downstream applications [90,163].",
            "score": 0.5492067766957445,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 175
                },
                {
                    "start": 175,
                    "end": 304
                },
                {
                    "start": 304,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 809
                },
                {
                    "start": 809,
                    "end": 987
                },
                {
                    "start": 987,
                    "end": 1305
                },
                {
                    "start": 1307,
                    "end": 1524
                },
                {
                    "start": 1524,
                    "end": 1719
                },
                {
                    "start": 1719,
                    "end": 1917
                },
                {
                    "start": 1917,
                    "end": 2178
                }
            ],
            "ref_mentions": [
                {
                    "start": 160,
                    "end": 164,
                    "matchedPaperCorpusId": "270350363"
                },
                {
                    "start": 164,
                    "end": 167,
                    "matchedPaperCorpusId": "224940171"
                },
                {
                    "start": 167,
                    "end": 170,
                    "matchedPaperCorpusId": "3710903"
                },
                {
                    "start": 170,
                    "end": 174,
                    "matchedPaperCorpusId": "260972090"
                },
                {
                    "start": 231,
                    "end": 234,
                    "matchedPaperCorpusId": "259521386"
                },
                {
                    "start": 234,
                    "end": 238,
                    "matchedPaperCorpusId": "13042931"
                },
                {
                    "start": 238,
                    "end": 242,
                    "matchedPaperCorpusId": "4230989"
                },
                {
                    "start": 536,
                    "end": 540,
                    "matchedPaperCorpusId": "4230989"
                },
                {
                    "start": 1108,
                    "end": 1112,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1295,
                    "end": 1298,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 1298,
                    "end": 1301,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1515,
                    "end": 1519,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1519,
                    "end": 1523,
                    "matchedPaperCorpusId": "216056269"
                },
                {
                    "start": 1714,
                    "end": 1718,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 1911,
                    "end": 1916,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76318359375
        },
        {
            "corpus_id": "276903168",
            "title": "Approximate Hausdorff Distance for Multi-Vector Databases",
            "text": "Recent advances in large-scale machine learning have driven the development of Retrieval-Augmented Generation (RAG), where vector databases (VectorDBs) [17,2] play a crucial role in providing efficient and scalable retrieval mechanisms. Unlike traditional information retrieval systems that rely solely on term-based search (e.g., BM25 [20], TF-IDF [22]), VectorDBs support semantic search [23,19,14] by storing and retrieving high-dimensional embeddings [18,11,1], allowing for more accurate context retrieval in generative AI applications. \n\nThe RAG system converts raw data from multiple modalities, including text, images, and audio, into high-dimensional vector embeddings using pretrained models such as OpenAI's Ada [16], Cohere [3], Sentence-BERT [19], CLIP [18], and BLIP [11]. Unlike traditional keyword-based retrieval, which relies on exact term matching, vectorization allows for semantic similarity search by mapping data points into a shared continuous space where proximity reflects conceptual similarity [14]. Recent advancements in contrastive learning [5] and multimodal pretraining [1] have further improved the quality and generalizability of vector representations. These models leverage large-scale self-supervised objectives to produce embeddings that capture deeper semantic relationships across diverse data types. As a result, vectorization is now a fundamental component of modern retrieval-augmented generation (RAG) systems. \n\nWhen a user submits a query, it is vectorized using the same embedding model. The query vector is then used to retrieve the most relevant vectors from the database using approximate nearest neighbor (ANN) search [8,13,10]. Unlike exact k-nearest neighbor (k-NN) search [4], which requires a linear scan or an exact tree-based search over all data points, ANN algorithms trade off a small amount of retrieval accuracy for significant gains in efficiency.",
            "score": 0.5490035566134588,
            "section_title": "Vector Databases",
            "char_start_offset": 5723,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 541
                },
                {
                    "start": 544,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1340
                },
                {
                    "start": 1341,
                    "end": 1454
                },
                {
                    "start": 1457,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 1910
                }
            ],
            "ref_mentions": [
                {
                    "start": 152,
                    "end": 156,
                    "matchedPaperCorpusId": "264426371"
                },
                {
                    "start": 156,
                    "end": 158,
                    "matchedPaperCorpusId": "265502503"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "207178704"
                },
                {
                    "start": 390,
                    "end": 394,
                    "matchedPaperCorpusId": "252995741"
                },
                {
                    "start": 394,
                    "end": 397,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 397,
                    "end": 400,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 755,
                    "end": 759,
                    "matchedPaperCorpusId": "201646309"
                },
                {
                    "start": 1021,
                    "end": 1025,
                    "matchedPaperCorpusId": "5959482"
                },
                {
                    "start": 1669,
                    "end": 1672,
                    "matchedPaperCorpusId": "6110572"
                },
                {
                    "start": 1672,
                    "end": 1675,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 1675,
                    "end": 1678,
                    "matchedPaperCorpusId": "926364"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75732421875
        },
        {
            "corpus_id": "271571143",
            "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications",
            "text": "Retrieval-Augmented Generation (RAG) enhances the performance of large language models by leveraging external knowledge bases to retrieve relevant document segments through semantic similarity computations [23], [29]. This approach significantly reduces the incidence of hallucinations-defined as instances where generated content deviates from factual accuracy [38], [42]. Early research on RAG primarily focused on developing sparse or dense retrievers [19], [20], whereas contemporary studies have emphasized optimizing the inte-gration of RAG with Large Language Models (LLMs). These optimizations encompass the timing of retrieval, methodological enhancements, and refined utilization of contextual information to mitigate noise within retrieved documents [10], [12]. \n\nWith respect to adaptive retrieval strategies, the FLARE project has proposed two novel methods: proactive retrieval based on retrieval instructions and confidence-based proactive retrieval, both designed to mitigate unnecessary retrievals [18]. Furthermore, process optimization has evolved from the \"Rewrite-Retrieve-Read\" paradigm [24] to \"ITER-RETGEN\" implementing iterative retrieval to incrementally access more granular and comprehensive knowledge [30]. Concerning the management of noise within retrieved documents, studies conducted demonstrated that irrelevant noise documents do not necessarily deteriorate system performance; conversely, they can enhance accuracy by up to 35%. Conversely, documents incorrectly classified as relevant to the query introduce significant interference, substantially impacting the model's generative performance [9]. \n\nIn the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG [16] project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28].",
            "score": 0.5480984203053578,
            "section_title": "B. Retrieval-augmented Generation",
            "char_start_offset": 9292,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 373
                },
                {
                    "start": 374,
                    "end": 581
                },
                {
                    "start": 582,
                    "end": 772
                },
                {
                    "start": 775,
                    "end": 1020
                },
                {
                    "start": 1021,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1634
                },
                {
                    "start": 1637,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2139
                },
                {
                    "start": 2140,
                    "end": 2419
                }
            ],
            "ref_mentions": [
                {
                    "start": 206,
                    "end": 210,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 212,
                    "end": 216,
                    "matchedPaperCorpusId": "256459451"
                },
                {
                    "start": 455,
                    "end": 459,
                    "matchedPaperCorpusId": "259316759"
                },
                {
                    "start": 1871,
                    "end": 1875,
                    "matchedPaperCorpusId": "267312134"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8701171875
        },
        {
            "corpus_id": "276249796",
            "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
            "text": "Recent advances in retrieval-augmented generation (RAG) for large language models (LLMs) have demonstrated remarkable capabilities in various tasks (Anthropic, 2024; Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). Hurst et al., 2024;Dubey et al., 2024;Yang et al., 2024a;Mesnard et al., 2024;Asai et al., 2024;Chen et al., 2024a;b;Wei et al., 2025;Sun et al., 2025b;a), empowering LLMs to acquire up-to-date or domain-specific knowledge while mitigating hallucinations (Gao et al., 2023;Fan et al., 2024;Qiao et al., 2024). The effectiveness of RAG systems, however, hinges on the alignment1 between the retriever and the LLM-an inherently challenging goal as these components are typically developed independently without co-training. This lack of co-training can result in semantic mismatch and suboptimal interactions: retrievers may fail to provide information tailored to the LLM's needs, while LLMs may struggle to generate effective queries or seamlessly incorporate retrieved content. Existing approaches address this misalignment through three main strategies: (1) fine-tuning retrievers to align with LLM preferences, (2) optimizing LLMs to adapt to retriever behavior, and (3) introducing intermediate modules to bridge the gap between them (Ma et al., 2023;Shi et al., 2024;Asai et al., 2024;Wei et al., 2025;Yu et al., 2024a;b). Despite progress, these methods face notable challenges: fine-tuning retrievers often requires carefully curated data and may not be feasible for commercial search engines (Schmidt, 2014;Nakano et al., 2021), while optimizing LLMs is resource-intensive and risks compromising their original capabilities (Zhou et al., 2024).",
            "score": 0.5480937599932949,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 271
                },
                {
                    "start": 272,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 826
                },
                {
                    "start": 827,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1757
                }
            ],
            "ref_mentions": [
                {
                    "start": 383,
                    "end": 401,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 401,
                    "end": 420,
                    "matchedPaperCorpusId": "270559546"
                },
                {
                    "start": 422,
                    "end": 439,
                    "matchedPaperCorpusId": "271909559"
                },
                {
                    "start": 439,
                    "end": 457,
                    "matchedPaperCorpusId": "270225999"
                },
                {
                    "start": 578,
                    "end": 595,
                    "matchedPaperCorpusId": "269740933"
                },
                {
                    "start": 1360,
                    "end": 1377,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 1377,
                    "end": 1395,
                    "matchedPaperCorpusId": "264288947"
                },
                {
                    "start": 1395,
                    "end": 1412,
                    "matchedPaperCorpusId": "271909559"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.60400390625
        },
        {
            "corpus_id": "272708597",
            "title": "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation",
            "text": "Moreover, while the fraction of retrieved sources that actually appear in the final response may vary, equitable retrieval frequently leads to more equitable usage of those sources by the generator. \n\nThis surprising finding offers significant insight into the potential of RAG-based applications, suggesting that fair treatment of individual content providers can be achieved without sacrificing much of the high-quality service delivered to end-users. This challenges the conventional assumption of an inevitable tradeoff between fairness and quality, opening new avenues for developing more equitable and effective RAG systems. \n\n2 Background & Related Work 2.1 Retrieval-Augmented Generation RAG, a specific type of retrieval-enhanced machine learning (REML) [32,60], has been widely adopted in various domains, including language modeling [31], question-answering [27], and personalization [40,49,50]. Studies on the evaluation of RAG models have primarily focused on their effectiveness, including end-to-end performance [20,27,33] and the assessment of individual components [14,47,48,51], such as retrieval relevance and model faithfulness. Furthermore, recent efforts have explored attribution mechanisms for ensuring the trustworthiness of RAG responses [16,17], examining how thoroughly models reference the source text and thus promoting a more faithful generation process. \n\nHowever, little research has focused on evaluating fairness in retrieval-enhanced generation models, with the exception of recent work [54], which improved demographic diversity in human image generation by conditioning a generative model with externally retrieved images that help debias the generation process.",
            "score": 0.5479405278785302,
            "section_title": "Introduction",
            "char_start_offset": 4553,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 198
                },
                {
                    "start": 201,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1700
                }
            ],
            "ref_mentions": [
                {
                    "start": 767,
                    "end": 770,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 844,
                    "end": 848,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 869,
                    "end": 873,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 895,
                    "end": 899,
                    "matchedPaperCorpusId": "268417177"
                },
                {
                    "start": 899,
                    "end": 902,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 902,
                    "end": 905,
                    "matchedPaperCorpusId": "258298303"
                },
                {
                    "start": 1027,
                    "end": 1031,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 1031,
                    "end": 1034,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 1034,
                    "end": 1037,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1082,
                    "end": 1086,
                    "matchedPaperCorpusId": "263152733"
                },
                {
                    "start": 1086,
                    "end": 1089,
                    "matchedPaperCorpusId": "271874517"
                },
                {
                    "start": 1089,
                    "end": 1092,
                    "matchedPaperCorpusId": "265221210"
                },
                {
                    "start": 1092,
                    "end": 1095,
                    "matchedPaperCorpusId": "269293655"
                },
                {
                    "start": 1264,
                    "end": 1268,
                    "matchedPaperCorpusId": "254247260"
                },
                {
                    "start": 1268,
                    "end": 1271,
                    "matchedPaperCorpusId": "258865710"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "268793876"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7802734375
        },
        {
            "corpus_id": "273403480",
            "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
            "text": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent in the RAG system with the rollout method, which prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes the generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes are available at https://github.com/OpenMatch/RAG-DDR.",
            "score": 0.5479337769593946,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97314453125
        },
        {
            "corpus_id": "278020653",
            "title": "ENHANCED RETRIEVAL-AUGMENTED GENERATION FOR STUDENT MENTAL HEALTH SUPPORT USING GENERATIVE ARTIFICIAL INTELLIGENCE",
            "text": "Generative AI, combined with Retrieval-Augmented Generation (RAG), enhances large language models (LLMs) by integrating dynamic information retrieval, significantly improving response accuracy and contextual relevance. This study explores the implementation of Generative AI-powered RAG in student mental health support systems, demonstrating how it mitigates hallucinations, incorporates real-time knowledge updates, and personalizes assistance. The proposed system integrates retrieval and generation components, leveraging vector-based search mechanisms to access domain-specific mental health knowledge. Comparative analysis with traditional LLMs highlights RAG\u2019s superior accuracy, reduced misinformation, and improved response reliability. Experimental evaluation using context precision, hit rate, faithfulness, and user satisfaction metrics validates the system's effectiveness. This research underscores the transformative potential of Generative AI-driven RAG in delivering scalable, evidence-based mental health support for students.",
            "score": 0.5478728435809639,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8515625
        },
        {
            "corpus_id": "271212541",
            "title": "Enhancing Retrieval and Managing Retrieval: A Four-Module Synergy for Improved Quality and Efficiency in RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) [20] leverages a retriever that provides substantial external information to enhance the output of Large Language Models (LLMs).This strategy utilizes knowledge in a parameter-free manner and circumvents the high training costs associated with LLMs' parameterized knowledge.Furthermore, it alleviates hallucination issues, significantly enhancing the factual accuracy and relevance of the generated content.The concept of RAG is rooted in the DrQA framework [5], which marked the initial phase of integrating retrieval mechanisms with Language Models through heuristic retrievers like TF-IDF for sourcing evidence.Subsequently, RAG evolved with the introduction of Dense Passage Retrieval [15] and REALM [26].These methods utilize pre-trained transformers and are characterized by the joint optimization of retrieval and generation process.Recent advancements have extended RAG's capabilities by integrating Large Language Models (LLMs), with developments such as REPLUG [29] and IC-RALM [26] demonstrating the potent generalization abilities of LLMs in zero-shot or few-shot scenarios.These models can follow complex instructions, understand retrieved information, and utilize limited demonstrations for generating high-quality responses.",
            "score": 0.5473260482125347,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 27814,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 311
                },
                {
                    "start": 311,
                    "end": 444
                },
                {
                    "start": 444,
                    "end": 651
                },
                {
                    "start": 651,
                    "end": 746
                },
                {
                    "start": 746,
                    "end": 877
                },
                {
                    "start": 877,
                    "end": 1123
                },
                {
                    "start": 1123,
                    "end": 1276
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 41,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.7939453125
        },
        {
            "corpus_id": "270737995",
            "title": "Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary",
            "text": "In the field of Retrieval-Augmented Generation (RAG), significant progress has been made recently. One study [Shahul et al., 2023] introduces the RAGAs evaluation framework for rapid assessment of Retrieval-Augmented Generation system performance, particularly suitable for the rapid deployment of LLMs. \n\nAnother study investigates the domain adaptability of RAG models and proposes the RAG-end2end approach, enabling them to adapt to specific domain knowledge bases [Siriwardhana et al., 2022]. Additionally, researchers propose MuRAG, a multi-modal Retrieval-Augmented Generator. It leverages external non-parametric multi-modal memory for enhanced language generation, demonstrating outstanding cross-dataset performance in tasks involving image and text-based questions and answers [Chen et al., 2022]. Finally, another study develops the ARM-RAG system, which enhances problem-solving performance by storing and retrieving inference chains. This effectively boosts the intelligence of large language models while reducing training costs [Melz, 2023]. These advancements demonstrate the potential of RAG technology in enhancing the accuracy of knowledge access, the quality of generation, and the narrative ability. Our research explores the integration of RAG with domain-specific strategies to enhance the strategic depth of game commentary.",
            "score": 0.5473260482125347,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 4157,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 303
                },
                {
                    "start": 306,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 807
                },
                {
                    "start": 808,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1348
                }
            ],
            "ref_mentions": [
                {
                    "start": 109,
                    "end": 130,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 468,
                    "end": 495,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 787,
                    "end": 806,
                    "matchedPaperCorpusId": "252735160"
                },
                {
                    "start": 1043,
                    "end": 1055,
                    "matchedPaperCorpusId": "265043634"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.85009765625
        },
        {
            "corpus_id": "274776545",
            "title": "RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation",
            "text": "Retrieval-augmented Generation Retrievalaugmented generation (RAG) improves generation quality by incorporating relevant context from external knowledge bases, which typically employ a separate dense retriever (Gao et al., 2024;Tan et al., 2024b;Jin et al., 2024b;Tan et al., 2024a;Zhou et al., 2024). Based on training approaches, current RAG systems fall into three categories: \n\n(1) Directly prompt of generative models with retrieved context (Press et al., 2023;Trivedi et al., et al., 2020;Singh et al., 2021). However, joint training faces challenges due to the architectural differences between retrieval and generation, as well as the need for updating document indices during training. Some approaches aim to unify dense retrieval and generation within a single model, including GritLM (Muennighoff et al., 2024) and OneGen (Zhang et al., 2024a). However, GritLM operates as two distinct models with separate attention mechanisms that share parameters, while OneGen still relies on retrieving passage chunks as input for subsequent generation. \n\nGenerative Retrieval Generative retrieval (GR) retrieves by directly generating document identifiers (DocIDs) without the need for traditional document indices (Metzler et al., 2021) (Li et al., 2023a;Tang et al., 2024), and learnable DocIDs (Sun et al., 2023;Wang et al., 2023;Yang et al., 2023). However, these methods mainly focus on optimizing retrieval tasks, without considering its connections with downstream tasks. Even though UniGen (Li et al., 2024c) and CorpusLM (Li et al., 2024a) address downstream tasks, they still require mapping the generated Do-cIDs to the corresponding documents before feeding them into the generator. While RICHES (Jain et al., 2024) attempts to streamline this process but fails to solve the false pruning issue, which leads to suboptimal downstream task performance.",
            "score": 0.546491130576402,
            "section_title": "Related Work",
            "char_start_offset": 22502,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 379
                },
                {
                    "start": 382,
                    "end": 515
                },
                {
                    "start": 516,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 855
                },
                {
                    "start": 856,
                    "end": 1052
                },
                {
                    "start": 1055,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1478
                },
                {
                    "start": 1479,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1862
                }
            ],
            "ref_mentions": [
                {
                    "start": 228,
                    "end": 246,
                    "matchedPaperCorpusId": "267750726"
                },
                {
                    "start": 246,
                    "end": 264,
                    "matchedPaperCorpusId": "267751485"
                },
                {
                    "start": 446,
                    "end": 466,
                    "matchedPaperCorpusId": "252762102"
                },
                {
                    "start": 495,
                    "end": 514,
                    "matchedPaperCorpusId": "235390519"
                },
                {
                    "start": 1297,
                    "end": 1315,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1315,
                    "end": 1333,
                    "matchedPaperCorpusId": "264350310"
                },
                {
                    "start": 1333,
                    "end": 1351,
                    "matchedPaperCorpusId": "264305656"
                },
                {
                    "start": 1498,
                    "end": 1516,
                    "matchedPaperCorpusId": "266359654"
                },
                {
                    "start": 1530,
                    "end": 1548,
                    "matchedPaperCorpusId": "267406766"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8154296875
        },
        {
            "corpus_id": "269043117",
            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
            "text": "Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
            "score": 0.5462770767312055,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.90576171875
        },
        {
            "corpus_id": "271843432",
            "title": "A Hybrid RAG System with Comprehensive Enhancement on Complex Reasoning",
            "text": "Retrieval-augmented generation (RAG) is a framework enabling large language models (LLMs) to enhance their accuracy and reduce hallucinations by integrating external knowledge bases. In this paper, we introduce a hybrid RAG system enhanced through a comprehensive suite of optimizations that significantly improve retrieval quality, augment reasoning capabilities, and refine numerical computation ability. We refined the text chunks and tables in web pages, added attribute predictors to reduce hallucinations, conducted LLM Knowledge Extractor and Knowledge Graph Extractor, and finally built a reasoning strategy with all the references. We evaluated our system on the CRAG dataset through the Meta CRAG KDD Cup 2024 Competition. Both the local and online evaluations demonstrate that our system significantly enhances complex reasoning capabilities. In local evaluations, we have significantly improved accuracy and reduced error rates compared to the baseline model, achieving a notable increase in scores. In the meanwhile, we have attained outstanding results in online assessments, demonstrating the performance and generalization capabilities of the proposed system. The source code for our system is released in \\url{https://gitlab.aicrowd.com/shizueyy/crag-new}.",
            "score": 0.545998487451854,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79248046875
        },
        {
            "corpus_id": "271571401",
            "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks",
            "text": "The development of RAG technology can be summarized in three stages. Initially, retrieval-augmented techniques were introduced to improve the performance of pre-trained language models on knowledge-intensive tasks [19], [20]. In specific implementations, Retro [21] optimized pre-trained autoregressive models through retrieval augmentation, while Atlas [22] utilized a retrieval-augmented few-shot fine-tuning method, enabling language models to adapt to diverse tasks. IRCOT [23] further enriched the reasoning process during the inference phase by combining chain-of-thought and multistep retrieval processes. Entering the second stage, as the language processing capabilities of LLMs significantly improved, retrieval-augmented techniques began to serve as a means of supplementing additional knowledge and providing references, aiming to reduce the hallucination. For instance, RRR [24] improved the rewriting phase, and LLMlingua [25] removed redundant tokens in retrieved document chunks. With the continuous progress of RAG technology, research has become more refined and focused, while also achieving innovative integration with other technologies such as graph neural networks [26] and fine-tuning techniques [27]. The overall pipeline has also become more flexible, such as using LLMs to proactively determine the timing of retrieval and generation [14], [28]. \n\nThe development of RAG technology has been accelerated by LLM technology and practical application needs. Researchers are examining and organizing the RAG framework and development pathways from different perspectives. Building upon the enhanced stages of RAG, Gao et al., [2] subdivided RAG into enhancement during pre-training, inference, and fine-tuning stages. Based on the main processes of RAG, relevant works on RAG were organized from the perspectives of retrieval, generation, and augmentation methods. Huang et al., [29] categorize RAG methods into four main classes: pre-retrieval, retrieval, post-retrieval, generation, and provide a detailed discussion of the methods and techniques within each class. Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications.",
            "score": 0.5459596975620752,
            "section_title": "II. RELATED WORK",
            "char_start_offset": 8318,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 612
                },
                {
                    "start": 613,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1225
                },
                {
                    "start": 1226,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1739
                },
                {
                    "start": 1740,
                    "end": 1886
                },
                {
                    "start": 1887,
                    "end": 2089
                },
                {
                    "start": 2090,
                    "end": 2318
                }
            ],
            "ref_mentions": [
                {
                    "start": 220,
                    "end": 224,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 261,
                    "end": 265,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 936,
                    "end": 940,
                    "matchedPaperCorpusId": "252186384"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.765625
        },
        {
            "corpus_id": "276617922",
            "title": "Faster, Cheaper, Better: Multi-Objective Hyperparameter Optimization for LLM and RAG Systems",
            "text": "Retrieval Augmented Generation (RAG) has emerged as a popular technique for improving the performance of Large Language Models (LLMs) on question-answering tasks over specific datasets. A benefit of using RAG pipelines is that they can often achieve high performance on specific tasks without the need for extensive alignment and fine-tuning (Gupta et al., 2024), a costly and timeconsuming process. However, the end-to-end pipeline of a RAG system is dependent on many parameters that span different components (or modules) of the system, such as the choice of LLM, the embedding model used in retrieval, the number of chunks retrieved and hyperparameters governing a reranking model. Examples of choices, parameters, and hyperparameters that are often made or tuned when implementing a RAG pipeline are listed in Table 1. Importantly, the performance of a RAG pipeline is dependent on these choices (Fu et al., 2024), many of which can be difficult to tune manually. While those building RAG pipelines might avoid fine-tuning costs, they often spend time and resources on hyperparameter optimization (HO). \n\nDespite this, there is little research exploring methods for collectively optimizing all the hyperparameters in a given LLM and RAG pipeline (Fu et al., 2024). Further, to the best of our knowledge, there is no work that addresses this challenge in multi-objective settings, where the RAG pipeline must achieve high performance across a range of objectives, like minimizing a system's inference time while maximizing its helpfulness. In this work, we aim to fill this gap by introducing an approach for collectively optimizing the hyperparameters of a RAG system in a multi-objective setting. \n\nFigure 1: A high-level overview of our approach. First, we source the datasets that we will use to optimize our RAG pipeline, define the choices, parameters and hyperparameters that will be optimized over (see Table 1), and select the objectives for optimization (e.g., cost, latency, safety, and alignment). Second, we introduce a train-test paradigm for evaluating the performance of RAG pipelines, and use Bayesian optimization (BO) to find the optimal parameter configurations.",
            "score": 0.545790261910013,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 968
                },
                {
                    "start": 969,
                    "end": 1107
                },
                {
                    "start": 1110,
                    "end": 1269
                },
                {
                    "start": 1270,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1702
                },
                {
                    "start": 1705,
                    "end": 1753
                },
                {
                    "start": 1754,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87060546875
        },
        {
            "corpus_id": "276287820",
            "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG",
            "text": "Retrieval Augmented Generation: RAG enhances Large Language Models (LLMs) by integrating external data sources, such as knowledge bases, to improve relevance and accuracy (Lewis et al., 2020;Guu et al., 2020;Karpukhin et al., 2020). Recent advancements have extended its applicability across domains (Asai et al., 2024;Kim et al., 2024;Yan et al., 2024;Liu et al., 2024), but RAG systems still face key challenges: hallucinations due to mismatches between retrieved data and the LLM's pre-existing knowledge (Setty et al., 2024;Jin et al., 2024), difficulty with complex multi-document reasoning (Setty et al., 2024), and an inability to fully leverage fixed-domain settings where all domain-specific documents are available beforehand because typically neither the retriever nor the generator LLM are trained on the domain data. Domain-Aware Fine-Tuning for RAG: Joint training of the retriever and LLM has been proposed as a way to improve RAG's domain-specific performance (Guu et al., 2020;Sachan et al., 2021;Siriwardhana et al., 2023;Shi et al., 2024). By jointly training the retriever and LLM, the system can better adapt to domain-specific contexts. However, this approach introduces complexities, including the need for specialized loss functions and frequent retriever updates. \n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference. \n\nRecently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.",
            "score": 0.5452360836095633,
            "section_title": "Related Work",
            "char_start_offset": 5971,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1058
                },
                {
                    "start": 1059,
                    "end": 1158
                },
                {
                    "start": 1159,
                    "end": 1288
                },
                {
                    "start": 1291,
                    "end": 1434
                },
                {
                    "start": 1435,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1677
                },
                {
                    "start": 1680,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 1980
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.92138671875
        },
        {
            "corpus_id": "273345235",
            "title": "Evaluating the Effectiveness and Efficiency of Demonstration Retrievers in RAG for Coding Tasks",
            "text": "Large language models (LLMs) have achieved remarkable success in natural language processing, but they still encounter significant limitations in the domain of knowledge-intensive tasks. In particular, LLMs are susceptible to \"hallucinations\" when confronted with queries that exceed the scope of their training data or necessitate the utilization of current information [1]. Retrieval-Augmented Generation (RAG) is the leading technique for improving LLMs by providing demonstrations from knowledge bases. By referencing external knowledge sources, RAG effectively mitigates the problem of generating factual inconsistency content and facilitates the continuous updating of knowledge [2]. \n\nRecently, the application of RAG has achieved promising results for various code-specific tasks such as Code Search [3], Program Synthesis [4], [5], and Assertion Generation [6]. In these tasks, LLM learns from contextual prompts consisting of task descriptions, queries, and additional demonstration examples without the need to fine-tune the model parameters. The retrieved demonstrations are typically used as the context to assist the pre-trained LLM in comprehending the task and regulating the generation behavior, which usually has a significant impact on the quality of the generated output. Therefore, it is important to retrieve appropriate demonstrations from a vast knowledge base. \n\nThe retriever plays a core role in retrieving relevant demonstrations from the external knowledge base and significantly affects the performance of RAG [7], [8]. Retrievers are typically classified into sparse and dense retrievers based on representation methods. Sparse retrievers operate at the token level, while dense retrievers operate at the level of latent semantics. The most widely-used sparse retriever, BM25 [9], ranks demonstrations based on term frequency (TF) and inverse document frequency (IDF) of the query. Dense retrievers perform retrieval by encoding the query and demonstrations into dense embedding representations and scoring each demonstration by its similarity with the query embedding. For instance, the popular RAG system Llamaindex [10] supports both BM25 and custom embedding encoders in RAG workflow.",
            "score": 0.5448335995239182,
            "section_title": "I. INTRODUCTION",
            "char_start_offset": 18,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1385
                },
                {
                    "start": 1388,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1651
                },
                {
                    "start": 1652,
                    "end": 1762
                },
                {
                    "start": 1763,
                    "end": 1912
                },
                {
                    "start": 1913,
                    "end": 2100
                },
                {
                    "start": 2101,
                    "end": 2219
                }
            ],
            "ref_mentions": [
                {
                    "start": 808,
                    "end": 811,
                    "matchedPaperCorpusId": "269130676"
                },
                {
                    "start": 831,
                    "end": 834,
                    "matchedPaperCorpusId": "258180059"
                },
                {
                    "start": 836,
                    "end": 839,
                    "matchedPaperCorpusId": "252734952"
                },
                {
                    "start": 866,
                    "end": 869,
                    "matchedPaperCorpusId": "259860357"
                },
                {
                    "start": 1545,
                    "end": 1548,
                    "matchedPaperCorpusId": "269293655"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8251953125
        },
        {
            "corpus_id": "271270246",
            "title": "Can Open-Source LLMs Compete with Commercial Models? Exploring the Few-Shot Performance of Current GPT Models in Biomedical Tasks",
            "text": "Retrieval augmented generation (RAG) is a technique [2] that combines information retrieval with language models to enhance their ability to generate relevant and factual text. In RAG, the language model is augmented with an external knowledge base or other information source, such as a collection of documents or web pages. When generating text, the model first retrieves relevant information, based on the input query, and then uses that information to guide the generation process. This process is applied in the BioASQ challenge, where the relevant information source is the annual baseline of PubMed. \n\nRAG has been shown to improve the factual accuracy of generated text compared to standalone language models [21]. It allows the model to access a vast amount of external knowledge and incorporate it into the generated output. RAG is particularly useful for tasks that require domain-specific knowledge or up-to-date information [3].",
            "score": 0.5439613087635999,
            "section_title": "Retrieval Augmented Generation (RAG)",
            "char_start_offset": 8906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 485
                },
                {
                    "start": 486,
                    "end": 606
                },
                {
                    "start": 609,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 941
                }
            ],
            "ref_mentions": [
                {
                    "start": 52,
                    "end": 55,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 717,
                    "end": 721,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.61328125
        },
        {
            "corpus_id": "277955197",
            "title": "Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale",
            "text": "Figure 8 presents model performance across various question-answering types with a 1M-token context, demonstrating patterns similar to those observed in Figure 3. \n\nFigure 9 presents the performance of models enhanced with Retrieval-Augmented Generation (RAG) modules over a 128K-token context. Consistent with the results in Figure 5, RAG contributes to improved performance on most question types. \n\nFigure 10 shows the performance with respect to the number of sessions elapsed since the most recent preferences were mentioned in the conversation history. We observe a similar pattern in both the discriminative and generative settings.",
            "score": 0.5439608026812669,
            "section_title": "B Supplementary Experiment Results",
            "char_start_offset": 28486,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 165,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 399
                },
                {
                    "start": 402,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 639
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.548828125
        },
        {
            "corpus_id": "271039066",
            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
            "text": "This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning. Both RETRO and GPT models perform optimally around the 8B parameter mark, balancing cost and performance. While P-tuning is effective in larger models, it lags behind other methods in smaller models, particularly for RETRO. Applying PEFT to Instructiontuned RETRO yields limited improvement compared to base RETRO, suggesting a saturation point in leveraging pre-training and fine-tuning benefits. Our comprehensive analysis offers valuable insights for optimizing large language models with PEFT and RAG to the community.",
            "score": 0.5438752896203215,
            "section_title": "Conclusion",
            "char_start_offset": 13407,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 321
                },
                {
                    "start": 322,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 669
                },
                {
                    "start": 670,
                    "end": 843
                },
                {
                    "start": 844,
                    "end": 968
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.97802734375
        },
        {
            "corpus_id": "273374817",
            "title": "Evaluation of Attribution Bias in Retrieval-Augmented Large Language Models",
            "text": "Retrieval Augmented Generation. Given a question q and a set of top-k retrieved documents D ={d 1 , d 2 , . . . , d k } from a collection C, the goal of retrieval augmented generation (RAG) is to generate an answer for q using D as context. LLMs are currently an important component of RAG pipelines, acting as the generator. The generator is given q, D, and an instruction prompt on how to generate the answer (Jeong et al., 2024;Lee et al., 2024;Li et al., 2024a). Using top-k retrieved documents helps LLMs to be exposed to information that it might not have been trained/fine-tuned with during development. These documents are commonly retrieved using an effective sparse and/or dense retriever (Lewis et al., 2020). \n\nAttributive RAG. LLMs are prone to generate hallucinated (and even factually incorrect) answers (Ji et al., 2023;Rawte et al., 2023;Yue et al., 2024). \n\nAttributing answers in RAG with LLMs is an approach taken as a step towards ensuring the veracity of the output of these models (Bohnet et al., 2022;Hu et al., 2024;Kamalloo et al., 2023;Khalifa et al.;Li et al., 2024b). Menick et al. (2022) teach language models to support answers with verified quotes. Ye et al. (2024) propose a learning-based framework in which they fine-tune LLMs to generate citations, as opposed to prompting or relying on post-hoc attribution. Stolfo (2024) analyzes whether every generated sentence in the output of LLMs is grounded in the retrieved documents or the LLM's pre-training data.",
            "score": 0.543575287751742,
            "section_title": "Background",
            "char_start_offset": 4962,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 111
                },
                {
                    "start": 112,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 466
                },
                {
                    "start": 467,
                    "end": 610
                },
                {
                    "start": 611,
                    "end": 720
                },
                {
                    "start": 723,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 873
                },
                {
                    "start": 876,
                    "end": 1096
                },
                {
                    "start": 1097,
                    "end": 1180
                },
                {
                    "start": 1181,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1493
                }
            ],
            "ref_mentions": [
                {
                    "start": 411,
                    "end": 431,
                    "matchedPaperCorpusId": "268553748"
                },
                {
                    "start": 431,
                    "end": 448,
                    "matchedPaperCorpusId": "270514562"
                },
                {
                    "start": 448,
                    "end": 465,
                    "matchedPaperCorpusId": "259501744"
                },
                {
                    "start": 699,
                    "end": 719,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 819,
                    "end": 836,
                    "matchedPaperCorpusId": "246652372"
                },
                {
                    "start": 836,
                    "end": 855,
                    "matchedPaperCorpusId": "263831293"
                },
                {
                    "start": 855,
                    "end": 872,
                    "matchedPaperCorpusId": "268667523"
                },
                {
                    "start": 1063,
                    "end": 1078,
                    "matchedPaperCorpusId": "268819100"
                },
                {
                    "start": 1078,
                    "end": 1095,
                    "matchedPaperCorpusId": "263830219"
                },
                {
                    "start": 1181,
                    "end": 1197,
                    "matchedPaperCorpusId": "265220884"
                },
                {
                    "start": 1345,
                    "end": 1358,
                    "matchedPaperCorpusId": "269033410"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "271329372",
            "title": "Customized Retrieval Augmented Generation and Benchmarking for EDA Tool Documentation QA",
            "text": "Retrieval augmented generation (RAG) enhances the accuracy and reliability of generative AI models by sourcing factual information from external databases, which is extensively employed in document-grounded question-answering (QA) tasks. Off-the-shelf RAG flows are well pretrained on general-purpose documents, yet they encounter significant challenges when being applied to knowledge-intensive vertical domains, such as electronic design automation (EDA). This paper addresses such issue by proposing a customized RAG framework along with three domain-specific techniques for EDA tool documentation QA, including a contrastive learning scheme for text embedding model fine-tuning, a reranker distilled from proprietary LLM, and a generative LLM fine-tuned with high-quality domain corpus. Furthermore, we have developed and released a documentation QA evaluation benchmark, ORD-QA, for OpenROAD, an advanced RTL-to-GDSII design platform. Experimental results demonstrate that our proposed RAG flow and techniques have achieved superior performance on ORD-QA as well as on a commercial tool, compared with state-of-the-arts. The ORD-QA benchmark and the training dataset for our customized RAG flow are open-source at https://github.com/lesliepy99/RAG-EDA.",
            "score": 0.5435015207138879,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70068359375
        },
        {
            "corpus_id": "274436303",
            "title": "MBA-RAG: a Bandit Approach for Adaptive Retrieval-Augmented Generation through Question Complexity",
            "text": "Retrieval Augmented Generation (RAG) has proven to be highly effective in boosting the generative performance of language model in knowledge-intensive tasks. However, existing RAG framework either indiscriminately perform retrieval or rely on rigid single-class classifiers to select retrieval methods, leading to inefficiencies and suboptimal performance across queries of varying complexity. To address these challenges, we propose a reinforcement learning-based framework that dynamically selects the most suitable retrieval strategy based on query complexity. % our solution Our approach leverages a multi-armed bandit algorithm, which treats each retrieval method as a distinct ``arm'' and adapts the selection process by balancing exploration and exploitation. Additionally, we introduce a dynamic reward function that balances accuracy and efficiency, penalizing methods that require more retrieval steps, even if they lead to a correct result. Our method achieves new state of the art results on multiple single-hop and multi-hop datasets while reducing retrieval costs. Our code are available at https://github.com/FUTUREEEEEE/MBA .",
            "score": 0.5433221376957301,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.8115234375
        },
        {
            "corpus_id": "273404015",
            "title": "FRAG: Toward Federated Vector Database Management for Collaborative and Secure Retrieval-Augmented Generation",
            "text": "In recent years, large-language models (LLMs) such as ChatGPT [36], LLaMA [47], BERT [11], and Transformer [40] have fundamentally transformed the landscape of natural language processing (NLP) and artificial intelligence (AI). These models, trained on vast amounts of textual data, have demonstrated unprecedented capabilities in generating human-like text, understanding complex queries, and performing tasks that require deep contextual understanding [7,11]. Applications leveraging LLMs span across numerous industries, from automated customer service and real-time translation to content creation and medical diagnostics. The flexibility and power of LLMs have made them central to many modern AI systems. \n\nA crucial enhancement in the effectiveness of LLMs has been the integration of Retrieval-Augmented Generation (RAG). Unlike purely generative models, RAG systems enhance their output by retrieving relevant external knowledge from vast databases or knowledge bases to augment their responses [27]. This mechanism allows LLMs to generate more informed, accurate, and contextually rich outputs by incorporating real-time or pre-stored information. For example, when answering factual queries, the model can retrieve up-to-date knowledge rather than relying solely on its pre-trained information. As a result, RAG has become a vital component in applications where the accuracy of the generated content is paramount, such as in recommendation systems, conversational agents, and information retrieval. \n\nAt the heart of RAG systems are vector embeddings, which represent pieces of data, such as words, sentences, or entire documents, as high-dimensional vectors. These embeddings capture semantic similarities between different entities, enabling efficient retrieval of related data. Vector databases like Faiss [23], Annoy [4], and HNSW [32] have been widely adopted for storing and querying these embeddings, offering optimized solutions for Approximate -Nearest Neighbor (ANN) searches on large-scale datasets [4,23,32]. These systems rely on specialized indexing and search algorithms to quickly identify the most relevant embeddings from millions or billions of records, making them indispensable in modern AI pipelines.",
            "score": 0.5430823580803471,
            "section_title": "Introduction 1.Background",
            "char_start_offset": 28,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 461
                },
                {
                    "start": 462,
                    "end": 626
                },
                {
                    "start": 627,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1305
                },
                {
                    "start": 1306,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1792
                },
                {
                    "start": 1793,
                    "end": 2032
                },
                {
                    "start": 2033,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 107,
                    "end": 111,
                    "matchedPaperCorpusId": "204838007"
                },
                {
                    "start": 454,
                    "end": 457,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1004,
                    "end": 1008,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1821,
                    "end": 1825,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 1847,
                    "end": 1851,
                    "matchedPaperCorpusId": "8915893"
                },
                {
                    "start": 2025,
                    "end": 2028,
                    "matchedPaperCorpusId": "926364"
                },
                {
                    "start": 2028,
                    "end": 2031,
                    "matchedPaperCorpusId": "8915893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.67626953125
        },
        {
            "corpus_id": "273098691",
            "title": "How Much Can RAG Help the Reasoning of LLM?",
            "text": "Retrieval-Augmented Generation (RAG) has gained significant popularity in modern Large Language Models (LLMs) due to its effectiveness in introducing new knowledge and reducing hallucinations. However, the deep understanding of RAG remains limited, how does RAG help the reasoning process and can RAG help improve the reasoning capability remains question. While external documents are typically considered as a method to incorporate domain-specific information, they also contain intermediate reasoning results related to the query, this suggests that documents could enhance the reasoning capability of LLMs, which has not been previously explored. In this paper, we investigate this issue in depth and find that while RAG can assist with reasoning, the help is limited. If we conceptualize the reasoning process as a tree with fixed depth, then RAG struggles to assist LLMs in performing deeper reasoning. Additionally, the information in the documents requires preprocessing to filter out noise. We demonstrate that this preprocessing is difficult to achieve simply fine-tuning of the LLM, it often necessitates numerous additional transformer layers to solve the problem. To simplify the problem, we propose DPrompt tuning, which effectively resolves the issue within just limited transformer layers, leading to improved performance.",
            "score": 0.5430167677913875,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.62255859375
        },
        {
            "corpus_id": "269005050",
            "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
            "text": "DPO obviates the need to train a reward model, the sample from the LLM during fine-tuning or conduct an extensive hyperparameter search.Figure 2 illustrates the pivotal procedures involved in RLHF and DPO.Retrieval Augmented Generation (RAG) has emerged as a favored paradigm for enabling Large Language Models (LLMs) to access external data, serving as a grounding mechanism to counter hallucinations.RAG models amalgamate pre-trained parametric and non-parametric memory for language generation.The parametric memory is a pre-trained seq2seq model, while the non-parametric memory is a dense vector index of Wikipedia accessed with a pre-trained neural retriever.With RAG, LLMs retrieve contextual documents from a database to enhance the accuracy of their responses.Frameworks such as LangChain, LlamaIndex, FastRAG, and others serve as orchestrators, connecting LLMs with tools, databases, memories, etc., thereby augmenting their capabilities.User instructions are not inherently optimized for retrieval.Various techniques, including multi-query retriever, HyDE, etc., can be employed to rephrase/expand them and enhance performance.Figure 3 elucidates the operational mechanism of the RAG.To recall previous instructions and responses, LLMs and chatbots like ChatGPT incorporate this history into their context window.This buffer can be enhanced with summarization (e.g., using a smaller LLM), a vector store + RAG, etc [71].Both the document retrieval (context precision and recall) and generation stages (faithfulness and answer relevancy) need to be evaluated.Tools like Ragas and DeepEval can simplify this process [72].\n\nAdvanced RAG techniques bolster retrieval techniques for RAG models and evaluate their performance using industry-standard metrics.Advanced RAG techniques systematize various approaches and provide a comprehensive examination of the progression of RAG paradigms.They encompass the Naive RAG, the Advanced RAG, and the Modular RAG.Pinecone's LLM Agent exemplifies an agent that can utilize tools like calculators, search, or executing code.Using agents, an LLM can write and execute Python code.It can search for information and even query a SQL database.",
            "score": 0.5429166331509734,
            "section_title": "The Art of Reasoning and Acting",
            "char_start_offset": 34328,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 136
                },
                {
                    "start": 136,
                    "end": 205
                },
                {
                    "start": 205,
                    "end": 402
                },
                {
                    "start": 402,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 665
                },
                {
                    "start": 665,
                    "end": 769
                },
                {
                    "start": 769,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1009
                },
                {
                    "start": 1009,
                    "end": 1138
                },
                {
                    "start": 1138,
                    "end": 1195
                },
                {
                    "start": 1195,
                    "end": 1324
                },
                {
                    "start": 1324,
                    "end": 1431
                },
                {
                    "start": 1431,
                    "end": 1569
                },
                {
                    "start": 1569,
                    "end": 1630
                },
                {
                    "start": 1632,
                    "end": 1763
                },
                {
                    "start": 1763,
                    "end": 1894
                },
                {
                    "start": 1894,
                    "end": 1962
                },
                {
                    "start": 1962,
                    "end": 2071
                },
                {
                    "start": 2071,
                    "end": 2126
                },
                {
                    "start": 2126,
                    "end": 2186
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.36669921875
        },
        {
            "corpus_id": "273638614",
            "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
            "text": "The primary aim of this work is to reduce irrelevance and hallucinations in the responses generated by Retrieval-Augmented Generation (RAG) systems through a novel, fine-grained filtering process that evaluates the relevance of each chunk of retrieved information before it is used in the response generation phase. Below, we describe the steps involved in a detailed, precise, and reproducible manner.",
            "score": 0.5429049944719765,
            "section_title": "Methodology",
            "char_start_offset": 7682,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 402
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.75146484375
        },
        {
            "corpus_id": "277349136",
            "title": "Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models",
            "text": "The application of generative AI to practical domains, including computer simulation modeling, is an emerging and rapidly evolving field of research. Our study has been designed to account for the following lessons and challenges identified in the existing literature: \n\n\u2022 User Expertise: The effectiveness of generative AI can depend on the expertise and skill level of the user. \n\n\u2022 Mitigating Hallucination: Generative AI models are prone to hallucination (producing incorrect or fabricated outputs). This risk can be reduced through strategies such as prompt engineering, retrieval-augmented generation (RAG), and iterative refinement during interactions. \n\n\u2022 Model Validation: AI-generated models require thorough testing and validation at each stage of development to ensure reliability and accuracy. \n\n\u2022 Selection of Test Data: The choice of test data is critical to avoid data leakage, which can compromise the validity of results in generative AI studies. \n\n\u2022 Model Complexity: Current research has predominantly focused on applying generative AI to relatively simple models, leaving its performance with more complex systems largely unexplored. \n\n4 Methods overview \n\nOur study followed four stages: setup and model design (Stage 0); prompt engineering and code generation (Stage 1); internal replication (Stage 2); and evaluation and preservation (Stage 3). Figure 1 illustrates these stages and the activities carried out in each. For model generation, we used Perplexity.AI's standard model (free tier) that includes RAG from internet sources. The RAG functionality provides the model with up-to-date and new online sources about simulation and SimPy.",
            "score": 0.5426419449046124,
            "section_title": "Summary of lessons from the literature",
            "char_start_offset": 23516,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 149
                },
                {
                    "start": 150,
                    "end": 268
                },
                {
                    "start": 271,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 503
                },
                {
                    "start": 504,
                    "end": 659
                },
                {
                    "start": 662,
                    "end": 806
                },
                {
                    "start": 809,
                    "end": 964
                },
                {
                    "start": 967,
                    "end": 1154
                },
                {
                    "start": 1157,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1556
                },
                {
                    "start": 1557,
                    "end": 1664
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.489990234375
        },
        {
            "corpus_id": "267027552",
            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
            "text": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, and we study a potentially disruptive application - what if we could provide location-specific insights to a farmer? Our results show the effectiveness of our dataset generation pipeline in capturing geographic-specific knowledge, and the quantitative and qualitative benefits of RAG and fine-tuning. We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further. In one particular experiment, we also demonstrate that the fine-tuned model leverages information from across geographies to answer specific questions, increasing answer similarity from 47% to 72%. Overall, the results point to how systems built using LLMs can be adapted to respond and incorporate knowledge across a dimension that is critical for a specific industry, paving the way for further applications of LLMs in other industrial domains.",
            "score": 0.5425869738792914,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.9619140625
        },
        {
            "corpus_id": "278165663",
            "title": "Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations",
            "text": "To address hallucination and improve the accuracy of AI-generated responses, Retrieval-Augmented Generation (RAG) has emerged as a robust solution. RAG enhances LLMs by incorporating an external knowledge retrieval process before generating responses. Instead of relying solely on pre-trained information, the model dynamically retrieves relevant data from structured and unstructured sources such as vector databases, document repositories, and enterprise knowledge bases. This approach ensures that AI responses are factually accurate and contextually relevant, reducing misinformation and increasing trust in AI-driven applications. By integrating retrieval-based mechanisms, RAG enables AI systems to provide verifiable, real-time insights, making them more reliable for enterprise and mission-critical use cases.",
            "score": 0.5425235314849921,
            "section_title": "Addressing Hallucination with RAG",
            "char_start_offset": 2251,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 635
                },
                {
                    "start": 636,
                    "end": 817
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.414794921875
        },
        {
            "corpus_id": "273963285",
            "title": "Qwen2.5-32B: Leveraging Self-Consistent Tool-Integrated Reasoning for Bengali Mathematical Olympiad Problem Solving",
            "text": "Initial experiments with Retrieval-Augmented Generation (RAG) aimed to enhance the model's responses by adding contextual information. However, RAG often introduced noise, leading to a decrease in performance. Removing RAG subsequently improved scores, indicating that the inherent capabilities of the larger Qwen models were sufficient without additional retrieved context for this task.",
            "score": 0.5418199205531851,
            "section_title": "C. RAG Limitations",
            "char_start_offset": 7990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 209
                },
                {
                    "start": 210,
                    "end": 388
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.426513671875
        },
        {
            "corpus_id": "277043297",
            "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
            "text": "With the ongoing advancements in the field of generative AI, specifically in RAG, numerous surveys have emerged. \n\nHowever, these surveys often focus on specific aspects of the field. They either concentrate solely on a single foundation of RAG or provide a concise overview of enhancement methods for RAG in limited scenarios. Most existing works emphasize text-related RAG tasks supported by large language models without delving into other modalities. The survey by Li et al. [131] offers a fundamental overview of RAG and discusses specific applications within the realm of text generation tasks. Recent surveys by Gao et al. and Fan et al. [63,73,94] explore RAG in the context of large language models, with a specific emphasis on query-oriented RAG enhancement methods. Wu et al. [246] delves into key technologies of RAG in the areas of retrieval, while simultaneously introducing its broad applications in natural language processing tasks. Several other works investigate a more general approach to RAG. Recent Works by Zhao et al. [282,283] extend RAG to multi-modal contexts, exploring its technologies and applications in broader AI-generated content (AIGC) scenarios. Another work by Peng et al. [184] examines how graph-structured information can aid more precise and comprehensive retrieval in RAG, enhancing relational knowledge acquisition and the generation of context-aware responses. In addition to concentrating on research pertaining to RAG technology, recent scholarship has increasingly directed its focus towards the assessment of RAG systems. A survey by Yu et al. [266] addresses the evaluation challenges Table 1. Comparison of Different RAG Surveys. LLM: whether the survey discusses RAG in the context of large language models; Multimodal: whether it covers multimodal RAG; Graph: whether it discusses graph-structured information in RAG; Advanced: the coverage of advanced RAG techniques; Evaluation: whether it addresses evaluation methods; Knowledge: whether it takes a knowledge-centric perspective.",
            "score": 0.5415910424302278,
            "section_title": "Related Surveys",
            "char_start_offset": 6672,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 115,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 600
                },
                {
                    "start": 601,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 949
                },
                {
                    "start": 950,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1181
                },
                {
                    "start": 1182,
                    "end": 1404
                },
                {
                    "start": 1405,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1679
                },
                {
                    "start": 1680,
                    "end": 2034
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.79736328125
        },
        {
            "corpus_id": "277621603",
            "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits",
            "text": "Currently, the generative AI landscape consists of a wide range of custom APIs tailored towards specific goals and targeted solutions. E.g., for retrieval augmented generation (RAG) alone, widely used solutions include Chroma, LangChain, Haystack, LlamaIndex, ChatGPT's retrieval plugin, Huggingface's retrieval plugin, and Azure's Machine Learning pipeline, to name name a few. Furthermore, the aforementioned RAG solutions may internally call several other generative AI APIs, differing based on the inference/LLMOps provider (e.g., OpenAI, Azure OpenAI, Together AI, and DataBricks for API endpoints or local models through Huggingface, Ollama, vLLM, etc.). Such recursive API calls are inherited by practitioners when developing their own specific applications, who in turn build their own custom APIs. Thus, while a large number of generative AI solutions exist, adapting such solutions for a particular use case requires significant developer time and effort due to the current ad-hoc state of generative AI APIs.",
            "score": 0.5414839567417209,
            "section_title": "Need for Standardized Generative AI APIs",
            "char_start_offset": 3924,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 135,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 660
                },
                {
                    "start": 661,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 1019
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0816650390625
        },
        {
            "corpus_id": "270845515",
            "title": "Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models",
            "text": "Retrieval-Augmented Generative (RAG) models (Chen et al., 2024;Gao et al., 2023;Lewis et al., 2020;Li et al., 2022Li et al., , 2024) ) represent a significant advancement in enhancing Large Language Models (LLMs) by dynamically retrieving information from external knowledge databases.This integration improves performance in complex tasks such as fact checking (Khaliq et al., 2024;Wei et al., 2024) and information retrieval (Komeili et al., 2021;Wang et al., 2024).Major search engines such as Google Search (Kaz Sato, 2024) and Bing (Heidi Steen, 2024) are increasingly looking to integrate RAG systems to elevate their perfor-* Equal contribution.",
            "score": 0.5412411067842993,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 285
                },
                {
                    "start": 285,
                    "end": 468
                },
                {
                    "start": 468,
                    "end": 652
                }
            ],
            "ref_mentions": [
                {
                    "start": 44,
                    "end": 63,
                    "matchedPaperCorpusId": "261530434"
                },
                {
                    "start": 80,
                    "end": 99,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.493896484375
        },
        {
            "corpus_id": "270620041",
            "title": "Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata",
            "text": "Large Language Models (LLMs) have shown remarkable language understanding and generation abilities [10,13]. However, there are two main challenges: static knowledge [8] and generative hallucination [5]. Retrieval-augmented generation [6] is an established process for answering user questions over entire datasets. RAG also helps mitigate generative hallucination and provides LLM with new information on which it was not trained [11]. Real-world RAG pipelines often need to retrieve evidence from multiple documents simultaneously, a procedure known as multi-hop querying. Nevertheless, existing RAG applications face challenges in answering multi-hop queries, requiring retrieval and reasoning over numerous pieces of evidence [12]. In this paper, we present Multi-Meta-RAG: an improved RAG using a database filtering approach with LLM-extracted metadata that significantly improves the results on the MultiHop-RAG benchmark.",
            "score": 0.5409507040452459,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 107
                },
                {
                    "start": 108,
                    "end": 202
                },
                {
                    "start": 203,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 927
                }
            ],
            "ref_mentions": [
                {
                    "start": 99,
                    "end": 103,
                    "matchedPaperCorpusId": "246426909"
                },
                {
                    "start": 234,
                    "end": 237,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 430,
                    "end": 434,
                    "matchedPaperCorpusId": "233240939"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.787109375
        },
        {
            "corpus_id": "271892298",
            "title": "Retrieval-augmented Few-shot Medical Image Segmentation with Foundation Models",
            "text": "Retrieval-Augmented Generation (RAG) in natural language processing combines the strengths of retrieval-based systems and generative models [19], [20]. RAG begins by retrieving relevant information from an external knowledge source based on the input query, providing contextual information that enhances the accuracy, informativeness, and contextual grounding of responses generated by large language models (LLMs). In the image domain, retrieval-augmented techniques have also been explored in various areas. For example, the Retrieval-Augmented Diffusion Model (RDM) was proposed for image synthesis, where the generative model is conditioned on informative samples retrieved from a database [34]. Similarly, Retrieval-Augmented Classification (RAC) integrates an explicit retrieval module into standard image classification pipelines to improve long-tail visual recognition [35]. Additionally, REtrieval-Augmented CusTomization (REACT) leverages relevant image-text pairs from a vast web-scale database to customize visual models for specific target domains, achieving significant performance across tasks while minimizing the need for extensive retraining [36]. In this work, we extend the retrieval strategy to the image segmentation task, designing a retrieval module that queries an external database for contextual and anatomical information, which is then used to guide the SAM 2 model in few-shot medical image segmentation without requiring any retraining.",
            "score": 0.5402747319668253,
            "section_title": "B. Retrieval-augmented Techniques",
            "char_start_offset": 8509,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1166
                },
                {
                    "start": 1167,
                    "end": 1468
                }
            ],
            "ref_mentions": [
                {
                    "start": 140,
                    "end": 144,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 695,
                    "end": 699,
                    "matchedPaperCorpusId": "248377386"
                },
                {
                    "start": 878,
                    "end": 882,
                    "matchedPaperCorpusId": "247058346"
                },
                {
                    "start": 1161,
                    "end": 1165,
                    "matchedPaperCorpusId": "255942790"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.845703125
        },
        {
            "corpus_id": "268510182",
            "title": "Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A Case Study on Domain-Specific Queries in Private Knowledge-Bases",
            "text": "We proposed an end-to-end system design towards utilizing Retrieval Augmented Generation (RAG) to improve the factual accuracy of Large Language Models (LLMs) for domain-specific and time-sensitive queries related to private knowledge-bases. Our system integrates RAG pipeline with upstream datasets processing and downstream performance evaluation. Addressing the challenge of LLM hallucinations, we finetune models with a curated dataset which originates from CMU's extensive resources and annotated with the teacher model. Our experiments demonstrate the system's effectiveness in generating more accurate answers to domain-specific and time-sensitive inquiries. The results also revealed the limitations of fine-tuning LLMs with small-scale and skewed datasets. This research highlights the potential of RAG systems in augmenting LLMs with external datasets for improved performance in knowledge-intensive tasks. Our code and models are available on Github.",
            "score": 0.53869276801255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.86328125
        },
        {
            "corpus_id": "273346023",
            "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
            "text": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
            "score": 0.5381896053819161,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.923828125
        },
        {
            "corpus_id": "278339615",
            "title": "Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing",
            "text": "Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance. This paper aims to address these limitations by introducing a new framework, \\textbf{SIM-RAG}, to explicitly enhance RAG systems' self-awareness and multi-round retrieval capabilities. To train SIM-RAG, we first let a RAG system self-practice multi-round retrieval, augmenting existing question-answer pairs with intermediate inner monologue reasoning steps to generate synthetic training data. For each pair, the system may explore multiple retrieval paths, which are labeled as successful if they reach the correct answer and unsuccessful otherwise. Using this data, we train a lightweight information sufficiency Critic. At inference time, the Critic evaluates whether the RAG system has retrieved sufficient information at each round, guiding retrieval decisions and improving system-level self-awareness through in-context reinforcement learning. Experiments across multiple prominent RAG benchmarks show that SIM-RAG is an effective multi-round RAG solution. Furthermore, this framework is system-efficient, adding a lightweight component to RAG without requiring modifications to existing LLMs or search engines, and data-efficient, eliminating the need for costly human-annotated mid-step retrieval process supervision data.",
            "score": 0.5377504580340813,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.76953125
        },
        {
            "corpus_id": "260704453",
            "title": "SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool",
            "text": "Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI performance while maintaining privacy standards. The tool is available at https://github.com/RCGAI/SimplyRetrieve with an MIT license.",
            "score": 0.5375338393170885,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.378173828125
        },
        {
            "corpus_id": "269282856",
            "title": "Generating Test Scenarios from NL Requirements Using Retrieval-Augmented LLMs: An Industrial Study",
            "text": "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines the capabilities of two major NLP techniques: information retrieval (IR) and generative LLMs.Generative LLMs like GPTx models [19] have revolutionized NLP domain.LLMs trained on vast datasets generate coherent and contextually relevant text.However, as discussed in Section I, LLMs' ability to generate accurate and specific information is often constrained by their training data scope and inherent limitations in accessing external, up-to-date domain information.Traditional IR systems, designed to search and retrieve information from large databases, provide the ability to access specific information.However, they lack LLMs' NL understanding and generation capabilities.RAG address these limitations by combining the generative capabilities of LLMs with IR efficiency.When a query is presented to a RAG model, the retrieval system fetches relevant documents or snippets.The retrieved snippets are fed into the generative model, which integrates them into its response generation process.This allows the model to produce responses that are not only contextually rich but also factually accurate.Next, we provide details on the RAG pipeline, visualized in Fig. 2. Indexing.In the RAG pipeline, the indexation process begins with the domain documentation loading, cleansing, and extracting the content.The documents can be loaded in different file formats, e.g., PDF, HTML, and Microsoft Word, and converted into standardized plain text.Thereafter, the text is split into smaller and more manageable passages or chunks (e.g., 512 tokens) to fit within the context limits of different LMs.These passages are subsequently transformed into vector representations through an embedding model, e.g., Sentence Bert (SBERT) [20].These embeddings are numerical representations that encapsulate the semantic content of the passage.An index is created to store the text passages and their vector embeddings as key-value pairs, which allows for retrieving contextually relevant data efficiently and accurately.These key-value pairs can be stored in a vector database to avoid re-indexing the documentation each time.A prompt for the RAG model is indexed using the same embedding model and passed onto the next step in the pipeline.Querying.",
            "score": 0.5373445747217385,
            "section_title": "C. Retrieval Augmented Generation (RAG)",
            "char_start_offset": 13536,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 165
                },
                {
                    "start": 165,
                    "end": 234
                },
                {
                    "start": 234,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 537
                },
                {
                    "start": 537,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 748
                },
                {
                    "start": 748,
                    "end": 846
                },
                {
                    "start": 846,
                    "end": 948
                },
                {
                    "start": 948,
                    "end": 1065
                },
                {
                    "start": 1065,
                    "end": 1172
                },
                {
                    "start": 1172,
                    "end": 1249
                },
                {
                    "start": 1249,
                    "end": 1377
                },
                {
                    "start": 1377,
                    "end": 1512
                },
                {
                    "start": 1512,
                    "end": 1663
                },
                {
                    "start": 1663,
                    "end": 1796
                },
                {
                    "start": 1796,
                    "end": 1896
                },
                {
                    "start": 1896,
                    "end": 2073
                },
                {
                    "start": 2073,
                    "end": 2179
                },
                {
                    "start": 2179,
                    "end": 2294
                },
                {
                    "start": 2294,
                    "end": 2303
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.51416015625
        },
        {
            "corpus_id": "269484332",
            "title": "Towards a Search Engine for Machines: Unified Ranking for Multiple Retrieval-Augmented Large Language Models",
            "text": "The vast majority of machine learning systems, including large generative models, are designed as self-contained systems, with both knowledge and reasoning encoded in model parameters.However, these models cannot work effectively for tasks that require knowledge grounding [1,48], especially in case of non-stationary Figure 1: A high-level overview of the uRAG ecosystem.The ecosystem consists of a shared search engine that serves multiple RAG models, each performing its own task.\n\ndata where new information is actively being produced [49,57].As suggested by Zamani et al. [57], this issue can be addressed when machine learning systems are being enhanced with the capability of retrieving stored content.For example, in retrieval-augmented generation (RAG), as a special case of retrieval-enhanced machine learning (REML) [57], systems consume the responses provided by a retrieval model for the purpose of text generation [25,26].RAG models demonstrate substantial promise across various applications, including open-domain question answering [6,20,25,44,60], fact verification [46], dialogue systems [8,45,53], machine translation [4], and personalized generation [37,38].\n\nIn the RAG literature, the retrieval component is often implemented using either of the following two approaches:\n\n(1) Employing an off-the-shelf retrieval model that does not require training for the downstream RAG system: in this category, RAG systems either use APIs from commercial web search engines [29], term matching retrieval models [11], such as TF-IDF and BM25, or neural ranking models trained on relevance annotations provided as an external resource [25]; (2) Training a retrieval model given the feedback from the downstream RAG system through knowledge distillation [15] or endto-end optimization [35].\n\nAs expected, the latter category offers the current state-of-theart performance for various tasks [17,52].From an IR perspective, in this category, the downstream RAG model is the only \"user\" of the search engine.",
            "score": 0.5371721348377524,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 184,
                    "end": 372
                },
                {
                    "start": 372,
                    "end": 483
                },
                {
                    "start": 485,
                    "end": 547
                },
                {
                    "start": 547,
                    "end": 709
                },
                {
                    "start": 709,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1179
                },
                {
                    "start": 1181,
                    "end": 1294
                },
                {
                    "start": 1296,
                    "end": 1799
                },
                {
                    "start": 1801,
                    "end": 1907
                },
                {
                    "start": 1907,
                    "end": 2014
                }
            ],
            "ref_mentions": [
                {
                    "start": 543,
                    "end": 546,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 577,
                    "end": 581,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 827,
                    "end": 831,
                    "matchedPaperCorpusId": "248506020"
                },
                {
                    "start": 928,
                    "end": 932,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1049,
                    "end": 1052,
                    "matchedPaperCorpusId": "230437698"
                },
                {
                    "start": 1052,
                    "end": 1055,
                    "matchedPaperCorpusId": "215737187"
                },
                {
                    "start": 1055,
                    "end": 1058,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1058,
                    "end": 1061,
                    "matchedPaperCorpusId": "252735056"
                },
                {
                    "start": 1084,
                    "end": 1088,
                    "matchedPaperCorpusId": "4711425"
                },
                {
                    "start": 1107,
                    "end": 1110,
                    "matchedPaperCorpusId": "53218829"
                },
                {
                    "start": 1110,
                    "end": 1113,
                    "matchedPaperCorpusId": "51879945"
                },
                {
                    "start": 1113,
                    "end": 1116,
                    "matchedPaperCorpusId": "52006529"
                },
                {
                    "start": 1138,
                    "end": 1141,
                    "matchedPaperCorpusId": "235166182"
                },
                {
                    "start": 1171,
                    "end": 1175,
                    "matchedPaperCorpusId": "269009728"
                },
                {
                    "start": 1523,
                    "end": 1527,
                    "matchedPaperCorpusId": "250391085"
                },
                {
                    "start": 1645,
                    "end": 1649,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 1794,
                    "end": 1798,
                    "matchedPaperCorpusId": "230437591"
                },
                {
                    "start": 1899,
                    "end": 1903,
                    "matchedPaperCorpusId": "251371732"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6845703125
        },
        {
            "corpus_id": "273186680",
            "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
            "text": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks.",
            "score": 0.5367619902863696,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.96728515625
        },
        {
            "corpus_id": "272689561",
            "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
            "text": "RAG is proposed to enhance generation quality by leveraging external knowledge bases. As research progresses, RAG technology has undergone three major developmental stages: Naive RAG, Advanced RAG, and Modular RAG. \n\nNaive RAG. Typically, naive RAG follows a \"Retrievalthen-Read\" process [21,26,27], consisting of a simple retriever and a pre-trained language model as the generator. Its workflow involves two simple steps: (1) retrieving relevant passages from a pre-constructed knowledge base based on the user query, and (2) combining the retrieved information with the input query to generate a response. \n\nEarly works primarily focused on optimizing the integration of retrievers and generators, including end-to-end joint training of retrievers and generators [21,26], separately training generators to better utilize retrieved documents with frozen retrievers [3,28], and modifying the model's decoding methods [10,29]. With the emergence of LLMs, the capabilities of generative models have significantly advanced. To further enhance the quality of generated context, prompt engineering have been proposed to optimize model outputs without additional training. To enhance the model's reasoning capabilities and the robustness of responses, various prompting techniques such as Chainof-Thought (CoT) [30], Tree-of-Thought (ToT) [31], and Self-Consistency [32] have been proposed. These methods extend the number of LLM's reasoning paths, thereby improving the likelihood of arriving at the correct result during the decoding process. However, Naive RAG also faces certain limitations. Firstly, the retrieved documents may contain noise or irrelevant information, which can interfere with the model's responses [5,33]. Secondly, the high reasoning cost inherent to large models is further exacerbated in the RAG process; the inclusion of lengthy retrieved documents can slow down the generation process and consume more computational resources. \n\nAdvanced RAG. To tackle the issues discussed earlier, additional components have been added to the RAG process, making it more complex. These enhanced systems, known as Advanced RAG, introduce specialized modules at different stages of the retrieval and generation pipeline, which can be categorized as pre-retrieval and post-retrieval components.",
            "score": 0.536753705974925,
            "section_title": "Retrieval-augmented Generation System",
            "char_start_offset": 6549,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 85
                },
                {
                    "start": 86,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 608
                },
                {
                    "start": 611,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1539
                },
                {
                    "start": 1540,
                    "end": 1590
                },
                {
                    "start": 1591,
                    "end": 1723
                },
                {
                    "start": 1724,
                    "end": 1949
                },
                {
                    "start": 1952,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2087
                },
                {
                    "start": 2088,
                    "end": 2299
                }
            ],
            "ref_mentions": [
                {
                    "start": 288,
                    "end": 292,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 295,
                    "end": 298,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 766,
                    "end": 770,
                    "matchedPaperCorpusId": "251371732"
                },
                {
                    "start": 867,
                    "end": 870,
                    "matchedPaperCorpusId": "220302360"
                },
                {
                    "start": 870,
                    "end": 873,
                    "matchedPaperCorpusId": "244954723"
                },
                {
                    "start": 918,
                    "end": 922,
                    "matchedPaperCorpusId": "256389797"
                },
                {
                    "start": 922,
                    "end": 925,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 1716,
                    "end": 1719,
                    "matchedPaperCorpusId": "256662612"
                },
                {
                    "start": 1719,
                    "end": 1722,
                    "matchedPaperCorpusId": "256459776"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.71728515625
        },
        {
            "corpus_id": "271097348",
            "title": "Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting",
            "text": "Retrieval Augmented Generation Retrieval Augmented Generation (RAG) enhances LLMs by retrieving relevant documents from external databases and incorporating them into the generation process (Gao et al., 2023b;Lewis et al., 2020;Khandelwal et al., 2020;Izacard & Grave, 2021;Luo et al., 2023a;Xia et al., 2024b;Wang et al., 2024). Recent work has primarily focused on enabling LLMs to understand when and what to retrieve (Ma et al., 2023;Chen et al., 2023b;Jiang et al., 2023b;Schick et al., 2024), or designing approaches to better utilize contexts (Yu et al., 2023;Yoran et al., 2023;Wang et al., 2023b;Sarthi et al., 2024;Baek et al., 2023;Xu et al., 2023;Kim et al., 2024). Among them, SAIL (Luo et al., 2023a) fine-tunes a pre-trained LLM on web search data to filter irrelevant contents. Self-Reflective RAG (Asai et al., 2023) introduces reflection tokens to guide retrieval and annotation in instruction-tuning datasets. However, both approaches require additional instruction-tuning of generic LLMs, which is resource-intensive and may lead to forgetting or over-fitting (Luo et al., 2023b). Furthermore, long context with retrieved documents can suffer from computational inefficiency and position bias (Liu et al., 2024). Corrective RAG (Yan et al., 2024) on the other hand proposes a lightweight retrieval evaluator, but it lacks the capability for high-level reasoning. In contrast, our proposed SPECULATIVE RAG addresses these limitations by leveraging a smaller RAG drafter model to efficiently understand diverse perspectives in retrieval results and generate drafts for the generalist LMs to verify and integrate.",
            "score": 0.5365731620352698,
            "section_title": "RELATED WORKS",
            "char_start_offset": 4900,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 329
                },
                {
                    "start": 330,
                    "end": 677
                },
                {
                    "start": 678,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1100
                },
                {
                    "start": 1101,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1382
                },
                {
                    "start": 1383,
                    "end": 1630
                }
            ],
            "ref_mentions": [
                {
                    "start": 209,
                    "end": 228,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 228,
                    "end": 252,
                    "matchedPaperCorpusId": "207870430"
                },
                {
                    "start": 457,
                    "end": 477,
                    "matchedPaperCorpusId": "15641339"
                },
                {
                    "start": 477,
                    "end": 497,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 625,
                    "end": 643,
                    "matchedPaperCorpusId": "264306280"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.70849609375
        },
        {
            "corpus_id": "275336185",
            "title": "Foundations of GenIR",
            "text": "Retrieval Augmented Generation, or RAG, refers to the process of augmenting LLMs with data retrieved from external collections or synthesizing multiple retrieval results with LLMs for downstream applications [84,85]. While the popularity of RAG rose after the release of large-scale pre-trained language models such as GPT [1] and BART [86], relevant topics and techniques have already been studied for at least more than two decades in both IR and NLP communities, e.g., extractive and abstractive summarization that generates summary based on retrieved sentences [87,88] or answer extraction from top retrieved document [89]. A major reason why RAG-like techniques were not as attractive as they are today is the limited performance of generative models before the era of LLMs. After ChatGPT [1] demonstrated superior ability text generation at the end of 2022, there have been many studies and surveys on RAG and its applications in LLMs [84,90,91]. As the intent of this chapter is not to provide yet another survey on existing RAG papers, we focus the following discussions on several present and future directions for RAG and their relations underneath.",
            "score": 0.5365496479153259,
            "section_title": "Retrieval Augmented Generation",
            "char_start_offset": 29413,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 216
                },
                {
                    "start": 217,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 779
                },
                {
                    "start": 780,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1159
                }
            ],
            "ref_mentions": [
                {
                    "start": 212,
                    "end": 215,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 336,
                    "end": 340,
                    "matchedPaperCorpusId": "204960716"
                },
                {
                    "start": 565,
                    "end": 569,
                    "matchedPaperCorpusId": "29980788"
                },
                {
                    "start": 569,
                    "end": 572,
                    "matchedPaperCorpusId": "198189946"
                },
                {
                    "start": 948,
                    "end": 951,
                    "matchedPaperCorpusId": "263866951"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.669921875
        },
        {
            "corpus_id": "275788867",
            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
            "text": "Retrieval augmented generation (RAG) models are a subset of large language models (LLMs) which combine the generation capabilities of conventional LLMs with the factual grounding of information retrieval (IR) models to create more factually accurate outputs from LLMs (Lewis et al., 2020). RAG models work by taking a user question as input, and then selecting several reference texts with high semantic similarity (determined by an IR model) from a database. An LLM is then given these texts with the original question and is instructed to answer the question basing the answer on the relevant reference texts. \n\nRAG not only allows for more accurate answers to questions regarding general public knowledge (Guu et al., 2020;Ram et al., 2023), it also allows LLMs to generate responses based on locally available or domain specific information that it has not necessarily been trained upon (Gao et al., 2023;Zhang et al., 2024). \n\nHowever, the models that have exhibited the highest performance in RAG tasks are based on proprietary cloud-based LLMs, meaning that LLMs run locally are more likely to generate hallucinations or other untruthful outputs when being used for RAG (Hughes et al., 2023). Moreover, LLMs that are not trained using data from a specific domain exhibit lower RAG accuracy in that domain (Zhang et al., 2024). \n\nTo address this, we propose a framework called Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG). ALoF-TRAG improves the accuracy of base RAG systems by automatically training on the data which the system will later be used, all without using larger models or labelled data. \n\nWe demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model. \n\nOur findings inform the future implementation of RAG systems, allowing users to fine-tune their RAG models on local data using modest hardware, enabling improved RAG accuracy while preserving data security.",
            "score": 0.5364287834315216,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 929
                },
                {
                    "start": 932,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1333
                },
                {
                    "start": 1336,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1640
                },
                {
                    "start": 1643,
                    "end": 1838
                },
                {
                    "start": 1839,
                    "end": 2002
                },
                {
                    "start": 2005,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 708,
                    "end": 726,
                    "matchedPaperCorpusId": "211204736"
                },
                {
                    "start": 726,
                    "end": 743,
                    "matchedPaperCorpusId": "256459451"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.93798828125
        },
        {
            "corpus_id": "276107364",
            "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) combines retrieval and generation to enhance language models in knowledge-intensive tasks. Early RAG systems [57] retrieve context documents relevant to a query and use these as input for generation, enabling models to answer queries beyond their training data. More recent approaches explore the integration of retrieval with both encoder-decoder [41] and decoder-only [87] architectures, optimizing performance for a variety of downstream tasks. \n\nOne key area of focus in RAG is mitigating hallucinations and improving the factuality of generated responses [69]. Techniques such as self-consistency [113] and noisy context filtering [28] have been proposed to enhance the reliability of RAG outputs. However, the quality of retrieved documents remains a critical factor in the overall effectiveness of RAG pipelines. Recent works emphasize the importance of combining retrieval with robust re-ranking techniques to ensure that the retrieved context is both relevant and precise [45].",
            "score": 0.5363789541679107,
            "section_title": "Retrieval-Augmented Generation (RAG)",
            "char_start_offset": 8442,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 298
                },
                {
                    "start": 299,
                    "end": 484
                },
                {
                    "start": 487,
                    "end": 602
                },
                {
                    "start": 603,
                    "end": 739
                },
                {
                    "start": 740,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1023
                }
            ],
            "ref_mentions": [
                {
                    "start": 146,
                    "end": 150,
                    "matchedPaperCorpusId": "218869575"
                },
                {
                    "start": 407,
                    "end": 411,
                    "matchedPaperCorpusId": "204838007"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.84228515625
        },
        {
            "corpus_id": "278339140",
            "title": "RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection via Retrieval-Augmented Generation",
            "text": "Retrieval-Augmented Generation (RAG) first combined parametric LLMs with non-parametric memory in a dense vector index, retrieving relevant passages at inference time to improve knowledge-intensive tasks [6]. Subsequent work has extended RAG to broad NLP paradigms, including modular and advanced RAG variants that dynamically adapt retrieval per token or per query [4]. RAG's decoupling of memory access and generation inspires our MCP-RAG approach, wherein MCP discovery is treated as a retrieval subproblem, orthogonal to core text generation.",
            "score": 0.5363789541679107,
            "section_title": "Retrieval-Augmented Generation",
            "char_start_offset": 10534,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 370
                },
                {
                    "start": 371,
                    "end": 546
                }
            ],
            "ref_mentions": [
                {
                    "start": 204,
                    "end": 207,
                    "matchedPaperCorpusId": "218869575"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.6630859375
        },
        {
            "corpus_id": "270870251",
            "title": "Searching for Best Practices in Retrieval-Augmented Generation",
            "text": "In this study, we aim to identify optimal practices for implementing retrieval-augmented generation in order to improve the quality and reliability of content produced by large language models.We systematically assessed a range of potential solutions for each module within the RAG framework and recommended the most effective approach for each module.Furthermore, we introduced a comprehensive evaluation benchmark for RAG systems and conducted extensive experiments to determine the best practices among various alternatives.Our findings not only contribute to a deeper understanding of retrieval-augmented generation systems but also establish a foundation for future research.",
            "score": 0.5363789541679107,
            "section_title": "Conclusion",
            "char_start_offset": 32122,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 193
                },
                {
                    "start": 193,
                    "end": 352
                },
                {
                    "start": 352,
                    "end": 527
                },
                {
                    "start": 527,
                    "end": 680
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.87841796875
        },
        {
            "paperId": "1b6c568f3b8c10f4f887d2f0487c7c0ad46093e4",
            "corpusId": 268510325,
            "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 10,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2403.09727, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2284077584",
                    "name": "R\u00f3bert Lakatos"
                },
                {
                    "authorId": "2279978930",
                    "name": "P. Pollner"
                },
                {
                    "authorId": "2260742529",
                    "name": "Andr\u00e1s Hajdu"
                },
                {
                    "authorId": "2243157601",
                    "name": "Tam\u00e1s Jo\u00f3"
                }
            ],
            "abstract": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
            "corpus_id": "268510325",
            "text": "Generative large language models (LLMs) have revolutionized the development of knowledge-based systems, enabling new possibilities in applications like ChatGPT, Bing, and Gemini. Two key strategies for domain adaptation in these systems are Domain-Specific Fine-Tuning (DFT) and Retrieval-Augmented Generation (RAG). In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation. Furthermore, we propose a simplified RAG-based architecture that maximizes efficiency and reduces hallucination, underscoring the advantages of RAG in building reliable, domain-adapted knowledge systems.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9658203125
        },
        {
            "paperId": "4656dad20e50c9cdf9dd226cb7d6b5954181f435",
            "corpusId": 274560162,
            "title": "Exploring Data Analysis Methods in Generative Models: From Fine-Tuning to RAG Implementation",
            "venue": "De Computis",
            "year": 2024,
            "referenceCount": 22,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/computers13120327?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/computers13120327, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2334171585",
                    "name": "Bogdan Mihai Gu\u021bu"
                },
                {
                    "authorId": "2329400025",
                    "name": "Nirvana Popescu"
                }
            ],
            "abstract": "The exponential growth in data from technological advancements has created opportunities across fields like healthcare, finance, and social media, but sensitive data raise security and privacy challenges. Generative models offer solutions by modeling complex data and generating synthetic data, making them useful for the analysis of large private datasets. This article is a review of data analysis techniques based on generative models, with a focus on large language models (LLMs). It covers the strengths, limitations, and applications of methods like the fine-tuning of LLMs and retrieval-augmented generation (RAG). This study consolidates, analyzes, and interprets the findings from the literature to provide a coherent overview of the current research landscape on this topic, aiming to guide effective, privacy-conscious data analysis and exploring future improvements, especially for low-resource languages.",
            "corpus_id": "274560162",
            "text": "The exponential growth in data from technological advancements has created opportunities across fields like healthcare, finance, and social media, but sensitive data raise security and privacy challenges. Generative models offer solutions by modeling complex data and generating synthetic data, making them useful for the analysis of large private datasets. This article is a review of data analysis techniques based on generative models, with a focus on large language models (LLMs). It covers the strengths, limitations, and applications of methods like the fine-tuning of LLMs and retrieval-augmented generation (RAG). This study consolidates, analyzes, and interprets the findings from the literature to provide a coherent overview of the current research landscape on this topic, aiming to guide effective, privacy-conscious data analysis and exploring future improvements, especially for low-resource languages.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7333984375
        },
        {
            "paperId": "79dcc5a39e79bd52119867f9644c3f1dbf38d2c5",
            "corpusId": 271088607,
            "title": "FACTS About Building Retrieval Augmented Generation-based Chatbots",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 16,
            "citationCount": 9,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2407.07858, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2305730480",
                    "name": "Rama Akkiraju"
                },
                {
                    "authorId": "2305675294",
                    "name": "Anbang Xu"
                },
                {
                    "authorId": "2310607259",
                    "name": "Deepak Bora"
                },
                {
                    "authorId": "2310671061",
                    "name": "Tan Yu"
                },
                {
                    "authorId": "2310609093",
                    "name": "Lu An"
                },
                {
                    "authorId": "2310609420",
                    "name": "Vishal Seth"
                },
                {
                    "authorId": "2305719307",
                    "name": "Aaditya Shukla"
                },
                {
                    "authorId": "1814422",
                    "name": "Pritam Gundecha"
                },
                {
                    "authorId": "2305730388",
                    "name": "Hridhay Mehta"
                },
                {
                    "authorId": "2310607670",
                    "name": "Ashwin Jha"
                },
                {
                    "authorId": "2310611084",
                    "name": "Prithvi Raj"
                },
                {
                    "authorId": "2310606332",
                    "name": "Abhinav Balasubramanian"
                },
                {
                    "authorId": "2310606434",
                    "name": "Murali Maram"
                },
                {
                    "authorId": "2310609411",
                    "name": "Guru Muthusamy"
                },
                {
                    "authorId": "2310609135",
                    "name": "Shivakesh Reddy Annepally"
                },
                {
                    "authorId": "2310610818",
                    "name": "Sidney Knowles"
                },
                {
                    "authorId": "2310608922",
                    "name": "Min Du"
                },
                {
                    "authorId": "2310607012",
                    "name": "Nick Burnett"
                },
                {
                    "authorId": "2310609533",
                    "name": "Sean Javiya"
                },
                {
                    "authorId": "2079090752",
                    "name": "Ashok Marannan"
                },
                {
                    "authorId": "2306407244",
                    "name": "Mamta Kumari"
                },
                {
                    "authorId": "2362583216",
                    "name": "Surbhi Jha"
                },
                {
                    "authorId": "2310610853",
                    "name": "Ethan Dereszenski"
                },
                {
                    "authorId": "2310610395",
                    "name": "Anupam Chakraborty"
                },
                {
                    "authorId": "2266083803",
                    "name": "Subhash Ranjan"
                },
                {
                    "authorId": "2310606269",
                    "name": "Amina Terfai"
                },
                {
                    "authorId": "2310606897",
                    "name": "Anoop Surya"
                },
                {
                    "authorId": "2310608340",
                    "name": "Tracey Mercer"
                },
                {
                    "authorId": "2310606272",
                    "name": "Vinodh Kumar Thanigachalam"
                },
                {
                    "authorId": "2310608308",
                    "name": "Tamar Bar"
                },
                {
                    "authorId": "2310609349",
                    "name": "Sanjana Krishnan"
                },
                {
                    "authorId": "2310609408",
                    "name": "Samy Kilaru"
                },
                {
                    "authorId": "2310607747",
                    "name": "Jasmine Jaksic"
                },
                {
                    "authorId": "2310606963",
                    "name": "Nave Algarici"
                },
                {
                    "authorId": "2310607177",
                    "name": "Jacob Liberman"
                },
                {
                    "authorId": "2310611450",
                    "name": "Joey Conway"
                },
                {
                    "authorId": "2310609824",
                    "name": "Sonu Nayyar"
                },
                {
                    "authorId": "2310606984",
                    "name": "Justin Boitano"
                }
            ],
            "abstract": "Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.\"",
            "corpus_id": "271088607",
            "text": "Enterprise chatbots, powered by generative AI, are emerging as key applications to enhance employee productivity. Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and orchestration frameworks like Langchain and Llamaindex are crucial for building these chatbots. However, creating effective enterprise chatbots is challenging and requires meticulous RAG pipeline engineering. This includes fine-tuning embeddings and LLMs, extracting documents from vector databases, rephrasing queries, reranking results, designing prompts, honoring document access controls, providing concise responses, including references, safeguarding personal information, and building orchestration agents. We present a framework for building RAG-based chatbots based on our experience with three NVIDIA chatbots: for IT/HR benefits, financial earnings, and general content. Our contributions are three-fold: introducing the FACTS framework (Freshness, Architectures, Cost, Testing, Security), presenting fifteen RAG pipeline control points, and providing empirical results on accuracy-latency tradeoffs between large and small LLMs. To the best of our knowledge, this is the first paper of its kind that provides a holistic view of the factors as well as solutions for building secure enterprise-grade chatbots.\"",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.400390625
        },
        {
            "paperId": "d806450ebb2f7da0a6accc20ff1931076103da49",
            "corpusId": 278327165,
            "title": "Real-time Spatial Retrieval Augmented Generation for Urban Environments",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 44,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2505.02271, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2359147657",
                    "name": "David Nazareno Campo"
                },
                {
                    "authorId": "2230852635",
                    "name": "Javier Conde"
                },
                {
                    "authorId": "2284338709",
                    "name": "\u00c1lvaro Alonso"
                },
                {
                    "authorId": "1856832",
                    "name": "G. Huecas"
                },
                {
                    "authorId": "2359145585",
                    "name": "Joaqu'in Salvach'ua"
                },
                {
                    "authorId": "2243081043",
                    "name": "Pedro Reviriego"
                }
            ],
            "abstract": "The proliferation of Generative Artificial Ingelligence (AI), especially Large Language Models, presents transformative opportunities for urban applications through Urban Foundation Models. However, base models face limitations, as they only contain the knowledge available at the time of training, and updating them is both time-consuming and costly. Retrieval Augmented Generation (RAG) has emerged in the literature as the preferred approach for injecting contextual information into Foundation Models. It prevails over techniques such as fine-tuning, which are less effective in dynamic, real-time scenarios like those found in urban environments. However, traditional RAG architectures, based on semantic databases, knowledge graphs, structured data, or AI-powered web searches, do not fully meet the demands of urban contexts. Urban environments are complex systems characterized by large volumes of interconnected data, frequent updates, real-time processing requirements, security needs, and strong links to the physical world. This work proposes a real-time spatial RAG architecture that defines the necessary components for the effective integration of generative AI into cities, leveraging temporal and spatial filtering capabilities through linked data. The proposed architecture is implemented using FIWARE, an ecosystem of software components to develop smart city solutions and digital twins. The design and implementation are demonstrated through the use case of a tourism assistant in the city of Madrid. The use case serves to validate the correct integration of Foundation Models through the proposed RAG architecture.",
            "corpus_id": "278327165",
            "text": "The proliferation of Generative Artificial Ingelligence (AI), especially Large Language Models, presents transformative opportunities for urban applications through Urban Foundation Models. However, base models face limitations, as they only contain the knowledge available at the time of training, and updating them is both time-consuming and costly. Retrieval Augmented Generation (RAG) has emerged in the literature as the preferred approach for injecting contextual information into Foundation Models. It prevails over techniques such as fine-tuning, which are less effective in dynamic, real-time scenarios like those found in urban environments. However, traditional RAG architectures, based on semantic databases, knowledge graphs, structured data, or AI-powered web searches, do not fully meet the demands of urban contexts. Urban environments are complex systems characterized by large volumes of interconnected data, frequent updates, real-time processing requirements, security needs, and strong links to the physical world. This work proposes a real-time spatial RAG architecture that defines the necessary components for the effective integration of generative AI into cities, leveraging temporal and spatial filtering capabilities through linked data. The proposed architecture is implemented using FIWARE, an ecosystem of software components to develop smart city solutions and digital twins. The design and implementation are demonstrated through the use case of a tourism assistant in the city of Madrid. The use case serves to validate the correct integration of Foundation Models through the proposed RAG architecture.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.81005859375
        },
        {
            "paperId": "09022e0e75fa472e9a1f6b743223c016a5ec35e2",
            "corpusId": 273566432,
            "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
            "venue": "Machine Learning and Knowledge Extraction",
            "year": 2024,
            "referenceCount": 11,
            "citationCount": 12,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3390/make6040116?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3390/make6040116, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327432431",
                    "name": "Arunabh Bora"
                },
                {
                    "authorId": "1806041",
                    "name": "H. Cuay\u00e1huitl"
                }
            ],
            "abstract": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
            "corpus_id": "273566432",
            "text": "Artificial Intelligence (AI) has the potential to revolutionise the medical and healthcare sectors. AI and related technologies could significantly address some supply-and-demand challenges in the healthcare system, such as medical AI assistants, chatbots and robots. This paper focuses on tailoring LLMs to medical data utilising a Retrieval-Augmented Generation (RAG) database to evaluate their performance in a computationally resource-constrained environment. Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning. Open-source LLMs (Flan-T5-Large, LLaMA-2-7B, and Mistral-7B) are fine-tuned using the medical datasets Meadow-MedQA and MedMCQA. Experiments are reported for response generation and multiple-choice question answering. The latter uses two distinct methodologies: Type A, as standard question answering via direct choice selection; and Type B, as language generation and probability confidence score generation of choices available. Results in the medical domain revealed that Fine-tuning and RAG are crucial for improved performance, and that methodology Type A outperforms Type B.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.9541015625
        },
        {
            "paperId": "3871a472268c806dedd1793178aaf7f9dc4f65fc",
            "corpusId": 273549218,
            "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models",
            "venue": "International Conference on AI-ML-Systems",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "http://arxiv.org/pdf/2410.18344",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2410.18344, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2327775708",
                    "name": "Fengchen Liu"
                },
                {
                    "authorId": "2327510830",
                    "name": "Jordan Jung"
                },
                {
                    "authorId": "2327335509",
                    "name": "Wei Feinstein"
                },
                {
                    "authorId": "2327336236",
                    "name": "Jeff DAmbrogia"
                },
                {
                    "authorId": "2327334401",
                    "name": "Gary Jung"
                }
            ],
            "abstract": "This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.",
            "corpus_id": "273549218",
            "text": "This paper introduces a novel approach to enhancing closed-domain Question Answering (QA) systems, focusing on the specific needs of the Lawrence Berkeley National Laboratory (LBL) Science Information Technology (ScienceIT) domain. Utilizing a rich dataset derived from the ScienceIT documentation, our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models. Through data processing techniques, we transform the documentation into structured context-question-answer triples, leveraging the latest Large Language Models (AWS Bedrock, GCP PaLM2, Meta LLaMA2, OpenAI GPT-4, Google Gemini-Pro) for data-driven insights. Additionally, we introduce the Aggregated Knowledge Model (AKM), which synthesizes responses from the seven models mentioned above using K-means clustering to select the most representative answers. The evaluation of these models across multiple metrics offers a comprehensive look into their effectiveness and suitability for the LBL ScienceIT environment. The results demonstrate the potential benefits of integrating fine-tuning and retrieval-augmented strategies, highlighting significant performance improvements achieved with the AKM. The insights gained from this study can be applied to develop specialized QA systems tailored to specific domains.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.93505859375
        },
        {
            "paperId": "670efdd79bd73251443c72a87bf03048357bb098",
            "corpusId": 278555129,
            "title": "Conversational AI - ADAM: A Solution to Enhance Well Engineering Decision-Making with Retrieval-Augmented Generation (RAG)",
            "venue": "SPE Conference at Oman Petroleum &amp; Energy Show",
            "year": 2025,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2118/224972-ms?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2118/224972-ms, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2360753122",
                    "name": "Sajad Al Lawati"
                },
                {
                    "authorId": "2360815984",
                    "name": "Tariq Al Sulaimani"
                },
                {
                    "authorId": "2360810570",
                    "name": "Al Salt Al Bahri"
                }
            ],
            "abstract": "\n The Adam project revolutionizes information retrieval and decision-making at PDO by deploying a secure, on-premise AI system using Retrieval-Augmented Generation (RAG). Designed for the oil and gas industry, Adam leverages Large Language Models (LLMs) to deliver real-time, precise insights, enhancing efficiency and reducing reliance on manual document searches. Adam integrates RAG technology, combining real-time data retrieval with LLMs trained on PDO's proprietary datasets. Deployed on-premise for security compliance, the system was fine-tuned in collaboration with internal teams and AI specialists to address oil and gas-specific challenges. By automating information retrieval and structuring domain-specific data, Adam improves decision-making across the organization. It has significantly enhanced information retrieval and decision-making efficiency, accelerating workflows and productivity, with projected annual savings of 2 FullTime Employees (FTE) and through reduced manual labor and optimized data management. Initially benefiting hundreds of employees, Adam's expansion aims to support thousands across PDO, offering real-time, context-driven insights that impact operations at all levels. Additionally, Adam enhances PDO's Health, Safety, and Environment (HSE) standards by improving access to critical safety and technical documentation. The system has significantly improved information retrieval and decision-making efficiency at PDO, automating complex searches to accelerate workflows and enhance productivity. With its ability to provide context-aware responses, Adam supports decision-making at all operational levels, ensuring safety and compliance with industry regulations.",
            "corpus_id": "278555129",
            "text": "\n The Adam project revolutionizes information retrieval and decision-making at PDO by deploying a secure, on-premise AI system using Retrieval-Augmented Generation (RAG). Designed for the oil and gas industry, Adam leverages Large Language Models (LLMs) to deliver real-time, precise insights, enhancing efficiency and reducing reliance on manual document searches. Adam integrates RAG technology, combining real-time data retrieval with LLMs trained on PDO's proprietary datasets. Deployed on-premise for security compliance, the system was fine-tuned in collaboration with internal teams and AI specialists to address oil and gas-specific challenges. By automating information retrieval and structuring domain-specific data, Adam improves decision-making across the organization. It has significantly enhanced information retrieval and decision-making efficiency, accelerating workflows and productivity, with projected annual savings of 2 FullTime Employees (FTE) and through reduced manual labor and optimized data management. Initially benefiting hundreds of employees, Adam's expansion aims to support thousands across PDO, offering real-time, context-driven insights that impact operations at all levels. Additionally, Adam enhances PDO's Health, Safety, and Environment (HSE) standards by improving access to critical safety and technical documentation. The system has significantly improved information retrieval and decision-making efficiency at PDO, automating complex searches to accelerate workflows and enhance productivity. With its ability to provide context-aware responses, Adam supports decision-making at all operational levels, ensuring safety and compliance with industry regulations.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2427978515625
        },
        {
            "paperId": "4060f651ed012e26b4424a9b59157f60534f6051",
            "corpusId": 271478872,
            "title": "Customising generative AI: Harnessing document retrieval and fine-tuning alternatives for dynamic marketing insights",
            "venue": "Applied marketing analytics",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.69554/ybxq5617?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.69554/ybxq5617, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2313217778",
                    "name": "Dakota Crisp"
                },
                {
                    "authorId": "2313217748",
                    "name": "Jacob Newsted"
                },
                {
                    "authorId": "2313217794",
                    "name": "Brendon Kirouac"
                },
                {
                    "authorId": "2313222529",
                    "name": "Danielle Barnes"
                },
                {
                    "authorId": "2313125600",
                    "name": "Catherine Hayes"
                },
                {
                    "authorId": "1394484535",
                    "name": "Jonathan Prantner"
                }
            ],
            "abstract": "This study delves into the transformative impact of leveraging large language models (LLMs) in marketing analytics, particularly emphasising a paradigm shift from fine-tuning models to the strategic application of document retrieval techniques and more. Focusing on innovative methods, such as retrieval augmented generation and low-rank adaptation, the paper explores how marketers can now activate against vast and unstructured datasets, such as call centre transcripts, unlocking valuable insights that were previously overlooked. By harnessing the power of document retrieval and adaptation, marketers can bring their data to life, enabling a more nuanced and adaptive approach to understanding consumer behaviour and preferences. This research contributes to the evolving landscape of applied marketing analytics by demonstrating the efficacy of document retrieval in enhancing the utilisation of LLMs for dynamic and data-driven marketing strategies.",
            "corpus_id": "271478872",
            "text": "This study delves into the transformative impact of leveraging large language models (LLMs) in marketing analytics, particularly emphasising a paradigm shift from fine-tuning models to the strategic application of document retrieval techniques and more. Focusing on innovative methods, such as retrieval augmented generation and low-rank adaptation, the paper explores how marketers can now activate against vast and unstructured datasets, such as call centre transcripts, unlocking valuable insights that were previously overlooked. By harnessing the power of document retrieval and adaptation, marketers can bring their data to life, enabling a more nuanced and adaptive approach to understanding consumer behaviour and preferences. This research contributes to the evolving landscape of applied marketing analytics by demonstrating the efficacy of document retrieval in enhancing the utilisation of LLMs for dynamic and data-driven marketing strategies.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.63623046875
        },
        {
            "paperId": "71472a18c682d2b937a6d838da347131bb920270",
            "corpusId": 270067650,
            "title": "Video-Enriched Retrieval Augmented Generation Using Aligned Video Captions",
            "venue": "The Web Conference",
            "year": 2024,
            "referenceCount": 12,
            "citationCount": 5,
            "influentialCitationCount": 1,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2405.17706, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2362920541",
                    "name": "Kevin Dela Rosa"
                }
            ],
            "abstract": "In this work, we propose the use of ''aligned video captions'' as an intelligent mechanism for integrating video content into retrieval augmented generation (RAG) based AI assistant systems. These captions serve as an efficient representation layer between videos and large language models (LLMs), describing both visual and audio content while requiring significantly less context window space compared to traditional frame sampling approaches. We demonstrate how this representation enables more effective agent-based retrieval and generation capabilities, with captions that can be dynamically adapted through targeted prompting or fine-tuning of the underlying models. Our empirical evaluation across multiple LLM configurations shows that this approach achieves comparable performance to direct video processing while being more computationally efficient and easier to reason about in downstream tasks. Notably, the approach shows particular strength in procedural content like How-To videos, where aligned captions significantly outperform speech-only baselines.",
            "corpus_id": "270067650",
            "text": "In this work, we propose the use of ''aligned video captions'' as an intelligent mechanism for integrating video content into retrieval augmented generation (RAG) based AI assistant systems. These captions serve as an efficient representation layer between videos and large language models (LLMs), describing both visual and audio content while requiring significantly less context window space compared to traditional frame sampling approaches. We demonstrate how this representation enables more effective agent-based retrieval and generation capabilities, with captions that can be dynamically adapted through targeted prompting or fine-tuning of the underlying models. Our empirical evaluation across multiple LLM configurations shows that this approach achieves comparable performance to direct video processing while being more computationally efficient and easier to reason about in downstream tasks. Notably, the approach shows particular strength in procedural content like How-To videos, where aligned captions significantly outperform speech-only baselines.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.7685546875
        },
        {
            "paperId": "78f02a5c5aad19af3f7cae2ead40569882760380",
            "corpusId": 275586237,
            "title": "Adversarial Training of Retrieval Augmented Generation to Generate Believable Fake News",
            "venue": "BigData Congress [Services Society]",
            "year": 2024,
            "referenceCount": 30,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/BigData62323.2024.10825933?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/BigData62323.2024.10825933, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2268427987",
                    "name": "Sonali Singh"
                },
                {
                    "authorId": "2336007",
                    "name": "A. Namin"
                }
            ],
            "abstract": "Recent advancements in Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Natural Language Understanding (NLU), showcasing their ability to produce coherent and contextually relevant responses. However, their widespread use raises serious concerns regarding the potential for generating and spreading false or misleading information. This research examines the effectiveness of customized Retrieval Augmented Generation (RAG) models, alongside fine-tuned versions of GPT-Neo and RoBERTa.The proposed framework leverages multiple generative language models, including GPT-Neo, RoBERTa, and a custom RAG model, to produce diverse fake news content grounded in retrieved contextual information. It employs a combination of large language models and a specialized fake news detection pipeline, which integrates embedding based retrieval with sentence transformers and Facebook AI Similarity Search (FAISS), while also enabling generation through GPT-Neo, RoBERTa, and custom RAG structures. Additionally, we employ a passive aggressive classifier trained on \"Fake\" and \"Real\" dataset from a public GitHub repository to assess the likelihood of generated responses being classified as \"Fake\" or \"Real.\" This pipeline evaluates the authenticity of news articles and incorporates believability scores to enhance interpretability.Results indicate that while all models perform comparably, the custom RAG model consistently excels in providing contextually grounded and highly relevant fake information,in these fake news scenarios. This study highlights the robustness of retrieval augmented frameworks in adversarial tasks, offering superior alignment with factual references. It contributes to the AI driven misinformation detection landscape, providing valuable insights into model selection and training methodologies to combat social engineering and the spread of fake news.",
            "corpus_id": "275586237",
            "text": "Recent advancements in Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Natural Language Understanding (NLU), showcasing their ability to produce coherent and contextually relevant responses. However, their widespread use raises serious concerns regarding the potential for generating and spreading false or misleading information. This research examines the effectiveness of customized Retrieval Augmented Generation (RAG) models, alongside fine-tuned versions of GPT-Neo and RoBERTa.The proposed framework leverages multiple generative language models, including GPT-Neo, RoBERTa, and a custom RAG model, to produce diverse fake news content grounded in retrieved contextual information. It employs a combination of large language models and a specialized fake news detection pipeline, which integrates embedding based retrieval with sentence transformers and Facebook AI Similarity Search (FAISS), while also enabling generation through GPT-Neo, RoBERTa, and custom RAG structures. Additionally, we employ a passive aggressive classifier trained on \"Fake\" and \"Real\" dataset from a public GitHub repository to assess the likelihood of generated responses being classified as \"Fake\" or \"Real.\" This pipeline evaluates the authenticity of news articles and incorporates believability scores to enhance interpretability.Results indicate that while all models perform comparably, the custom RAG model consistently excels in providing contextually grounded and highly relevant fake information,in these fake news scenarios. This study highlights the robustness of retrieval augmented frameworks in adversarial tasks, offering superior alignment with factual references. It contributes to the AI driven misinformation detection landscape, providing valuable insights into model selection and training methodologies to combat social engineering and the spread of fake news.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8125
        },
        {
            "paperId": "02a10b16775ba18c00f841fdede9155adb6fdadd",
            "corpusId": 273913535,
            "title": "Advanced Chunking and Search Methods for Improved Retrieval-Augmented Generation (RAG) System Performance in E-Learning",
            "venue": "AHFE International",
            "year": 2024,
            "referenceCount": 0,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.54941/ahfe1005756?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.54941/ahfe1005756, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2329819048",
                    "name": "Daniel Danter"
                },
                {
                    "authorId": "2329816818",
                    "name": "Heidrun M\u00fchle"
                },
                {
                    "authorId": "2243998024",
                    "name": "Andreas St\u00f6ckl"
                }
            ],
            "abstract": "Our study evaluates different search methodologies\u2014Hybrid Search and Semantic Search\u2014within a Retrieval-Augmented Generation (RAG) framework specifically for E-Learning. The primary objective is to enhance the accuracy and efficiency of using Large Language Models (LLMs), such as GPT-4, by employing advanced Prompt Engineering Techniques in E-Learning environments. Efficient search and chunking methods are critical for optimizing the quality of answers provided by these systems.To achieve this, we utilized the RAGas testing framework, focusing on performance parameters including Answer Correctness, Context Recall, Context Precision, Faithfulness, and Answer Relevancy. In our implementation, documents were divided into text chunks and indexed in a database using both vector and keyword indexing. This allowed for searches by vectors for similar records and keyword searches for exact matches. These records were then incorporated into prompts as context to improve LLM responses. The AI model used for generating embeddings, such as OpenAI's text-embedding-ada-002, plays a crucial role in this process by creating high-dimensional representations that capture deep semantic meanings.Current retrieval methods, like keyword and similarity-based searches, often fall short due to limitations in chunk quality, which directly impacts the accuracy of the RAG system. This study aims to improve the retriever component and, consequently, the overall accuracy of the RAG system by comparing three different chunking methods and two search approaches. We conducted tests using 57 questions across multiple files under various configurations.This research examines different search methods, including Hybrid Search, which integrates traditional keyword search with semantic search in order to provide more accurate and contextually relevant results. In comparison, Semantic Search utilizes deep learning models to comprehend the context and meaning of search queries and documents, thereby providing more precise information retrieval. The analysis also compared different chunking methods, such as Recursive Chunking, which divides text into hierarchical sections that are further subdivided until the desired granularity is reached. BERT Chunking utilizes the BERT model to segment text, taking semantic meaning into account to ensure coherent chunks. Token Chunking segments text based on individual tokens, offering fine-grained control over segmentation.Our results, evaluated using the RAGas testing framework, highlight the strengths and weaknesses of each search method and chunking technique. This study provides valuable insights into optimizing RAG Systems for E-Learning through advanced Prompt Engineering Techniques, aiming to improve knowledge transfer regarding efficiency and accuracy.",
            "corpus_id": "273913535",
            "text": "Our study evaluates different search methodologies\u2014Hybrid Search and Semantic Search\u2014within a Retrieval-Augmented Generation (RAG) framework specifically for E-Learning. The primary objective is to enhance the accuracy and efficiency of using Large Language Models (LLMs), such as GPT-4, by employing advanced Prompt Engineering Techniques in E-Learning environments. Efficient search and chunking methods are critical for optimizing the quality of answers provided by these systems.To achieve this, we utilized the RAGas testing framework, focusing on performance parameters including Answer Correctness, Context Recall, Context Precision, Faithfulness, and Answer Relevancy. In our implementation, documents were divided into text chunks and indexed in a database using both vector and keyword indexing. This allowed for searches by vectors for similar records and keyword searches for exact matches. These records were then incorporated into prompts as context to improve LLM responses. The AI model used for generating embeddings, such as OpenAI's text-embedding-ada-002, plays a crucial role in this process by creating high-dimensional representations that capture deep semantic meanings.Current retrieval methods, like keyword and similarity-based searches, often fall short due to limitations in chunk quality, which directly impacts the accuracy of the RAG system. This study aims to improve the retriever component and, consequently, the overall accuracy of the RAG system by comparing three different chunking methods and two search approaches. We conducted tests using 57 questions across multiple files under various configurations.This research examines different search methods, including Hybrid Search, which integrates traditional keyword search with semantic search in order to provide more accurate and contextually relevant results. In comparison, Semantic Search utilizes deep learning models to comprehend the context and meaning of search queries and documents, thereby providing more precise information retrieval. The analysis also compared different chunking methods, such as Recursive Chunking, which divides text into hierarchical sections that are further subdivided until the desired granularity is reached. BERT Chunking utilizes the BERT model to segment text, taking semantic meaning into account to ensure coherent chunks. Token Chunking segments text based on individual tokens, offering fine-grained control over segmentation.Our results, evaluated using the RAGas testing framework, highlight the strengths and weaknesses of each search method and chunking technique. This study provides valuable insights into optimizing RAG Systems for E-Learning through advanced Prompt Engineering Techniques, aiming to improve knowledge transfer regarding efficiency and accuracy.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.8876953125
        },
        {
            "paperId": "f716a18b462826004899010dfc30947f9c01ef90",
            "corpusId": 276743362,
            "title": "RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models",
            "venue": "North American Chapter of the Association for Computational Linguistics",
            "year": 2025,
            "referenceCount": 98,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2504.18041, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2320837964",
                    "name": "Shiyue Zhang"
                },
                {
                    "authorId": "1478928280",
                    "name": "Mark Dredze"
                },
                {
                    "authorId": "2348274137",
                    "name": "AI Bloomberg"
                },
                {
                    "authorId": "2347353861",
                    "name": "Marinka Zitnik"
                },
                {
                    "authorId": "2279159644",
                    "name": "Meng Jiang"
                },
                {
                    "authorId": "2289005170",
                    "name": "Mohit Bansal"
                },
                {
                    "authorId": "2287863492",
                    "name": "James Zou"
                },
                {
                    "authorId": "2228505567",
                    "name": "Jian Pei"
                },
                {
                    "authorId": "2320145734",
                    "name": "Jian Liu"
                },
                {
                    "authorId": "2288643686",
                    "name": "Jianfeng Gao"
                },
                {
                    "authorId": "2285893376",
                    "name": "Jiawei Han"
                },
                {
                    "authorId": "2309202279",
                    "name": "Jieyu Zhao"
                },
                {
                    "authorId": "2279062891",
                    "name": "Jiliang Tang"
                },
                {
                    "authorId": "2324891509",
                    "name": "Jindong Wang"
                },
                {
                    "authorId": "2281975152",
                    "name": "Joaquin Vanschoren"
                },
                {
                    "authorId": "2319671705",
                    "name": "John C. Mitchell"
                },
                {
                    "authorId": "2348273681",
                    "name": "Kai Shu"
                },
                {
                    "authorId": "2348317878",
                    "name": "Kaidi Xu"
                },
                {
                    "authorId": "2289604896",
                    "name": "Kai-Wei Chang"
                },
                {
                    "authorId": "2348517655",
                    "name": "Lifang He"
                },
                {
                    "authorId": "2319173963",
                    "name": "Lifu Huang"
                },
                {
                    "authorId": "2347147641",
                    "name": "Michael Backes"
                },
                {
                    "authorId": "2347575422",
                    "name": "Aaron Hurst"
                },
                {
                    "authorId": "1977806",
                    "name": "Adam Lerer"
                },
                {
                    "authorId": "1964382",
                    "name": "Adam P. Goucher"
                },
                {
                    "authorId": "2275245529",
                    "name": "Adam Perelman"
                },
                {
                    "authorId": "2261024614",
                    "name": "Aditya Ramesh"
                },
                {
                    "authorId": "2293772267",
                    "name": "Aidan Clark"
                },
                {
                    "authorId": "2348273516",
                    "name": "AJ Os-trow"
                },
                {
                    "authorId": "2275245663",
                    "name": "Akila Welihinda"
                },
                {
                    "authorId": "2347575420",
                    "name": "Alan Hayes"
                },
                {
                    "authorId": "38909097",
                    "name": "Alec Radford"
                },
                {
                    "authorId": "41016275",
                    "name": "Alon Jacovi"
                },
                {
                    "authorId": "2336876301",
                    "name": "Andrew Wang"
                },
                {
                    "authorId": "114577307",
                    "name": "Chris Alberti"
                },
                {
                    "authorId": "2218112465",
                    "name": "Connie Tao"
                },
                {
                    "authorId": "2336819290",
                    "name": "Jon Lipovetz"
                },
                {
                    "authorId": "2275180557",
                    "name": "Kate Olszewska"
                },
                {
                    "authorId": "2336820230",
                    "name": "Lukas Haas"
                },
                {
                    "authorId": "2336829596",
                    "name": "Michelle Liu"
                },
                {
                    "authorId": "2336820056",
                    "name": "Nate Keating"
                },
                {
                    "authorId": "2728564",
                    "name": "Adam Bloniarz"
                },
                {
                    "authorId": "2275186597",
                    "name": "Carl Saroufim"
                },
                {
                    "authorId": "2336816667",
                    "name": "Corey Fry"
                },
                {
                    "authorId": "2307471502",
                    "name": "Dror Marcus"
                },
                {
                    "authorId": "2771709",
                    "name": "Doron Kukliansky"
                },
                {
                    "authorId": "2348274256",
                    "name": "Gau-rav Singh Tomar"
                },
                {
                    "authorId": "2336819438",
                    "name": "James Swirhun"
                },
                {
                    "authorId": "2260046021",
                    "name": "J. Xing"
                },
                {
                    "authorId": "2336829198",
                    "name": "Lily Wang"
                },
                {
                    "authorId": "2275187445",
                    "name": "Madhu Gurumurthy"
                },
                {
                    "authorId": "2336819607",
                    "name": "Michael Aaron"
                },
                {
                    "authorId": "2311700543",
                    "name": "Moran Ambar"
                },
                {
                    "authorId": "2203789378",
                    "name": "Rachana Fellinger"
                },
                {
                    "authorId": "2337079313",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "2336277273",
                    "name": "Zizhao Zhang"
                },
                {
                    "authorId": "35540270",
                    "name": "S. Goldshtein"
                },
                {
                    "authorId": "2348305792",
                    "name": "Dipanjan Das. 2025"
                },
                {
                    "authorId": "2204647912",
                    "name": "Neel Jain"
                },
                {
                    "authorId": "102604362",
                    "name": "Avi Schwarzschild"
                },
                {
                    "authorId": "123191916",
                    "name": "Yuxin Wen"
                },
                {
                    "authorId": "2306783462",
                    "name": "Gowthami Somepalli"
                },
                {
                    "authorId": "2237425585",
                    "name": "John Kirchenbauer"
                },
                {
                    "authorId": "143923090",
                    "name": "Ping-yeh Chiang"
                },
                {
                    "authorId": "121592562",
                    "name": "Micah Goldblum"
                },
                {
                    "authorId": "2056290221",
                    "name": "Aniruddha Saha"
                },
                {
                    "authorId": "2284863781",
                    "name": "Jonas Geiping"
                },
                {
                    "authorId": "2348276774",
                    "name": "Tom Goldstein. 2023"
                },
                {
                    "authorId": "2273548793",
                    "name": "Jiaming Ji"
                },
                {
                    "authorId": "2210950163",
                    "name": "Mickel Liu"
                },
                {
                    "authorId": "2260610683",
                    "name": "Josef Dai"
                },
                {
                    "authorId": "2190800297",
                    "name": "Xuehai Pan"
                },
                {
                    "authorId": "2347919703",
                    "name": "Chi Zhang"
                },
                {
                    "authorId": "14548852",
                    "name": "Juntao Dai"
                },
                {
                    "authorId": "2263267720",
                    "name": "Tianyi Qiu"
                },
                {
                    "authorId": "2222521695",
                    "name": "Bo Chen"
                },
                {
                    "authorId": "152705071",
                    "name": "Borong Zhang"
                },
                {
                    "authorId": "2263298625",
                    "name": "Hantao Lou"
                },
                {
                    "authorId": "2263734134",
                    "name": "Kaile Wang"
                },
                {
                    "authorId": "2341723291",
                    "name": "Ya Duan"
                }
            ],
            "abstract": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.",
            "corpus_id": "276743362",
            "text": "Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.904296875
        },
        {
            "paperId": "b3876204758c54f63c64cd82d5b21b7f4983736d",
            "corpusId": 260165124,
            "title": "Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering",
            "venue": "arXiv.org",
            "year": 2023,
            "referenceCount": 31,
            "citationCount": 1,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://arxiv.org/pdf/2307.05915",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2307.05915, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "1734786937",
                    "name": "C. Krishna"
                }
            ],
            "abstract": "We propose a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation (RAG) model to the target domain using supervised fine-tuning and reinforcement learning with synthetic feedback in a few-shot setting. This, we hypothesize, will yield an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The framework's synthetic generation pipeline will generate synthetic training data comprisingtuples using an open-source LLM and a novel consistency filtering scheme. The pipeline will be designed to generate both abstractive and extractive questions that span the entire corpus. The framework proposes to fine-tune a smaller RAG model comprising a dense retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In parallel, the framework will train a Reward model to score domain grounded answers higher than hallucinated answers using an a priori relevance ordering of synthetically assembled samples. In the next phase, the framework will align the RAG model with the target domain using reinforcement learning (Proximal Policy Optimization). This step may improve the RAG model's ability to generate grounded answers and ignore out of domain questions. In the final phase, the framework will calibrate the model's uncertainty for extractive question-answers.",
            "corpus_id": "260165124",
            "text": "We propose a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation (RAG) model to the target domain using supervised fine-tuning and reinforcement learning with synthetic feedback in a few-shot setting. This, we hypothesize, will yield an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The framework's synthetic generation pipeline will generate synthetic training data comprisingtuples using an open-source LLM and a novel consistency filtering scheme. The pipeline will be designed to generate both abstractive and extractive questions that span the entire corpus. The framework proposes to fine-tune a smaller RAG model comprising a dense retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In parallel, the framework will train a Reward model to score domain grounded answers higher than hallucinated answers using an a priori relevance ordering of synthetically assembled samples. In the next phase, the framework will align the RAG model with the target domain using reinforcement learning (Proximal Policy Optimization). This step may improve the RAG model's ability to generate grounded answers and ignore out of domain questions. In the final phase, the framework will calibrate the model's uncertainty for extractive question-answers.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.85107421875
        },
        {
            "paperId": "46b26abc9db27c72a17420d346c86c23ccb5f970",
            "corpusId": 278096418,
            "title": "Tailoring Large Language Models for Drilling Applications: A Comparative Study of Retrieval-Augmented Generation and Fine-Tuning",
            "venue": "SPE Western Regional Meeting",
            "year": 2025,
            "referenceCount": 9,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": "CLOSED",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.2118/224128-ms?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.2118/224128-ms, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2357517547",
                    "name": "Odai A. Elyas"
                },
                {
                    "authorId": "2357517375",
                    "name": "Hassan W. Al Hashim"
                },
                {
                    "authorId": "2357867008",
                    "name": "John R. Williams"
                }
            ],
            "abstract": "\n The Fourth Industrial Revolution (4IR) is defined by its pure data-driven nature, leading to advancements in computing, sensors, and machine learning to enhance operational efficiency through data-driven approaches. However, a significant amount of data remains underutilized due to its complex and unstructured formats. It is estimated that 50% of large enterprises manage at least 5 Petabytes (PB) of data, with 80% being unstructured (Mcdowell, 2023). Some examples of these datasets in the drilling industry are derived from daily operation remarks, engineering programs, and domain literature, all containing valuable insights.\n While significant effort is being made in the drilling industry through Artificial Intelligence (AI) systems, these models often depend on data gathered from over 80,000 sensors deployed on drilling platforms (Patel, 2024), limiting them to quantitative, single-modality datasets. Emerging technologies in Natural Language Processing (NLP), and particularly Large Language Models (LLMs) offer promising solutions to processing unstructured data, thereby unlocking additional insights from untapped resources. Nonetheless, developing domain-specific language models from scratch is a challenging endeavor. For reference, it is estimated that the smaller 175 billion parameters GPT-3 was trained using 1024 Graphics Processing Units (GPUs) for 34 days (Narayanan et al., 2021). While the larger 1.8 trillion parameters GPT-4 was trained with approximately 25,000 GPUs over 90 days (Treiber, 2023).\n This paper demonstrates the potential of adapting current Generative Pre-trained Transformer models (GPT) for drilling applications. The first objective is to improve drilling knowledge of base GPT models by analyzing the effectiveness of Retrieval Augmented Generation (RAG), and Low-Rank Adaptive (LoRA) fine-tuning. The second objective is to utilize LoRA fine-tuning for Lost Time Incident (LTI) prediction using a curated dataset from operational reports. The approaches highlighted in this paper will showcase how LLMs can be tailored to the drilling domain for various tasks, cost-effectively and without extensive computational resources.",
            "corpus_id": "278096418",
            "text": "\n The Fourth Industrial Revolution (4IR) is defined by its pure data-driven nature, leading to advancements in computing, sensors, and machine learning to enhance operational efficiency through data-driven approaches. However, a significant amount of data remains underutilized due to its complex and unstructured formats. It is estimated that 50% of large enterprises manage at least 5 Petabytes (PB) of data, with 80% being unstructured (Mcdowell, 2023). Some examples of these datasets in the drilling industry are derived from daily operation remarks, engineering programs, and domain literature, all containing valuable insights.\n While significant effort is being made in the drilling industry through Artificial Intelligence (AI) systems, these models often depend on data gathered from over 80,000 sensors deployed on drilling platforms (Patel, 2024), limiting them to quantitative, single-modality datasets. Emerging technologies in Natural Language Processing (NLP), and particularly Large Language Models (LLMs) offer promising solutions to processing unstructured data, thereby unlocking additional insights from untapped resources. Nonetheless, developing domain-specific language models from scratch is a challenging endeavor. For reference, it is estimated that the smaller 175 billion parameters GPT-3 was trained using 1024 Graphics Processing Units (GPUs) for 34 days (Narayanan et al., 2021). While the larger 1.8 trillion parameters GPT-4 was trained with approximately 25,000 GPUs over 90 days (Treiber, 2023).\n This paper demonstrates the potential of adapting current Generative Pre-trained Transformer models (GPT) for drilling applications. The first objective is to improve drilling knowledge of base GPT models by analyzing the effectiveness of Retrieval Augmented Generation (RAG), and Low-Rank Adaptive (LoRA) fine-tuning. The second objective is to utilize LoRA fine-tuning for Lost Time Incident (LTI) prediction using a curated dataset from operational reports. The approaches highlighted in this paper will showcase how LLMs can be tailored to the drilling domain for various tasks, cost-effectively and without extensive computational resources.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.603515625
        }
    ],
    "quotes": {
        "cost": 0.20925000000000002,
        "quotes": [
            {
                "idx": 0,
                "key": "[264288947 | Asai et al. | 2023 | Citations: 780]",
                "snippets": "Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020)(Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 32,
                        "end": 535,
                        "sentence_offsets": [
                            {
                                "start": 32,
                                "end": 296
                            },
                            {
                                "start": 297,
                                "end": 535
                            }
                        ],
                        "ref_mentions": [
                            "211204736",
                            "218869575"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020)(Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b)."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[267027552 | Balaguer et al. | 2024 | Citations: 91]",
                "snippets": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4...We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 576,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4"
                    },
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 1299,
                        "end": 1446,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further."
                    }
                ]
            },
            {
                "idx": 2,
                "key": "[267061013 | Elgedawy et al. | 2024 | Citations: 6]",
                "snippets": "In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval Augmented Generation (RAG) in Various Contexts",
                        "pdf_hash": "",
                        "start": 1005,
                        "end": 1196,
                        "sentence_offsets": [
                            {
                                "start": 1005,
                                "end": 1196
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning."
                    }
                ]
            },
            {
                "idx": 3,
                "key": "[267412954 | Zhang et al. | 2024 | Citations: 20]",
                "snippets": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 4,
                "key": "[268248396 | Soudani et al. | 2024 | Citations: 37]",
                "snippets": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 942,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge."
                    }
                ]
            },
            {
                "idx": 5,
                "key": "[268510325 | Lakatos et al. | 2024 | Citations: 10]",
                "snippets": "In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 6,
                "key": "[268819923 | Eibich et al. | 2024 | Citations: 4]",
                "snippets": "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 756,
                        "end": 1171,
                        "sentence_offsets": [
                            {
                                "start": 756,
                                "end": 1033
                            },
                            {
                                "start": 1033,
                                "end": 1171
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing."
                    }
                ]
            },
            {
                "idx": 7,
                "key": "[269043117 | Sawarkar et al. | 2024 | Citations: 57]",
                "snippets": "We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 639,
                        "end": 817,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."
                    }
                ]
            },
            {
                "idx": 8,
                "key": "[269149041 | Weng | 2024 | Citations: 10]",
                "snippets": "\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[252735056 | Siriwardhana et al. | 2022 | Citations: 179]": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."
                },
                "metadata": [
                    {
                        "section_title": "XIII. RAG-MEMORY-FINETUNING",
                        "pdf_hash": "",
                        "start": 1261,
                        "end": 1435,
                        "sentence_offsets": [
                            {
                                "start": 1181,
                                "end": 1547
                            }
                        ],
                        "ref_mentions": [
                            "252735056"
                        ],
                        "quote": "\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"
                    }
                ]
            },
            {
                "idx": 9,
                "key": "[269283058 | Jin et al. | 2024 | Citations: 47]",
                "snippets": "Recent work [1](Borgeaud et al., 2021)22,(Lewis et al., 2020)37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                },
                "metadata": [
                    {
                        "section_title": "Background",
                        "pdf_hash": "",
                        "start": 610,
                        "end": 778,
                        "sentence_offsets": [
                            {
                                "start": 610,
                                "end": 778
                            }
                        ],
                        "ref_mentions": [
                            "244954723",
                            "218869575"
                        ],
                        "quote": "Recent work [1](Borgeaud et al., 2021)22,(Lewis et al., 2020)37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models."
                    }
                ]
            },
            {
                "idx": 10,
                "key": "[269292881 | Efeoglu et al. | 2024 | Citations: 9]",
                "snippets": "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 768,
                        "end": 1031,
                        "sentence_offsets": [
                            {
                                "start": 768,
                                "end": 1031
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."
                    }
                ]
            },
            {
                "idx": 11,
                "key": "[270123034 | Zhu et al. | 2024 | Citations: 6]",
                "snippets": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 448,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."
                    }
                ]
            },
            {
                "idx": 12,
                "key": "[270560495 | Barnett et al. | 2024 | Citations: 2]",
                "snippets": "This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 801,
                        "end": 1441,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI."
                    }
                ]
            },
            {
                "idx": 13,
                "key": "[270560505 | Balakrishnan et al. | 2024 | Citations: 14]",
                "snippets": "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "INTRODUCTION",
                        "pdf_hash": "",
                        "start": 1246,
                        "end": 1414,
                        "sentence_offsets": [
                            {
                                "start": 1040,
                                "end": 1414
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."
                    }
                ]
            },
            {
                "idx": 14,
                "key": "[270688152 | Lyu et al. | 2024 | Citations: 11]",
                "snippets": "Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 719,
                        "end": 1292,
                        "sentence_offsets": [
                            {
                                "start": 719,
                                "end": 834
                            },
                            {
                                "start": 835,
                                "end": 1165
                            },
                            {
                                "start": 1166,
                                "end": 1292
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts."
                    }
                ]
            },
            {
                "idx": 15,
                "key": "[271039066 | Ficek et al. | 2024 | Citations: 1]",
                "snippets": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT.",
                        "pdf_hash": "",
                        "section_title": "abstract"
                    }
                ]
            },
            {
                "idx": 16,
                "key": "[271329121 | Zhao et al. | 2024 | Citations: 4]",
                "snippets": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[207870430 | Khandelwal et al. | 2019 | Citations: 842]": "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.",
                    "[211204736 | Guu et al. | 2020 | Citations: 2119]": "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.",
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[251371732 | Izacard et al. | 2022 | Citations: 783]": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 1212,
                        "end": 2095,
                        "sentence_offsets": [
                            {
                                "start": 1212,
                                "end": 1351
                            },
                            {
                                "start": 1352,
                                "end": 1510
                            },
                            {
                                "start": 1511,
                                "end": 1707
                            },
                            {
                                "start": 1708,
                                "end": 1752
                            },
                            {
                                "start": 1753,
                                "end": 1955
                            },
                            {
                                "start": 1956,
                                "end": 2095
                            }
                        ],
                        "ref_mentions": [
                            "218869575",
                            "251371732",
                            "244954723",
                            "211204736",
                            "207870430"
                        ],
                        "quote": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge."
                    }
                ]
            },
            {
                "idx": 17,
                "key": "[271843111 | Kahl et al. | 2024 | Citations: 3]",
                "snippets": "Our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations (Shuster et al., 2021)...RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first...Fine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[233240939 | Shuster et al. | 2021 | Citations: 743]": "Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots."
                },
                "metadata": [
                    {
                        "section_title": "Discussion and Conclusion",
                        "pdf_hash": "",
                        "start": 237,
                        "end": 617,
                        "sentence_offsets": [
                            {
                                "start": 230,
                                "end": 418
                            },
                            {
                                "start": 419,
                                "end": 618
                            }
                        ],
                        "ref_mentions": [
                            "233240939"
                        ],
                        "quote": "Our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations (Shuster et al., 2021)"
                    },
                    {
                        "section_title": "Discussion and Conclusion",
                        "pdf_hash": "",
                        "start": 803,
                        "end": 922,
                        "sentence_offsets": [
                            {
                                "start": 792,
                                "end": 922
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first"
                    },
                    {
                        "section_title": "Discussion and Conclusion",
                        "pdf_hash": "",
                        "start": 1420,
                        "end": 2078,
                        "sentence_offsets": [
                            {
                                "start": 1420,
                                "end": 1482
                            },
                            {
                                "start": 1483,
                                "end": 1551
                            },
                            {
                                "start": 1552,
                                "end": 1757
                            },
                            {
                                "start": 1758,
                                "end": 1838
                            },
                            {
                                "start": 1839,
                                "end": 1914
                            },
                            {
                                "start": 1915,
                                "end": 2077
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG."
                    }
                ]
            },
            {
                "idx": 18,
                "key": "[273233795 | Lu et al. | 2024 | Citations: 18]",
                "snippets": "Recent studies (Borgeaud et al., 2021)Jiang et al., 2024;Trivedi et al., 2022;(Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2022)Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[244954723 | Borgeaud et al. | 2021 | Citations: 1100]": "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.",
                    "[252735056 | Siriwardhana et al. | 2022 | Citations: 179]": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency.",
                    "[256459451 | Ram et al. | 2023 | Citations: 605]": "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"
                },
                "metadata": [
                    {
                        "section_title": "RELATED WORK",
                        "pdf_hash": "",
                        "start": 320,
                        "end": 756,
                        "sentence_offsets": [
                            {
                                "start": 320,
                                "end": 756
                            }
                        ],
                        "ref_mentions": [
                            "244954723",
                            "256459451",
                            "252735056"
                        ],
                        "quote": "Recent studies (Borgeaud et al., 2021)Jiang et al., 2024;Trivedi et al., 2022;(Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2022)Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc."
                    }
                ]
            },
            {
                "idx": 19,
                "key": "[273346023 | Chen et al. | 2024 | Citations: 6]",
                "snippets": "In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 396,
                        "end": 1058,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark."
                    }
                ]
            },
            {
                "idx": 20,
                "key": "[273375021 | Liu et al. | 2024 | Citations: 5]",
                "snippets": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al., 2022).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[251371732 | Izacard et al. | 2022 | Citations: 783]": "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 381,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 151
                            },
                            {
                                "start": 152,
                                "end": 381
                            }
                        ],
                        "ref_mentions": [
                            "251371732"
                        ],
                        "quote": "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al., 2022)."
                    }
                ]
            },
            {
                "idx": 21,
                "key": "[273403480 | Li et al. | 2024 | Citations: 11]",
                "snippets": "To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 176,
                        "end": 591,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system."
                    }
                ]
            },
            {
                "idx": 22,
                "key": "[273403839 | Li et al. | 2024 | Citations: 0]",
                "snippets": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 323,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear."
                    }
                ]
            },
            {
                "idx": 23,
                "key": "[273403870 | Gupta et al. | 2024 | Citations: 1]",
                "snippets": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 562,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data."
                    }
                ]
            },
            {
                "idx": 24,
                "key": "[273501949 | Anaissi et al. | 2024 | Citations: 3]",
                "snippets": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]...The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches...SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation...Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22].",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 242,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 243
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]"
                    },
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 531,
                        "end": 1163,
                        "sentence_offsets": [
                            {
                                "start": 531,
                                "end": 694
                            },
                            {
                                "start": 695,
                                "end": 858
                            },
                            {
                                "start": 859,
                                "end": 1025
                            },
                            {
                                "start": 1026,
                                "end": 1163
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches"
                    },
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 1166,
                        "end": 1498,
                        "sentence_offsets": [
                            {
                                "start": 1166,
                                "end": 1336
                            },
                            {
                                "start": 1337,
                                "end": 1498
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation"
                    },
                    {
                        "section_title": "II. RELATED WORK",
                        "pdf_hash": "",
                        "start": 1691,
                        "end": 2031,
                        "sentence_offsets": [
                            {
                                "start": 1691,
                                "end": 1826
                            },
                            {
                                "start": 1827,
                                "end": 2030
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."
                    }
                ]
            },
            {
                "idx": 25,
                "key": "[273502659 | Chen et al. | 2024 | Citations: 2]",
                "snippets": "We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 530,
                        "end": 1025,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."
                    }
                ]
            },
            {
                "idx": 26,
                "key": "[273549218 | Liu et al. | 2024 | Citations: 1]",
                "snippets": "Our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 27,
                "key": "[273566432 | Bora et al. | 2024 | Citations: 12]",
                "snippets": "Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning.",
                        "section_title": "abstract",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 28,
                "key": "[273654156 | Wang et al. | 2024 | Citations: 0]",
                "snippets": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 304,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications."
                    }
                ]
            },
            {
                "idx": 29,
                "key": "[273962778 | Liu et al. | 2024 | Citations: 2]",
                "snippets": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 470,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability."
                    }
                ]
            },
            {
                "idx": 30,
                "key": "[273969615 | Zhang et al. | 2024 | Citations: 3]",
                "snippets": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 (Lewis et al., 2020), enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                },
                "metadata": [
                    {
                        "section_title": "C. Retrieval-Augmented Generation",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 865,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 143
                            },
                            {
                                "start": 144,
                                "end": 309
                            },
                            {
                                "start": 310,
                                "end": 479
                            },
                            {
                                "start": 480,
                                "end": 635
                            },
                            {
                                "start": 636,
                                "end": 865
                            }
                        ],
                        "ref_mentions": [
                            "218869575"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 (Lewis et al., 2020), enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."
                    }
                ]
            },
            {
                "idx": 31,
                "key": "[274776379 | Wang et al. | 2024 | Citations: 2]",
                "snippets": "Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities...Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "A. RAG Technologies",
                        "pdf_hash": "",
                        "start": 549,
                        "end": 1105,
                        "sentence_offsets": [
                            {
                                "start": 549,
                                "end": 783
                            },
                            {
                                "start": 784,
                                "end": 1106
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities"
                    },
                    {
                        "section_title": "A. RAG Technologies",
                        "pdf_hash": "",
                        "start": 1369,
                        "end": 1899,
                        "sentence_offsets": [
                            {
                                "start": 1357,
                                "end": 1598
                            },
                            {
                                "start": 1599,
                                "end": 1898
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance."
                    }
                ]
            },
            {
                "idx": 32,
                "key": "[275788867 | Devine | 2025 | Citations: 0]",
                "snippets": "We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 211,
                        "end": 888,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance."
                    }
                ]
            },
            {
                "idx": 33,
                "key": "[275993994 | Tu et al. | 2025 | Citations: 6]",
                "snippets": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 655,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks."
                    }
                ]
            },
            {
                "idx": 34,
                "key": "[276287820 | Bhushan et al. | 2025 | Citations: 2]",
                "snippets": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.\n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "quote": "Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts.\n\nAnother line of work (Mecklenburg et al., 2024;Zhang et al., 2024b) focuses solely on adding domain knowledge to LLMs as an alternative to RAG. These approaches fine-tune LLMs using questionanswer (QA) pairs derived from domain data and aim to answer any new test query without retrieving any document. As a result, they fail to leverage access to the domain documents during inference.",
                        "pdf_hash": ""
                    }
                ]
            },
            {
                "idx": 35,
                "key": "[276408784 | Baqar et al. | 2025 | Citations: 1]",
                "snippets": "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 655,
                        "end": 1531,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision."
                    }
                ]
            },
            {
                "idx": 36,
                "key": "[276449952 | Zhang et al. | 2025 | Citations: 2]",
                "snippets": "Retrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. (Lewis et al., 2020) introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval (Cheng et al., 2023)[36] and enhancing generation capabilities (Anderson et al., 2022)[17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[218869575 | Lewis et al. | 2020 | Citations: 6476]": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
                    "[258479968 | Cheng et al. | 2023 | Citations: 102]": "With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research."
                },
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 39,
                        "end": 905,
                        "sentence_offsets": [
                            {
                                "start": 39,
                                "end": 236
                            },
                            {
                                "start": 237,
                                "end": 368
                            },
                            {
                                "start": 369,
                                "end": 523
                            },
                            {
                                "start": 524,
                                "end": 695
                            },
                            {
                                "start": 696,
                                "end": 905
                            }
                        ],
                        "ref_mentions": [
                            "218869575",
                            "258479968",
                            "252186384"
                        ],
                        "quote": "Retrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. (Lewis et al., 2020) introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval (Cheng et al., 2023)[36] and enhancing generation capabilities (Anderson et al., 2022)[17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself."
                    }
                ]
            },
            {
                "idx": 37,
                "key": "[277501853 | Srinivas et al. | 2025 | Citations: 1]",
                "snippets": "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering).",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {
                    "[268510197 | Zhang et al. | 2024 | Citations: 208]": "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a\"open-book\"in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla."
                },
                "metadata": [
                    {
                        "section_title": "Introduction",
                        "pdf_hash": "",
                        "start": 540,
                        "end": 1059,
                        "sentence_offsets": [
                            {
                                "start": 540,
                                "end": 700
                            },
                            {
                                "start": 701,
                                "end": 1059
                            }
                        ],
                        "ref_mentions": [
                            "268510197"
                        ],
                        "quote": "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering)."
                    }
                ]
            },
            {
                "idx": 38,
                "key": "[278033562 | Park et al. | 2025 | Citations: 1]",
                "snippets": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "abstract",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 456,
                        "sentence_offsets": [],
                        "ref_mentions": [],
                        "quote": "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment."
                    }
                ]
            },
            {
                "idx": 39,
                "key": "[278714952 | Lee et al. | 2025 | Citations: 0]",
                "snippets": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Methodology",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 403,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 201
                            },
                            {
                                "start": 202,
                                "end": 403
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.032649,
        "cot": "For the query \"What recent research compares Retrieval-Augmented Generation (RAG) with fine-tuning for improving generative AI models?\", I need to organize the quotes into meaningful dimensions that will allow for a comprehensive answer.\n\nFirst, I'll start with an \"Introduction\" section to provide background on RAG and fine-tuning as approaches for improving generative AI models. This will set the stage for understanding the comparison.\n\nNext, I'll include a \"Performance Comparison\" section that synthesizes research findings on how RAG and fine-tuning compare in terms of effectiveness. Many quotes directly compare the performance of these approaches, making this a crucial dimension.\n\nThen, I'll have a \"Combined Approaches\" dimension to discuss research that explores using both RAG and fine-tuning together, as several quotes mention this hybrid approach.\n\nA \"Domain-Specific Applications\" dimension is important because many studies compare RAG and fine-tuning in specific domains like healthcare, which provides valuable context for when each approach might be preferred.\n\nI'll include a \"Technical Innovations\" dimension to cover recent methodological advances in both RAG and fine-tuning approaches.\n\nFinally, a \"Limitations and Challenges\" dimension will present research findings on the drawbacks or issues with each approach, which is important for a balanced comparison.\n\nFor each dimension, I need to decide whether it should be presented as a synthesis paragraph or a list:\n\n1. \"Introduction/Background\" - Synthesis format is appropriate as this provides foundational information that needs to be explained coherently.\n\n2. \"Performance Comparison\" - Synthesis format is best as we need to integrate multiple research findings on comparative performance.\n\n3. \"Combined Approaches\" - Synthesis format will work well to explain how RAG and fine-tuning can be integrated.\n\n4. \"Domain-Specific Applications\" - List format is appropriate here as we can enumerate different domains where these approaches have been compared.\n\n5. \"Technical Innovations\" - List format works well to present distinct innovative approaches.\n\n6. \"Limitations and Challenges\" - Synthesis format will help present a coherent analysis of the limitations of each approach.",
        "plan": {
            "Introduction/Background on RAG and Fine-tuning (synthesis)": [
                0,
                1,
                20,
                24,
                28,
                30
            ],
            "Performance Comparison: RAG vs. Fine-tuning (synthesis)": [
                2,
                4,
                5,
                7,
                9,
                10,
                17,
                18,
                25,
                35
            ],
            "Combined Approaches: Integrating RAG with Fine-tuning (synthesis)": [
                1,
                3,
                11,
                13,
                15,
                19,
                21,
                24,
                27,
                31,
                34,
                37
            ],
            "Domain-Specific Applications (list)": [
                2,
                12,
                16,
                23,
                27,
                29,
                31
            ],
            "Technical Innovations in RAG and Fine-tuning Research (list)": [
                6,
                14,
                22,
                24,
                32,
                33,
                36,
                39
            ],
            "Limitations and Challenges (synthesis)": [
                8,
                16,
                26,
                38
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction/Background on RAG and Fine-tuning",
                "tldr": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving relevant external knowledge during generation, while fine-tuning adapts model parameters to specific domains or tasks. Both approaches address different limitations of large language models, with RAG focusing on knowledge access and fine-tuning on specialized capabilities. (9 sources)",
                "text": "\nRetrieval-Augmented Generation (RAG) emerged as a groundbreaking approach to enhance large language models (LLMs) by combining parametric memory (knowledge stored in model weights) with non-parametric memory (external knowledge bases). First introduced by Facebook AI Research in 2020, RAG addresses fundamental limitations of traditional generative models, particularly their struggle with specialized or rare information that can lead to hallucinations <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. The core mechanism of RAG involves augmenting the input space of language models with retrieved text passages, which has demonstrated significant improvements in knowledge-intensive tasks <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>.\n\nRAG systems improve model performance by generating more accurate and reliable answers through the integration of retrieved context with queries, effectively mitigating hallucinations common in standalone generative models <Paper corpusId=\"273375021\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper>. A key advantage of RAG is its dynamic updating mechanism that refreshes the knowledge base without requiring retraining of the entire model, enhancing both reliability and clarity of responses <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. This capability has made RAG a prominent component in improving the functionality and performance of LLM-based applications <Paper corpusId=\"273654156\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nIn contrast, fine-tuning focuses on adjusting a model's weights according to new data, allowing for customization without retraining the entire model. This approach is particularly effective for adapting pre-trained LLMs to specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. When building applications with domain-specific data, developers commonly use either RAG to augment prompts with external data or fine-tuning to incorporate additional knowledge directly into the model parameters <Paper corpusId=\"267027552\" paperTitle=\"(Balaguer et al., 2024)\" isShortName></Paper>.\n\nRecent research has begun exploring the integration of RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. Comparative studies show that RAG outperforms unsupervised fine-tuning in scenarios involving new or unseen knowledge, highlighting its superiority in knowledge injection and model adaptation <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Advanced implementations like SELF-RAG have further refined traditional RAG by incorporating selective retrieval and self-reflection mechanisms that ensure only relevant content is retrieved based on the model's self-evaluation <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Zhang et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 (Lewis et al., 2020), enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."
                        ],
                        "paper": {
                            "corpus_id": 273969615,
                            "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG",
                            "authors": [
                                {
                                    "authorId": "2270181751",
                                    "name": "Zilun Zhang"
                                },
                                {
                                    "authorId": "2174678931",
                                    "name": "Haozhan Shen"
                                },
                                {
                                    "authorId": "8200875",
                                    "name": "Tiancheng Zhao"
                                },
                                {
                                    "authorId": "2330774884",
                                    "name": "Yuhao Wang"
                                },
                                {
                                    "authorId": "2330612748",
                                    "name": "Bin Chen"
                                },
                                {
                                    "authorId": "2149196373",
                                    "name": "Yuxiang Cai"
                                },
                                {
                                    "authorId": "2093090552",
                                    "name": "Yongheng Shang"
                                },
                                {
                                    "authorId": "2111612160",
                                    "name": "Jianwei Yin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.96533203125
                    },
                    {
                        "id": "(Asai et al., 2023)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020)(Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b)."
                        ],
                        "paper": {
                            "corpus_id": 264288947,
                            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
                            "authors": [
                                {
                                    "authorId": "35584853",
                                    "name": "Akari Asai"
                                },
                                {
                                    "authorId": "7806955",
                                    "name": "Zeqiu Wu"
                                },
                                {
                                    "authorId": "1705260",
                                    "name": "Yizhong Wang"
                                },
                                {
                                    "authorId": "2707234",
                                    "name": "Avirup Sil"
                                },
                                {
                                    "authorId": "2548384",
                                    "name": "Hannaneh Hajishirzi"
                                }
                            ],
                            "year": 2023,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 780
                        },
                        "score": 0.888671875
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Liu et al., 2024)",
                        "snippets": [
                            "In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al., 2022)."
                        ],
                        "paper": {
                            "corpus_id": 273375021,
                            "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity",
                            "authors": [
                                {
                                    "authorId": "2326248349",
                                    "name": "Jintao Liu"
                                },
                                {
                                    "authorId": "2058085406",
                                    "name": "Ruixue Ding"
                                },
                                {
                                    "authorId": "2326165573",
                                    "name": "Linhao Zhang"
                                },
                                {
                                    "authorId": "2326115683",
                                    "name": "Pengjun Xie"
                                },
                                {
                                    "authorId": "2326114691",
                                    "name": "Fie Huang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 5
                        },
                        "score": 0.896484375
                    },
                    {
                        "id": "(Izacard et al., 2022)",
                        "snippets": [
                            "Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."
                        ],
                        "paper": {
                            "corpus_id": 251371732,
                            "title": "Few-shot Learning with Retrieval Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "1410231361",
                                    "name": "Gautier Izacard"
                                },
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3376175",
                                    "name": "M. Lomeli"
                                },
                                {
                                    "authorId": "26360550",
                                    "name": "Lucas Hosseini"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "32246932",
                                    "name": "Timo Schick"
                                },
                                {
                                    "authorId": "2129456957",
                                    "name": "Jane A. Yu"
                                },
                                {
                                    "authorId": "2319608",
                                    "name": "Armand Joulin"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "3024698",
                                    "name": "Edouard Grave"
                                }
                            ],
                            "year": 2022,
                            "venue": "Journal of machine learning research",
                            "n_citations": 783
                        },
                        "score": 0
                    },
                    {
                        "id": "(Anaissi et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]",
                            "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches",
                            "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation",
                            "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."
                        ],
                        "paper": {
                            "corpus_id": 273501949,
                            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                            "authors": [
                                {
                                    "authorId": "3333168",
                                    "name": "Ali Anaissi"
                                },
                                {
                                    "authorId": "3069261",
                                    "name": "Ali Braytee"
                                },
                                {
                                    "authorId": "1992906806",
                                    "name": "Junaid Akram"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
                            "n_citations": 3
                        },
                        "score": 0.8994140625
                    },
                    {
                        "id": "(Wang et al., 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications."
                        ],
                        "paper": {
                            "corpus_id": 273654156,
                            "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2259065706",
                                    "name": "Zihan Wang"
                                },
                                {
                                    "authorId": "1380224383",
                                    "name": "Xuri Ge"
                                },
                                {
                                    "authorId": "2286309062",
                                    "name": "Joemon M. Jose"
                                },
                                {
                                    "authorId": "2327995614",
                                    "name": "Haitao Yu"
                                },
                                {
                                    "authorId": "2311314421",
                                    "name": "Weizhi Ma"
                                },
                                {
                                    "authorId": "2260895127",
                                    "name": "Zhaochun Ren"
                                },
                                {
                                    "authorId": "2294565066",
                                    "name": "Xin Xin"
                                }
                            ],
                            "year": 2024,
                            "venue": "SIGIR-AP",
                            "n_citations": 0
                        },
                        "score": 0.89892578125
                    },
                    {
                        "id": "(Balaguer et al., 2024)",
                        "snippets": [
                            "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4",
                            "We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further."
                        ],
                        "paper": {
                            "corpus_id": 267027552,
                            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
                            "authors": [
                                {
                                    "authorId": "34938986",
                                    "name": "M. A. D. L. Balaguer"
                                },
                                {
                                    "authorId": "3456623",
                                    "name": "Vinamra Benara"
                                },
                                {
                                    "authorId": "2279752416",
                                    "name": "Renato Luiz de Freitas Cunha"
                                },
                                {
                                    "authorId": "2279750954",
                                    "name": "Roberto de M. Estevao Filho"
                                },
                                {
                                    "authorId": "2279749685",
                                    "name": "Todd Hendry"
                                },
                                {
                                    "authorId": "2279750514",
                                    "name": "Daniel Holstein"
                                },
                                {
                                    "authorId": "2279752770",
                                    "name": "Jennifer Marsman"
                                },
                                {
                                    "authorId": "2279750706",
                                    "name": "Nick Mecklenburg"
                                },
                                {
                                    "authorId": "145707932",
                                    "name": "S. Malvar"
                                },
                                {
                                    "authorId": "2256989583",
                                    "name": "Leonardo Nunes"
                                },
                                {
                                    "authorId": "2279548480",
                                    "name": "Rafael Padilha"
                                },
                                {
                                    "authorId": "2279750745",
                                    "name": "Morris Sharp"
                                },
                                {
                                    "authorId": "2257019569",
                                    "name": "B. Silva"
                                },
                                {
                                    "authorId": "2279667352",
                                    "name": "Swati Sharma"
                                },
                                {
                                    "authorId": "2257349985",
                                    "name": "Vijay Aski"
                                },
                                {
                                    "authorId": "2256993742",
                                    "name": "Ranveer Chandra"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 91
                        },
                        "score": 0.9619140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Performance Comparison: RAG vs. Fine-tuning",
                "tldr": "Recent studies consistently show RAG outperforms fine-tuning for low-frequency knowledge and factual accuracy, while fine-tuning offers computational efficiency and better performance on familiar data. Comparative evaluations across multiple metrics demonstrate RAG's superior ability to reduce hallucinations and handle domain-specific information. (15 sources)",
                "text": "\nExtensive comparative studies have revealed distinct performance patterns between RAG and fine-tuning approaches across various domains and model architectures. Research examining performance on low-frequency entities and domain-specific knowledge has demonstrated that while fine-tuning boosts performance across entities of varying popularity, RAG significantly outperforms fine-tuning particularly for rare or less popular factual knowledge <Paper corpusId=\"268248396\" paperTitle=\"(Soudani et al., 2024)\" isShortName></Paper>. This advantage becomes especially apparent in specialized domains such as medical question answering, where RAG has proven more efficient than traditional model fine-tuning <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>.\n\nQuantitative evaluations across multiple metrics further substantiate RAG's advantages. When comparing RAG with Domain-specific Fine-Tuning (DFT) across multiple language model architectures including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2, RAG consistently outperforms DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in Coverage Score metrics. Interestingly, DFT showed only modest advantages in METEOR scores, suggesting slightly better creative capabilities <Paper corpusId=\"268510325\" paperTitle=\"(Lakatos et al., 2024)\" isShortName></Paper>. This pattern of RAG superiority extends to generative question-answering tasks, where advanced RAG implementations like \"Blended Retriever\" have demonstrated results that surpass fine-tuning performance on datasets such as SQUAD <Paper corpusId=\"269043117\" paperTitle=\"(Sawarkar et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of RAG in improving generation quality compared to purely generative models has been consistently demonstrated across various benchmarks <Paper corpusId=\"269283058\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. Multiple studies confirm that RAG significantly outperforms pure generative models across various domains including question answering, code generation, and content creation <Paper corpusId=\"273233795\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. Particularly notable is RAG's effectiveness in reducing hallucinations by providing factual answers <Paper corpusId=\"271843111\" paperTitle=\"(Kahl et al., 2024)\" isShortName></Paper> <Paper corpusId=\"233240939\" paperTitle=\"(Shuster et al., 2021)\" isShortName></Paper>.\n\nIn knowledge injection capacity evaluation, research has found that while LLMs can address performance issues through unsupervised fine-tuning, RAG consistently outperforms fine-tuning approaches in unsupervised learning scenarios <Paper corpusId=\"269292881\" paperTitle=\"(Efeoglu et al., 2024)\" isShortName></Paper>. This advantage extends to classification tasks, where RAG-based approaches have demonstrated greater flexibility, transparency in decision-making, superior classification performance, and improved robustness against adversarial attacks compared to model fine-tuning <Paper corpusId=\"273502659\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nDespite RAG's numerous advantages, fine-tuning approaches offer important complementary benefits. A recent large-scale empirical evaluation comparing RAG with parameter-efficient fine-tuning methods (LoRA and DoRA) on 20,000 FAQ-based queries revealed that DoRA achieved the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in domain-specific applications <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>. This highlights the trade-offs between these approaches: RAG excels in knowledge grounding, LoRA offers cost-efficient domain adaptation, while DoRA balances fine-tuning efficiency with model precision <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>.\n\nAn important practical consideration is that combining RAG with fine-tuned models can sometimes lead to unexpected performance degradation, suggesting challenges in effectively integrating these approaches <Paper corpusId=\"268510325\" paperTitle=\"(Lakatos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271843111\" paperTitle=\"(Kahl et al., 2024)\" isShortName></Paper>. This indicates that while both approaches have distinct strengths, their integration requires careful implementation to maximize benefits.",
                "citations": [
                    {
                        "id": "(Soudani et al., 2024)",
                        "snippets": [
                            "Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge."
                        ],
                        "paper": {
                            "corpus_id": 268248396,
                            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
                            "authors": [
                                {
                                    "authorId": "2165569122",
                                    "name": "Heydar Soudani"
                                },
                                {
                                    "authorId": "1713134",
                                    "name": "E. Kanoulas"
                                },
                                {
                                    "authorId": "1951737",
                                    "name": "Faegheh Hasibi"
                                }
                            ],
                            "year": 2024,
                            "venue": "SIGIR-AP",
                            "n_citations": 37
                        },
                        "score": 0.947265625
                    },
                    {
                        "id": "(Elgedawy et al., 2024)",
                        "snippets": [
                            "In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 267061013,
                            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2280063225",
                                    "name": "Ran Elgedawy"
                                },
                                {
                                    "authorId": "2274464131",
                                    "name": "Ioana Danciu"
                                },
                                {
                                    "authorId": "1387927897",
                                    "name": "Maria Mahbub"
                                },
                                {
                                    "authorId": "2149506151",
                                    "name": "Sudarshan Srinivasan"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.9013671875
                    },
                    {
                        "id": "(Lakatos et al., 2024)",
                        "snippets": [
                            "In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation."
                        ],
                        "paper": {
                            "corpus_id": 268510325,
                            "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems",
                            "authors": [
                                {
                                    "authorId": "2284077584",
                                    "name": "R\u00f3bert Lakatos"
                                },
                                {
                                    "authorId": "2279978930",
                                    "name": "P. Pollner"
                                },
                                {
                                    "authorId": "2260742529",
                                    "name": "Andr\u00e1s Hajdu"
                                },
                                {
                                    "authorId": "2243157601",
                                    "name": "Tam\u00e1s Jo\u00f3"
                                }
                            ],
                            "year": 2024,
                            "venue": "Machine Learning and Knowledge Extraction",
                            "n_citations": 10
                        },
                        "score": 0.9658203125
                    },
                    {
                        "id": "(Sawarkar et al., 2024)",
                        "snippets": [
                            "We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."
                        ],
                        "paper": {
                            "corpus_id": 269043117,
                            "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers",
                            "authors": [
                                {
                                    "authorId": "2003089508",
                                    "name": "Kunal Sawarkar"
                                },
                                {
                                    "authorId": "2295990127",
                                    "name": "Abhilasha Mangal"
                                },
                                {
                                    "authorId": "2295990033",
                                    "name": "S. R. Solanki"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Multimedia Information Processing and Retrieval",
                            "n_citations": 57
                        },
                        "score": 0.90576171875
                    },
                    {
                        "id": "(Jin et al., 2024)",
                        "snippets": [
                            "Recent work [1](Borgeaud et al., 2021)22,(Lewis et al., 2020)37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models."
                        ],
                        "paper": {
                            "corpus_id": 269283058,
                            "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2170749149",
                                    "name": "Chao Jin"
                                },
                                {
                                    "authorId": "2182609505",
                                    "name": "Zili Zhang"
                                },
                                {
                                    "authorId": "2297730218",
                                    "name": "Xuanlin Jiang"
                                },
                                {
                                    "authorId": "2297499793",
                                    "name": "Fangyue Liu"
                                },
                                {
                                    "authorId": "2305828489",
                                    "name": "Xin Liu"
                                },
                                {
                                    "authorId": "2237080638",
                                    "name": "Xuanzhe Liu"
                                },
                                {
                                    "authorId": "2182349318",
                                    "name": "Xin Jin"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 47
                        },
                        "score": 0.9052734375
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Borgeaud et al., 2021)",
                        "snippets": [
                            "We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."
                        ],
                        "paper": {
                            "corpus_id": 244954723,
                            "title": "Improving language models by retrieving from trillions of tokens",
                            "authors": [
                                {
                                    "authorId": "148016269",
                                    "name": "Sebastian Borgeaud"
                                },
                                {
                                    "authorId": "1697879",
                                    "name": "A. Mensch"
                                },
                                {
                                    "authorId": "46616544",
                                    "name": "Jordan Hoffmann"
                                },
                                {
                                    "authorId": "2072572294",
                                    "name": "Trevor Cai"
                                },
                                {
                                    "authorId": "2143538252",
                                    "name": "Eliza Rutherford"
                                },
                                {
                                    "authorId": "2143434227",
                                    "name": "Katie Millican"
                                },
                                {
                                    "authorId": "47568983",
                                    "name": "George van den Driessche"
                                },
                                {
                                    "authorId": "143783339",
                                    "name": "Jean-Baptiste Lespiau"
                                },
                                {
                                    "authorId": "2143374656",
                                    "name": "Bogdan Damoc"
                                },
                                {
                                    "authorId": "31993415",
                                    "name": "Aidan Clark"
                                },
                                {
                                    "authorId": "40550616",
                                    "name": "Diego de Las Casas"
                                },
                                {
                                    "authorId": "40895205",
                                    "name": "Aurelia Guy"
                                },
                                {
                                    "authorId": "10698483",
                                    "name": "Jacob Menick"
                                },
                                {
                                    "authorId": "81387328",
                                    "name": "Roman Ring"
                                },
                                {
                                    "authorId": "4629007",
                                    "name": "T. Hennigan"
                                },
                                {
                                    "authorId": "2148653469",
                                    "name": "Saffron Huang"
                                },
                                {
                                    "authorId": "108173905",
                                    "name": "Lorenzo Maggiore"
                                },
                                {
                                    "authorId": "2115601070",
                                    "name": "Chris Jones"
                                },
                                {
                                    "authorId": "51042571",
                                    "name": "Albin Cassirer"
                                },
                                {
                                    "authorId": "2065040422",
                                    "name": "Andy Brock"
                                },
                                {
                                    "authorId": "35550664",
                                    "name": "Michela Paganini"
                                },
                                {
                                    "authorId": "2060655766",
                                    "name": "G. Irving"
                                },
                                {
                                    "authorId": "1689108",
                                    "name": "O. Vinyals"
                                },
                                {
                                    "authorId": "2217144",
                                    "name": "Simon Osindero"
                                },
                                {
                                    "authorId": "34838386",
                                    "name": "K. Simonyan"
                                },
                                {
                                    "authorId": "34269227",
                                    "name": "Jack W. Rae"
                                },
                                {
                                    "authorId": "152585800",
                                    "name": "Erich Elsen"
                                },
                                {
                                    "authorId": "2175946",
                                    "name": "L. Sifre"
                                }
                            ],
                            "year": 2021,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 1100
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lu et al., 2024)",
                        "snippets": [
                            "Recent studies (Borgeaud et al., 2021)Jiang et al., 2024;Trivedi et al., 2022;(Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2022)Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc."
                        ],
                        "paper": {
                            "corpus_id": 273233795,
                            "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text",
                            "authors": [
                                {
                                    "authorId": "2303957265",
                                    "name": "Songshuo Lu"
                                },
                                {
                                    "authorId": "2325461964",
                                    "name": "Hua Wang"
                                },
                                {
                                    "authorId": "2325157287",
                                    "name": "Yutian Rong"
                                },
                                {
                                    "authorId": "2325197565",
                                    "name": "Zhi Chen"
                                },
                                {
                                    "authorId": "2304014129",
                                    "name": "Yaohua Tang"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 18
                        },
                        "score": 0.89599609375
                    },
                    {
                        "id": "(Siriwardhana et al., 2022)",
                        "snippets": [
                            "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."
                        ],
                        "paper": {
                            "corpus_id": 252735056,
                            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "52001535",
                                    "name": "Rivindu Weerasekera"
                                },
                                {
                                    "authorId": "2114425044",
                                    "name": "Elliott Wen"
                                },
                                {
                                    "authorId": "1992921690",
                                    "name": "Tharindu Kaluarachchi"
                                },
                                {
                                    "authorId": "1814487",
                                    "name": "R. Rana"
                                },
                                {
                                    "authorId": "1486464114",
                                    "name": "Suranga Nanayakkara"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 179
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ram et al., 2023)",
                        "snippets": [
                            "Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"
                        ],
                        "paper": {
                            "corpus_id": 256459451,
                            "title": "In-Context Retrieval-Augmented Language Models",
                            "authors": [
                                {
                                    "authorId": "73775461",
                                    "name": "Ori Ram"
                                },
                                {
                                    "authorId": "152754428",
                                    "name": "Yoav Levine"
                                },
                                {
                                    "authorId": "1491822146",
                                    "name": "Itay Dalmedigos"
                                },
                                {
                                    "authorId": "51918041",
                                    "name": "Dor Muhlgay"
                                },
                                {
                                    "authorId": "3140335",
                                    "name": "A. Shashua"
                                },
                                {
                                    "authorId": "2066411743",
                                    "name": "Kevin Leyton-Brown"
                                },
                                {
                                    "authorId": "1701353",
                                    "name": "Y. Shoham"
                                }
                            ],
                            "year": 2023,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 605
                        },
                        "score": 0
                    },
                    {
                        "id": "(Kahl et al., 2024)",
                        "snippets": [
                            "Our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations (Shuster et al., 2021)",
                            "RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first",
                            "Fine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG."
                        ],
                        "paper": {
                            "corpus_id": 271843111,
                            "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course",
                            "authors": [
                                {
                                    "authorId": "2315811033",
                                    "name": "Sebastian Kahl"
                                },
                                {
                                    "authorId": "2320842831",
                                    "name": "Felix L\u00f6ffler"
                                },
                                {
                                    "authorId": "2315810380",
                                    "name": "Martin Maciol"
                                },
                                {
                                    "authorId": "2315810328",
                                    "name": "Fabian Ridder"
                                },
                                {
                                    "authorId": "2315811128",
                                    "name": "Marius Schmitz"
                                },
                                {
                                    "authorId": "2315812881",
                                    "name": "Jennifer Spanagel"
                                },
                                {
                                    "authorId": "2315811156",
                                    "name": "Jens Wienkamp"
                                },
                                {
                                    "authorId": "1397379404",
                                    "name": "Christopher Burgahn"
                                },
                                {
                                    "authorId": "1913256",
                                    "name": "M. Schilling"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 3
                        },
                        "score": 0.896484375
                    },
                    {
                        "id": "(Shuster et al., 2021)",
                        "snippets": [
                            "Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots."
                        ],
                        "paper": {
                            "corpus_id": 233240939,
                            "title": "Retrieval Augmentation Reduces Hallucination in Conversation",
                            "authors": [
                                {
                                    "authorId": "35752280",
                                    "name": "Kurt Shuster"
                                },
                                {
                                    "authorId": "1753626755",
                                    "name": "Spencer Poff"
                                },
                                {
                                    "authorId": "2108267192",
                                    "name": "Moya Chen"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                },
                                {
                                    "authorId": "145183709",
                                    "name": "J. Weston"
                                }
                            ],
                            "year": 2021,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 743
                        },
                        "score": 0
                    },
                    {
                        "id": "(Efeoglu et al., 2024)",
                        "snippets": [
                            "Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."
                        ],
                        "paper": {
                            "corpus_id": 269292881,
                            "title": "Retrieval-Augmented Generation-based Relation Extraction",
                            "authors": [
                                {
                                    "authorId": "2189018699",
                                    "name": "Sefika Efeoglu"
                                },
                                {
                                    "authorId": "2259621860",
                                    "name": "Adrian Paschke"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 9
                        },
                        "score": 0.9521484375
                    },
                    {
                        "id": "(Chen et al., 2024)",
                        "snippets": [
                            "We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."
                        ],
                        "paper": {
                            "corpus_id": 273502659,
                            "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2327003851",
                                    "name": "Jianfa Chen"
                                },
                                {
                                    "authorId": "2326992786",
                                    "name": "Emily Shen"
                                },
                                {
                                    "authorId": "2297187181",
                                    "name": "Trupti Bavalatti"
                                },
                                {
                                    "authorId": "2327028660",
                                    "name": "Xiaowen Lin"
                                },
                                {
                                    "authorId": "2326986310",
                                    "name": "Yongkai Wang"
                                },
                                {
                                    "authorId": "2327158340",
                                    "name": "Shuming Hu"
                                },
                                {
                                    "authorId": "2322094813",
                                    "name": "Harihar Subramanyam"
                                },
                                {
                                    "authorId": "2149726609",
                                    "name": "Ksheeraj Sai Vepuri"
                                },
                                {
                                    "authorId": "2327303021",
                                    "name": "Ming Jiang"
                                },
                                {
                                    "authorId": "2327505613",
                                    "name": "Ji Qi"
                                },
                                {
                                    "authorId": "2287762612",
                                    "name": "Li Chen"
                                },
                                {
                                    "authorId": "2326964342",
                                    "name": "Nan Jiang"
                                },
                                {
                                    "authorId": "2287848816",
                                    "name": "Ankit Jain"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 2
                        },
                        "score": 0.9482421875
                    },
                    {
                        "id": "(Baqar et al., 2025)",
                        "snippets": [
                            "This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision."
                        ],
                        "paper": {
                            "corpus_id": 276408784,
                            "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA",
                            "authors": [
                                {
                                    "authorId": "2316485338",
                                    "name": "Mohammad Baqar"
                                },
                                {
                                    "authorId": "69923048",
                                    "name": "Rajat Khanda"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.986328125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Combined Approaches: Integrating RAG with Fine-tuning",
                "tldr": "Research shows combining RAG with fine-tuning offers cumulative performance improvements that exceed either approach alone, with studies reporting accuracy increases of over 6 percentage points from fine-tuning plus an additional 5 percentage points when RAG is added. Novel integration frameworks like Retrieval-Augmented Fine-Tuning (RAFT) teach models to dynamically leverage external knowledge while prioritizing relevant content and ignoring distractors. (12 sources)",
                "text": "\nRecent research has increasingly focused on combining the strengths of RAG and fine-tuning approaches to maximize performance gains. Studies show that these approaches can provide cumulative benefits, with one investigation reporting an accuracy increase of over 6 percentage points when fine-tuning models, followed by an additional 5 percentage point improvement when RAG was subsequently applied <Paper corpusId=\"267027552\" paperTitle=\"(Balaguer et al., 2024)\" isShortName></Paper>. This complementary relationship has been consistently observed across multiple studies, with researchers noting that the combination of fine-tuning with RAG generates responses with improved accuracy <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>.\n\nA notable advancement in this combined approach is Retrieval-Augmented Fine-Tuning (RAFT), which integrates retrieval methods directly with language model supervised fine-tuning. Unlike traditional RAG implementations that simply retrieve documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge while prioritizing relevant content and ignoring distracting information <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268510197\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. This approach has shown consistent performance improvements across domain-specific applications including PubMed, HotpotQA, and Gorilla datasets <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper>.\n\nSome researchers have explored Parameter-Efficient Fine-Tuning (PEFT) methods in conjunction with retrieval-enhanced models. One study applied various PEFT approaches (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) across model sizes ranging from 823 million to 48 billion parameters. Results showed that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process, though GPT models demonstrated higher performance potential with PEFT <Paper corpusId=\"271039066\" paperTitle=\"(Ficek et al., 2024)\" isShortName></Paper>.\n\nFor domain-specific applications, hybrid approaches have proven particularly effective. In medical contexts, studies have combined RAG with fine-tuned models and compared their performance against base models using either RAG or fine-tuning alone, finding the combined approach most effective <Paper corpusId=\"273566432\" paperTitle=\"(Bora et al., 2024)\" isShortName></Paper>. Another novel strategy, named \"Honest AI,\" demonstrated that while RAG alone provided some improvement, the hybrid approach that combined RAG with fine-tuning achieved the highest scores in the CRAG benchmark <Paper corpusId=\"273346023\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n\nThe integration of these approaches can extend beyond text-only applications. Research has shown that RAG-based frameworks for visual-linguistic models significantly enhance performance in complex tasks requiring background knowledge <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>. Furthermore, studies of pre-training and fine-tuning processes with RAG demonstrate how the approach can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\nCurrent instruction tuning approaches for optimizing LLMs in RAG systems typically use supervised fine-tuning (SFT) to equip models to handle diverse RAG tasks with different instructions. However, researchers have noted limitations in this approach, as it can cause RAG modules to overfit training signals and overlook varying data preferences among agents within the RAG system <Paper corpusId=\"273403480\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This has led to exploration of more sophisticated integration methods that preserve the general capabilities of LLMs while adapting them to RAG scenarios <Paper corpusId=\"270123034\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nAs the field evolves, researchers continue to develop novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments <Paper corpusId=\"270560505\" paperTitle=\"(Balakrishnan et al., 2024)\" isShortName></Paper>. These advances demonstrate the complementary nature of parametric and non-parametric approaches, with integration strategies like SELF-RAG incorporating selective retrieval and self-reflection mechanisms to ensure only relevant content is retrieved based on the model's self-evaluation <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Balaguer et al., 2024)",
                        "snippets": [
                            "There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4",
                            "We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further."
                        ],
                        "paper": {
                            "corpus_id": 267027552,
                            "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture",
                            "authors": [
                                {
                                    "authorId": "34938986",
                                    "name": "M. A. D. L. Balaguer"
                                },
                                {
                                    "authorId": "3456623",
                                    "name": "Vinamra Benara"
                                },
                                {
                                    "authorId": "2279752416",
                                    "name": "Renato Luiz de Freitas Cunha"
                                },
                                {
                                    "authorId": "2279750954",
                                    "name": "Roberto de M. Estevao Filho"
                                },
                                {
                                    "authorId": "2279749685",
                                    "name": "Todd Hendry"
                                },
                                {
                                    "authorId": "2279750514",
                                    "name": "Daniel Holstein"
                                },
                                {
                                    "authorId": "2279752770",
                                    "name": "Jennifer Marsman"
                                },
                                {
                                    "authorId": "2279750706",
                                    "name": "Nick Mecklenburg"
                                },
                                {
                                    "authorId": "145707932",
                                    "name": "S. Malvar"
                                },
                                {
                                    "authorId": "2256989583",
                                    "name": "Leonardo Nunes"
                                },
                                {
                                    "authorId": "2279548480",
                                    "name": "Rafael Padilha"
                                },
                                {
                                    "authorId": "2279750745",
                                    "name": "Morris Sharp"
                                },
                                {
                                    "authorId": "2257019569",
                                    "name": "B. Silva"
                                },
                                {
                                    "authorId": "2279667352",
                                    "name": "Swati Sharma"
                                },
                                {
                                    "authorId": "2257349985",
                                    "name": "Vijay Aski"
                                },
                                {
                                    "authorId": "2256993742",
                                    "name": "Ranveer Chandra"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 91
                        },
                        "score": 0.9619140625
                    },
                    {
                        "id": "(Zhang et al._1, 2024)",
                        "snippets": [
                            "Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."
                        ],
                        "paper": {
                            "corpus_id": 267412954,
                            "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately",
                            "authors": [
                                {
                                    "authorId": "2279813822",
                                    "name": "Liang Zhang"
                                },
                                {
                                    "authorId": "2279831793",
                                    "name": "Katherine Jijo"
                                },
                                {
                                    "authorId": "2282528163",
                                    "name": "Spurthi Setty"
                                },
                                {
                                    "authorId": "2279830841",
                                    "name": "Eden Chung"
                                },
                                {
                                    "authorId": "2282539958",
                                    "name": "Fatima Javid"
                                },
                                {
                                    "authorId": "2279830757",
                                    "name": "Natan Vidra"
                                },
                                {
                                    "authorId": "2279838243",
                                    "name": "Thomas Clifford"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 20
                        },
                        "score": 0.91943359375
                    },
                    {
                        "id": "(Srinivas et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering)."
                        ],
                        "paper": {
                            "corpus_id": 277501853,
                            "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding",
                            "authors": [
                                {
                                    "authorId": "2203079037",
                                    "name": "Sakhinana Sagar Srinivas"
                                },
                                {
                                    "authorId": "2139833562",
                                    "name": "Venkataramana Runkana"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.94287109375
                    },
                    {
                        "id": "(Zhang et al._2, 2024)",
                        "snippets": [
                            "Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a\"open-book\"in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla."
                        ],
                        "paper": {
                            "corpus_id": 268510197,
                            "title": "RAFT: Adapting Language Model to Domain Specific RAG",
                            "authors": [
                                {
                                    "authorId": "1993655237",
                                    "name": "Tianjun Zhang"
                                },
                                {
                                    "authorId": "2257979820",
                                    "name": "Shishir G. Patil"
                                },
                                {
                                    "authorId": "1646458461",
                                    "name": "Naman Jain"
                                },
                                {
                                    "authorId": "2191455",
                                    "name": "Sheng Shen"
                                },
                                {
                                    "authorId": "143834867",
                                    "name": "M. Zaharia"
                                },
                                {
                                    "authorId": "2055174324",
                                    "name": "Ion Stoica"
                                },
                                {
                                    "authorId": "2254681613",
                                    "name": "Joseph Gonzalez"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 208
                        },
                        "score": 0
                    },
                    {
                        "id": "(Ficek et al., 2024)",
                        "snippets": [
                            "Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT."
                        ],
                        "paper": {
                            "corpus_id": 271039066,
                            "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning",
                            "authors": [
                                {
                                    "authorId": "2186740325",
                                    "name": "Aleksander Ficek"
                                },
                                {
                                    "authorId": "2266881428",
                                    "name": "Jiaqi Zeng"
                                },
                                {
                                    "authorId": "2787022",
                                    "name": "Oleksii Kuchaiev"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 1
                        },
                        "score": 0.97802734375
                    },
                    {
                        "id": "(Bora et al., 2024)",
                        "snippets": [
                            "Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 273566432,
                            "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
                            "authors": [
                                {
                                    "authorId": "2327432431",
                                    "name": "Arunabh Bora"
                                },
                                {
                                    "authorId": "1806041",
                                    "name": "H. Cuay\u00e1huitl"
                                }
                            ],
                            "year": 2024,
                            "venue": "Machine Learning and Knowledge Extraction",
                            "n_citations": 12
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Chen et al._1, 2024)",
                        "snippets": [
                            "In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark."
                        ],
                        "paper": {
                            "corpus_id": 273346023,
                            "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
                            "authors": [
                                {
                                    "authorId": "2325887387",
                                    "name": "Xinxi Chen"
                                },
                                {
                                    "authorId": "2284477218",
                                    "name": "Li Wang"
                                },
                                {
                                    "authorId": "2325928099",
                                    "name": "Wei Wu"
                                },
                                {
                                    "authorId": "2312876232",
                                    "name": "Qizhi Tang"
                                },
                                {
                                    "authorId": "2325900000",
                                    "name": "Yiyao Liu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.923828125
                    },
                    {
                        "id": "(Wang et al._1, 2024)",
                        "snippets": [
                            "Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities",
                            "Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance."
                        ],
                        "paper": {
                            "corpus_id": 274776379,
                            "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
                            "authors": [
                                {
                                    "authorId": "2309429494",
                                    "name": "Yujin Wang"
                                },
                                {
                                    "authorId": "2310858751",
                                    "name": "Quanfeng Liu"
                                },
                                {
                                    "authorId": "1934959729",
                                    "name": "Jiaqi Fan"
                                },
                                {
                                    "authorId": "9451675",
                                    "name": "Jinlong Hong"
                                },
                                {
                                    "authorId": "2309098",
                                    "name": "Hongqing Chu"
                                },
                                {
                                    "authorId": "30849412",
                                    "name": "Mengjian Tian"
                                },
                                {
                                    "authorId": "2292395467",
                                    "name": "Bingzhao Gao"
                                },
                                {
                                    "authorId": "2238391457",
                                    "name": "Hong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.94384765625
                    },
                    {
                        "id": "(Li et al., 2024)",
                        "snippets": [
                            "To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system."
                        ],
                        "paper": {
                            "corpus_id": 273403480,
                            "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards",
                            "authors": [
                                {
                                    "authorId": "2261354998",
                                    "name": "Xinze Li"
                                },
                                {
                                    "authorId": "2124028252",
                                    "name": "Senkun Mei"
                                },
                                {
                                    "authorId": "49047064",
                                    "name": "Zhenghao Liu"
                                },
                                {
                                    "authorId": "2277242040",
                                    "name": "Yukun Yan"
                                },
                                {
                                    "authorId": "2267033597",
                                    "name": "Shuo Wang"
                                },
                                {
                                    "authorId": "2314785970",
                                    "name": "Shi Yu"
                                },
                                {
                                    "authorId": "1633538428",
                                    "name": "Zheni Zeng"
                                },
                                {
                                    "authorId": "2327546188",
                                    "name": "Hao Chen"
                                },
                                {
                                    "authorId": "2204644192",
                                    "name": "Ge Yu"
                                },
                                {
                                    "authorId": "2290295914",
                                    "name": "Zhiyuan Liu"
                                },
                                {
                                    "authorId": "2273551430",
                                    "name": "Maosong Sun"
                                },
                                {
                                    "authorId": "2139787803",
                                    "name": "Chenyan Xiong"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 11
                        },
                        "score": 0.97314453125
                    },
                    {
                        "id": "(Zhu et al., 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."
                        ],
                        "paper": {
                            "corpus_id": 270123034,
                            "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models",
                            "authors": [
                                {
                                    "authorId": "1900406",
                                    "name": "Yutao Zhu"
                                },
                                {
                                    "authorId": "2187935160",
                                    "name": "Zhaoheng Huang"
                                },
                                {
                                    "authorId": "1897235",
                                    "name": "Zhicheng Dou"
                                },
                                {
                                    "authorId": "2186578511",
                                    "name": "Ji-Rong Wen"
                                }
                            ],
                            "year": 2024,
                            "venue": "AAAI Conference on Artificial Intelligence",
                            "n_citations": 6
                        },
                        "score": 0.958984375
                    },
                    {
                        "id": "(Balakrishnan et al., 2024)",
                        "snippets": [
                            "More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."
                        ],
                        "paper": {
                            "corpus_id": 270560505,
                            "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability",
                            "authors": [
                                {
                                    "authorId": "2356633197",
                                    "name": "Gautam Balakrishnan"
                                },
                                {
                                    "authorId": "33856997",
                                    "name": "A. Purwar"
                                }
                            ],
                            "year": 2024,
                            "venue": "IEEE India Conference",
                            "n_citations": 14
                        },
                        "score": 0.90966796875
                    },
                    {
                        "id": "(Anaissi et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]",
                            "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches",
                            "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation",
                            "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."
                        ],
                        "paper": {
                            "corpus_id": 273501949,
                            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                            "authors": [
                                {
                                    "authorId": "3333168",
                                    "name": "Ali Anaissi"
                                },
                                {
                                    "authorId": "3069261",
                                    "name": "Ali Braytee"
                                },
                                {
                                    "authorId": "1992906806",
                                    "name": "Junaid Akram"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
                            "n_citations": 3
                        },
                        "score": 0.8994140625
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Domain-Specific Applications",
                "tldr": "RAG and fine-tuning approaches have been successfully applied across numerous specialized domains, with particularly strong results in medical question answering, dialogue systems, and visual-linguistic tasks. These domain-specific implementations demonstrate how RAG's ability to incorporate external knowledge complements fine-tuning's adaptation capabilities in knowledge-intensive contexts. (10 sources)",
                "text": "\n- **Medical Question Answering**: RAG has proven particularly effective in medical contexts, where accuracy is critical. Studies show that RAG implementations for medical question answering outperform traditional model fine-tuning approaches in both efficiency and accuracy <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>. Combined approaches that integrate RAG with fine-tuned models have demonstrated superior performance compared to using either base models with RAG or fine-tuning alone in medical applications <Paper corpusId=\"273566432\" paperTitle=\"(Bora et al., 2024)\" isShortName></Paper>.\n\n- **Dialogue Systems**: RAG has enhanced generative dialogue models by integrating knowledge from external databases, improving accuracy and relevance of generated responses. This approach enables continuous updates of domain-specific knowledge and excels particularly in knowledge-intensive conversation tasks <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. However, challenges remain in ensuring accurate retrieval and effectively integrating retrieved information with the model's prior knowledge <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\n- **Question-Answering Systems**: RAG pipelines have become standard in question-answering tasks, leveraging vector stores computed using pretrained embedding models. Despite advancements in embedding technologies, domain adaptation remains challenging, particularly in industry settings where fine-tuning data may be limited <Paper corpusId=\"273403870\" paperTitle=\"(Gupta et al., 2024)\" isShortName></Paper>. Recent LLM-based retrieval approaches have shown substantial improvements in information retrieval due to their enhanced semantic understanding capabilities <Paper corpusId=\"273962778\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>.\n\n- **Visual-Linguistic Models**: RAG-based frameworks have significantly enhanced model performance in complex visual-linguistic tasks, especially those requiring substantial background knowledge. Traditional end-to-end visual-linguistic models often struggle with insufficient knowledge, whereas RAG-enhanced models integrate more contextual information during generation, improving reasoning and generative abilities <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n- **Cross-Modal Reasoning**: Studies of pre-training and fine-tuning processes with RAG demonstrate how the approach can enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. This has proven particularly effective for cross-modal reasoning and retrieval tasks, where RAG significantly improves model performance by leveraging broader background knowledge <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n- **Multi-Domain Adaptation**: Research examining fine-tuning effects on LLMs' ability to extract and integrate contextual data for RAG systems across multiple domains has yielded unexpected results. Contrary to the typical improvements observed in standalone LLM applications, some studies found that fine-tuning actually decreased performance compared to baseline models when used in RAG contexts <Paper corpusId=\"270560495\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>.",
                "citations": [
                    {
                        "id": "(Elgedawy et al., 2024)",
                        "snippets": [
                            "In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 267061013,
                            "title": "Dynamic Q&A of Clinical Documents with Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2280063225",
                                    "name": "Ran Elgedawy"
                                },
                                {
                                    "authorId": "2274464131",
                                    "name": "Ioana Danciu"
                                },
                                {
                                    "authorId": "1387927897",
                                    "name": "Maria Mahbub"
                                },
                                {
                                    "authorId": "2149506151",
                                    "name": "Sudarshan Srinivasan"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 6
                        },
                        "score": 0.9013671875
                    },
                    {
                        "id": "(Bora et al., 2024)",
                        "snippets": [
                            "Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning."
                        ],
                        "paper": {
                            "corpus_id": 273566432,
                            "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications",
                            "authors": [
                                {
                                    "authorId": "2327432431",
                                    "name": "Arunabh Bora"
                                },
                                {
                                    "authorId": "1806041",
                                    "name": "H. Cuay\u00e1huitl"
                                }
                            ],
                            "year": 2024,
                            "venue": "Machine Learning and Knowledge Extraction",
                            "n_citations": 12
                        },
                        "score": 0.9541015625
                    },
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge."
                        ],
                        "paper": {
                            "corpus_id": 271329121,
                            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
                            "authors": [
                                {
                                    "authorId": "2312343839",
                                    "name": "Yuetong Zhao"
                                },
                                {
                                    "authorId": "2312344958",
                                    "name": "Hongyu Cao"
                                },
                                {
                                    "authorId": "2312340862",
                                    "name": "Xianyu Zhao"
                                },
                                {
                                    "authorId": "2243267608",
                                    "name": "Zhijian Ou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Chinese Spoken Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Khandelwal et al., 2019)",
                        "snippets": [
                            "We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."
                        ],
                        "paper": {
                            "corpus_id": 207870430,
                            "title": "Generalization through Memorization: Nearest Neighbor Language Models",
                            "authors": [
                                {
                                    "authorId": "3030219",
                                    "name": "Urvashi Khandelwal"
                                },
                                {
                                    "authorId": "39455775",
                                    "name": "Omer Levy"
                                },
                                {
                                    "authorId": "1746807",
                                    "name": "Dan Jurafsky"
                                },
                                {
                                    "authorId": "1982950",
                                    "name": "Luke Zettlemoyer"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                }
                            ],
                            "year": 2019,
                            "venue": "International Conference on Learning Representations",
                            "n_citations": 842
                        },
                        "score": 0
                    },
                    {
                        "id": "(Guu et al., 2020)",
                        "snippets": [
                            "Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."
                        ],
                        "paper": {
                            "corpus_id": 211204736,
                            "title": "REALM: Retrieval-Augmented Language Model Pre-Training",
                            "authors": [
                                {
                                    "authorId": "2091768",
                                    "name": "Kelvin Guu"
                                },
                                {
                                    "authorId": "2544107",
                                    "name": "Kenton Lee"
                                },
                                {
                                    "authorId": "9941702",
                                    "name": "Zora Tung"
                                },
                                {
                                    "authorId": "2616463",
                                    "name": "Panupong Pasupat"
                                },
                                {
                                    "authorId": "1744179",
                                    "name": "Ming-Wei Chang"
                                }
                            ],
                            "year": 2020,
                            "venue": "International Conference on Machine Learning",
                            "n_citations": 2119
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lewis et al., 2020)",
                        "snippets": [
                            "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
                        ],
                        "paper": {
                            "corpus_id": 218869575,
                            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
                            "authors": [
                                {
                                    "authorId": "145222654",
                                    "name": "Patrick Lewis"
                                },
                                {
                                    "authorId": "3439053",
                                    "name": "Ethan Perez"
                                },
                                {
                                    "authorId": "1716179427",
                                    "name": "Aleksandara Piktus"
                                },
                                {
                                    "authorId": "40052301",
                                    "name": "F. Petroni"
                                },
                                {
                                    "authorId": "2067091563",
                                    "name": "Vladimir Karpukhin"
                                },
                                {
                                    "authorId": "39589154",
                                    "name": "Naman Goyal"
                                },
                                {
                                    "authorId": "103131985",
                                    "name": "Heinrich Kuttler"
                                },
                                {
                                    "authorId": "35084211",
                                    "name": "M. Lewis"
                                },
                                {
                                    "authorId": "144105277",
                                    "name": "Wen-tau Yih"
                                },
                                {
                                    "authorId": "2620211",
                                    "name": "Tim Rockt\u00e4schel"
                                },
                                {
                                    "authorId": "48662861",
                                    "name": "Sebastian Riedel"
                                },
                                {
                                    "authorId": "1743722",
                                    "name": "Douwe Kiela"
                                }
                            ],
                            "year": 2020,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 6476
                        },
                        "score": 0
                    },
                    {
                        "id": "(Gupta et al., 2024)",
                        "snippets": [
                            "Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data."
                        ],
                        "paper": {
                            "corpus_id": 273403870,
                            "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models",
                            "authors": [
                                {
                                    "authorId": "2326450664",
                                    "name": "Ambuje Gupta"
                                },
                                {
                                    "authorId": "2326298544",
                                    "name": "Mrinal Rawat"
                                },
                                {
                                    "authorId": "2306165476",
                                    "name": "A. Stolcke"
                                },
                                {
                                    "authorId": "2326296747",
                                    "name": "Roberto Pieraccini"
                                }
                            ],
                            "year": 2024,
                            "venue": "Applied Informatics",
                            "n_citations": 1
                        },
                        "score": 0.900390625
                    },
                    {
                        "id": "(Liu et al._1, 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability."
                        ],
                        "paper": {
                            "corpus_id": 273962778,
                            "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation",
                            "authors": [
                                {
                                    "authorId": "2330357180",
                                    "name": "Ziwei Liu"
                                },
                                {
                                    "authorId": "2290883726",
                                    "name": "Liangyin Zhang"
                                },
                                {
                                    "authorId": "2342500515",
                                    "name": "Qian Li"
                                },
                                {
                                    "authorId": "2333407955",
                                    "name": "Jianghua Wu"
                                },
                                {
                                    "authorId": "2330382982",
                                    "name": "Guangxu Zhu"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.89501953125
                    },
                    {
                        "id": "(Wang et al._1, 2024)",
                        "snippets": [
                            "Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities",
                            "Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance."
                        ],
                        "paper": {
                            "corpus_id": 274776379,
                            "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models",
                            "authors": [
                                {
                                    "authorId": "2309429494",
                                    "name": "Yujin Wang"
                                },
                                {
                                    "authorId": "2310858751",
                                    "name": "Quanfeng Liu"
                                },
                                {
                                    "authorId": "1934959729",
                                    "name": "Jiaqi Fan"
                                },
                                {
                                    "authorId": "9451675",
                                    "name": "Jinlong Hong"
                                },
                                {
                                    "authorId": "2309098",
                                    "name": "Hongqing Chu"
                                },
                                {
                                    "authorId": "30849412",
                                    "name": "Mengjian Tian"
                                },
                                {
                                    "authorId": "2292395467",
                                    "name": "Bingzhao Gao"
                                },
                                {
                                    "authorId": "2238391457",
                                    "name": "Hong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.94384765625
                    },
                    {
                        "id": "(Barnett et al., 2024)",
                        "snippets": [
                            "This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI."
                        ],
                        "paper": {
                            "corpus_id": 270560495,
                            "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models",
                            "authors": [
                                {
                                    "authorId": "2279752649",
                                    "name": "Scott Barnett"
                                },
                                {
                                    "authorId": "2279020735",
                                    "name": "Zach Brannelly"
                                },
                                {
                                    "authorId": "2266469333",
                                    "name": "Stefanus Kurniawan"
                                },
                                {
                                    "authorId": "2307101480",
                                    "name": "Sheng Wong"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.9296875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Technical Innovations in RAG and Fine-tuning Research",
                "tldr": "Recent technical innovations in RAG systems include adaptive retrieval approaches like FLARE and Self-RAG that dynamically assess and improve content quality during generation. Novel frameworks such as RbFT, ALoFTRAG, and Finetune-RAG focus on enhancing model resilience against retrieval defects and improving accuracy without manual labeling. (9 sources)",
                "text": "\n- **Adaptive Retrieval Mechanisms**: Recent advances have shifted from static retrieval to adaptive approaches that dynamically assess and modify retrieval during generation. FLARE uses confidence scores to identify when additional knowledge is needed, retrieving external information for low-confidence sentences and regenerating them. Similarly, Self-RAG incorporates special tokens that enable the model to adaptively retrieve information and reflect on the quality of generated content, ensuring only relevant information is included based on the model's self-evaluation. Another approach, SuRe, generates conditional summarizations of retrievals and evaluates them with carefully designed prompts. <Paper corpusId=\"270688152\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>\n\n- **Robust Fine-Tuning (RbFT)**: This innovative method addresses a fundamental limitation of RAG systems: their dependence on reliable retrievers and knowledge bases. RbFT enhances LLM resilience against retrieval defects through targeted fine-tuning tasks specifically designed to handle noisy, irrelevant, or misleading counterfactual information that might be retrieved in real-world scenarios. This approach represents a significant advancement in making RAG systems more trustworthy when faced with imperfect retrieval components. <Paper corpusId=\"275993994\" paperTitle=\"(Tu et al., 2025)\" isShortName></Paper>\n\n- **Automatic Local Fine-Tuning of RAG (ALoFTRAG)**: This framework improves RAG accuracy on specific domains without requiring manually labeled data or larger teacher models. ALoFTRAG generates and filters synthetic training data and performs LoRA fine-tuning, demonstrating significant improvements in citation accuracy (8.3%) and answer accuracy (3.0%) across 20 datasets in 26 languages. This approach provides a practical, cost-effective solution particularly valuable for sensitive domains like healthcare and finance. <Paper corpusId=\"275788867\" paperTitle=\"(Devine, 2025)\" isShortName></Paper>\n\n- **Self-Memory Generation (SelfMem)**: This framework moves beyond the limitations of fixed corpus retrieval by iteratively employing a retrieval-augmented generator to create an unbounded memory pool. A memory selector then chooses one output as memory for subsequent generation rounds, enabling the model to leverage its own output as self-memory. This approach has achieved state-of-the-art results in multiple text generation tasks including neural machine translation, abstractive text summarization, and dialogue generation. <Paper corpusId=\"276449952\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258479968\" paperTitle=\"(Cheng et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: This specialized fine-tuning method trains LLMs to distinguish between correct and fictitious context within RAG systems. Unlike approaches that focus on improving retrieval quality, Finetune-RAG targets the generation behavior of models when faced with imperfect or misleading inputs, addressing a critical challenge in RAG implementation. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **Knowledge Selection Optimization**: While substantial research has focused on improving knowledge retrieval for RAG systems, emerging work highlights the importance of knowledge selection\u2014determining which retrieved information to actually use in generation. This represents an underexplored area with significant potential for improving RAG performance by ensuring only the most relevant retrieved information influences generation. <Paper corpusId=\"273403839\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Comprehensive Experimental Frameworks**: Despite growing interest in RAG techniques, researchers have identified a significant gap in the literature: the lack of comprehensive experimental comparisons across the spectrum of advanced RAG techniques. Current literature primarily consists of systematic reviews and direct comparisons between successive state-of-the-art models, indicating a need for more holistic evaluation approaches that can better guide future development. <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>",
                "citations": [
                    {
                        "id": "(Lyu et al., 2024)",
                        "snippets": [
                            "Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts."
                        ],
                        "paper": {
                            "corpus_id": 270688152,
                            "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation",
                            "authors": [
                                {
                                    "authorId": "2187857206",
                                    "name": "Yuanjie Lyu"
                                },
                                {
                                    "authorId": "2307915655",
                                    "name": "Zihan Niu"
                                },
                                {
                                    "authorId": "2202470155",
                                    "name": "Zheyong Xie"
                                },
                                {
                                    "authorId": "2260850374",
                                    "name": "Chao Zhang"
                                },
                                {
                                    "authorId": "2277237058",
                                    "name": "Tong Xu"
                                },
                                {
                                    "authorId": "2308313519",
                                    "name": "Yang Wang"
                                },
                                {
                                    "authorId": "2265580543",
                                    "name": "Enhong Chen"
                                }
                            ],
                            "year": 2024,
                            "venue": "Conference on Empirical Methods in Natural Language Processing",
                            "n_citations": 11
                        },
                        "score": 0.9052734375
                    },
                    {
                        "id": "(Anaissi et al., 2024)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]",
                            "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches",
                            "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation",
                            "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."
                        ],
                        "paper": {
                            "corpus_id": 273501949,
                            "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services",
                            "authors": [
                                {
                                    "authorId": "3333168",
                                    "name": "Ali Anaissi"
                                },
                                {
                                    "authorId": "3069261",
                                    "name": "Ali Braytee"
                                },
                                {
                                    "authorId": "1992906806",
                                    "name": "Junaid Akram"
                                }
                            ],
                            "year": 2024,
                            "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)",
                            "n_citations": 3
                        },
                        "score": 0.8994140625
                    },
                    {
                        "id": "(Tu et al., 2025)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks."
                        ],
                        "paper": {
                            "corpus_id": 275993994,
                            "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects",
                            "authors": [
                                {
                                    "authorId": "2275628230",
                                    "name": "Yiteng Tu"
                                },
                                {
                                    "authorId": "2147219374",
                                    "name": "Weihang Su"
                                },
                                {
                                    "authorId": "2290870875",
                                    "name": "Yujia Zhou"
                                },
                                {
                                    "authorId": "2260835922",
                                    "name": "Yiqun Liu"
                                },
                                {
                                    "authorId": "2256982003",
                                    "name": "Qingyao Ai"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 6
                        },
                        "score": 0.95849609375
                    },
                    {
                        "id": "(Devine, 2025)",
                        "snippets": [
                            "We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance."
                        ],
                        "paper": {
                            "corpus_id": 275788867,
                            "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2341534946",
                                    "name": "Peter Devine"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 0
                        },
                        "score": 0.9384765625
                    },
                    {
                        "id": "(Zhang et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. (Lewis et al., 2020) introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval (Cheng et al., 2023)[36] and enhancing generation capabilities (Anderson et al., 2022)[17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself."
                        ],
                        "paper": {
                            "corpus_id": 276449952,
                            "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue",
                            "authors": [
                                {
                                    "authorId": "2343693016",
                                    "name": "Feiyuan Zhang"
                                },
                                {
                                    "authorId": "2346899749",
                                    "name": "Dezhi Zhu"
                                },
                                {
                                    "authorId": "2346111453",
                                    "name": "James Ming"
                                },
                                {
                                    "authorId": "2297644069",
                                    "name": "Yilun Jin"
                                },
                                {
                                    "authorId": "2064208321",
                                    "name": "Di Chai"
                                },
                                {
                                    "authorId": "2145494870",
                                    "name": "Liu Yang"
                                },
                                {
                                    "authorId": "2308447281",
                                    "name": "Han Tian"
                                },
                                {
                                    "authorId": "2346645031",
                                    "name": "Zhaoxin Fan"
                                },
                                {
                                    "authorId": "2343460791",
                                    "name": "Kai Chen"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 2
                        },
                        "score": 0.90234375
                    },
                    {
                        "id": "(Cheng et al., 2023)",
                        "snippets": [
                            "With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research."
                        ],
                        "paper": {
                            "corpus_id": 258479968,
                            "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory",
                            "authors": [
                                {
                                    "authorId": "2193630544",
                                    "name": "Xin Cheng"
                                },
                                {
                                    "authorId": "2215612529",
                                    "name": "Di Luo"
                                },
                                {
                                    "authorId": "2116950235",
                                    "name": "Xiuying Chen"
                                },
                                {
                                    "authorId": "2978364",
                                    "name": "Lemao Liu"
                                },
                                {
                                    "authorId": "144060462",
                                    "name": "Dongyan Zhao"
                                },
                                {
                                    "authorId": "144539156",
                                    "name": "Rui Yan"
                                }
                            ],
                            "year": 2023,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 102
                        },
                        "score": 0
                    },
                    {
                        "id": "(Lee et al., 2025)",
                        "snippets": [
                            "We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs."
                        ],
                        "paper": {
                            "corpus_id": 278714952,
                            "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation",
                            "authors": [
                                {
                                    "authorId": "2362089035",
                                    "name": "Zhan Peng Lee"
                                },
                                {
                                    "authorId": "2362188632",
                                    "name": "Andre Lin"
                                },
                                {
                                    "authorId": "2363425126",
                                    "name": "Calvin Tan"
                                }
                            ],
                            "year": 2025,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.95068359375
                    },
                    {
                        "id": "(Li et al._1, 2024)",
                        "snippets": [
                            "Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear."
                        ],
                        "paper": {
                            "corpus_id": 273403839,
                            "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?",
                            "authors": [
                                {
                                    "authorId": "89919188",
                                    "name": "Xiangci Li"
                                },
                                {
                                    "authorId": "2284862335",
                                    "name": "Jessica Ouyang"
                                }
                            ],
                            "year": 2024,
                            "venue": "",
                            "n_citations": 0
                        },
                        "score": 0.89013671875
                    },
                    {
                        "id": "(Eibich et al., 2024)",
                        "snippets": [
                            "Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing."
                        ],
                        "paper": {
                            "corpus_id": 268819923,
                            "title": "ARAGOG: Advanced RAG Output Grading",
                            "authors": [
                                {
                                    "authorId": "2294361167",
                                    "name": "Matouvs Eibich"
                                },
                                {
                                    "authorId": "2294361283",
                                    "name": "Shivay Nagpal"
                                },
                                {
                                    "authorId": "2294362877",
                                    "name": "Alexander Fred-Ojala"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 4
                        },
                        "score": 0.92138671875
                    }
                ],
                "format": "list",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Limitations and Challenges",
                "tldr": "Despite their advantages, both RAG and fine-tuning approaches face significant technical limitations including retrieval quality issues, domain adaptation challenges, and computational overhead. The integration of these methods introduces additional complexities like decreased performance in combined systems and difficulties in effectively balancing retrieved knowledge with model parameters. (5 sources)",
                "text": "\nDespite the impressive results demonstrated by both RAG and fine-tuning approaches, researchers have identified several significant limitations and challenges that affect their practical implementation and performance. A fundamental challenge for RAG systems is their heavy dependence on retrieval quality and accuracy. Poor-quality or irrelevant retrieval results can significantly degrade the performance of RAG models, negatively impacting the generated content <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. This retrieval dependency represents a critical vulnerability in RAG implementations, as their effectiveness is directly tied to the retrieval component's ability to surface relevant information.\n\nAnother significant challenge involves effectively integrating retrieved information with the model's prior knowledge, which remains an open research problem <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Finding the optimal balance between leveraging external information and the model's parametric knowledge is not straightforward, particularly when retrieved information contradicts what the model has learned during pre-training. This integration challenge becomes even more pronounced when attempting to combine RAG with fine-tuning approaches, as the interaction between updated model parameters and retrieval mechanisms can produce unexpected results.\n\nDomain adaptation presents a persistent challenge for both approaches. While RAG has shown impressive results in general contexts, adapting it to specialized domains requires careful optimization of both retrieval and generation components <Paper corpusId=\"269149041\" paperTitle=\"(Weng, 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. The original RAG model was primarily trained and explored with Wikipedia-based knowledge, requiring significant modifications for effective deployment in specialized domains such as healthcare or news <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. This has led researchers to develop domain-specific variations like RAG-end2end that can adapt to domain-specific knowledge bases by updating all components during training <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\nThe evaluation of RAG systems presents another substantial challenge due to the complex interaction between retrieval and generation components <Paper corpusId=\"278033562\" paperTitle=\"(Park et al., 2025)\" isShortName></Paper>. The scarcity of benchmarks that facilitate detailed, component-specific assessment has hindered systematic comparison and improvement of RAG implementations <Paper corpusId=\"278033562\" paperTitle=\"(Park et al., 2025)\" isShortName></Paper>. This evaluation challenge extends to comparative studies between RAG and fine-tuning approaches, where the multifaceted nature of performance metrics (factuality, coherence, relevance, etc.) makes comprehensive assessment difficult <Paper corpusId=\"273549218\" paperTitle=\"(Liu et al._2, 2024)\" isShortName></Paper>.\n\nComputational and storage requirements present practical constraints for both approaches. RAG systems typically require maintaining and searching through large document collections, which can introduce latency during inference and significant storage overhead. Fine-tuning approaches, while generally more computationally efficient during inference, require substantial resources during the training phase and result in multiple model versions for different domains or tasks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. These resource constraints become particularly relevant in production environments where efficiency and scalability are critical considerations.\n\nFor combined approaches that integrate RAG with fine-tuning, researchers have identified an additional challenge: the fine-tuned models sometimes underperform baseline models when used in RAG contexts, contrary to the typical improvements observed in standalone LLM applications <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This suggests that the interaction between fine-tuning and retrieval mechanisms may introduce unexpected complications that require careful consideration and specialized optimization strategies.",
                "citations": [
                    {
                        "id": "(Zhao et al., 2024)",
                        "snippets": [
                            "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge."
                        ],
                        "paper": {
                            "corpus_id": 271329121,
                            "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought",
                            "authors": [
                                {
                                    "authorId": "2312343839",
                                    "name": "Yuetong Zhao"
                                },
                                {
                                    "authorId": "2312344958",
                                    "name": "Hongyu Cao"
                                },
                                {
                                    "authorId": "2312340862",
                                    "name": "Xianyu Zhao"
                                },
                                {
                                    "authorId": "2243267608",
                                    "name": "Zhijian Ou"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Symposium on Chinese Spoken Language Processing",
                            "n_citations": 4
                        },
                        "score": 0.90380859375
                    },
                    {
                        "id": "(Weng, 2024)",
                        "snippets": [
                            "\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"
                        ],
                        "paper": {
                            "corpus_id": 269149041,
                            "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
                            "authors": [
                                {
                                    "authorId": "2296715370",
                                    "name": "Benjue Weng"
                                }
                            ],
                            "year": 2024,
                            "venue": "arXiv.org",
                            "n_citations": 10
                        },
                        "score": 0.892578125
                    },
                    {
                        "id": "(Siriwardhana et al., 2022)",
                        "snippets": [
                            "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."
                        ],
                        "paper": {
                            "corpus_id": 252735056,
                            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
                            "authors": [
                                {
                                    "authorId": "51516859",
                                    "name": "Shamane Siriwardhana"
                                },
                                {
                                    "authorId": "52001535",
                                    "name": "Rivindu Weerasekera"
                                },
                                {
                                    "authorId": "2114425044",
                                    "name": "Elliott Wen"
                                },
                                {
                                    "authorId": "1992921690",
                                    "name": "Tharindu Kaluarachchi"
                                },
                                {
                                    "authorId": "1814487",
                                    "name": "R. Rana"
                                },
                                {
                                    "authorId": "1486464114",
                                    "name": "Suranga Nanayakkara"
                                }
                            ],
                            "year": 2022,
                            "venue": "Transactions of the Association for Computational Linguistics",
                            "n_citations": 179
                        },
                        "score": 0
                    },
                    {
                        "id": "(Park et al., 2025)",
                        "snippets": [
                            "Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment."
                        ],
                        "paper": {
                            "corpus_id": 278033562,
                            "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation",
                            "authors": [
                                {
                                    "authorId": "2357698631",
                                    "name": "Chanhee Park"
                                },
                                {
                                    "authorId": "2101318920",
                                    "name": "Hyeonseok Moon"
                                },
                                {
                                    "authorId": "2115195904",
                                    "name": "Chanjun Park"
                                },
                                {
                                    "authorId": "83056580",
                                    "name": "Heu-Jeoung Lim"
                                }
                            ],
                            "year": 2025,
                            "venue": "North American Chapter of the Association for Computational Linguistics",
                            "n_citations": 1
                        },
                        "score": 0.92626953125
                    },
                    {
                        "id": "(Liu et al._2, 2024)",
                        "snippets": [
                            "Our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models."
                        ],
                        "paper": {
                            "corpus_id": 273549218,
                            "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models",
                            "authors": [
                                {
                                    "authorId": "2327775708",
                                    "name": "Fengchen Liu"
                                },
                                {
                                    "authorId": "2327510830",
                                    "name": "Jordan Jung"
                                },
                                {
                                    "authorId": "2327335509",
                                    "name": "Wei Feinstein"
                                },
                                {
                                    "authorId": "2327336236",
                                    "name": "Jeff DAmbrogia"
                                },
                                {
                                    "authorId": "2327334401",
                                    "name": "Gary Jung"
                                }
                            ],
                            "year": 2024,
                            "venue": "International Conference on AI-ML-Systems",
                            "n_citations": 1
                        },
                        "score": 0.93505859375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.18575999999999998
    }
}