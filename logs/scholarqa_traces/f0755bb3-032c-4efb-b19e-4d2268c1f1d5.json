{
    "query": "Many papers optimal scaling laws training compute Has anyone optimality for inference time compute For instance self consistency samples many outputs find right answers effective ca just stronger model",
    "user_id": "lib_user",
    "task_id": "f0755bb3-032c-4efb-b19e-4d2268c1f1d5",
    "timestamp": "2025-06-23T21:24:37.410485",
    "n_retrieval": 256,
    "n_retrieved": 263,
    "n_candidates": 3,
    "n_rerank": 50,
    "opt_in": true,
    "total_cost": 0.065097,
    "decomposed_query": {
        "rewritten_query": "Optimal scaling laws for training compute versus optimality for inference time compute, including self-consistency sampling to find right answers compared to using stronger models.",
        "keyword_query": "optimal scaling laws training compute inference time self consistency sampling stronger model",
        "search_filters": {
            "fieldsOfStudy": "Computer Science"
        },
        "cost": 0.009861,
        "model": "claude-3-7-sonnet-20250219"
    },
    "candidates": [
        {
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "venue": "arXiv.org",
            "year": 2025,
            "reference_count": 24,
            "citation_count": 1,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2501.02156, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2338865687",
                    "name": "Chien-Ping Lu"
                }
            ],
            "abstract": "As large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.",
            "corpus_id": 275336968,
            "sentences": [
                {
                    "corpus_id": "275336968",
                    "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                    "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
                    "score": 0.577838306597307,
                    "section_title": "Related Work",
                    "char_start_offset": 4305,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 151
                        },
                        {
                            "start": 152,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 425
                        },
                        {
                            "start": 426,
                            "end": 595
                        },
                        {
                            "start": 596,
                            "end": 744
                        },
                        {
                            "start": 747,
                            "end": 851
                        },
                        {
                            "start": 852,
                            "end": 1081
                        },
                        {
                            "start": 1082,
                            "end": 1173
                        },
                        {
                            "start": 1174,
                            "end": 1347
                        },
                        {
                            "start": 1348,
                            "end": 1516
                        },
                        {
                            "start": 1517,
                            "end": 1636
                        },
                        {
                            "start": 1637,
                            "end": 1831
                        },
                        {
                            "start": 1832,
                            "end": 2046
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 1221,
                            "end": 1225,
                            "matchedPaperCorpusId": "246473179"
                        },
                        {
                            "start": 1803,
                            "end": 1806,
                            "matchedPaperCorpusId": "6519532"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.705078125
                }
            ],
            "relevance_judgement": 0.705078125,
            "relevance_judgment_input_expanded": "# Title: The Race to Efficiency: A New Perspective on AI Scaling Laws\n# Venue: arXiv.org\n# Authors: Chien-Ping Lu\n## Abstract\nAs large-scale AI models expand, training becomes costlier and sustaining progress grows harder. Classical scaling laws (e.g., Kaplan et al. (2020), Hoffmann et al. (2022)) predict training loss from a static compute budget yet neglect time and efficiency, prompting the question: how can we balance ballooning GPU fleets with rapidly improving hardware and algorithms? We introduce the relative-loss equation, a time- and efficiency-aware framework that extends classical AI scaling laws. Our model shows that, without ongoing efficiency gains, advanced performance could demand millennia of training or unrealistically large GPU fleets. However, near-exponential progress remains achievable if the\"efficiency-doubling rate\"parallels Moore's Law. By formalizing this race to efficiency, we offer a quantitative roadmap for balancing front-loaded GPU investments with incremental improvements across the AI stack. Empirical trends suggest that sustained efficiency gains can push AI scaling well into the coming decade, providing a new perspective on the diminishing returns inherent in classical scaling.\n## Related Work\nThe study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "reference_string": "[275336968 | Lu | 2025 | Citations: 1]"
        },
        {
            "title": "Scaling Laws for Linear Complexity Language Models",
            "venue": "Conference on Empirical Methods in Natural Language Processing",
            "year": 2024,
            "reference_count": 57,
            "citation_count": 8,
            "influential_citation_count": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.16690, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2116517206",
                    "name": "Xuyang Shen"
                },
                {
                    "authorId": "2179703418",
                    "name": "Dong Li"
                },
                {
                    "authorId": "2308039623",
                    "name": "Ruitao Leng"
                },
                {
                    "authorId": "2171650015",
                    "name": "Zhen Qin"
                },
                {
                    "authorId": "2225238340",
                    "name": "Weigao Sun"
                },
                {
                    "authorId": "2266275708",
                    "name": "Yiran Zhong"
                }
            ],
            "abstract": "The interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for comparison with softmax attention. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.",
            "corpus_id": 270703481,
            "sentences": [
                {
                    "corpus_id": "270703481",
                    "title": "Scaling Laws for Linear Complexity Language Models",
                    "text": "Scaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
                    "score": 0.542546780489402,
                    "section_title": "Related work",
                    "char_start_offset": 22035,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 286
                        },
                        {
                            "start": 286,
                            "end": 497
                        },
                        {
                            "start": 497,
                            "end": 623
                        },
                        {
                            "start": 623,
                            "end": 918
                        },
                        {
                            "start": 918,
                            "end": 1039
                        },
                        {
                            "start": 1039,
                            "end": 1214
                        },
                        {
                            "start": 1214,
                            "end": 1388
                        },
                        {
                            "start": 1388,
                            "end": 1524
                        }
                    ],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.5947265625
                }
            ],
            "relevance_judgement": 0.5947265625,
            "relevance_judgment_input_expanded": "# Title: Scaling Laws for Linear Complexity Language Models\n# Venue: Conference on Empirical Methods in Natural Language Processing\n# Authors: Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, Yiran Zhong\n## Abstract\nThe interest in linear complexity models for large language models is on the rise, although their scaling capacity remains uncertain. In this study, we present the scaling laws for linear complexity language models to establish a foundation for their scalability. Specifically, we examine the scaling behaviors of three efficient linear architectures. These include TNL, a linear attention model with data-independent decay; HGRN2, a linear RNN with data-dependent decay; and cosFormer2, a linear attention model without decay. We also include LLaMA as a baseline architecture for comparison with softmax attention. These models were trained with six variants, ranging from 70M to 7B parameters on a 300B-token corpus, and evaluated with a total of 1,376 intermediate checkpoints on various downstream tasks. These tasks include validation loss, commonsense reasoning, and information retrieval and generation. The study reveals that existing linear complexity language models exhibit similar scaling capabilities as conventional transformer-based models while also demonstrating superior linguistic proficiency and knowledge retention.\n## Related work\nScaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
            "reference_string": "[270703481 | Shen et al. | 2024 | Citations: 8]"
        },
        {
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "venue": "Neural Information Processing Systems",
            "year": 2024,
            "reference_count": 62,
            "citation_count": 26,
            "influential_citation_count": 4,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2406.19146, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2308470091",
                    "name": "Tomer Porian"
                },
                {
                    "authorId": "52193502",
                    "name": "Mitchell Wortsman"
                },
                {
                    "authorId": "2191688",
                    "name": "J. Jitsev"
                },
                {
                    "authorId": "2253541812",
                    "name": "Ludwig Schmidt"
                },
                {
                    "authorId": "2444742",
                    "name": "Y. Carmon"
                }
            ],
            "abstract": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.",
            "corpus_id": 270764838,
            "sentences": [
                {
                    "corpus_id": "270764838",
                    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                    "text": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
                    "score": 0.6972994981730255,
                    "section_title": "Related work",
                    "char_start_offset": 24085,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 88
                        },
                        {
                            "start": 89,
                            "end": 362
                        },
                        {
                            "start": 363,
                            "end": 556
                        },
                        {
                            "start": 557,
                            "end": 676
                        },
                        {
                            "start": 679,
                            "end": 765
                        },
                        {
                            "start": 766,
                            "end": 955
                        },
                        {
                            "start": 956,
                            "end": 1186
                        },
                        {
                            "start": 1187,
                            "end": 1355
                        },
                        {
                            "start": 1356,
                            "end": 1431
                        },
                        {
                            "start": 1432,
                            "end": 1602
                        },
                        {
                            "start": 1603,
                            "end": 1701
                        },
                        {
                            "start": 1702,
                            "end": 1917
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 514,
                            "end": 518,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 636,
                            "end": 639,
                            "matchedPaperCorpusId": "258888192"
                        },
                        {
                            "start": 1199,
                            "end": 1203,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1299,
                            "end": 1303,
                            "matchedPaperCorpusId": "254636568"
                        },
                        {
                            "start": 1453,
                            "end": 1456,
                            "matchedPaperCorpusId": "258832817"
                        },
                        {
                            "start": 1616,
                            "end": 1620,
                            "matchedPaperCorpusId": "269033049"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.54443359375
                },
                {
                    "corpus_id": "270764838",
                    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                    "text": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.",
                    "score": 0.5430443496781514,
                    "section_title": "abstract",
                    "char_start_offset": 0,
                    "sentence_offsets": [],
                    "ref_mentions": [],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.327880859375
                },
                {
                    "corpus_id": "270764838",
                    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                    "text": "We consider the problem of compute-optimal language model training: given a compute budget C, we wish to predict how to best allocate it across model size (in parameters) and dataset size (in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al. [30] proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of C. This scaling law was influential in determining the size of GPT-3 and several subsequent models [12,51,43,32,47,62,52]. However, Hoffmann et al. [25] challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of C, and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget. \n\nWhile Hoffmann et al. [25] and subsequent work [55,56,17,26,15] established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand why the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling. \n\nHoffmann et al. [25] hypothesize that the scaling law discrepancy is due to Kaplan et al. [30] not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. [25]'s hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far. \n\nOur contribution. In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our main  Cosine decay (no tuning) results.",
                    "score": 0.5735739869532981,
                    "section_title": "Introduction",
                    "char_start_offset": 15,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 200
                        },
                        {
                            "start": 201,
                            "end": 305
                        },
                        {
                            "start": 306,
                            "end": 452
                        },
                        {
                            "start": 453,
                            "end": 572
                        },
                        {
                            "start": 573,
                            "end": 825
                        },
                        {
                            "start": 826,
                            "end": 973
                        },
                        {
                            "start": 976,
                            "end": 1242
                        },
                        {
                            "start": 1243,
                            "end": 1369
                        },
                        {
                            "start": 1370,
                            "end": 1468
                        },
                        {
                            "start": 1471,
                            "end": 1640
                        },
                        {
                            "start": 1641,
                            "end": 1795
                        },
                        {
                            "start": 1796,
                            "end": 1967
                        },
                        {
                            "start": 1970,
                            "end": 1987
                        },
                        {
                            "start": 1988,
                            "end": 2204
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 549,
                            "end": 553,
                            "matchedPaperCorpusId": "218971783"
                        },
                        {
                            "start": 598,
                            "end": 602,
                            "matchedPaperCorpusId": "258509679"
                        },
                        {
                            "start": 998,
                            "end": 1002,
                            "matchedPaperCorpusId": "258509679"
                        },
                        {
                            "start": 1487,
                            "end": 1491,
                            "matchedPaperCorpusId": "258509679"
                        },
                        {
                            "start": 1831,
                            "end": 1835,
                            "matchedPaperCorpusId": "258509679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.28955078125
                },
                {
                    "corpus_id": "270764838",
                    "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                    "text": "Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28]. In particular, Paquette et al. [38] obtain a power law with exponent a = 0.5 (as in Hoffmann et al. [25]) for a random-feature linear regression setting [35,5], conjecturing that this is a part of a broader, universal phenomenon. Jeon and Van Roy [28] also establish an exponent of 0.5 for data generated by infinite-width two-layer ReLU networks, using information-theoretic arguments. \n\nWe also remark on two themes of our paper that draw from prior work. The first is the importance of hyperparameter tuning: several works [30,27,17,15] make the case that smooth, predictable scaling laws emerge when models on all scales are properly tuned. Our work (and particularly Section 4.1) provides another example of this principle and agrees with previous observations that tuning is particularly important at smaller scales. Second, previous studies [59,8,15,26] as well as the concurrent work [22], propose alternative learning rate schedules that address a key shortcoming of cosine decay: the need to commit to a step budget in advance. We consider a constant learning rate that requires no commitment at all. We show this simple choice suffices to reproduce the Hoffmann et al. law and quantify the computational savings compared to a cosine schedule. However, Section 4.1 (and also [25], among others) show that in terms of loss, the constant schedule clearly underperforms the cosine schedule. \n\nConcurrent and independent work. Pearce and Song [40] [30] (see discussion in Section 3.1, Section 3.5, and Appendix H). In addition, our experiments roughly match the Kaplan et al. [30] compute budget, which is about 3 orders of magnitudes larger than budget in Pearce and Song [40], and we perform careful tuning of both the learning rate and the batch size.",
                    "score": 0.5017287521299295,
                    "section_title": "Related work",
                    "char_start_offset": 25787,
                    "sentence_offsets": [
                        {
                            "start": 0,
                            "end": 215
                        },
                        {
                            "start": 216,
                            "end": 445
                        },
                        {
                            "start": 446,
                            "end": 602
                        },
                        {
                            "start": 605,
                            "end": 673
                        },
                        {
                            "start": 674,
                            "end": 860
                        },
                        {
                            "start": 861,
                            "end": 1038
                        },
                        {
                            "start": 1039,
                            "end": 1253
                        },
                        {
                            "start": 1254,
                            "end": 1326
                        },
                        {
                            "start": 1327,
                            "end": 1469
                        },
                        {
                            "start": 1470,
                            "end": 1613
                        },
                        {
                            "start": 1616,
                            "end": 1648
                        },
                        {
                            "start": 1649,
                            "end": 1736
                        },
                        {
                            "start": 1737,
                            "end": 1976
                        }
                    ],
                    "ref_mentions": [
                        {
                            "start": 25,
                            "end": 28,
                            "matchedPaperCorpusId": "259243822"
                        },
                        {
                            "start": 201,
                            "end": 205,
                            "matchedPaperCorpusId": "267406160"
                        },
                        {
                            "start": 316,
                            "end": 320,
                            "matchedPaperCorpusId": "258509679"
                        },
                        {
                            "start": 373,
                            "end": 375,
                            "matchedPaperCorpusId": "255096726"
                        },
                        {
                            "start": 746,
                            "end": 749,
                            "matchedPaperCorpusId": "246823711"
                        },
                        {
                            "start": 1064,
                            "end": 1068,
                            "matchedPaperCorpusId": "235367962"
                        },
                        {
                            "start": 1501,
                            "end": 1505,
                            "matchedPaperCorpusId": "258509679"
                        }
                    ],
                    "pdf_hash": "",
                    "stype": "vespa",
                    "rerank_score": 0.274169921875
                }
            ],
            "relevance_judgement": 0.54443359375,
            "relevance_judgment_input_expanded": "# Title: Resolving Discrepancies in Compute-Optimal Scaling of Language Models\n# Venue: Neural Information Processing Systems\n# Authors: Tomer Porian, Mitchell Wortsman, J. Jitsev, Ludwig Schmidt, Y. Carmon\n## Abstract\nKaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.\n## Introduction\nWe consider the problem of compute-optimal language model training: given a compute budget C, we wish to predict how to best allocate it across model size (in parameters) and dataset size (in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al. [30] proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of C. This scaling law was influential in determining the size of GPT-3 and several subsequent models [12,51,43,32,47,62,52]. However, Hoffmann et al. [25] challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of C, and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget. \n\nWhile Hoffmann et al. [25] and subsequent work [55,56,17,26,15] established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand why the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling. \n\nHoffmann et al. [25] hypothesize that the scaling law discrepancy is due to Kaplan et al. [30] not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. [25]'s hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far. \n\nOur contribution. In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our main  Cosine decay (no tuning) results.\n\n## Related work\nRecent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].\n...\nFinally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28]. In particular, Paquette et al. [38] obtain a power law with exponent a = 0.5 (as in Hoffmann et al. [25]) for a random-feature linear regression setting [35,5], conjecturing that this is a part of a broader, universal phenomenon. Jeon and Van Roy [28] also establish an exponent of 0.5 for data generated by infinite-width two-layer ReLU networks, using information-theoretic arguments. \n\nWe also remark on two themes of our paper that draw from prior work. The first is the importance of hyperparameter tuning: several works [30,27,17,15] make the case that smooth, predictable scaling laws emerge when models on all scales are properly tuned. Our work (and particularly Section 4.1) provides another example of this principle and agrees with previous observations that tuning is particularly important at smaller scales. Second, previous studies [59,8,15,26] as well as the concurrent work [22], propose alternative learning rate schedules that address a key shortcoming of cosine decay: the need to commit to a step budget in advance. We consider a constant learning rate that requires no commitment at all. We show this simple choice suffices to reproduce the Hoffmann et al. law and quantify the computational savings compared to a cosine schedule. However, Section 4.1 (and also [25], among others) show that in terms of loss, the constant schedule clearly underperforms the cosine schedule. \n\nConcurrent and independent work. Pearce and Song [40] [30] (see discussion in Section 3.1, Section 3.5, and Appendix H). In addition, our experiments roughly match the Kaplan et al. [30] compute budget, which is about 3 orders of magnitudes larger than budget in Pearce and Song [40], and we perform careful tuning of both the learning rate and the batch size.",
            "reference_string": "[270764838 | Porian et al. | 2024 | Citations: 26]"
        }
    ],
    "retrieved": [
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models. Gadre et al. [17] directly predict the loss and downstream performance for models trained past the point of compute optimality, and Muennighoff et al. [37] model joint compute-data bottlenecks. All three works rely on the Hoffmann et al. law as a reference point, with [17,37] baking it to their parametric forms. \n\nCompute-optimal scaling is studied beyond the language domain, particularly in vision. Henighan et al. [23] study autoregressive modeling for a variety of tasks and find scaling laws roughly consistent with the Kaplan et al. [30] adjusted scaling law (with exponent a = 0.73). That work shares the methodological issues described in the top row of Figure 1 (FLOP count and long warmup), but performs hyperparameter tuning for smaller scale models; in Appendix H we reach similar results when doing the same. Zhai et al. [59] characterize the compute-efficient frontier of Vision Transformers (ViTs), while Cherti et al. [13] studies compute constrained scaling of CLIP models. However, they do not offer a power law for scaling model size with compute. Alabdulmohsin et al. [4] tackle model design under an inference compute constraint by fitting multi-term parametric forms to obtain predictions for the optimal ViT shape. Goyal et al. [20] point out an intricate interplay between data filtering and compute constraints. Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28].",
            "score": 0.6972994981730255,
            "section_title": "Related work",
            "char_start_offset": 24085,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 556
                },
                {
                    "start": 557,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1917
                }
            ],
            "ref_mentions": [
                {
                    "start": 514,
                    "end": 518,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 636,
                    "end": 639,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1199,
                    "end": 1203,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1299,
                    "end": 1303,
                    "matchedPaperCorpusId": "254636568"
                },
                {
                    "start": 1453,
                    "end": 1456,
                    "matchedPaperCorpusId": "258832817"
                },
                {
                    "start": 1616,
                    "end": 1620,
                    "matchedPaperCorpusId": "269033049"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.54443359375
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "To compare the inference compute budgets of different models, we plot the figures with the number of FLOPs used per question during inference. We compute the inference FLOPs based on the commonly-used formula proposed by Kaplan et al. (2020). \n\nScaling law of compute-optimal inference for model size. Fig. 1 shows the relationship between inference compute and error rate for different model sizes. The error rate first decreases steadily and then starts to saturate. Initially, sampling many times from smaller models is compute-optimal. At larger compute budgets the larger models are preferable, since the performance of small models has saturated. As highlighted in the right panel of Fig. 1, the optimal model size varies based on the inference budget. We performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget. \n\nLlemma-7B achieves competitive accuracy to Llemma-34B with less compute. Fig. 4 and Fig. 5 shows the relationship between error rate and inference FLOPs for Llemma 7B and Llemma 34B using different inference strategies. Llemma-7B requires around 2\u00d7 less total FLOPs than Llemma-34B to achieve comparable accuracy. This held across inference strategies (sampling strategies, MCTS, REBASE) and tasks (MATH, GSM8K). This result suggests that, with the same training dataset and model family, generating more tokens with a suitable inference strategy using a smaller model can have more favorable cost-performance tradeoffs than using a larger model.",
            "score": 0.6816786308082612,
            "section_title": "COMPUTE-OPTIMAL MODEL SIZE",
            "char_start_offset": 21697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 242
                },
                {
                    "start": 245,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 468
                },
                {
                    "start": 469,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1068
                },
                {
                    "start": 1071,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1290
                },
                {
                    "start": 1291,
                    "end": 1384
                },
                {
                    "start": 1385,
                    "end": 1483
                },
                {
                    "start": 1484,
                    "end": 1717
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0775146484375
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings suggest that scaling inference compute with inference strategies can be more computationally efficient than scaling model parameters. Additionally, smaller models combined with advanced inference algorithms offer Pareto-optimal trade-offs in cost and performance. For example, the Llemma-7B model, when paired with our novel tree search algorithm, consistently outperforms the Llemma-34B model across all tested inference strategies on the MATH benchmark. We hope these insights contribute to a deeper understanding of inference scaling laws (test-time scaling laws) for LLMs.",
            "score": 0.6499945893908966,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2366943359375
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "Figure 1 | Summary of our main results. Left: Compute-optimal scaling for iterative self-refinement (i.e., revisions) and search. On the left, we compare the compute-optimal scaling policy for our PaLM 2-S* revision model against baselines in the revision setting (top) and the PRM search setting (bottom). We see that in the revisions case, the gap between standard best-of-N (e.g. \"parallel\") and compute-optimal scaling gradually widens, enabling compute-optimal scaling to outperform best-of-N with 4\u00d7 less test-time compute. Similarly, in the PRM search setting, we observe significant early improvements over best-of-N from compute-optimal scaling, nearly outperforming best-of-N with 4\u00d7 less compute at points. See Sections 5 and 6 for details. Right: Comparing test-time compute and model parameter scaling. We compare the performance of compute-optimal test-time scaling with PaLM 2-S* against the performance of a \u223c 14\u00d7 larger pretrained model without additional test-time compute (e.g. greedy sampling). We consider the setting where we expect  tokens of pretraining for both models and  tokens of inference. By training a larger model, we effectively multiply the FLOPs requirement for both of these terms. If we were to apply additional test-time compute with the smaller model, so as to match this larger model's FLOPs requirement, how would it compare in terms of accuracy? We see that for the revisions (top) when  << , test-time compute is often preferable to additional pretraining. However, as the inference to pretraining token ratio increases, test-time compute remains preferable on easy questions. Whereas on harder questions, pretraining is preferable in these settings. We also see a similar trend with PRM search (bottom). See Section 7 for more details. \n\nWe are interested in understanding the benefits of scaling up test-time compute.",
            "score": 0.6240792779678788,
            "section_title": "Test-time Search Against a PRM Verifier",
            "char_start_offset": 1744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 39
                },
                {
                    "start": 40,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 529
                },
                {
                    "start": 530,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 751
                },
                {
                    "start": 752,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 996
                },
                {
                    "start": 997,
                    "end": 1014
                },
                {
                    "start": 1015,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1500
                },
                {
                    "start": 1501,
                    "end": 1620
                },
                {
                    "start": 1621,
                    "end": 1694
                },
                {
                    "start": 1695,
                    "end": 1748
                },
                {
                    "start": 1749,
                    "end": 1780
                },
                {
                    "start": 1783,
                    "end": 1863
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0751953125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "We study the relationship between task performance and the amount of compute expended during inference for various model sizes, model families, and inference strategies, to form empirical inference scaling laws. These relationships let us reason about compute-optimal inference: inference configurations that give the best performance at a given compute budget. \n\nOur results lead to three main takeaways. First, we find that using a smaller model and generating more tokens in an inference strategy often outperforms using a larger model at a fixed compute budget. This has implications for models deployed in the real world, where inference compute is constrained in various ways. Specifically, it is potentially beneficial to deploy smaller models with more sophisticated inference strategies for better cost-performance trade-off. Second, we show that in the limit of infinite compute (allocated by drawing more samples), sampling-based majority voting strategies inevitably saturate to a distribution that depends on the underlying generation policy. Hence, it is of interest to alter the sampling distribution by designing an alternative inference strategy. Third, we design such an inference strategy-the novel REBASE tree search-and find it is Pareto optimal, in that it achieves the best performance across all tested compute budgets. Notably, it outperforms commonly used weighted majority voting and MCTS methods that have attracted much interest and widespread use. This finding not only shows the strength of REBASE, but also indicates that there is large headroom to improve language model performances via inference-time algorithms. \n\nAndy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning, acting, and planning in language models. In Proceedings of the 41st International Conference on Machine Learning, pp. 62138-62160, 2024.",
            "score": 0.6078587688633557,
            "section_title": "CONCLUSIONS",
            "char_start_offset": 26183,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 211
                },
                {
                    "start": 212,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 405
                },
                {
                    "start": 406,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 682
                },
                {
                    "start": 683,
                    "end": 834
                },
                {
                    "start": 835,
                    "end": 1055
                },
                {
                    "start": 1056,
                    "end": 1163
                },
                {
                    "start": 1164,
                    "end": 1343
                },
                {
                    "start": 1344,
                    "end": 1477
                },
                {
                    "start": 1478,
                    "end": 1647
                },
                {
                    "start": 1650,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1816
                },
                {
                    "start": 1817,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 1912
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "We show that the loss of over-trained models, trained past compute-optimality, is predictable.Furthermore, we propose and validate a scaling law relating loss to average downstream task performance.We hope our work will inspire others to further examine the relationship between model training and downstream generalization.Our testbed will be made publicly available, and we hope it will make scaling research more accessible to researchers and practitioners alike.\n\nThis is equal to Equation ( 4), making the substitutions \u03b7 = \u03b1/2, a = A(1/6) \u2212\u03b7 , b = B(1/6) \u2212\u03b7 , as noted in the main body.\n\nRelation to compute-optimal training.Recall that we made the assumption \u03b1 = \u03b2, which implies equal scaling of parameters and tokens to realize compute-optimal models.While this assumption is empirically justified [45], even if \u03b1 \u0338 = \u03b2, we get a parameterization that implies the power law exponent in Equation ( 4) remains constant with over-training, while the power law scalar changes.\n\nTo find a compute-optimal training setting, Hoffmann et al. [45] propose to minimize the right-hand side of Equation We now deviate from compute-optimal training by modifying the model size and tokens by multiplication with a constant \u221a m, according to\n\nThis modification keeps the compute constant (i.e., 6N m D m = 6N * D * ).The risk, then, becomes\n\nWe again expect the same power law exponent and changing power law scalar.Note that m in Equation ( 8) is similar to M in Equation ( 4).Specifically, m is a multiple of the Chinchilla-optimal token multiplier M * = D * /N * , which is no longer fixed as a compute budget changes for \u03b1 \u0338 = \u03b2.",
            "score": 0.6023119686724412,
            "section_title": "Conclusion.",
            "char_start_offset": 28698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 94
                },
                {
                    "start": 94,
                    "end": 198
                },
                {
                    "start": 198,
                    "end": 324
                },
                {
                    "start": 324,
                    "end": 466
                },
                {
                    "start": 468,
                    "end": 592
                },
                {
                    "start": 594,
                    "end": 631
                },
                {
                    "start": 631,
                    "end": 760
                },
                {
                    "start": 760,
                    "end": 981
                },
                {
                    "start": 983,
                    "end": 1235
                },
                {
                    "start": 1237,
                    "end": 1311
                },
                {
                    "start": 1311,
                    "end": 1334
                },
                {
                    "start": 1336,
                    "end": 1410
                },
                {
                    "start": 1410,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1627
                }
            ],
            "ref_mentions": [
                {
                    "start": 807,
                    "end": 811,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1043,
                    "end": 1047,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0548095703125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Figure 1: Inference scaling laws exhibited for Pythia (Biderman et al., 2023) models and GSM8K test error. We evaluate the error rate (lower is better) of models using various sizes and numbers of sampled solutions for weighted majority voting. Left: the error rate for each model size decreases steadily as inference-compute increases, and converges at the end. Right: the optimal model size (shown as stars for 2 41 , 2 44 , and 2 47 FLOPs) varies based on the inference-time compute budget. For instance, smaller models are compute-optimal at 2 41 and 2 44 FLOPs. Both axes are log scale.",
            "score": 0.5958340696325028,
            "section_title": "body",
            "char_start_offset": 1,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 362
                },
                {
                    "start": 363,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 591
                }
            ],
            "ref_mentions": [
                {
                    "start": 54,
                    "end": 77,
                    "matchedPaperCorpusId": "257921893"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06982421875
        },
        {
            "corpus_id": "275337290",
            "title": "Test-Time Compute: from System-1 Thinking to System-2 Thinking",
            "text": "Unlike training-time computation scaling, test-time compute still lacks a universal scaling law. Some works have attempted to derive scaling laws for specific test-time compute strategies (Wu et al., 2024c;Levi, 2024). Brown et al. (2024) demonstrate that the performance has an approximately loglinear relationship with repeated sampling times. Chen et al. (2024e) models repeated sampling as a knockout tournament and league-style algorithm, proving theoretically that the failure probability of repeated sampling follows a power-law scaling. Snell et al. (2024) investigate the scaling laws of repeated sampling and self-correction, and propose the computing-optimal scaling strategy. There are two major challenges to achieving a universal scaling law: first, current test-time compute strategies are various, each with different mechanisms to steer the model; thus, it lacks a universal framework for describing them; second, the performance of testtime compute is affected by a variety of factors, including the difficulty of samples, the accuracy of feedback signals, and decoding hyperparameters, and we need empirical studies to filter out the critical factors.",
            "score": 0.591978802114807,
            "section_title": "Scaling Law",
            "char_start_offset": 41242,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 544
                },
                {
                    "start": 545,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 1170
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.229248046875
        },
        {
            "corpus_id": "278636433",
            "title": "Parallel Scaling Law for Language Models",
            "text": "Training Inference-Optimal Language Models Chinchilla (Hoffmann et al., 2022) explored the scaling law to determine the training-optimal amounts for parameters and training data under a training FLOP budget. On the other hands, modern LLMs are increasingly interested on inference-optimal models. Some practitioners use much more data than the Chinchilla recommendation to train small models due to their high inference efficiency (Qwen Team, 2024;Allal et al., 2025;Sardana et al., 2024). Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2024) is a promising direction. \n\nFurther Theoretical Analysis for Parallel Scaling Laws One of our contributions is quantitatively computing the impact of parameters and computation on model capability. Although we present some theoretical results (Proposition 1), the challenge of directly modeling DIVERSITY limits us to using extensive experiments to fit parallel scaling laws. Why the diversity is related to log P, is there a growth rate that exceeds O(log P), and whether there is a performance upper bound for P \u226b 8, remain open questions.",
            "score": 0.5856303381872827,
            "section_title": "Discussion and Future Work",
            "char_start_offset": 32802,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 207
                },
                {
                    "start": 208,
                    "end": 296
                },
                {
                    "start": 297,
                    "end": 489
                },
                {
                    "start": 490,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 966
                },
                {
                    "start": 969,
                    "end": 1138
                },
                {
                    "start": 1139,
                    "end": 1316
                },
                {
                    "start": 1317,
                    "end": 1482
                }
            ],
            "ref_mentions": [
                {
                    "start": 467,
                    "end": 488,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 605,
                    "end": 622,
                    "matchedPaperCorpusId": "271601023"
                },
                {
                    "start": 622,
                    "end": 641,
                    "matchedPaperCorpusId": "278498044"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.337646484375
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Scaling laws of neural networks (Hestness et al., 2017;Rosenfeld et al., 2020) have been established across a range of domains, including language modeling (Kaplan et al., 2020;Hoffmann et al., 2022;OpenAI, 2023), image modeling (Henighan et al., 2020;Yu et al., 2022;Peebles & Xie, 2023), video modeling (Brooks et al., 2024), reward modeling (Gao et al., 2023), and board games (Jones, 2021). These studies have demonstrated how model performance is influenced by both the size of the model and the amount of training compute. However, there is limited knowledge on how varying the compute during inference affects model performance after the model has been trained. \n\nTo improve the task performance of large language models (LLMs), inference techniques typically involve additional compute as a performance maximization step at inference time (Nye et al., 2021;Wei et al., 2022;Wang et al., 2023b;Yao et al., 2023;Chen et al., 2024b). The computational cost of these techniques must be taken into account for compute-optimal inference. For example, Monte Carlo Tree Search (MCTS) may improve task performance, but it potentially requires much more compute than simply sampling solutions multiple times (Jones, 2021). Generally speaking, we need a comprehensive understanding of how various inference-time methods (e.g., best-of-n, majority voting (Wang et al., 2023a;Li et al., 2023)) trade off between performance and cost. To improve our understanding, this paper presents a thorough empirical evaluation with careful analysis over various configurations of representative LLMs and inference algorithms. \n\nSpecifically, we explore how to select an optimal size for the language model and an effective inference strategy (e.g., greedy search, majority voting, best-of-n, weighted voting, and their treesearch variants) to maximize performance (i.e., accuracy) with a given compute budget. We control the inference compute (FLOPs) of a fixed model by generating more tokens through the language model1 , sampling further candidate solutions, and ranking them with a reward model.",
            "score": 0.5799154772868835,
            "section_title": "INTRODUCTION",
            "char_start_offset": 609,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 528
                },
                {
                    "start": 529,
                    "end": 668
                },
                {
                    "start": 671,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 1039
                },
                {
                    "start": 1040,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1609
                },
                {
                    "start": 1612,
                    "end": 1893
                },
                {
                    "start": 1894,
                    "end": 2083
                }
            ],
            "ref_mentions": [
                {
                    "start": 55,
                    "end": 78,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 252,
                    "end": 268,
                    "matchedPaperCorpusId": "249926846"
                },
                {
                    "start": 268,
                    "end": 288,
                    "matchedPaperCorpusId": "254854389"
                },
                {
                    "start": 344,
                    "end": 362,
                    "matchedPaperCorpusId": "252992904"
                },
                {
                    "start": 865,
                    "end": 882,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 882,
                    "end": 901,
                    "matchedPaperCorpusId": "254877310"
                },
                {
                    "start": 901,
                    "end": 918,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 918,
                    "end": 937,
                    "matchedPaperCorpusId": "267740392"
                },
                {
                    "start": 1351,
                    "end": 1371,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1371,
                    "end": 1387,
                    "matchedPaperCorpusId": "259370847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27001953125
        },
        {
            "corpus_id": "275336968",
            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
            "text": "The study of AI scaling laws has become a cornerstone in understanding how training loss decreases as compute increases under optimized configurations. Kaplan et al. [9] introduced the concept of compute-optimal scaling, demonstrating predictable relationships among model size, dataset size, and compute. Brown et al. [11] reinforced these findings through the scaling behavior of Large Language Models (LLMs) such as GPT-3. Hoffmann et al. [10] refined the framework in the Chinchilla setting, underscoring the importance of balancing model size and dataset size to achieve compute-optimality. Collectively, these foundational studies provide empirical measurements of scaling exponents and form the basis for much of the work in this domain. \n\nBuilding on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time. To address various optimizations, Clark et al. [14] introduced sparsity-aware scaling laws for Mixture-of-Experts (MoE) architectures, formalizing an \"effective model size.\" Building on that framework, Kumar et al. [15] examined precision-aware scaling, showing how precision influences effective parameter counts in a compute-optimal regime. Despite these advancements, most studies treat compute as a static input rather than a dynamic, time-evolving resource. This paper addresses that gap by integrating empirically established scaling exponents with the temporal dynamics of efficiency improvements, inspired by Moore's Law [7] and Dennard Scaling [8]. Our work bridges the gap between classical scaling laws and the real-world constraints of time and efficiency, providing a framework for understanding how diminishing returns can be offset by continuous innovation.",
            "score": 0.577838306597307,
            "section_title": "Related Work",
            "char_start_offset": 4305,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 151
                },
                {
                    "start": 152,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1173
                },
                {
                    "start": 1174,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1831
                },
                {
                    "start": 1832,
                    "end": 2046
                }
            ],
            "ref_mentions": [
                {
                    "start": 1221,
                    "end": 1225,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 1803,
                    "end": 1806,
                    "matchedPaperCorpusId": "6519532"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.705078125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "We consider the problem of compute-optimal language model training: given a compute budget C, we wish to predict how to best allocate it across model size (in parameters) and dataset size (in tokens). With pretraining budgets ever-increasing, compute-optimal scaling is a question of paramount importance. In their seminal work, Kaplan et al. [30] proposed a scaling law predicting that the optimal ratio of tokens to parameters decays as a power of C. This scaling law was influential in determining the size of GPT-3 and several subsequent models [12,51,43,32,47,62,52]. However, Hoffmann et al. [25] challenged its validity, arguing instead that the optimal token-to-parameter ratio should be approximately independent of C, and that contemporary models had too many parameters relative to their number of training tokens. Based on this prediction, they trained a 67B parameters model called Chinchilla and which outperformed larger models with a similar compute budget. \n\nWhile Hoffmann et al. [25] and subsequent work [55,56,17,26,15] established that following the Hoffmann et al. scaling law leads to better performance than Kaplan et al. scaling, it is still important to understand why the two works arrived at different conclusions. Is the difference due to architecture, training setup, pretraining data, results analysis, or perhaps something else entirely? The answer could teach us important lessons on how to correctly predict and perform model scaling. \n\nHoffmann et al. [25] hypothesize that the scaling law discrepancy is due to Kaplan et al. [30] not tailoring the learning rate schedule for each token budget separately. While they demonstrate that mismatched learning rate decay results in a higher loss, they do not show it leads to a different compute-optimal scaling law. We further discuss Hoffmann et al. [25]'s hypothesis in Appendix A. To the best of our knowledge, this hypothesis is the only explanation offered in the literature so far. \n\nOur contribution. In this work, we uncover three factors contributing to the discrepancy, and disprove Hoffman et al.'s hypothesis about the role of learning rate decay; Figure 1 illustrates our main  Cosine decay (no tuning) results.",
            "score": 0.5735739869532981,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 305
                },
                {
                    "start": 306,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 825
                },
                {
                    "start": 826,
                    "end": 973
                },
                {
                    "start": 976,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1468
                },
                {
                    "start": 1471,
                    "end": 1640
                },
                {
                    "start": 1641,
                    "end": 1795
                },
                {
                    "start": 1796,
                    "end": 1967
                },
                {
                    "start": 1970,
                    "end": 1987
                },
                {
                    "start": 1988,
                    "end": 2204
                }
            ],
            "ref_mentions": [
                {
                    "start": 549,
                    "end": 553,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 598,
                    "end": 602,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 998,
                    "end": 1002,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1487,
                    "end": 1491,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1831,
                    "end": 1835,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.28955078125
        },
        {
            "corpus_id": "268064306",
            "title": "Sequence modeling and design from molecular to genome scale with Evo",
            "text": "We provide a summary of the steps involved in our scaling laws analysis. Quantifying scaling rates allows us to predict performance as model size, dataset size, and compute grow. \n\n1. Define a set of compute budgets to study. We use 8 \u00d7 10 18 , 2 \u00d7 10 19 , 4 \u00d7 10 19 and 8 \u00d7 10 19 FLOPS. 2. Calculate the FLOPS (floating point operations) required to process a fixed input size for the model architecture of interest (i.e. the \"cost\" of using the model). 3. Identify the model's compute-optimal allocation for each compute budget: \n\n(a) Select a wide range of possible model sizes, and calculate for each model size the corresponding number of tokens that need to be processed to reach the compute budget. Other hyperparameters are chosen according to Table S1. We generally observe minor changes to model topology (depth, width) to only minimally affect perplexity, aligning our results with the findings presented by Kaplan et al. (2020) for Transformers. (b) Train a model of each size and record its performance (e.g., in terms of perplexity). (c) Identify the optimal compute allocation: Following prior analysis, we fit a second-order polynomial as a function from (log) model size to perplexity, and extract obtained the compute-optimal point as its minimum. The compute-optimal point identifies the optimal allocation of model size and training tokens at the given compute budget. \n\nAfter deriving the compute-optimal scaling rates (Figure 1G), we compare architectures and compute optimal allocation of tokens and model size (Figure S5). In Figure S3, we also show rates for compute-suboptimal model sizes by architecture. In particular, we quantify the effect on perplexity scaling caused by a suboptimal allocation of compute budget to model or dataset size (e.g., training a smaller model for more tokens). We estimate the compute-optimal model size for each compute budget, then reduce it by a percentage (the offset). The corresponding perplexity is obtained via the IsoFLOP curves (Figure 1F).",
            "score": 0.5730950417851707,
            "section_title": "Scaling laws procedure",
            "char_start_offset": 67912,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 72
                },
                {
                    "start": 73,
                    "end": 178
                },
                {
                    "start": 181,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 705
                },
                {
                    "start": 706,
                    "end": 761
                },
                {
                    "start": 762,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1265
                },
                {
                    "start": 1266,
                    "end": 1388
                },
                {
                    "start": 1391,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1931
                },
                {
                    "start": 1932,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.252685546875
        },
        {
            "corpus_id": "253397775",
            "title": "Will we run out of data? Limits of LLM scaling based on human-generated data",
            "text": "In this appendix we sketch a model of optimal scaling decisions under data scarcity. In particular, we examine how the decision to overtrain models might be affected by data scarcity. \n\nOur starting point is the parametric scaling law of Hoffmann et al. (2022), which predicts the reducible loss of a model L given its number of parameters N and the size of its training dataset D (see Equation 9). Hoffmann et al. (2022) derive from this scaling law a relation between the sizes of the model and the dataset that minimize the reducible loss of their model given a fixed training compute budget. In particular, in compute-optimal models the ratio D/N is around 20. We call this the Chinchilla scaling law, and we call models that follow it Chinchilla-optimal. \n\nModels for which the ratio D/N is above the Chinchillaoptimal ratio are commonly called overtrained, while models that are below that ratio are called undertrained. At a fixed training compute budget, overtrained models require less compute during inference but more data during training. This is currently attractive for developers, as compute is relatively scarce compared to data. As a consequence, some well-known models, like Llama 3 (Meta, 2024), are overtrained. 36 rofit maximization problem maximize \n\nwhere \n\nHere N is the number of parameters of the model, D is the size of the training dataset in tokens and P is the price of each inference token in (some multiple of) dollars. C 0 is the total computational budget for training and inference and I is the number of tokens produced during inference. A, a, B and b are fitted parameters of the scaling law, and I 0 , r and h are parameters of the inference demand function. All parameters are positive. \n\nWe now examine the relationship between overtraining and undertraining in the context of a data bottleneck. To simplify the analysis, here we ignore the cost of gathering data and focus on the computational cost of the model during training and inference. We assume that developers want to achieve the maximum possible profit within their computational budget, and that this computational budget includes both training and inference.",
            "score": 0.5721447700744597,
            "section_title": "F. Overtraining in the context of data scarcity",
            "char_start_offset": 53221,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 183
                },
                {
                    "start": 186,
                    "end": 398
                },
                {
                    "start": 399,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 926
                },
                {
                    "start": 927,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1270
                },
                {
                    "start": 1273,
                    "end": 1278
                },
                {
                    "start": 1281,
                    "end": 1451
                },
                {
                    "start": 1452,
                    "end": 1573
                },
                {
                    "start": 1574,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1835
                },
                {
                    "start": 1836,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2161
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1163330078125
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "We observe that the size of compute-optimal models increases as we increase the training budget (measured in terms of total FLOPs) while the active number of parameters, hence FLOPs per example, decrease for compute-optimal models. \u2022 During inference, FLOPs per example seem to play a more important role 2 . For many tasks, upstream performance is a good predictor of downstream performance and the relationship between upstream and downstream performance is not impacted by the sparsity level. However, we observe that for models with the same perplexity on the pretraining data distribution, sparser models, i.e., models with fewer number of active parameters, perform worse on specific types of downstream tasks that presumably require more \"reasoning\". \n\nOur results, inline with findings from previous relevant studies (Ludziejewski et al., 2024;He, 2024) on scaling laws for MoEs, show increasing sparsity level leads to better performance and efficiency during pretraining. Considering the various methods to increase compute per example during inference adaptively conditioned on task or example complexity, we conclude that approaches like Mixture-of-Experts (MoEs), which reduce the unit compute cost (i.e., FLOPs per token) by increasing the sparsity level, hold significant promise given their potential to enhance efficiency in both pretraining and inference.",
            "score": 0.5697279436097049,
            "section_title": "Introduction",
            "char_start_offset": 3522,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 231
                },
                {
                    "start": 232,
                    "end": 308
                },
                {
                    "start": 309,
                    "end": 495
                },
                {
                    "start": 496,
                    "end": 757
                },
                {
                    "start": 760,
                    "end": 981
                },
                {
                    "start": 982,
                    "end": 1373
                }
            ],
            "ref_mentions": [
                {
                    "start": 825,
                    "end": 852,
                    "matchedPaperCorpusId": "267626982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07763671875
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "We fit the scaling law of Eq. ( 2) with E = 0 to these experiments. For the baseline transformer, we find coefficients \n\nThe equivariant model yields  Confidence intervals are provided in Tbl. 2. \n\nThese two models scale quite differently with model size and training length, which has implications for the optimal allocation of a compute budget. We will discuss this later. \n\nFit quality First, we show how well these fitted scaling laws agree with the data in Figs. 3 and 4. \n\nComparing the observed values of the test loss to the predictions from the scaling laws, we overall find good agreement. There are no glaring deviations, although the power law underestimates the loss for the largest equivariant models and for one baseline outlier. Most measurements fall within the uncertainty bands, but less than the 95% one would expect if the bootstrap would cover all relevant sources of error. This is evidence that the ansatz of Eq. ( 2) does not describe the data perfectly. \n\nScaling with compute Next, we analyze the model performance and its scaling with compute. \n\nFrom the training laws in Eqs. ( 6) and ( 7), we compute best achievable test loss L * as a function of the training compute budget C, as given by Eq. ( 5). We find \n\nand the exponents are compatible with each other within the confidence intervals shown in Tbl. 2. We visualize the empirical compute-loss measurements and the derived optimal compute-loss relationship in Fig. 1. \n\nFor any given compute budget, the equivariant transformer significantly outperforms the baseline. Over the range of compute budgets we tested, the equivariant model achieves a loss that is lower by approximately a factor of 2. \n\nOptimal allocation of compute From the scaling laws we can also derive the optimal allocation of a given computational budget to the parameter count and training duration, see Eq. ( 4). We show our results for both models in Fig. 5.  ). The equivariant architecture requires smaller models to achieve a compute-optimal performance, but this gap closes for bigger compute budgets. \n\nWe find that a compute-optimal equivariant transformer has less parameters than a computeoptimal baseline transformer.",
            "score": 0.5691537725426203,
            "section_title": "Scaling laws",
            "char_start_offset": 20812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 118
                },
                {
                    "start": 121,
                    "end": 195
                },
                {
                    "start": 198,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 374
                },
                {
                    "start": 377,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 599
                },
                {
                    "start": 600,
                    "end": 744
                },
                {
                    "start": 745,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 979
                },
                {
                    "start": 982,
                    "end": 1071
                },
                {
                    "start": 1074,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1238
                },
                {
                    "start": 1241,
                    "end": 1452
                },
                {
                    "start": 1455,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1681
                },
                {
                    "start": 1684,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2063
                },
                {
                    "start": 2066,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.13818359375
        },
        {
            "corpus_id": "253116832",
            "title": "Scaling Laws Beyond Backpropagation",
            "text": "Can alternative training methods accelerate neural network training? Surveying the current state-of-the-art, one may find numerous claims of alternative training methods achieving competitive performance with BP across a variety of settings and tasks (e.g., [12,18,24]). \n\nWe seek to study this claim, with three restrictions in scope: \n\n1. We focus on Direct Feedback Alignment [25], due its simplicity and wide applicability [12], as well as its broad hardware prospects [14,26,27], and theoretical background [28]. \n\n2. We study compute-efficiency specifically (i.e, best performance achievable for a given compute budget), as this usually a significant bottleneck for scaling-up models. \n\n3. We conduct our study on \"GPT-like\" [29] causal decoder-only Transformers trained on English data. These models are known to possess smooth scaling laws [19,30]. Because of their unique abilities [31], they also command some of the largest training budgets in machine learning [32], making them a prime target for more compute-efficient training. \n\nThese restrictions lead us to test the following hypothesis: \n\nHypothesis. Direct Feedback Alignment can train causal decoder-only models more efficiently than backpropagation, achieving better performance for a given compute budget. \n\nScaling laws as a holistic empirical tool. Scaling laws have been proposed as an empirical approach to connect hyperparameters of neural networks (e.g., parameter count, training dataset size) to their performance. They have been derived both on specific downstream tasks [33,34] and on upstream modeling loss [19]. Scaling laws can characterize the influence of data & modeling decisions [21,22], or even unveil new, more optimal training practices [35,36]. \n\nAs illustrated in Figure 1, it is possible to derive a so-called compute optimal frontier for a class of models: this defines L(C), the best performance L achievable for a compute budget C. We fit a power-law L(C) = (C c C) \u03b1 C over the Pareto front of multiple runs, as proposed in [19]. C c is a constant offsetting the frontier, while \u03b1 C controls the slope.",
            "score": 0.568158996300153,
            "section_title": "Framing",
            "char_start_offset": 2719,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 68
                },
                {
                    "start": 69,
                    "end": 270
                },
                {
                    "start": 273,
                    "end": 335
                },
                {
                    "start": 338,
                    "end": 517
                },
                {
                    "start": 520,
                    "end": 690
                },
                {
                    "start": 693,
                    "end": 793
                },
                {
                    "start": 794,
                    "end": 856
                },
                {
                    "start": 857,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1104
                },
                {
                    "start": 1107,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1277
                },
                {
                    "start": 1280,
                    "end": 1322
                },
                {
                    "start": 1323,
                    "end": 1494
                },
                {
                    "start": 1495,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1738
                },
                {
                    "start": 1741,
                    "end": 2029
                },
                {
                    "start": 2030,
                    "end": 2102
                }
            ],
            "ref_mentions": [
                {
                    "start": 258,
                    "end": 262,
                    "matchedPaperCorpusId": "219981351"
                },
                {
                    "start": 265,
                    "end": 268,
                    "matchedPaperCorpusId": "57189514"
                },
                {
                    "start": 379,
                    "end": 383,
                    "matchedPaperCorpusId": "2843914"
                },
                {
                    "start": 427,
                    "end": 431,
                    "matchedPaperCorpusId": "219981351"
                },
                {
                    "start": 477,
                    "end": 480,
                    "matchedPaperCorpusId": "221896106"
                },
                {
                    "start": 512,
                    "end": 516,
                    "matchedPaperCorpusId": "235390926"
                },
                {
                    "start": 731,
                    "end": 735,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 891,
                    "end": 895,
                    "matchedPaperCorpusId": "248118752"
                },
                {
                    "start": 1673,
                    "end": 1676,
                    "matchedPaperCorpusId": "246608156"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.45849609375
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "Still, our approximation is flexible enough to account for heterogeneous hardware utilization and costs. \n\nIn Figure 6, we show how inference-adjusted cost-optimal models compare to Chinchilla-style models, assuming typical training and inference hardware costs and MFU. For a 30B-Chinchilla-quality model, LLM practitioners expecting 1.5B inference requests can reduce costs by 17% by instead training a 16B model on 3.35T tokens. In Sec. B.2, we show further results for various configurations. \n\nComparing our compute-optimal analysis in Fig. 2 to our real-world cost analysis in Fig. 6, we see that for the same inference demand of 2T tokens (7.02B requests), a Chinchilla-70B model requires only 1.3% extra FLOPs compared to an equal-quality compute-optimal model, but costs 36% more than a cost-optimal model. This difference is attributable to the 50\u00d7 lower MFU of each inference output token compared to training, which our FLOP-based analysis in Sec. 2 fails to capture. A few studies have also (gently) critiqued the general parametric function fitting approach of Hoffmann et al. (2022). Besiroglu et al. (2024) attempted to replicate the methodology used in Hoffmann et al. (2022) and found that the confidence intervals reported in the original study were implausibly narrow. The broader implication is that confidence intervals are quite wide for parametric function fitting on a small number of data points. This is also a potentially valid critique of our empirical results, as our analysis only includes 47 separate experiments (Chinchilla included more than 400 experiments). In a similar vein, Porian et al. (2024) investigate discrepancies between the scaling laws from Kaplan et al. (2020) and Hoffmann et al. (2022). The results presented in Gadre et al. (2024) are particularly relevant to this paper. The authors train 100 models between the sizes of 1.4B and 6.9B parameters and on data with tokens-per-parameter ratios between 20 and 640. Similar to our study, they find reliable scaling laws in these model and data regimes.",
            "score": 0.5644916616800078,
            "section_title": "Estimating Real-World Cost Optimality",
            "char_start_offset": 22264,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 270
                },
                {
                    "start": 271,
                    "end": 431
                },
                {
                    "start": 432,
                    "end": 439
                },
                {
                    "start": 440,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1422
                },
                {
                    "start": 1423,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1738
                },
                {
                    "start": 1739,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2051
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0271148681640625
        },
        {
            "corpus_id": "269148735",
            "title": "Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies",
            "text": "In real-world scenarios, it is often helpful to deviate from compute-optimal settings by using extra resources to train a smaller model longer, gaining memory and inference efficiency without harming performance.To guide this decision making process, we study the tradeoff between model size and compute overhead (Kaplan et al., 2020;Vries, 2023).\n\nGiven a desired model size that is \u03c9 N \u00d7 as large as the compute-optimal model size N * , we can find the multiple \u03c9 D of the optimal dataset size D * needed to maintain the expected loss of the optimal model and dataset sizes.In particular, we ensure no change in our Approach 2 scaling law predicted loss by choosing \u03c9 D as follows:\n\nFigure 6.Training settings estimated to be compute-efficient by our scaling laws are state-of-the-art, validating our scaling law fits.We find N * for the DG-20 dataset for Approaches 1-3 and train models using values of N in between at least two approaches' predicted values for N * (see the \"compute-efficient region\").The trained models are more efficient and robust than the prior SOTA.\n\nand E \u2032 and B \u2032 assume a particular dataset quality.Given new data and model sizes, the \"compute overhead\" is the increase in training FLOPs as a percentage of the training FLOPs needed for the optimal model and dataset sizes.\n\nAs can be seen in Figure 7, there exists a substantial region where the model size can be reduced with minimal compute overhead.For example, reducing the model size by half (\u03c9 N = 0.5) results in only a \u223c10% increase in computational requirements, demonstrating an efficient tradeoff between model compactness and training cost.We also show that, given the prior SOTA's training FLOPs (fixing compute overhead to 0%), larger models are predicted to require increasingly more compute to reach the performance of the compute-optimal model, consistent with the displayed AutoAttack performances falling as model size increases.",
            "score": 0.5639044035105815,
            "section_title": "TRADING OFF TRAINING EFFICIENCY FOR INFERENCE EFFICIENCY",
            "char_start_offset": 22364,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 212,
                    "end": 347
                },
                {
                    "start": 349,
                    "end": 576
                },
                {
                    "start": 576,
                    "end": 683
                },
                {
                    "start": 685,
                    "end": 694
                },
                {
                    "start": 694,
                    "end": 820
                },
                {
                    "start": 820,
                    "end": 1006
                },
                {
                    "start": 1006,
                    "end": 1075
                },
                {
                    "start": 1077,
                    "end": 1129
                },
                {
                    "start": 1129,
                    "end": 1303
                },
                {
                    "start": 1305,
                    "end": 1433
                },
                {
                    "start": 1433,
                    "end": 1633
                },
                {
                    "start": 1633,
                    "end": 1929
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11065673828125
        },
        {
            "corpus_id": "225094178",
            "title": "Scaling Laws for Autoregressive Generative Modeling",
            "text": "Instead of focusing on converged performance, one can study the loss L achieved with a finite training compute budget C when training with a large enough dataset to avoid overfitting. We define C theoretically rather than empirically, and approximate7 it as C \u2261 6N E where N is the non-embedding parameter count (model size) and E = SB is the total number of tokens processed during training (with S the number of parameter updates and B the batch size in tokens). The results for L(C) from a variety of model sizes are depicted in figure 5, along with the pareto-frontier of optimal loss for a given compute budget, and a power-law plus constant fit forced to lie below this frontier. Scaling laws with compute-Scaling laws with compute (total estimated floating point operations) for various domains, along with power-law plus constant fits (dashed). This is identical to figure 1, except that we do not subtract the fitted constant irreducible loss. Note that very small models underperform compared to the trends when they model images or videos with very large contexts. Note also that the largest language models [BMR + 20] were not trained to convergence. \n\nThe compute trends are most relevant for differentiating between the irreducible loss and reducible losses, since they avoid the issue of training to convergence, which makes the interpretation of L(N ) difficult. We display the reducible loss trends for L(C) in figure 1, and emphasize that these appear to be pure power-laws, even when the reducible loss is much smaller than the irreducible loss. \n\nWe can use the L(C) trends to estimate the model size N opt that optimizes the loss when training is constrained by a fixed compute8 budget C. For this purpose we select points on the convex hull of the loss versus compute frontier; these can be seen as blue points in figure 5. The results for all domains together appear in figure 2, while each domain is shown separately with individual fits in figure 16. In all cases we find that N opt (C) \u221d C \u03b2 can be fit with a pure power-law, with all exponents fairly close to \u03b2 \u223c 0.7.",
            "score": 0.5628375096770741,
            "section_title": "Compute Scaling and Optimal Model Sizes",
            "char_start_offset": 21358,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 685
                },
                {
                    "start": 686,
                    "end": 852
                },
                {
                    "start": 853,
                    "end": 952
                },
                {
                    "start": 953,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1162
                },
                {
                    "start": 1165,
                    "end": 1378
                },
                {
                    "start": 1379,
                    "end": 1564
                },
                {
                    "start": 1567,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2095
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2181396484375
        },
        {
            "corpus_id": "269484186",
            "title": "Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning",
            "text": "Training-vs.Test-Time Compute Scaling.Our method integrates MCTS with preference learning, aiming to enhance both preference quality and policy reasoning via step-level alignment.We analyze the impact of training-time compute scaling versus increased inference-time sampling.The cumulative pass rate of our iterative learning method can be seen as the pass rate of an ensemble of different model checkpoints.We use greedy decoding to obtain the inference time performance of our method of iterative learning.We measure success by the pass rate, indicating the percentage of correctly elicited answers.Figure 3 displays the cumulative pass rate at each checkpoint, aggregating the pass rates up to that point.For test-time scaling, we increase the number of sampled reasoning chains.Additionally, we compare the inference performance of our checkpoints with a sampling-only method, self-consistency, to assess their potential performance ceilings.The pass rate curves on ARC-C, SciQ, and MATH datasets reveal that our MCTS-enhanced approach yields a higher training compute scaling exponent.This effect is particularly pronounced on the unseen SciQ dataset, highlighting our method's efficiency and effectiveness in enhancing specific reasoning abilities with broad applicability.Inference-time performance analysis shows higher performance upper bounds of our method on ARC-C and SciQ.For instance, while self-consistency on SciQ plateaus at around 84%, our framework pushes performance to 88.6%.However, on MATH, the sampling-only approach outperforms training compute scaling: more sampling consistently enhances performance beyond 35%, whereas post-training performance hovers around 32.2%.This observation suggests that in-domain SFT already aligns the model well with task-specific requirements.\n\nFunctions of Self-Evaluation Mechanism.As illustrated in Section 2.1, the self-evaluation score inherently revises the Q value estimation for subsequent preference data collection.In practice, we find that the ground-truth information, i.e., the \"EXAMPLE ANSWER\" in Table 6, is crucial to ensure the reliability of self-evaluation.We now compare the score distribution and discriminative abilities when including v.s.excluding this ground-truth information in Table 3.",
            "score": 0.5617433932077602,
            "section_title": "Further Analysis",
            "char_start_offset": 23785,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 12
                },
                {
                    "start": 12,
                    "end": 38
                },
                {
                    "start": 38,
                    "end": 179
                },
                {
                    "start": 179,
                    "end": 275
                },
                {
                    "start": 275,
                    "end": 408
                },
                {
                    "start": 408,
                    "end": 508
                },
                {
                    "start": 508,
                    "end": 601
                },
                {
                    "start": 601,
                    "end": 708
                },
                {
                    "start": 708,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 946
                },
                {
                    "start": 946,
                    "end": 1090
                },
                {
                    "start": 1090,
                    "end": 1279
                },
                {
                    "start": 1279,
                    "end": 1385
                },
                {
                    "start": 1385,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1693
                },
                {
                    "start": 1693,
                    "end": 1800
                },
                {
                    "start": 1802,
                    "end": 1841
                },
                {
                    "start": 1841,
                    "end": 1982
                },
                {
                    "start": 1982,
                    "end": 2133
                },
                {
                    "start": 2133,
                    "end": 2219
                },
                {
                    "start": 2219,
                    "end": 2270
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08758544921875
        },
        {
            "corpus_id": "258967177",
            "title": "Likelihood-Based Diffusion Language Models",
            "text": "Our next goal is to understand how to optimally use a given compute budget C to maximize the held-out likelihood of a model. Specifically, we must choose between training a large model for fewer iterations or training a small model for longer. For this, we leverage our parameter scaling law N * (C) which predicts the optimal model size given a compute budget. \n\nWe plot both of our parameter scaling laws in Figure 3 and again find that the trends have nearly the same slope but differ by a constant factor. Specifically, compute-optimal Plaid models should be about 4\u00d7 smaller (and therefore trained for 4\u00d7 longer) than compute-optimal autoregressive models. The large gap in compute-optimal settings suggests that selecting model sizes based on existing scaling laws [15,13], which were developed for autoregressive models, could incur a substantial loss in the effective compute budget.",
            "score": 0.5615788871046519,
            "section_title": "Compute-optimal training recipe",
            "char_start_offset": 18093,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 124
                },
                {
                    "start": 125,
                    "end": 243
                },
                {
                    "start": 244,
                    "end": 361
                },
                {
                    "start": 364,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 891
                }
            ],
            "ref_mentions": [
                {
                    "start": 775,
                    "end": 778,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.165283203125
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "(2) \n\nN * and D * tr are functions that describe the optimal parameters and pre-training tokens, respectively, that minimize total training and inference compute. The pre-training loss constraint ensures that we minimize compute for a given quality. \n\nWe use the standard approximation of FLOPs for transformer models with N parameters: 6N per training token and 2N per inference token (Kaplan et al., 2020). Thus, our 1 In practice, smaller models of equivalent quality may have greater demand since they can have lower inference latency. \n\nobjective simplifies to: \n\nWe note that this is the \"converse\" of the Chinchilla optimization problem. In the Chinchilla paper, the authors assumed a fixed compute budget and found N * and D * tr that minimized pre-training loss. Our objective is to fix pretraining loss and find N * and D * tr that minimize compute costs. \n\nCrucially, our total computational cost depends on the inference demand over the lifetime of the model, but our model's parameter count and data size are determined prior to training. Thus, our analysis is predicated on the assumption that LLM practitioners can estimate their inference demand prior to training. \n\nWithout inference (D inf = 0), the optimization problem in Eq. 3 can be solved analytically. Unfortunately, accounting for inference (D inf > 0), determining N * and D * tr analytically as functions of \u2113 and D inf is intractable (we defer our proof to Appendix A). Instead, we computationally solve for N * and D * tr across a range of values of \u2113 and D inf using the Newton root-finding method. In practice, this method converges for relevant inputs and we are able to determine optimal parameter/token counts. \n\nIn Figure 2, we show how our inference-adjusted model's FLOP counts, parameters, and pre-training tokens compare to Chinchilla-style models across a range of loss values and inference demands. When inference usage is significantly less than the number of pre-training tokens, Chinchilla models are essentially compute-optimal. However, as demand increases, inference costs becomes a significant factor.",
            "score": 0.5612637086647194,
            "section_title": "Computational Optimality",
            "char_start_offset": 7306,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 3
                },
                {
                    "start": 6,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 249
                },
                {
                    "start": 252,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 539
                },
                {
                    "start": 542,
                    "end": 566
                },
                {
                    "start": 569,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1180
                },
                {
                    "start": 1183,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1447
                },
                {
                    "start": 1448,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1694
                },
                {
                    "start": 1697,
                    "end": 1889
                },
                {
                    "start": 1890,
                    "end": 2023
                },
                {
                    "start": 2024,
                    "end": 2099
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0509033203125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Finally, we validate our compute-optimal loss scaling law by training and evaluating a model using a larger compute budget. Specifically, we train a 901M parameter model with a compute budget of C + \u2248 8e19 FLOPs, which our scaling law predicts to be compute-optimal, using the batch size and learning prescribed by our hyperparameter scaling laws (see Table 4), reaching a loss value of L + = 2.943. In Figure 6, we add the point (C + , L + ) to the red curve in Figure 4 and find that it falls within the predicted trend. Notably, L + is obtained with a single training run using our predicted optimal configuration, while at lower compute values we estimate the optimal loss by interpolating an IsoFLOP curve.",
            "score": 0.5606662227027257,
            "section_title": "Additional Analysis 4.1 Trends in compute-optimal loss",
            "char_start_offset": 20138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 123
                },
                {
                    "start": 124,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 522
                },
                {
                    "start": 523,
                    "end": 711
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1153564453125
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "In Figure 14 (left), we see that as one increases the amount of compute, it is possible to get better fits with lower relative error.In Figure 14 (right), we see a similar trend as one increases the number of data points used to fit a scaling law.Blue stars indicate the configurations from Table 1, which provide accurate predictions relative to the general trends-hinting at their usefulness for our investigation.In Figures 15 and 16 we repeat the compute analysis comparing trade-offs for loss prediction and error prediction for our RedPajama 1.4B parameter, 900B token and 6.9B parameter, 138B token runs respectively.We find that less compute is generally necessary to construct a loss scaling law that achieves the same relative error as that of an error prediction scaling law.\n\nOn compute-optimal token multipliers.We consider 20 tokens per parameter as close to compute-optimal for our experiments.Here we investigate, using different approaches, what the compute-optimal token multipliers are for each dataset-assuming one should scale number of parameter and training tokens equally as Hoffmann et al. [45] suggest.\n\nTurning to Figure 9, we notice that there are many multipliers, between 10 and 80 that yield models close to the frontier.Hence, empirically, it appears choices within this range should be suitable for the optimal token multiplier.\n\nWe can also compute an optimal token multiplier using the coefficients in Table 6.Based on Hoffmann et al. [45]'s Equation ( 4) and the assumption that \u03b1 = \u03b2, we write,\n\nTo compute M * = D * /N * , we then have,\n\nUsing the values from   4).Recall that the C4 training set is English-filtered.Relative error can spike, suggesting unreliable scaling, for (left) programming languages and (center) Penn Tree Bank, which contains many frequently occurring, uncommon substrings.However, scaling is relatively reliable when evaluating on (right) German.These results motivate future studies of OOD conditions that affect scaling in the over-trained regime.observation in Figure 9, which suggests M = 5 is already too small to give points on the Pareto frontier.We hypothesize this mismatch arises because we fit our scaling laws using models with M \u2265 20.",
            "score": 0.560054167954347,
            "section_title": "3.6%",
            "char_start_offset": 40138,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 133,
                    "end": 247
                },
                {
                    "start": 247,
                    "end": 416
                },
                {
                    "start": 416,
                    "end": 624
                },
                {
                    "start": 624,
                    "end": 786
                },
                {
                    "start": 788,
                    "end": 825
                },
                {
                    "start": 825,
                    "end": 909
                },
                {
                    "start": 909,
                    "end": 1128
                },
                {
                    "start": 1130,
                    "end": 1252
                },
                {
                    "start": 1252,
                    "end": 1361
                },
                {
                    "start": 1363,
                    "end": 1445
                },
                {
                    "start": 1445,
                    "end": 1531
                },
                {
                    "start": 1533,
                    "end": 1574
                },
                {
                    "start": 1576,
                    "end": 1603
                },
                {
                    "start": 1603,
                    "end": 1655
                },
                {
                    "start": 1655,
                    "end": 1836
                },
                {
                    "start": 1836,
                    "end": 1910
                },
                {
                    "start": 1910,
                    "end": 2013
                },
                {
                    "start": 2013,
                    "end": 2118
                },
                {
                    "start": 2118,
                    "end": 2211
                }
            ],
            "ref_mentions": [
                {
                    "start": 1115,
                    "end": 1119,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1470,
                    "end": 1474,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07122802734375
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "There has been a proliferation of research in recent years pointing to the pivotal role of scale, and how its benefits could be predicted empirically (Hestness et al., 2017;Kaplan et al., 2020;Alabdulmohsin et al., 2022;Bansal et al., 2022;Zhai et al., 2022). Generally, the performance of deep neural networks f (x) (such as its error rate or log-perplexity) often follows a power law f (x) \u223c \u03b2x \u2212c + \u03b5 as one varies a dimension x, such as the data size or model parameters. These \"scaling laws,\" as they are known today, have been used, among others, to determine the training data size needed for a specified level of accuracy (Cho et al., 2015;Beleites et al., 2013;Figueroa et al., 2012) and to optimize the model architecture (Kaplan et al., 2020;Hoffmann et al., 2022;Alabdulmohsin et al., 2024b), with some theoretical justification (Bahri et al., 2021;Hutter, 2021;Sharma and Kaplan, 2022). \n\nBesides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2024), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such In RINS, the model f : X \u2192 Y is split into two parts: the first block f A : X \u2192 X is applied iteratively to its own output r times before passing the output to the second block. RIGHT: Illustrative examples of models with different signatures and degrees.",
            "score": 0.5594290046258295,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 899
                },
                {
                    "start": 902,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1345
                },
                {
                    "start": 1346,
                    "end": 1480
                },
                {
                    "start": 1481,
                    "end": 1758
                },
                {
                    "start": 1759,
                    "end": 1836
                }
            ],
            "ref_mentions": [
                {
                    "start": 193,
                    "end": 220,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 240,
                    "end": 258,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 648,
                    "end": 670,
                    "matchedPaperCorpusId": "35138335"
                },
                {
                    "start": 874,
                    "end": 898,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1248,
                    "end": 1266,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 1362,
                    "end": 1379,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.205322265625
        },
        {
            "corpus_id": "276258898",
            "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning",
            "text": "Generating N M C = 64 CoT's for each of 128 examples, in turn, requires approximately 50 petaflops. The actual inference compute cost depends on model performance: stronger models, which will discard more samples, will need to process more samples in order to construct a full batch for the next training step. Therefore there is a tradeoff: constructing a batch of 128 hard samples requires running inference on more samples, but also means fewer steps are required to make a full pass through the dataset. We fine-tune Llama-3-8B base models on the MATH dataset to produce direct answers. We fine-tune for 4 epochs one model using CE loss and several models under the L N \u2032 DCO objective, for choices of N \u2032 indicated by color. Note that there is no choice of N \u2032 that is optimal across all N (different colors are higher at different N ). The black curve is a Pareto-optimal performance frontier traced by the max of coverage curves for DCO over all N \u2032 . Pareto-optimality at a given test strategy pass@N for some N on the x-axis is obtained by DCO training for some N \u2032 close to N .",
            "score": 0.5570228106980996,
            "section_title": "MATH.",
            "char_start_offset": 41351,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 310
                },
                {
                    "start": 311,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 590
                },
                {
                    "start": 591,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1087
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.026702880859375
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Hoffmann et al. [45] find that as the compute budget increases, N * and D * scale roughly evenly.Assuming equal scaling, there is a fixed compute-optimal token multiplier M * = D * /N * per training distribution.\n\nOver-training.We define over-training as the practice of allocating compute sub-optimally, so smaller models train on a disproportionately large number of tokens (i.e., M > M * ).While loss should be higher than in the compute-optimal allocation for a given training budget, the resulting models have fewer parameters and thus incur less inference cost.",
            "score": 0.5545948737497527,
            "section_title": "Preliminaries",
            "char_start_offset": 7295,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 97,
                    "end": 212
                },
                {
                    "start": 214,
                    "end": 228
                },
                {
                    "start": 228,
                    "end": 393
                },
                {
                    "start": 393,
                    "end": 567
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1171875
        },
        {
            "corpus_id": "269457365",
            "title": "More Compute Is What You Need",
            "text": "Large language model pre-training has become increasingly expensive, with most practitioners relying on scaling laws to allocate compute budgets for model size and training tokens, commonly referred to as Compute-Optimal or Chinchilla Optimal. In this paper, we hypothesize a new scaling law that suggests model performance depends mostly on the amount of compute spent for transformer-based models, independent of the specific allocation to model size and dataset size. Using this unified scaling law, we predict that (a) for inference efficiency, training should prioritize smaller model sizes and larger training datasets, and (b) assuming the exhaustion of available web datasets, scaling the model size might be the only way to further improve model performance.",
            "score": 0.5543654124791199,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2037353515625
        },
        {
            "corpus_id": "278338773",
            "title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
            "text": "Inference scaling law. It is costly to generate many reasoning paths and select a correct one. The more difficult a problem is, the more reasoning paths we may need to sample. A key research question here is how to use the compute resource wisely to find a right path for any given problem. [10] illustrates the scaling law for inference-time compute. They observe that the coverage grows nearly log-linearly with the number of samples generated from LLM and may reach to 100% coverage if many reasoning paths are generated. They further discover that it may be more costeffective to generate more samples with a weaker model than using larger LLMs when solving some simpler problems; however, stronger LLMs are preferred when solving more difficult problems. [114] investigates the \"compute-optimal\" strategy for scaling inference-time compute in multiple aspects when generating a right reasoning path. When using a reward model to search for a good reasoning path, they evaluated different search strategies, including best-of-N search, beam search and lookahead search, and conclude that beam search is preferable for harder problems and lower computational budgets, while best-of-N is more effective for easier problems and higher budgets. Another aspect is to update the proposal distribution of the generator model to increase the probability of generating a good reasoning path. An option is to generate multiple reasoning path in parallel, while the other is to use a fine-tuned LLM to iteratively revise their own answers, which results in a sequential test-time inference. They show that easier questions benefit from sequential inference while harder problems require some ratio of sequential to parallel inference. Compute resources can also be allocated to pre-training. To solve hard problems, some compute resources should be used for pre-training while for easier problems, we only need to use compute resources for inference.",
            "score": 0.5541665287076163,
            "section_title": "Multi-step reasoning",
            "char_start_offset": 24575,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 23,
                    "end": 94
                },
                {
                    "start": 95,
                    "end": 175
                },
                {
                    "start": 176,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 524
                },
                {
                    "start": 525,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1244
                },
                {
                    "start": 1245,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1727
                },
                {
                    "start": 1728,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1943
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1502685546875
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "We explore the following question: Given a fixed FLOPs budget, how should one select an optimal model size for the policy model, and an effective inference strategy to maximize performance (i.e., accuracy)? We are the first to formulate this problem and study the associated inference-time scaling laws, setting our work apart from previous scaling law studies (Fig. 2). \n\nTo address this, we represent the problem-solving error rate E(N, T ; S) as a function of the number of model parameters N , the number of generated tokens T and the inference strategy S. The computational budget C is a deterministic function FLOPs(N, T ; S), based on N and T . Our goal is to minimize E under the test-time compute constraint FLOPs(N, T, S) = C: \n\nwhere N opt (C) and T opt (C) denote the optimal allocation of a computational budget C. \n\nHere, the inference compute (FLOPs) for a fixed model can be modulated by generating more tokens with the policy model and an inference strategy, e.g., sampling additional candidate solutions and subsequently ranking them using a reward model. As the inference strategies, we primarily consider sampling and tree-search approaches paired with re-ranking or majority voting. This includes greedy search, majority voting, best-of-n, weighted voting, and their tree-search variants.",
            "score": 0.5494231323675274,
            "section_title": "COMPUTE-OPTIMAL INFERENCE FOR PROBLEM-SOLVING",
            "char_start_offset": 10439,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 206
                },
                {
                    "start": 207,
                    "end": 370
                },
                {
                    "start": 373,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 827
                },
                {
                    "start": 830,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1309
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1207275390625
        },
        {
            "corpus_id": "269148735",
            "title": "Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies",
            "text": "In this section, we explore three parametric forms for our scaling laws to fit the data generated from our large-scale experiments.Next, we use our scaling laws to recommend: a) the optimal resource allocation (i.e., model and dataset size) at various training FLOPs values, allowing robustness to be maximized given a budget; and b) the optimal return on investment, i.e., the best robust loss/accuracy that can be achieved for a given compute budget.Finally, we also derive the tradeoff between model size and training compute overhead, showing that there are opportunities to reduce model size (and improve inference speed).",
            "score": 0.5472846347693112,
            "section_title": "Deriving Scaling Laws",
            "char_start_offset": 13519,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 131
                },
                {
                    "start": 131,
                    "end": 452
                },
                {
                    "start": 452,
                    "end": 627
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1727294921875
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "We review the most closely related work in this section.For additional related work, see Appendix G.\n\nScaling laws.Early works on scaling artificial neural networks observe predictable power-law scaling in the training set size and number of model parameters [43,44,93].Alabdulmohsin et al. [2] stress the importance of looking at the extrapolation regime of a scaling law.Yang et al. [124] prescribe architectural and hyperparameter changes when scaling model width to realize performant models; Yang et al. [125] make analogous recommendations when scaling model depth.Bi et al. [13] propose hyperparameter aware scaling laws.Unlike the aforementioned work, our investigation focuses on over-training and predicting downstream accuracy.\n\nHoffmann et al. [45] investigate how the number of model parameters N and training tokens D should be chosen to minimize loss L given a compute budget C. Hoffmann et al. [45] find that when scaling up C, both N and D should be scaled equally up to a multiplicative constant (i.e., N \u221d C \u223c0.5 and D \u221d C \u223c0.5 ) to realize compute-optimality.Appendix C of the Chinchilla paper additionally suggests that these findings hold across three datasets.However, Hoffmann et al. [45] do not verify their scaling laws for training beyond compute-optimality, or for downstream error prediction-both of which are central to our work.\n\nSardana & Frankle [98] propose modifications to the Chinchilla formulation to incorporate inference costs into the definition of compute-optimality and solve for various fixed inference budgets.Their key finding, which is critical for our work, is that when taking into account a large enough inference budget, it is optimal to train smaller models for longer than the original Chinchilla recommendations.\n\nOur work presupposes that over-training can be beneficial.Instead of solving for inference-optimal schemes, we support empirically a predictive theory of scaling in the over-trained regime.Additionally, we provide experiments across many validation and training sets.\n\nFor predicting downstream scaling beyond loss, Isik et al. [47] relate the number of pre-training tokens to downstream cross-entropy and machine translation BLEU score [78] after fine-tuning.",
            "score": 0.5471399378027145,
            "section_title": "Related work",
            "char_start_offset": 23805,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 56
                },
                {
                    "start": 56,
                    "end": 100
                },
                {
                    "start": 102,
                    "end": 115
                },
                {
                    "start": 115,
                    "end": 270
                },
                {
                    "start": 270,
                    "end": 373
                },
                {
                    "start": 373,
                    "end": 571
                },
                {
                    "start": 571,
                    "end": 628
                },
                {
                    "start": 628,
                    "end": 738
                },
                {
                    "start": 740,
                    "end": 1079
                },
                {
                    "start": 1079,
                    "end": 1183
                },
                {
                    "start": 1183,
                    "end": 1359
                },
                {
                    "start": 1361,
                    "end": 1555
                },
                {
                    "start": 1555,
                    "end": 1766
                },
                {
                    "start": 1768,
                    "end": 1826
                },
                {
                    "start": 1826,
                    "end": 1957
                },
                {
                    "start": 1957,
                    "end": 2035
                },
                {
                    "start": 2037,
                    "end": 2228
                }
            ],
            "ref_mentions": [
                {
                    "start": 263,
                    "end": 266,
                    "matchedPaperCorpusId": "59617568"
                },
                {
                    "start": 266,
                    "end": 269,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 291,
                    "end": 294,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 385,
                    "end": 390,
                    "matchedPaperCorpusId": "247292726"
                },
                {
                    "start": 509,
                    "end": 514,
                    "matchedPaperCorpusId": "227229101"
                },
                {
                    "start": 756,
                    "end": 760,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 910,
                    "end": 914,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1208,
                    "end": 1212,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1379,
                    "end": 1383,
                    "matchedPaperCorpusId": "266693796"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.35400390625
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e.,\"Chinchilla optimal\"regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute. Our experiments are available at https://github.com/mlfoundations/scaling.",
            "score": 0.5453417131758493,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.174072265625
        },
        {
            "corpus_id": "275993956",
            "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer",
            "text": "Training Scaling in LLM and DiT Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34]. Furthermore, extensive experiments on scaling both denoising backbones and training sets reveal that increasing transformer blocks is more parameter-efficient than increasing channel numbers for improving text-image alignment. The quality and diversity of the training set prove more crucial than mere dataset size [33]. These findings provide valuable insights for determining optimal model architectures and data requirements in both domains. \n\nInference Scaling Law Recent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship. Building upon this, self-consistency approaches demonstrated that sampling multiple reasoning paths and selecting the most consistent answer can substantially improve model accuracy [53]. This was further enhanced by progressive-hint prompting techniques [54], achieving significant gains on various reasoning benchmarks. Recent theoretical work [55] shows that smaller models paired with advanced inference algorithms can outperform larger models under the same computation budget. However, studies on compound inference systems [56] reveal that increasing LLM calls shows non-monotonic behavior, performing better on \"easy\" queries but worse on \"hard\" ones. These findings collectively demonstrate the importance of optimizing inference strategies rather than simply scaling up model size or increasing the sampling budget. Concurrent works [37,38] have also independently explored and validated the effectiveness of inference scaling in diffusion models.",
            "score": 0.5448149705569943,
            "section_title": "A. Full Related Work",
            "char_start_offset": 25352,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 139
                },
                {
                    "start": 140,
                    "end": 290
                },
                {
                    "start": 291,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 593
                },
                {
                    "start": 594,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 914
                },
                {
                    "start": 915,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1167
                },
                {
                    "start": 1168,
                    "end": 1346
                },
                {
                    "start": 1347,
                    "end": 1534
                },
                {
                    "start": 1535,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1829
                },
                {
                    "start": 1830,
                    "end": 2006
                },
                {
                    "start": 2007,
                    "end": 2172
                },
                {
                    "start": 2173,
                    "end": 2304
                }
            ],
            "ref_mentions": [
                {
                    "start": 105,
                    "end": 108,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 120,
                    "end": 124,
                    "matchedPaperCorpusId": "268875775"
                },
                {
                    "start": 909,
                    "end": 913,
                    "matchedPaperCorpusId": "268875775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.415283203125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Kaplan et al. and Hoffmann et al. developed influential scaling laws for the optimal model size as a function of the compute budget, but these laws yield substantially different predictions. We explain the discrepancy by reproducing the Kaplan scaling law on two datasets (OpenWebText2 and RefinedWeb) and identifying three factors causing the difference: last layer computational cost, warmup duration, and scale-dependent optimizer tuning. With these factors corrected, we obtain excellent agreement with the Hoffmann et al. (i.e.,\"Chinchilla\") scaling law. Counter to a hypothesis of Hoffmann et al., we find that careful learning rate decay is not essential for the validity of their scaling law. As a secondary result, we derive scaling laws for the optimal learning rate and batch size, finding that tuning the AdamW $\\beta_2$ parameter is essential at lower batch sizes.",
            "score": 0.5430443496781514,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.327880859375
        },
        {
            "corpus_id": "270703481",
            "title": "Scaling Laws for Linear Complexity Language Models",
            "text": "Scaling laws in large language models aim for an ideal balance between increasing the number of parameters and enlarging the training corpus, given limited computation resources (Kaplan et al., 2020;Henighan et al., 2020;Hernandez et al., 2021;Hoffmann et al., 2022;Clark et al., 2022).The initial scaling laws (Kaplan et al., 2020) use the test-time cross-entropy loss as a regression target to investigate its power-law correlations with model size, dataset size and training computation budget.Hoffmann et al. ( 2022) use three approaches to find the optimal model size and dataset size given a fixed computation budget.By 1) freezing model size and varying number of training tokens, 2) fixing FLOPs and changing model sizes and dataset sizes and 3) directly solving a constrained optimization equation, they conclude that models and the training corpus should be scaled equally when enlarging computing resources.They use the revised scaling law to train a compute-optimal model, Chinchilla, that stands out across various benchmarks.Other works extend scaling laws to multiple modalities (Henighan et al., 2020), mixture of expert models (Clark et al., 2022) and reinforcement learning (Hilton et al., 2023).Recently, Su et al. (2024);Bi et al. (2024) studied the influence of additional factors such as learning rate, context length, and batch size on the scaling-law coefficients.(Isik et al., 2024) studies scaling laws of downstream task performance in a transfer learning setting for the machine translation task.",
            "score": 0.542546780489402,
            "section_title": "Related work",
            "char_start_offset": 22035,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 286
                },
                {
                    "start": 286,
                    "end": 497
                },
                {
                    "start": 497,
                    "end": 623
                },
                {
                    "start": 623,
                    "end": 918
                },
                {
                    "start": 918,
                    "end": 1039
                },
                {
                    "start": 1039,
                    "end": 1214
                },
                {
                    "start": 1214,
                    "end": 1388
                },
                {
                    "start": 1388,
                    "end": 1524
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.5947265625
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Training large language models is expensive.Furthermore, training high-quality models requires a complex recipe of algorithmic techniques and training data.To reduce the cost of finding successful training recipes, researchers first evaluate ideas with small experiments and then extrapolate their efficacy to larger model and data regimes via scaling laws.With reliable extrapolation, it is possible to quickly iterate at small scale and still pick the method that will perform best for the final large training run.Indeed, this workflow has become commonplace for training state-of-the-art language models like Chinchilla 70B [45], PaLM 540B [19], GPT-4 [76], and many others.We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training.We are able to extrapolate, in both N and M , the validation performance of models requiring more than 300\u00d7 the training compute used to construct the scaling law.(right) We also fit a scaling law to predict average downstream top-1 error as a function of validation loss.We find that fitting scaling laws for downstream error benefits from using more expensive models when compared to fitting for loss prediction.We predict the average error over 17 downstream tasks for models trained with over 20\u00d7 the compute.For this figure, we train all models on RedPajama [112].\n\nDespite their importance for model development, published scaling laws differ from the goals of training state-of-the-art models in important ways.For instance, scaling studies usually focus on the compute-optimal training regime (\"Chinchilla optimality\" [45]), where model and dataset size are set to yield minimum loss for a given compute budget.However, this setting ignores inference costs.As larger models are more expensive at inference, it is now common practice to over-train smaller models [113].Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.",
            "score": 0.5406947006422507,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 44
                },
                {
                    "start": 44,
                    "end": 156
                },
                {
                    "start": 156,
                    "end": 357
                },
                {
                    "start": 357,
                    "end": 517
                },
                {
                    "start": 517,
                    "end": 678
                },
                {
                    "start": 678,
                    "end": 958
                },
                {
                    "start": 958,
                    "end": 1121
                },
                {
                    "start": 1121,
                    "end": 1230
                },
                {
                    "start": 1230,
                    "end": 1372
                },
                {
                    "start": 1372,
                    "end": 1471
                },
                {
                    "start": 1471,
                    "end": 1527
                },
                {
                    "start": 1529,
                    "end": 1676
                },
                {
                    "start": 1676,
                    "end": 1877
                },
                {
                    "start": 1877,
                    "end": 1923
                },
                {
                    "start": 1923,
                    "end": 2034
                },
                {
                    "start": 2034,
                    "end": 2207
                }
            ],
            "ref_mentions": [
                {
                    "start": 628,
                    "end": 632,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 644,
                    "end": 648,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1784,
                    "end": 1788,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.196533203125
        },
        {
            "corpus_id": "273507611",
            "title": "Non-myopic Generation of Language Models for Reasoning and Planning",
            "text": "We follow setting in \u00a75.3 using reward model Math-Shepherd. In Figure 6, we plot the inference scaling law of various methods and observe that: (i) Sampling-based methods have better computation efficiency compared to search-based methods. However autoregressive generation using simple reward ranking tends to saturate when more computation is available. (ii) Predictive-Decoding which directly samples from optimal solution space achieves better scaling law than all sampling and search baselines. Predictive-Decoding performance also consistently improves with more computation.",
            "score": 0.5405431576088081,
            "section_title": "DISCUSSION: INFERENCE SCALING LAW",
            "char_start_offset": 29748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 59
                },
                {
                    "start": 60,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 581
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0321044921875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Inference Scaling Laws. We can better allocate resources by understanding the characteristics of different inference scaling strategies. [35] study how scaling test-time compute, through search against dense rewards and adaptive response updates impacts reasoning performance, revealing prompt difficultly as a key factor. In contrast, our work takes into account the verification budget and studies the trade-off between allocating compute to generate new candidate solutions versus verifying existing ones. [39] investigate compute-optimal scaling, specifically examining the trade-off between model size and generating multiple samples. Consistent with findings in [5,6,39], they demonstrate that sampling multiple times from a smaller model can outperform a larger and stronger model within a fixed budget. However, we focus our scaling analysis on the trade-off between generating additional solution candidates and generating verifications for existing solutions. [34] argue that test-time compute scaling without verification is suboptimal which is consistent with our overall findings.",
            "score": 0.5385485267540737,
            "section_title": "Related Work",
            "char_start_offset": 29257,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 136
                },
                {
                    "start": 137,
                    "end": 322
                },
                {
                    "start": 323,
                    "end": 508
                },
                {
                    "start": 509,
                    "end": 639
                },
                {
                    "start": 640,
                    "end": 810
                },
                {
                    "start": 811,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1093
                }
            ],
            "ref_mentions": [
                {
                    "start": 668,
                    "end": 671,
                    "matchedPaperCorpusId": "272146630"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10894775390625
        },
        {
            "corpus_id": "267028569",
            "title": "One-Step Diffusion Distillation via Deep Equilibrium Models",
            "text": "Implicit models are different from explicit models as they utilize more computation through weight-tying under similar parameters and model designs. Therefore, it is natural to question whether their scaling laws align with those of their explicit counterparts. \n\nScaling Model Size. We conduct extensive experiments to understand the trends of sample quality as we scale the model size of GET. Table 2 provides a summary of our findings on single-step unconditional image generation. We find that even small GET models with 10-20M parameters can generate images with sample quality on par with NAS-derived AutoGAN [31]. In general, sample quality improves with the increase in model size. \n\nScaling Training Compute. Our experimental results support the findings of Peebles and Xie [75] for explicit models (DiT) and extend them to implicit models. Specifically, for both implicit and explicit models, larger models are better at exploiting training FLOPs. Figure 3 shows that larger models eventually outperform smaller models when the training compute increases. For implicit models, there also exists a \"sweet spot\" in terms of model size under a fixed training budget, e.g., GET-Small outperforms both smaller and larger GETs at 2 31 training GFLOPs. Furthermore, because of the internal dynamics of implicit models, they can match a much larger explicit model in terms of performance while using fewer parameters. This underscores the potential of implicit models as candidates for compute-optimal models [37] with substantially better parameter efficiency. For example, at 2 31 training GFLOPs, Figure 3(b) suggests that we should choose GET-Small (31.2M) among implicit models for the best performance, which is much more parameter efficient and faster in sampling than the best-performing explicit model, ViT-L (302M), at this training budget. \n\nComparizon of NFEs of teacher model. Offline distillation requires significantly fewer number of function evaluations (NFEs) for the teacher network compared to other online distillation methods. In the experimental setup used in this paper, GET requires 35M overall NFEs for the teacher model, as we train on 1M data samples, and use 35 NFEs to generate each data sample with EDM.",
            "score": 0.5363211746442202,
            "section_title": "Experiment Results",
            "char_start_offset": 14182,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 394
                },
                {
                    "start": 395,
                    "end": 484
                },
                {
                    "start": 485,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 689
                },
                {
                    "start": 692,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1065
                },
                {
                    "start": 1066,
                    "end": 1255
                },
                {
                    "start": 1256,
                    "end": 1419
                },
                {
                    "start": 1420,
                    "end": 1563
                },
                {
                    "start": 1564,
                    "end": 1852
                },
                {
                    "start": 1855,
                    "end": 1891
                },
                {
                    "start": 1892,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2236
                }
            ],
            "ref_mentions": [
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "199543358"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04248046875
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "However, as demand increases, inference costs becomes a significant factor. For a 7B-Chinchilla-quality model with an inference demand of 10 11 tokens, our formula suggests the compute-optimal method is to train a 6B parameter model on 1.18\u00d7 the original (Chinchilla-prescribed) amount of data. For higher quality models (i.e. models that are larger and/or trained for longer), the volume of inference demand required to shift the scaling law increases: An LLM developer that expects a 30B-Chinchilla-quality model will see 10 13 tokens during inference can reduce their total FLOPs by 28% by training a 13.6B model on 2.84\u00d7 the data. We provide additional results in Sec. B.1 in the Appendix.",
            "score": 0.5341483098748746,
            "section_title": "Computational Optimality",
            "char_start_offset": 9330,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 294
                },
                {
                    "start": 295,
                    "end": 326
                },
                {
                    "start": 327,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 672
                },
                {
                    "start": 673,
                    "end": 693
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0284423828125
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "We propose an alternative approach leveraging insights from the estimation of the FLOPs itself. Prior work [26,30] usually considers a fixed compute budget in FLOPs and then aims to minimize loss by finding the optimal allocation to model parameters N and training tokens D. Here we flip this recipe on its head following recent work [57]. We aim to find the minimum FLOPs to achieve a certain loss L u (N nv , V, H) = \u2113 through optimal allocation of the vocabulary size V : \n\nBy computing the minimum point of FLOPs C with respect to V via derivative: \n\nwe can estimate the optimal V under the assumption that it can achieve a certain loss \n\nThe parameters a, b and c can be easily obtained from building f (V ) ( \u00a72.2). In theory, as long as the non-vocabulary parameters N nv are provided, V can be numerically searched via the solution of \u2202C \u2202V = 0. More details are in \u00a7A.1. Usage When the compute allocation is near optimal, the loss exhibits a power-law relationship with respect to the FLOPs budget, as described by the scaling law [30]. This relationship allows us to use FLOPs as a reliable proxy for observing the scaling behavior of the optimal vocabulary parameters. In practice, we can first determine an empirically optimal vocabulary size in a low-cost setting (e.g., finding the compute-optimal vocabulary parameters on a small model). Then, we can scale the optimal vocabulary parameters proportionally based on \u03b3. Specifically, we obtain a set of derivative-optimal vocabulary parameters N v for different non-vocabulary parameters N nv , represented as \n\nWe then fit the relationship between N nv and N v using the power-law function N v \u221d N \u03b3 nv . This results in the scaling equation: \n\nnv is a small model (e.g., 33M), and N 0 v is the searched optimal vocabulary parameter. By combining the \u03b3 from the derivative and the empirical solution on a small model, we can estimate the optimal vocabulary by: \n\nwhere the scaling proportion \u03b3 = 0.83 after our fitting. Consistent with the observation in Approach 1, we find that non-vocabulary parameters should be scaled faster than vocabulary parameters to achieve an optimal allocation.",
            "score": 0.5338408750383591,
            "section_title": "Approach 2: Derivative-based fast estimation",
            "char_start_offset": 17735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 95
                },
                {
                    "start": 96,
                    "end": 274
                },
                {
                    "start": 275,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 474
                },
                {
                    "start": 477,
                    "end": 552
                },
                {
                    "start": 555,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 853
                },
                {
                    "start": 854,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1045
                },
                {
                    "start": 1046,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1432
                },
                {
                    "start": 1433,
                    "end": 1572
                },
                {
                    "start": 1575,
                    "end": 1668
                },
                {
                    "start": 1669,
                    "end": 1706
                },
                {
                    "start": 1709,
                    "end": 1797
                },
                {
                    "start": 1798,
                    "end": 1924
                },
                {
                    "start": 1927,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2154
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05499267578125
        },
        {
            "corpus_id": "252683988",
            "title": "Calibrating Sequence likelihood Improves Conditional Language Generation",
            "text": "Scaling properties are important for projecting a technique's future relevance as models scale up (Kaplan et al., 2020a). In Figure 3, we compare generation quality versus inference compute at different model sizes and number of decoding candidates using beam search. Appendix F describes the method to estimate inference compute FLOPs. As mentioned earlier in subsection 3.4, fine-tuned-only models have optimal decoding beam sizes while calibrated models' performance monotonically increase with larger decoding beam sizes. Even in the case of greedy decoding (beam size of 1), the calibrated models' performance exceeds the fine-tuned-only models, by a large margin for some datasets (CNN/DailyMail and RedditTIFUlong). Their gaps grow larger with increasing number of beam sizes. \n\nThe magnitude of quality improvement from calibration persists over models sizes spanning from 50M to 2B. There is no obvious sign of diminishing return as model size scales up. \n\nInference compute may be used for decoding rather than on larger models. A calibrated model, once trained, can improve its performance by decoding more candidates, usually more effectively in the beginning, although returns diminish over 10 candidates. In some cases (SAMSum and especially CNN/DailyMail), a smaller model decoding more candidates can beat a larger one at both quality and efficiency. \n\nTL;DR: Calibration benefits persist as model sizes scale up. Smaller calibrated models can outperform larger ones under the same inference compute budget.",
            "score": 0.5308578006396919,
            "section_title": "SCALING PROPERTIES OF CALIBRATED MODELS",
            "char_start_offset": 16748,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 121
                },
                {
                    "start": 122,
                    "end": 267
                },
                {
                    "start": 268,
                    "end": 336
                },
                {
                    "start": 337,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 783
                },
                {
                    "start": 786,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 963
                },
                {
                    "start": 966,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1366
                },
                {
                    "start": 1369,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1523
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036285400390625
        },
        {
            "corpus_id": "259243822",
            "title": "Scaling MLPs: A Tale of Inductive Bias",
            "text": "Parameters or examples. Given a fixed level of compute C, what is the optimal way to allocate it to parameter count P and number of examples N ? In order to be more comparable to previous work, we assume a fixed training time T = 50. To answer this question, we follow the approach outlined in Hoffmann et al. ( 2022) and plot the optimal compute models identified in Fig. 1 both against model size P and number of examples N . We visualize the results in Fig. 7. We empirically observe that the optimal parameter count P * (C) and dataset size N * (C) as a function of compute C exhibit power-law behaviour of the approximate form \n\nWhile for transformers, the number of examples (or tokens) N and parameters P are scaled equally (Hoffmann et al., 2022) (i.e. \u03b1 P \u2248 \u03b1 N \u2248 0.5), in contrast we observe that the optimal strategy for MLPs invests significantly more compute into dataset size N . This is further evidence for the weaker inductive bias present in MLPs, which needs more examples in order to be compensated for.",
            "score": 0.5286531915100399,
            "section_title": "Scaling Laws",
            "char_start_offset": 27049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 23
                },
                {
                    "start": 24,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 233
                },
                {
                    "start": 234,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 463
                },
                {
                    "start": 464,
                    "end": 631
                },
                {
                    "start": 634,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 893
                },
                {
                    "start": 894,
                    "end": 1023
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054290771484375
        },
        {
            "corpus_id": "276107826",
            "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
            "text": "Recent advances in language models highlight the importance of test-time compute scaling wherein one uses more compute during inference to enhance reasoning capabilities [OpenAI, 2024, Team, 2025, Agarwal et al., 2024, Wei et al., 2022, Yao et al., 2023, Aky\u00fcrek et al., 2024]. There are many methods for increasing test-time compute usage, including implicitly encouraging longer responses via reinforcement learning [OpenAI, 2024, Team, 2025] or explicitly via prompting [Wei et al., 2022, Yao et al., 2023]. However, sampling-based search-an instance of the generate-and-test approach where a model generates many responses in parallel, e.g. via random sampling or delegation, and selects what the model guesses to be the best one-remains one of the most natural and fundamental paradigms. In addition to being complementary with other test-time compute scaling strategies, it also has the unique advantage of being embarrassingly parallel and allowing for arbitrarily scaling: simply sample more responses [Cobbe et al., 2021, Wang et al., 2023]. As a result, sampling-based search plays an increasingly crucial role as language 1 arXiv:2502.01839v2 [cs.LG] 20 Feb 2025 models are set loose on frontier mathematical and scientific problems where inference compute budgets reach thousands of dollars or more per problem. \n\nThough recent works demonstrate the benefits of sampling-based search [Cobbe et al., 2021, Wang et al., 2023, Xue et al., 2023], many questions remain as to what scaling trends govern this fundamental test-time compute scaling strategy. To develop this understanding, we study a minimalist-yet remarkably effective-instantiation of sampling-based search that uses a language model [Gemini Team, 2024] to both generate a set of candidate responses via random sampling and select the best one by attempting to verify each response with natural language. Specifically, we consider the case where models must self-verify their responses to select the best answer, and do not make the strong assumption that one can access groundtruth answers or symbolic systems that exactly verify correctness.",
            "score": 0.5274094894112155,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 644
                },
                {
                    "start": 645,
                    "end": 792
                },
                {
                    "start": 793,
                    "end": 1050
                },
                {
                    "start": 1051,
                    "end": 1153
                },
                {
                    "start": 1154,
                    "end": 1323
                },
                {
                    "start": 1326,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2116
                }
            ],
            "ref_mentions": [
                {
                    "start": 217,
                    "end": 235,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 473,
                    "end": 490,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033538818359375
        },
        {
            "corpus_id": "273233741",
            "title": "Scaling Laws For Diffusion Transformers",
            "text": "Scaling laws in large language models (LLMs) (Kaplan et al., 2020;Hestness et al., 2017;Henighan et al., 2020;Hoffmann et al., 2022) have been widely observed and validated, suggesting that pretraining performance follows a power-law relationship with the compute C. The actual compute could be roughly calculated as C = 6N D, where N is the model size and D is the data quantity. Therefore, determining the scaling law helps us make informed decisions about resource allocation to maximize computational efficiency, namely, figure out the optimal balance between model size and training data (i.e., the optimal model and data scale) given a fixed compute budget. However, scaling laws in diffusion models remain less explored. \n\nThe scalability has already been demonstrated in diffusion models, especially for diffusion transformers (DiT). Specifically, several prior works (Mei et al., 2024;Li et al., 2024) reveal that larger models always result in better visual quality and improved text-image alignment. However, the scaling property of diffusion transformers is clearly observed but not accurately predicted. Besides, the absence of explicit scaling laws also hinders a comprehensive understanding of how training budget relate to model size, data quantity, and loss. As a result, we cannot determine accordingly the optimal model and data sizes for a given compute budget and accurately predict training loss. Instead, heuristic configuration searches of models and data are required, which are costly and challenging to ensure optimal balance. \n\nIn this work, we characterize the scaling behavior of diffusion models for text-to-image synthesis, resulting in the explicit scaling laws of DiT for the first time. To investigate the explicit relationship between pretraining loss and compute, a wide range of compute budgets from 1e17 to 6e18 FLOPs are used. Models ranging from 1M to 1B are pretrained under given compute budgets. As shown in Fig. 1, for each compute budget, we can fit a parabola and extract an optimal point that corresponds to the optimal model size and consumed data under that specific compute constraint. Using these optimal configurations, we derive scaling laws by fitting a power-law relationship between compute budgets, model size, consumed data, and training loss.",
            "score": 0.5235193882114848,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 663
                },
                {
                    "start": 664,
                    "end": 727
                },
                {
                    "start": 730,
                    "end": 841
                },
                {
                    "start": 842,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1275
                },
                {
                    "start": 1276,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1553
                },
                {
                    "start": 1556,
                    "end": 1721
                },
                {
                    "start": 1722,
                    "end": 1866
                },
                {
                    "start": 1867,
                    "end": 1939
                },
                {
                    "start": 1940,
                    "end": 2136
                },
                {
                    "start": 2137,
                    "end": 2302
                }
            ],
            "ref_mentions": [
                {
                    "start": 894,
                    "end": 910,
                    "matchedPaperCorpusId": "268875775"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.354736328125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Scaling laws. Recent research on scaling laws has established that model performance follows predictable power-law relationships with respect to the number of parameters, the size of the training dataset, and the available compute (Hestness et al., 2017;Rosenfeld et al., 2020). The seminal work by Kaplan et al. (2020) demonstrates that the test loss of language models decays as a function of model size and data in a highly regular manner. Subsequent studies refine these initial observations and extend them into more diverse settings (Hoffmann et al., 2022;Alabdulmohsin et al., 2022;Muennighoff et al., 2023;Lin et al., 2024;Goyal et al., 2024b). However, most of these existing works are primarily focused on the training regime. Inference strategies and inference-time compute utilization in LLM problem-solving. A variety of inference strategies have been developed to generate sequences with a trained model (Welleck et al., 2024). Deterministic methods such as greedy decoding and beam search (Teller, 2000;Graves, 2012) find highly probable sequences which typically have decent quality but lacks diversity. Sampling algorithms (e.g., temperature sampling (Ackley et al., 1985)) can produce a diverse set of results which are then aggregated to achieve higher accuracy (e.g., via the self-consistency approach (Wang et al., 2023a)). Recent methods combine search algorithms with LLMs, including breadth-first or depth-first search (Yao et al., 2023), Monte-Carlo Tree Search (MCTS) (Zhang et al., 2023;Zhou et al., 2024;Liu et al., 2024;Choi et al., 2023), and guided beam search (Xie et al., 2023). Several prior studies also find that LLM problem-solving performance can be improved by outputting \"dummy\" tokens at inference time (Goyal et al., 2024a;Pfau et al., 2024).",
            "score": 0.5224199883784622,
            "section_title": "RELATED WORKS",
            "char_start_offset": 7022,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 652
                },
                {
                    "start": 653,
                    "end": 736
                },
                {
                    "start": 737,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1119
                },
                {
                    "start": 1120,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1611
                },
                {
                    "start": 1612,
                    "end": 1784
                }
            ],
            "ref_mentions": [
                {
                    "start": 254,
                    "end": 277,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 562,
                    "end": 589,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 589,
                    "end": 614,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 614,
                    "end": 631,
                    "matchedPaperCorpusId": "267411718"
                },
                {
                    "start": 631,
                    "end": 651,
                    "matchedPaperCorpusId": "269033049"
                },
                {
                    "start": 918,
                    "end": 940,
                    "matchedPaperCorpusId": "270703266"
                },
                {
                    "start": 1168,
                    "end": 1189,
                    "matchedPaperCorpusId": "12174018"
                },
                {
                    "start": 1322,
                    "end": 1342,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1443,
                    "end": 1461,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1494,
                    "end": 1514,
                    "matchedPaperCorpusId": "257427177"
                },
                {
                    "start": 1532,
                    "end": 1549,
                    "matchedPaperCorpusId": "262824527"
                },
                {
                    "start": 1549,
                    "end": 1567,
                    "matchedPaperCorpusId": "264128245"
                },
                {
                    "start": 1592,
                    "end": 1610,
                    "matchedPaperCorpusId": "258426922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2095947265625
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "Scaling laws consider a computational budget, C, which is measured in FLOPs. The goal is to optimally allocate the compute budget to model parameters N and the number of training tokens D [30,6,26,44]. It can be formulated as: \n\nFollowing Radford et al. [51], the loss function is typically the language modeling loss when evaluating language models, which can be written as: \n\nwhere p(w i |w 1:i\u22121 , V ) is the output probability of word w i given the context w 1:i\u22121 and the tokenizer with vocabulary size V . Generally, the lower L indicates better performance of the language model. However, due to its dependency on V , L cannot be used to compare language models with different vocabulary sizes. Thus, we propose an adaptation later in \u00a72.2. Fitting scaling laws generally requires various models trained for different configurations [23]. A common approach is to select several compute budgets and train models with varying N and D for each budget to find the best one, i.e. the one with the lowest loss (\"IsoFLOPs\") [26]. Using fitting techniques we can then estimate a function that maps from the compute budget to the optimal allocation to N and D.",
            "score": 0.5223912966662394,
            "section_title": "Scaling law",
            "char_start_offset": 6697,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 201
                },
                {
                    "start": 202,
                    "end": 226
                },
                {
                    "start": 229,
                    "end": 375
                },
                {
                    "start": 378,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 586
                },
                {
                    "start": 587,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 747
                },
                {
                    "start": 748,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 1029
                },
                {
                    "start": 1030,
                    "end": 1158
                }
            ],
            "ref_mentions": [
                {
                    "start": 197,
                    "end": 200,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1358642578125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Training curves follow predictable power-laws whose parameters are roughly independent of the model size. By extrapolating the early part of a training curve, we can roughly predict the loss that would be achieved if we trained for much longer. (Section 5) \n\nTransfer improves with test performance: When we evaluate models on text with a different distribution than they were trained on, the results are strongly correlated to those on the training validation set with a roughly constant offset in the loss -in other words, transfer to a different distribution incurs a constant penalty but otherwise improves roughly in line with performance on the training set. (Section 3.2.2) Sample efficiency: Large models are more sample-efficient than small models, reaching the same level of performance with fewer optimization steps (Figure 2) and using fewer data points (Figure 4). \n\nConvergence is inefficient: When working within a fixed compute budget C but without any other restrictions on the model size N or available data D, we attain optimal performance by training very large models and stopping significantly short of convergence (see Figure 3). Maximally compute-efficient training would therefore be far more sample efficient than one might expect based on training small models to convergence, with data requirements growing very slowly as D \u223c C 0.27 with training compute. (Section 6) \n\nOptimal batch size: The ideal batch size for training these models is roughly a power of the loss only, and continues to be determinable by measuring the gradient noise scale [MKAT18]; it is roughly 1-2 million tokens at convergence for the largest models we can train. (Section 5.1) Taken together, these results show that language modeling performance improves smoothly and predictably as we appropriately scale up model size, data, and compute. We expect that larger language models will perform better and be more sample efficient than current models. The optimal model size grows smoothly with the loss target and compute budget Figure 3 As more compute becomes available, we can choose how much to allocate towards training larger models, using larger batches, and training for more steps. We illustrate this for a billion-fold increase in compute. For optimally compute-efficient training, most of the increase should go towards increased model size. A relatively small increase in data is needed to avoid reuse.",
            "score": 0.5219416431395898,
            "section_title": "Universality of training:",
            "char_start_offset": 2894,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 256
                },
                {
                    "start": 259,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 877
                },
                {
                    "start": 880,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1383
                },
                {
                    "start": 1384,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1667
                },
                {
                    "start": 1668,
                    "end": 1845
                },
                {
                    "start": 1846,
                    "end": 1953
                },
                {
                    "start": 1954,
                    "end": 2193
                },
                {
                    "start": 2194,
                    "end": 2252
                },
                {
                    "start": 2253,
                    "end": 2355
                },
                {
                    "start": 2356,
                    "end": 2417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06866455078125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Figure 12 Left: Given a fixed compute budget, a particular model size is optimal, though somewhat larger or smaller models can be trained with minimal additional compute. Right: Models larger than the computeefficient size require fewer steps to train, allowing for potentially faster training if sufficient additional parallelism is possible. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve, after initial transient effects. Figure 13 When adjusting performance to simulate training far below the critical batch size, we find a somewhat altered power law for L(C min ) when compared with the fully empirical results. The conspicuous lump at 10 \u22125 PF-days marks the transition from 1-layer to 2-layer networks; we exclude 1-layer networks in the power-law fits. It is the L(C min ) trend that we expect to provide a reliable extrapolation for larger compute. \n\nthat in fact we could train more efficiently 6 by training at the batch size B crit discussed in Section 5.1. Large and small values of the loss could have been achieved with fewer samples or fewer steps, respectively, and correcting for this inefficiency by standardizing to the critical batch size results in cleaner and more predictable trends. \n\nIn this section we will adjust for this oversight. More importantly, we will use the results of Section 5 to determine the optimal allocation of compute between model size N and the quantity of data processed during training, namely 2B crit S min . We will determine this allocation both empirically and theoretically, by using the equation for L(N, S min ), and we will demonstrate that these methods agree.",
            "score": 0.521862839346007,
            "section_title": "Our framework does not capture early training dynamics",
            "char_start_offset": 28961,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 170
                },
                {
                    "start": 171,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 704
                },
                {
                    "start": 705,
                    "end": 848
                },
                {
                    "start": 849,
                    "end": 945
                },
                {
                    "start": 948,
                    "end": 1057
                },
                {
                    "start": 1058,
                    "end": 1295
                },
                {
                    "start": 1298,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1546
                },
                {
                    "start": 1547,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0970458984375
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Scaling laws for loss.Typically, scaling laws predict model loss L as a function of the compute C in FLOPs used for training.If one increases the number of parameters N in a model or the number of tokens D that a model is trained on, compute requirements naturally increase.Hence, we assume C is a function of N, D. Following Kaplan et al. [51], we use the approximation C = 6N D, which Hoffmann et al. [45] independently verify.We consider,\n\nwhere E is an irreducible loss and L \u2032 is the reducible loss.E captures the Bayes error or minimum possible loss achievable on the validation domain.The L \u2032 (C) term captures what can possibly be learned about the validation domain by training on a source domain.L \u2032 (C) should approach zero with increased training data and model capacity.L \u2032 (C) is often assumed to follow a power law: Hestness et al. [43], OpenAI [76]).It is also often helpful to consider a power law in a log-log plot, where it appears as a line with slope \u2212\u03b7 and y-intercept log (\u03bb).\n\n, the exponent \u03b7 remains relatively constant resulting in lines with approximately fixed slope (Figure 17).The scalar \u03bb that determines the y-intercept, however, shifts with different token multipliers.This suggests \u03bb is a function of the token multiplier, while \u03b7 is not.\n\nToken multipliers.We define a token multiplier M = D/N as the ratio of training tokens to model parameters for notational convenience.M allows us to consider fixed relationships between D and N even as a model gets bigger (i.e., as N becomes larger).\n\nCompute-optimal training.Hoffmann et al. [45] establish compute-optimal training, where, for any compute budget H, the allocation of parameters and tokens is given by, arg min\n\nTo solve for the optimal N * , D * , one can sweep N, D for each compute budget, retaining the best configurations.Hoffmann et al. [45] find that as the compute budget increases, N * and D * scale roughly evenly.",
            "score": 0.5213503681182762,
            "section_title": "Preliminaries",
            "char_start_offset": 5476,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 22
                },
                {
                    "start": 22,
                    "end": 125
                },
                {
                    "start": 125,
                    "end": 274
                },
                {
                    "start": 274,
                    "end": 429
                },
                {
                    "start": 429,
                    "end": 441
                },
                {
                    "start": 443,
                    "end": 504
                },
                {
                    "start": 504,
                    "end": 592
                },
                {
                    "start": 592,
                    "end": 706
                },
                {
                    "start": 706,
                    "end": 783
                },
                {
                    "start": 783,
                    "end": 866
                },
                {
                    "start": 866,
                    "end": 999
                },
                {
                    "start": 1001,
                    "end": 1108
                },
                {
                    "start": 1108,
                    "end": 1203
                },
                {
                    "start": 1203,
                    "end": 1273
                },
                {
                    "start": 1275,
                    "end": 1293
                },
                {
                    "start": 1293,
                    "end": 1409
                },
                {
                    "start": 1409,
                    "end": 1525
                },
                {
                    "start": 1527,
                    "end": 1552
                },
                {
                    "start": 1552,
                    "end": 1702
                },
                {
                    "start": 1704,
                    "end": 1819
                },
                {
                    "start": 1819,
                    "end": 1916
                }
            ],
            "ref_mentions": [
                {
                    "start": 403,
                    "end": 407,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1568,
                    "end": 1572,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.27490234375
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "-The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see \u00a72.1). \n\n-While the trend of increasing active number of parameters is similar across all training compute budgets; the optimal active number of parameters decrease more rapidly with sparsity as the training compute budget increases (see \u00a73). \u2022 Effect of Sparsity on Downstream Performance: For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2019), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022b;Goyal et al., 2024) may address this gap. We use a \"length-controlled\" variant of few-shot Chain-of-Thought (CoT) prompting to study whether this strategy may help close the gap between MoEs and dense models on reasoning tasks in Appendix E. Empirical evidence suggests that MoEs may benefit more from dynamically increased inference-time compute than dense models under a fixed generated tokens budget. \u2022 Parametric Scaling Law: We propose a parametric form for scaling laws that accounts for sparsity. The model coefficients are estimated using the empirical data obtained by training compute-optimal models. An interesting observation from Appendix F is that the exponent for sparsity term \u03bb is negative which is consistent with our intuition that sparser models lead to a lower perplexity.",
            "score": 0.5212053636206392,
            "section_title": "Discussion",
            "char_start_offset": 22382,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 217,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 976
                },
                {
                    "start": 977,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1728
                }
            ],
            "ref_mentions": [
                {
                    "start": 678,
                    "end": 698,
                    "matchedPaperCorpusId": "52055325"
                },
                {
                    "start": 706,
                    "end": 730,
                    "matchedPaperCorpusId": "47018994"
                },
                {
                    "start": 916,
                    "end": 935,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 935,
                    "end": 954,
                    "matchedPaperCorpusId": "263608983"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10650634765625
        },
        {
            "corpus_id": "268379614",
            "title": "Language models scale reliably with over-training and on downstream tasks",
            "text": "Another potential mismatch is that most scaling laws quantify model performance by perplexity in next-token prediction instead of accuracy on widely used benchmark datasets.However, practitioners usually turn to benchmark performance, not loss, to compare models.\n\nIn this paper, we conduct an extensive set of experiments to address both scaling in the over-trained regime and benchmark performance prediction.\n\nMotivated by the practice of training beyond compute-optimality, we first investigate whether scaling follows reliable trends in the over-trained regime.We notice, as implied by Hoffmann et al. [45], for a set of models of different sizes trained with a constant ratio of tokens to parameters, models' reducible loss L \u2032 [43,45] follows a power law (L \u2032 = \u03bb \u2022 C \u2212\u03b7 ) in the amount of training compute C. We find that as one increases the ratio of tokens to parameters, corresponding to more over-training, the scaling exponent \u03b7 remains about the same, while the scalar \u03bb changes.We explain our observations by reparameterizing existing scaling laws in relation to the amount of over-training.\n\nTo establish empirically that scaling extrapolates in the over-trained regime, we further experiment with a testbed of 104 models, trained from scratch on three different datasets: C4 [27,88], RedPajama [112], and RefinedWeb [82].We find that scaling laws fit to small models can accurately predict the performance of larger models that undergo more over-training.Figure 1 (left) illustrates our main over-training result, where we invest 2.4e19 FLOPs to extrapolate the C4 validation performance of a 1.4B parameter model trained on 900B tokens, which requires 300\u00d7 more compute to train.\n\nIn addition to over-training, we also investigate if scaling laws can predict the performance of a model on downstream tasks.We establish a power law relationship between language modeling perplexity and the average top-1 error on a suite of downstream tasks.While it can be difficult to predict the error on individual tasks, we find it possible to predict aggregate performance from a model's perplexity among models trained on the same training data.",
            "score": 0.5210292751959562,
            "section_title": "Introduction",
            "char_start_offset": 2049,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 173,
                    "end": 263
                },
                {
                    "start": 265,
                    "end": 411
                },
                {
                    "start": 413,
                    "end": 566
                },
                {
                    "start": 566,
                    "end": 993
                },
                {
                    "start": 993,
                    "end": 1106
                },
                {
                    "start": 1108,
                    "end": 1338
                },
                {
                    "start": 1338,
                    "end": 1472
                },
                {
                    "start": 1472,
                    "end": 1697
                },
                {
                    "start": 1699,
                    "end": 1824
                },
                {
                    "start": 1824,
                    "end": 1958
                },
                {
                    "start": 1958,
                    "end": 2152
                }
            ],
            "ref_mentions": [
                {
                    "start": 607,
                    "end": 611,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 738,
                    "end": 741,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1292,
                    "end": 1296,
                    "matchedPaperCorpusId": "237568724"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "270068102",
            "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
            "text": "However, they did not investigate the relation of weight averaging to compute optimality and its implications for scaling experiments. \n\nScaling Law Experiments for Neural Language Models. Kaplan et al. (2020) were the first to establish scaling laws for language models by training a suite of models for a fixed token count. Important to our work, Hoffmann et al. (2022) revise their laws and demonstrate specific methods to establish laws, notably training a family of models for different cosine lengths. The subsequent models like LLama and LLama2 (Touvron et al., 2023a,b) further improve performance of smaller models by training beyond the Chinchilla optimal point, motivated by lower inference costs (Gadre et al., 2024;De Vries, 2023;Sardana & Frankle, 2023). Recent works (Muennighoff et al., 2023;Bi et al., 2024;Goyal et al., 2024) highlight how data repetition and quality affect the scaling behavior, which suggests that scaling laws should be updated more frequently. However, these works do not consider efficient experiments for scaling laws, which is the focus of our work.",
            "score": 0.5209528960553949,
            "section_title": "Related work",
            "char_start_offset": 25271,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 134
                },
                {
                    "start": 137,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 325
                },
                {
                    "start": 326,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 982
                },
                {
                    "start": 983,
                    "end": 1091
                }
            ],
            "ref_mentions": [
                {
                    "start": 349,
                    "end": 371,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.251953125
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We can solve A.1 to find an expression for the amount of compute needed to reach a given value of the loss L with a model of size N : \n\nUsing A.6 and A.9, we can eliminate L in favor of N eff (L), the model size which reaches L most efficiently. \n\nFrom there, we find an expression for the excess compute needed as a consequence of using a suboptimal model size: \n\nThe result is shown in Figure X. Models between 0.6x and 2.2x the optimal size can be used with only a 20% increase in compute budget. Using a smaller model is useful when accounting for the cost inference. A larger model can be trained the the same level of performance in fewer steps, allowing for more parallelism and faster training if sufficient harware is available (see Figure Y): \n\nA 2.2x larger model requires 45% fewer steps at a cost of 20% more training compute. Note that this equation should not be trusted for very large models, as it is only valid in the power-law region of the learning curve after initial transient effects.",
            "score": 0.5199082607357335,
            "section_title": "B.4 Suboptimal Model Sizes",
            "char_start_offset": 45287,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 136,
                    "end": 245
                },
                {
                    "start": 248,
                    "end": 362
                },
                {
                    "start": 365,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 499
                },
                {
                    "start": 500,
                    "end": 571
                },
                {
                    "start": 572,
                    "end": 752
                },
                {
                    "start": 755,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1007
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0633544921875
        },
        {
            "corpus_id": "276575434",
            "title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs",
            "text": "To derive scaling laws to compare D2Z and 10\u00d7 models, we used the power law functional form y = cx m , where x is the pre-training FLOPs, y is the loss on the Pile validation set, and c and m are parameters to be fit. To fit these parameters, we transformed our Loss-to-FLOPs equation, y = cx m , to a logarithmic form, log(y) = log(c) + mlog(x), and fit the slope and intercept parameters of this line using a least-squares linear regression. We trained models at four sizes to compute-optimal 20 TPP (Table 6), and computed total FLOPs spent as well as validation loss on the Pile. We then fit the power law free parameters to obtain our scaling laws. \n\nEncouragingly, here we find the scaling law slope of D2Z is roughly 2.5% better than 10\u00d7 decay (Figure 26). This translates to an improvement of roughly 1% at 1.3B and 2.7B scales, broadly similar to our earlier results at 1.7B scale. Projecting our scaling law to a 70B model trained to a compute optimal 20 TPP, D2Z would achieve a roughly 2% loss improvement over 10\u00d7 decay. we can also use the extended EMA perspective in order to design novel LR schedules that have desirable blends of weight updates. Optimizing these coefficients thus provides a dual to optimizing the LR decay function, which is why we refer to these coefficients as the dual coefficients of the LR schedule. In this section, we first describe the design of one such LR schedule, and then we follow with example plots for this and other schedules.",
            "score": 0.519672192054089,
            "section_title": "C.10 NANOGPT EXPERIMENTS",
            "char_start_offset": 54761,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 583
                },
                {
                    "start": 584,
                    "end": 653
                },
                {
                    "start": 656,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 890
                },
                {
                    "start": 891,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1162
                },
                {
                    "start": 1163,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1478
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0294189453125
        },
        {
            "corpus_id": "277502385",
            "title": "ToolACE-R: Tool Learning with Adaptive Self-Refinement",
            "text": "In this subsection, we examine the effects of our two iterative modules, i.e., model-aware iterative training and adaptive self-refine iterative inference. \n\nIterative Training. . \"Vanilla Self-Refine\" always picks the final answer at each iteration. \"Adaptive Self-Refine\" is our proposed method, while \"Self-Consistency\" selects the majority answer during the iteration. monotonically across iterations, suggesting that the model's performance on the training set improves, as the selected samples are those the model is more likely to answer correctly (as indicated by pass@k). On the other hand, accuracy improves rapidly during the first two iterations, reaching its peak at iteration 3. Subsequently, accuracy plateaus, with no further improvement in later iterations. This suggests that excessive training could lead to overfitting on the training set. \n\nIterative Inference. To assess the effects of our adaptive self-refine method for iterative inference, we conducted additional experiments comparing it with two other inference-time scaling approaches. The first, termed \"Vanilla Self-Refine,\" always selects the last answer at each iteration as the final answer. The second method, \"Self-Consistency at Itr n\", chooses the majority answer from the answers collected during iterations 1 to n. The results of these inference methods on BFCL, all based on the same model, are shown in Figure 3. \n\nAs observed, the performance of \"Vanilla Self-Refine\" fluctuates across iterations, indicating that the model does not always refine the answer correctly. In contrast, our \"Adaptive Self-Refine\" method demonstrates more stability across iterations, suggesting that adjacent identical answers serve as a reliable signal for stopping further iterations. The \"Self-Consistency at Itr n\" method likely represents an upper bound for answer selection, with our method achieving competitive performance compared to this strong baseline. Furthermore, the performance of our method remains consistent across different iteration counts (3, 5, 10), likely because most cases terminate early, well Table 4: Accuracy performance on BFCL Live category when data selection with Pass@8 and Pass@1. \"Direct\" and \"Refine\" indicate the results with direct output and adaptive self-refine, respectively.",
            "score": 0.5185940068971171,
            "section_title": "Effects of Iterative",
            "char_start_offset": 18760,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 155
                },
                {
                    "start": 158,
                    "end": 179
                },
                {
                    "start": 180,
                    "end": 250
                },
                {
                    "start": 251,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 859
                },
                {
                    "start": 862,
                    "end": 882
                },
                {
                    "start": 883,
                    "end": 1063
                },
                {
                    "start": 1064,
                    "end": 1174
                },
                {
                    "start": 1175,
                    "end": 1303
                },
                {
                    "start": 1304,
                    "end": 1403
                },
                {
                    "start": 1406,
                    "end": 1560
                },
                {
                    "start": 1561,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1935
                },
                {
                    "start": 1936,
                    "end": 2187
                },
                {
                    "start": 2188,
                    "end": 2289
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0108184814453125
        },
        {
            "corpus_id": "268351635",
            "title": "Unraveling the Mystery of Scaling Laws: Part I",
            "text": "The significance of scaling laws extends beyond mere prediction of the loss trajectory.More importantly, they can aid in pinpointing the optimal experimental configuration without requiring extensive tuning on very large models, thereby transforming the training of large language models from an alchemy-like trialand-error process into a principled methodology.In this section, we highlight main benefits of scaling laws and discuss ways to further advance beyond them.\n\nDetermining B As long as all hyperparameters are well-tuned (especially the learning rate and regularization hyperparameters) and the number of training steps is sufficient, it is believed that the same final performance should be attainable using any batch size [SLA + 19], so the batch size mainly influences the training speed of language models.Often, when training large language models, the ideal batch size is suggested to be set as the largest batch size supported by the available hardware [GDG + 23], so as to maximize the training speed without considering the computational cost.In Eq 3.12, we show that the critical batch size with the optimal speed/computation trade-off can be analytically computed from the loss value.Under the guidance of this formula, we would be able to estimate the preferred batch size under any loss trajectory.Furthermore, this optimal batch size in Eq 3.12 is determined by equally minimizing the training time and required computation, as shown in Eq 3.9.In practice, if we would like to prioritize one over the other, we can follow the same process to derive the optimal batch size.By this means, we are able to obtain the optimal batch size based on our customized need in a systematic way.\n\nDetermining N and S In practice, we often opt for the largest affordable model size and train the model until convergence.Nevertheless, this simplistic approach can deviate significantly from the optimal configuration and result in substantial resource wastage.Scaling laws provide a principled approach to choosing the optimal model size N and number of training steps S given a fixed computational budget C8 .Given that Eq 3.13 already provides the precise relation between the loss L, batch size B, model size N and training steps S, we could find the model size that minimizes L under the critical batch size (B = B crit ).",
            "score": 0.5185557295050647,
            "section_title": "Discussions",
            "char_start_offset": 19934,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 362
                },
                {
                    "start": 362,
                    "end": 470
                },
                {
                    "start": 472,
                    "end": 821
                },
                {
                    "start": 821,
                    "end": 1063
                },
                {
                    "start": 1063,
                    "end": 1206
                },
                {
                    "start": 1206,
                    "end": 1322
                },
                {
                    "start": 1322,
                    "end": 1469
                },
                {
                    "start": 1469,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1706
                },
                {
                    "start": 1708,
                    "end": 1830
                },
                {
                    "start": 1830,
                    "end": 1969
                },
                {
                    "start": 1969,
                    "end": 2119
                },
                {
                    "start": 2119,
                    "end": 2335
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1866455078125
        },
        {
            "corpus_id": "269009975",
            "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
            "text": "Scaling laws serve as a fundamental guiding principle in the development of LLMs.Although these scaling laws exhibit variability in specific coefficients due to diverse configurations across model series, the compute optimal data-to-model ratio remains a meaningful metric across different scaling law functions, which \"marginalizes\" out the specific value of loss.Regarding this ratio, Kaplan et al. (2020)  In this section, we introduce the utilization of the WSD scheduler as an effective approach to explore the scaling law with linear cost (O(mC)).Since the WSD scheduler has the advantage of arriving at the optimal loss of Cosine LRS after decaying from stable stage checkpoints of any step, we are now able to precisely measure the optimal scaling properties without re-training the models from scratch to different amounts of tokens, thus making the scaling law measurement much more efficient along the data axis.\n\nWe measure the scaling law along the data and model axes by training SLMs of 6 sizes ranging from 0.04B to 2B, each with 6 decayed models starting from the checkpoint of 10N to 60N data during the stable training stage.The final loss is evaluated on five heldout evaluation datasets.To potentially compare the loss when the model uses different tokenizers, we take the average of loss by a number of bytes instead of a number of tokens, following Achiam et al. (2023).The final loss of each pair of data size and model size is shown in the blue lines in Figure 17.\n\nThen we fit the losses with model size N and data size D following Hoffmann et al. ( 2022) using scipy curvefit function:\n\nThe fitted curve along the data axis for each dataset and each checkpoint are shown in orange lines in Figure 17.Then we have the optimal model size N opt , dataset size D opt , given a fixed amount of compute C = 6ND (Rae et al., 2021) as: 6N in Equation 2, and minimize L(N) given C. A similar way is adopted for D opt .",
            "score": 0.5182334356383498,
            "section_title": "Measuring the Scaling Law with WSD LRS",
            "char_start_offset": 11316,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 81
                },
                {
                    "start": 81,
                    "end": 365
                },
                {
                    "start": 365,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 923
                },
                {
                    "start": 925,
                    "end": 1144
                },
                {
                    "start": 1144,
                    "end": 1208
                },
                {
                    "start": 1208,
                    "end": 1393
                },
                {
                    "start": 1393,
                    "end": 1489
                },
                {
                    "start": 1491,
                    "end": 1612
                },
                {
                    "start": 1614,
                    "end": 1727
                },
                {
                    "start": 1727,
                    "end": 1936
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "This implies that, without an oracle verifier, simple strategies like sampling cannot achieve perfect accuracy even with an infinite number of samples, leading to diminishing returns. Therefore, this highlights the necessity for more sophisticated inference algorithms. \n\nWe have also found that the commonly-used MCTS method does not perform well with weighted voting, as it often yields many unfinished solutions, hence having less effective votes. To address this issue, we propose a novel tree search algorithm, REward BAlanced SEarch (REBASE), which pairs well with weighted voting and achieves a Pareto-optimal trade-off between accuracy and inference compute. The key idea of REBASE is to use a node-quality reward to control node expansion, which eliminates the need for explicit rollouts while ensuring enough candidate solutions for voting. \n\nIn our experiments, REBASE consistently outperforms sampling and MCTS methods across all settings, models, and tasks. Importantly, we find that REBASE with a smaller language model typically achieves a Pareto-optimal trade-off. For instance, we show that the Llemma-7B model can achieve competitive accuracy to a Llemma-34B model while using 2\u00d7 less FLOPs when evaluating on MATH500 (Fig. 4) or GSM8K (Fig. 5). Moreover, Llemma-7B with REBASE outperforms Llemma-34B with standard majority voting across all compute budgets. Our results show the value of using smaller models with advanced inference-time algorithms, and the benefits of new algorithms for achieving better returns on inference-time compute. \n\nOur contributions are summarized as follows: \n\n\u2022 We explore new inference scaling laws and compute-optimal inference by evaluating the performance of various model sizes under a fixed inference strategy. We show that smaller models can outperform larger ones under the same compute budget by increasing the number of samples. \n\n\u2022 We provide new theoretical analysis of the scaling behavior of voting methods, presenting convergence bounds and rates. Our analysis shows performance limits and diminishing returns from sampling, pointing to the need for more sophisticated inference algorithms. \n\n\u2022 We formulate a new compute-optimal inference problem and propose a novel tree search algorithm, REBASE, which is compute-optimal compared to widely-used sampling and MCTS methods.",
            "score": 0.5178595196305086,
            "section_title": "INTRODUCTION",
            "char_start_offset": 4512,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 269
                },
                {
                    "start": 272,
                    "end": 450
                },
                {
                    "start": 451,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 850
                },
                {
                    "start": 853,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1559
                },
                {
                    "start": 1562,
                    "end": 1606
                },
                {
                    "start": 1609,
                    "end": 1765
                },
                {
                    "start": 1766,
                    "end": 1887
                },
                {
                    "start": 1890,
                    "end": 2011
                },
                {
                    "start": 2012,
                    "end": 2154
                },
                {
                    "start": 2157,
                    "end": 2338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056976318359375
        },
        {
            "corpus_id": "273228196",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "text": "After fitting the training loss (L) as a function of the number of tokens (D), the model scale (N ), and the number of experts (E), we proceed to derive the optimal computing resource allocation strategy for model scale and the number of training tokens given a fixed compute budget. \n\nThe objective can be defined as follows: given a fixed number of compute budget, how to estimate the optimal resource allocation strategy for model scale and the number of training tokens to minimize the training loss. \n\nWe take the differentiation of Equation 5 with respect to C = DN , we derive the optimal values of D and N for a given compute budget: \n\nwhere \n\nwhere k N = ( \u03b1A \u03b2BE \u03b3 ) \n\n1 \u03b1+\u03b2 and \u03b1 N = \u03b2 \u03b1+\u03b2 . Equation 7 and 8 indicate how the model scale (compute budget per token N ) and the optimal number of training tokens D scale with the overall compute budget C. The most crucial part of this process is to find the scaling exponent of the model scale and tokens number with reference to the compute budget. Then we use the empirical fitting results obtained previously to calculate these two exponents. From the data shown in Table 1, we observe that the exponent for the optimal model scale (N ) in Mixture of Experts (MoE) models is larger, while the exponent for the optimal number of training tokens (D) is smaller, compared to their Dense Model counterparts. This suggests that MoE Models benefit more from increasing model size relative to the number of training tokens. \n\nThis finding helps explain why MoE Models can outperform larger Dense Models. The higher utilization of training data by MoE Models allows them to leverage their diverse sub-networks more effectively, capturing a broader range of features and patterns. Moreover, this suggests that it is more advantageous to allocate a larger computing budget to MoE Models compared to Dense Models.",
            "score": 0.516736964833523,
            "section_title": "Estimating Optimal Resource Allocation Strategy Scaling",
            "char_start_offset": 7195,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 286,
                    "end": 504
                },
                {
                    "start": 507,
                    "end": 641
                },
                {
                    "start": 644,
                    "end": 649
                },
                {
                    "start": 652,
                    "end": 676
                },
                {
                    "start": 679,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1104
                },
                {
                    "start": 1105,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1478
                },
                {
                    "start": 1481,
                    "end": 1558
                },
                {
                    "start": 1559,
                    "end": 1733
                },
                {
                    "start": 1734,
                    "end": 1864
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1195068359375
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "Figure 1: Scaling with compute. The dots show the training compute budget and test loss in our experiments, the lines indicate the compute-optimal performance according to the scaling laws we find. The test losses of both non-equivariant ( \n\n) and equivariant ( \n\n) transformers scale as a power law with compute, and the equivariant model outperforms the nonequivariant model by a similar factor at all tested compute budgets. Our experiments provide evidence for three conclusions. As expected, equivariance improves data efficiency. However, data augmentation largely closes this gap. Second, equivariant transformers are also more compute-efficient, and this advantage persists across all compute budgets studied. Both model classes exhibit power-law scaling behaviour. Finally, the optimal allocation of a training compute budget to model size and training steps differs between equivariant and non-equivariant models. Overall, our findings hint that strong inductive biases may not only yield benefits in the low-data regime, but can also be beneficial with large datasets and large compute budgets.",
            "score": 0.5155286847638184,
            "section_title": "Loss Baseline Equivariant",
            "char_start_offset": 237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 31
                },
                {
                    "start": 32,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 239
                },
                {
                    "start": 242,
                    "end": 261
                },
                {
                    "start": 264,
                    "end": 427
                },
                {
                    "start": 428,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 535
                },
                {
                    "start": 536,
                    "end": 587
                },
                {
                    "start": 588,
                    "end": 717
                },
                {
                    "start": 718,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1105
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1837158203125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "While neural scaling laws have provided valuable insights into model performance, their current formulations often fail to account for recent advancements in architecture, data efficiency, and inference strategies. The following directions highlight key areas where scaling laws should be adapted to improve their predictive power and practical utility. \n\nInference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications. \n\nCompute-optimal model selection: Scaling laws should not only predict performance improvements but also guide model selection given a fixed compute budget. Future work should explore multiobjective optimization frameworks that balance performance, energy efficiency, and cost to drive more sustainable AI development. \n\nEfficient data scaling and pruning: The optimization of model scaling necessitates a shift from volume-based to quality-focused data selection. Future frameworks should prioritize informative examples and integrate diversity metrics to enhance generalization, moving beyond simple dataset expansion.",
            "score": 0.5135769284183476,
            "section_title": "Future recommendations",
            "char_start_offset": 25490,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 353
                },
                {
                    "start": 356,
                    "end": 601
                },
                {
                    "start": 602,
                    "end": 749
                },
                {
                    "start": 752,
                    "end": 907
                },
                {
                    "start": 908,
                    "end": 1069
                },
                {
                    "start": 1072,
                    "end": 1215
                },
                {
                    "start": 1216,
                    "end": 1371
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.272216796875
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "In order to be feasible in practice, a compute-optimal scaling strategy conditioned on difficulty needs to first assess difficulty and then utilize the right scaling strategy to solve this problem. Therefore, we approximate the problem's difficulty via a model-predicted notion of difficulty, which performs the same binning procedure over the the averaged final answer score from a learned verifier (and not groundtruth answer correctness checks) on the same set of 2048 samples per problem. We refer to this setting as model-predicted difficulty and the setting which relies on the ground-truth correctness as oracle difficulty. \n\nWhile model-predicted difficulty removes the need for need knowing the ground truth label, estimating difficulty in this way still incurs additional computation cost during inference. That said, this one-time inference cost can be subsumed within the cost for actually running an inference-time strategy (e.g., when using a verifier, one could use the same inference computation for also running search). More generally, this is akin to exploration-exploitation tradeoff in reinforcement learning: in actual deployment conditions, we must balance the compute spent in assessing difficulty vs applying the most computeoptimal approach. This is a crucial avenue for future work (see Section 8) and our experiments do not account for this cost largely for simplicity, since our goal is to present some of the first results of what is in fact possible by effectively allocating test-time compute. \n\nSo as to avoid confounders with using the same test set for computing difficulty bins and for selecting the compute-optimal strategy, we use two-fold cross validation on each difficulty bin in the test set. We select the best-performing strategy according to performance on one fold and then measure performance using that strategy on the other fold and vice versa, averaging the results of the two test folds.",
            "score": 0.5134544872945048,
            "section_title": "Estimating Question Difficulty for Compute-Optimal Scaling",
            "char_start_offset": 16925,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 197
                },
                {
                    "start": 198,
                    "end": 492
                },
                {
                    "start": 493,
                    "end": 630
                },
                {
                    "start": 633,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1037
                },
                {
                    "start": 1038,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1525
                },
                {
                    "start": 1528,
                    "end": 1734
                },
                {
                    "start": 1735,
                    "end": 1938
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.056549072265625
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "We extend the training of our largest model (with 901M parameters) to \u223c14B RefinedWeb tokens (the compute-optimal value for that model size according to our scaling law), and test whether our predicted batch size and learning rate are optimal at this scale. In particular, we compare the learning and batch size prescribed in Table 4 (0.0024 and 640, respectively) to 12 configurations. In 8 of them we very either the learning rate or the batch size, and in 4 of them we very both. We evaluate the models on held-out RefinedWeb data and calculate the validation loss and standard deviation due to sampling as described in Appendix D. The results are shown in Table 7. We find that the prescribed learning rate and batch size are indeed optimal, as they yield the lowest validation loss.",
            "score": 0.5131424850350981,
            "section_title": "G.5 Validating the hyperparameter scaling laws on a larger compute scale",
            "char_start_offset": 44998,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 386
                },
                {
                    "start": 387,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 668
                },
                {
                    "start": 669,
                    "end": 787
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.052520751953125
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "Scaling Laws. Hoffmann et al. [2022] model loss scaling using the functional form L(N, D) = AN \u2212\u03b1 + BD \u2212\u03b2 + E where A, B, \u03b1, \u03b2, E are positive fitted constants, finding that data and parameters should be scaled in roughly equal proportion as more compute becomes available. We will refer to the scaling of [Hoffmann et al., 2022] as \"Chinchilla-optimal\" or just \"Chinchilla\" and note this is often used colloquially as D/N \u2248 20 being pretraining compute-optimal. On the theoretical front, work on scaling laws [Bahri et al., 2024, Bordelon et al., 2024, Lin et al., 2024a] finds that noise to various parts of model or data affects loss in a predictable way. While previous works have explored the scaling behavior of post-training quantization in terms of total model bits [Dettmers and Zettlemoyer, 2023] and knowledge capacity [Allen-Zhu and Li, 2024], we focus instead on data scaling. We note that in general the exact fitted values of all coefficients and exponents can vary drastically based on small implementation differences: Besiroglu et al. [2024] find different constants when attempting to replicate [Hoffmann et al., 2022], Sardana and Frankle [2023] fit coefficients A, B of different orders of magnitude. For this reason, we emphasize our contribution is not the numerical values we fit, but the trends and functional forms we identify. \n\nOvertraining. In practice, accounting for inference costs means training smaller models for substantially longer than Chinchilla-optimal [Sardana andFrankle, 2023, Gadre et al., 2024]. For instance, Llama-3-8B is trained to D/N \u2248 2000 [Dubey et al., 2024] and the Gemma-2 series up to D/N > 1000 [Team et al., 2024]. We refer to such models as \"overtrained\" in this paper, with the token/parameter ratio D/N being a key quantity throughout.",
            "score": 0.5125112392257628,
            "section_title": "Scaling Laws and Parametric Fits",
            "char_start_offset": 7612,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 13
                },
                {
                    "start": 14,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 889
                },
                {
                    "start": 890,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1353
                },
                {
                    "start": 1356,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1540
                },
                {
                    "start": 1541,
                    "end": 1672
                },
                {
                    "start": 1673,
                    "end": 1796
                }
            ],
            "ref_mentions": [
                {
                    "start": 510,
                    "end": 529,
                    "matchedPaperCorpusId": "231918701"
                },
                {
                    "start": 774,
                    "end": 806,
                    "matchedPaperCorpusId": "254853733"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.230224609375
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "For each set of analyses, we the scaling laws found by (Kaplan et al., 2020) and (Hoffmann et al., 2022) for comparison. We also include markers indicating 3 existing models for comparison purposes: Llama 3 405B (Dubey et al., 2024), the Chinchilla model (Hoffmann et al., 2022), and an estimate of the 1.5B GPT-2 model (Radford et al., 2019), for which we know details of the dataset storage size and word count, but not an exact count of data BPE tokens, which we estimate at 100B. We additionally annotate, at the compute budget C for each of these 3 reference points, the maximum and minimum predicted (i.e. extrapolated) optimal model parameter count N opt and data budget D opt from the fitted power laws. We use a thicker, solid line for the method in each plot which achieves the lowest optimization loss, with the exception of the plots comparing power law form, those comparing loss functions and those comparing optimizers, for which this would be nonsensical. We find overall, throughout our analyses, that all of the decisions we explore have an impact on the final fit of the power law, supporting our conclusion that more thorough reporting of these decisions is critical for scaling law reproducibility. 2022)), in which we fit an optimal N for each compute budget C, C(N ), which can then be used to fit the predicted loss L(N, D). This approach usually necessitates the usage of mid-training checkpoints (discussed further in \u00a77.3), as it is infeasible to train a large enough number of models for each FLOP budget considered. However, we apply it here without using only final model checkpoints, and extend to mid-training checkpoints in \u00a77.3). We adapt the implementation from Porian et al. ( 2024), which contains more details about interpolation of data points and specific hyperparameters.",
            "score": 0.5119785556540204,
            "section_title": "Ours",
            "char_start_offset": 33173,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 120
                },
                {
                    "start": 121,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1348
                },
                {
                    "start": 1349,
                    "end": 1544
                },
                {
                    "start": 1545,
                    "end": 1663
                },
                {
                    "start": 1664,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 320,
                    "end": 342,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "To estimate the optimal batch size and learning rate for each model size, we adopt a two-stage interpolation approach. In the first stage, for each model size and batch size, we estimate the optimal learning rate by interpolating (in log-space) the loss as a function of learning rate using Akima [3] interpolation, where for every learning rate we assign the lowest loss obtained from the three values of \u03b2 2 . We minimize the interpolant and save its minimizing argument and minimum value. In the second stage, repeat this procedure over the sequence of batch size and interpolated loss pairs, finding an optimal batch size for each model size. To extract an estimate of the optimal learning rate, we simply interpolate the (batch size, minimizing learning rate) sequence and evaluate it at the optimal batch size. Token-to-parameter ratio  Figure 17: Top-left: The estimated excess loss caused by using the hyperparameters in Table 4 instead of the ideal hyperparameters for each model size N and token-to-parameter ratio \u03c1. Light dashed lines show the raw excess loss estimates. To these we apply a median filter of width 2 (in terms of token-top-parameter ratio) and plot the results in solid lines. Top-right: The estimated IsoFLOP curves with ideal tuning, obtained by subtracting the excess loss from the actual loss. \n\nBottom: Comparing the compute-optimal model sizes obtained from direct observations (with hyperparameters as in Table 4) with our estimate for compute-optimal model sizes given ideal tuning per model. \n\nIn this section, we use the training loss data from our hyperparameter sweep to approximate such ideal tuning and estimate its effect on the compute-optimal scaling law. We do so in three steps. \n\n1. Estimating suboptimality as a function of token-to-parameter ratio. We estimate the best hyperparameters for \u03c1 < 20 using the same interpolation logic as in Section 3.5 but for the training loss after D = \u03c1N tokens. (We do not consider values of \u03c1 below 2 since they are too close to the warmup period.) Thus, for every value of N and \u03c1, we obtain an estimate of the loss with optimal hyperparameters, denoted L \u22c6 .",
            "score": 0.5119393448431299,
            "section_title": "G.2 Estimating the optimal batch size and learning rate via interpolation",
            "char_start_offset": 40624,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 491
                },
                {
                    "start": 492,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 816
                },
                {
                    "start": 817,
                    "end": 1027
                },
                {
                    "start": 1028,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1204
                },
                {
                    "start": 1205,
                    "end": 1325
                },
                {
                    "start": 1328,
                    "end": 1528
                },
                {
                    "start": 1531,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1725
                },
                {
                    "start": 1728,
                    "end": 1798
                },
                {
                    "start": 1799,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2034
                },
                {
                    "start": 2035,
                    "end": 2146
                }
            ],
            "ref_mentions": [
                {
                    "start": 297,
                    "end": 300,
                    "matchedPaperCorpusId": "33862277"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0728759765625
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "Jointly minimizing L(N, D, P ) with C \u221d N DP . This is the setting of pretraining without constraints on N, D, P except for a fixed compute budget. Solving this joint minimization problem gives an implicit equation for P * (C). Denoting u(P ) = [1 \u2212 e \u2212P/\u03b3 ] \u22123\u03b1 , we find (see Appendix E) that this equation takes the form 3\u03b1 \u03b3 u(P ) 3\u03b1+1 3\u03b1 e \u2212P/\u03b3 = P \u22121 u(P ) (7) which reveals that in general the optimal pretraining precision is independent of compute budget. This suggests that compute-optimal precision should be held fixed to P * while N, D are scaled according to Equation 6. We find this P * to be around 7-8 bits when fitting our scaling law on runs with quantization done to integer type. This has two consequences: first, this means the de-facto practice of training models in 16-bit may be suboptimal. Second, the race to low-precision training may have to stop before going below 4-bits, since this would force model sizes to become disproportionately (more than 4x) larger to maintain loss scaling (see Figure 3, left). We test our predictions in Figure 6 at a larger scale. We train compute-matched models at various parameter count and precision ranging from FP4 to FP32 and 220M to 1.6B parameters. We train in floating-point type since that is standard in pretraining [Groeneveld et al., 2024, Deitke et al., 2024], though our scaling laws are fitted on integer type. We plot our predicted trend in Figure 6 (left) and the empirical values in the middle. We find that scaling fits on integer type are a strong fit until 4-bit precision, at which points the difference between the two types becomes more apparent. The matching of qualitative trends throughout, with the optimum being close to the predicted optimum of P * near 7-8 bits suggests that similar scaling laws may exist across types. We initiate a similar analysis for floating-point type in Appendix N.",
            "score": 0.5118105656800249,
            "section_title": "Compute-Optimal Pretraining Precision is in General Independent of Compute",
            "char_start_offset": 19296,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 46
                },
                {
                    "start": 47,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 584
                },
                {
                    "start": 585,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 1035
                },
                {
                    "start": 1036,
                    "end": 1090
                },
                {
                    "start": 1091,
                    "end": 1217
                },
                {
                    "start": 1218,
                    "end": 1387
                },
                {
                    "start": 1388,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1632
                },
                {
                    "start": 1633,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1883
                }
            ],
            "ref_mentions": [
                {
                    "start": 1312,
                    "end": 1334,
                    "matchedPaperCorpusId": "272880654"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184814453125
        },
        {
            "corpus_id": "270702885",
            "title": "The Responsible Foundation Model Development Cheatsheet: A Review of Tools & Resources",
            "text": "Several decisions made prior to model training can have significant impacts on the upstream and downstream environmental impact of a given model. Empirical scaling laws can be used to find the best allocation of resources. Kaplan et al. (2020);Hoffmann et al. (2022) estimate the optimal model size and training duration, given a training compute budget. And Aghajanyan et al. (2023) investigates the equivalent efficient compute allocation for multimodal settings. When working with text training data that is constrained, recent work explores how to allocate compute efficiently (Muennighoff et al., 2023b). For models frequently used downstream, it is important to consider the inference footprint and inference cost during model creation (Gadre et al., 2024b), to minimize the environmental impact of inference. For further resources and discussion, see 6.3.",
            "score": 0.5099323194033013,
            "section_title": "Effective use of resources",
            "char_start_offset": 58758,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 222
                },
                {
                    "start": 223,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 609
                },
                {
                    "start": 610,
                    "end": 815
                },
                {
                    "start": 816,
                    "end": 862
                }
            ],
            "ref_mentions": [
                {
                    "start": 359,
                    "end": 383,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 581,
                    "end": 608,
                    "matchedPaperCorpusId": "10475843"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09893798828125
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "For many, inference is the primary cost of training and serving models. Here, we present a preliminary analysis of an inference-time cost model. The key tension is that inference cost scales as N P , so that inference costs at a fixed pretraining loss can be reduce by either reducing model size (and overtraining more) or quantizing post-training We will assume here that P = P post refers to the precision weights will be quantized to. In practice, inference costs may depend on the precision of the KV cache and activations to some extent as well, but we assume this for tractability of the following mathematical model, and to get a sense of how overtraining and post-train quantization concerns play out at inference-time. We can phrase this minimization problem in the following way. The system of first-order conditions that results from this constrained optimization problem is not in general tractable analytically, so we solve the above constrained optimization problem for P * (C), N * (C), D * (C) numerically via a simple grid search. We find that N * , D * grow as a power law in C while P * \u221d log C. The clumping in points is an artifact of the numerics of the grid search; the fitted lines represent the loglinear (left) and loglog (middle, right) trends overall. \n\nIt might be surprising that D * is not taken to infinity since it does not appear in the cost function. The reason for this is because if it was, post-train degradation (the third term) would become large. It might also be surprising that D * changes with compute at all. The reason for this is because, once again, of the third term: as we allow more inference-time compute we use more N , and at a larger N we can now tolerate a larger data budget for a given post-train quantization degradation, so being compute-optimal means taking advantage of this and training that larger parameter count on more data. \n\nThe intuition for why P * \u223c log C might be as follows.",
            "score": 0.5099226638990384,
            "section_title": "E.3 Inference-time Cost Model",
            "char_start_offset": 42587,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 71
                },
                {
                    "start": 72,
                    "end": 144
                },
                {
                    "start": 145,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1279
                },
                {
                    "start": 1282,
                    "end": 1385
                },
                {
                    "start": 1386,
                    "end": 1487
                },
                {
                    "start": 1488,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 1948
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09173583984375
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Figure 4 shows the minimum loss achievable for each compute budget C in the experiments shown in Figure 1. We estimate the minimum loss using the same interpolation procedure we use to extract the optimal parameter number N \u22c6 and token count D \u22c6 . The figure shows that, at low compute scales, shortening the warmup duration and tuning hyperparameters leads to substantial loss improvements (each by up to 0.5 nat per token). However, at larger scales these interventions do not significantly improve the loss. In contrast, learning rate decay becomes increasingly beneficial as compute grows, and appears to also improve the rate of decrease in the loss. Perhaps coincidentally, the effects of overestimating the optimal loss (due to long warmup and large batch size) seem to closely offset the effect of underestimating computational cost (by discounting the contribution from the model's head): the first and last curves in Figure 4 closely overlap. \n\nSimilarly to Hoffmann et al. [25] we observe a curvature in the optimal loss, while Kaplan et al. [30] report a near-perfect power law behavior. This difference is due to a combination of the difference in FLOP counts discussed in Section 3. Each subplot uses a different number of lower-compute loss measurement to fit the loss trend. \n\nextend to higher compute budgets where the loss is closer to its irreducible level. Indeed, for the tuned optimizer experiment (Section 3.5) we find that a saturating power law fits the optimal loss and extrapolates well, while extrapolating poorly for other experiments (see Figure 19 in the appendix). This suggests that a predictable trend in L(N \u22c6 (C), D \u22c6 (C)) is an indicator of locallyoptimal hyperparameters. The exponent of our saturating power fit is approximately \u22120.1, twice as large as the exponent found in Kaplan et al. [30]. \n\nFinally, we validate our compute-optimal loss scaling law by training and evaluating a model using a larger compute budget.",
            "score": 0.5096229121482823,
            "section_title": "Additional Analysis 4.1 Trends in compute-optimal loss",
            "char_start_offset": 18302,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 510
                },
                {
                    "start": 511,
                    "end": 655
                },
                {
                    "start": 656,
                    "end": 952
                },
                {
                    "start": 955,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1290
                },
                {
                    "start": 1293,
                    "end": 1376
                },
                {
                    "start": 1377,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1709
                },
                {
                    "start": 1710,
                    "end": 1833
                },
                {
                    "start": 1836,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 984,
                    "end": 988,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0987548828125
        },
        {
            "corpus_id": "258740735",
            "title": "PaLM 2 Technical Report",
            "text": "Here we briefly describe the methodology used to construct scaling law curves used in Figure 4, Figure 5 and Table 1.\n\nFollowing (Hoffmann et al., 2022), we construct isoflop curves for four compute scales (1 \u00d7 10 19 , 1 \u00d7 10 20 , 1 \u00d7 10 21 , and 1 \u00d7 10 22 FLOPs) by training models across several parameter and token counts. For each compute scale, we fit a quadratic curve to the final validation loss values and interpolate the minimum (as shown in Figure 4). Given these four estimates for the minimum parameter count, we fit a power-law curve shown in Figure 5 to predict the optimal parameter and token counts at larger scales. Because we share parameters between the embedding and output layers in these experiments, we use the total parameter count to estimate total FLOPs (as 6 \u00d7 N \u00d7 D in Table 1), but we find the non-embedding parameter count (excluding the output layer) to fit better in e.g. Figure 4 when estimating the optimal parameter counts. Note that we are also able to predict error bars on extrapolated predictions with a \"leave-one-out\" estimator by estimating the scaling coefficients with only 3 of the 4 points.\n\nWe also evaluate models of different sizes on the downstream tasks described in Section 4.2 at a single compute scale (1 \u00d7 10 22 FLOPs). We show the results in Table 15.",
            "score": 0.5095946856639524,
            "section_title": "A.1 Scaling laws",
            "char_start_offset": 57195,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 129,
                    "end": 152,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059326171875
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "(1.8) which closely matches the empirically optimal results N \u221d C 0.73 min , B \u221d C 0.24 min , and S \u221d C 0.03 min . As the computational budget C increases, it should be spent primarily on larger models, without dramatic increases in training time or dataset size (see Figure 3). This also implies that as models grow larger, they become increasingly sample efficient. In practice, researchers typically train smaller models for longer than would be maximally compute-efficient because of hardware constraints. Optimal performance depends on total compute as a power law (see Equation (1.3)). \n\nWe provide some basic theoretical motivation for Equation (1.5), an analysis of learning curve fits and their implications for training time, and a breakdown of our results per token. We also make some brief comparisons to LSTMs and recurrent Transformers [DGV + 18].",
            "score": 0.509181116717899,
            "section_title": "Summary of Scaling Laws",
            "char_start_offset": 7393,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 278
                },
                {
                    "start": 279,
                    "end": 367
                },
                {
                    "start": 368,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 591
                },
                {
                    "start": 594,
                    "end": 777
                },
                {
                    "start": 778,
                    "end": 861
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.194580078125
        },
        {
            "corpus_id": "271270413",
            "title": "Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies",
            "text": "where training compute budget is the main constraint and we seek to allocate it optimally to parameters and training data. This is the typical setting in scaling law studies [30,26,52]. However, in the real world, we often deal with scarce data (\"data-constrained [44]\") forcing us to train sub-optimally or would like to make use of excessive data to train a smaller model that is cheaper to use [84]. To verify that our Approach 3 can handle these practical scenarios, we compare the model with V = 32K and the model with the vocabulary size V opt predicted by Approach 3. As shown in Table 3, our prediction enables a better model by only adjusting the vocabulary size in different FLOPs budgets. \n\nIn Figure 7, we further study the trend about how does the optimal vocabulary size shift with different number of training data. We only vary the amount of data but keep the non-vocabulary parameters fixed. The choices of vocabulary size are 8K, 10K, 16K, 24K, 32K and 48K. Taking N nv = 302M as an example, when available data is the bottleneck, the optimal vocabulary size decreases empirically, i.e. 16K \u2192 10K. This is a mechanism to prevent over-fitting. Conversely, when training on excessive amounts of data, e.g., Llama3-8B uses much more training tokens than what would be compute-optimal for its budget, the optimal vocabulary size increases, i.e. 16K \u2192 24K. Note that here we focus solely on training compute-optimal. It is also important to note that expanding the vocabulary size also increases the computational demands during inference. Therefore, we recommend using the optimal vocabulary size corresponding to a given N nv , assuming optimal allocation of training data, even in scenarios where overtraining may occur.",
            "score": 0.508640712362879,
            "section_title": "Experiments with scarce and excessive training data Our prior experiments focus on the setting",
            "char_start_offset": 24397,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 185
                },
                {
                    "start": 186,
                    "end": 402
                },
                {
                    "start": 403,
                    "end": 699
                },
                {
                    "start": 702,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 908
                },
                {
                    "start": 909,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1429
                },
                {
                    "start": 1430,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1736
                }
            ],
            "ref_mentions": [
                {
                    "start": 264,
                    "end": 268,
                    "matchedPaperCorpusId": "258888192"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.090087890625
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Given the option to allocate a fixed inference budget toward either scaling solutions via Self-Consistency or verifying them using GenRMs, it remains unclear which approach is compute-optimal for LLM reasoning. To address this question, we conduct a compute-matched analysis of their respective scaling behaviors. We consider an autoregressive LLM with  parameters that will perform 2 FLOPs per output token during inference [23]. Hence, the number of inference FLOPs for generating  tokens is 2  . \n\nLet the number of tokens required for generating a solution and verification be   and   , respectively. Following [43], we use the same model for problem-solving and generative verification. For instance, one might use Llama-8B to generate solutions and a fine-tuned version of the same model as GenRM-FT (or the same model without fine-tuning as GenRM-Base). Hence, the number of model parameters for the solution generator and verifier is identical, say  . Thus, the total inference compute (FLOPs) required to solve a reasoning problem with  solutions and  verifications is 2 (   +    ) . Further, we consider   =   where  is the ratio of the number of tokens per verification and solution. In our analysis, we use the formula (,  ) = (1 +  ) to measure inference compute for simplicity, as it is proportional to the total inference FLOPs for a given LLM. For Self-Consistency (SC), we set the number of verifications to  = 0. \n\nWe evaluate SC by sampling  solutions and performing majority voting over them, for all  \u2208  = {2 0 , 2 1 , ..., 2  }, where 2  is the maximum number of solutions. Similarly, we evaluate GenRM by varying the number of solutions  \u2208  and verifications  \u2208  = {2 0 , 2 1 , ..., 2  }, where 2  is the maximum number of verifications per solution. For every combination ,  \u2208 { \u00d7 }, we sample the corresponding number of solutions and verifications, and pick the final answer via Best-of-N. We compare the final answers against the ground-truth to compute success rates (SR), and plot them against the total compute, (,  ). Thus, we compare the performance of GenRM and SC at the same compute budget.",
            "score": 0.5080009136676038,
            "section_title": "Compute-Matched Analysis of Test-time Scaling Strategies",
            "char_start_offset": 10744,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 210
                },
                {
                    "start": 211,
                    "end": 313
                },
                {
                    "start": 314,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 498
                },
                {
                    "start": 501,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1092
                },
                {
                    "start": 1093,
                    "end": 1194
                },
                {
                    "start": 1195,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1430
                },
                {
                    "start": 1433,
                    "end": 1595
                },
                {
                    "start": 1596,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 1915
                },
                {
                    "start": 1916,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2125
                }
            ],
            "ref_mentions": [
                {
                    "start": 615,
                    "end": 619,
                    "matchedPaperCorpusId": "271963324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.011962890625
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "By contrast, the Llama 2 family of models were trained on 2 trillion tokens and the Llama 3 family of models were trained on 15 trillion tokens, which is far more data than the Chinchilla scaling laws would deem \"optimal\" (Touvron et al., 2023a;b;AI@Meta, 2024). Since inference costs are lower for smaller models, the extra training compute required to train a Llama-style model over a Chinchilla-style model of equivalent quality pays off after enough inference requests. \n\nPrior work has discussed the training-inference compute trade-off (Touvron et al., 2023a;b;Tow et al., 2023;De Vries, 2023;Villalobos & Atkinson, 2023). Touvron et al. (2023a) cites the lower inference cost of smaller models as inspiration for the LLaMA series. De Vries (2023) calculates the compute overhead of training longer than Chinchilla, but does not discuss quantify compute savings from inference. Villalobos & Atkinson (2023) discuss this trade-off in more detail, but show the shift in scaling laws for only a single particular number of inferences. \n\nOther related work includes Muennighoff et al. (2023), which adapts the Chinchilla scaling laws for the dataconstrained regime, where we have to repeat training tokens. The extra compute required to train the 7B model beyond its Chinchilla-optimal point to match the 13B's quality is made up for during inference (b), (c). Our method quantifies this training-inference trade-off, producing models that are optimal over their total lifetime. \n\nOur problem setting is the opposite: We assume we are data rich but compute constrained, and seek to minimize computation costs assuming we have enough data to train high-quality models. \n\nIn this paper, we modify the Chinchilla scaling laws to account for inference costs by calculating the optimal parameter and training token counts-both in terms of compute (Sec. 2) and dollar costs (Sec. 6)-to train and deploy a model of any given quality and inference demand.",
            "score": 0.5074916967791859,
            "section_title": "Introduction",
            "char_start_offset": 1735,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 473
                },
                {
                    "start": 476,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 737
                },
                {
                    "start": 738,
                    "end": 883
                },
                {
                    "start": 884,
                    "end": 1037
                },
                {
                    "start": 1040,
                    "end": 1208
                },
                {
                    "start": 1209,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1480
                },
                {
                    "start": 1483,
                    "end": 1669
                },
                {
                    "start": 1672,
                    "end": 1849
                },
                {
                    "start": 1850,
                    "end": 1875
                },
                {
                    "start": 1876,
                    "end": 1949
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.110107421875
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "Using certain assumptions-either standard in prior work or validated through empirical observations, we provide an analytical derivation demonstrating that the average parameter count in sparse pre-training plays the equivalent role of the fixed parameter count in dense training. This motivates unifying both pre-training techniques under the same scaling law as shown in Equation (2). \n\nAssumptions. Our analysis rests on two key assumptions: one empirically justified and one extensively used in prior work: \n\n1. Log loss decays linearly with log compute for fixed parameter count pre-training; 2. Total compute for processing a fixed number of tokens is proportional to the number of active parameters in the model. Kaplan et al. (2020) presented empirical evidence for Assumption (1) when fixed parameter count pre-training falls within the \"compute optimal\" regime, meaning that the model is appropriately sized to fully utilize the available compute. In our experiments, we extensively tune our sparse pretraining configurations to bring us close to this regime. In this setting, it is known that the loss L evolves as a function of the training compute C as \n\nwhere L(C) represents the loss at compute C, and \u03b1 > 0 is a constant that governs the rate of loss decay as compute increases. We apply this relationship to model loss decay within each pruning iteration, where parameter count stays fixed. Assumption (2) has been heavily used in previous scaling law work (Kaplan et al., 2020;Sardana et al., 2024;Frantar et al., 2024), which model compute as proportional to the number of parameters, batch size, and number of iterations. Since we use a fixed number of tokens per step, the compute per step is directly proportional to the number of active parameters at that step. \n\nLoss modeling. To derive the average parameter scaling law, we start with a Taylor series expansion of the loss, as modeled by Equation (3) (Assumption 1), at pruning iteration k, where C 0:k denotes the cumulative training compute up to iteration k. This yields an approximation for the change in loss due to an incremental increase in compute \u2206C k : \n\nApplying the above approximation across all pruning iterations, and relying on Assumption 2, we express the total changes in loss over T training steps as proportional to",
            "score": 0.5064793837208191,
            "section_title": "THEORETICAL ANALYSIS",
            "char_start_offset": 12889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 401
                },
                {
                    "start": 402,
                    "end": 510
                },
                {
                    "start": 513,
                    "end": 719
                },
                {
                    "start": 720,
                    "end": 957
                },
                {
                    "start": 958,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1407
                },
                {
                    "start": 1408,
                    "end": 1641
                },
                {
                    "start": 1642,
                    "end": 1784
                },
                {
                    "start": 1787,
                    "end": 1801
                },
                {
                    "start": 1802,
                    "end": 2037
                },
                {
                    "start": 2038,
                    "end": 2138
                },
                {
                    "start": 2141,
                    "end": 2311
                }
            ],
            "ref_mentions": [
                {
                    "start": 1495,
                    "end": 1516,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1516,
                    "end": 1537,
                    "matchedPaperCorpusId": "260611140"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.305908203125
        },
        {
            "corpus_id": "277467423",
            "title": "Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute",
            "text": "Scaling has been a major driving force of recent rapid advancements in large language models (LLMs). While scaling training-time compute [1] appears to be hitting a plateau, scaling inference-time compute stands out as a promising alternative [2]. An emerging direction is to scale inference-time compute based on the generation-verification paradigm. By querying an LLM with the same question for multiple times, a number of samples (or candidate answers) are generated, and then these samples are verified to deliver a final answer. Studies across various LLMs and benchmarks consistently demonstrate that simply scaling the number of generated samples significantly improves the coverage of correct answers [3]. Thus, it is perhaps unsurprising that recent attempts have pushed the number of samples to the scale of hundreds or even thousands [3,4,5], in pursuit of improved answer correctness. \n\nHowever, do we truly need so many samples? Scaling repeated sampling is undeniably computationally expensive, with the consumption of floating point operations (FLOPs) increasing linearly with the number of samplings [6]. Additionally, in terms of user experience, repeated sampling often leads to significant delay in providing final answers [7], and no one enjoys waiting too long for a response from AI. Therefore, improving sample efficiency is of paramount importance, and there is a pressing need for methods that can deliver correct final answers while minimizing the number of samples required. Recent approaches have primarily focused on the verification side-a great number of outcome or process reward models [8,9,10] and automatic verifiers [11,12] have been proposed, whereas LLM-as-a-judge [13,14] has also been extensively explored. \n\nFigure 1: Performance comparison of ModelSwitch and self-consistency [17] on Math [18] and MathBench [19] dataset. ModelSwitch switches between Gemini 1.5 Flash and GPT-4o mini on MATH, and between Gemma-2-9B-It and Llama-3.1-8B-Instruct on MathBench. The curves illustrate the performance of individual LLMs under self-consistency.",
            "score": 0.5057626162461752,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 100
                },
                {
                    "start": 101,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 714
                },
                {
                    "start": 715,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 942
                },
                {
                    "start": 943,
                    "end": 1121
                },
                {
                    "start": 1122,
                    "end": 1306
                },
                {
                    "start": 1307,
                    "end": 1502
                },
                {
                    "start": 1503,
                    "end": 1747
                },
                {
                    "start": 1750,
                    "end": 1864
                },
                {
                    "start": 1865,
                    "end": 2001
                },
                {
                    "start": 2002,
                    "end": 2082
                }
            ],
            "ref_mentions": [
                {
                    "start": 1653,
                    "end": 1657,
                    "matchedPaperCorpusId": "246527904"
                },
                {
                    "start": 1657,
                    "end": 1660,
                    "matchedPaperCorpusId": "256697342"
                },
                {
                    "start": 1704,
                    "end": 1708,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1819,
                    "end": 1823,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1832,
                    "end": 1836,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 1851,
                    "end": 1855,
                    "matchedPaperCorpusId": "269921620"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05792236328125
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges.",
            "score": 0.5054794335136733,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07476806640625
        },
        {
            "corpus_id": "266818336",
            "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
            "text": "Research on scaling laws (Hestness et al., 2017) predates the emergence of large language models. Scaling laws (Henighan et al., 2020;Hoffmann et al., 2022;Kaplan et al., 2020) suggest that model performance can be predictably improved with increases in compute budget , model scale , and data scale . When model scale  is represented by model parameters and data scale  by the number of tokens,  can be approximated as  = 6 . Therefore, how to optimize the allocation between model and data scales when increasing the compute budget is also a crucial research objective in scaling laws. \n\nThe development of LLMs (Dai et al., 2019;Radford et al., 2019), with larger models achieving unexpected and significant performance improvements, has brought scaling laws research to a new peak. Results in scaling laws demonstrate that expanding the compute budget continues to yield significant benefits, which further encourages the increase in model scales (Brown et al., 2020;Smith et al., 2022). \n\nHowever, as shown in Table 4, early works (Hoffmann et al., 2022;Kaplan et al., 2020) on the optimal model/data scaling-up allocation strategy have shown varying conclusions, raising doubts about the general applicability of scaling laws. Moreover, these studies often lacked a complete description of hyperparameter settings, leaving it uncertain whether models under different compute budgets reached optimal performance. Therefore, we revisit scaling laws in this section to address these uncertainties and ensure we are on the right path to efficiently scaleup compute, which reflects the long-term perspective and is key to developing continuously improving models. \n\nTo ensure that models under different compute budgets can achieve optimal performance, we first studied the scaling laws of hyperparameters. Empirically, it has been observed that the optimal values of most parameters during training do not change when varying compute budgets. Therefore, these parameters are consistent with those outlined in Section 2.3 and remain unchanged across different compute budgets. However, the hyperparameters that have the most significant impact on performance, namely batch size and learning rate, were re-examined.",
            "score": 0.5050994392908505,
            "section_title": "Scaling Laws",
            "char_start_offset": 13043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 301
                },
                {
                    "start": 302,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 587
                },
                {
                    "start": 590,
                    "end": 785
                },
                {
                    "start": 786,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1232
                },
                {
                    "start": 1233,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1664
                },
                {
                    "start": 1667,
                    "end": 1807
                },
                {
                    "start": 1808,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2077
                },
                {
                    "start": 2078,
                    "end": 2215
                }
            ],
            "ref_mentions": [
                {
                    "start": 632,
                    "end": 653,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2161865234375
        },
        {
            "corpus_id": "272593336",
            "title": "Scaling Law Hypothesis for Multimodal Model",
            "text": "Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance. The initial scaling laws proposed by OpenAI [1] suggested that larger models are more sample-efficient, leading to the creation of massive models like GPT-3. However, subsequent research from DeepMind, notably the Chinchilla study [2], revealed that many large models were undertrained. Their findings indicated that smaller models trained on more data could outperform larger models when the compute budget is held constant. Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8]. \n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]. This approach introduces bits per character (BPC) as a performance metric that reflects the model's compression efficiency [9]. BPC has been shown to correlate linearly with model performance across various modalities (Figure 1). This perspective reveals a linear relationship between BPC and the logarithm of compute used, which can be formalized as: \n\nwhere N is the number of training tokens, and P is the number of model parameters (Figure 2). \n\nThis unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant. In multimodal systems, diverse types of data, such as text, audio, images and video, are processed through specialized tokenization techniques, each with varying levels of compression efficiency. \n\nText is tokenized using methods like Byte Pair Encoding (BPE) [10], which offers relatively stable compression efficiency.",
            "score": 0.5045148089887523,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 164
                },
                {
                    "start": 165,
                    "end": 307
                },
                {
                    "start": 308,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 809
                },
                {
                    "start": 810,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1142
                },
                {
                    "start": 1145,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1455
                },
                {
                    "start": 1456,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1679
                },
                {
                    "start": 1682,
                    "end": 1775
                },
                {
                    "start": 1778,
                    "end": 1983
                },
                {
                    "start": 1984,
                    "end": 2179
                },
                {
                    "start": 2182,
                    "end": 2304
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234619140625
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "The scaling-law fit depends on two hyperparameters: whether we include the offset E as a fit parameter and the value of \u03b4. We determine both through leaveone-out cross-validation, performing scaling-law fits on all but one experiment and evaluating the error |log L(N i , D i ) \u2212 log L i | on the left-out experiment. In this way, we choose fixing E = 0 and \u03b4 = 0.001, though the qualitative fit results are not sensitive to these choices. \n\nCompute-optimal performance From a scaling law as in Eq. ( 2) and a FLOP function as in Eq. ( 1), we can derive the compute-optimal model size N * (C) and the compute-optimal training duration D * (C) as a function of the FLOP budget C as \n\nwhere G = ( \u03b1A \u03b2B ) 1/(\u03b1+\u03b2) , a = \u03b2/(\u03b1 + \u03b2), and b = \u03b1/(\u03b1 + \u03b2) (Hoffmann et al., 2022). The optimal loss achievable for a given FLOP budget is then \n\nwith F = AG \u2212\u03b1 \u03be \u03b3 + BG \u03b2 \u03be \u03b3 and \u03b3 = \u03b1\u03b2 \u03b1+\u03b2 . \n\nUncertainties No realistic scaling study directly measures the optimal model performance as a function of some parameters. Reasons for sub-optimality include the choice of hyperparameters, stochasticity in initialization and training, choosing a scaling-law ansatz that does not include the true functional form, and finite sampling of the space of model capacities and training tokens. We estimate the effect of the latter with a nonparametric bootstrap, similar to Hoffmann et al. (2022). \n\nFrom 10 4 bootstraps, we construct 95 % confidence intervals on the scaling law coefficients as well as on any derived predictions, using the empirical (or basic) bootstrap method.",
            "score": 0.504484305699962,
            "section_title": "Scaling-law hyperparameters",
            "char_start_offset": 19054,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 439
                },
                {
                    "start": 442,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 770
                },
                {
                    "start": 771,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1004
                },
                {
                    "start": 1005,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1372
                },
                {
                    "start": 1375,
                    "end": 1555
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.050506591796875
        },
        {
            "corpus_id": "273323177",
            "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
            "text": "We train a series of 12 sampling LMs up to 3B parameters to predict the performance of target LLMs with 7B and 13B parameters. The configurations of LMs are shown in Tab. 1. We adopt the fixed data-model ratio scaling strategy (Kaplan et al., 2020;Hoffmann et al., 2022) that maintains a fixed ratio between training tokens and model size while varying compute (FLOPs). In this scaling strategy, for any given compute budget, there exists a pre-determined allocation between training tokens and model size. We first determine the number of training tokens required for the 7B LLM (approximately 180 times the model size), considering practical needs and inference-time costs. In real-world applications, prioritizing inference efficiency often involves training smaller LMs with a higher token-to-parameter ratio beyond the optimal factor of 20x (Hoffmann et al., 2022). Our preliminary experiments indicate that scaling laws remain applicable even in this over-training regime (within 2.8% error margins). We then proportionally scale down this number to determine the required training tokens for the sampling LMs.",
            "score": 0.5043188295540336,
            "section_title": "Sampling and Target LMs",
            "char_start_offset": 9493,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 369
                },
                {
                    "start": 370,
                    "end": 506
                },
                {
                    "start": 507,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 870
                },
                {
                    "start": 871,
                    "end": 1006
                },
                {
                    "start": 1007,
                    "end": 1116
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09222412109375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "The test loss of a Transformer trained to autoregressively model language can be predicted using a power-law when performance is limited by only either the number of non-embedding parameters N , the dataset size D, or the optimally allocated compute budget C min (see Figure 1): \n\n1. For models with a limited number of parameters, trained to convergence on sufficiently large datasets: \n\n2. For large models trained with a limited dataset with early stopping: \n\nWhen training with a limited amount of compute, a sufficiently large dataset, an optimally-sized model, and a sufficiently small batch size (making optimal3 use of compute): The critical batch size, which determines the speed/efficiency tradeoff for data parallelism ( [MKAT18]), also roughly obeys a power law in L: \n\nEquation (1.1) and (1.2) together suggest that as we increase the model size, we should increase the dataset size sublinearly according to D \u221d N \u03b1 N \u03b1 D \u223c N 0.74 . In fact, we find that there is a single equation combining (1.1) and (1.2) that governs the simultaneous dependence on N and D and governs the degree of overfitting: \n\nwith fits pictured on the left in figure 4. We conjecture that this functional form may also parameterize the trained log-likelihood for other generative modeling tasks. \n\nWhen training a given model for a finite number of parameter update steps S in the infinite data limit, after an initial transient period, the learning curves can be accurately fit by (see the right of figure 4) \n\nwhere S c \u2248 2.1 \u00d7 10 3 and \u03b1 S \u2248 0.76, and S min (S) is the minimum possible number of optimization steps (parameter updates) estimated using Equation (5.4). \n\nWhen training within a fixed compute budget C, but with no other constraints, Equation (1.6) leads to the prediction that the optimal model size N , optimal batch size B, optimal number of steps S, and dataset size D should grow as \n\n(1.8) which closely matches the empirically optimal results N \u221d C 0.73 min , B \u221d C 0.24 min , and S \u221d C 0.03 min .",
            "score": 0.5041271162329723,
            "section_title": "Summary of Scaling Laws",
            "char_start_offset": 5499,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 278
                },
                {
                    "start": 281,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 386
                },
                {
                    "start": 389,
                    "end": 460
                },
                {
                    "start": 463,
                    "end": 779
                },
                {
                    "start": 782,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1283
                },
                {
                    "start": 1286,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1657
                },
                {
                    "start": 1660,
                    "end": 1891
                },
                {
                    "start": 1894,
                    "end": 2008
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12127685546875
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "Concurrent with our work, Song et al. [51] finds that using the best available sample improves LLM performance on chat, math, and code tasks, sweeping up to a max of 128 samples. Additionally, Hassid et al. [24] find that when solving coding tasks, it can be more effective to draw more samples from a smaller model than draw fewer samples from a larger one. \n\nScaling Laws: Characterizing how scaling affects model performance can lead to more informed decisions on how to allocate resources. Scaling laws for LLM training find a power law relationship between loss and the amount of training compute and provide estimates for the optimal model and dataset size given a fixed compute budget [27,36,28]. Jones [33] finds scaling laws in the context of the board game Hex, observing that performance scales predictably with model size and the difficulty of the problem. Interestingly, they also show that performance scales with the amount of test-time compute spent while performing tree search. Recently, Shao et al. [49] observe scaling laws when augmenting LLMs with external retrieval datasets, finding that performance on retrieval tasks scales smoothly with the size of the retrieval corpus.",
            "score": 0.5041160297912393,
            "section_title": "Related Work",
            "char_start_offset": 32886,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 703
                },
                {
                    "start": 704,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 995
                },
                {
                    "start": 996,
                    "end": 1197
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.3115234375
        },
        {
            "corpus_id": "273695243",
            "title": "Scaling LLM Inference with Optimized Sample Compute Allocation",
            "text": "Large language models (LLMs) solve more problems with more inference compute. Different ways of scaling up LLM inference include sampling (Chen et al., 2021), self-consistency (Wang et al., 2023c), tree search (Yao et al., 2024), and multiagent systems (Du et al., 2023), etc. Among these, sampling is the most basic and serves as an atomic operation needed in all other more complicated methods. Therefore, it is crucial to do it well. \n\nPrevious studies (Wang et al., 2023b) have investigated how to find the optimal sampling configuration, such as the best temperature, model, and prompt. While these methods are effective, they miss one key fact: Not all problems require * Equal contribution. Correspondence to kexun@cmu.edu. Figure 1: On 2 single-turn benchmarks and 1 agentic benchmark with a total of 6 tasks, our optimized allocations of sample compute are better than both optimal pure allocations and uniform allocations in most cases, especially when the compute budget is small. \n\nthe same optimal sampling configuration. Some problems are easier solved with higher temperatures while others with lower temperatures (Li et al., 2022). In this case, the best way to use a limited compute budget is not to choose between high or low temperatures, but to split the budget between both. This highlights the need for a mixed allocation of the sample budget instead of a pure allocation that uses a single configuration. As shown in Figure 1, just by uniformly allocating the compute budget over all possible inference configurations (dubbed \"uniform mixed\"), LLMs' accuracy can already surpass the optimal pure allocation on LiveCodeBench. Uniform mixed allocation gets 64% accuracy with 4 samples, while optimal pure needs 16x more samples to get a similar accuracy. Since uniform allocation is only one of the exponentially many allocations in the search space, it is natural to ask: How do we find the optimal sample budget allocation for LLM inference? \n\nWe formulate this as a learning problem: given a set of different inference configurations, a training problem set, and a compute budget, we need to distribute the budget over different configurations such that the expected accuracy is maximized.",
            "score": 0.5040136066947576,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 396
                },
                {
                    "start": 397,
                    "end": 436
                },
                {
                    "start": 439,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 730
                },
                {
                    "start": 731,
                    "end": 991
                },
                {
                    "start": 994,
                    "end": 1034
                },
                {
                    "start": 1035,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1295
                },
                {
                    "start": 1296,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1647
                },
                {
                    "start": 1648,
                    "end": 1775
                },
                {
                    "start": 1776,
                    "end": 1964
                },
                {
                    "start": 1967,
                    "end": 2213
                }
            ],
            "ref_mentions": [
                {
                    "start": 176,
                    "end": 196,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 210,
                    "end": 228,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 253,
                    "end": 270,
                    "matchedPaperCorpusId": "258841118"
                },
                {
                    "start": 456,
                    "end": 476,
                    "matchedPaperCorpusId": "257405357"
                },
                {
                    "start": 1129,
                    "end": 1146,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0255126953125
        },
        {
            "corpus_id": "247447316",
            "title": "Staged Training for Transformer Language Models",
            "text": "Prior work (Gu et al., 2021;Gong et al., 2019) used heuristics to determine the training schedule. In contrast, our goal is to find the optimal training schedule. An optimal training schedule is one that, given a target model size, specifies the optimal sequence of growth operators, intermediate model sizes, and number of training steps in each stage leading to the most compute saving. This section will explain our intuition behind our optimal schedule, then explains how to mathematically find it. \n\nTraining to OPTIMALITY We start from the scaling laws (Kaplan et al., 2020), which showed that the training of transformer language models is initially efficient with fast loss reduction, then the compute-efficient regime ends and the rate of the loss reduction slows down. In addition, the initial compute-efficient regime is longer for larger models. These ideas are illustrated in Figure 1where \u2202L \u2202C is initially large then it slows down. As shown in Kaplan et al. (2020) and Li et al. (2020b), the optimal compute allocation should favor a large model size and stop the training by the end of the initial compute-efficient regime when \u2202L \u2202C = \u03c4 opt , where \u03c4 opt is some threshold. We call this training to \"Optimality\" as opposed to training to \"Completion\" or to convergence. We discuss later this section how to find the point of OPTIMALITY using constrained optimization in an idealistic scenario, then later in Section 5 using a more practical method that estimates \u03c4 opt .",
            "score": 0.5034008507838974,
            "section_title": "Optimal Schedule",
            "char_start_offset": 12384,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 502
                },
                {
                    "start": 505,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 947
                },
                {
                    "start": 948,
                    "end": 1191
                },
                {
                    "start": 1192,
                    "end": 1287
                },
                {
                    "start": 1288,
                    "end": 1488
                }
            ],
            "ref_mentions": [
                {
                    "start": 11,
                    "end": 28,
                    "matchedPaperCorpusId": "225062299"
                },
                {
                    "start": 28,
                    "end": 46,
                    "matchedPaperCorpusId": "174799716"
                },
                {
                    "start": 985,
                    "end": 1002,
                    "matchedPaperCorpusId": "263868979"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1619873046875
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "We control the inference compute (FLOPs) of a fixed model by generating more tokens through the language model1 , sampling further candidate solutions, and ranking them with a reward model. We analyze the performance of fine-tuned models of various sizes given different inference FLOPs on mathematical reasoning benchmarks (e.g., GSM8K test set (Cobbe et al., 2021) and MATH500 test set (Hendrycks et al., 2021b;Lightman et al., 2024)). Our experiments cover several model families, including general-purpose LLMs (e.g., Pythia (Biderman et al., 2023) & Mistral (Jiang et al., 2023)) as well as math-specialized ones (e.g., Llemma (Azerbayev et al., 2024)). \n\nOur results on Pythia (Fig. 1) illustrate how performance scales with increased inference compute across various model sizes. Typically, increasing the compute budget leads to higher accuracy until the accuracy reaches saturation. As the compute budget increases, smaller models initially perform better than larger ones, but once the accuracy of the smaller models saturates, the larger models have favorable performance. The right panel of Figure 1 demonstrates that the optimal model size for inference varies with different levels of computational budgets. However, in real-world deployment, the available compute is typically much lower than the point where the accuracy of relatively small models saturates and larger models begin to show their advantage (as shown in Fig. 4, where the 7B model outperforms the 34B model until 128 Llemma 7B solutions are sampled). This indicates that relatively smaller models could be more compute-optimal for inference. \n\nWe analyze the asymptotic behavior of sampling and voting-based inference strategies, showing their convergence upper bound and rate of convergence. Given a dataset, the accuracy of the language model will ultimately saturate to a fixed limit which is determined by the output probabilities assigned by the model, exhibiting exponential convergence speed through sampling and voting. This implies that, without an oracle verifier, simple strategies like sampling cannot achieve perfect accuracy even with an infinite number of samples, leading to diminishing returns.",
            "score": 0.5031783425457623,
            "section_title": "INTRODUCTION",
            "char_start_offset": 2503,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 189
                },
                {
                    "start": 190,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 658
                },
                {
                    "start": 661,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1221
                },
                {
                    "start": 1222,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1622
                },
                {
                    "start": 1625,
                    "end": 1773
                },
                {
                    "start": 1774,
                    "end": 2008
                },
                {
                    "start": 2009,
                    "end": 2192
                }
            ],
            "ref_mentions": [
                {
                    "start": 388,
                    "end": 413,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 413,
                    "end": 435,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 529,
                    "end": 551,
                    "matchedPaperCorpusId": "257921893"
                },
                {
                    "start": 632,
                    "end": 656,
                    "matchedPaperCorpusId": "264172303"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03912353515625
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.",
            "score": 0.5028835530923421,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1077880859375
        },
        {
            "corpus_id": "262013578",
            "title": "Scaling Laws for Sparsely-Connected Foundation Models",
            "text": "One particularly interesting feature of the joint scaling law just derived is that it allows easily comparing models with different sparsities but the same number of non-zero parameters and training cost. Thus, we can determine in which situations sparse models are better than dense ones, according to all criteria discussed in Section 2. Specifically, we can define the following quantity: \n\nAn interesting property about this contour is that it implies D S = O(N b N /b D ), meaning that if data-is stronger than size-scaling, then the same sparsity is optimal for a smaller data-to-size ratio on larger models. This is sensible as a process bottlenecked more by capacity than by data will benefit more from increasing the former, e.g., by adding sparsity. Finally, we want to point out that S opt can often also be determined explicitly by solving (4) for S, e.g., here for dense training costs with c mul (S) = 1/(1 \u2212 S): \n\n, 0 . (6)  Empirical results. We now compute optimal sparsity curves for our experimental T5 and ViT data, for which we fit scaling laws in the previous subsection. Figure 1 (Right) and 4 show the optimal sparsity contours, both for dense and sparse costs. An interesting feature of Equation ( 5) is that all sparsity contours are, by construction, parallel to the Chinchilla compute optimal line (Hoffmann et al., 2022), which denotes ideal utilization of training FLOPs for fully dense models; this can be clearly observed in the plots as well. However, we note that the Chinchilla line does not necessarily correspond to the S = 0 case since non-zero sparsity may be optimal in this regime (this is the case for sparse-FLOPs). \n\nThe key take-away from these results is that as one trains significantly longer than Chinchilla (dense compute optimal), more and more sparse models start to become optimal in terms of loss for the same number of non-zero parameters. This is because the gains of further training dense models start to slow down significantly at some point, allowing sparse models to overtake them. We further illustrate this effect on a subset of our actual ViT data in Figure 5.",
            "score": 0.5026269876952237,
            "section_title": "OPTIMAL SPARSITY",
            "char_start_offset": 17641,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 204
                },
                {
                    "start": 205,
                    "end": 391
                },
                {
                    "start": 394,
                    "end": 614
                },
                {
                    "start": 615,
                    "end": 759
                },
                {
                    "start": 760,
                    "end": 926
                },
                {
                    "start": 929,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 958
                },
                {
                    "start": 959,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1185
                },
                {
                    "start": 1186,
                    "end": 1475
                },
                {
                    "start": 1476,
                    "end": 1658
                },
                {
                    "start": 1661,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2124
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07696533203125
        },
        {
            "corpus_id": "263310723",
            "title": "Memory in Plain Sight: Surveying the Uncanny Resemblances of Associative Memories and Diffusion Models",
            "text": "The performance of Transformers on language is famously characterized by the \"scaling laws\", which claim that a model's performance will improve as a power-law with model size, dataset size, and the amount of compute used for training [83]. DMs exhibit similar scaling behaviors [84], where larger models perform better than smaller ones in a predictably improving trend. However, the \"scaling laws\" are empirical only, and there is no agreed-upon theory to justify why a model's performance would continue to grow with the model size. \n\nAMs offer one possible answer by characterizing large-model performance as one of memory capacity (see \u00a7 4.2), as recently noted by [85,86]. In the world of AMs, each parameter can be seen as an \"attractor\" in the data space; thus, more parameters means more local energy minima (memories). Similarly, more data means the parameters can identify more meaningful local minima in the energy. More compute, as measured in terms of model depth or number of iterations down an energy landscape, means more optimal (lower energy) retrievals. These hypotheses are still unexplored research questions that come from intuitively understanding large models as AMs.",
            "score": 0.5023174175852798,
            "section_title": "Scaling Laws from the Perspective of Associative Memory",
            "char_start_offset": 32890,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 240
                },
                {
                    "start": 241,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1192
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.092529296875
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "Power laws can arise from a wide variety of sources [THK18]. Power-law scalings with model and dataset size in density estimation [Was06] and in random forest models [Bia12] may be connected with our results. These models suggest that power-law exponents may have a very rough interpretation as the inverse of the number of relevant features in the data. Some early [BB01,Goo01] work found power-law scalings between performance and dataset size. More recent work [HNA + 17, HAD19] also investigated scaling between model size and data size; their work is perhaps the closest to ours in the literature8 . Note, however, that [HNA + 17] found super-linear scaling of dataset size with model size, whereas we find a sub-linear scaling. There are some parallels between our findings on optimal allocation of compute and [Kom19], including power-law learning curves. EfficientNets [TL19] also appear to obey an approximate power-law relation between accuracy and model size. Very recent work [RRBS19b] studies scaling with both dataset size and model size for a variety of datasets, and fits an ansatz similar to ours. \n\nEfficientNet [TL19] advocates scaling depth and width exponentially (with different coefficients) for optimal performance of image models, resulting in a power-law scaling of width as a function of depth. We find that for language models this power should be roughly one when scaling up (as width/depth should remain fixed). But more importantly, we find that the precise architectural hyperparameters are unimportant compared to the overall scale of the language model. In [VWB16] it was argued that deep models can function as ensembles of shallower models, which could potentially explain this finding. Earlier work [ZK16] has compared width and depth, and found that wide ResNets can outperform deep ResNets on image classification. Some studies fix computation per data example, which tends to scale in proportion to the number of model parameters, whereas we investigate scaling with both model size and the quantity of training computation.",
            "score": 0.5019364172121235,
            "section_title": "Related Work",
            "char_start_offset": 36240,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 60
                },
                {
                    "start": 61,
                    "end": 208
                },
                {
                    "start": 209,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 446
                },
                {
                    "start": 447,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 862
                },
                {
                    "start": 863,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1114
                },
                {
                    "start": 1117,
                    "end": 1321
                },
                {
                    "start": 1322,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1587
                },
                {
                    "start": 1588,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1853
                },
                {
                    "start": 1854,
                    "end": 2064
                }
            ],
            "ref_mentions": [
                {
                    "start": 166,
                    "end": 173,
                    "matchedPaperCorpusId": "14463568"
                },
                {
                    "start": 366,
                    "end": 372,
                    "matchedPaperCorpusId": "6645623"
                },
                {
                    "start": 1736,
                    "end": 1742,
                    "matchedPaperCorpusId": "15276198"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07708740234375
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Finally, Bachmann et al. [7] study compute-optimal scaling of MLP's and obtain exponent a = 0. Recent theoretical work study compute-optimal scaling laws in simplified, analytically tractable settings [11,33,38,28]. In particular, Paquette et al. [38] obtain a power law with exponent a = 0.5 (as in Hoffmann et al. [25]) for a random-feature linear regression setting [35,5], conjecturing that this is a part of a broader, universal phenomenon. Jeon and Van Roy [28] also establish an exponent of 0.5 for data generated by infinite-width two-layer ReLU networks, using information-theoretic arguments. \n\nWe also remark on two themes of our paper that draw from prior work. The first is the importance of hyperparameter tuning: several works [30,27,17,15] make the case that smooth, predictable scaling laws emerge when models on all scales are properly tuned. Our work (and particularly Section 4.1) provides another example of this principle and agrees with previous observations that tuning is particularly important at smaller scales. Second, previous studies [59,8,15,26] as well as the concurrent work [22], propose alternative learning rate schedules that address a key shortcoming of cosine decay: the need to commit to a step budget in advance. We consider a constant learning rate that requires no commitment at all. We show this simple choice suffices to reproduce the Hoffmann et al. law and quantify the computational savings compared to a cosine schedule. However, Section 4.1 (and also [25], among others) show that in terms of loss, the constant schedule clearly underperforms the cosine schedule. \n\nConcurrent and independent work. Pearce and Song [40] [30] (see discussion in Section 3.1, Section 3.5, and Appendix H). In addition, our experiments roughly match the Kaplan et al. [30] compute budget, which is about 3 orders of magnitudes larger than budget in Pearce and Song [40], and we perform careful tuning of both the learning rate and the batch size.",
            "score": 0.5017287521299295,
            "section_title": "Related work",
            "char_start_offset": 25787,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 602
                },
                {
                    "start": 605,
                    "end": 673
                },
                {
                    "start": 674,
                    "end": 860
                },
                {
                    "start": 861,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1253
                },
                {
                    "start": 1254,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1469
                },
                {
                    "start": 1470,
                    "end": 1613
                },
                {
                    "start": 1616,
                    "end": 1648
                },
                {
                    "start": 1649,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1976
                }
            ],
            "ref_mentions": [
                {
                    "start": 25,
                    "end": 28,
                    "matchedPaperCorpusId": "259243822"
                },
                {
                    "start": 201,
                    "end": 205,
                    "matchedPaperCorpusId": "267406160"
                },
                {
                    "start": 316,
                    "end": 320,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 373,
                    "end": 375,
                    "matchedPaperCorpusId": "255096726"
                },
                {
                    "start": 746,
                    "end": 749,
                    "matchedPaperCorpusId": "246823711"
                },
                {
                    "start": 1064,
                    "end": 1068,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 1501,
                    "end": 1505,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.274169921875
        },
        {
            "corpus_id": "256416224",
            "title": "Scaling laws for single-agent reinforcement learning",
            "text": "Although our power law for intrinsic performance holds across environments and model sizes, we only obtain a good fit by excluding an initial transient period of training. Put another way, the scaling constants vary over the course of training. \n\nThis phenomenon is clearest with with our MNIST environment, since we were able to use many random seeds to reduce variance. Recall that in this environment, the agent observes a randomly sampled MNIST training set digit each timestep, and the horizon length of the task is artificially controlled using the GAE discount rate \u03b3, as explained in Section 3.3. We fitted our power law to three different periods of training for this environment: an early period (2 16 -2 19 interactions), a middle period (2 19 -2 22 interactions), and a late period (2 22 -2 25 interactions). \n\nFigure 6 shows the fitted values of \u03b1 N and \u03b1 E for these different periods of training. We found \u03b1 E to be significantly lower during the early and middle periods of training, especially for the shorter horizon lengths. \n\nIn order to accurately measure the scaling constants for optimal model size vs compute, it is best to use a period of training during which the learning curves reach the compute-efficient frontier, since otherwise the measurement is an extrapolation. As shown in Figure 7, this is always in the late period  of training, if at all. For this reason, we use the late period of training for all of our results on MNIST outside of this section. \n\nFigure 7 also shows that, for the longer horizon lengths, the learning curves of the larger models did not reach the compute-efficient frontier even during the late period of training. Hence our measurements of 1 1+\u03b1 N /\u03b1 E , the exponent for the scaling of optimal model size with compute, are likely underestimates for these longer horizon lengths. \n\nFor our other environments, we found that it was enough to exclude only the first 1 64 of training in order for our power law for intrinsic performance to be a good fit around the compute-efficient frontier. This is similar to what is needed for the corresponding law for language [Kaplan et al., 2020, Figure 4, right]. Nevertheless, it is possible that the measurement problem identified in this section affects some of our other results.",
            "score": 0.5016083710426269,
            "section_title": "Variability of exponents over training",
            "char_start_offset": 23333,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 244
                },
                {
                    "start": 247,
                    "end": 371
                },
                {
                    "start": 372,
                    "end": 604
                },
                {
                    "start": 605,
                    "end": 820
                },
                {
                    "start": 823,
                    "end": 911
                },
                {
                    "start": 912,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1296
                },
                {
                    "start": 1297,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1673
                },
                {
                    "start": 1674,
                    "end": 1839
                },
                {
                    "start": 1842,
                    "end": 2049
                },
                {
                    "start": 2050,
                    "end": 2162
                },
                {
                    "start": 2163,
                    "end": 2282
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0810546875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.",
            "score": 0.500978199311038,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08648681640625
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "Our findings amplify the findings of Ludziejewski et al. (2024) and further justify the effort to work toward MoEs with experts larger in number and smaller in size (He, 2024). For downstream tasks which their performance is predictable given the pretraining loss (i.e., perplexity), sparsity potentially provides efficiency gains both during pretraining and inference. \n\nHere is a summary of our observations as discussed in Sections 2 to 5 : \n\n\u2022 Larger, Sparser Models Perform Better under a Fixed Compute Budget: When memory and communication overheads are disregarded, increasing sparsity while proportionally expanding the total number of parameters consistently leads to a lower pretraining loss, even when constrained by a fixed training compute budget (see \u00a7 2). \n\n\u2022 Optimal Sparsity for Fixed Model Size: For any given number of parameters and under a fixed training compute budget, model performance as a function of sparsity exhibits a parabolic pattern, reaching its peak at an optimal sparsity level (see \u00a72.2). Specifically, the optimal sparsity level: \n\n-Increases with the total number of parameters approaching 1.0 for larger models. i.e., if a model is relatively small for a given training compute budget, sparsifying it more than a threshold will hurt its performance. On the other hand, if a model is relatively large for a given compute budget, further sparsifying it helps as it leads to increase in the number of tokens the model is trained on under the given training budget constraints (see \u00a72.2). \n\n-Increases across all model sizes as the training compute budget increases (see \u00a7D.1 and \u00a7D.2). \u2022 Effect of Sparsity on Scaling Laws for Optimal Model Size: For any specific sparsity level, performance of the models as a function of their size exhibits parabolic behavior under a fixed training compute budget. i.e., the model reaches its optimal performance at a vertex, that indicates optimal model size. Under these conditions: \n\n-The optimal active number of parameters decreases as the sparsity level increases, leading to smaller FLOPs per example and more efficient inference even though the total number of parameters increases (see \u00a72.1).",
            "score": 0.500749557471759,
            "section_title": "Discussion",
            "char_start_offset": 20423,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 369
                },
                {
                    "start": 372,
                    "end": 443
                },
                {
                    "start": 446,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 1024
                },
                {
                    "start": 1025,
                    "end": 1066
                },
                {
                    "start": 1069,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1523
                },
                {
                    "start": 1526,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1932
                },
                {
                    "start": 1933,
                    "end": 1956
                },
                {
                    "start": 1959,
                    "end": 2173
                }
            ],
            "ref_mentions": [
                {
                    "start": 37,
                    "end": 63,
                    "matchedPaperCorpusId": "267626982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.20751953125
        },
        {
            "corpus_id": "277104404",
            "title": "\u03d5-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation",
            "text": "Inference-Time Optimization. To alleviate the post-training workload (Zelikman et al., 2024;Liu et al., 2024;Team, 2024;Guo et al., 2025), inference-time optimization methods arouse wide concerns, showcasing a notable performance boost in reasoning scenarios (Snell et al., 2024;Sun et al., 2023;Zhao et al., 2024). Mainstream methods can be categorized into searching-based (Yao et al., 2024;Hao et al., 2023;Xie et al., 2024;Wu et al., 2024) and sampling-based (Ma et al., 2024;Chen et al., 2023;Zhang et al., 2024). Although these works achieve the globally-optimal inference, they either induce large computation costs or yield inadequate step value estimation. Other classical methods, such as Best-of-N, usually involve delegating the step selection to the external reward model (Wang et al., 2024;Guan et al., 2025), and self-reflection strategies (Cheng et al., 2024;Xu et al., 2024) usually involve extra training. \u03d5-Decoding stands out as an optimal and efficient decoding choice without reliance on external auxiliary. \n\nAdaptive Inference-time Scaling. Though scaling of inference-time computations has proved to be effective (Snell et al., 2024), the issue of over-thinking is widely observed and remains to be addressed (Chen et al., 2024). One line of works (Team et al., 2025;Han et al., 2024) stress on the control of the generation length, while another line of methods (Manvi et al., 2024;Sun et al., 2024) leverage the idea of early-stopping. In contrast, the adaptive scaling technique presented in our work is training-free and independent of external models. Based on the self-evaluation of stepwise value, \u03d5-Decoding introduces the comprehensive pruning strategy from the dimensions of width and depth. It stands out as a light-weight solution to alleviate the inference-time over-thinking.",
            "score": 0.5000234060481228,
            "section_title": "Related Works",
            "char_start_offset": 19863,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 29,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 665
                },
                {
                    "start": 666,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1029
                },
                {
                    "start": 1032,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1726
                },
                {
                    "start": 1727,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 393,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 410,
                    "end": 427,
                    "matchedPaperCorpusId": "258426922"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015777587890625
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "6)-to train and deploy a model of any given quality and inference demand. \n\nOur principled derivation estimates that LLM practitioners expecting significant demand ( 10 9 inference requests) should train models substantially smaller and longer than Chinchilla-optimal. Figure 1 illustrates the benefits of our compute-optimal method for a realistic scenario. \n\nIn inference-heavy regimes, our modification predicts that it is optimal to train a model far smaller and longer than Chinchilla predicts, up to thousands of tokens per parameter. Does this hold true in practice? Do transformer models see continued improvements at such extreme cases? Or is there a point beyond which models saturate, and additional tokens provide no further improvement, as De Vries (2023) suggests? To uncover the behavior of transformer models in these cases, we train 47 models ranging from 150M to 6B parameters, on various data budgets from 10 to 10,000 tokens per parameter. We find that model quality continues to improve as we scale token ratios. We do not find evidence of a \"saturation point,\" beyond which models do not improve, even with additional data. \n\nLastly, we ablate the parametric curve fitting procedure from the Chinchilla scaling laws. The Chinchilla scaling laws use empirical data from over 400 training runs to determine coefficients that estimate precisely how additional model parameters and training data impact loss. Hoffmann et al. (2022) collected data only from standard token ratio training runs (\u2264 100 tokens/parameters). Our ablation indicates that when fitting Chinchilla coefficients using only typical token ratio runs, this formula overestimates the impact of additional training data as we move to the extreme ratio regime.",
            "score": 0.499355178036446,
            "section_title": "Introduction",
            "char_start_offset": 3611,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 76,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 358
                },
                {
                    "start": 361,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 573
                },
                {
                    "start": 574,
                    "end": 645
                },
                {
                    "start": 646,
                    "end": 778
                },
                {
                    "start": 779,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1145
                },
                {
                    "start": 1148,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1536
                },
                {
                    "start": 1537,
                    "end": 1744
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0309600830078125
        },
        {
            "corpus_id": "256416224",
            "title": "Scaling laws for single-agent reinforcement learning",
            "text": "The scaling of optimal model size with compute is a key input into the biological anchors framework for forecasting transformative artificial intelligence [Cotra, 2020]. In this framework, the human brain is used as a biological anchor for estimating the number of parameters in a transformative model, and optimal model size vs compute scaling laws are used to forecast the total compute required to train such a model. In this section we summarize the main implications of our work for this framework. \n\nScaling exponents for reinforcement learning lie in a similar range to generative modeling. The exponent for the scaling of optimal model size with compute, 1 1+\u03b1 N /\u03b1 E , varied between around 0.4 and 0.8 for our environments, a range that encompasses previous measurements of this exponent for generative modeling. However, as discussed in Section 5.3, we do not think our measurements of this exponent should be taken literally, due to the limitations of our experiments. The results of Hoffmann et al. [2022] and Bahri et al. [2021] suggest the possibility that this exponent would be around 0.5 in every domain if it were measured carefully enough, and we consider our results to be inconclusive on this question. \n\nScaling coefficients for reinforcement learning vary by multiple orders of magnitude. The coefficient for the scaling of optimal model size with compute, N c 1 + \u03b1 N \u03b1 E 1 \u03b1 N , varied substantially, enough that we do not think this variation is attributable only to the limitations of our experiments. For example, the scaling exponents for MNIST (with a horizon length of 1) and Dota 2 are very similar, but a model of the same size needs to be trained for around 2,000 times longer on Dota 2 than on MNIST to be compute-efficient. By comparison, Henighan et al. [2020] found generative modeling to require around 20 times as much training on 32x32 images than on language. Moreover, our analysis of the effect of the task horizon length gives a plausible mechanism for this variation. \n\nArithmetic intensity may confound scaling coefficients.",
            "score": 0.4985209710513768,
            "section_title": "Forecasting compute requirements",
            "char_start_offset": 36063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 169
                },
                {
                    "start": 170,
                    "end": 420
                },
                {
                    "start": 421,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 597
                },
                {
                    "start": 598,
                    "end": 822
                },
                {
                    "start": 823,
                    "end": 980
                },
                {
                    "start": 981,
                    "end": 1224
                },
                {
                    "start": 1227,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1529
                },
                {
                    "start": 1530,
                    "end": 1760
                },
                {
                    "start": 1761,
                    "end": 1902
                },
                {
                    "start": 1903,
                    "end": 2014
                },
                {
                    "start": 2017,
                    "end": 2072
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1282958984375
        },
        {
            "corpus_id": "252683098",
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "text": "The recent observation of neural power-law scaling relations has made a significant impact in the field of deep learning. A substantial amount of attention has been dedicated as a consequence to the description of scaling laws, although mostly for supervised learning and only to a reduced extent for reinforcement learning frameworks. In this paper we present an extensive study of performance scaling for a cornerstone reinforcement learning algorithm, AlphaZero. On the basis of a relationship between Elo rating, playing strength and power-law scaling, we train AlphaZero agents on the games Connect Four and Pentago and analyze their performance. We find that player strength scales as a power law in neural network parameter count when not bottlenecked by available compute, and as a power of compute when training optimally sized agents. We observe nearly identical scaling exponents for both games. Combining the two observed scaling laws we obtain a power law relating optimal size to compute similar to the ones observed for language models. We find that the predicted scaling of optimal neural network size fits our data for both games. This scaling law implies that previously published state-of-the-art game-playing models are significantly smaller than their optimal size, given the respective compute budgets. We also show that large AlphaZero models are more sample efficient, performing better than smaller models with the same amount of training data.",
            "score": 0.498370546443463,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0975341796875
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "However, in scenarios or for architectures where the number of parameters and FLOPs per example are not directly linked, it is essential to jointly consider the effects of these variables on scaling model capacity (Clark et al., 2022). We therefore ask \"Can we draw scaling laws for the optimal trade-off between parameter count and FLOPs per example?\" \n\nTo address this question, we study sparse Mixture-of-Expert Transformers (MoEs) (Shazeer et al., 2017;Lepikhin et al., 2021;Fedus et al., 2022;Zoph et al., 2022;Muennighoff et al., 2024) in the context of language modeling. Existing scaling law studies for MoEs, investigate the role of variables like number and granularity (Ludziejewski et al., 2024)of experts, underlying dense model size and inference compute in predicting the performance of the models under different conditions such as training or inference compute optimality (Du et al., 2021;Clark et al., 2022;Yun et al., 2024;Ludziejewski et al., 2024). In this paper, we focus on the interaction between FLOPs per example and total parameter count, and their impact on model performance in MoEs, through a large-scale empirical study. \n\nWe define sparsity as the ratio of inactive experts to the total number of experts, which controls the ratio of the total number of parameters to FLOPs per example in MoEs. We evaluate loss and downstream metrics for different sparsities, model sizes, and compute budgets. Through qualitative and quantitative analysis to derive scaling laws which disentangle total parameters vs FLOPs per example in MoEs, we can estimate the optimal sparsity level under the setting where both total training FLOPs and total number of parameters are given and fixed. Generally, we find that: \n\n\u2022 During pretraining, increasing a model's capacity by adding more parameters yields greater benefits than increasing FLOPs per example. We observe that the size of compute-optimal models increases as we increase the training budget (measured in terms of total FLOPs) while the active number of parameters, hence FLOPs per example, decrease for compute-optimal models.",
            "score": 0.4953098198833269,
            "section_title": "Introduction",
            "char_start_offset": 1652,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 352
                },
                {
                    "start": 355,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1151
                },
                {
                    "start": 1154,
                    "end": 1326
                },
                {
                    "start": 1327,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1705
                },
                {
                    "start": 1706,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1869
                },
                {
                    "start": 1870,
                    "end": 2101
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 234,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 435,
                    "end": 457,
                    "matchedPaperCorpusId": "12462234"
                },
                {
                    "start": 457,
                    "end": 479,
                    "matchedPaperCorpusId": "220265858"
                },
                {
                    "start": 479,
                    "end": 498,
                    "matchedPaperCorpusId": "231573431"
                },
                {
                    "start": 680,
                    "end": 707,
                    "matchedPaperCorpusId": "267626982"
                },
                {
                    "start": 906,
                    "end": 925,
                    "matchedPaperCorpusId": "246473179"
                },
                {
                    "start": 942,
                    "end": 968,
                    "matchedPaperCorpusId": "267626982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.14990234375
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "In this case, the dynamics smoothly transition between following the gradient descent trajectory at early time to an asymptote that depends on N as t \u2192 \u221e. In Figure 2 (c)-(d) we illustrate these learning curves from our theory and from finite N simulations, showing a good match of the theory to experiment. \n\nWe derive the asymptotic scaling of N \u2212\u03b1 min{2,\u03b2} in Appendix E.3. Intuitively, at finite N , the dynamics only depend on the filtered signal 1 N A(0) \u22a4 A(0) w \u22c6 . Thus the algorithm can only estimate, at best, the top N components of w \u22c6 , resulting in the following t \u2192 \u221e loss \n\nSGD Noise Effects The variance in the learned model predictions due to random sampling of minibatches during SGD also alters the mean field prediction of the loss. In Figure 3, we show SGD noise effects from finite batch size B for hard \u03b2 < 1 and super easy \u03b2 > 2 \u2212 1/\u03b1 tasks. \n\nCompute Optimal Scaling Laws in Feature Learning Regime At a fixed compute budget C = N t, one can determine how to allocate compute towards training time t and model size N \n\nTable 1: Compute optimal scaling exponents r C for the loss L \u22c6 (C) \u223c C \u2212r C for tasks of varying difficulty in the feature learning regime. For \u03b2 > 1, the exponents coincide with the lazy model analyzed by Bordelon et al. (2024a); Paquette et al. (2024), while for hard tasks they are improved. using our derived exponents from the previous sections. Choosing N, t optimally, we derive the following compute optimal scaling laws L \u22c6 (C) in the feature learning regime \u03b3 > 0. These are also summarized in Figure 1. 5 . Hard task regime (\u03b2 < 1): the compute optimum balances the population gradient flow term t \u2212 2\u03b2 1+\u03b2 and the model bottleneck \n\nWe work out the complete compute optimal scaling laws for these three settings by imposing the constraint C = N t, identifying the optimal choice of N and t at fixed t and verifying the assumed dominant balance. We summarize the three possible compute scaling exponents in Table 1.",
            "score": 0.4948544456767074,
            "section_title": "Accelerated Training in Rich Regime",
            "char_start_offset": 20444,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 154
                },
                {
                    "start": 155,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 867
                },
                {
                    "start": 870,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1186
                },
                {
                    "start": 1187,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1562
                },
                {
                    "start": 1563,
                    "end": 1689
                },
                {
                    "start": 1692,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 1973
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.158935546875
        },
        {
            "corpus_id": "266755802",
            "title": "TinyLlama: An Open-Source Small Language Model",
            "text": "Recent progress in natural language processing (NLP) has been largely propelled by scaling up language model sizes (Brown et al., 2020;Chowdhery et al., 2022;Touvron et al., 2023a,b). Large Language Models (LLMs) pre-trained on extensive text corpora have demonstrated their effectiveness on a wide range of tasks (OpenAI, 2023;Touvron et al., 2023b). Some empirical studies demonstrated emergent abilities in LLMs, abilities that may only manifest in models with a sufficiently large number of parameters, such as few-shot prompting (Brown et al., 2020) and chain-of-thought reasoning (Wei et al., 2022). Other studies focus on modeling the scaling behavior of LLMs (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) suggest that, to train a compute-optimal model, the size of the model and the amount of training data should be increased proportionally. This provides a guideline on how to optimally select the model size and allocate the amount of training data when the compute budget is fixed. \n\nAlthough these works show a clear preference on large models, the potential of training smaller models with larger datasets remains under-explored. Instead of training compute-optimal language models, Touvron et al. (2023a) highlight the importance of the inference budget, instead of focusing solely on training compute-optimal language models. Inference-optimal language models aim for optimal performance within specific inference constraints. This is achieved by training models with more tokens than what is recommended by the scaling law (Hoffmann et al., 2022). Touvron et al. (2023a) demonstrates that smaller models, when trained with more data, can match or even outperform their larger counterparts. Also, Thadd\u00e9e (2023) suggest that existing scaling laws (Hoffmann et al., 2022) may not predict accurately in situations where smaller models are trained for longer periods. \n\nMotivated by these new findings, this work focuses on exploring the behavior of smaller models when trained with a significantly larger number of tokens than what is suggested by the scaling law (Hoffmann et al., 2022).",
            "score": 0.4946883800091475,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 183
                },
                {
                    "start": 184,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 605
                },
                {
                    "start": 606,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 872
                },
                {
                    "start": 873,
                    "end": 1015
                },
                {
                    "start": 1018,
                    "end": 1165
                },
                {
                    "start": 1166,
                    "end": 1363
                },
                {
                    "start": 1364,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1586
                },
                {
                    "start": 1587,
                    "end": 1728
                },
                {
                    "start": 1729,
                    "end": 1902
                },
                {
                    "start": 1905,
                    "end": 2124
                }
            ],
            "ref_mentions": [
                {
                    "start": 115,
                    "end": 135,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 534,
                    "end": 554,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 586,
                    "end": 604,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 688,
                    "end": 710,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 712,
                    "end": 734,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1562,
                    "end": 1585,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1785,
                    "end": 1808,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.040252685546875
        },
        {
            "corpus_id": "269033049",
            "title": "Scaling Laws for Data Filtering\u2014Data Curation Cannot be Compute Agnostic",
            "text": "Some other works include Yu et al. (2023) which uses a mixture of rules and Xu et al. (2023) which uses similarity with downstream metadata.\n\nIn this work, we highlight why data filtering cannot be agnostic to training compute and how the ordering varies as one changes the training paradigm.In fact, we showcase LAION filtering (used to train state-of-the-art OpenCLIP models ) can even underperform no-filtering or training on the raw common crawl under certain settings.\n\nScaling Laws in Language Modeling One of the most salient trends in recent deep learning research is the observation that neural network performance tends to improve predictably with increases in model size, data size, and computation.In the domain of language modeling, such observations have been systematized into a set of principles known as scaling laws.Kaplan et al. (2020) conducted a comprehensive study on scaling laws for neural language models.They observed that, given fixed computational budgets, there exists an optimal model size, training data size, and training time.Interestingly, the triple (model size, data size, batch size) corresponding to the state of the art tends to scale in lockstep, reinforcing the intuition that larger models require more data and more computation to be trained effectively.This observation is corroborated by Hernandez et al. (2021);Hoffmann et al. (2022) who delve deeper into training compute-optimal language models and highlight the importance of balancing computation with model and data sizes.Sardana and Frankle (2023) propose modifications to incorporate the inference cost into the scaling laws.Bahri et al. (2021); Hutter (2021) theoretically study neural scaling laws.\n\nMost closely related to our work, Muennighoff et al. (2023) show that training on tokens beyond four epochs yields negligible gains compared to training on new language data due to diminishing utility.However, they do not consider the case of different data quality pools.In this work, we how that mixture of data pools cannot be modeled with an effective dataset size formulation of Muennighoff et al. (2023).Crucially, one needs to model a decay in utility factor (the scaling parameter b in y = an b ) as well.",
            "score": 0.4938345386705344,
            "section_title": "Related Work",
            "char_start_offset": 8767,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 140
                },
                {
                    "start": 142,
                    "end": 292
                },
                {
                    "start": 292,
                    "end": 473
                },
                {
                    "start": 475,
                    "end": 710
                },
                {
                    "start": 710,
                    "end": 834
                },
                {
                    "start": 834,
                    "end": 930
                },
                {
                    "start": 930,
                    "end": 1059
                },
                {
                    "start": 1059,
                    "end": 1297
                },
                {
                    "start": 1297,
                    "end": 1523
                },
                {
                    "start": 1523,
                    "end": 1628
                },
                {
                    "start": 1628,
                    "end": 1703
                },
                {
                    "start": 1705,
                    "end": 1906
                },
                {
                    "start": 1906,
                    "end": 1977
                },
                {
                    "start": 1977,
                    "end": 2115
                },
                {
                    "start": 2115,
                    "end": 2218
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2078857421875
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Our first experimental setting considers scaling in a setting where all models have the same data constraint. For these experiments, the unique training data budget D C is fixed at either 100M, 400M or 1.5B tokens. For each data budget, we train a set of language models with increasing amounts of compute that is allocated to either more parameters or more epochs on the unique training data. \n\nFigure 3 (left) shows the main results for scaling with 100M unique tokens2 (see Appendix C for 400M and 1.5B tokens). For 100M tokens, the corresponding one-epoch compute-optimal model  according to scaling laws from [42] has U N of approximately 7M parameters (see Appendix B for the scaling coefficients we use). Results show that more than a 50% reduction in loss can be attained by training for several epochs (R D > 0) and increasing model size beyond what would be compute-optimal for 100M tokens (R N > 0). We find the best loss to be at around 20-60\u00d7 more parameters and epochs, which corresponds to spending around 7000\u00d7 more FLOPs. These results suggest that one-epoch models significantly under-utilize their training data and more signal can be extracted by repeating data and adding parameters at the cost of sub-optimal compute utilization. \n\nFigure 3 (right) shows the predicted contours created by fitting our data-constrained scaling laws on 182 training runs. In the single-epoch case (R D = 0) with near compute-optimal parameters (R N = 0) our scaling equation ( \u00a73.1) reduces to the Chinchilla equation. In this case, both formulas predict the optimal allocation of compute to parameters and data to be the same, resulting in overlapping efficient frontiers. As data is repeated for more than a single epoch, our fit predicts that excess parameters decay faster in value than repeated data (R * N < R * D ). As a result, the dataconstrained efficient frontier suggests allocating most additional compute to more epochs rather than more parameters. This contrasts the Chinchilla scaling laws [42], which suggest equally scaling both. However, note that they do not repeat the entire training data and their parametric fit explicitly relies on the assumption that models are trained for a single epoch only.",
            "score": 0.4929039398237775,
            "section_title": "Results: Resource Allocation for Data-Constrained Scaling",
            "char_start_offset": 11698,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 109
                },
                {
                    "start": 110,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 393
                },
                {
                    "start": 396,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 711
                },
                {
                    "start": 712,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1251
                },
                {
                    "start": 1254,
                    "end": 1374
                },
                {
                    "start": 1375,
                    "end": 1521
                },
                {
                    "start": 1522,
                    "end": 1676
                },
                {
                    "start": 1677,
                    "end": 1825
                },
                {
                    "start": 1826,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2050
                },
                {
                    "start": 2051,
                    "end": 2223
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07452392578125
        },
        {
            "corpus_id": "247778764",
            "title": "Training Compute-Optimal Large Language Models",
            "text": "In our second approach we vary the model size5 for a fixed set of 9 different training FLOP counts6 (ranging from 6 \u00d7 10 18 to 3 \u00d7 10 21 FLOPs), and consider the final training loss for each point7 . in contrast with Approach 1 that considered points (, , ) along the entire training runs. This allows us to directly answer the question: For a given FLOP budget, what is the optimal parameter count? For various model sizes, we choose the number of training tokens such that the final FLOPs is a constant. The cosine cycle length is set to match the target FLOP count. We find a clear valley in loss, meaning that for a given FLOP budget there is an optimal model to train (left). Using the location of these valleys, we project optimal model size and number of tokens for larger models (center and right). In green, we show the estimated number of parameters and tokens for an optimal model trained with the compute budget of Gopher. \n\nFor each FLOP budget, we plot the final loss (after smoothing) against the parameter count in Figure 3 (left). In all cases, we ensure that we have trained a diverse enough set of model sizes to see a clear minimum in the loss. We fit a parabola to each IsoFLOPs curve to directly estimate at what model size the minimum loss is achieved (Figure 3 (left)). As with the previous approach, we then fit a power law between FLOPs and loss-optimal model size and number of training tokens, shown in Figure 3 (center, right). Again, we fit exponents of the form   \u221d   and   \u221d   and we find that  = 0.49 and  = 0.51-as summarized in Table 2. (2) \n\nThe first term captures the loss for an ideal generative process on the data distribution, and should correspond to the entropy of natural text. The second term captures the fact that a perfectly trained transformer with  parameters underperforms the ideal generative process. The final term captures the fact that the transformer is not trained to convergence, as we only make a finite number of optimisation steps, on a sample of the dataset distribution. \n\nModel fitting.",
            "score": 0.4916643285387472,
            "section_title": "Approach 2: IsoFLOP profiles",
            "char_start_offset": 12346,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 505
                },
                {
                    "start": 506,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 806
                },
                {
                    "start": 807,
                    "end": 934
                },
                {
                    "start": 937,
                    "end": 1047
                },
                {
                    "start": 1048,
                    "end": 1164
                },
                {
                    "start": 1165,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1456
                },
                {
                    "start": 1457,
                    "end": 1575
                },
                {
                    "start": 1578,
                    "end": 1722
                },
                {
                    "start": 1723,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2035
                },
                {
                    "start": 2038,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00945281982421875
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "Another work (Hoffmann et al., 2022) re-examined the particular values in the scaling laws of (Kaplan et al., 2020) while using a learning rate schedule. They demonstrated that one can use a schedule to affect the scaling laws so that the compute optimal model is achieved with a smaller size model trained on more tokens, as compared to the values obtained in (Kaplan et al., 2020) for a fixed learning rate schedule. They observed that large models in practice followed the compute optimal T, P allocation suggested by (Kaplan et al., 2020), and so the authors were able to leverage their own scaling laws to obtain a 70B parameter LLM \"Chinchilla\", which was competitive with much larger models of size greater than 175B parameters. \n\nStudying scaling laws for fine-tuning would require additional consideration in comparison to full training on a fixed dataset. \n\nIf we change the pre-trained model to adjust the number of parameters, the differences in pre-trained model performance can vary greatly due to completely different pre-train datasets and setups, with large effects that may supersede mere model parameter count. For example, the aforementioned pre-trained chinchilla model of (Hoffmann et al., 2022) which outperforms larger models that were trained differently. To circumvent this issue while adhering to common practice in fine-tuning, we leverage the fine-tuning technique of Low-Rank Adaptation of LLMs (LoRA) (Hu et al., 2022). LoRA allows for adding or removing parameters fine-tuned while incorporating the pre-trained knowledge of the base model.",
            "score": 0.4904723981608566,
            "section_title": "Scaling Laws for Training LLMs",
            "char_start_offset": 8597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 418
                },
                {
                    "start": 419,
                    "end": 735
                },
                {
                    "start": 738,
                    "end": 865
                },
                {
                    "start": 868,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1450
                },
                {
                    "start": 1451,
                    "end": 1572
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.11920166015625
        },
        {
            "corpus_id": "278636485",
            "title": "Analog Foundation Models",
            "text": "As conventional scaling laws approach their limits, novel directions for scaling LLMs have been explored. In test-time compute scaling, model performance is improved by increasing the amount of compute used by the model at inference time. As AIMC is rather unsuitable for training, but promises orders of magnitudes higher power efficiency for inference, shifting the compute budget from training to inference is an ideal trend for AIMC. Therefore, we investigated whether our models also performed on-par to 4-bit weight quantized models when the compute used for inference is scaled up. To test this, we follow Snell et al. [66] and generate n responses to each prompt from the MATH-500 [61] dataset. Using a math process reward model [67,68], answers are assigned a reward and the best answer is chosen by performing weighted majority voting or by simply picking the answer with the highest reward. For each model, we pick the strategy that performed best. As figure 4 shows, the analog foundation models with noisy weights perform better compared to models trained with 8-bit static input and 4-bit per-channel weight quantization. Additionally, we observe that the gap between the noisy analog foundation model and the original off-the-shelf model decreases as we increase n. While this increase is small (0.4%) for Phi-3-mini-4k-instruct, it is much bigger for Llama-3.2-1B-Instruct (3.58%). Interestingly, our results show that the analog foundation models scale better compared to the LLM-QAT models. For Phi-3-mini-4k-instruct, the performance gap between the analog foundation model and the quantized model grows from \u22120.8% to 1.8%. For Llama-3.2-1B-Instruct, the gap grows from 1.34% to 4.74%.",
            "score": 0.48946567196747454,
            "section_title": "Test-time compute scaling",
            "char_start_offset": 24753,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 702
                },
                {
                    "start": 703,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1388
                },
                {
                    "start": 1389,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1508
                },
                {
                    "start": 1509,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1704
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0248870849609375
        },
        {
            "corpus_id": "272753259",
            "title": "Training Language Models to Self-Correct via Reinforcement Learning",
            "text": "Next, we investigate if SCoRe can be used in conjunction with inference-time compute scaling strategies. \n\nTo do so, we evaluate self-consistency decoding (Wang et al., 2022), where we sample a diverse set of solutions, and then select the most consistent answer among these solutions. Typically, the default strategy is to sample all solutions in parallel to perform majority voting. However, we show in Figure 1 (right) that instead of sampling 2 solutions in parallel, it is more compute-efficient to sample  solutions in parallel, then perform one round of self-correction on each solution. With 32 solution budget per problem, parallel sampling shows a 7.4% accuracy gain, while combining it with sequential sampling using self-correction yields a 10.5% improvement. Finally, we also present a number of ablation studies to understand the importance of various components in SCoRe. We perform these ablations on the MATH dataset. Concretely, we aim to answer the following questions: (1) the importance of multi-turn training: Can RL trained to maximize single-turn performance achieve better accuracy@t1 or accuracy@t2?; The results of all of these ablation experiments are shown in Table 4. As expected, single-turn training improves turn 1 performance, but has negative \u0394(t1, t2). As shown in Figure 6, Stage I is critical to SCoRe; without it, the model achieves 2% lower \u0394(t1, t2) and 3% lower accuracy@t2. Similarly, we find that removing reward shaping also hurts performance, indicating that the RL objectives in both stages play a significant role in teaching the self-correction behavior. We also find that replacing REINFORCE with STaR in Stage II results in significantly lower absolute performance with no visible improvements in self-improvement performance, which contrasts with the findings in Havrilla et al. (2024a) that STaR and on-policy RL have similar convergence rates for single-turn RL. This suggests that leveraging on-policy samples is especially critical in the self-correction setting, which presents a multi-turn problem that admits potentially spurious solutions.",
            "score": 0.48822715566618513,
            "section_title": "Inference-Compute Scaling with Self-Correction",
            "char_start_offset": 29788,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 107,
                    "end": 285
                },
                {
                    "start": 286,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 771
                },
                {
                    "start": 772,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 934
                },
                {
                    "start": 935,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1197
                },
                {
                    "start": 1198,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1416
                },
                {
                    "start": 1417,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1916
                },
                {
                    "start": 1917,
                    "end": 2099
                }
            ],
            "ref_mentions": [
                {
                    "start": 155,
                    "end": 174,
                    "matchedPaperCorpusId": "254017497"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0237274169921875
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "Our contributions are 1. We propose a simple two-layer linear neural network model trained with a form of projected gradient descent. We show that this model reproduces power law scalings in training time, model size and training set size. The predicted scaling law exponents are summarized in terms of two parameters related to the data and architecture (\u03b1, \u03b2). 2. We identify a condition on the difficulty of the learning task, measured by the source exponent \u03b2, under which feature learning can improve the scaling of the loss with time and with compute. For easy tasks, which we define as tasks with \u03b2 > 1 where the RKHS norm of the target is finite, there is no improvement in the power-law exponent while for hard tasks (\u03b2 < 1) that are outside the RKHS of the initial limiting kernel, there can be an improvement. For super-easy tasks \u03b2 > 2 \u2212 1 \u03b1 , which have very low RKHS norm, variance from stochastic gradient descent (SGD) can alter the scaling law at large time. Figure 1 summarizes these results. 3. We provide an approximate prediction of the compute optimal scaling laws for hard, easy tasks and super-easy tasks. Each of these regimes has a different exponent for the compute optimal neural scaling law. Table 1 summarizes these results. 4. We test our predicted feature learning scalings by training deep nonlinear neural networks fitting nonlinear functions. In many cases, our predictions from the initial kernel spectra accurately capture the test loss of the network in the feature learning regime. \n\nOverall our results suggest that feature learning may improve scaling law exponents by changing the optimization trajectory for tasks that are hard for the initial kernel.",
            "score": 0.48775859912353337,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3701,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 24
                },
                {
                    "start": 25,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 557
                },
                {
                    "start": 558,
                    "end": 820
                },
                {
                    "start": 821,
                    "end": 975
                },
                {
                    "start": 976,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1129
                },
                {
                    "start": 1130,
                    "end": 1220
                },
                {
                    "start": 1221,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1377
                },
                {
                    "start": 1378,
                    "end": 1520
                },
                {
                    "start": 1523,
                    "end": 1694
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.123779296875
        },
        {
            "corpus_id": "276961790",
            "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
            "text": "Neural scaling laws aim to model and predict loss values as a function of the compute scale in FLOPs. FLOPs are often estimated as B \u2248 6pt, which involves contributions from the size of the training set in tokens, t, and the parameter count of the model, p. Typically, scaling laws that model these two quantities in relation to the loss exhibit a tradeoff between parameter count and compute scale. These are depicted using IsoFLOP curves, where the x-axis is the parameter count, and the y-axis is the loss on the training or validation set. To obtain those curves, Hoffmann et al. ( 2022) pioneered using a 2D power law model which has separate power law components for parameter and token counts. In our case, we found 2D power law models to produce a poor fit to downstream evaluations, which we attribute to noise and small dataset sizes, compared to validation sets. Instead, we opted to follow the approach used by Dubey et al. (2024), which involves fitting separate degree-2 polynomials to each compute scale. We extend this by fitting a power law to the optima of the compute scales. \n\nCompute budget and COs Central to the idea of IsoFLOP curves, which model the loss at many compute scales, is the idea of IsoFLOP groups which model tradeoffs between dataset and model size at fixed compute scales. In other words, an IsoFLOP group is a set of models where the amount of training data, t, and model size, p, is varied subject to a fixed approximate compute budget B \u2248 6pt. This tradeoff between dataset size and parameter count can be optimised at each compute scale, which means that at a given compute budget, there is a optimal parameter count and dataset size, (p * , t * ) s.t. B FLOPs. Typically, IsoFLOP curves (and therefore COs) are computed using the loss on the training set, or for flexibility, a validation set. Since both the training and validation sets include mixtures of data that might influence scaling behaviours differently, we refer to the losses on these sets as aggregate performance estimators, or APEs. Under this terminology, standard practice involves selecting COs based on APEs.",
            "score": 0.4873015455361617,
            "section_title": "Scaling laws",
            "char_start_offset": 5728,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 101
                },
                {
                    "start": 102,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 399
                },
                {
                    "start": 400,
                    "end": 543
                },
                {
                    "start": 544,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1094
                },
                {
                    "start": 1097,
                    "end": 1311
                },
                {
                    "start": 1312,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1837
                },
                {
                    "start": 1838,
                    "end": 2042
                },
                {
                    "start": 2043,
                    "end": 2122
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.187744140625
        },
        {
            "corpus_id": "258740735",
            "title": "PaLM 2 Technical Report",
            "text": "To determine the scaling laws for our configuration, we follow the same procedure as Hoffmann et al. (2022). We train several differently sized models with 4 different compute budgets: 1 \u00d7 10 19 , 1 \u00d7 10 20 , 1 \u00d7 10 21 , and 1 \u00d7 10 22 FLOPs. For each compute budget, we use the heuristic FLOPs \u2248 6ND (Kaplan et al., 2020) to determine how many tokens to train each model for. Critically, we use cosine learning rate decay and ensure that each model's learning rate fully decays at its final training token.\n\nSmoothing final validation loss for each model, we perform quadratic fits for each isoFLOPS band ( Figure 4). The minima of those quadratic fits indicate the projected optimal model sizes (N) for each isoFLOPS band. The optimal D is derived from the heuristic FLOPs. Plotting these optimal Ns and optimal Ds against FLOPs ( Figure 5), we find that D and N should grow in equal proportions as the FLOPs budget increases. This is a strikingly similar conclusion to Hoffmann et al. (2022), despite that study being conducted at a smaller scale, and with a different training mixture.\n\nWe use the scaling laws from Figure 5 to compute the optimal model parameters (D) and training tokens (N) for 1 \u00d7 10 22 , 1 \u00d7 10 21 and 1 \u00d7 10 20 FLOPs. We then train several models from 400M to 15B on the same pre-training mixture for up to 1 \u00d7 10 22 FLOPs. Finally, we compute loss at the three FLOP points for each model. The resulting training losses and their associated optimal model parameters are included in Table 1. We can observe that the lowest loss is achieved by Figure 5: The scaling law obtained from all 4 compute scales. Table 1: Estimated optimal parameter size at a given number of FLOPs in our study compared to the study of Hoffmann et al. (2022). Please note that these models were used only for the scaling law study, and do not reflect the model sizes and FLOPs used in PaLM 2 models. 2.25 \u00d7 10 10 1.14 \u00d7 10 10 8.43 \u00d7 10 9 3.75 \u00d7 10 9 \u223c 1B 1.04B the models that approximately follow the optimal model parameters (D) given",
            "score": 0.48728264858308334,
            "section_title": "Scaling laws",
            "char_start_offset": 7847,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 85,
                    "end": 107,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 971,
                    "end": 993,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1736,
                    "end": 1758,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0784912109375
        },
        {
            "corpus_id": "268691910",
            "title": "Mechanistic Design and Scaling of Hybrid Architectures",
            "text": "Finding 8: There exists a relation of the type P * \u221d M c between compute-optimal perplexity P * and total state size M , with c \u2248 \u22120.28 in our scaling experimental setup, consistent across all model architectures. The model class determines the offset of the state-optimal curve. Concretely, state-optimal scaling indicates that one may reach any target perplexity (up to saturation of compute-optimal scaling laws i.e., approaching entropy of text) with fixed-state architectures, by paying a FLOP cost multiplier that depends on the model class -training longer to maximize state utilization. Input-varying recurrences, multihead and striped hybrid architectures achieve a favourable trade-off between metrics, with comparable or improved compute-optimal perplexity to Transformers++ and a reduced total state dimension. \n\n4.3 Compute-optimal scaling at byte resolution 8 Accounting for state-optimality shifts the optimal ratio to 10%. Compute-optimal and state-optimal scaling on The Pile. We report total state dimension, fixed (recurrences) and dynamic (attention). All models are trained at sequence length 8k. We identify distinct regions in the state-optimal frontier, indicating that one may pay an additional FLOP cost to obtain the same perplexity with a state of smaller dimension, by using other classes of architectures. Scaling laws analysis primarily focus on sub-word level tokenization. With a new range of architectural options, we also explore computeoptimal scaling of a subset of architectures (Transformer++, Mamba, Hyena and StripedHyena) at byte resolution. We scale the models across FLOP budgets from 8e18 to 8e19 with model sizes from 6M to 1B parameters. The compute-optimal frontier is obtained using a similar protocol as outlined in Sec. C, with additional details and results shown in Sec. D.2. \n\nWe find attention-based models to yield significantly higher perplexity at all IsoFLOP groups, with alternative architectures outperforming Transformer++, including non-striped variants (Figure 4.4). These results show that model ranking varies significantly across domains and tokenization strategies.",
            "score": 0.48578765956189063,
            "section_title": ".3, right).",
            "char_start_offset": 28178,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 213
                },
                {
                    "start": 214,
                    "end": 279
                },
                {
                    "start": 280,
                    "end": 594
                },
                {
                    "start": 595,
                    "end": 822
                },
                {
                    "start": 825,
                    "end": 938
                },
                {
                    "start": 939,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1071
                },
                {
                    "start": 1072,
                    "end": 1117
                },
                {
                    "start": 1118,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1405
                },
                {
                    "start": 1406,
                    "end": 1583
                },
                {
                    "start": 1584,
                    "end": 1684
                },
                {
                    "start": 1685,
                    "end": 1770
                },
                {
                    "start": 1771,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1828
                },
                {
                    "start": 1831,
                    "end": 2030
                },
                {
                    "start": 2031,
                    "end": 2133
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037811279296875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Test-Time Compute Scaling. Leveraging more test-time compute to improve the performance of LLMs has gained a lot of popularity. Recent studies have explored various methods to scale test-time compute. A widely recognized baseline technique is repeatedly sampling candidate solutions from a model to choose the most frequent answer (aka self-consistency or majority-voting) [38]. However, recent studies are pushing beyond this, investigating methods that leverage LLMs to iteratively refine their generated outputs [9,12,26]. Reasoning models, such as OpenAI o3 series [31] and DeepSeek R1 [10] have enabled sequential scaling of test-time compute by scaling the length of the generated CoT (rather than parallel scaling by generating multiple shorter candidate solutions). While these long CoTs may implicitly incorporate forms of reflection, verification, or refinement within their extended reasoning sequence, such models and previous studies do not primarily address the compute optimality of their proposed methods, the main focus of our investigation. \n\nVerification. Another common method to scale inference-time compute is to score candidate solutions through verification, which can be achieved via several techniques. Traditionally, discriminative models are employed, trained via binary classification [8,28,41] or preferences [20,42]. Generative verifiers frame verification as a next-token-prediction task, enabling the use of CoT reasoning and another axis to increase inference-time compute, either with trained verifiers [1,29,43] or simply via prompting an off-the-shelf LLM (aka LLM-as-ajudge) [4,7,24,45,46] or self-verification based sampling [44]. These studies evaluate candidate solutions at outcome level, rather than process level verification [27,37]. Discriminative verifiers have become less favored due to the inherent challenges in their training and their tendency to exhibit lower performance compared to generative approaches [43]. Existing verification-based scaling studies focus on improving accuracy but ignore the overall cost of adding more verifications. They are not concerned with the compute optimal setting about spending one's budget on generating more candidate solutions or more verifications for existing solutions. \n\nInference Scaling Laws.",
            "score": 0.4836918894685609,
            "section_title": "Related Work",
            "char_start_offset": 26990,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 200
                },
                {
                    "start": 201,
                    "end": 378
                },
                {
                    "start": 379,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 773
                },
                {
                    "start": 774,
                    "end": 1058
                },
                {
                    "start": 1061,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1228
                },
                {
                    "start": 1229,
                    "end": 1347
                },
                {
                    "start": 1348,
                    "end": 1669
                },
                {
                    "start": 1670,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1965
                },
                {
                    "start": 1966,
                    "end": 2095
                },
                {
                    "start": 2096,
                    "end": 2264
                },
                {
                    "start": 2267,
                    "end": 2290
                }
            ],
            "ref_mentions": [
                {
                    "start": 518,
                    "end": 521,
                    "matchedPaperCorpusId": "258823123"
                },
                {
                    "start": 1320,
                    "end": 1323,
                    "matchedPaperCorpusId": "265221057"
                },
                {
                    "start": 1339,
                    "end": 1343,
                    "matchedPaperCorpusId": "267617275"
                },
                {
                    "start": 1544,
                    "end": 1547,
                    "matchedPaperCorpusId": "271963324"
                },
                {
                    "start": 1618,
                    "end": 1621,
                    "matchedPaperCorpusId": "265675839"
                },
                {
                    "start": 1621,
                    "end": 1624,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1624,
                    "end": 1627,
                    "matchedPaperCorpusId": "259129398"
                },
                {
                    "start": 1770,
                    "end": 1774,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 1774,
                    "end": 1777,
                    "matchedPaperCorpusId": "266209760"
                },
                {
                    "start": 1960,
                    "end": 1964,
                    "matchedPaperCorpusId": "271963324"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037689208984375
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Our experiments so far have shown that GenRM becomes a favourable choice as the compute budget increases. However, it is important to strike a careful balance between the number of solutions and verifications to achieve optimal performance at a given budget. This raises the question: What is the optimal way to allocate a given compute budget between generating solutions and verifying them? To address this, we derive inference scaling laws, which describe how the optimal number of solutions and verifications scales with compute budget. \n\nWe sample up to 128 solutions per problem and up to 128 verifications per solution for the MATH test split using Llama-3.1-8B-Instruct and GenRM-FT. Then, we compute the performance at various values of  and  (Figure 6a) and identify the optimal number of solutions and verifications for a given budget. Subsequently, we study how the optimal number of solutions and verifications must be scaled as the budget is increased by fitting power law curves. Our findings in Figure 6b show that the optimal number of solutions scales as  opt \u221d  0.57 while the optimal number of verifications scales as  opt \u221d  0.39 . The larger exponent associated with  opt indicates that while both solutions and verifications should be scaled in tandem, solutions should be scaled at a faster rate for compute-optimal performance. Further experiments in Appendix G show that this finding holds across models. Optimal #Verifications (V opt ) (b) Optimal Num. Verifications at Budget V opt at given budget Power law: V opt C 0.39 (b) The optimal number of (Left) solutions and (Right) verifications for a given compute budget. Every point corresponds to a compute budget. The plots show that as the budget scales, the optimal number of solutions and verifications follows a power law, with the number of solutions increasing more rapidly.",
            "score": 0.4833727491643947,
            "section_title": "Inference Scaling Laws for Generative Reward Models",
            "char_start_offset": 24827,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 540
                },
                {
                    "start": 543,
                    "end": 691
                },
                {
                    "start": 692,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1430
                },
                {
                    "start": 1431,
                    "end": 1479
                },
                {
                    "start": 1480,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1691
                },
                {
                    "start": 1692,
                    "end": 1858
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046630859375
        },
        {
            "corpus_id": "276575732",
            "title": "Forecasting Frontier Language Model Agent Capabilities",
            "text": "The amount of compute used to train the model measured in FLOP. Previous literature has found an approximately log-linear relationship between input compute and model performance (e.g. Finnveden, 2020; Owen, 2024). Sevilla et al. (2022) observes that the amount of compute utilized by large scale pre-training runs tends to double approximately once every 9 months, providing us with a broad reference class for the compute requirements of frontier model training. \n\nIn this paper, we always use scaled FLOP as described in Owen (2024). It is a common practice to \"overtrain\" models (Shafkat, 2023;Dubey et al., 2024) by reducing parameter count and increasing dataset size beyond what would be optimal under Hoffman scaling laws (Hoffmann et al., 2022) to reduce cost at inference time. Therefore, raw (unscaled) FLOP estimates are less comparable. We can overcome this by normalizing all models to the lowest possible FLOP count that would achieve the same loss using Hoffman scaling laws. See Appendix A for details. \n\nNote that we only focus on pre-training FLOP and do not take into account post-training such as RLHF/RLAIF (Ouyang et al., 2022;Bai et al., 2022), nor inference time compute. The \"inference compute\" paradigm started by OpenAI's o1 (OpenAI, 2024a) is not accounted for in our methodology. Column 1 of Figure 3 depicts the relationship between scaled log-FLOP and intermediate capability metrics.",
            "score": 0.482017458737413,
            "section_title": "Scaled training log-FLOP:",
            "char_start_offset": 4746,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 64,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 214
                },
                {
                    "start": 215,
                    "end": 464
                },
                {
                    "start": 467,
                    "end": 536
                },
                {
                    "start": 537,
                    "end": 787
                },
                {
                    "start": 788,
                    "end": 849
                },
                {
                    "start": 850,
                    "end": 991
                },
                {
                    "start": 992,
                    "end": 1019
                },
                {
                    "start": 1022,
                    "end": 1196
                },
                {
                    "start": 1197,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1416
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 236,
                    "matchedPaperCorpusId": "246822642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1007080078125
        },
        {
            "corpus_id": "271212835",
            "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
            "text": "The scaling law can also be transformed into a form that is dependent on the activated parameters N a , which reflects the effective compute (i.e., FLOPs) of the model during inference: \n\nwhere N a is the number of activated parameters in the model, which is equal to N \u00d7 (1 \u2212 S). Since A(S) is an increasing function and (1 \u2212 S) \u03b1 is a decreasing function, there exists a sparsity ratio S * > 0 that minimizes the loss of the sparsely-activated models. This leads to the inference-optimal scaling law of the sparsely-activated models: \n\nIt shows that the performance of the sparsely-activated models is better than the dense baselines with the same inference compute budget. We further solve the optimal sparsity ratio S * , finding that S * \u2248 45.58%. It means that a sparsely-activated model with a sparsity ratio of 45.58% (or 1.84N a parameters) can achieve the best performance with the same inference budget N a . We follow the same process to estimate the inference-optimal scaling law for 1.58-bit Q-Sparse models. We find that the optimal sparsity ratio is 61.25% (or 2.58N a parameters). Figure 4 shows the inference-optimal scaling curves of the sparsely-activated models with full-precision and 1.58-bit weight. It shows that with the same performance, the sparsely-activated models can achieve a significant reduction in the number of activated parameters or FLOPs during inference. \n\nThe inference-optimal scaling law shows that the performance of the sparsely-activated models can be optimized by adjusting the sparsity ratio S. It can be used to guide the training of the sparsely-activated models and to optimize the performance of the models during inference.",
            "score": 0.4819322910814072,
            "section_title": "Inference-Optimal Scaling Law",
            "char_start_offset": 14220,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 185
                },
                {
                    "start": 188,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 453
                },
                {
                    "start": 454,
                    "end": 535
                },
                {
                    "start": 538,
                    "end": 675
                },
                {
                    "start": 676,
                    "end": 752
                },
                {
                    "start": 753,
                    "end": 919
                },
                {
                    "start": 920,
                    "end": 1022
                },
                {
                    "start": 1023,
                    "end": 1097
                },
                {
                    "start": 1098,
                    "end": 1223
                },
                {
                    "start": 1224,
                    "end": 1395
                },
                {
                    "start": 1398,
                    "end": 1677
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06890869140625
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "We will now show the existence of a compute-optimal ratio between sequential and parallel sampling, and understand their relative pros and cons based on difficulty of a given prompt. \n\nTrading off sequential and parallel test-time compute. To understand how to optimally allocate sequential and parallel compute, we perform a sweep over a number of different ratios. We see, in Figure 7 (left), that indeed, at a given generation budget, there exists an ideal sequential to parallel ratio, that line represents a fixed generation budget as the ratio is changed. We use the verifier for answer selection. We see that while increased sequential revisions tends to outperform more parallel compute, at higher generation budgets there is an ideal ratio that strikes a balance between the two extremes. Right: Varying the sequential to parallel ratio for a generation budget of 128 across difficulty bins. Using verifier-based selection, we see that the easier questions attain the best performance with full sequential compute. On the harder questions, there is an ideal ratio of sequential to parallel test-time compute. \n\nachieves the maximum accuracy. We also see in Figure 7 (right) that the ideal ratio of sequential to parallel varies depending on a given question's difficulty. In particular, easy questions benefit more from sequential revisions, whereas on difficult questions it is optimal to strike a balance between sequential and parallel computation. This finding supports the hypothesis that sequential revisions (i.e., varying the proposal distribution) and parallel sampling (i.e., search with verifiers) are two complementary axes for scaling up test-time compute, which may be more effective on a per-prompt basis. We include examples of our model's generations in Appendix L. Additional results are shown in Appendix B. to question difficulty, we find that we can outperform best-of-N using up to 4x less test-time compute (e.g. 64 samples verses 256). \"Compute-optimal oracle\" refers to using the oracle difficulty bins derived from the ground truth correctness information, and \"compute optimal predicted\" refers to using the PRM's predictions to produce model-predicted difficulty bins. \n\nCompute-optimal revisions. Given that the efficacy of sequential and parallel sampling depends on question difficulty, we can select the ideal ratio of sequential to parallel compute per difficulty bin.",
            "score": 0.4818571224306224,
            "section_title": "\u221a \ud835\udc41).",
            "char_start_offset": 38012,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 182
                },
                {
                    "start": 185,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 366
                },
                {
                    "start": 367,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 900
                },
                {
                    "start": 901,
                    "end": 1023
                },
                {
                    "start": 1024,
                    "end": 1117
                },
                {
                    "start": 1120,
                    "end": 1150
                },
                {
                    "start": 1151,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1460
                },
                {
                    "start": 1461,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 1968
                },
                {
                    "start": 1969,
                    "end": 2205
                },
                {
                    "start": 2208,
                    "end": 2234
                },
                {
                    "start": 2235,
                    "end": 2410
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0304412841796875
        },
        {
            "corpus_id": "268691910",
            "title": "Mechanistic Design and Scaling of Hybrid Architectures",
            "text": "To investigate the link between MAD synthetics and real-world scaling, we execute the largest scaling law analysis on emerging architectures to date, training over 500 language models between 70 million and 7 billion parameters with different architectures. Our protocol builds and expands on compute-optimal scaling laws for LSTMs and Transformers [1,22,2]. Our findings show that hybrid architectures improve on all scaling measures, resulting in lower pretraining losses at different floating point operation (FLOP) compute-budgets at the compute-optimal frontier 1 . We also verify new architectures to be more robust to large pretraining runs outside the efficient frontier e.g., smaller models trained for significantly more tokens, which make up a majority of training settings in practice due to inference cost considerations [23]. \n\nHybridization insights at scale Building on our scaling law analysis, we investigate hybridization schedules and model topology. Our findings uncover optimal hybridization ratios for attention [6], Hyena [10], and Mamba [12] mixtures, as well as the respective placement of these layers in an architecture. \n\nState-optimal scaling laws The size of the state -the analog of kv-caches in standard Transformers [24] of emerging convolutional and recurrent primitives [10,12] plays a central role in MAD and our scaling analysis, as it determines inference efficiency, memory cost, and provably has a direct effect on recall capabilities [17]. We introduce a state-optimal scaling analysis, with the objective of estimating how perplexity scales with the state dimension of different model architectures. We find hybrid architectures to balance the trade-off between compute requirements, state dimension, and perplexity. \n\nNew state-of-the-art architectures Leveraging MAD and new computational primitives, derived from the insights developed in this work, we design new state-of-the-art hybrid architectures, outperforming the best Transformer, convolutional, and recurrent baselines (Transformer++ [4], Hyena, Mamba) with a reduction of up to 20% in perplexity for the same compute budget.",
            "score": 0.4814986839630927,
            "section_title": "Scaling laws of emerging architectures",
            "char_start_offset": 4487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 358
                },
                {
                    "start": 359,
                    "end": 570
                },
                {
                    "start": 571,
                    "end": 839
                },
                {
                    "start": 842,
                    "end": 970
                },
                {
                    "start": 971,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1759
                },
                {
                    "start": 1762,
                    "end": 2130
                }
            ],
            "ref_mentions": [
                {
                    "start": 1035,
                    "end": 1038,
                    "matchedPaperCorpusId": "13756489"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2015380859375
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.",
            "score": 0.48092126742244656,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.311767578125
        },
        {
            "corpus_id": "257985427",
            "title": "Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster",
            "text": "For perspective, the run-to-run standard deviation in loss when using different initialization and data random seeds is around 0.35%. In addition to its pre-training advantages, \u00b5P also improves downstream capabilities of these models. In the previous Figure 4, we plotted downstream results for \u00b5P models, where we see improved accuracy and distinctively smoother scaling than SP models. Table 3 also lists these zero-shot downstream results for SP and \u00b5P models. In particular, \u00b5P models show a 1.7% relative improvement in downstream tasks on average. These results are robust across model scales besides the 2.7B parameter model. We believe that we were just lucky when choosing the SP 2.7B model hyperparameters such that it performs significantly better than the SP Pile scaling law. Despite the SP model's upstream advantage, however, the 2.7B + \u00b5P model still performs as well on downstream tasks on average. \n\ninference time, the compute cost is proportional to the model's size and number of inferences. Thus, smaller models will have an overall inference cost advantage proportional to their size. \n\nWe propose a technique to identify training+inference compute-optimal frontiers that practitioners can use to estimate how they should pre-train their models when considering inference deployment costs. Specifically, we define a compute cost metric equal to pre-training FLOPs added to the model's inference cost and the expected number of inference tokens. Here, F is the total compute cost, f represents FLOPs costs for full pre-training and per-token inference, n infer_tokens is the number of expected inference tokens for the given model, and p is the model's parameter count6 : With this formulation, we can estimate the number of model inferences before the total compute budget matches models trained on fewer or more tokens. Figure 6 plots a comparison of total pre-train + inference compute cost for Cerebras-GPT, GPT-J, GPT-NeoX, and Pythia models assuming either 20B, 200B, or 2T inference tokens. These results show that most Cerebras-GPT models would provide better Pile test-lossper-compute-FLOP than Pythia models until all models reach roughly 200B inference tokens.",
            "score": 0.48072215523481504,
            "section_title": "Maximal Update Parameterization (\u00b5P) and \u00b5Transfer",
            "char_start_offset": 19388,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 133
                },
                {
                    "start": 134,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 464
                },
                {
                    "start": 465,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 633
                },
                {
                    "start": 634,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 916
                },
                {
                    "start": 919,
                    "end": 1013
                },
                {
                    "start": 1014,
                    "end": 1108
                },
                {
                    "start": 1111,
                    "end": 1313
                },
                {
                    "start": 1314,
                    "end": 1468
                },
                {
                    "start": 1469,
                    "end": 1844
                },
                {
                    "start": 1845,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2194
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0276336669921875
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03. \n\n(13) \n\nThese findings indicate that hybrid approaches combining model size optimization with test-time computation strategies may offer the best trade-off for organizations deploying language models. Practitioners should assess the complexity of their target tasks and consider adaptive computation strategies that adjust inference resources based on problem difficulty. This approach can lead to significant cost savings while maintaining performance, particularly for routine tasks that do not require the full capacity of larger models. \n\nRQ5. How does scaling fine-tuning parameters affect performance on downstream tasks? Hernandez et al. (2021) proposed scaling laws for transfer by fine-tuning decoder-only transformer models on python code. They introduced a concept of effective data transferred D t , i.e., the amount of additional python data that a model of the same size trained on only python would have needed to achieve the same loss on python as a model pretrained on language, as a function of fine tuning data D f . The law is given as : \n\nAlong the same tracks, Lin et al. (2024a) proposed a rectified scaling law given by Equation 15. They introduced the term pre-learned data size D l that indicates how much amount of downstream data a model has learned from pre-training: \n\nwhere D is the fine-tuning data size and B, E, \u03b2 are fitting parameters. Abnar et al. (2021) predicted, downstream error e DS for image recognitions tasks on ViTs and ResNets as a function of upstream error e U S , given by the equation: \n\nThis was further explored by Mikami et al. (2021), modelling pre-training data size, consisting of syntheic dataset to predict downstream error with following equation: \n\nwhere D f denotes fine-tuning data, X denotes either LLM model size or pretraining data size or PEFT parameter size, and A, E, \u03b1, \u03b2 are fitting parameters. \n\nRQ5.",
            "score": 0.4804321405380644,
            "section_title": "D Additional research questions and guidelines",
            "char_start_offset": 41734,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 259
                },
                {
                    "start": 260,
                    "end": 444
                },
                {
                    "start": 447,
                    "end": 451
                },
                {
                    "start": 454,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 817
                },
                {
                    "start": 818,
                    "end": 986
                },
                {
                    "start": 989,
                    "end": 993
                },
                {
                    "start": 994,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1195
                },
                {
                    "start": 1196,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1503
                },
                {
                    "start": 1506,
                    "end": 1602
                },
                {
                    "start": 1603,
                    "end": 1742
                },
                {
                    "start": 1745,
                    "end": 1817
                },
                {
                    "start": 1818,
                    "end": 1982
                },
                {
                    "start": 1985,
                    "end": 2153
                },
                {
                    "start": 2156,
                    "end": 2311
                },
                {
                    "start": 2314,
                    "end": 2318
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.087890625
        },
        {
            "corpus_id": "273185794",
            "title": "Inference Scaling for Long-Context Retrieval Augmented Generation",
            "text": "In this paper, we explore inference scaling in long-context RAG. By systematically studying the performance with different inference configurations, we demonstrate that RAG performance improves almost linearly with the increasing order of magnitude of the test-time compute under optimal inference parameters. Based on our observations, we derive inference scaling laws for RAG and the corresponding computation allocation model, designed to predict RAG performance on varying hyperparameters. Through extensive experiments, we show that optimal configurations can be accurately estimated and align closely with the experimental results. These insights provide a strong foundation for future research in optimizing inference strategies for long-context RAG.",
            "score": 0.48040901750432174,
            "section_title": "Conclusion",
            "char_start_offset": 35000,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 64
                },
                {
                    "start": 65,
                    "end": 309
                },
                {
                    "start": 310,
                    "end": 493
                },
                {
                    "start": 494,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 757
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06817626953125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.",
            "score": 0.4783077967728977,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.316162109375
        },
        {
            "corpus_id": "272525306",
            "title": "Unified Neural Network Scaling Laws and Scale-time Equivalence",
            "text": "Progress in artificial intelligence (AI) has relied heavily on the dramatic growth in the size of models and datasets. An active area of research focuses on understanding how test error decreases with increases in model and data size. This work has led to the development of scaling laws which posit that test error decreases as a power law with both. However, several theoretical aspects remain unclear. One significant gap is understanding how test error and the existing scaling laws change as the training time is varied (Kaplan et al., 2020;Bahri et al., 2021;Rosenfeld et al., 2020;Sharma & Kaplan, 2022). \n\nThe practical relevance of this question is clear: under a fixed compute budget, what is the optimal balance between scaling the model size and dataset volume, and what is the right amount of training for a given data volume? This is particularly relevant in the context of large language models (LLMs), which are often trained for a single epoch, raising questions about the potential efficacy of training smaller models for longer (more epochs). Furthermore, current scaling laws do not account for other well-known phenomena in learning, such as double descent (Belkin et al., 2019), in which model performance exhibits non-monotonic changes with respect to training data volume, model size, and training time. In particular, double descent theory predicts that test error should increase rapidly at the interpolation threshold, the point at which the model interpolates the training set (Nakkiran et al., 2021;Advani & Saxe, 2017). Like scaling laws, current theories of double descent leave several empirical phenomena unexplained: past explanations of double descent require it to occur, but empirically double descent is often not observed; it is unclear whether the interpolation threshold should grow or shrink with model size; prior theory does not explain why models in the infinite-parameter limit sometimes perform worse than their finite-parameter counterparts. \n\nWe seek the simplest possible unified framework in which to understand learning with respect to model size, data volume, and training time. In doing so, we aim to capture the essential scaling properties of learning in deep neural networks.",
            "score": 0.47802213544632394,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 118
                },
                {
                    "start": 119,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 351
                },
                {
                    "start": 352,
                    "end": 404
                },
                {
                    "start": 405,
                    "end": 611
                },
                {
                    "start": 614,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 1061
                },
                {
                    "start": 1062,
                    "end": 1327
                },
                {
                    "start": 1328,
                    "end": 1549
                },
                {
                    "start": 1550,
                    "end": 1989
                },
                {
                    "start": 1992,
                    "end": 2131
                },
                {
                    "start": 2132,
                    "end": 2232
                }
            ],
            "ref_mentions": [
                {
                    "start": 588,
                    "end": 610,
                    "matchedPaperCorpusId": "246559072"
                },
                {
                    "start": 1505,
                    "end": 1528,
                    "matchedPaperCorpusId": "207808916"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1475830078125
        },
        {
            "corpus_id": "273695243",
            "title": "Scaling LLM Inference with Optimized Sample Compute Allocation",
            "text": "Inference Time Algorithms. Following the taxonomy of Welleck et al. (2024) on inference-time algorithms, chained meta-generators run multiple LLM calls sequentially and use the output sample from each call as the input to the next one (Dohan et al., 2022;Schlag et al., 2023). Parallel meta-generators samples multiple candidates for a problem and selects the best candidate (Wang et al., 2023c;Chen et al., 2022;Jiang et al., 2023;Zhang et al., 2023;Huang et al., 2023). \n\nStep-level search methods regards problem-solving as a multistep process and sample candidate next steps at each intermediate state, using algorithms like tree search (Yao et al., 2024), graph search, and Monte-Carlo Tree Search (Lample et al., 2022;Tian et al., 2024;Chi et al., 2024). Refinement-based methods samples candidate solutions sequentially, relying on some feedback to revise the next candidate (Madaan et al., 2024;Shinn et al., 2024). Although these algorithms scale up inference differently, they all need LLM sampling as a basic operation. \n\nScaling Inference. Many studies investigate how scaling in inference affects LLM performance. Al-phaCode (Li et al., 2022;Leblond et al., 2023) scales up the sample number and finds the solve rates scale log-linearly with more samples. Brown et al. (2024) improves LLMs' performance on math problems by repetitively sampling candidate solutions with a high temperature. Wu et al. (2024) and Snell et al. (2024) study the scaling behaviors of various inference time algorithms, reward models, and the model sizes. In this paper, we investigate the allocation algorithm between various inference configurations for scaling up inference. \n\nInference Compute Optimization. There are two typical ways to optimize inference compute. One is to search for a single optimal configuration.",
            "score": 0.47786674461753215,
            "section_title": "Related Work",
            "char_start_offset": 3633,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 26
                },
                {
                    "start": 27,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 760
                },
                {
                    "start": 761,
                    "end": 923
                },
                {
                    "start": 924,
                    "end": 1030
                },
                {
                    "start": 1033,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1402
                },
                {
                    "start": 1403,
                    "end": 1545
                },
                {
                    "start": 1546,
                    "end": 1667
                },
                {
                    "start": 1670,
                    "end": 1701
                },
                {
                    "start": 1702,
                    "end": 1759
                },
                {
                    "start": 1760,
                    "end": 1812
                }
            ],
            "ref_mentions": [
                {
                    "start": 375,
                    "end": 395,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 413,
                    "end": 432,
                    "matchedPaperCorpusId": "259075564"
                },
                {
                    "start": 432,
                    "end": 451,
                    "matchedPaperCorpusId": "258865731"
                },
                {
                    "start": 641,
                    "end": 659,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 703,
                    "end": 724,
                    "matchedPaperCorpusId": "248986689"
                },
                {
                    "start": 882,
                    "end": 903,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 903,
                    "end": 922,
                    "matchedPaperCorpusId": "258833055"
                },
                {
                    "start": 1138,
                    "end": 1155,
                    "matchedPaperCorpusId": "246527904"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06622314453125
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this. In this work, we devise\"precision-aware\"scaling laws for both training and inference. We propose that training in lower precision reduces the model's\"effective parameter count,\"allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal. We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on model sizes up to 1.7B parameters trained on up to 26B tokens.",
            "score": 0.47779504607612955,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08660888671875
        },
        {
            "corpus_id": "269009975",
            "title": "MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies",
            "text": "Then we have the optimal model size N opt , dataset size D opt , given a fixed amount of compute C = 6ND (Rae et al., 2021) as: 6N in Equation 2, and minimize L(N) given C. A similar way is adopted for D opt .From Equation 3, when \u03b1 = \u03b2, N opt /D opt is a constant, supporting Hoffmann et al. ( 2022)'s claim, and when \u03b1 < \u03b2, we should emphasize more on parameter scaling (Kaplan et al., 2020), and vise versa.\n\nIn our experiments, the fitted relationship between loss and N, D is shown in the contour plot of equal loss in Figure 10.The equation of fitted scaling law is shown in the first text box in each subplot.We can see that in all evaluation corpora, we have \u03b2 < \u03b1.More specifically, on average, we have \u03b1 = 0.29, \u03b2 = 0.23, K 2 = 0.01, \u03b7 = \u22120.10.Since \u03b1 is slightly larger than \u03b2, this result shows that as the computation scale, we should slightly emphasize more on data scaling than model scaling, which aligns with Hoffmann et al. (2022).\n\nAs for the concrete data-to-model ratio\n\nN opt , we notice that there is a huge gap in compute optimal regime between ours and Hoffmann et al. (2022) despite that the trend of D opt N opt ' with compute C is aligned between ours and theirs.Specifically, the data size should be 192 times larger than the model size on average, as opposed to 20 times in Hoffmann et al. (2022).We note that this aligns with the observation in Section 4.3 and Figure 6.\n\nWith respect to the large deviation from Chinchilla Optimal N opt D opt , we notice that their scaling experiment was conducted in a not very recent configuration.To compare with more recent configuration such as Llama2 (Touvron et al., 2023), we extract the training loss data from Llama2 paper (left part) in Appendix Figure 18 and estimate the compute optimal D opt N opt in their paper using the right part of Figure 18.",
            "score": 0.47744353183519617,
            "section_title": "Measuring the Scaling Law with WSD LRS",
            "char_start_offset": 13043,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 209
                },
                {
                    "start": 209,
                    "end": 410
                },
                {
                    "start": 412,
                    "end": 534
                },
                {
                    "start": 534,
                    "end": 616
                },
                {
                    "start": 616,
                    "end": 673
                },
                {
                    "start": 673,
                    "end": 754
                },
                {
                    "start": 754,
                    "end": 949
                },
                {
                    "start": 951,
                    "end": 990
                },
                {
                    "start": 992,
                    "end": 1191
                },
                {
                    "start": 1191,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1401
                },
                {
                    "start": 1403,
                    "end": 1566
                },
                {
                    "start": 1566,
                    "end": 1827
                }
            ],
            "ref_mentions": [
                {
                    "start": 926,
                    "end": 948,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1078,
                    "end": 1100,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1304,
                    "end": 1326,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06866455078125
        },
        {
            "corpus_id": "276574828",
            "title": "Learning to Reason from Feedback at Test-Time",
            "text": "Table 2 compares FTTT with various baselines across four reasoning datasets. FTTT, both with and without self-reflected feedback, outperforms conventional test-time scaling methods on average. This success is partially explained by the findings of Ye et al. (2024a), which show that training with error-correction data enhances reasoning capabilities and models do not retry during inference. FTTT is also efficient. For instance, the inference time of Llama-3.1-8B-Instruct on GSM8K with a budget of 32 is 3 GPU hours for the best parallel sampling method (Best-of-N) and 20 GPU hours for the best sequential revision method (Self-Refine). In contrast, FTTT achieves inference times of approximately 3 GPU hours without self-reflected feedback and 4 GPU hours with selfreflected feedback. \n\nNotably, self-reflected feedback does not always improve results. Its effectiveness appears to depend on the LLM's self-reflection ability. To test this, we computed the Spearman rank correlation between FTTT and Self-Refine, a self-reflection-based algorithm. The Spearman coefficient (r = 0.8656, p \u2264 0.05) indicates a strong positive correlation, supporting our hypothesis. We also observe that Self-Consistency performs poorly on code tasks because sampled code snippets rarely match exactly, making majority voting akin to random selection. \n\nFigure 3 illustrates performance for FTTT and baselines under varying budgets. FTTT consistently outperforms baselines, with greater gains under constrained budgets. In contrast, Revision and Self-Consistency do not scale well. Revision struggles with long-context reasoning due to length generalization issues (Li et al., 2024), while Self-Consistency fails to leverage feedback, often discarding correct answers during majority voting due to long-tailed distributions of correct answers (Brown et al., 2024).",
            "score": 0.47715819045834906,
            "section_title": "Training-Free Results",
            "char_start_offset": 13435,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 76
                },
                {
                    "start": 77,
                    "end": 192
                },
                {
                    "start": 193,
                    "end": 392
                },
                {
                    "start": 393,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 789
                },
                {
                    "start": 792,
                    "end": 857
                },
                {
                    "start": 858,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1337
                },
                {
                    "start": 1340,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1505
                },
                {
                    "start": 1506,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1850
                }
            ],
            "ref_mentions": [
                {
                    "start": 1651,
                    "end": 1668,
                    "matchedPaperCorpusId": "271719726"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01132965087890625
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Our experiments are centered around two main questions: \n\n\u2022 Compute-optimal model size: How does performance scale as inference-time compute is increased with a fixed inference strategy, but with varying model size? \n\n\u2022 Compute-optimal inference strategy: How does performance scale as inference-time compute is increased with various inference strategies (and various model sizes)? \n\nWe detail our experimental setup below.",
            "score": 0.4757768092357937,
            "section_title": "EXPERIMENTS",
            "char_start_offset": 19650,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 55
                },
                {
                    "start": 58,
                    "end": 215
                },
                {
                    "start": 218,
                    "end": 382
                },
                {
                    "start": 385,
                    "end": 424
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1187744140625
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "With learning rate warmup corrected, we turn to study learning rate decay, which Hoffmann et al. [25] conjecture to be a main cause of the difference between their result and Kaplan et al. [30]. We  The optimal number of tokens D \u22c6 as a function of the compute budget C. Left: Using the warmup period of Kaplan et al. [30], smaller models reach compute-optimality during warmup. \n\nRight: Setting the number of warmup tokens to be identical to the model size (visualized using the power law fit) ensures models reach compute-optimality well after the warmup and yields a scaling law closer to Hoffmann et al.. We replicate these plots for all of our experiments in Appendix E. \n\nobserve that the long 131B tokens decay period in Kaplan et al. [30], which is aimed toward training to full convergence, means that their compute-constrained experiments see virtually no learning rate decay: Figure 2 shows that, at our compute scales, it is never optimal to train for more than 10B, which corresponds to less than 1.5% decay with a cosine schedule. \n\nTo correct this, we follow the second approach of Hoffmann et al. [25] and choose the learning rate schedule for every model and FLOP budget individually. For each FLOP value in our grid, we pick the 7 models from Table 2 which yield token-to-parameter ratios in the range 1 to 100, and train them with a cosine learning rate schedule that decays to 1% of the maximum learning rate when reaching the target FLOP value. 5 This is roughly twice as expensive as previous experiments, which required only a single training run for each model size (see additional discussion in Section 4.2). As Figure 1d shows, adding cosine decay results in a slightly cleaner linear trend (R 2 improves from 0.993 to 0.998) and an exponent slightly closer to the Hoffmann et al. scaling law (0.57 instead of 0.6), but most of the gap remains. Therefore, even with the FLOP count and warmup issues corrected, adding learning rate decay is not sufficient to reproduce the Hoffmann et al. scaling law.",
            "score": 0.4748607033364189,
            "section_title": "Learning rate decay has limited impact on compute-optimal allocation",
            "char_start_offset": 12342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 378
                },
                {
                    "start": 381,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 675
                },
                {
                    "start": 678,
                    "end": 1044
                },
                {
                    "start": 1047,
                    "end": 1201
                },
                {
                    "start": 1202,
                    "end": 1467
                },
                {
                    "start": 1468,
                    "end": 1633
                },
                {
                    "start": 1634,
                    "end": 1824
                },
                {
                    "start": 1825,
                    "end": 1870
                },
                {
                    "start": 1871,
                    "end": 2026
                }
            ],
            "ref_mentions": [
                {
                    "start": 97,
                    "end": 101,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1113,
                    "end": 1117,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0582275390625
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "Recent work on compute-optimal language models [42] shows that many previously trained large language models (LLMs, which we define as having more than one billion parameters) could have attained better performance for a given compute budget by training a smaller model on more data. Notably, the 70-billion parameter Chinchilla model [42] outperforms the 280-billion parameter Gopher model [89] while using a similar compute budget by being trained on four times more data. Extrapolating these laws for compute allocation (hereafter \"Chinchilla scaling laws\") to a 530 billion parameter model, such as the under-trained MT-NLG model [99], would require training on a massive 11 trillion tokens, corresponding to more than 30 terabytes of text data. For most languages, available data is several orders of magnitude smaller, meaning that LLMs in those languages are already data-constrained. Villalobos et al. [112] estimate that even high-quality English language data will be exhausted by the year 2024 given the Chinchilla scaling laws and the trend of training ever-larger models. This motivates the question [112,81]: what should we do when we run out of data? \n\nIn this work we investigate scaling large language models in a data-constrained regime, and whether training an LLM with multiple epochs of repeated data impacts scaling. Using multiple epochs is, of course, standard in machine learning generally; however, most prior large language models have been trained for a single epoch [51,15] and some work explicitly advocates against reusing data [40]. An exception is the recent Galactica models [108] that were trained for 4.25 epochs and exhibit continually decreasing validation loss and improving downstream performance throughout training. However, the experiments of Galactica do not compare this setup to an alternative non-dataconstrained model trained for one epoch on unique data. Without this comparison, it is difficult to quantify the trade-off between additional compute versus additional data collection. \n\nOur main focus is to quantify the impact of multiple epochs in LLM training such that practitioners can decide how to allocate compute when scaling models. Toward this end, we assembled a battery of empirical training runs of varying data and compute constraints.",
            "score": 0.4742510036403623,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 283
                },
                {
                    "start": 284,
                    "end": 474
                },
                {
                    "start": 475,
                    "end": 749
                },
                {
                    "start": 750,
                    "end": 891
                },
                {
                    "start": 892,
                    "end": 1084
                },
                {
                    "start": 1085,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1564
                },
                {
                    "start": 1565,
                    "end": 1757
                },
                {
                    "start": 1758,
                    "end": 1903
                },
                {
                    "start": 1904,
                    "end": 2032
                },
                {
                    "start": 2035,
                    "end": 2190
                },
                {
                    "start": 2191,
                    "end": 2298
                }
            ],
            "ref_mentions": [
                {
                    "start": 1499,
                    "end": 1502,
                    "matchedPaperCorpusId": "218971783"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043365478515625
        },
        {
            "corpus_id": "276079888",
            "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling",
            "text": "Large Language Models (LLMs) have revolutionized artificial intelligence by demonstrating remarkable capabilities in planning, reasoning, and problem-solving across diverse tasks (Achiam et al., 2023;anthropic, 2024;Team et al., 2024;Touvron et al., 2023). Their success stems not only from \"training scaling\", i.e. their ability to leverage vast datasets and computational resources during training (Kaplan et al., 2020), but also from their ability to benefit from increased compute at test-time to better address more challenging queries -commonly referred to as \"test-time (inference) scaling\" (Snell et al., 2024;Wu et al., 2024). \n\nConventional test-time scaling approaches, such as repeated sampling (Brown et al., 2024), involve generating multiple candidate solutions and selecting the optimal one using techniques like majority voting or task-specific reward models. While these approaches can be effective in certain scenarios, they have notable limitations. The performance improvements from repeated sampling often quickly plateau as test-time compute increases. Moreover, the reliance on task-specific reward models adds significant training overhead, limiting both efficiency and scalability. \n\nTo effectively enable more optimal scaling for test-time compute with a canonical framework, we propose an alternative approach that leverages the inherent self-verification and self-correction capabilities of LLMs. Such strategies had been underexplored, likely due to the limited effectiveness of self-correction in earlier generations of LLMs (Huang et al.). However, recent advancements in LLMs have led to significantly improved self-verification and self-correction abilities. These improvements present an opportunity to rethink test-time scaling by moving beyond repeated sampling and reward model dependency, potentially achieving greater efficiency and generalizability in solving complex tasks. \n\nAlgorithm 1 SETS: Self-Enhanced Test-Time Scaling Require: The query x, the LLM F , the Sampling prompt   , the Self-Verify prompt   , the Self-Correct prompt   , the number of samples , the maximum number of rounds , the judgement function  and the indicator function 1. 1: for  = 1, . . . ,  do 2: \n\n{Sampling Operation} 3: \n\nfor  = 0, . . . ,  \u2212 1 do 4:",
            "score": 0.4737562712054568,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 635
                },
                {
                    "start": 638,
                    "end": 876
                },
                {
                    "start": 877,
                    "end": 969
                },
                {
                    "start": 970,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1207
                },
                {
                    "start": 1210,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1571
                },
                {
                    "start": 1572,
                    "end": 1692
                },
                {
                    "start": 1693,
                    "end": 1915
                },
                {
                    "start": 1918,
                    "end": 2208
                },
                {
                    "start": 2209,
                    "end": 2217
                },
                {
                    "start": 2220,
                    "end": 2243
                },
                {
                    "start": 2246,
                    "end": 2261
                },
                {
                    "start": 2262,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05889892578125
        },
        {
            "corpus_id": "276107088",
            "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods",
            "text": "Inference-time scaling has been a key training-free strategy for enhancing LLM performance. Brown et al. (2024) explores a best-of-N (BoN) decoding strategy, demonstrating improvements in output quality through selective refinement. (Snell et al., 2024) provides insights into how scaling compute resources can yield better inference efficiency from a compute optimality perspective. While not implementing full Monte Carlo tree search (MCTS), Zhou et al. (2024) explores a tree-search-like approach within language models. Additionally, Guan et al. (2025) 1997) or particle filtering (Swendsen & Wang, 1986) has been the classical way to approximate complex posterior distributions over state-space models. Particle Gibbs (PG) sampling (Andrieu et al., 2010) extends these approaches by integrating MCMC techniques for improved inference.",
            "score": 0.4736623960814519,
            "section_title": "Related Work",
            "char_start_offset": 6107,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 91
                },
                {
                    "start": 92,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 383
                },
                {
                    "start": 384,
                    "end": 523
                },
                {
                    "start": 524,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 839
                }
            ],
            "ref_mentions": [
                {
                    "start": 737,
                    "end": 759,
                    "matchedPaperCorpusId": "13962777"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033721923828125
        },
        {
            "corpus_id": "266933290",
            "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models",
            "text": "The quantitative empirical investigation of forgetting in LLMs in terms of scale and length of fine-tuning, is similar in nature to carrying out such a study on pre-training performance. As such, our work shares considerable similarity with works on scaling laws for pre-training of LLMs (Kaplan et al., 2020;Henighan et al., 2020). In these papers, the authors empirically show that, after an initial transient phase in learning, the pre-training test loss L pre follows a (constant shifted, in the case of (Henighan et al., 2020)) power law in the number of non-embedding parameters of the transformer P , and the number of tokens seen in training T . Explicitly, (Kaplan et al., 2020) finds the relationship \n\nfor some constants a pre , b pre , \u03b1, \u03b2. The authors of (Kaplan et al., 2020) then use this scaling law to conclude that larger models are significantly more sample efficient, interpolate what the optimal model size for a given compute budget would be (compute is a function of P, T ), and conclude that optimally compute-efficient training involves training very large models on a smaller amount of data while stopping significantly before convergence. \n\nA follow-up work (Clark et al., 2022) generalizes the scaling laws to mixture of experts LLMs, which use routing to select subnetworks, and show that the loss scales with the number of experts in addition to P, T . \n\nIn (OpenAI, 2023), the authors empirically calculate the power law relationships for the training performance of smaller models in their given training setup, so that they can predict full-size GPT 4 performance by extrapolating to 1000\u00d7 the compute. Similarly, (Anil et al., 2023) used a scaling laws analysis to calculate the optimal parameter size for a given number of FLOPs for the Palm 2 model. \n\nAnother work (Hoffmann et al., 2022) re-examined the particular values in the scaling laws of (Kaplan et al., 2020) while using a learning rate schedule.",
            "score": 0.47321414429783626,
            "section_title": "Scaling Laws for Training LLMs",
            "char_start_offset": 6808,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 186
                },
                {
                    "start": 187,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 710
                },
                {
                    "start": 713,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 1166
                },
                {
                    "start": 1169,
                    "end": 1383
                },
                {
                    "start": 1386,
                    "end": 1636
                },
                {
                    "start": 1637,
                    "end": 1786
                },
                {
                    "start": 1789,
                    "end": 1942
                }
            ],
            "ref_mentions": [
                {
                    "start": 1186,
                    "end": 1206,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.127197265625
        },
        {
            "corpus_id": "268875826",
            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
            "text": "Optimal inference cost for a bounded loss.Based on the scaling law, the loss L is monotonic to model size N before the loss-optimal point.Besides, the inference cost I is also monotonic to N. As a result, to minimize inference cost I, the model size N should be as low as possible, meaning the loss is as large as possible.As a result, I min E \u2032 corresponds to the case when the loss is exactly L opt E .\n\nBased on the above analysis, we do dichotomy search for equation L E \u2032 (N, B) = L opt E to find the solution N E \u2032 , and use it to compute I min E \u2032 (the detail is in Algorithm 1). Figure 5 (left) shows the result for E = 1 (dense Transformer) and 4 (4-expert MoE).To reach the model performance (validation loss) similar to that of the dense model, over-training an 8-expert MoE with the same training budget has the lowest inference cost, which is 31.6%-38.1% as large as that of the dense model when B ranges from 5.15e21 to 8.18e21.When using 4-expert loss-optimal MoE's quality as a standard, 8-expert over-trained MoE saves 49.0%-52.3%inference cost per token, and 16-expert over-trained MoE saves 48%-53% inference cost.MoE with more experts has a higher cost than 8-or 16-expert.We reason this as that 4-expert MoE's optimal loss is too far away from 32-or-more expert MoE's, making the over-training no longer appealing as it already leaves the \"flat area\" in the size-loss curve.Figure 5 (right) further shows how much smaller than the loss-optimal model is trained.",
            "score": 0.47228275784761,
            "section_title": "loss-optimal",
            "char_start_offset": 18501,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 42
                },
                {
                    "start": 42,
                    "end": 138
                },
                {
                    "start": 138,
                    "end": 323
                },
                {
                    "start": 323,
                    "end": 404
                },
                {
                    "start": 406,
                    "end": 671
                },
                {
                    "start": 671,
                    "end": 942
                },
                {
                    "start": 942,
                    "end": 1047
                },
                {
                    "start": 1047,
                    "end": 1133
                },
                {
                    "start": 1133,
                    "end": 1193
                },
                {
                    "start": 1193,
                    "end": 1395
                },
                {
                    "start": 1395,
                    "end": 1482
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0087127685546875
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "Figure 3: ( \u00a77) We replicate the plots in Figure 2, reorganized so that analyses for datasets with mid-training checkpoints appear alongside those for data with final checkpoints only. This side-byside comparison makes the difference in power law fits apparent, further underscoring the impact of including mid-training datapoints. We study the effects of various decisions in the fitting of a power law, as outlined in our checklist (Appendix B) and detailed in \u00a73- \u00a76. For each set of analyses, we the scaling laws found by (Kaplan et al., 2020) and (Hoffmann et al., 2022) for comparison. We also include markers indicating 3 existing models for comparison purposes: Llama 3 405B (Dubey et al., 2024), the Chinchilla model (Hoffmann et al., 2022), and an estimate of the 1.5B GPT-2 model (Radford et al., 2019), for which we know details of the dataset storage size and word count, but not an exact count of data BPE tokens, which we estimate at 100B. We additionally annotate, at the compute budget C for each of these 3 reference points, the maximum and minimum predicted (i.e. extrapolated) optimal model parameter count N opt and data budget D opt from the fitted power laws. We use a thicker, solid line for the method in each plot which achieves the lowest optimization loss, with the exception of the plots comparing power law form, those comparing loss functions and those comparing optimizers, for which this would be nonsensical. We find overall, throughout our analyses, that all of the decisions we explore have an impact on the final fit of the power law, supporting our conclusion that more thorough reporting of these decisions is critical for scaling law reproducibility.",
            "score": 0.472210097115404,
            "section_title": "E FULL ANALYSIS PLOTS",
            "char_start_offset": 57127,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 591
                },
                {
                    "start": 592,
                    "end": 954
                },
                {
                    "start": 955,
                    "end": 1082
                },
                {
                    "start": 1083,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1442
                },
                {
                    "start": 1443,
                    "end": 1690
                }
            ],
            "ref_mentions": [
                {
                    "start": 791,
                    "end": 813,
                    "matchedPaperCorpusId": "160025533"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.025421142578125
        },
        {
            "corpus_id": "270379481",
            "title": "Are Protein Language Models Compute Optimal?",
            "text": "In this section we aim to solve the questions posed at the beginning of the manuscript through the proposed methods.\n\n3.1.What is the optimal ratio between the number of tokens and model parameters given a fixed compute budget?\n\nWe sought to derive power laws that describe the relationship between the computational budget, measured in FLOPs, and the optimal model size and training tokens for a range of model configurations.With this purpose, we systematically vary the number of training steps across different model parameter sizes for a later fitting of power laws to the resulting training loss curves and a final fitting of the parametric loss with the data obtained.\n\nFinding a power-law relationship between the parameter count, the number of tokens and computing budgets.By fitting power laws to the data of the most efficient models, we derived the relationships N opt \u221d C 0.27 and D opt \u221d C 0.71 .These relationships suggest that the optimal model size scales sublinearly with compute budget, while the optimal number of training tokens scales superlinearly, indicating that encoder-only, protein language models exhibit diminishing returns in training loss improvements with increasing model size.\n\nIntrigued by this findings we switched approaches to fit a parametric loss in order to obtain a more in depth explanation of the scaling parameters.   1 and 2), that the learning dynamics of all models for all token sets are lower bounded by the same loss value (around 2.45 ).\n\n3.2.Finding the optimal tradeoff between parameters and model size to efficiently reach the observed plateau.\n\nIn order to avoid any forced dynamics in the convergence of the losses upon training, we re-fitted a subset of the models with the different token splits used in the previous section 6 .We speculated that the learning rate decay upon tokens seen in training could interfere with standard training processes in pLMs.We trained and evaluated models with the following configurations: 5M, 15M, 35M, 50M, 100M, and 150M parameters, covering a broad spectrum of model complexities.With this configurations, we observed that all of the fitted models reach the same plateau at the same loss value, regardless of the learning rate decay strategy.The realization that all training configurations reached a common plateau led us to question whether there was an alternative approach to optimize training for pre-trained language models (pLMs).",
            "score": 0.4720578801909885,
            "section_title": "Experiments and Discussion",
            "char_start_offset": 4985,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 118,
                    "end": 122
                },
                {
                    "start": 122,
                    "end": 227
                },
                {
                    "start": 229,
                    "end": 427
                },
                {
                    "start": 427,
                    "end": 675
                },
                {
                    "start": 677,
                    "end": 782
                },
                {
                    "start": 782,
                    "end": 910
                },
                {
                    "start": 910,
                    "end": 1211
                },
                {
                    "start": 1213,
                    "end": 1490
                },
                {
                    "start": 1492,
                    "end": 1496
                },
                {
                    "start": 1496,
                    "end": 1601
                },
                {
                    "start": 1603,
                    "end": 1789
                },
                {
                    "start": 1789,
                    "end": 1918
                },
                {
                    "start": 1918,
                    "end": 2079
                },
                {
                    "start": 2079,
                    "end": 2241
                },
                {
                    "start": 2241,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.037689208984375
        },
        {
            "corpus_id": "271571035",
            "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
            "text": "Here, we develop an explicit model for the relationship between coverage and the number of samples. The GPT-4 technical report [46] finds that the relationship between a model's mean-log-pass-rate on coding problems and its training compute can be modelled well using a power law. We start by adopting the same function class, but now modelling the log of coverage c as a function of the number of samples k: \n\nwhere a, b \u2208 R are fitted model parameters. In order to directly predict coverage, we exponentiate both sides, ending up with the final model of: \n\nWe provide examples of fitted coverage curves in Figure 5, and additional curves in Appendix C.2. While these laws are not as exact as training scaling laws (most strikingly on MiniF2F-MATH), they provide encouraging early evidence that the benefits of inference scaling can be characterized.",
            "score": 0.4713227149846211,
            "section_title": "Scaling Laws for Repeated Sampling",
            "char_start_offset": 17741,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 99
                },
                {
                    "start": 100,
                    "end": 280
                },
                {
                    "start": 281,
                    "end": 408
                },
                {
                    "start": 411,
                    "end": 454
                },
                {
                    "start": 455,
                    "end": 556
                },
                {
                    "start": 559,
                    "end": 656
                },
                {
                    "start": 657,
                    "end": 851
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09808349609375
        },
        {
            "corpus_id": "268064306",
            "title": "Sequence modeling and design from molecular to genome scale with Evo",
            "text": "Aiding our model design, we performed the first scaling laws analysis (to our knowledge) for DNA sequence modeling. The main objective of this type of analysis is to determine the relationship between training, architectural details, and performance metrics via a systematic experimental protocol (Hoffmann et al., 2022;Kaplan et al., 2020). Once a set of scaling laws is obtained, it can then be used as a guide to optimally scale training to larger models and datasets. \n\nHere, we compare different classes of architectures via a compute-optimal protocol, aimed at evaluating results on the compute-optimal frontier (Methods). We trained over 300 models across four architectures: Transformer++, Mamba, Hyena, and StripedHyena. Transformer++ is a state-of-the-art Transformer, and Mamba is a modern architecture using data-controlled state-space models (Gu and Dao, 2023). \n\nWe found Transformer++ to yield significantly worse perplexity (a measure of next token prediction quality) at all compute budgets (Figures 1G), a symptom of the inefficiency of the architecture at the byte resolution. State-space and deep signal processing architectures are observed to improve on the scaling rate over Transformer++, with Hyena and StripedHyena resulting in the best scaling rate. We observed stable training for StripedHyena throughout all the studied model sizes and learning rates during the scaling analysis. \n\nWe also compare architecture performance outside the compute-optimal frontier, namely with allocations of the computational budget that may be suboptimal. Performance outside the compute-optimal frontier is important in practice, as most models (including Evo) are trained for more tokens than recommended by compute-optimal scaling laws. We estimate 250 billion to be the compute-optimal number of tokens for Evo 7B given the FLOP budget, meaning the model was trained at a 17% offset from the compute-optimal model size during the initial 8192 sequence length pretraining phase of 300 billion tokens. Both Transformer++ and Mamba experienced numerical instability during training, and suffered from a sharper performance degradation of the scaling rate outside the compute-optimal frontier, in contrast to StripedHyena (further analysis in Figure S3).",
            "score": 0.4712171898580293,
            "section_title": "StripedHyena demonstrates favorable scaling laws on DNA sequence data",
            "char_start_offset": 9995,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 115
                },
                {
                    "start": 116,
                    "end": 341
                },
                {
                    "start": 342,
                    "end": 471
                },
                {
                    "start": 474,
                    "end": 628
                },
                {
                    "start": 629,
                    "end": 729
                },
                {
                    "start": 730,
                    "end": 874
                },
                {
                    "start": 877,
                    "end": 1095
                },
                {
                    "start": 1096,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1408
                },
                {
                    "start": 1411,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1749
                },
                {
                    "start": 1750,
                    "end": 2013
                },
                {
                    "start": 2014,
                    "end": 2264
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.147705078125
        },
        {
            "corpus_id": "271601023",
            "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving",
            "text": "Several prior studies also find that LLM problem-solving performance can be improved by outputting \"dummy\" tokens at inference time (Goyal et al., 2024a;Pfau et al., 2024). All of these methods show that using search at inference time can lead to performance gains at the cost of increased inference-time compute, but they do not characterize the cost-performance trade-off systematically. We are the first to formulate and study compute-optimal inference, analyzing the trade-off between compute budget and problem-solving performance and proposing the REBASE method that is empirically Pareto-optimal. Concurrently, Snell et al. (2025) also study how to scale inference-compute optimally and provide complementary insights, since we consider more model families and sizes while they study several different inference strategies. \n\nMathematical Reasoning with LLMs. Large language models have made significant progress in recent years, and have exhibited strong reasoning abilities (Brown et al., 2020;Hoffmann et al., 2022;Lewkowycz et al., 2022;Chowdhery et al., 2023). Mathematical problem solving is a key task for measuring LLM reasoning abilities (Cobbe et al., 2021;Hendrycks et al., 2021b). Ling et al. (2017) first developed the method of producing step by step solutions that lead to the final answer. Later, Cobbe et al. (2021) extended the work by training a verifier for evaluating and ranking solutions. Subsequent research has shown the performance benefits of inference-time techniques such as majority voting and weighted majority voting (Lewkowycz et al., 2022;Li et al., 2023). \n\nWe choose problem solving in mathematics as the task to study compute-optimal strategies since it allows us to accurately evaluate problem solving ability.",
            "score": 0.4702248787259099,
            "section_title": "RELATED WORKS",
            "char_start_offset": 8634,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 389
                },
                {
                    "start": 390,
                    "end": 603
                },
                {
                    "start": 604,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 866
                },
                {
                    "start": 867,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1199
                },
                {
                    "start": 1200,
                    "end": 1312
                },
                {
                    "start": 1313,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1755
                }
            ],
            "ref_mentions": [
                {
                    "start": 132,
                    "end": 153,
                    "matchedPaperCorpusId": "263608983"
                },
                {
                    "start": 153,
                    "end": 171,
                    "matchedPaperCorpusId": "269362669"
                },
                {
                    "start": 983,
                    "end": 1003,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1025,
                    "end": 1048,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 1048,
                    "end": 1071,
                    "matchedPaperCorpusId": "247951931"
                },
                {
                    "start": 1174,
                    "end": 1198,
                    "matchedPaperCorpusId": "232134851"
                },
                {
                    "start": 1200,
                    "end": 1218,
                    "matchedPaperCorpusId": "12777818"
                },
                {
                    "start": 1556,
                    "end": 1580,
                    "matchedPaperCorpusId": "250144408"
                },
                {
                    "start": 1580,
                    "end": 1596,
                    "matchedPaperCorpusId": "259370847"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.034698486328125
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "We now estimate the computational cost of our scaling law experiments, quantifying the effect of the learning rate schedule, and plot how our predictions improve and become more confident with increased computation. We find that the training cost of each experiment that utilized a fixed learning rate schedule was 1.54e20 FLOPs, while the experiments that used a varying-length cosine learning rate schedule required 2.99e20 FLOPs; essentially double the compute; see Appendix J for more details. We also find that the cost of the hyperparameter sweep described in Section 3.5 was 2.04e20 FLOPs-slightly less than the combined cost of two scaling experiments that leveraged it (one on each dataset). Moreover, in hindsight, we could have arrived at similar hyperparameters using only models of size at most 57M and a simple heuristic for choosing \u03b2 2 based on batch size, which would have cost only 1.44e19 FLOPs. \n\nFigure 5 shows the evolution of the predicted compute-optimal model size exponent a, its confidence interval, and a measure of the prediction accuracy as we modulate the experiment's compute budget by truncating our FLOP grid. The figure shows that the prediction becomes steadily more accurate and confident as compute increases. We present these results, as well as the results in Section 4.1, on the OpenWebText2 dataset as well (see Appendix I).",
            "score": 0.4693423981841992,
            "section_title": "Scaling law accuracy as a function of compute",
            "char_start_offset": 20899,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 215
                },
                {
                    "start": 216,
                    "end": 497
                },
                {
                    "start": 498,
                    "end": 700
                },
                {
                    "start": 701,
                    "end": 914
                },
                {
                    "start": 917,
                    "end": 1143
                },
                {
                    "start": 1144,
                    "end": 1247
                },
                {
                    "start": 1248,
                    "end": 1366
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.070068359375
        },
        {
            "corpus_id": "252683098",
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "text": "We find that the optimal neural network size scales as a power law with compute, with an exponent that can be derived from the individual size-scaling and compute-scaling exponents. All code and data used in our experiments are available online1 2 RELATED WORK Little work on power-law scaling has been published for MARL algorithms. Schrittwieser et al. (2021) report reward scaling as a power law with data frames when training a data efficient variant of MuZero. Jones (2021), the closest work to our own, shows evidence of power-law scaling of performance with compute, by measuring the performance of AlphaZero agents on small-board variants of the game of Hex. For board-sizes 3-9, log-scaling of Elo rating with compute is found when plotting the maximal scores reached among training runs. Without making an explicit connection to power-law scaling, the results reported by Jones (2021) can be characterized by a compute exponent of \u03b1 C \u2248 1.3, which can be shown when using Eq. 3. In the paper, the author suggests a phenomenological explanation for the observation that an agent with twice the compute of its opponent seems to win with a probability of roughly 2/3, which in fact corresponds to a compute exponent of \u03b1 C = 1. Similarly, Liu et al. (2021) report Elo scores that appear to scale as a log of environment frames for humanoid agents playing football, which would correspond to a power-law exponent of roughly 0.5 for playing strength scaling with data. Lee et al. (2022) apply the Transformer architecture to Atari games and plot performance scaling with the number of model parameters. Due to the substantially increased cost of calculating model-size scaling compared to compute or dataset-size scaling, they obtain only a limited number of data points, each generated by a single training seed. On this ba-sis, analyzing the scaling behavior seems difficult. First indications of the model-size scaling law presented in this work can be found in Neumann & Gros (2022).",
            "score": 0.46856800853767705,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3937,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 181
                },
                {
                    "start": 182,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 465
                },
                {
                    "start": 466,
                    "end": 666
                },
                {
                    "start": 667,
                    "end": 797
                },
                {
                    "start": 798,
                    "end": 1234
                },
                {
                    "start": 1235,
                    "end": 1473
                },
                {
                    "start": 1474,
                    "end": 1607
                },
                {
                    "start": 1608,
                    "end": 1818
                },
                {
                    "start": 1819,
                    "end": 1882
                },
                {
                    "start": 1883,
                    "end": 1992
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 361,
                    "matchedPaperCorpusId": "233219551"
                },
                {
                    "start": 1970,
                    "end": 1991,
                    "matchedPaperCorpusId": "252341912"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.189697265625
        },
        {
            "corpus_id": "268875826",
            "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
            "text": "Figure 5 (right) further shows how much smaller than the loss-optimal model is trained.When using the loss-optimal dense Transformer as baseline, with a training budget ranging from 2.12e21 to 5.96e21 (which means the dense model has a number of parameters from 3.36B to 6.14B), an 8-expert MoE uses 23.3% \u223c 28.2% activated parameters of the loss-optimal dense model and 21.0% \u223c 25.1% of the loss-optimal 4-expert MoE.Two consistent trends emerge: first, as the number of experts increases, the ratio of activated parameters in the MoE model compared to the base model decreases.Second, a higher budget correlates with a lower dense model parameter ratio.\n\nOptimal loss for bounded inference cost.Similarly, given a training budget B, we firstly compute the loss-optimal configuration for E-expert MoE, with its inference cost I E and L E .For MoE with E \u2032 experts, we compute the model size N E \u2032 which has an inference cost of I E \u2032 .The monotonicity discussed before guarantees that this is the model size with the lowest loss under the inference bound.Then we use the scaling law to estimate its loss, say\n\n(the detail of the algorithm is in Algorithm 2). Figure 6 (left) shows the result when the base model is a dense Transformer or 4-experts MoE.Overtraining more experts always has a better validation loss, but the gain of scaling from 16 to 32 experts already shows a diminishing return.\n\nAlike the bounded loss case, we also study how small is the over-trained model.Figure 6 (right) gives the ratio between the size of the over-trained E \u2032 -expert MoE and the lossoptimal E-expert MoE.When using the loss-optimal dense Transformer as the baseline, an 8-expert MoE uses 84.1% as large as the loss-optimal base model under a training budget of 5.15e21, while other number of experts also varies in a range of 37.1%\u223c125.2%.",
            "score": 0.4685394517058271,
            "section_title": "loss-optimal",
            "char_start_offset": 19896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 87
                },
                {
                    "start": 87,
                    "end": 418
                },
                {
                    "start": 418,
                    "end": 579
                },
                {
                    "start": 579,
                    "end": 655
                },
                {
                    "start": 657,
                    "end": 697
                },
                {
                    "start": 697,
                    "end": 840
                },
                {
                    "start": 840,
                    "end": 936
                },
                {
                    "start": 936,
                    "end": 1056
                },
                {
                    "start": 1056,
                    "end": 1109
                },
                {
                    "start": 1111,
                    "end": 1253
                },
                {
                    "start": 1253,
                    "end": 1397
                },
                {
                    "start": 1399,
                    "end": 1478
                },
                {
                    "start": 1478,
                    "end": 1597
                },
                {
                    "start": 1597,
                    "end": 1832
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.006511688232421875
        },
        {
            "corpus_id": "272910876",
            "title": "How Feature Learning Can Improve Neural Scaling Laws",
            "text": "Several works have found that this fails to capture the scaling laws of deep networks in the feature learning regime (Fort et al., 2020;Vyas et al., 2022;2023a;Bordelon et al., 2024a). A theory of scaling laws that can capture consistent feature learning even in an infinite parameter N \u2192 \u221e limit is especially pressing given the success of mean field and \u00b5-parameterizations which generate constant scale feature updates across model widths and depths (Mei et al., 2019;Geiger et al., 2020;Yang & Hu, 2021;Bordelon & Pehlevan, 2022;Yang et al., 2022;Bordelon et al., 2023;2024b) Figure 1: Our model changes its scaling law exponents for hard tasks, where the source is sufficiently small \u03b2 < 1. (a) The exponent \u03c7(\u03b2) which appears in the loss scaling L(t) \u223c t \u2212\u03c7(\u03b2) of our model. (b)-(c) Phase plots in the \u03b1, \u03b2 plane of the observed scalings that give rise to the compute-optimal trade-off. Arrows (\u2192) represent a transition from one scaling behavior to another as t \u2192 \u221e, where the balancing of these terms at fixed compute C = N t gives the compute optimal scaling law. In the lazy limit \u03b3 \u2192 0, we recover the phase plot for \u03b1 > 1 of Paquette et al. (2024). At nonzero \u03b3, however, we see that the set of \"hard tasks\", as given by \u03b2 < 1 exhibits an improved scaling exponent. The compute optimal curves for the easy tasks with \u03b2 > 1 are unchanged. \n\ninfinite width/depth limits in such models can significantly differ from the lazy training regime. \n\nInfinite limits which preserve feature learning are better descriptors of practical networks Vyas et al. (2023a). Motivated by this, we ask the following: \n\nQuestion: Under what conditions can feature learning improve the scaling law exponents of neural networks compared to lazy training regime? \n\n1.1 OUR CONTRIBUTIONS \n\nIn this work, we develop a theoretical model of neural scaling laws that allows for improved scaling exponents compared to lazy training under certain settings. Our contributions are 1.",
            "score": 0.46844085797452684,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1764,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 184
                },
                {
                    "start": 185,
                    "end": 695
                },
                {
                    "start": 696,
                    "end": 780
                },
                {
                    "start": 781,
                    "end": 892
                },
                {
                    "start": 893,
                    "end": 1072
                },
                {
                    "start": 1073,
                    "end": 1160
                },
                {
                    "start": 1161,
                    "end": 1277
                },
                {
                    "start": 1278,
                    "end": 1349
                },
                {
                    "start": 1352,
                    "end": 1450
                },
                {
                    "start": 1453,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1607
                },
                {
                    "start": 1610,
                    "end": 1749
                },
                {
                    "start": 1752,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1936
                },
                {
                    "start": 1937,
                    "end": 1961
                }
            ],
            "ref_mentions": [
                {
                    "start": 117,
                    "end": 136,
                    "matchedPaperCorpusId": "225094694"
                },
                {
                    "start": 453,
                    "end": 471,
                    "matchedPaperCorpusId": "62841577"
                },
                {
                    "start": 471,
                    "end": 491,
                    "matchedPaperCorpusId": "209926574"
                },
                {
                    "start": 533,
                    "end": 551,
                    "matchedPaperCorpusId": "248505757"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.137939453125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Scaling laws have become a fundamental aspect of modern AI development, especially for large language models (LLMs). In recent years, researchers have identified consistent relationships between model size, dataset volume, and computational resources, demonstrating that increasing these factors leads to systematic improvements in performance. These empirical patterns have been formalized into mathematical principles, known as scaling laws, which provide a framework for understanding how * Equal contribution the capabilities of neural networks evolve as they grow. Mastering these laws is crucial for building more powerful AI models, optimizing efficiency, reducing costs, and improving generalization. \n\nThe study of neural scaling laws gained prominence with the foundational work of Kaplan et al. (2020), who demonstrated that model performance follows a power-law relationship with respect to size, data, and compute. Their findings suggested that larger language models (LMs) achieve lower loss when trained on sufficiently large datasets with increased computational resources. Later, Hoffmann et al. (2022) refined these ideas, introducing the notion of compute-optimal scaling, which revealed that training a moderate-sized model on a larger dataset is often more effective than scaling model size alone. However, recent studies (Muennighoff et al., 2023;Caballero et al., 2023;Krajewski et al., 2024) have challenged the universality of these laws, highlighting cases where sparse models, mixture-of-experts architectures, and retrievalaugmented methods introduce deviations from traditional scaling patterns. These findings suggested that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies. \n\nDespite the growing importance of scaling laws, existing research remains fragmented, with limited synthesis of theoretical foundations, empirical findings, and practical implications. Given the rapid evolution of this field, there is a need for a structured analysis that consolidates key insights, identifies limitations, and outlines future research directions. While theoretical studies have established the mathematical principles governing scaling, their real-world applications, such as efficient model training, optimized resource allocation, and improved inference strategies, are less explored.",
            "score": 0.46837293004924185,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 344
                },
                {
                    "start": 345,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 708
                },
                {
                    "start": 711,
                    "end": 927
                },
                {
                    "start": 928,
                    "end": 1089
                },
                {
                    "start": 1090,
                    "end": 1318
                },
                {
                    "start": 1319,
                    "end": 1624
                },
                {
                    "start": 1625,
                    "end": 1777
                },
                {
                    "start": 1780,
                    "end": 1964
                },
                {
                    "start": 1965,
                    "end": 2144
                },
                {
                    "start": 2145,
                    "end": 2384
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.459228515625
        },
        {
            "corpus_id": "225094178",
            "title": "Scaling Laws for Autoregressive Generative Modeling",
            "text": "In all cases we find that N opt (C) \u221d C \u03b2 can be fit with a pure power-law, with all exponents fairly close to \u03b2 \u223c 0.7. This suggests that one should spend most of a growing training compute budget by training much larger generative models. \n\nWhen estimating N opt (C), one might worry about errors due to a sub-optimal usage of data. Specifically, if the batch size is too large early in training, then some compute may effectively be wasted. This can be studied by identifying the critical batch size [MBB17, MKAT18] above which there are diminishing returns to further data parallelism. In prior work [KMH + 20] this was taken into account by measuring the critical batch size and using relations derived in [MKAT18] to adjust compute estimates. We have not made this adjustment here, as it would require a number of additional experiments in order to measureme the critical batch size in each domain. For large model sizes and compute budgets these effects should be small, because most or all of training involves batches smaller than the critical batch size (which grows quickly during training [MKAT18]), but this issue may be worth revisiting in the future. \n\nThe total number of tokens processed during all of training is E = C 6N \u2265 D, where D is the dataset size, with equality representing training for only a single epoch. This means that D \u221d C 1\u2212\u03b2 \u221d N 1\u2212\u03b2 \u03b2 . We clearly have \u03b2 > 0.6 for all data modalities and by a comfortable margin, suggesting that dataset size should not grow faster than D \u221d N 2/3 during compute-optimal training, with a more reasonable median estimate of D \u221d N 0.4 . This unambiguously sub-linear scaling across all data modalities runs somewhat counter to conventional wisdom. As a word of caution, we have yet to train models in a regime where compute optimal training actually implies D N numerically. We discuss this further in section 6. On the left we have the mean loss over the three colors for images of various resolutions. The top-left pixel actually has significantly higher loss, off the color scale, which was set to make the pattern clear for the image as a whole.",
            "score": 0.4681423825820986,
            "section_title": "Compute Scaling and Optimal Model Sizes",
            "char_start_offset": 23334,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 240
                },
                {
                    "start": 243,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 589
                },
                {
                    "start": 590,
                    "end": 748
                },
                {
                    "start": 749,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1165
                },
                {
                    "start": 1168,
                    "end": 1334
                },
                {
                    "start": 1335,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1714
                },
                {
                    "start": 1715,
                    "end": 1841
                },
                {
                    "start": 1842,
                    "end": 1879
                },
                {
                    "start": 1880,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2116
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1474609375
        },
        {
            "corpus_id": "273234123",
            "title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System",
            "text": "Recent research emphasizes inference-time scaling, which describes how model performance improves with increased compute during inference, typically by generating multiple samples per problem (Brown et al., 2024;Wu et al., 2024). Unlike training scaling laws, which focus on model size, dataset size, and performance, inference-time scaling explore the trade-off between compute budget and task accuracy, offering a promising way to en-0 2 1 2 2 2 3 2 4 2  hance model capabilities without further training. Fig. 3 illustrates OPTIMA's impact on inferencetime scaling. The left panel shows the relationship between SC steps and performance on multiagent debate tasks. While majority voting accuracy plateaus after a certain number of steps, coverage (the percentage of problems answered correctly at least once) improves logarithmically with increased sampling. This aligns with recent studies (Wu et al., 2024;Chen et al., 2024a), suggesting advanced answer selection techniques could further boost OPTIMA's performance. Additional scaling law figures for all OPTIMA variants and tasks are in Appendix A, where similar trends are observed. \n\nThe right panel demonstrates OPTIMA's efficiency in improving inference scaling laws on GSM8k. OPTIMA variants, including those transferred from MATH, consistently outperform CoT SC, except for MATH-trained iSFT. Notably, GSM8k-trained iDPO matches CoT-SC performance with 88.5% fewer tokens, effectively \"shifting the curve left\". This reduction in token usage translates to significant computational savings without sacrificing accuracy. MATH-trained OPTIMA variants, except iSFT, also deliver better scaling laws on GSM8k than CoT SC, highlighting OP-TIMA's cross-task generalization. \n\nThese results underscore OPTIMA's potential to reshape inference-time scaling for MAS and general LLM systems. By enabling more efficient use of compute budgets, OPTIMA achieves better performance at lower costs or higher performance at the same cost. This efficiency opens possibilities for integrating advanced inference techniques like weighted voting or tree-search (Wu et al., 2024), potentially leading to further performance gains.",
            "score": 0.46797262843009557,
            "section_title": "Can OPTIMA Improve Inference Scaling?",
            "char_start_offset": 20679,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 229
                },
                {
                    "start": 230,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 568
                },
                {
                    "start": 569,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 861
                },
                {
                    "start": 862,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1140
                },
                {
                    "start": 1143,
                    "end": 1237
                },
                {
                    "start": 1238,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1474
                },
                {
                    "start": 1475,
                    "end": 1582
                },
                {
                    "start": 1583,
                    "end": 1730
                },
                {
                    "start": 1733,
                    "end": 1843
                },
                {
                    "start": 1844,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2171
                }
            ],
            "ref_mentions": [
                {
                    "start": 911,
                    "end": 930,
                    "matchedPaperCorpusId": "266573087"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10125732421875
        },
        {
            "corpus_id": "247778764",
            "title": "Training Compute-Optimal Large Language Models",
            "text": "In our first approach we vary the number of training steps for a fixed family of models (ranging from 70M to over 10B parameters), training each model for 4 different number of training sequences. From these runs, we are able to directly extract an estimate of the minimum loss achieved for a given number of training FLOPs. Training details for this approach can be found in Appendix D. \n\nFor each parameter count  we train 4 different models, decaying the learning rate by a factor of 10\u00d7 over a horizon (measured in number of training tokens) that ranges by a factor of 16\u00d7. Then, for each run, we smooth and then interpolate the training loss curve. From this, we obtain a continuous mapping from FLOP count to training loss for each run. Then, for each FLOP count, we determine which run achieves the lowest loss. Using these interpolants, we obtain a mapping from any FLOP count , to the most efficient choice of model size  and number of training tokens  such that FLOPs(, ) = .4 At 1500 logarithmically spaced FLOP values, we find which model size achieves the lowest loss of all models along with the required number of training tokens. Finally, we fit power laws to estimate the optimal model size and number of training tokens for any given amount of compute (see the center and right panels of Figure 2), obtaining a relationship   \u221d   and   \u221d   . We find that  = 0.50 and  = 0.50-as summarized in Table 2. In Section D.4, we show a head-to-head comparison at 10 21 FLOPs, using the model size recommended by our analysis and by the analysis of Kaplan et al. (2020)-using the model size we predict has a clear advantage.",
            "score": 0.46793161169213093,
            "section_title": "Approach 1: Fix model sizes and vary number of training tokens",
            "char_start_offset": 10681,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 196
                },
                {
                    "start": 197,
                    "end": 324
                },
                {
                    "start": 325,
                    "end": 387
                },
                {
                    "start": 390,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 742
                },
                {
                    "start": 743,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 986
                },
                {
                    "start": 987,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1418
                },
                {
                    "start": 1419,
                    "end": 1632
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0273284912109375
        },
        {
            "corpus_id": "264172438",
            "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
            "text": "Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models. \n\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both BitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training tokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16 Transformer, which follows a power-law. We then fit the scaling law with an irreducible loss term: To evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M to 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It shows that the fitted scaling law predicted BitNet's loss with high accuracy. Besides, the gap between BitNet and FP16 Transformer becomes smaller as the model size grows. \n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3 shows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet has much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly better loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16 models.",
            "score": 0.46758105293385144,
            "section_title": "Inference-Optimal Scaling Law",
            "char_start_offset": 13008,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 105
                },
                {
                    "start": 106,
                    "end": 188
                },
                {
                    "start": 189,
                    "end": 344
                },
                {
                    "start": 347,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 577
                },
                {
                    "start": 578,
                    "end": 687
                },
                {
                    "start": 688,
                    "end": 945
                },
                {
                    "start": 946,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1120
                },
                {
                    "start": 1123,
                    "end": 1278
                },
                {
                    "start": 1279,
                    "end": 1371
                },
                {
                    "start": 1372,
                    "end": 1462
                },
                {
                    "start": 1463,
                    "end": 1543
                },
                {
                    "start": 1544,
                    "end": 1671
                },
                {
                    "start": 1672,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1842
                },
                {
                    "start": 1843,
                    "end": 1896
                },
                {
                    "start": 1897,
                    "end": 1985
                },
                {
                    "start": 1986,
                    "end": 2043
                },
                {
                    "start": 2044,
                    "end": 2122
                },
                {
                    "start": 2123,
                    "end": 2216
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09722900390625
        },
        {
            "corpus_id": "262013578",
            "title": "Scaling Laws for Sparsely-Connected Foundation Models",
            "text": "In more detail, the first multiplicative term captures the impact of sparsity, here expressed as remaining density (1 \u2212 S), which itself follows a saturating power-law with coefficient a S , exponent b S and limit constant c S . The exponents b N and b D scale the (non-zero) parameter count N , and the data D term, respectively, as is common in classical scaling laws (Kaplan et al., 2020). \n\nWe validate this formula empirically using large vision and language datasets, several model sizes, amounts of training data and sparsity levels. Please see Figure 1 (Left) for an illustration of the scaling law fit and extrapolation quality. In turn, this law allows us to obtain several new insights regarding both the power and limitations of weight sparsity, in the foundation model setting: \n\n\u2022 First, it suggests that sparsity affects each model size in a similar way, i.e., as a multiplicative constant to the size scaling. At the same time, sparsification does not appear to interact significantly with the data scaling; the original dense term in D is preserved. \u2022 Second, we can use our scaling law in Equation (1) to analytically derive the optimal sparsity S opt for a given inference size and training budget, allowing us to predict the regime where sparsity could actually provide benefits over simple dense model rescaling and extended training. \u2022 Our analysis of optimal sparsity S opt , demonstrated in Figure 1 (Right), shows that its iso-contours run parallel to the dense compute optimal Chinchilla line (Hoffmann et al., 2022) of the respective model and task. Importantly, the optimal sparsity increases with longer training. Further, while optimal dense models define a line on the parameter-FLOPs surface, optimal sparse models form a half-plane (with different sparsities unlocking multiple optimal sizes for a fixed training cost).",
            "score": 0.4669871126377151,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3277,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 228
                },
                {
                    "start": 229,
                    "end": 392
                },
                {
                    "start": 395,
                    "end": 540
                },
                {
                    "start": 541,
                    "end": 637
                },
                {
                    "start": 638,
                    "end": 790
                },
                {
                    "start": 793,
                    "end": 925
                },
                {
                    "start": 926,
                    "end": 1066
                },
                {
                    "start": 1067,
                    "end": 1355
                },
                {
                    "start": 1356,
                    "end": 1576
                },
                {
                    "start": 1577,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1131591796875
        },
        {
            "corpus_id": "273877502",
            "title": "Scaling Laws for Pre-training Agents and World Models",
            "text": "Much progress in AI in the early 2020's has been driven by increasing model size, dataset size, and training compute. Whilst conceptually simple, the importance of this practice has led to an emerging subfield studying the science of scaling. This field answers questions such as how to estimate the benefit of increased compute investment, or how to optimally trade-off model and dataset size. \n\nThe role of scale in pre-training is until now best understood in the context of large language models (LLMs). Following the observation that the empirical relationship between loss and key scaling quantities can be accurately described by power laws (Kaplan et al., 2020), ensuing work studied the precise trade-off between model and dataset size (Hoffmann et al., 2022), as well as considerations about inference compute (Sardana and Frankle, 2023), repeated training data (Muennighoff et al., 2024), parameter counting (Pearce and Song, 2024), and more (Section 2). \n\nIn comparison, less is understood about scaling in embodied AI. Recent high-impact works suggest increasing model and dataset size can lead to ever more capable agents for two pre-training objectives; behavior cloning (BC) (Reed et al., 2022;Baker et al., 2022;Brohan et al., 2023) and world modeling (Hafner et al., 2020;Hu et al., 2023;Yang et al., 2023;Bruce et al., 2024). Such works typically demonstrate the benefit of scale through ablations over only a few model sizes, shown in terms of downstream agent performance, confirming the intuition that 'bigger is better' (Sartor and Thompson (2024) provide an aggregated analysis). However, this leaves a large gap to the precise understanding of scale in LLMs, where for a given increase in compute, models can be sized optimally, and their expected performance accurately predicted. This paper helps close this gap. Similar to the initial study of scale in LLMs, we focus on the effect of scaling on a generative pre-training loss (rather than on downstream agent performance, or rewardor representation-centric objectives), in the infinite data regime, on a fixed offline dataset.",
            "score": 0.4666014019444885,
            "section_title": "INTRODUCTION",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 242
                },
                {
                    "start": 243,
                    "end": 394
                },
                {
                    "start": 397,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 965
                },
                {
                    "start": 968,
                    "end": 1031
                },
                {
                    "start": 1032,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1603
                },
                {
                    "start": 1604,
                    "end": 1806
                },
                {
                    "start": 1807,
                    "end": 1839
                },
                {
                    "start": 1840,
                    "end": 2105
                }
            ],
            "ref_mentions": [
                {
                    "start": 872,
                    "end": 898,
                    "matchedPaperCorpusId": "258888192"
                },
                {
                    "start": 1210,
                    "end": 1229,
                    "matchedPaperCorpusId": "249953673"
                },
                {
                    "start": 1324,
                    "end": 1343,
                    "matchedPaperCorpusId": "267897982"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1541748046875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "Generative Reward Models (GenRMs) introduce a novel approach to scaling test-time compute through verifications. While prior work demonstrates that scaling both solutions and verifications can surpass Self-Consistency (SC), it often overlooks verification costs. In this study, we investigate whether scaling verifications improves performance under a fixed budget. We find that SC outperforms GenRMs at lower budgets, whereas GenRMs excel at higher ones. Our conclusions regarding the compute-optimality of SC and GenRMs across different budgets remain robust across various model families (including thinking models), sizes, and reasoning tasks. Furthermore, we derive inference scaling laws to optimize budget allocation between solutions and verifications in GenRM. Overall, our findings provide practical guidance for compute-efficient scaling to achieve optimal performance.",
            "score": 0.4665521378969101,
            "section_title": "Conclusion",
            "char_start_offset": 30365,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 112
                },
                {
                    "start": 113,
                    "end": 262
                },
                {
                    "start": 263,
                    "end": 365
                },
                {
                    "start": 366,
                    "end": 455
                },
                {
                    "start": 456,
                    "end": 647
                },
                {
                    "start": 648,
                    "end": 769
                },
                {
                    "start": 770,
                    "end": 880
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.059417724609375
        },
        {
            "corpus_id": "274280605",
            "title": "Towards Precise Scaling Laws for Video Diffusion Transformers",
            "text": "We first investigate whether model size should be considered an independent variable in the scaling laws of batch size and learning rate. \n\nTo optimize the objective function L(\u03b8), we need to compute its gradient G(\u03b8) = \u2207L(\u03b8). Since the objective function L(\u03b8) is complex, we approximate its gradient using the stochastic function. At iteration k, we randomly sample a mini-batch of data {\u03be \n\nb=1 from the training data distribution \u03c1. Using these samples, we compute an estimated gradient g k as: \n\nwhere \u03be \n\nTo facilitate convergence analysis, we assume that, conditioned on the filtration G B k , \n\nthe mini-batch gradient g k is an unbiased estimate on \u2207L(\u03b8 k ) = G(\u03b8 k ), and the variance is bounded by \u03c3 2 B : To simplify the analysis, we assume that L(\u03b8 k ) is L-smooth for any iteration k, and we derive the convergence properties under the framework of Mini-Batch Stochastic Gradient Descent (SGD) [37]. For effective updates, the learning rate \u03b7 must satisfy \u03b7 \u2264 1/L. Under these conditions, the convergence of L(\u03b8 k ) is characterized as follows (proof provided in Appendix B): \n\nHere, \u2206 0 = L(\u03b8 0 ) \u2212 L \u22c6 represents the initial gap to the optimal objective value L \u22c6 , and L \u22c6 denotes the optimal convergence point. \n\nAs the model size increases, the complexity of the model also grows, leading to a corresponding increase in the Lipschitz constant L. To ensure that the upper bound on the right-hand side of Equation ( 5) remains manageable, both the learning rate and batch size must be adjusted with respect to L. Specifically, a larger L necessitates a smaller learning rate \u03b7 and a larger batch size B for convergence. This relationship illustrates how the scaling of model size impacts the optimal choice of hyperparameters. Unlike prior work [1], which treats model size implicitly, our analysis explicitly incorporates model size as an independent variable. This enables a more accurate estimation of optimal hyperparameters by addressing the interplay between model complexity, learning rate, and batch size in a principled manner.",
            "score": 0.4665329669195014,
            "section_title": "Impact of model size on hyperparameters",
            "char_start_offset": 10263,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 137
                },
                {
                    "start": 140,
                    "end": 226
                },
                {
                    "start": 227,
                    "end": 331
                },
                {
                    "start": 332,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 497
                },
                {
                    "start": 500,
                    "end": 507
                },
                {
                    "start": 510,
                    "end": 599
                },
                {
                    "start": 602,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1088
                },
                {
                    "start": 1091,
                    "end": 1227
                },
                {
                    "start": 1230,
                    "end": 1635
                },
                {
                    "start": 1636,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1877
                },
                {
                    "start": 1878,
                    "end": 2052
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017578125
        },
        {
            "corpus_id": "276961790",
            "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
            "text": "Scaling laws are a critical component of the LLM development pipeline, most famously as a way to forecast training decisions such as 'compute-optimally' trading-off parameter count and dataset size, alongside a more recent growing list of other crucial decisions. In this work, we ask whether compute-optimal scaling behaviour can be skill-dependent. In particular, we examine knowledge and reasoning-based skills such as knowledge-based QA and code generation, and we answer this question in the affirmative: scaling laws are skill-dependent. Next, to understand whether skill-dependent scaling is an artefact of the pretraining datamix, we conduct an extensive ablation of different datamixes and find that, also when correcting for datamix differences, knowledge and code exhibit fundamental differences in scaling behaviour. We conclude with an analysis of how our findings relate to standard compute-optimal scaling using a validation set, and find that a misspecified validation set can impact compute-optimal parameter count by nearly 50%, depending on its skill composition.",
            "score": 0.46630927146158263,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1739501953125
        },
        {
            "corpus_id": "270371652",
            "title": "Compute Better Spent: Replacing Dense Layers with Structured Matrices",
            "text": "Having developed an effective procedure to automatically scale the initialization and learning rates for structured layers, we now aim to understand how various structures compare in performance.\n\nWhen data is not a bottleneck, a neural network's test error or loss on a task follows a power law E \u221d P \u2212\u03b1 P if trained to (near) convergence, where P is the number of parameters and \u03b1 P is a constant (Kaplan et al., 2020;Hoffmann et al., 2022;Henighan et al., 2020).For dense models, compute per forward pass C \u221d P , so E \u221d C \u2212\u03b1 C for some constant \u03b1 C .We explore how different structures change how E scales with C, as P does not consistently relate to training or inference cost when varying the structure (Table 1).\n\nWe train all models for a fixed number of iterations T , so the total training compute C tot \u221d C. Thus, the scaling laws in C can differ from compute-optimal scaling laws, which require carefully optimizing the allocation of C tot \u221d CT between C and T (Kaplan et al., 2020;Hoffmann et al., 2022), which we leave to future work.\n\nTo compare multiple structures across compute scales, we conduct experiments primarily using MLPs and ViTs on CIFAR-10 and CIFAR-100.In Section 5, we present largerscale experiments on ImageNet and language modeling.With limited training data in CIFAR-10 and CIFAR-100, we apply heavy augmentation to alleviate over-fitting.The augmented training set is sufficiently large, resulting in relatively clean power-law scaling of training error with C. We extract these power law parameters, reflecting the expressivity afforded by each structure as a function of C, and visualize the scaling of test error with C, which is not well-described by a power law due to train-test discrepancy.",
            "score": 0.46592859085179383,
            "section_title": "Scaling Laws of Structured Matrices",
            "char_start_offset": 21342,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 195
                },
                {
                    "start": 197,
                    "end": 465
                },
                {
                    "start": 465,
                    "end": 553
                },
                {
                    "start": 553,
                    "end": 718
                },
                {
                    "start": 720,
                    "end": 1047
                },
                {
                    "start": 1049,
                    "end": 1182
                },
                {
                    "start": 1182,
                    "end": 1265
                },
                {
                    "start": 1265,
                    "end": 1373
                },
                {
                    "start": 1373,
                    "end": 1732
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.142822265625
        },
        {
            "corpus_id": "267782738",
            "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
            "text": "In previous experiments, we have fixed training hyperparameters and focused on adapting the model's shape. Training hyperparameters can nonetheless also be altered optimally. Previous work has already established that bigger batch sizes are beneficial during later stages in training (Kaplan et al., 2020;Hoffmann et al., 2022;Zhai et al., 2023). Different training objectives are also known to contribute differently to downstream performance at distinct stages in training (Zhai et al., 2023;Singh et al., 2023). Previous work has relied on heuristics and brute force exploration to determine when these should change. Here, we demonstrate how scaling laws can help decide when to change these parameters and descend based on the optimal one. \n\nBatch size. We first focus on the batch size used during training. We train two Vision Transformers, one at a larger batch size and the other at a smaller batch size. We find that in terms of number of FLOPs, the smaller batch size initially dominates but again is surpassed at later stages in training by the large batch size run (Fig. 12 left). Our strategy allows us to maximally take advantage of this difference by optimally transitioning between the two batch sizes, leading to a more optimal model. \n\nDistillation. We additionally train ViT models by distilling from a powerful teacher, a bigger pretrained ViT (Fig. 12 right). Distilled labels naturally come at an additional computational cost, due to queries to the teacher, and lead to slower convergence in terms of FLOPs initially in training (FLOPs here include the teacher compute). Distillation objectives, however, were found to lead to increases in performance (Hinton et al., 2015;Furlanello et al., 2018;Touvron et al., 2021). Thus, in later stages of training, such an objective will dominate the standard supervised loss. We again leverage this discrepancy and optimally switch from the standard to the distilled loss, allowing us to reach the same level of performance with fewer FLOPs.",
            "score": 0.46547239005327873,
            "section_title": "ADAPTING THE TRAINING OBJECTIVE",
            "char_start_offset": 24244,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 107,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 620
                },
                {
                    "start": 621,
                    "end": 744
                },
                {
                    "start": 747,
                    "end": 758
                },
                {
                    "start": 759,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1252
                },
                {
                    "start": 1255,
                    "end": 1268
                },
                {
                    "start": 1269,
                    "end": 1381
                },
                {
                    "start": 1382,
                    "end": 1594
                },
                {
                    "start": 1595,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1840
                },
                {
                    "start": 1841,
                    "end": 2006
                }
            ],
            "ref_mentions": [
                {
                    "start": 1697,
                    "end": 1721,
                    "matchedPaperCorpusId": "4110009"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.306640625
        },
        {
            "corpus_id": "247447316",
            "title": "Staged Training for Transformer Language Models",
            "text": "We next discuss where in training to grow a model. Intuitively, the optimal schedule is one where the original small model is trained until its compute-efficient regime ends, then grown to a larger model to continue its computeefficient regime. Figure 1 highlights one such potential schedule. Notice that there's a specific point on the loss curve of the original model that leads to the most compute saving; growing the model earlier means a wasted opportunity skipping some of the fast loss reduction stage of the original model, and growing the model later means wasting compute by continuing to train after the loss of the original model begins to plateau. \n\nSchedule which minimizes compute Next, we describe how to mathematically find this optimal schedule. For that, we use the scaling laws (Kaplan et al., 2020) which derived empirical fits for the language model loss L as it relates to compute C, number of non-embedding parameters N , number of gradient update steps S, and batch size B. The total compute and the loss are given by \n\nwhere \u03b1 N , \u03b1 S , \u03b1 B , N c , S c , B * are all modelspecific constants. Thus, finding the the optimal schedule can be formulated as a constrained optimization problem. The output is the intermediate model sizes, and the amount of compute for each stage. We discuss the details in Appendix C.",
            "score": 0.464643481886539,
            "section_title": "Intermediate Stages",
            "char_start_offset": 13896,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 50
                },
                {
                    "start": 51,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 293
                },
                {
                    "start": 294,
                    "end": 661
                },
                {
                    "start": 664,
                    "end": 764
                },
                {
                    "start": 765,
                    "end": 1043
                },
                {
                    "start": 1046,
                    "end": 1118
                },
                {
                    "start": 1119,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1338
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1610107421875
        },
        {
            "corpus_id": "273404359",
            "title": "Scaling Laws for Multilingual Language Models",
            "text": "This provides an important simplification for analyzing multilingual scaling behavior. \u2022 Multilingual scaling law: Based on the validated hypothesis, we propose a scaling law that relates the test cross-entropy loss (L i ) for each language family i to model size (N ), dataset size (D) and language family sampling ratios (p): \n\n, where E i , A i , B i , \u03b1 i , \u03b2 i , \u03b3 i are fixed parameters for the i-th family. One key implication of the above form is that the scaling law of each language family only depends on its own sampling ratio p i , independent of the sampling ratios of other families p j\u0338 =i . Additionally, we discover that the exponent \u03b3 i , which governs how much loss reduces as the proportion of data from family i increases, remains invariant to model size N and dataset size D. This finding further strengthens the applicability of our scaling law across different compute scales. \u2022 Derivation of the optimal sampling ratios: Leveraging the proposed scaling law, we derive the optimal sampling ratios that minimize the total loss for the LM, thus providing an effective data mixing strategy for multilingual pretraining. We validate the optimality of these ratios by comparing them against other baseline sampling methods. We demonstrate that the optimal sampling ratios obtained from small models (85M parameters) generalize well to models that are several orders of magnitude larger (1.2B parameters). This insight implies that for resource-efficient LM training, practitioners can optimize training mixtures for largescale models by only training smaller and more affordable models, drastically reducing computational overhead while maintaining performance consistency across model scales.",
            "score": 0.4644368321356064,
            "section_title": "INTRODUCTION",
            "char_start_offset": 3906,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 86
                },
                {
                    "start": 87,
                    "end": 327
                },
                {
                    "start": 330,
                    "end": 413
                },
                {
                    "start": 414,
                    "end": 607
                },
                {
                    "start": 608,
                    "end": 798
                },
                {
                    "start": 799,
                    "end": 901
                },
                {
                    "start": 902,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1243
                },
                {
                    "start": 1244,
                    "end": 1424
                },
                {
                    "start": 1425,
                    "end": 1713
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04327392578125
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "We are interested in scaling behavior in the data-constrained regime. Specifically, given a limited amount of unique data, what is the best Allocation of and Return for computational resources. Prior work [46,42] assumes that the necessary data to support scaling is unlimited. Our aim is therefore to introduce a modified version of Equation 2 that accounts for data constraints and fit the terms in the modified scaling law to data from a large body of experiments. \n\nThe primary method we consider is repeating data, i.e. allocating FLOPs to multiple epochs on the same data. Given a budget of unique data D C , we split the Chinchilla total data term D into two parts: the number of unique tokens used, U D , and the number of repetitions, R D (i.e. epochs -1). Given \n\nSymmetrically, for mathematical convenience, we split the parameter term N into two parts: the base number of parameters needed to optimally fit the unique tokens U N , and the number of times to \"repeat\" this initial allocation, R N . We compute U N by first rearranging Equation 3 to find the optimal compute budget for the unique tokens used (U D ). We input this value into the N opt formula of Equation 3 to get U N = min{N opt , N }. U N thus corresponds to the compute-optimal number of parameters for U D or less if N < N opt . Once we have U N , we compute the repeat value as R N = (N/U N ) \u2212 1. \n\nTo empirically explore the scaling behavior in a data-limited setting we train LLMs under these constraints. We consider three different experimental protocols in this work: \n\n\u2022 (Fixed Unique Data) In \u00a75 we fix the data constraint D C and train models varying epochs and parameters. These experiments target Allocation, specifically tradeoff of D and N . \n\n\u2022 (Fixed FLOPs) In \u00a76 we fix the computation available and vary D C (and thus also U D and U N ). These experiments target Return, i.e. how well does repeating scale compared to having more unique data. \n\n\u2022 (Parametric Fit) We fit a formula introduced in \u00a73.1 on all our training runs and evaluate its predictive capability throughout \u00a75 and \u00a76.",
            "score": 0.46438773896254715,
            "section_title": "Method: Data-Constrained Scaling Laws",
            "char_start_offset": 6399,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 69
                },
                {
                    "start": 70,
                    "end": 193
                },
                {
                    "start": 194,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 467
                },
                {
                    "start": 470,
                    "end": 578
                },
                {
                    "start": 579,
                    "end": 753
                },
                {
                    "start": 754,
                    "end": 765
                },
                {
                    "start": 766,
                    "end": 771
                },
                {
                    "start": 774,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1126
                },
                {
                    "start": 1127,
                    "end": 1213
                },
                {
                    "start": 1214,
                    "end": 1309
                },
                {
                    "start": 1310,
                    "end": 1379
                },
                {
                    "start": 1382,
                    "end": 1490
                },
                {
                    "start": 1491,
                    "end": 1555
                },
                {
                    "start": 1558,
                    "end": 1664
                },
                {
                    "start": 1665,
                    "end": 1736
                },
                {
                    "start": 1739,
                    "end": 1836
                },
                {
                    "start": 1837,
                    "end": 1941
                },
                {
                    "start": 1944,
                    "end": 2084
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0309600830078125
        },
        {
            "corpus_id": "270068102",
            "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations",
            "text": "We have demonstrated the reliability of an alternative learning rate schedule to replace cosine for LLM training, which uses a constant rate with a cooldown phase. Across a multitude of experiments, we analyze different recipes for the decay form and length. Importantly, we do not claim to have established the best learning rate schedule -instead, we investigate and demonstrate how an arguably simple recipe can match the performance of the current best practice of cosine, and discuss how it provides compelling advantages such as continual training and a strong reduction in costs for scaling law research. In addition, we find that SWA can give reliable (strong, but not optimal) estimates of models during runs, without additional overhead or training. \n\nWe believe the results are of great importance to the present and future of LLM training: the presented methods facilitate research for the current post-Chinchilla era, where models are trained much beyond compute-optimal, by allowing more flexibility to continue training whenever needed. At the same time, recent results that suggest data-dependency in scaling (Bi et al., 2024;Goyal et al., 2024;Aghajanyan et al., 2023;Pandey, 2024) imply the need to frequently update scaling laws, which is economically more feasible with reduced costs. We therefore hope that our work will make scaling research more accessible to researchers and practitioners alike.",
            "score": 0.4628749530838825,
            "section_title": "Conclusion",
            "char_start_offset": 26377,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 163
                },
                {
                    "start": 164,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 611
                },
                {
                    "start": 612,
                    "end": 759
                },
                {
                    "start": 762,
                    "end": 1051
                },
                {
                    "start": 1052,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1419
                }
            ],
            "ref_mentions": [
                {
                    "start": 1161,
                    "end": 1185,
                    "matchedPaperCorpusId": "255570036"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.09075927734375
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "allowing us to predict loss scaling in many situations of practical interest. This functional form that posits bit precision and parameter count interchangeably contribute to a model's \"effective parameter count,\" N eff , and implementation details like which parts of a model are quantized to what precision, interact with loss scaling only through their effect on this quantity. \n\nOverall, we study the scaling of the effects of precision on loss as we vary data and parameters, both during and after training. We first study how the degradation induced by post-train quantization scales with parameters and data. We find that the degradation increases with data, so that for a fixed model, training on additional data after a certain point can be actively harmful if the model will be quantized after training. We then shift our focus to quantized training, examining both the quantization-aware-training (weights only) and low-precision training (weights, activations, attention all quantized) settings. Our scaling laws for pretraining suggest that the compute-optimal pretraining precision is in general independent of compute budget. Surprisingly, however, this independence ceases to be true if model size is constrained, in which case the compute-optimal precision grows slowly in compute. \n\nIn all, we pretrain a suite of 465 language models in 3 to 16 bit precisions, as well as post-train quantize each to multiple precisions. For a language model with N parameters, trained on D tokens with training precision P train , and post-train weight precision P post , we ultimately find a unified scaling law that takes the following form: (1) \n\nwhere A, B, E, \u03b1, \u03b2 are positive fitted constants, and \u03b4 PTQ refers to the loss degradation induced by post-training quantization before inference. Altogether, our results for post-train quantization illustrate how more pretraining FLOPs do not always lead to better models at inferencetime, and our results for low-precision pretraining suggest that both the standard practice of training models in 16-bit, and the race to extremely low (sub 4-bit) pretraining precision, may be suboptimal. \n\n2 Background, Related Work, and Setup \n\nNotation. Throughout, D denotes dataset size in tokens and N denotes model size in parameters.",
            "score": 0.46225221980715836,
            "section_title": "Introduction",
            "char_start_offset": 1986,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 380
                },
                {
                    "start": 383,
                    "end": 512
                },
                {
                    "start": 513,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 813
                },
                {
                    "start": 814,
                    "end": 1007
                },
                {
                    "start": 1008,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1298
                },
                {
                    "start": 1301,
                    "end": 1438
                },
                {
                    "start": 1439,
                    "end": 1649
                },
                {
                    "start": 1652,
                    "end": 1799
                },
                {
                    "start": 1800,
                    "end": 2143
                },
                {
                    "start": 2146,
                    "end": 2183
                },
                {
                    "start": 2186,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2280
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08990478515625
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "We display empirical trends for the test loss as a function of dataset size D (in tokens) and training compute C in Figure 1. \n\nFor the trend with D we trained a model with (n layer , n embd ) = (36, 1280) on fixed subsets of the WebText2 dataset. We stopped training once the test loss ceased to decrease. We see that the resulting test losses can be fit with simple power-law \n\nin the dataset size. The data and fit appear in Figure 1. \n\nThe total amount of non-embedding compute used during training can be estimated as C = 6N BS, where B is the batch size, S is the number of parameter updates, and the factor of 6 accounts for the forward and backward passes. Thus for a given value of C we can scan over all models with various N to find the model We compare generalization of converged models (points) to that of a single large model (dashed curves) as it trains. \n\nwith the best performance on step S = C 6BS . Note that in these results the batch size B remains fixed for all models, which means that these empirical results are not truly optimal. We will account for this in later sections using an adjusted C min to produce cleaner trends. \n\nThe result appears as the heavy black line on the left-hand plot in Figure 1. It can be fit with \n\nThe figure also includes images of individual learning curves to clarify when individual models are optimal. We will study the optimal allocation of compute more closely later on. The data strongly suggests that sample efficiency improves with model size, and we also illustrate this directly in Figure 19 in the appendix.",
            "score": 0.46224594348959136,
            "section_title": "Performance with Dataset Size and Compute",
            "char_start_offset": 16799,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 247
                },
                {
                    "start": 248,
                    "end": 306
                },
                {
                    "start": 307,
                    "end": 377
                },
                {
                    "start": 380,
                    "end": 400
                },
                {
                    "start": 401,
                    "end": 437
                },
                {
                    "start": 440,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 870
                },
                {
                    "start": 873,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1056
                },
                {
                    "start": 1057,
                    "end": 1150
                },
                {
                    "start": 1153,
                    "end": 1230
                },
                {
                    "start": 1231,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1360
                },
                {
                    "start": 1361,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1574
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0535888671875
        },
        {
            "corpus_id": "277467695",
            "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning",
            "text": "At lower inference-compute budgets, scaling solutions using Self-Consistency leads to better performance than scaling both solutions and verifications with GenRM. However, at higher budgets, GenRM surpasses Self-Consistency in performance. FT provides significant improvements over Self-Consistency (SC) on these harder problems, demonstrating its generalization ability, though it requires substantially more compute to outperform SC. (Right) Comparison of GenRM-Base versus SC for an RL-tuned QwQ-32B model. This confirms previous observations: SC performs better at lower budgets, while GenRM shines at higher budgets.We extrapolate the SC curves (dashed lines) because their performance saturates beyond a certain point. \n\nImpact of Problem Difficulty. When GenRM outperforms SC at higher inference budgets, we investigate which types of problems benefit the most. In Figure 3a, we analyze the relative improvement achieved by applying Best-of-N with GenRM-FT over Self-Consistency, computed as Improvement = (SR GenRM \u2212 SR SC )/SR SC . Specifically, we evaluate Llama-3.1-8B-Instruct on two difficulty levels from the MATH dataset: level 1 and level 5, which we denote as MATH (Easy) and MATH (Hard), respectively. Our results indicate that GenRM is particularly advantageous for more challenging problems, yielding up to a 30% relative improvement in performance. These findings can inform the choice between GenRM and SC, with GenRM offering greater benefits for harder problems. \n\nImpact of Verifier Quality. We also compare the performance of GenRM-FT against GenRM-Base at a fixed compute budget in Figure 3b. We find that GenRM-FT consistently outperforms GenRM-Base, requiring up to 16\u00d7 less compute to reach the same performance. This highlights the benefit of fine-tuning LLMs for verification, especially for smaller models with weaker instruction-following and reasoning capabilities. Additionally, this suggests that as the verification capabilities of LLMs improve, GenRM based methods might become more compute-efficient. More details are available in Appendix E. \n\nEasy-To-Hard Generalization. In practice, GenRM-FT may encounter unseen problems with higher difficulty than the ones seen during training [36].",
            "score": 0.4621045301930277,
            "section_title": "Takeaway",
            "char_start_offset": 19273,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 162
                },
                {
                    "start": 163,
                    "end": 239
                },
                {
                    "start": 240,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 724
                },
                {
                    "start": 727,
                    "end": 756
                },
                {
                    "start": 757,
                    "end": 868
                },
                {
                    "start": 869,
                    "end": 1040
                },
                {
                    "start": 1041,
                    "end": 1219
                },
                {
                    "start": 1220,
                    "end": 1369
                },
                {
                    "start": 1370,
                    "end": 1486
                },
                {
                    "start": 1489,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1619
                },
                {
                    "start": 1620,
                    "end": 1742
                },
                {
                    "start": 1743,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2040
                },
                {
                    "start": 2041,
                    "end": 2082
                },
                {
                    "start": 2085,
                    "end": 2113
                },
                {
                    "start": 2114,
                    "end": 2229
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0247955322265625
        },
        {
            "corpus_id": "270063468",
            "title": "gzip Predicts Data-dependent Scaling Laws",
            "text": "Early work established that a neural network's test error is a power law of training dataset size [Cortes et al., 1993], model parameter count [Rosenfeld et al., 2019], and that the relationship holds over many orders of magnitude [Hestness et al., 2017].Kaplan et al. [2020] applied scaling laws to transformer-based language models and identified a compute-optimal frontier along which parameter & dataset size should be scaled.Hoffmann et al. [2022] propose the Chinchilla scaling laws, finding that Kaplan et al. [2020] and Rae et al. [2021] overparameterized their models and that the compute-optimal frontier requires parameter & dataset size to be scaled equally (rather than parameters scaling at 3x the rate of data).Sorscher et al. [2022] find that we can reach exponential (rather than power law) scaling on dataset size by pruning out redundant examples that do not provide much information to learn from.Liu and Tegmark [2023] identify a mechanistic explanation of why scaling follows a power law on model width.Aghajanyan et al. [2023] extend the Chinchilla scaling laws to a variety of modalities such as speech, image-text, and code.Caballero et al. [2023] propose a novel functional form for scaling laws that better models complex non-monotonic behavior and also apply it to several modalities including code.Both these works and Hoffmann et al. [2022] (in Appendix C) find that scaling behavior for code is different than for natural language-code is easier to learn and its compute-optimal frontier prefers parameters slightly over data.Bi et al. [2024] cursorily investigate scaling laws across datasets of different qualities, finding that cleaner & higher quality data results in the \"model scaling exponent increasing\" because \"high-quality data usually implies logical clarity and less predictive difficulty after sufficient training\".However, none of these works identify an underlying general principle that explains why scaling behavior varies across data modalities & complexities (or even just between code and natural language).",
            "score": 0.4618270841688745,
            "section_title": "Scaling Laws",
            "char_start_offset": 3721,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 255,
                    "end": 430
                },
                {
                    "start": 430,
                    "end": 726
                },
                {
                    "start": 726,
                    "end": 917
                },
                {
                    "start": 917,
                    "end": 1025
                },
                {
                    "start": 1025,
                    "end": 1149
                },
                {
                    "start": 1149,
                    "end": 1327
                },
                {
                    "start": 1327,
                    "end": 1557
                },
                {
                    "start": 1557,
                    "end": 1860
                },
                {
                    "start": 1860,
                    "end": 2059
                }
            ],
            "ref_mentions": [
                {
                    "start": 98,
                    "end": 119,
                    "matchedPaperCorpusId": "1364116"
                },
                {
                    "start": 726,
                    "end": 748,
                    "matchedPaperCorpusId": "250113273"
                },
                {
                    "start": 1025,
                    "end": 1049,
                    "matchedPaperCorpusId": "255570036"
                },
                {
                    "start": 1149,
                    "end": 1172,
                    "matchedPaperCorpusId": "253117181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.239501953125
        },
        {
            "corpus_id": "251788140",
            "title": "Human Activity Recognition on Microcontrollers with Quantized and Adaptive Deep Neural Networks",
            "text": "In fact, as shown by our results of Section 5, large networks provide diminishing returns in terms of accuracy versus inference cost: they often achieve a little accuracy gain in exchange for an explosion in inference cycles (and hence latency and energy consumption). Consequently, a reduced-width version of the most accurate network would still be very complex, and would achieve a similar accuracy even on hard inputs, nullifying the principle of input-adaptive inference. On the other hand, selecting a too small/inaccurate   would make the corresponding sub-network   unable to correctly estimate output scores, rendering the SM policy inaccurate. Accordingly, we instead select   by computing the following \"gain\" metric   for each network in our Pareto frontier: \n\nwhere  (  ) is the classification score (accuracy or other metric) of the -th model, and  (  ) is its inference cost (latency or energy). The index  refers to the Pareto-optimal models ordered by increasing accuracy. We then select as   the model with the largest   , among those with an accuracy drop < 5% with respect to the most accurate network in the set. Lastly, we derive the small model   by activating either the first 25% or the first 50% channels/features in each layer of the network, and selecting, between the two, the version that yields the best accuracy versus cost trade-off, with the expected cost formulation of (3).",
            "score": 0.46134813322540297,
            "section_title": "Flexibility-oriented Optimization with Input-adaptive Variable-width CNNs",
            "char_start_offset": 46307,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 268
                },
                {
                    "start": 269,
                    "end": 476
                },
                {
                    "start": 477,
                    "end": 653
                },
                {
                    "start": 654,
                    "end": 770
                },
                {
                    "start": 773,
                    "end": 910
                },
                {
                    "start": 911,
                    "end": 989
                },
                {
                    "start": 990,
                    "end": 1133
                },
                {
                    "start": 1134,
                    "end": 1409
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00579833984375
        },
        {
            "corpus_id": "277994172",
            "title": "Learning Adaptive Parallel Reasoning with Language Models",
            "text": "We use SGLang (Zheng et al., 2024) for inference due to its high-performance and support for continuous batching and radix attention, which enables efficient running of APR. Baselines We compare our method against two baselines: search via long chain-of-thought reasoning (SoS+) and self-consistency selection (denoted cons@n), the standard parallel inference method for scaling test-time compute. The cons@n approach is implemented by independently sampling reasoning traces from SoS+, excluding outputs that fail to find a valid solution, then applying majority voting to the final search paths from the remaining outputs. The most frequently occurring solution is then selected as final answer. Additionally, we report pass@n, the rate at which at least one returned solution is correct, to illustrate the upper-bound performance achievable through repeated sampling with simple ensemble-based parallel inference.",
            "score": 0.4610582808865974,
            "section_title": "Experiments",
            "char_start_offset": 18855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 173
                },
                {
                    "start": 174,
                    "end": 397
                },
                {
                    "start": 398,
                    "end": 624
                },
                {
                    "start": 625,
                    "end": 697
                },
                {
                    "start": 698,
                    "end": 916
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01049041748046875
        },
        {
            "corpus_id": "273695243",
            "title": "Scaling LLM Inference with Optimized Sample Compute Allocation",
            "text": "We consider three baselines in our experimentsdefault pure allocation, optimal pure allocation, and uniform mixed allocation: \n\nDefault pure allocation is the one used to produce the leaderboard results on the benchmarks. These benchmarks usually run LLM inference once for each problem instance, with a very basic prompt and a low temperature for reproducibility and fair comparison. Therefore, the default pure allocation is not optimized for scaling up. \n\nOptimal pure allocation is the one that has the highest pass@C 0 on the training set given a compute budget of C 0 . Since the actual C is larger than C 0 in most cases, we use pass@C 0 to select the optimal configuration. Comparison between the optimal pure allocation and the default pure allocation indicates whether it is necessary to search for a good set of inference hyperparameters, which is what existing hyperparameter optimization techniques do. \n\nUniform mixed allocation naively distributes the compute budget evenly to every sampling configuration h i in H. We compare the uniform mixed allocation with our learned mixed allocation to examine whether it is necessary to optimize sample compute allocations for them to perform well.",
            "score": 0.46096033435286077,
            "section_title": "Baselines",
            "char_start_offset": 11916,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 125
                },
                {
                    "start": 128,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 456
                },
                {
                    "start": 459,
                    "end": 575
                },
                {
                    "start": 576,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 915
                },
                {
                    "start": 918,
                    "end": 1030
                },
                {
                    "start": 1031,
                    "end": 1204
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0293731689453125
        },
        {
            "corpus_id": "219955888",
            "title": "An Integer Linear Programming Framework for Mining Constraints from Data",
            "text": "There are two main factors affecting the running time: the number of data samples, the number of constraints K, and the size of the output variables D. Our approach contains two steps: identifying the feasible set (training) and solving ILPs (inference). In training, as shown in Sec 3, our approach is linear in K since we only needs to pass all samples once, and no worse than quadratic in D. In inference, we solve ILP which is generally NP-hard, and the main factor in complexity is D. However, for most structured prediction tasks, D is small. In our experiments, D is 21 and 729 and 96 in MST, Sudoku and HMC, respectively. \n\nTable 4 shows the training and test running time of our approaches compared to neural network models. In training, our approach is more efficient than the baseline neural network in the Sudoku and MST experiments. For the HMC experiment, the training time of our approach includes updating the model parameters of the underlying neural networks. Therefore, the training time is longer compared to the Sudoku and MST cases. \n\nFor the inference time, we report the average time on solving one test sample. Despite ILP is NP-hard, a commercial solver (e.g., Gurobi) is capable of solving the problem within a reasonable time. Therefore, without carefully engineering to optimize the running time, the ILP solver can produce solutions within a few seconds. \n\nTo empirically understand the scalability of our approach in the inference, we test the inference time in the MST experiment with larger graph. Here we fix the number of constraints (number of training sampels) as 20, 000. The results are shown in Fig. 4. We find that despite some extreme cases, the inference time grows generally linearly in terms of the number of variables empirically. \n\nNote that although the constraints are mined using the ILP framework, it does not mean that the inference has to be solved by an ILP solver. Once the constraints are identified, one can design a specific constraint solver to speed up the inference. Besides, the ILP inference can be accelerated Results are on MST. The x-axis is the allowed maximum relative gap between the returned solution and the optimum solution.",
            "score": 0.4607890591780397,
            "section_title": "Discussion about Running Time",
            "char_start_offset": 29956,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 254
                },
                {
                    "start": 255,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 629
                },
                {
                    "start": 632,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 845
                },
                {
                    "start": 846,
                    "end": 977
                },
                {
                    "start": 978,
                    "end": 1054
                },
                {
                    "start": 1057,
                    "end": 1135
                },
                {
                    "start": 1136,
                    "end": 1254
                },
                {
                    "start": 1255,
                    "end": 1384
                },
                {
                    "start": 1387,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1776
                },
                {
                    "start": 1779,
                    "end": 1919
                },
                {
                    "start": 1920,
                    "end": 2027
                },
                {
                    "start": 2028,
                    "end": 2093
                },
                {
                    "start": 2094,
                    "end": 2196
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015838623046875
        },
        {
            "corpus_id": "266693796",
            "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws",
            "text": "Large language models (LLMs) have substantial training and inference compute and energy costs (Knight, 2023;Pope et al., 2022). Training computation costs are primarily determined by the size of the model and the amount of data seen during training (Hoffmann et al., 2022). For state-ofthe-art models with tens of billions of parameters trained on trillions of tokens, training costs can easily exceed millions of dollars. Similarly, inference costs depend on the size of the model and the volume of user queries over the lifetime Proceedings of the 41 st International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). of the model. This volume can be significant; demand for popular models can exceed billions of tokens per day (OpenAI & Pilipiszyn, 2021;Shazeer & Freitas, 2022). \n\nAccounting for both training and inference, how does one minimize the cost required to produce and serve a high quality model? \n\nRecent studies have proposed scaling laws, empirical formulas that estimate how changes in model and training data size impact model quality (Kaplan et al., 2020;Hoffmann et al., 2022). Hoffmann et al. (2022) is perhaps the most influential of these works, finding that to scale language models most efficiently, parameters and tokens should grow approximately linearly. The authors applied this scaling law to train a 70B parameter model (dubbed Chinchilla) that outperformed much larger and more expensive models such as GPT-3. As a result, many subsequent LLMs have been trained following the Chinchilla scaling laws (Dey et al., 2023;Muennighoff et al., 2023). \n\nHowever, the Chinchilla scaling laws only account for the computational costs of training. By contrast, the Llama 2 family of models were trained on 2 trillion tokens and the Llama 3 family of models were trained on 15 trillion tokens, which is far more data than the Chinchilla scaling laws would deem \"optimal\" (Touvron et al., 2023a;b;AI@Meta, 2024).",
            "score": 0.4605042853606086,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 127
                },
                {
                    "start": 128,
                    "end": 273
                },
                {
                    "start": 274,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 830
                },
                {
                    "start": 833,
                    "end": 959
                },
                {
                    "start": 962,
                    "end": 1147
                },
                {
                    "start": 1148,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1626
                },
                {
                    "start": 1629,
                    "end": 1719
                },
                {
                    "start": 1720,
                    "end": 1982
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1181640625
        },
        {
            "corpus_id": "276259519",
            "title": "When More is Less: Understanding Chain-of-Thought Length in LLMs",
            "text": "Furthermore, Li et al. [2024a] show that fast thinking without CoT results in larger gradients and greater gradient differences across layers compared to slow thinking with detailed CoT, highlighting the improved learning stability provided by the latter. Ye et al. [2024] investigate CoT in a controlled setting by training GPT-2 models on a synthetic GSM dataset, revealing hidden mechanisms through which language models solve mathematical problems. Unlike these theoretical studies, our work focuses on the impact of different lengths of CoT on final performance and tries to understand CoT from task decomposition and error accumulation perspective. \n\nOverthinking With the remarkable success of OpenAI's o1 model, test-time computation scaling has become increasingly important. More and more works [Snell et al., 2024, Chen et al., 2024d, Wu et al., 2024, Brown et al., 2024] have explored the scaling laws during inference using various methods, such as greedy search, majority voting, best-of-n, and their combinations. They concluded that with a compute-optimal strategy, a smaller base model can achieve non-trivial success rates, and test-time compute can outperform larger models. This highlights the importance of designing optimal inference strategies. However, Chen et al. [2024a] hold a contrastive opinion that in some cases, the performance of the Best-of-N method may decline as N increases. Similarly, the overthinking phenomenon [Chen et al., 2024c] becomes more and more important as o1-like reasoning models allocate excessive computational resources to simple problems (e.g., 2 + 3 = 5) with minimal gains. These findings indicate the need to balance computation based on model capabilities and task difficulty. In our study, we focus on different types of CoT reasoning, categorized by CoT length. Moreover, we theoretically identify a balanced CoT strategy that adapts to model size and task difficulty, optimizing performance under these constraints. To begin, we aim to empirically investigate the relationship between reasoning performance and CoT length. Therefore, we need to control a given model to generate reasoning chains of varying lengths for a specific task. Unfortunately, no existing real-world dataset or model fully meets these strict requirements.",
            "score": 0.4602757868349039,
            "section_title": "Related Work",
            "char_start_offset": 5889,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 255
                },
                {
                    "start": 256,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 654
                },
                {
                    "start": 657,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 1028
                },
                {
                    "start": 1029,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1411
                },
                {
                    "start": 1412,
                    "end": 1631
                },
                {
                    "start": 1632,
                    "end": 1736
                },
                {
                    "start": 1737,
                    "end": 1823
                },
                {
                    "start": 1824,
                    "end": 1978
                },
                {
                    "start": 1979,
                    "end": 2085
                },
                {
                    "start": 2086,
                    "end": 2198
                },
                {
                    "start": 2199,
                    "end": 2292
                }
            ],
            "ref_mentions": [
                {
                    "start": 1277,
                    "end": 1296,
                    "matchedPaperCorpusId": "276184932"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06610107421875
        },
        {
            "corpus_id": "269982407",
            "title": "Surge Phenomenon in Optimal Learning Rate and Batch Size Scaling",
            "text": "Prior research has explored methods to determine an optimal learning rate according to the batch size in scenarios utilizing SGD optimizers, including square root scaling [22], linear scaling [19,23], and others [24]. Among these, the empirical model focusing on large batch training [24] yields convincing results in both theoretical and empirical contexts, proposing the following rule to depict the relationship between the optimal learning rate and the batch size: \n\nFor Adam-style optimizers, though existing works also provide some approximation [24,23], they fail to capture the true scaling law of optimal learning rates with batch sizes. For illustrative purposes, Figure 1 presents curves that simulate the optimal learning rates for the Adam optimizer. We find that, in scenarios involving small batch sizes, the optimal learning rate initially increases and then decreases, resembling a surge in the sea, as depicted by the dashed orange line. For large batch sizes, we identify a value to which the optimal learning rate converges. The solid orange line represents a schematic based on our findings for both small and large batch sizes, showing that the learning rate tends to rise initially, then decrease, and subsequently gradually ascend to asymptotically approach a stable value. For clarity in visualization, we have omitted the final asymptotic portion of the curve. \n\nIn this paper, we aim to elucidate and formalize the connection between optimal learning rates and batch sizes for Adam-style optimizers. By following the notation from the empirical model [24] and conducting a more in-depth theoretical analysis, we discover that the relationship between the optimal learning rate and the batch size in the above parameter update formular satisfies: \n\nwhich differs from SGD, especially when the batch size is not too large. Here the meaning of B noise is consistent with papers of scaling laws [24,25], representing the trade-off point between training speed and data efficiency. When the batch size is equal to B noise , the optimal learning rate reaches a local maximum in accordance with Eq 2. Furthermore, we provide additional proof that when the batch size becomes significantly large, the optimal learning rate gradually converges to a non-zero value.",
            "score": 0.4602717027664896,
            "section_title": "Introduction",
            "char_start_offset": 1837,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 217
                },
                {
                    "start": 218,
                    "end": 468
                },
                {
                    "start": 471,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 955
                },
                {
                    "start": 956,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1297
                },
                {
                    "start": 1298,
                    "end": 1386
                },
                {
                    "start": 1389,
                    "end": 1526
                },
                {
                    "start": 1527,
                    "end": 1772
                },
                {
                    "start": 1775,
                    "end": 1847
                },
                {
                    "start": 1848,
                    "end": 2003
                },
                {
                    "start": 2004,
                    "end": 2282
                }
            ],
            "ref_mentions": [
                {
                    "start": 196,
                    "end": 199,
                    "matchedPaperCorpusId": "226281826"
                },
                {
                    "start": 556,
                    "end": 559,
                    "matchedPaperCorpusId": "226281826"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0517578125
        },
        {
            "corpus_id": "276421468",
            "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines",
            "text": "Large-scale adoption of neural models requires careful determination of several key factors: \n\n\u2022 Model size vs. data size: How should the number of parameters and dataset size be balanced to achieve optimal performance? \u2022 Compute budget: Given a fixed computational budget, how can training and inference be optimized for efficiency? \u2022 Training strategy: Should longer training duration be prioritized over increased model size, especially in data-constrained environments? \u2022 Architecture choice: Should the model be dense or sparse (e.g., MoE) to optimize performance for a given budget? \u2022 Inference efficiency: What test-time strategies, such as retrieval augmentation or tree search, can be employed to enhance model output quality? \u2022 Quantization and pruning: How can quantization and structured sparsity be used to minimize memory footprint while maintaining accuracy? Based on the selections of the above choices, one can follow the steps highlighted in Figure 4 for developing large-scale neural models. Figure 4a illustrates that for developing large-scale models with multimodal capabilities, multimodal scaling laws (Aghajanyan et al., 2023) can be followed. For developing domain-specific nuanced application, one can follow D-CPT law (Que et al., 2024), which provides a comprehensive guideline for domain-continuous pre-training. For developing general-purpose LLMs, one may need to follow Kaplan/Chinchilla scaling laws, if sufficient data is available and the model choice is dense; otherwise, MoE scaling laws (Krajewski et al., 2024) be utilized to develop sparser models. If sufficient data is not available, one can either train the model for longer epochs, following Sardana et al. (2024), or use data mixing strategies (Ye et al., 2024) for deriving optimal data composition for pre-training the model. \n\nPost-training, inference strategies (c.f. Figure 4b) can be followed to ensure that the model is utilized efficiently for the downstream applications. If the high memory consumption of the model is a concern, compression strategies like pruning (Chen et al., 2024b) or low-precision quantization (Cao et al., 2024) can be used.",
            "score": 0.4602198674547666,
            "section_title": "Practical guidelines for utilizing scaling laws",
            "char_start_offset": 20852,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 92
                },
                {
                    "start": 95,
                    "end": 219
                },
                {
                    "start": 220,
                    "end": 333
                },
                {
                    "start": 334,
                    "end": 473
                },
                {
                    "start": 474,
                    "end": 588
                },
                {
                    "start": 589,
                    "end": 735
                },
                {
                    "start": 736,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1010
                },
                {
                    "start": 1011,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1342
                },
                {
                    "start": 1343,
                    "end": 1589
                },
                {
                    "start": 1590,
                    "end": 1823
                },
                {
                    "start": 1826,
                    "end": 1976
                },
                {
                    "start": 1977,
                    "end": 2153
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10650634765625
        },
        {
            "corpus_id": "276421903",
            "title": "Revisiting the Test-Time Scaling of o1-like Models: Do they Truly Possess Test-Time Scaling Capabilities?",
            "text": "The success of o1 has ushered in a new scaling paradigm, test-time compute scaling, which enables continuous improvements in model performance by increasing computational expenditure during inference (OpenAI, 2024a;b). Currently, scaling test-time compute can be approached in two dimensions: parallel scaling and sequential scaling (Snell et al., 2024;Zeng et al., 2024). \n\nParallel Scaling Parallel scaling typicallly samples multiple solutions in parallel and pick one according to some guidence signal like reward. Notable examples of parallel scaling include Bestof-N Search (Cobbe et al., 2021;Sun et al., 2024;Gui et al., 2024;Amini et al., 2024;Sessa et al., 2024), which is based on a reward model (Cobbe et al., 2021;Lightman et al., 2024), and Majority Vote (Wang et al., 2023), which exploits model uncertainty. The primary distinction between these approaches lies in the method used to select the final solution or answer after sampling multiple candidates. Both Best-of-N Search and Majority Vote are parallel scaling techniques at the solution level, while Tree-Search algorithms can be viewed as parallel scaling at the token or step level. Beam-Search (Qiu et al., 2024;Yu et al., 2024;Xie et al., 2023;Kool et al., 2019) and MCTS (Hao et al., 2023;Wan et al., 2024;Chen et al., 2024a;Zhang et al., 2023) are classic examples of Tree-Search algorithms. All parallel scaling methods rely on guidance signals to select the optimal token, step, or solution from a set of candidates. \n\nSequential Scaling Sequential scaling enhances test-time computation by generating progressively longer solutions along the sequence dimension. The most prevalent method of sequential scaling is Self-Revision, where Madaan et al. (2023) first generate an initial response and then iteratively evaluate and refine it based on self-assessment.",
            "score": 0.4601183723684987,
            "section_title": "Related Work",
            "char_start_offset": 4866,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 218
                },
                {
                    "start": 219,
                    "end": 372
                },
                {
                    "start": 375,
                    "end": 518
                },
                {
                    "start": 519,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 971
                },
                {
                    "start": 972,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1370
                },
                {
                    "start": 1371,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1643
                },
                {
                    "start": 1644,
                    "end": 1841
                }
            ],
            "ref_mentions": [
                {
                    "start": 215,
                    "end": 217,
                    "matchedPaperCorpusId": "257427177"
                },
                {
                    "start": 727,
                    "end": 749,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 769,
                    "end": 788,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 1188,
                    "end": 1204,
                    "matchedPaperCorpusId": "265221057"
                },
                {
                    "start": 1204,
                    "end": 1221,
                    "matchedPaperCorpusId": "258426922"
                },
                {
                    "start": 1221,
                    "end": 1239,
                    "matchedPaperCorpusId": "76662039"
                },
                {
                    "start": 1249,
                    "end": 1267,
                    "matchedPaperCorpusId": "258865812"
                },
                {
                    "start": 1267,
                    "end": 1284,
                    "matchedPaperCorpusId": "263310590"
                },
                {
                    "start": 1303,
                    "end": 1322,
                    "matchedPaperCorpusId": "257427177"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1820068359375
        },
        {
            "corpus_id": "56262183",
            "title": "An Empirical Model of Large-Batch Training",
            "text": "In order to test our model we train each task on a range of batch sizes, selecting the optimal constant learning rate separately for each batch size using a simple grid search. Across a range of tasks, we produce the following results and compare with our model: \n\n\u2022 Optimal learning rates: When optimizing with plain SGD or momentum, we find that the optimal learning rate follows the functional form of Equation 2.6, as shown in Figure 5. For Adam and RMSProp the optimal learning rate initially obeys a power law (B) \u221d B \u03b1 with \u03b1 between 0.5 and 1.0 depending on the task, then becomes roughly constant. The scale at which the optimal learning rate stops increasing is generally somewhat smaller than the typical noise scale. (See Appendix E.2 for a potential explanation for this power law behavior.) \n\n\u2022 Pareto frontiers: For each batch size, we observe the number of optimization steps and total number of data samples needed to achieve various levels of performance. This allows us to visualize the tradeoff between time-efficiency and compute-efficiency as a Pareto frontier (see Figures 6 and 7). We find that Equation 2.11 fits the shape of these tradeoff curves remarkably well in most cases. \n\n\u2022 Critical batch size (B crit ): We determine the critical batch size over the course of a training run by fitting the Pareto fronts to the functional form of Equation 2.11 (see Figure 7). This quantifies the point at which scaling efficiency begins to drop. In particular, training runs at batch sizes much less than B crit behave similarly per training example, while training runs at batch sizes much larger than B crit behave similarly per optimization step (see Figure 6). The critical batch size typically increases by an order of magnitude or more over the course of training. \n\n\u2022 Simple noise scale (B simple ): We measure the simple noise scale of Equation 2.9 over the course of a single training run using the minimal-overhead procedure described in Appendix A.1. Note that some care must be taken to obtain a proper estimate of B simple due to its dependence on the learning rate via the 'temperature' of training.",
            "score": 0.4599103920673564,
            "section_title": "Quantities Measured",
            "char_start_offset": 25217,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 262
                },
                {
                    "start": 265,
                    "end": 440
                },
                {
                    "start": 441,
                    "end": 606
                },
                {
                    "start": 607,
                    "end": 728
                },
                {
                    "start": 729,
                    "end": 804
                },
                {
                    "start": 807,
                    "end": 973
                },
                {
                    "start": 974,
                    "end": 1105
                },
                {
                    "start": 1106,
                    "end": 1203
                },
                {
                    "start": 1206,
                    "end": 1394
                },
                {
                    "start": 1395,
                    "end": 1464
                },
                {
                    "start": 1465,
                    "end": 1683
                },
                {
                    "start": 1684,
                    "end": 1789
                },
                {
                    "start": 1792,
                    "end": 1980
                },
                {
                    "start": 1981,
                    "end": 2132
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "276107826",
            "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
            "text": "Test-time compute. Many of the recent advances in language model reasoning capabilities can be traced to increasing use of test-time compute. Inference strategies like chain-of-thought reasoning [Wei et al., 2022], tree-of-thoughts [Yao et al., 2023] and self-critique [Valmeekam et al., 2023] result in improved reasoning performance at the cost of forming longer responses. Reinforcement learning has emerged as a particularly successful strategy for effectively leveraging more test-time compute, wherein models learn from exploration to form lengthy chain-of-thought outputs that incorporate backtracking and search, despite not being explicitly taught to do so [OpenAI, 2024, Team, 2025]. Inference-time model adaptation, whether through many-shot learning [Agarwal et al., 2024, Anil et al., 2024] or finetuning [Aky\u00fcrek et al., 2024], provides another avenue when training data is available. We study sampling-based search: obtain a set of candidate responses from a model and apply an aggregation method to select a response, such as self-consistency/plurality voting [Wang et al., 2023] or selecting a response with a reward/verifier model [Cobbe et al., 2021]. These various methods for scaling test-time compute are complementary; for example, sampling-based search can also be used on models trained to produce longer outputs. We note that it is possible for models trained to produce long chains of thought to perform something resembling samplingbased search internally, in which case we still expect our observed scaling trends to hold. However, we also expect explicit sampling-based search will remain indispensable, due to its greater parallelism and robustness than internally implemented search. Table 7: Accuracy rates of commercial language models on our verification benchmark. For the task of response scoring (Scoring Accuracy), accuracy rates are broken down for entries that require identifying a correct response as being correct (Correct), entries that require identifying a wrong response as being wrong (Wrong), and entries that require identifying a wrong response that coincidentally reaches the correct answer as being wrong (Flawed).",
            "score": 0.45926239716072215,
            "section_title": "Related Work",
            "char_start_offset": 24920,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 141
                },
                {
                    "start": 142,
                    "end": 375
                },
                {
                    "start": 376,
                    "end": 693
                },
                {
                    "start": 694,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1338
                },
                {
                    "start": 1339,
                    "end": 1551
                },
                {
                    "start": 1552,
                    "end": 1715
                },
                {
                    "start": 1716,
                    "end": 1800
                },
                {
                    "start": 1801,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 195,
                    "end": 213,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 232,
                    "end": 250,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1076,
                    "end": 1095,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.031982421875
        },
        {
            "corpus_id": "270392261",
            "title": "Training Compute-Optimal Protein Language Models",
            "text": "We find that the scaling behaviors of sparse parameter counts in Mixture of Expert (MoE) models are remarkably similar to those of dense model sizes, potentially allowing for a reduced compute budget for modeling scaling behaviors due to less activated parameters per token. \n\nIn our experiments, we evaluate MoE models ranging from 10M to 500M sparse parameter counts, using a model size of 17 with eight experts, following the settings outlined in Mixtral of experts [40], including its load-balancing scheme. The figure below shows different IsoFLOPs curves. Notably, the FLOPs here are calculated based on sparse parameters rather than actually activated ones. We use the method described in the main text to select optimal loss points and fit these around the sample points, enabling us to project the optimal model size and number of tokens for larger models (center and right). We observe that the power-law coefficients for CLM and MLM are similar to those of dense models, with MoE CLM vs. Dense CLM at approximately 0.57 vs. 0.58, and MoE MLM vs. Dense MLM at 0.74 vs. 0.77. \n\nOur study strictly focuses on models with eight experts, which may not be entirely rigorous. Clark et al. [17] proposed a unified scaling law defining effective training parameters for MoE, aiming to harmonize the scaling laws for Dense and MoE models. Investigation of biological data will be considered as future work.",
            "score": 0.4590041072292598,
            "section_title": "I MoE Scaling",
            "char_start_offset": 32862,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 274
                },
                {
                    "start": 277,
                    "end": 511
                },
                {
                    "start": 512,
                    "end": 561
                },
                {
                    "start": 562,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 884
                },
                {
                    "start": 885,
                    "end": 1084
                },
                {
                    "start": 1087,
                    "end": 1179
                },
                {
                    "start": 1180,
                    "end": 1339
                },
                {
                    "start": 1340,
                    "end": 1407
                }
            ],
            "ref_mentions": [
                {
                    "start": 1193,
                    "end": 1197,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03753662109375
        },
        {
            "corpus_id": "276961790",
            "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
            "text": "Used both to forecast performance for early pretraining decisions as well as decide on the optimal trade-off between parameters and pretraining dataset size given a particular compute budget, scaling laws (Kaplan et al., 2020, i.a.) have played an important role in the development of large language models (LLMs). Famously, with a series of experiments with models with different data/parameter trade-offs, Hoffmann et al. (2022) showed that previous LLMs were all erring on the side of too many parameters, causing a shift in the amount of training tokens to train LLMs. More recently, Dubey et al. (2024) used scaling laws not only to determine the optimal parameter count given their available compute budget, but also to forecast the impact of data selection decisions on evaluation scores. \n\nIn these works, the compute optima (COs), describing the optimal parameter count and number of training tokens, are selected based on aggregate performance estimators (APEs), in the form of negative log-likelihood (NLL) on a validation set not part of the pretraining corpus. Little is known, however, about whether the COs of individual skills such as mathematical reasoning, question answering (QA), or coding, align with these APE COs. While some studies use scaling laws to predict how downstream task performance improves with scale (e.g. Ye et al., 2025;Held et al., 2025), none of these studies cover whether COs themselves may be skill dependent. Is it possible that some skills are more data-hungry, whereas others benefit more from extra parameters? If so, how should that impact model training and training data selection? \n\nIn this paper, with an extensive set of experiments across 9 different compute scales and 2 skills as measured with 19 datasets across two different splits, we study exactly that. Specifically, we focus on the three research questions: \n\nR1. Are COs skill dependent? First, we consider how the IsoFLOP curves and corresponding COs for code1 -and knowledge-based skills compare to the APE COs, given a canonical datamix. Across the board, we find pronounced differences between the COs for these different skills: where knowledge QA tasks are capacity-hungry, code tasks instead prefer data.",
            "score": 0.4588316269821653,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 314
                },
                {
                    "start": 315,
                    "end": 572
                },
                {
                    "start": 573,
                    "end": 795
                },
                {
                    "start": 798,
                    "end": 1073
                },
                {
                    "start": 1074,
                    "end": 1236
                },
                {
                    "start": 1237,
                    "end": 1341
                },
                {
                    "start": 1342,
                    "end": 1452
                },
                {
                    "start": 1453,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1631
                },
                {
                    "start": 1634,
                    "end": 1813
                },
                {
                    "start": 1814,
                    "end": 1869
                },
                {
                    "start": 1872,
                    "end": 1900
                },
                {
                    "start": 1901,
                    "end": 2053
                },
                {
                    "start": 2054,
                    "end": 2224
                }
            ],
            "ref_mentions": [
                {
                    "start": 1342,
                    "end": 1358,
                    "matchedPaperCorpusId": "268681464"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.371826171875
        },
        {
            "corpus_id": "277781065",
            "title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
            "text": "Given a fixed time budget, M1 can generate more sequences or longer sequences compared to a transformer model, which can hopefully boost its performance. We evaluate the effect of test-time compute scaling on model performance. We scale both the number of samples generated as well as the length of generated samples, to see if M1 benefits from additional compute along these axes. We aim to investigate whether the speed benefit from section 4.2 can translate into an accuracy gain. \n\nScaling with majority vote. \n\nThe left side of Figure 3 shows the effect of scaling the number of generated samples (while fixing the maximum decoding length) on AIME24 accuracy. Both the baseline model and M1 see increasing accuracy as the number of samples increases, with M1 nearly matching the baseline performance for larger sample sizes. The efficient generation of M1 also means that generating large number of samples at test-time is faster than for the baseline transformer model. \n\nWe quantify this efficiency in the right side of Figure 3, which compares the number of seconds spent generating samples against the resulting accuracy. To compute the time values on the x-axis, we find an optimal throughput value (in tokens per second) for each model by increasing batch sizes until throughput decreases. The optimal values were 7263 T/s for DeepSeek-R1-Distill-Qwen-1.5B, and 15169 T/s for M1. We then assume that each generated sample is maximum length (8K), and compute the seconds required for one sample from one model as 8K divided by the throughput. We then convert the left graph of Figure 3 into the right graph, by multiplying the number of samples for each datapoint by the seconds required per sample for each model. As an example, M1 requires roughly a half second (8K/15K) per sample, so the accuracy value for M1 at 32 samples on the left graph appears at approximately 16 seconds on the right graph.",
            "score": 0.4587101310153184,
            "section_title": "Test-Time Scaling",
            "char_start_offset": 20144,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 153
                },
                {
                    "start": 154,
                    "end": 227
                },
                {
                    "start": 228,
                    "end": 381
                },
                {
                    "start": 382,
                    "end": 483
                },
                {
                    "start": 486,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 664
                },
                {
                    "start": 665,
                    "end": 829
                },
                {
                    "start": 830,
                    "end": 975
                },
                {
                    "start": 978,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1300
                },
                {
                    "start": 1301,
                    "end": 1390
                },
                {
                    "start": 1391,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1724
                },
                {
                    "start": 1725,
                    "end": 1911
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.019195556640625
        },
        {
            "corpus_id": "273877502",
            "title": "Scaling Laws for Pre-training Agents and World Models",
            "text": "Similar to the initial study of scale in LLMs, we focus on the effect of scaling on a generative pre-training loss (rather than on downstream agent performance, or rewardor representation-centric objectives), in the infinite data regime, on a fixed offline dataset. Under this setting, we train families of transformers on next-token prediction tasks using architectures popular in both world modeling and BC tasks. This leads to several contributions, summarized in Figure 1. \n\n\u2022 Scaling laws similar to those in LLMs can be observed in world modeling with tokenized observations and actions (Section 4.1, Figure 1a). \u2022 The optimal trade-off between model and dataset size in world modeling is influenced by the tokenizer's compression rate (number of tokens per observation) (Section 4.1, Figure 1a & b). Figure 1: This paper observes that scaling laws, as originally found in LLMs, also emerge in the tasks of world modeling and BC, when studying the pre-training loss on large datasets of human behavior. a, b) For world modeling, the power law coefficient determining optimal model size is affected by the compression rate of the tokenizer. c) In BC with tokenized image observations (BC-Token), small models need a large FLOPs budget to saturate, making these scaling laws less clear cut. d) However, moving to a single continuous embedding per observation remedies this (BC-CNN), producing prototypical scaling laws and a more balanced optimal model size coefficient. \n\n\u2022 Scaling laws for BC with tokenized observations are harder to observe under modest compute budgets. The optimal trade-off favors smaller models and more data (Section 4.2, Figure 1c). \n\n\u2022 Scaling laws similar to those in LLMs can once again be observed in BC with one continuous encoding per observation (Section 4.2, Figure 1d). \n\n\u2022 Our findings can be understood through small-scale language modeling experiments (Section 5). \n\nOrganization. Section 2 provides detailed related work, contrasting the current understanding of scaling in embodied AI with other domains. Section 3 introduces details for our main experiments, including the architectures considered and details of scaling laws analyses. Section 4.1 & 4.2 present our main results in world modeling and BC respectively.",
            "score": 0.45860632292143044,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1855,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 265
                },
                {
                    "start": 266,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 476
                },
                {
                    "start": 479,
                    "end": 618
                },
                {
                    "start": 619,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1294
                },
                {
                    "start": 1295,
                    "end": 1474
                },
                {
                    "start": 1477,
                    "end": 1578
                },
                {
                    "start": 1579,
                    "end": 1662
                },
                {
                    "start": 1665,
                    "end": 1808
                },
                {
                    "start": 1811,
                    "end": 1906
                },
                {
                    "start": 1909,
                    "end": 1922
                },
                {
                    "start": 1923,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2180
                },
                {
                    "start": 2181,
                    "end": 2262
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0496826171875
        },
        {
            "corpus_id": "277954852",
            "title": "Think Deep, Think Fast: Investigating Efficiency of Verifier-free Inference-time-scaling Methods",
            "text": "Our comprehensive analysis reveals critical insights into the performance of reasoning and non-reasoning models under various inference-time scaling methods. Figure 1 demonstrates a compelling finding: non-reasoning models, even when enhanced by advanced inference-time scaling techniques, consistently underperform compared to reasoning models. Specifically, we observed that R1-Distilled versions of Llama-3.3-70B significantly outperform their original Instruct counterparts. Despite employing sophisticated inferencetime scaling methods, non-reasoning models fail to match the performance of purpose-built reasoning models. This empirical evidence suggests that for compute-optimal approaches, investing in training specialized reasoning models may provide substantially better longterm efficiency compared to repeated inference-time scaling of general models. Note that in Figure 1, we added another baseline \"reasoning truncation\" by simply truncating the reasoning process (encapsulated in \"\u27e8 think\u27e9\" \"\u27e8/think\u27e9\" tokens) and then extended the response to the end by sequence completions. We found that this can significantly degrade the response quality, and extreme truncation eventually crossed the curve of non-reasoning models.",
            "score": 0.4582117947114074,
            "section_title": "Non-reasoning models augmented by inference-time methods cannot match reasoning models",
            "char_start_offset": 14632,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 345
                },
                {
                    "start": 346,
                    "end": 415
                },
                {
                    "start": 416,
                    "end": 478
                },
                {
                    "start": 479,
                    "end": 627
                },
                {
                    "start": 628,
                    "end": 864
                },
                {
                    "start": 865,
                    "end": 1093
                },
                {
                    "start": 1094,
                    "end": 1237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01036834716796875
        },
        {
            "corpus_id": "272368345",
            "title": "Path-Consistency: Prefix Enhancement for Efficient Inference in LLM",
            "text": "Self-consistency in large language models (LLMs) involves generating multiple branches of reasoning, each potentially leading to different answers. The final answer is determined through aggregation methods, such as majority voting. Unlike greedy decoding, self-consistency avoids the shortcomings of local optimality, and multiple samplings reduce the randomness of single-step sampling. The proportion of tokens generated by selfconsistency on correct or incorrect inference paths. In most tasks, self-consistency wastes over 25%, or even 50%, of tokens on incorrect branches, resulting in significant additional costs. \n\nHowever, the primary drawback of self-consistency is the significant computational redundancy. With N branches, each answer is derived from N similar but independent inference processes, leading to an N -fold increase in computational cost compared to greedy decoding for a single problem. To improve self-consistency, the goal is to achieve similar sampling effects while reducing the time cost of redundant computations. \n\nWe propose an intuitive hypothesis: for example, a particular mathematical problem might have five different reason-ing paths, p 1 to p 5 . If the model frequently errs on paths p 1 to p 3 while p 4 and p 5 are relatively simpler, then full self-consistency wastes significant computational resources on the problematic paths. As shown in Figure 2, statistics on the number of tokens generated during self-consistency across various datasets reveal that over 25%, and sometimes even 50%, of tokens are wasted on incorrect branches. By sampling multiple times only on p 4 and p 5 , we could enhance resource utilization and improve output accuracy. Furthermore, storing limited information from paths p 4 and p 5 to guide subsequent branch generation could significantly accelerate inference speed and efficiency. \n\nAdditionally, self-consistency involves extensive redundant processes without yielding intermediate results, with the final answer only emerging at the end. If useful information could be identified early in the generation process to guide subsequent branching, outcomes might improve. Intuitively, when tackling complex problems, using simple criteria to preliminarily assess the quality or correctness of the current generation during intermediate stages can enhance the effectiveness of subsequent steps. Our method aims to reduce the time wasted on incorrect branches while increasing the efficiency of generating correct inference paths.",
            "score": 0.4574289726072801,
            "section_title": "Characterization of Self-Consistency",
            "char_start_offset": 5844,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 147
                },
                {
                    "start": 148,
                    "end": 232
                },
                {
                    "start": 233,
                    "end": 388
                },
                {
                    "start": 389,
                    "end": 483
                },
                {
                    "start": 484,
                    "end": 621
                },
                {
                    "start": 624,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 913
                },
                {
                    "start": 914,
                    "end": 1046
                },
                {
                    "start": 1049,
                    "end": 1188
                },
                {
                    "start": 1189,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1696
                },
                {
                    "start": 1697,
                    "end": 1861
                },
                {
                    "start": 1864,
                    "end": 2020
                },
                {
                    "start": 2021,
                    "end": 2149
                },
                {
                    "start": 2150,
                    "end": 2371
                },
                {
                    "start": 2372,
                    "end": 2506
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0041351318359375
        },
        {
            "corpus_id": "276249117",
            "title": "MATH-Perturb: Benchmarking LLMs' Math Reasoning Abilities against Hard Perturbations",
            "text": "In this subsection, we investigate the inference-time scaling behaviors of LLMs on our benchmarks. We compute the pass@k metric following Chen et al. (2021). Specifically, for each problem, we generate N solutions independently, and compute the pass@k metric via the following formula for each 1 \u2264 k \u2264 N : \n\n, where c is the number of correct answers of the n runs. \n\nWe also compute the performance of self-consistency (Wang et al., 2022), a.k.a., majority voting, where for each k, we randomly sample k responses from the N runs and get the majority-voted answer. We report the average and standard deviation among 5 random draws. We only evaluate three models: o1-mini, Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct. For Llama-3.1-8B-Instruct, and Qwen2.5-Math-7B-Instruct, we choose N = 64, while for o1-mini we set N = 8. The results are plotted in Figure 9.",
            "score": 0.45732318312132936,
            "section_title": "C.5. Inference-time Scaling Behaviors",
            "char_start_offset": 33082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 98
                },
                {
                    "start": 99,
                    "end": 157
                },
                {
                    "start": 158,
                    "end": 305
                },
                {
                    "start": 308,
                    "end": 365
                },
                {
                    "start": 368,
                    "end": 565
                },
                {
                    "start": 566,
                    "end": 632
                },
                {
                    "start": 633,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 782
                },
                {
                    "start": 783,
                    "end": 832
                },
                {
                    "start": 833,
                    "end": 869
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.004180908203125
        },
        {
            "corpus_id": "267628004",
            "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
            "text": "The role of optimization can also be taken into account in this setting (Nitanda & Suzuki, 2021). In the nonparametric literature, for example Schmidt-Hieber (2017) and Suzuki (2019) derived the test error scaling of deep neural network in fitting certain target functions and (Bordelon et al., 2020) analyze spectral dependence. \n\nMore recently, scaling laws have been shown for kernel models under the Gaussian design, e.g. in (Spigler et al., 2020;Cui et al., 2021;2022) for regression and (Cui et al., 2023) for classification. Maloney et al. (2022) study scaling laws for the random feature model in the context of regression. In the context of memorization for heavy-tailed data scaling laws have been shown in the infinite-memory setting (Hutter, 2021), for \"quantized\" skills (Michaud et al., 2023) and for certain random data-generation processes (Debowski, 2023). When taking model capacity and optimization into account, Cabannes et al. (2023) recently proved scaling laws in constraint-capacity associative memories. \n\nTo our knowledge, however, very few papers deal with the decay of scaling in the case of self-consuming loops. A notable example is (Mobahi et al., 2020) which studies iterated retraining in the context of self-(knowledge-)distillation in the kernel setting. However, this analysis is very distinct from our work, not only because it places itself in the kernel setting with Gaussian design, but also because it assumes the distillation setting, where the \"generation\" stage is carefully optimized for the next stage training. In the case of synthesized data in the wild, this assumption can of course not be made. \n\nEmergence of \"Skills\" and Scaling Laws: Scaling laws give us an insight on bang-for-the-buck style trade-off for model training. However, cross-entropy loss is not a goal in and of itself: we want to train models that are endowed with a larger and larger skill set as we scale them up.",
            "score": 0.45732318312132936,
            "section_title": "A. Prior Work",
            "char_start_offset": 42569,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 97
                },
                {
                    "start": 98,
                    "end": 329
                },
                {
                    "start": 332,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 631
                },
                {
                    "start": 632,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 1028
                },
                {
                    "start": 1031,
                    "end": 1141
                },
                {
                    "start": 1142,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1557
                },
                {
                    "start": 1558,
                    "end": 1645
                },
                {
                    "start": 1648,
                    "end": 1776
                },
                {
                    "start": 1777,
                    "end": 1933
                }
            ],
            "ref_mentions": [
                {
                    "start": 72,
                    "end": 96,
                    "matchedPaperCorpusId": "219966188"
                },
                {
                    "start": 277,
                    "end": 300,
                    "matchedPaperCorpusId": "211066351"
                },
                {
                    "start": 429,
                    "end": 451,
                    "matchedPaperCorpusId": "260540101"
                },
                {
                    "start": 451,
                    "end": 468,
                    "matchedPaperCorpusId": "235254538"
                },
                {
                    "start": 493,
                    "end": 511,
                    "matchedPaperCorpusId": "246430659"
                },
                {
                    "start": 784,
                    "end": 806,
                    "matchedPaperCorpusId": "257687665"
                },
                {
                    "start": 1163,
                    "end": 1184,
                    "matchedPaperCorpusId": "211096976"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.055511474609375
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a\"compute-optimal\"scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.",
            "score": 0.45635452669744647,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.053802490234375
        },
        {
            "corpus_id": "244477730",
            "title": "Scaling Law for Recommendation Models: Towards General-purpose User Representations",
            "text": "Recently, several works empirically demonstrate the existence of a scaling law, where the training error scales as a power-law with model capacity, data size, and the amount of computation (Brown et al. 2020;Kaplan et al. 2020;Zhai et al. 2021;Bahri et al. 2021). Studies on the scaling law have significantly broadened the field of reasoning, but the findings are mostly restricted to NLP and computer vision. In this subsection, we empirically observe the power-law decrease of the pretraining error in user modeling areas w.r.t. scales. The computation (PF-days) is calculated as 6 \u00d7 # of parameters \u00d7 batch size \u00d7 # of training steps \u00d7 sequence length divided by one PF-day = 8.64 \u00d7 10 19 . We train all models for 100,000 steps. -days). This shows that the pretraining learning curve has a power-law scaling as a function of the total training computation (PFdays) when not bottlenecked by other factors. (Right) The generalization on downstream tasks depends on the test loss of pretraining. We observe a strong trend that a lower loss on the in-distribution data results in a lower loss on the outof-distribution data. \n\nPerformance on Scale. In contrast to existing work on the scaling law in other domains, we observe that the performance of the model does not depend dominantly on model capacity alone in the user representation tasks (Figure Left). Previous studies argued that learning high-quality representations from batch-wise contrastive loss requires a sufficient amount of negative samples (Chen et al. 2020;Mitrovic et al. 2021;Gao et al. 2021). Thus, we speculate that the scaling law when learning with a contrastive objective is more complex than that of supervisory signals due to the bottleneck induced by the batch size. CLUE's performance grows smoothly as the sequence length of the input data increases, suggesting that user behavior models benefit from observing longer sequences of customer behavior (Figure 2(a)-Right). We can see the performance change according to the amount of training computation (Figure 2(b)). These results give us an insight into how to appropriately allocate computing resources for efficient training schemes.",
            "score": 0.4561606409453701,
            "section_title": "Scaling Laws and Generalization",
            "char_start_offset": 16707,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 263
                },
                {
                    "start": 264,
                    "end": 410
                },
                {
                    "start": 411,
                    "end": 531
                },
                {
                    "start": 532,
                    "end": 539
                },
                {
                    "start": 540,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 733
                },
                {
                    "start": 734,
                    "end": 741
                },
                {
                    "start": 742,
                    "end": 909
                },
                {
                    "start": 910,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1125
                },
                {
                    "start": 1128,
                    "end": 1149
                },
                {
                    "start": 1150,
                    "end": 1359
                },
                {
                    "start": 1360,
                    "end": 1565
                },
                {
                    "start": 1566,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2048
                },
                {
                    "start": 2049,
                    "end": 2168
                }
            ],
            "ref_mentions": [
                {
                    "start": 189,
                    "end": 208,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1509,
                    "end": 1527,
                    "matchedPaperCorpusId": "211096730"
                },
                {
                    "start": 1527,
                    "end": 1548,
                    "matchedPaperCorpusId": "202621357"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.184326171875
        },
        {
            "corpus_id": "270764838",
            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
            "text": "Hoffmann et al. [25] hypothesize that the scaling law discrepancy is due to a difference in learning rate schedules. The term \"learning rate schedule\" comprises both the warmup and decay components, and it is plausible to interpret the \"Modeling the scaling behavior\" paragraph in Hoffmann et al. [25] \u00a72 as conjecturing that both components contribute to the scaling law discrepancy. However, we believe that Hoffmann et al. emphasize the decay component of the learning rate schedule in their hypothesis due to two main reasons. \n\nWe begin by considering, instead of the number of weights in linear layers, the total number of non-embedding learnable weights N exact (e.g., including also LayerNorm gains). The fourth column of Table 2 shows that the difference between this number and N is negligible. We also note that embedding layers have a negligible contribution to the model FLOP counts, since they do not require matrix-vector products. \n\nConsequently, the only non-negligible source of error in the approximation FLOPs(N, D) = 6N D is the attention layers. Since in OpenLM the attention dimension is identical to the model width, Kaplan et al. [30,Table 1] shows that the attention operation costs an additional 6nd FLOPs per token per layer for a forward and backward pass, where n = 2048 is the sequence length. Thus, if we define an effective model size of \n\nwe have that 6N eff D captures the cost of training the model for D tokens, including attention. \n\nWe now consider the difference between these approximations and its effect on compute-optimal scaling laws. The fifth column of Table 2 compares N eff to N . It shows that the ratio N eff /N changes smoothly between roughly 1.1 to roughly 1.2 and back to 1.1 as our model sizes grow. We note that had this ratio been completely constant, there would have been no essentially no difference between working with N and working with N eff since a power law in one would correspond directly to a power law in the other. Since in our model grid this ratio is approximately constant, we expect to see limited differences between the scaling laws resulting from each definition.",
            "score": 0.45587594053718017,
            "section_title": "A Interpreting Hoffmann et al.'s hypothesis",
            "char_start_offset": 30355,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 116
                },
                {
                    "start": 117,
                    "end": 384
                },
                {
                    "start": 385,
                    "end": 530
                },
                {
                    "start": 533,
                    "end": 708
                },
                {
                    "start": 709,
                    "end": 804
                },
                {
                    "start": 805,
                    "end": 946
                },
                {
                    "start": 949,
                    "end": 1067
                },
                {
                    "start": 1068,
                    "end": 1324
                },
                {
                    "start": 1325,
                    "end": 1370
                },
                {
                    "start": 1373,
                    "end": 1469
                },
                {
                    "start": 1472,
                    "end": 1579
                },
                {
                    "start": 1580,
                    "end": 1629
                },
                {
                    "start": 1630,
                    "end": 1755
                },
                {
                    "start": 1756,
                    "end": 1986
                },
                {
                    "start": 1987,
                    "end": 2142
                }
            ],
            "ref_mentions": [
                {
                    "start": 16,
                    "end": 20,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 297,
                    "end": 301,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.046539306640625
        },
        {
            "corpus_id": "270620400",
            "title": "Consistency Models Made Easy",
            "text": "Differences between 1-step and 2-step Generation. Our empirical results suggest that the training recipe for the best 1-step generative models can differ from the best few-step generative models in many aspects, including weighting function, dropout rate, and EMA rate/length. Fig. 7 shows an example of how FIDs from different numbers of function evaluations (NFEs) at inference vary with dropout rates. \n\nIn our setups, starting from a proper model size, the improvements from 2-step sampling seem larger than doubling the model size but keeping 1-step sampling. In the prior works, iCT (Song and Dhariwal, 2023) employs 2\u00d7 deeper model, but the 1-step generative performance can be inferior to the 2-step results from ECT. This finding is consistent with recent theoretical analysis (Lyu et al., 2023), which indicates a tighter bound on the sample quality for the 2-step generation compared to the 1-step generation. \n\nPareto Frontier & Scaling Law. The Pareto Frontier reveals a seemingly power law scaling behavior. Training configurations not optimized for the current compute budget, i.e., not on the Pareto Frontier, deviate from this scaling. Simply scaling up the training compute without adjusting other parameters may result in suboptimal performance. In our compute scaling experiments, we increased the batch size and enabled the smoothing factor c in the adaptive weighting to maintain this trend. \n\nC EXTENSION TO CONSISTENCY DISTILLATION. \n\nContinuous-time Consistency Distillation Consistency Distillation (CD) (Song et al., 2023) uses a fixed discrete schedule derived from a specific sampler, such as the EDM sampler. This approach inherently limits \u2206t \u2192 dt, therefore causing a non-trivial discretization error on approximating the differential consistency condition. Building upon ECT, we extend our continuous-time schedule to Consistency Distillation, which we term Easy Consistency Distillation (ECD). \n\nOn ImageNet 64\u00d764, we implement Consistency Distillation as the baseline using pretrained EDM2-S (Karras et al., 2024) with weighting functions, noise distribution, and dropout.",
            "score": 0.45553231882878753,
            "section_title": "Ablation on Pretraining",
            "char_start_offset": 44515,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 49
                },
                {
                    "start": 50,
                    "end": 276
                },
                {
                    "start": 277,
                    "end": 404
                },
                {
                    "start": 407,
                    "end": 564
                },
                {
                    "start": 565,
                    "end": 725
                },
                {
                    "start": 726,
                    "end": 920
                },
                {
                    "start": 923,
                    "end": 953
                },
                {
                    "start": 954,
                    "end": 1021
                },
                {
                    "start": 1022,
                    "end": 1152
                },
                {
                    "start": 1153,
                    "end": 1264
                },
                {
                    "start": 1265,
                    "end": 1413
                },
                {
                    "start": 1416,
                    "end": 1456
                },
                {
                    "start": 1459,
                    "end": 1638
                },
                {
                    "start": 1639,
                    "end": 1789
                },
                {
                    "start": 1790,
                    "end": 1927
                },
                {
                    "start": 1930,
                    "end": 2107
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06243896484375
        },
        {
            "corpus_id": "273098522",
            "title": "Searching for Efficient Linear Layers over a Continuous Space of Structured Matrices",
            "text": "While prior works have shown that certain structured matrices such as Monarch and BTT have better scaling laws than dense matrices as a function of model size on datasets such as CIFAR-10, CIFAR-100, and ImageNet [6,19], their experimental setups do not reflect today's large-scale training, where the models 1) typically do not train for multiple epochs on the training set, and 2) are heavily compute-bottlenecked such that we care primarily about performance as a function of training compute rather than model size (omitting the cost of training). These attributes of large-scale training make the compute-optimal scaling rather than scaling in model size alone more relevant. \n\nIn this section, we investigate the compute-optimal scaling laws of a wide range of Einsums -how their performance scales as a function of training compute. We will show that we can understand the systematic differences in the scaling laws of various Einsums by leveraging the taxonomy we have developed. While we do not find a structure that achieves noticeably better scaling laws compared to dense matrices, we identify the set of common properties shared across a wide range of structures that match the performance of dense matrices, based on which we will propose a significantly more efficient alternative to dense layers in Section 5.",
            "score": 0.45490269390250576,
            "section_title": "Scaling Laws of Einsums",
            "char_start_offset": 11064,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 551
                },
                {
                    "start": 552,
                    "end": 680
                },
                {
                    "start": 683,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 987
                },
                {
                    "start": 988,
                    "end": 1325
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.303466796875
        },
        {
            "corpus_id": "247778764",
            "title": "Training Compute-Optimal Large Language Models",
            "text": "We present three different approaches to answer the question driving our research:  On the left we show all of our different runs. We launched a range of model sizes going from 70M to 10B, each for four different cosine cycle lengths. From these curves, we extracted the envelope of minimal loss per FLOP, and we used these points to estimate the optimal model size (center) for a given compute budget and the optimal number of training tokens (right). In green, we show projections of optimal model size and training token count based on the number of FLOPs used to train Gopher (5.76 \u00d7 10 23 ).",
            "score": 0.45450738728834517,
            "section_title": "Estimating the optimal parameter/training tokens allocation",
            "char_start_offset": 10018,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 130
                },
                {
                    "start": 131,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 596
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0137939453125
        },
        {
            "corpus_id": "258832817",
            "title": "Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design",
            "text": "The de-facto approach for improving performance of vision and language models today is scale: large models are trained on more data for longer [64,43,24,19,80,23,13,16]. Empirically, it has been observed that the benefit of scale often follows a predictable power law in which the performance f (x) (e.g. error rate or log-perplexity) satisfies f (x) \u223c \u03b2x \u2212c + \u03b5 \u221e for some \u03b2, c > 0 as one varies the scaling dimension x (e.g. data or model size), if the remaining dimensions are not bottlenecks [34,39,27,26,3,1]. Here, \u03b5 \u221e is the irreducible loss. \n\nHowever, the simple power-law relation becomes more complicated when compute is considered. In this case, power laws are observed only along the compute-optimal frontier. Otherwise, scaling up the model size for a fixed compute budget can deteriorate performance (see [39,35] and Figure 4). Since one often has a fixed compute budget in mind (e.g. available hardware and time), one should pick the model size that maximizes performance subject to the compute budget constraint, which may imply not training until convergence. Indeed, this approach was used successfully in the recent Chinchilla [35] that outperformed its predecessor Gopher [55] despite being 4\u00d7 smaller in size. \n\nUnfortunately, in both [39] and [35] among others, the \"size\" of a model is equated with its parameter count, with no special consideration for model \"shape dimensions\", such as \"depth\" or \"width\". The rationale behind this choice follows from the surprising observation that the transformer shape had little impact on its scaling behavior in language modeling (LM) when performance is measured upstream (e.g. using log-perplexity) [39,32,33]. Nevertheless, follow-up analysis suggests that shape plays a pivotal role in other domains, such as in machine translation [47] and also in language modeling for downstream performance [66], with recent works even advocating for extreme aspect ratios, such as a single wide attention layer [12].",
            "score": 0.45440658473648904,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 304
                },
                {
                    "start": 305,
                    "end": 426
                },
                {
                    "start": 427,
                    "end": 514
                },
                {
                    "start": 515,
                    "end": 549
                },
                {
                    "start": 552,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 722
                },
                {
                    "start": 723,
                    "end": 842
                },
                {
                    "start": 843,
                    "end": 899
                },
                {
                    "start": 900,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1231
                },
                {
                    "start": 1234,
                    "end": 1431
                },
                {
                    "start": 1432,
                    "end": 1677
                },
                {
                    "start": 1678,
                    "end": 1973
                }
            ],
            "ref_mentions": [
                {
                    "start": 143,
                    "end": 147,
                    "matchedPaperCorpusId": "167217261"
                },
                {
                    "start": 147,
                    "end": 150,
                    "matchedPaperCorpusId": "214728308"
                },
                {
                    "start": 150,
                    "end": 153,
                    "matchedPaperCorpusId": "225039882"
                },
                {
                    "start": 153,
                    "end": 156,
                    "matchedPaperCorpusId": "235376986"
                },
                {
                    "start": 156,
                    "end": 159,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 162,
                    "end": 165,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 503,
                    "end": 506,
                    "matchedPaperCorpusId": "243865620"
                },
                {
                    "start": 511,
                    "end": 513,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 824,
                    "end": 827,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1147,
                    "end": 1151,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 1266,
                    "end": 1270,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2587890625
        },
        {
            "corpus_id": "277150783",
            "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models",
            "text": "A higher certaindex indicates that further reasoning steps are unlikely to change the final answer, allowing early termination to free resources for more challenging queries. Dynasor, an inference system built on this principle, optimizes compute scheduling by dynamically tracking reasoning progress instead of allocating resources uniformly. Dynasor-CoT [33] uses a probing scheme to exit early if the model is confident enough. Length-filtered Vote [123] is another work that leverages uncertainty to improve CoT reasoning. The study finds that longer reasoning chains do not always improve accuracy; instead, performance initially improves but eventually declines due to error accumulation. The authors provide a mathematical analysis proving the existence of an optimal CoT length, determined by model capability and task difficulty. To exploit this, they propose Length-filtered Voting, a length-aware majority voting method that groups answers by CoT length and selects the most reliable group based on prediction uncertainty. Self-Calib [46] introduces confidence-driven adaptive scaling strategies at test time to efficiently address queries with differing complexity levels, including Early-Stopping mechanisms for Best-of-N sampling and confidence-calibrated Self-Consistency approaches. CISC [108] implements a weighted majority voting scheme utilizing model-derived confidence scores. By emphasizing reasoning paths with higher confidence, it efficiently determines the correct response with substantially fewer samples. \n\nConsistency-based Selective Reasoning. Self-Truncation Best-of-N (ST-BoN) [119] enhances BoN sampling efficiency by introducing early termination (as shown in Figure 8, right), similar to Speculative Rejection [105]. However, unlike Speculative Rejection using reward models, ST-BoN leverages consistency as the metric to measure the importance. Specifically, it leverages the consistency of latent embeddings to evaluate response quality. The core insight is that \"the closer a sample is to others, the more likely its path will lead to the correct answer\". Then, ST-BoN selects the most consistent Chain-of-Embedding (CoE) to others and regards it as the optimal sample.",
            "score": 0.45408186328555766,
            "section_title": "Summarizationbased Dynamic Reasoning",
            "char_start_offset": 34572,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 343
                },
                {
                    "start": 344,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 526
                },
                {
                    "start": 527,
                    "end": 694
                },
                {
                    "start": 695,
                    "end": 838
                },
                {
                    "start": 839,
                    "end": 1033
                },
                {
                    "start": 1034,
                    "end": 1298
                },
                {
                    "start": 1299,
                    "end": 1397
                },
                {
                    "start": 1398,
                    "end": 1533
                },
                {
                    "start": 1536,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1752
                },
                {
                    "start": 1753,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1975
                },
                {
                    "start": 1976,
                    "end": 2094
                },
                {
                    "start": 2095,
                    "end": 2208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018829345703125
        },
        {
            "corpus_id": "276580891",
            "title": "How Do Large Language Monkeys Get Their Power (Laws)?",
            "text": "First, focusing on Large Language Monkeys, we used all available real data from all problems and all samples per  (2) the distributional estimator of pass i @1 assuming a scaled Kumaraswamy-Binomial distribution. Using all available data to fit both estimators, we find agreement between the least-squares estimate (ordinate) and the distribution-derived estimate (abscissa) for both Pythia models on MATH (left) and for frontier AI systems on HarmBench (right). For an explanation of why the two estimators match more closely for Large Language Monkeys than for Best-of-N Jailbreaking, see Appendix A. problem to compare the standard least squares regression estimator against the distributional estimator. We found close agreement between the two estimators (Fig. 6), giving us a sense that the two estimators yield reasonably consistent estimates under large sampling budgets. \n\nSecond, the distributional estimator also comes with another benefit: it directly provides an estimate of the power law's exponent b in a k \u2212b . Estimating the power law's exponent is especially valuable because the exponent dictates how success rates are improving with increasing inference compute. To test how the distributional estimator and least squares estimator compare at recovering the true asymptotic power law exponent, we generated synthetic data so that we would have ground-truth knowledge of the true power law exponent, then backtested how the two scaling estimators compare at recovering the true exponent (Alabdulmohsin et al., 2022a;Owen, 2024) by subsampling data with fewer problems and fewer samples per problem. We found that the distributional estimator obtains significantly better sample efficiency, with approximately an order of magnitude lower relative error def = | b \u2212 b|/b compared with the least squares estimator (Fig. 7), or equivalently, \u223c2 \u2212 4 orders of magni-tude less inference-compute. The distributional estimator performs well even under distributional mismatch.",
            "score": 0.45378930546207785,
            "section_title": "A New Distributional Estimator for Predicting Power Law Scaling",
            "char_start_offset": 14132,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 212
                },
                {
                    "start": 213,
                    "end": 462
                },
                {
                    "start": 463,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 879
                },
                {
                    "start": 882,
                    "end": 1026
                },
                {
                    "start": 1027,
                    "end": 1182
                },
                {
                    "start": 1183,
                    "end": 1617
                },
                {
                    "start": 1618,
                    "end": 1908
                },
                {
                    "start": 1909,
                    "end": 1987
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00577545166015625
        },
        {
            "corpus_id": "211532277",
            "title": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers",
            "text": "Moreover, this increase in convergence outpaces the additional computational overhead of using larger modelsthe most compute-efficient models are extremely large and stopped well short of convergence (e.g., Figure 2, left). We also show that this acceleration in wall-clock convergence (a) demonstrates the training speedup for ROBERTA models of different sizes on the masked language modeling pretraining task. In (b), we take ROBERTA checkpoints that have been pretrained for the same amount of wall-clock time and finetune them on a downstream dataset (MNLI). We then iteratively prune model weights to zero and find that the best models for a given test-time memory budget are ones which are trained large and then heavily compressed. \n\nis largely a function of parameter count and only weakly influenced by model width, depth, and batch size. \n\nAlthough larger models train faster, they also increase the computational and memory requirements of inference. This increased cost is especially problematic in real-world applications, where the cost of inference dominates the cost of training (Jouppi et al., 2017;Crankshaw et al., 2017;Metz, 2017). However, we show that for ROBERTA, this apparent trade-off can be reconciled with compression: large models are considerably more robust to compression as compared to small models (Section 4). Thus, large, heavily compressed models outperform small, lightly compressed models using comparable inference costs (e.g., Figure 2, right). \n\nWe finally analyze when and why large models train fast and compress well (Section 5). We show that the optimal model size is closely linked to the dataset size. In particular, large models perform favorably in big data settings where overfitting is a limited concern. We then analyze why larger models are more compressible by measuring the difference in weights when using quantized or sparse weight matrices. This error decreases as model size increases, i.e., greater overparameterization leads to easy-to-compress weights.",
            "score": 0.45334039309141805,
            "section_title": "Introduction",
            "char_start_offset": 2047,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 411
                },
                {
                    "start": 412,
                    "end": 562
                },
                {
                    "start": 563,
                    "end": 738
                },
                {
                    "start": 741,
                    "end": 847
                },
                {
                    "start": 850,
                    "end": 961
                },
                {
                    "start": 962,
                    "end": 1151
                },
                {
                    "start": 1152,
                    "end": 1344
                },
                {
                    "start": 1345,
                    "end": 1485
                },
                {
                    "start": 1488,
                    "end": 1574
                },
                {
                    "start": 1575,
                    "end": 1649
                },
                {
                    "start": 1650,
                    "end": 1756
                },
                {
                    "start": 1757,
                    "end": 1899
                },
                {
                    "start": 1900,
                    "end": 2015
                }
            ],
            "ref_mentions": [
                {
                    "start": 1095,
                    "end": 1116,
                    "matchedPaperCorpusId": "4202768"
                },
                {
                    "start": 1116,
                    "end": 1139,
                    "matchedPaperCorpusId": "1701442"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.018798828125
        },
        {
            "corpus_id": "277150607",
            "title": "Does Context Matter? ContextualJudgeBench for Evaluating LLM-based Judges in Contextual Settings",
            "text": "Inspired by recent efforts in inference-time scaling (Jaech et al., 2024;Snell et al., 2024), we investigate the impact of two test-time scaling techniques: LLM-as-jury (Verga et al., 2024) and selfconsistency (Wang et al., 2022). We experiment with three smaller (8B) and three larger (70B) judges, and for both settings, aggregate judgments via majority vote 2 . In Fig. 6, we present our results for both LLM-as-jury (top) using responses from different three judges and self-consistency (bottom) 2 We treat inconsistent judgments as ties. For a sample, if the aggregated judgments do not have a clear winner, e.g., (A, Tie, B) or (Tie, Tie, Tie), then we consider it incorrect. using 10 responses per judge (using a temperature of 0.7). The results are similar between smaller and larger models: LLM-as-jury rarely outperforms the strongest judge in the jury, while using selfconsistency similarly has little impact on judge performance. These trends may be surprising given the strong performance of reasoning models like o1 and DeepSeek-R1. The lack of jury success stems from the fact that judges do not exhibit structured agreement. We use all judge outputs to compute Krippendorff's alpha coefficient (Krippendorff, 2011), which measures inter-annotator agreement on a range from -1 (complete disagreement) to 1 (complete agreement), with 0 indicating random chance. As shown in Fig. 6, judge agreement is extremely random: Even on the best-performing split, the alpha coefficient barely exceeds 0.2. \n\nLack of improvement from self-consistency likely results from the fact that contextual assessment is largely unseen in judge training. As a result, better judgments cannot be extracted via random sampling. To better visualize judge performance for self-consistency, we plot a histogram  of the response distribution for each consistency run in Fig. 7. Interestingly, the self-consistency judgment distribution may be the byproduct of positional bias, where the judge outcome changes as a result of the order of responses in the prompt. \n\nOur findings in Sec.",
            "score": 0.45306523493292317,
            "section_title": "Can scaling inference-time compute help?",
            "char_start_offset": 26902,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 230
                },
                {
                    "start": 231,
                    "end": 364
                },
                {
                    "start": 365,
                    "end": 542
                },
                {
                    "start": 543,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 740
                },
                {
                    "start": 741,
                    "end": 941
                },
                {
                    "start": 942,
                    "end": 1046
                },
                {
                    "start": 1047,
                    "end": 1140
                },
                {
                    "start": 1141,
                    "end": 1375
                },
                {
                    "start": 1376,
                    "end": 1509
                },
                {
                    "start": 1512,
                    "end": 1646
                },
                {
                    "start": 1647,
                    "end": 1717
                },
                {
                    "start": 1718,
                    "end": 2047
                },
                {
                    "start": 2050,
                    "end": 2070
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00327301025390625
        },
        {
            "corpus_id": "259837466",
            "title": "Large Language Models",
            "text": "Natural language tasks are very complex, and benchmarks constructed from real world data cannot be used directly in theoretical considerations. For this purpose one generally defines \"toy worlds\" and generates synthetic data. The possibilities are endless, but some which have been used are arithmetic problems (decimal arithmetic; modular arithmetic), game play, solving systems of equations, and parsing formal languages. A particularly interesting task is linear regression [48]; since this is the prototypical case of statistical inference, a system which learns to do it can be said to be \"learning how to learn.\" \n\nComing to scaling laws, denote the model size (number of parameters) as P and the dataset size (number of tokens in the corpus) as D, then there are two general regimes. If we hold one of these (say P ) fixed and take the other (say D) to infinity, then a law of large numbers applies and L \u223c 1/D. On the other hand, if we take one parameter very large and study the dependence on the other, nontrivial power law scaling can emerge. In principle one can get different exponents for D and P , suggesting the ansatz \n\nwhere L is test loss Eq. 3 computed in an optimally regularized model. 24 This is a good fit to Figure 2. While in Figure 2 the two exponents appear to differ, there is not really convincing evidence that this is significant. Before working hard on this, one should ask if there is any way to control the many choices involved, so as to define universal exponents. One context in which this can be studied systematically is transfer learning, by distinguishing the dependence on the pretraining and fine tuning datasets [55]. Another relevant and practical question is whether one can prune the dataset to improve the scaling. It is intuitively plausible and can be shown in examples that sets of data items are worth more if they are diverse than if they are similar. The challenge is to find simple ways to quantify this similarity; in [126] many proposals are studied. \n\nScaling laws can arise in many ways, not specific to language models. One hypothesis is that the data lies on a low dimensional submanifold in a higher dimensional space. 25",
            "score": 0.4524957267629478,
            "section_title": "Phenomenology of language models",
            "char_start_offset": 30134,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 143
                },
                {
                    "start": 144,
                    "end": 225
                },
                {
                    "start": 226,
                    "end": 423
                },
                {
                    "start": 424,
                    "end": 618
                },
                {
                    "start": 621,
                    "end": 790
                },
                {
                    "start": 791,
                    "end": 918
                },
                {
                    "start": 919,
                    "end": 1053
                },
                {
                    "start": 1054,
                    "end": 1134
                },
                {
                    "start": 1137,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1242
                },
                {
                    "start": 1243,
                    "end": 1362
                },
                {
                    "start": 1363,
                    "end": 1501
                },
                {
                    "start": 1502,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1763
                },
                {
                    "start": 1764,
                    "end": 1905
                },
                {
                    "start": 1906,
                    "end": 2008
                },
                {
                    "start": 2011,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2184
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0142822265625
        },
        {
            "corpus_id": "210861095",
            "title": "Scaling Laws for Neural Language Models",
            "text": "In this section we list some potential caveats to our analysis. \n\n\u2022 At present we do not have a solid theoretical understanding for any of our proposed scaling laws. \n\nThe scaling relations with model size and compute are especially mysterious. It may be possible to understand scaling at very large D holding model size fixed [AS17], and also the shape of learning curves late in training, by modeling the loss with a noisy quadratic. But the scaling with D at very large model size still remains mysterious. Without a theory or a systematic understanding of the corrections to our scaling laws, it's difficult to determine in what circumstances they can be trusted. We display train and test loss for a series of 300M parameter models trained on different sized dataset subsamples. The test loss typically follows that of a run done with unrestricted data until diverging. Note that the degree of overfitting (as compared to the infinite data limit) is significantly overestimated by L test \u2212 L train (denoted by a black bar for each run). \n\n\u2022 We are not especially confident in the prediction of B crit (L) for values of the loss far outside the range we have explored. Changes in B crit could have a significant impact on trade-offs between data parallelism and the number of serial training steps required, which would have a major impact on training time. \u2022 We did not thoroughly investigate the small data regime, and our fits for L(N, D) were poor for the smallest values of D (where an epoch corresponded to only 40 steps). Furthermore, we did not experiment with regularization and data augmentation. Improvements in these could alter our results, quantitatively or qualitatively. \u2022 We used the estimated training compute C \u2248 6N BS, which did not include contributions proportional to n ctx (see Section 2.1). So our scalings with compute may be confounded in practice in the regime of very large n ctx , specifically where n ctx 12d model . \u2022 We tuned learning rates, and we experimented with learning rate schedules. But we may have neglected to tune some hyperparameter (e.g. intialization scale or momentum) that have an important effect on scaling. \u2022 The optimal choice of learning rate is sensitive to the target loss.",
            "score": 0.4523139858577318,
            "section_title": "C Caveats",
            "char_start_offset": 46308,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 63
                },
                {
                    "start": 66,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 244
                },
                {
                    "start": 245,
                    "end": 435
                },
                {
                    "start": 436,
                    "end": 509
                },
                {
                    "start": 510,
                    "end": 667
                },
                {
                    "start": 668,
                    "end": 783
                },
                {
                    "start": 784,
                    "end": 874
                },
                {
                    "start": 875,
                    "end": 1041
                },
                {
                    "start": 1044,
                    "end": 1172
                },
                {
                    "start": 1173,
                    "end": 1361
                },
                {
                    "start": 1362,
                    "end": 1532
                },
                {
                    "start": 1533,
                    "end": 1610
                },
                {
                    "start": 1611,
                    "end": 1690
                },
                {
                    "start": 1691,
                    "end": 1819
                },
                {
                    "start": 1820,
                    "end": 1951
                },
                {
                    "start": 1952,
                    "end": 2028
                },
                {
                    "start": 2029,
                    "end": 2088
                },
                {
                    "start": 2089,
                    "end": 2163
                },
                {
                    "start": 2164,
                    "end": 2234
                }
            ],
            "ref_mentions": [
                {
                    "start": 327,
                    "end": 333,
                    "matchedPaperCorpusId": "38293418"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04510498046875
        },
        {
            "corpus_id": "258888192",
            "title": "Scaling Data-Constrained Language Models",
            "text": "However, note that they do not repeat the entire training data and their parametric fit explicitly relies on the assumption that models are trained for a single epoch only. Thus, there is no guarantee that their scaling predictions hold for repeated data. \n\nFor all three data budgets, our results suggest that Allocation is optimized by scaling epochs faster than additional parameters. We confirm this at scale by training the data-constrained compute-optimal model for 9.3 \u00d7 10 21 FLOPs and 25 billion unique tokens as suggested by our efficient frontier. Despite having 27% less parameters, this model achieves better loss than the model suggested by the Chinchilla scaling laws (Figure 1, right). Similarly, the 120 billion parameter Galactica model trained on repeated data should have been significantly smaller according to data-constrained scaling laws (Appendix G). An additional benefit of using a smaller model is cheaper inference, though adding parameters can make it easier to parallelize training across GPUs. Adding parameters and epochs causes the loss to decrease and eventually increase again, suggesting that too much compute can hurt performance. Results from [46] also show that loss can increase when too many parameters are used, even with early stopping. However, we expect that appropriate regularization (such as simply removing all excess parameters as an extreme case) could prevent this behavior. Thus, our formula presented in \u00a73 and its predicted isoLoss contours in Figure 3 do not model the possibility that excess epochs or parameters could hurt performance. \n\n6 Results: Resource Return for Data-Constrained Scaling \n\nNext, consider the question of Return on scaling. To quantify this value, we run experiments with three FLOP budgets across eight respective data budgets to compare return on FLOPs. \n\nFigure 4 shows the configurations and validation curves for models trained on the same number of total tokens. Conforming to intuition and prior work on deduplication [55], repeated data is worth less, thus models trained on less unique data (and, correspondingly, more epochs) have consistently higher loss. However, the loss difference for a few epochs is negligible.",
            "score": 0.4518165860018569,
            "section_title": "Results: Resource Allocation for Data-Constrained Scaling",
            "char_start_offset": 13749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 172
                },
                {
                    "start": 173,
                    "end": 255
                },
                {
                    "start": 258,
                    "end": 387
                },
                {
                    "start": 388,
                    "end": 558
                },
                {
                    "start": 559,
                    "end": 701
                },
                {
                    "start": 702,
                    "end": 875
                },
                {
                    "start": 876,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1168
                },
                {
                    "start": 1169,
                    "end": 1280
                },
                {
                    "start": 1281,
                    "end": 1427
                },
                {
                    "start": 1428,
                    "end": 1594
                },
                {
                    "start": 1597,
                    "end": 1652
                },
                {
                    "start": 1655,
                    "end": 1704
                },
                {
                    "start": 1705,
                    "end": 1836
                },
                {
                    "start": 1839,
                    "end": 1949
                },
                {
                    "start": 1950,
                    "end": 2147
                },
                {
                    "start": 2148,
                    "end": 2208
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04046630859375
        },
        {
            "corpus_id": "267406160",
            "title": "A Dynamical Model of Neural Scaling Laws",
            "text": "1. We analyze the learning dynamics of a structured and randomly projected linear model trained with gradient flow, discrete time SGD, and momentum. In an asymptotic limit of the model, we obtain a dynamical mean field theory (DMFT) description of the learning curve in terms of correlation functions, which measure the crosstime correlation of training and test errors, and response functions which measure sensitivity of the dynamics to small perturbations. 2. We solve for the response functions exactly in Fourier domain. This solution reveals faster training for larger models. The low frequency range of these functions allow us to extract the long time limit of the loss. 3. We show that the model and data corrections to the dynamics accumulate over time. At early time, each of these corrections has a universal scaling, consistent with prior works (Bahri et al., 2021). 4. For power-law structured features we show that the model exhibits power law scaling of test loss with time, model size and dataset size. While the data and model exponents are the same, the time and model exponents are different in general. We show that this gives rise to an asymmetric compute optimal scaling strategy where training time increases faster than model size. 5. Our theory explains why ensembling is not compute optimal as it gives less benefit to performance than increase in model size. 6. We observe in Section 5.1 that feature learning networks can obtain better power law scalings, leading to a better compute optimal frontier. We empirically study this phenomenon in Appendix L.",
            "score": 0.451099613500297,
            "section_title": "Introduction",
            "char_start_offset": 5467,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 149,
                    "end": 459
                },
                {
                    "start": 460,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 678
                },
                {
                    "start": 679,
                    "end": 763
                },
                {
                    "start": 764,
                    "end": 879
                },
                {
                    "start": 880,
                    "end": 1019
                },
                {
                    "start": 1020,
                    "end": 1123
                },
                {
                    "start": 1124,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1530
                },
                {
                    "start": 1531,
                    "end": 1582
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07794189453125
        },
        {
            "corpus_id": "275133600",
            "title": "Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs",
            "text": "Enhancing model performance on complex tasks can be achieved by scaling test-time compute, which involves: \n\nExpanding Search Space LLMs have strong reasoning abilities, but their auto-regressive decoding often misses optimal solutions. Self-consistency generates multiple responses and use majority voting to select the best answer (Wang et al., 2023b). Other approaches include best-of-n decoding, minimum Bayes risk decoding (Lightman et al., 2024;Li et al., 2023;Khanov et al., 2024;Heineman et al., 2024;Wu et al., 2024), and structured search methods such as Tree-of-Thought, Graph-of-Thought, and Monte Carlo Tree Search (Yao et al., 2024;Besta et al., 2024;Luo et al., 2024;Tian et al., 2024;Wan et al., 2024).",
            "score": 0.45106447495594537,
            "section_title": "Scaling Test-Time Compute",
            "char_start_offset": 19901,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 106
                },
                {
                    "start": 109,
                    "end": 236
                },
                {
                    "start": 237,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 718
                }
            ],
            "ref_mentions": [
                {
                    "start": 333,
                    "end": 353,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 428,
                    "end": 451,
                    "matchedPaperCorpusId": "258987659"
                },
                {
                    "start": 451,
                    "end": 467,
                    "matchedPaperCorpusId": "259370847"
                },
                {
                    "start": 467,
                    "end": 487,
                    "matchedPaperCorpusId": "267411977"
                },
                {
                    "start": 487,
                    "end": 509,
                    "matchedPaperCorpusId": "271329400"
                },
                {
                    "start": 628,
                    "end": 646,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 646,
                    "end": 665,
                    "matchedPaperCorpusId": "261030303"
                },
                {
                    "start": 682,
                    "end": 700,
                    "matchedPaperCorpusId": "258967540"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0094146728515625
        },
        {
            "corpus_id": "274981761",
            "title": "Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration",
            "text": "Beyond scaling parameters and data during pretraining of large language models (Kaplan et al., 2020), recent research has increasingly focused on scaling compute at inference time (Brown et al., 2024;Snell et al., 2024). Unlike rapid problem solving, scaling inference compute generates more output tokens, allowing the model to address problems more deliberately, potentially engaging in planning, reasoning, or refinement before finalizing an answer. Such an emerging scaling paradigm in inference significantly enhances the model's problem solving and data synthesis abilities. \n\nScaling laws of inference compute have been widely studied in the single-agent scenarios (Brown et al., 2024;Bansal et al., 2024). However, scaling compute from multi-agent systems remains underexplored. We hypothesize that a multi-agent system offers a higher performance ceiling compared to a single-agent system. This is based on the observation that different language models exhibit varying strengths, and leveraging the unique advantages of each model can lead to enhanced overall capabilities. To address this gap, we propose to study multi-agent sampling that scales the number of generated samples (compute) from multiple distinct language models (see Figure 1). \n\nTo scale inference compute, we adopt the widely used best-of-N sampling method, which generates N outputs for each input prompt (Liu et al., 2024;G\u00fcl\u00e7ehre et al., 2023). In multi-agent-based sampling, the primary challenge is effectively coordinating models to achieve compute-optimal generation. We begin by revisiting and formalizing previous methods for model fusion, such as mixture of agents (Wang et al., 2024b), within a unified framework. These methods typically rely on arXiv:2412.17061v2 [cs.CL] 19 May 2025 fixed workflows with a modular approach to model combination, where the responses of other models are encoded as input context to generate new outputs. However, fixed structures fail to account for question-specific variability, as the optimal structure varies across different questions. To overcome this limitation, we propose Tree Search-based Orchestrated Agents (TOA), which treat model coordination as a multi-step decision-making process, aiming to maximize the rewards of sequentially generated responses.",
            "score": 0.45093996062644004,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 220
                },
                {
                    "start": 221,
                    "end": 452
                },
                {
                    "start": 453,
                    "end": 580
                },
                {
                    "start": 583,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 898
                },
                {
                    "start": 899,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1254
                },
                {
                    "start": 1257,
                    "end": 1426
                },
                {
                    "start": 1427,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1754
                },
                {
                    "start": 1755,
                    "end": 1926
                },
                {
                    "start": 1927,
                    "end": 2063
                },
                {
                    "start": 2064,
                    "end": 2288
                }
            ],
            "ref_mentions": [
                {
                    "start": 1385,
                    "end": 1403,
                    "matchedPaperCorpusId": "261705578"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08819580078125
        },
        {
            "corpus_id": "265552007",
            "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey",
            "text": "The work [126] presents a thorough study of the empirical scaling laws of transformer-based large language models. The authors observe that model performance (objective function ) primarily depends on three factors: the number of model parameters  , dataset size , and the computing budget for training. They demonstrate a power-law relationship between model performance (measured in objective function, ) and these factors. For instance, they found that the relationship between performance and dataset size can be represented as () \u2248 (5.4 \u00d7 10 13 /) 0.095 . This formula suggests that as the dataset size increases, the model's performance improves following a specific pattern. While theoretical generalization bounds may suggest a similar power-law relationships, they generally do not provide specific coefficients like those identified in Kaplan et al.'s work [126]. This specificity is crucial for accurately predicting model performance. Additionally, the study highlights that transformers, known for their effective handling of long-range data dependencies, tend to outperform Long Short-Term Memory networks (LSTMs) [106] as they scale. This observation underscores the potential of transformers in large-scale language processing tasks. \n\nCompute-Optimal Models via Scaling Law. When working within a fixed computational budget, it is crucial to find the right balance between model size ( ) and dataset size (). This is where the scaling law curve, ( , ), becomes a vital tool. It helps determine the most effective trade-off between these two factors. The scaling law, as observed in [126], was instrumental in designing GPT-3, a 175 billion parameter language model. Interestingly, GPT-3 was trained on fewer tokens than was typical for its time [23]. Different forms of the scaling law curve have led to the development of diverse models, as seen in subsequent studies [9,25,107]. A notable application of these predicted scaling laws is found in [107]. Their research revealed that many previously trained LLMs, including Gopher [216], could have achieved better performance within the same compute budget. They demonstrated this by training a smaller model, Chinchilla, with 70 billion parameters, which outperformed the larger 280 billion parameter Gopher model [216] while using a similar compute budget. \n\nScaling Law for Transfer Learning.",
            "score": 0.4505341758045266,
            "section_title": "Scaling Law",
            "char_start_offset": 15063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 114
                },
                {
                    "start": 115,
                    "end": 303
                },
                {
                    "start": 304,
                    "end": 425
                },
                {
                    "start": 426,
                    "end": 560
                },
                {
                    "start": 561,
                    "end": 681
                },
                {
                    "start": 682,
                    "end": 873
                },
                {
                    "start": 874,
                    "end": 946
                },
                {
                    "start": 947,
                    "end": 1148
                },
                {
                    "start": 1149,
                    "end": 1249
                },
                {
                    "start": 1252,
                    "end": 1291
                },
                {
                    "start": 1292,
                    "end": 1425
                },
                {
                    "start": 1426,
                    "end": 1491
                },
                {
                    "start": 1492,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1767
                },
                {
                    "start": 1768,
                    "end": 1897
                },
                {
                    "start": 1898,
                    "end": 1970
                },
                {
                    "start": 1971,
                    "end": 2124
                },
                {
                    "start": 2125,
                    "end": 2325
                },
                {
                    "start": 2328,
                    "end": 2362
                }
            ],
            "ref_mentions": [
                {
                    "start": 1128,
                    "end": 1133,
                    "matchedPaperCorpusId": "1915014"
                },
                {
                    "start": 1762,
                    "end": 1766,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1886,
                    "end": 1889,
                    "matchedPaperCorpusId": "252220884"
                },
                {
                    "start": 1889,
                    "end": 1892,
                    "matchedPaperCorpusId": "253117181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.2010498046875
        },
        {
            "corpus_id": "276961790",
            "title": "Compute Optimal Scaling of Skills: Knowledge vs Reasoning",
            "text": "First, we consider the difference in COs for different skills for our canonical datamix. \n\nMethodology Roughly following the approach used by Dubey et al. ( 2024), we train models for compute budget between 6 \u00d7 10 18 and 3 \u00d7 10 21 FLOPs. At each compute scale, we pretrain models ranging in size between 40M and 8B parameters.2 Using these models, we compute IsoFLOP curves and corresponding COs for the APE (validation loss) and the two skills under consideration -as measured by NLL of the target answer -using the methodology described in \u00a7 2.1. Here, in order to compare APE COs with the COs for skills, we first obtain the APE CO parameter counts, p c from the power law fit given by Dubey et al. ( 2024), for each compute scale. Next, for a given skill s, for each dataset D belonging to that skill, we fit degree-2 polynomials for each compute scale and identify optimal parameter counts for p s . We then project the APE COs onto the polynomial fits for s, and fit power law curves to the APE COs and the skilldependent COs for s. In all our experiments, we first analyse the results on our hypothesis split and confirm our findings on the held-out split.",
            "score": 0.4503191628449402,
            "section_title": "Can COs differ between skills?",
            "char_start_offset": 12475,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 91,
                    "end": 237
                },
                {
                    "start": 238,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 548
                },
                {
                    "start": 549,
                    "end": 734
                },
                {
                    "start": 735,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1038
                },
                {
                    "start": 1039,
                    "end": 1163
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0202484130859375
        },
        {
            "corpus_id": "277467964",
            "title": "Inference-Time Scaling for Complex Tasks: Where We Stand and What Lies Ahead",
            "text": "There are several reasons why a deeper analysis using the above scaling approaches is important. First, comparing the average performance of models across a diverse set of reasoning tasks, enables a broader perspective on how well the current training methods for reasoning generalize to different types of reasoning. Evaluating best-of-n performance for conventional models approximate an upper bound on the potential of these models to be adapted as reasoning models by lengthening their generations via simple verifier-in-the-loop RL or fine-tuning methods that teach the model how to pick the best answer from a set of candidates. We specifically study the gap between the best-of-n performance for conventional models and the average performance of reasoning models, which we refer to as the conventional-to-reasoning gap. This serves as an estimate of the gap that needs to be addressed either via sampling beyond N candidates or via more sophisticated RL that introduces feedback and backtracking in a more fine-grained manner, rather than at the end of a generation. Estimates of best-of-n performance for reasoning models demonstrate the untapped potential of current methods, showing that better inference paths in such models are still possible but need to be better extracted to serve the best possible reasoning capability. \n\nEvaluation metrics. Compared to standard inference, test-time scaling aims at improving performance with additional computation at test time (i.e. longer generations). Therefore, our evaluation metrics include both the performance accuracy and the amount of computation in terms of the number of tokens generated, including both completion and reasoning tokens. Associating accuracy with token usage of inference-time scaling approaches portrays the Pareto trade-off between accuracy and compute as an assessment of token efficiency. Unless otherwise specified, for all benchmarks, accuracy is defined as how often a given scaling approach leads to the correct answer. \n\nModels and data sourcing.",
            "score": 0.4501389354794084,
            "section_title": "Benefits of evaluating inference-time scaling.",
            "char_start_offset": 5105,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 96
                },
                {
                    "start": 97,
                    "end": 317
                },
                {
                    "start": 318,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 827
                },
                {
                    "start": 828,
                    "end": 1074
                },
                {
                    "start": 1075,
                    "end": 1336
                },
                {
                    "start": 1339,
                    "end": 1358
                },
                {
                    "start": 1359,
                    "end": 1485
                },
                {
                    "start": 1486,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1872
                },
                {
                    "start": 1873,
                    "end": 2007
                },
                {
                    "start": 2010,
                    "end": 2035
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.036163330078125
        },
        {
            "corpus_id": "257833842",
            "title": "BloombergGPT: A Large Language Model for Finance",
            "text": "is low. To provide externally comparable results, we developed a few-shot strategy for FLUE, but also decided to augment the publicly available evaluation tasks with company-internal benchmarks.\n\nModel Size. Large language model training remains expensive in terms of the computational cost and human effort to assemble data and train the model. Determining the optimal amount of training data and model shape and size for the best utilization of resources becomes important. Kaplan et al. (2020) first studied the dependence of language model performance on architecture, parameter size, compute power, and dataset size. They reported that the number of model parameters, the dataset size, and the amount of compute improves performance on the autoregressive language modeling objective smoothly according to the power law. A similar investigation by Hernandez et al. (2021) into data transfer for differing distributions found that this also follows a power law. Moving beyond studying the effect on loss, Rae et al. (2021) analyzed the effect of scale on undesirable properties such as bias and toxicity by training a wide range of model sizes.\n\nComparing model architectures, Levine et al. (2020) studied the scaling of models that use self-attention and derived guidelines for depth-to-width allocation. Tay et al. (2021) reported that model shape (depth-width ratio) impacted performance on downstream tasks even if it had minimal impact on the pretraining objective. Tay et al. (2022a) further studied the effect of scaling for different model architectures and showed that architecture choice is pertinent when scaling and that the vanilla transformer architecture scales best.\n\nOf particular importance to this work is the study of Hoffmann et al. (2022), who investigated the effect of model size and the number of training tokens on the performance of a model given a fixed compute budget. They posited that existing large language models were undertrained and that model size and the number of training tokens should be scaled equally. They demonstrated this hypothesis through Chinchilla, a model significantly smaller, yet higher performing, than most of the largest LLMs. These findings opened the door for \"Chinchilla optimal\" training of smaller models that achieve strong performance, and for which inference can be run much more efficiently than for their larger counterparts. These findings led us to consider a nearly Chinchilla-optimal model",
            "score": 0.4501179667715349,
            "section_title": "Related Work",
            "char_start_offset": 70997,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 476,
                    "end": 496,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 1180,
                    "end": 1200,
                    "matchedPaperCorpusId": "219965648"
                },
                {
                    "start": 1741,
                    "end": 1763,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04913330078125
        },
        {
            "corpus_id": "275458278",
            "title": "Test-time Alignment of Diffusion Models without Reward Over-optimization",
            "text": "Test-time Scaling. While DAS offers a powerful test-time reward maximization scheme, even outperforming fine-tuning methods, it requires additional computation during inference. Similarly, recent literature on language models suggests that leveraging additional computation during inference can further improve the response's quality (Wei et al., 2022;Khanov et al., 2024;Wu et al., 2024;Snell et al., 2024). However, it's important to verify which method can most effectively utilize additional resources and achieve the highest performance as we scale test-time computation. \n\nExperiment Setup. To demonstrate DAS is indeed an effective way to leverage computation during inference, we compare the test-time scaling of DAS with other possible test-time alignment methods for diffusion models. Baseline methods include Best-of-N (Touvron et al., 2023;Beirami et al., 2024;Ma et al., 2025) which runs N (number of particles) independent generation process and selects the final sample with the highest reward. SMC uses a Sequential Monte Carlo sampler as DAS does, but without the approximate locally optimal proposal in Section 3.3.3 and tempering, using the original generation process as the proposal for each step. Including DAS, these methods are naturally scalable by increasing the number of particles used in the algorithm. We use the number of particles in {1, 2, 4, 8, 16, 32, 64, 128} for each method and compare how target reward and unseen reward for cross-reward generalization scale as inference-time computation increase. We use PickScore for target reward and HPSv2 for unseen reward. \n\nResults. Figure 8 shows that as the inference compute increases, DAS consistently outperforms both Best-of-N and SMC across both PickScore (target reward) and HPSv2 (unseen reward). Notably, DAS already achieves superior performance over other baselines even when using fewer particles and significantly less compute. This highlights DAS's efficiency in maximizing rewards compared to methods like Best-of-N, which shows limited improvements beyond a certain compute threshold, and SMC, which scales better than Best-of-N but still falls short of DAS.",
            "score": 0.4492108564654844,
            "section_title": "E TEST-TIME SCALING",
            "char_start_offset": 44810,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 18
                },
                {
                    "start": 19,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 576
                },
                {
                    "start": 579,
                    "end": 596
                },
                {
                    "start": 597,
                    "end": 794
                },
                {
                    "start": 795,
                    "end": 1009
                },
                {
                    "start": 1010,
                    "end": 1134
                },
                {
                    "start": 1135,
                    "end": 1218
                },
                {
                    "start": 1219,
                    "end": 1331
                },
                {
                    "start": 1332,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1601
                },
                {
                    "start": 1604,
                    "end": 1785
                },
                {
                    "start": 1786,
                    "end": 1921
                },
                {
                    "start": 1922,
                    "end": 2155
                }
            ],
            "ref_mentions": [
                {
                    "start": 334,
                    "end": 352,
                    "matchedPaperCorpusId": "246411621"
                },
                {
                    "start": 352,
                    "end": 372,
                    "matchedPaperCorpusId": "267411977"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0352783203125
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "So far, we saw that utilizing additional test-time computation can enable us to represent more complex distributions than the one predicted by the base LLM itself, thereby improving performance. We now posit that this increased flexibility of representing distributions means that we can expect additional test-time compute to make up for the lack of a higher-capacity model or training for more FLOPs during pre-training. In this section, we study to what extent this is possible. We pose the following question: \n\nQuestion: Exchanging pretraining and test-time compute Suppose a model was pre-trained with  FLOPs. Assume that we plan to run  FLOPs of inference with this model. If we want to improve performance by increasing the total FLOPs budget by a factor of  (i.e., ( +  ) total FLOPs across both pretraining and inference), should we spend our FLOPs on increased pretraining compute or on additional test-time compute? \n\nIncreasing pretraining FLOPS introduces the additional design decision of whether to allocate compute to training with more data or more parameters [14]. We focus on the setting in which model parameters are scaled up and training data amount is fixed, matching the approach taken with the open-source LLaMA series of models [41]. We choose this setting as it is representative of a canonical approach to scaling pretraining compute and leave the analysis of compute-optimal scaling of pretraining compute [29] where the data and parameters are both scaled equally to future work. \n\nDefining an exchange rate between FLOPs. We now describe how we define the exchange rate between pretraining and inference FLOPs. To determine pretraining FLOPs, use use the common approximation  = 6  pretrain [14], and for inference FLOPs, we use  = 2  inference [29]. Here  represents model parameters,  pretrain is the number of tokens used for pretraining, and  inference the total number of tokens generated at inference time. With these approximations, we can see that, if we multiply the model parameters by a factor of , then both the pretraining and inference FLOPs (due to the cost of greedy decoding with the larger model), increase by a factor of  (giving ( +  ) total FLOPs).",
            "score": 0.44881246544060904,
            "section_title": "Putting it Together: Exchanging Pretraining and Test-Time Compute",
            "char_start_offset": 42011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 194
                },
                {
                    "start": 195,
                    "end": 422
                },
                {
                    "start": 423,
                    "end": 481
                },
                {
                    "start": 482,
                    "end": 513
                },
                {
                    "start": 516,
                    "end": 615
                },
                {
                    "start": 616,
                    "end": 679
                },
                {
                    "start": 680,
                    "end": 927
                },
                {
                    "start": 930,
                    "end": 1083
                },
                {
                    "start": 1084,
                    "end": 1260
                },
                {
                    "start": 1261,
                    "end": 1510
                },
                {
                    "start": 1513,
                    "end": 1553
                },
                {
                    "start": 1554,
                    "end": 1642
                },
                {
                    "start": 1643,
                    "end": 1782
                },
                {
                    "start": 1783,
                    "end": 1944
                },
                {
                    "start": 1945,
                    "end": 2201
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06854248046875
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "In order to effectively analyze the test-time scaling properties of the different mechanisms discussed in Section 2 (e.g. the proposal distribution and the verifier), we will prescribe an approximation to this optimal strategy  * , * () () as a function of a statistic of a given prompt. This statistic estimates a notion of difficulty for a given prompt. The compute-optimal strategy is defined as a function of the difficulty of this prompt. Despite being only an approximate solution to the problem shown in Equation 1, we find that it can still induce substantial improvements in performance over a baseline strategy of allocating this inference-time compute in an ad-hoc or uniformly-sampled manner. \n\nOur estimate of the question difficulty assigns a given question to one of five difficulty levels. We can then use this discrete difficulty categorization to estimate  * , * () () on a validation set for a given test-time compute budget. We then apply these compute-optimal strategies on the test-set. Concretely, we select the best performing test-time compute strategy for each difficulty bin independently. In this way, question difficulty acts as a sufficient statistic of a question when designing the compute-optimal strategy. Defining difficulty of a problem. Following the approach of Lightman et al. [22], we define question difficulty as a function of a given base LLM. Specifically, we bin the model's pass@1 rate -estimated from 2048 samples -on each question in the test set into five quantiles, each corresponding to increasing difficulty levels. We found this notion of model-specific difficulty bins to be more predictive of the efficacy of using test-time compute in contrast to the hand-labeled difficulty bins in the MATH dataset. \n\nThat said, we do note that assessing a question's difficulty as described above assumes oracle access to a ground-truth correctness checking function, which is of course not available upon deployment where we are only given access to test prompts that we don't know the answer to. In order to be feasible in practice, a compute-optimal scaling strategy conditioned on difficulty needs to first assess difficulty and then utilize the right scaling strategy to solve this problem.",
            "score": 0.44869142199669265,
            "section_title": "Estimating Question Difficulty for Compute-Optimal Scaling",
            "char_start_offset": 14885,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 287
                },
                {
                    "start": 288,
                    "end": 355
                },
                {
                    "start": 356,
                    "end": 443
                },
                {
                    "start": 444,
                    "end": 704
                },
                {
                    "start": 707,
                    "end": 805
                },
                {
                    "start": 806,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1008
                },
                {
                    "start": 1009,
                    "end": 1116
                },
                {
                    "start": 1117,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1273
                },
                {
                    "start": 1274,
                    "end": 1386
                },
                {
                    "start": 1387,
                    "end": 1567
                },
                {
                    "start": 1568,
                    "end": 1756
                },
                {
                    "start": 1759,
                    "end": 2039
                },
                {
                    "start": 2040,
                    "end": 2237
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0296478271484375
        },
        {
            "corpus_id": "239016819",
            "title": "PAGnol: An Extra-Large French Generative Model",
            "text": "Scaling. We use scaling laws to inform the duration of the training of our largest models. Rather than training to convergence, which would be wasteful, we train to optimality, as predicted by the equations provided in (Kaplan et al., 2020). This is akin to what has been done for GPT-3, and this enables us to keep our computational budget in line with that of CamemBERT, a model 13x smaller than PAGnol-XL. We find that training all of our models for a single epoch on the 30GT of CCNet enables us to reach optimality for the most expensive XL model. Table 3 presents the ratios between the compute budget effectively used and that to optimality (r C opt ) or to convergence (r C conv ). While our small model is trained to convergence, others are trained significantly short of it. We find that our training performance matches nicely with the estimated 2.6 PF-days for the training of GPT fr -LARGE from (Simoulin and Crabb\u00e9, 2021).",
            "score": 0.4484274206673905,
            "section_title": "Efficient training with scaling laws",
            "char_start_offset": 8115,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 8
                },
                {
                    "start": 9,
                    "end": 90
                },
                {
                    "start": 91,
                    "end": 241
                },
                {
                    "start": 242,
                    "end": 408
                },
                {
                    "start": 409,
                    "end": 552
                },
                {
                    "start": 553,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 784
                },
                {
                    "start": 785,
                    "end": 936
                }
            ],
            "ref_mentions": [
                {
                    "start": 908,
                    "end": 935,
                    "matchedPaperCorpusId": "236145057"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06939697265625
        },
        {
            "corpus_id": "277314094",
            "title": "The Surprising Effectiveness of Test-Time Training for Few-Shot Learning",
            "text": "This procedure is shown in Figure 15. We initially described 10 training tasks with the hierarchical-style shown in Figure 11. Then, for other training tasks tasks, we obtained less quality crowd-worker annotations from LARC (Acquaviva et al., 2022) project. By using our high-quality seed annotations and their LARC version, we 10-shot prompt and LM to produce high quality annotations for the other training tasks. \n\nYou Recent work has shown that scaling test-time compute can significantly improve the performance of LMs. One of the most common techniques to do this is by sampling multiple responses, and then selecting the best response using a ranker. However, while sampling is very effective in domains with multiple possible solutions (programs in code) or multiple possible paths to the final answer (math), it can be detrimental when generating answers directly, as there is no way to directly enforce diversity across samples while ensuring coherence within samples. As an alternative inference-time scaling, we use an augmented inference strategy that generates multiple prediction candidates by using geometric transformations, combined with a decoding scheme. \n\nFor a given task with training examples (x k , y k ) K k=1 and test input x test , we use invertible transformations to produce equivalent transformed versions of the task, as in Figure 5. Let T be some set set of invertible geometric transformations (e.g., rotations and reflections). For each transformation t \u2208 T , we apply t to all demonstrations and the test input and run our model with these transformed inputs. We then apply the inverse transformation to obtain the final prediction for that transformation. \n\n\u1ef9 LM(t(d input )) := [t(x 1 ), t(y 1 ), . . . , t(x )] (6) \n\nfurther augment our predictions by permuting the order of training For each transformation g, we sample n = 2 different permutations of the demonstration sequence, resulting in n \u2022 |T | total per task. This is to mitigate any bias in the model's processing of the demonstration sequence. (Bober-Irizar & Banerjee, 2024) also find transpose and rotation is helpful to produce extra prediction",
            "score": 0.448359861272253,
            "section_title": "D.1. Getting Descriptions for Tasks",
            "char_start_offset": 38361,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 37
                },
                {
                    "start": 38,
                    "end": 126
                },
                {
                    "start": 127,
                    "end": 258
                },
                {
                    "start": 259,
                    "end": 416
                },
                {
                    "start": 419,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 658
                },
                {
                    "start": 659,
                    "end": 979
                },
                {
                    "start": 980,
                    "end": 1175
                },
                {
                    "start": 1178,
                    "end": 1366
                },
                {
                    "start": 1367,
                    "end": 1463
                },
                {
                    "start": 1464,
                    "end": 1596
                },
                {
                    "start": 1597,
                    "end": 1693
                },
                {
                    "start": 1696,
                    "end": 1741
                },
                {
                    "start": 1742,
                    "end": 1754
                },
                {
                    "start": 1757,
                    "end": 1958
                },
                {
                    "start": 1959,
                    "end": 2044
                },
                {
                    "start": 2045,
                    "end": 2148
                }
            ],
            "ref_mentions": [
                {
                    "start": 225,
                    "end": 249,
                    "matchedPaperCorpusId": "235435890"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01473236083984375
        },
        {
            "corpus_id": "252683098",
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "text": "Reinforcement learning models like AlphaZero are often required to perform fast at test time, either in order to finish their calculation within a thinking time limit or to react quickly to a rapidly changing environment. The AlphaZero algorithm provides the ability to tailor the move-selection inference time to a given time limit by changing the number of MCTS steps, which does not have to match the number of steps used during training. Since our main results all use a fixed number of 300 MCTS steps both at train and test time, we add here the scaling behavior of different sized Connect Four agents using varying numbers of MCTS steps. We look at the case where training compute is abundant but performance is bottlenecked by inference-time compute. To do so we match fully converged agents against each other, all receiving a fixed amount of inference-time compute enforced by scaling MCTS search size inversely with forward-pass compute costs. \n\nFig. 6 shows the Elo scaling of different sized agents under inference-time compute constraints. Agents steadily improve with size at smaller sizes for all amounts of available inference-time compute. Larger agents break off from this trend at different points which depend on the amount of compute given, such that the the trend seems to hold longer for larger compute values. Interestingly, a power law appears at the limit of large inference-time compute which breaks off only for the largest agents. The scaling exponent is significantly smaller than that of Fig. 2, which is not surprising given that small agents have access to up to two orders of magnitude more MCTS steps than the largest agents, see Fig. 6 right. This scaling law may be related to the power law scaling of inference-time compute with training compute found by Jones (2021) when fixing the Elo score of AlphaZero agents.",
            "score": 0.44825072240466834,
            "section_title": "B INFERENCE-TIME COMPUTE SCALING",
            "char_start_offset": 27610,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 222,
                    "end": 441
                },
                {
                    "start": 442,
                    "end": 643
                },
                {
                    "start": 644,
                    "end": 757
                },
                {
                    "start": 758,
                    "end": 953
                },
                {
                    "start": 956,
                    "end": 1052
                },
                {
                    "start": 1053,
                    "end": 1156
                },
                {
                    "start": 1157,
                    "end": 1333
                },
                {
                    "start": 1334,
                    "end": 1459
                },
                {
                    "start": 1460,
                    "end": 1678
                },
                {
                    "start": 1679,
                    "end": 1852
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.12286376953125
        },
        {
            "corpus_id": "267682083",
            "title": "How to Train Data-Efficient LLMs",
            "text": "This is primarily true because of no batch size requirement for inference vs. large batch size requirement while training. This enables scaling-up hardware to happen via a large number of small-compute setups (e.g., 4 interconnected GPUs per node) versus increasing the number of large-compute setups (e.g., 1000s of interconnected GPUs per node). \n\n\u2022 ASK-LLM also uses strictly less compute compared to teacher-student knowledge distillation based training setups (Agarwal et al., 2023). This is true simply because knowledge distillation require (i) bigger teacher model's softmax predictions (ii) for each token in our training data. On the other hand, ASK-LLM requires just the score of the token \"yes\" given the prompt.",
            "score": 0.44803060327342736,
            "section_title": "A. Algorithms",
            "char_start_offset": 27640,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 347
                },
                {
                    "start": 350,
                    "end": 488
                },
                {
                    "start": 489,
                    "end": 636
                },
                {
                    "start": 637,
                    "end": 724
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0177764892578125
        },
        {
            "corpus_id": "270870624",
            "title": "Information-Theoretic Foundations for Neural Scaling Laws",
            "text": "The analysis of Hoffmann et al. [2022] is somewhat generic rather than specialized to the particular neural network architecture used in that paper.In this paper, building on the work of Jeon and Van Roy [2022a,b], we develop rigorous information-theoretic foundations and use them to derive similar scaling laws.To keep things simple and concrete, we carry out the analysis with a particular data generating process for which neural networks are well-suited.The sorts of arguments developed by Hoffmann et al. [2022] are just as relevant to this context as they are to language models.Hoffmann et al. [2022] suggest that the compute optimal trade-off between parameter count and number of training tokens is linear, though the authors expressed some doubt and considered other possibilities that are near-linear as well.\n\nWe establish an upper bound on the minimal information-theoretically achievable expected error as a function of p and T and derive the relation required to minimize this bound for each compute budget.For large compute budgets, this relation is linear, as suggested by Hoffmann et al. [2022].\n\nOur main contributions include a first rigorous mathematical characterization of the compute-optimal efficient frontier for a neural network model and development of information-theoretic tools which enable that.A limitation of our analysis is in its simplified treatment of computational complexity as the product of the model and data set sizes; we do not assume any constraints on computation beyond those imposed by choices of p and T .In particular, we analyze, algorithms which carry out perfect Bayesian inference with respect to a model that is misspecificified due to its restricted size.While this abstracts away the details of practical training algorithms, empirical evidence suggests that our idealized framework leads to useful approximations [Zhu et al., 2022].In spite of these limitations, we hope our results set the stage for further mathematical work to guide hyperparameter selection when training large neural networks.",
            "score": 0.44756050415837206,
            "section_title": "Introduction",
            "char_start_offset": 2159,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 148,
                    "end": 313
                },
                {
                    "start": 313,
                    "end": 459
                },
                {
                    "start": 459,
                    "end": 586
                },
                {
                    "start": 586,
                    "end": 821
                },
                {
                    "start": 823,
                    "end": 1023
                },
                {
                    "start": 1023,
                    "end": 1114
                },
                {
                    "start": 1116,
                    "end": 1328
                },
                {
                    "start": 1328,
                    "end": 1556
                },
                {
                    "start": 1556,
                    "end": 1713
                },
                {
                    "start": 1713,
                    "end": 1892
                },
                {
                    "start": 1892,
                    "end": 2057
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.08770751953125
        },
        {
            "corpus_id": "277103878",
            "title": "Tapered Off-Policy REINFORCE: Stable and efficient reinforcement learning for LLMs",
            "text": "This translates into greater self-consistency [Wang et al., 2022] efficiency: more solutions must be generated  Not shown here, TOPR also yields higher inference efficiency at test time: greater maj@n performance for all n. Bottom left: Distribution of GSM8K problem questions as a function of the number of correct generations. TOPR more effectively reduces the number of questions with none (0) to few (1-4) correct generations. Bottom right: Pass@1 accuracy on MATH. at test time to reach the same level of performance (top right). Breaking down the test results as a function of the number of rationales that conclude in the correct answer (\"Correct answer cardinality\", bottom left), we find that TOPR's performance gains from using negative examples can be attributed to reducing the number of questions for which no or few solutions are found, guaranteeing a strong majority for self-consistency. We find similar results on MATH, where using TOPR enables us to almost double the pass@1 accuracy compared to the base model (bottom right). Beyond these results, it is also worthwhile noting that TOPR is more training inference-efficient: indeed, because all vLLM generations are used to improve the model, training data is effectively generated as a faster rate. \n\nStriking the right balance of positive and negative examples. We now refine the previous analysis by varying the effective proportion of positive examples in the dataset. We do this by selecting two datasets containing 50,000 examples: one with 10% of positive examples (labelled \"10p\") and one with 50% of positive examples (\"50p\"). We then vary the baseline for each model to reach an effective proportion of positive examples from 1% all the way to 100% (achieved by setting the baseline to c = \u22121). \n\nFigure 5 shows that TOPR's performance is maximal around 10-20% of effective positive examples, regardless of the actual proportion of positive samples in the training -10% for the solid curve, 50% for the dashed curve. The performance drops if the proportion is either too small or too large, and markedly decreases as the proportion of positive examples goes above 50%.",
            "score": 0.4473627083247994,
            "section_title": "Fine-tuning chain-of-thought reasoning",
            "char_start_offset": 28448,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 223
                },
                {
                    "start": 224,
                    "end": 328
                },
                {
                    "start": 329,
                    "end": 430
                },
                {
                    "start": 431,
                    "end": 469
                },
                {
                    "start": 470,
                    "end": 534
                },
                {
                    "start": 535,
                    "end": 903
                },
                {
                    "start": 904,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1268
                },
                {
                    "start": 1271,
                    "end": 1332
                },
                {
                    "start": 1333,
                    "end": 1441
                },
                {
                    "start": 1442,
                    "end": 1604
                },
                {
                    "start": 1605,
                    "end": 1773
                },
                {
                    "start": 1776,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2147
                }
            ],
            "ref_mentions": [
                {
                    "start": 46,
                    "end": 65,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0030269622802734375
        },
        {
            "corpus_id": "274280605",
            "title": "Towards Precise Scaling Laws for Video Diffusion Transformers",
            "text": "Achieving optimal performance of video diffusion transformers within given data and compute budget is crucial due to their high training costs. This necessitates precisely determining the optimal model size and training hyperparameters before large-scale training. While scaling laws are employed in language models to predict performance, their existence and accurate derivation in visual generation models remain underexplored. In this paper, we systematically analyze scaling laws for video diffusion transformers and confirm their presence. Moreover, we discover that, unlike language models, video diffusion models are more sensitive to learning rate and batch size, two hyperparameters often not precisely modeled. To address this, we propose a new scaling law that predicts optimal hyperparameters for any model size and compute budget. Under these optimal settings, we achieve comparable performance and reduce inference costs by 40.1% compared to conventional scaling methods, within a compute budget of 1e10 TFlops. Furthermore, we establish a more generalized and precise relationship among validation loss, any model size, and compute budget. This enables performance prediction for non-optimal model sizes, which may also be appealed under practical inference cost constraints, achieving a better trade-off.",
            "score": 0.4468834113411255,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10968017578125
        },
        {
            "corpus_id": "258959511",
            "title": "Honey, I Shrunk the Language: Language Model Behavior at Reduced Scale",
            "text": "In recent years, language models have drastically grown in size, and the abilities of these models have been shown to improve with scale. The majority of recent scaling laws studies focused on high-compute high-parameter count settings, leaving the question of when these abilities begin to emerge largely unanswered. In this paper, we investigate whether the effects of pre-training can be observed when the problem size is reduced, modeling a smaller, reduced-vocabulary language. We show the benefits of pre-training with masked language modeling (MLM) objective in models as small as 1.25M parameters, and establish a strong correlation between pre-training perplexity and downstream performance (GLUE benchmark). We examine downscaling effects, extending scaling laws to models as small as ~1M parameters. At this scale, we observe a break of the power law for compute-optimal models and show that the MLM loss does not scale smoothly with compute-cost (FLOPs) below $2.2 \\times 10^{15}$ FLOPs. We also find that adding layers does not always benefit downstream performance.",
            "score": 0.4468662544204608,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06097412109375
        },
        {
            "corpus_id": "275789021",
            "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws",
            "text": "One widely used scaling law is the Chinchilla scaling law by Hoffmann et al. (2024). This law models the relationship between the loss L, the number of parameters N , and the number of training tokens D using the following equation: \n\nwhere A, B, \u03b1, \u03b2, and E are free parameters: A and \u03b1 describe how loss decreases with increasing model size N , while B and \u03b2 describe how loss decreases with increasing training tokens D. The constant E represents an irreducible loss. Scaling laws allow practitioners to optimize training configurations to fit within their compute budgets without running costly experiments (Sardana et al., 2024). The laws also hold theoretical value, as they capture the dynamics of how loss changes with data and parameter scaling. We present an overview of existing sparse scaling law in Appendix A. \n\nModels. We pre-train both sparse and dense models with starting parameter count ranging from 58M to 468M. We use the LLaMA 2 (Touvron et al., 2023) base model architecture. For each unique model size, we train two versions: one using over 10x the number of tokens corresponding to Chinchilla optimal, and the other using over 20x Chinchilla optimal. We train substantially past Chinchilla-optimal dataset size, following prevailing practice (Touvron et al., 2023;Grattafiori et al., 2024) which benefits inference efficiency. \n\nTable 1: Training details for models by size and token count. Numbers in parentheses show the token-to-prunable parameter ratio. \n\nTable 1 provides model details. Each model name contains the parameter count and a suffix indicating training duration: \"10x\" means training tokens exceed 10x Chinchilla-optimal, and \"20x\" means they exceed 20x (Hoffmann et al., 2024). We prune only linear layer parameters, leaving embedding and normalization layers dense. For each dense model, we list its total training tokens and the ratio of tokens to prunable parameters (in parentheses). We train four sparse variants for each dense model, matching their training compute while varying sparsity from 20% to 80%. \n\nDataset.",
            "score": 0.44632976077153347,
            "section_title": "PRELIMINARIES",
            "char_start_offset": 9084,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 84
                },
                {
                    "start": 85,
                    "end": 232
                },
                {
                    "start": 235,
                    "end": 470
                },
                {
                    "start": 471,
                    "end": 634
                },
                {
                    "start": 635,
                    "end": 754
                },
                {
                    "start": 755,
                    "end": 823
                },
                {
                    "start": 826,
                    "end": 833
                },
                {
                    "start": 834,
                    "end": 931
                },
                {
                    "start": 932,
                    "end": 998
                },
                {
                    "start": 999,
                    "end": 1175
                },
                {
                    "start": 1176,
                    "end": 1351
                },
                {
                    "start": 1354,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1482
                },
                {
                    "start": 1485,
                    "end": 1516
                },
                {
                    "start": 1517,
                    "end": 1720
                },
                {
                    "start": 1721,
                    "end": 1809
                },
                {
                    "start": 1810,
                    "end": 1930
                },
                {
                    "start": 1931,
                    "end": 2054
                },
                {
                    "start": 2057,
                    "end": 2065
                }
            ],
            "ref_mentions": [
                {
                    "start": 61,
                    "end": 83,
                    "matchedPaperCorpusId": "247778764"
                },
                {
                    "start": 611,
                    "end": 633,
                    "matchedPaperCorpusId": "266693796"
                },
                {
                    "start": 1696,
                    "end": 1719,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.30029296875
        },
        {
            "corpus_id": "274131186",
            "title": "Character-level Tokenizations as Powerful Inductive Biases for RNA Foundational Models",
            "text": "We follow Hoffmann et al. (2022) to fit a parametric loss function. Using the data collected during the experimental phases, we fit the power laws to establish the relationships N opt \u221d C a and D opt \u221d C b , where N is the model size, D is the number of tokens, and C represents the computational budget in FLOPs1 . The exponents a and b were determined based on the fitted parameters, of the scaling law: \n\nHere, N is the number of model parameters, D is the size of the data set (in tokens), and E captures the natural entropy of the text (ideal loss). The terms with A and B reflect the deviation of the model from the ideal loss, due to the limited size of the model and data. The exponents \u03b1 and \u03b2 determine the impact of model and dataset size on the loss2 . \n\nWe fit the expression in Eq. 16 following (Hoffmann et al., 2022) 3 . In particular, we utilized the most optimal results obtained for each model size from the experiment described in Section 5.2. This analysis was intended to capture the effects of both model size and dataset size on the model performance. \n\nWe found that for both GBST and EM, the final loss decreases predictably as the model parameters increase (Figure 11), following a general trend of improvement with larger models. However, in both cases, improvements plateau or even cease to be significant once a certain parameter threshold is exceeded, particularly around 30 to 50 million parameters. \n\nFrom the obtained parameters we derived the power-law exponents for model size and token count as a function of compute N opt \u221d C 0.2279 and D opt \u221d C 0.7720 . A key finding is that the optimal model size scales sublinearly with the compute budget. Specifically, the relationship N opt \u221d C 0.2279 indicates that the model size grows at a slower rate than the compute budget. In contrast, the optimal number of training tokens scales superlinearly with compute. The relationship D opt \u221d C 0.7720 shows that, as compute grows, the number of training tokens increases more rapidly than the model size.",
            "score": 0.44630107904202976,
            "section_title": "Increasing model size",
            "char_start_offset": 33149,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 67
                },
                {
                    "start": 68,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 405
                },
                {
                    "start": 408,
                    "end": 554
                },
                {
                    "start": 555,
                    "end": 680
                },
                {
                    "start": 681,
                    "end": 764
                },
                {
                    "start": 767,
                    "end": 836
                },
                {
                    "start": 837,
                    "end": 963
                },
                {
                    "start": 964,
                    "end": 1075
                },
                {
                    "start": 1078,
                    "end": 1257
                },
                {
                    "start": 1258,
                    "end": 1431
                },
                {
                    "start": 1434,
                    "end": 1593
                },
                {
                    "start": 1594,
                    "end": 1682
                },
                {
                    "start": 1683,
                    "end": 1808
                },
                {
                    "start": 1809,
                    "end": 1894
                },
                {
                    "start": 1895,
                    "end": 2032
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.05718994140625
        },
        {
            "corpus_id": "267782738",
            "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
            "text": "C. The crosses denote the points where the scheduler switches patch size. We observe a significant improvement in terms of compute efficiency, allowing for up to \u221260% FLOPs to achieve the same performance. While switching patch sizes may initially result in a slight decrease, attributed to changes in the entropy within the self-attention layers, this minor setback is rapidly offset as the image is parsed in a more fine-grained manner. Such degradation is thus not even visible in Fig. 5 4 . \n\nTo facilitate comparison across all model sizes at once, we further visualize the compute-optimal barrier for both fixed and scheduled training in Fig. 6. By compute-optimal, we refer to a model that optimally trades off model size, patch size, and number of samples seen for a given level of compute C, i.e. achieving the lowest error E. We observe that the optimally scheduled models significantly outperform the optimal static models, halving the required compute to optimally train a ViT-Base model (for our compute budget). Figure 6: Compute-optimal static and scheduled models for various patch and model sizes. We plot using a log-log scale. Is the schedule optimal? While our scheduler improves over the individual models, it is not clear yet that it does so in an optimal sense, i.e. can other schedules achieve similar benefits? Beyer et al. \n\n(2023) also employ a patch size scheduler but use a uniformly random sampling of the patch size at every step. We compare against their model FlexiViT in Fig. 7 and observe that our scheduler indeed outperforms FlexiViT as expected; FlexiViT targets a lower inference cost by making the model robust to many patch sizes (hence the random scheduler). Compute-optimality is not their objective. Additionally, we conduct comparisons with straightforward patch size scheduling strategies-both linear and logarithmic. Specifically, for a predetermined total computational budget, we distribute the transition points evenly or according to a logarithmic scale across the training process. This way, we assess whether simply any monotonic scheduler leads to the same improvements, or whether the position of the transition points matter. We display the results in Fig. 7.",
            "score": 0.44584515544003755,
            "section_title": "ADAPTIVE PATCH SIZES AND TRAVERSING SCALING LAWS",
            "char_start_offset": 15527,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 73
                },
                {
                    "start": 74,
                    "end": 205
                },
                {
                    "start": 206,
                    "end": 438
                },
                {
                    "start": 439,
                    "end": 494
                },
                {
                    "start": 497,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1114
                },
                {
                    "start": 1115,
                    "end": 1145
                },
                {
                    "start": 1146,
                    "end": 1170
                },
                {
                    "start": 1171,
                    "end": 1335
                },
                {
                    "start": 1336,
                    "end": 1348
                },
                {
                    "start": 1351,
                    "end": 1461
                },
                {
                    "start": 1462,
                    "end": 1700
                },
                {
                    "start": 1701,
                    "end": 1743
                },
                {
                    "start": 1744,
                    "end": 1863
                },
                {
                    "start": 1864,
                    "end": 2033
                },
                {
                    "start": 2034,
                    "end": 2181
                },
                {
                    "start": 2182,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.18701171875
        },
        {
            "corpus_id": "276647416",
            "title": "Thinking Slow, Fast: Scaling Inference Compute with Distilled Reasoners",
            "text": "Scaling inference time compute has emerged as a promising strategy to improve the performance of LLMs. Techniques such as Chain of Thought (CoT) and its variants have demonstrated significant performance improvements across various reasoning benchmarks by decomposing complex tasks into intermediate steps (Wei et al., 2023;Yao et al., 2023) While these approaches improve reasoning through task decomposition, they also increase computational demands due to longer generation sequences. Recent work suggests that this additional compute may itself contribute to improved model abilities (Pfau et al., 2024). Dynamic compute allo-cation during inference has further advanced this paradigm. For instance, Goyal et al. (2024) introduced pause tokens into the LLM vocabulary, enabling models to allocate compute more effectively and achieve better reasoning and task performance. \n\nAnother prominent approach involves generating and searching through multiple model outputs to select the best answer. Various sampling algorithms have been proposed to increase the diversity and quality of generated outputs to increase the likelihood of the correct or best answer being selected (Wang et al., 2023;Renze & Guven, 2024;Zhang et al., 2023). In parallel, outcome and process reward models (ORMs and PRMs) have been introduced to help evaluate the best response and guide intermediate generation steps within the LLM model (Lightman et al., 2023;Zhang et al., 2024a;Luo et al., 2024;Uesato et al., 2022). \n\nRecent work has shown that smaller LLMs, when scaled through inference-time compute (e.g., via majority voting or PRM-guided search), can outperform larger models under fixed compute budgets (Snell et al., 2024;Wu et al., 2024;Beeching et al., 2024). However, these findings are primarily limited to Transformer-based architectures. The extent to which these scaling laws apply to subquadratic architectures, which offer faster inference but may trade off expressiveness, remains underexplored.",
            "score": 0.44478774446719993,
            "section_title": "Scaling Inference Time Compute for Reasoning",
            "char_start_offset": 4487,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 487
                },
                {
                    "start": 488,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 689
                },
                {
                    "start": 690,
                    "end": 876
                },
                {
                    "start": 879,
                    "end": 997
                },
                {
                    "start": 998,
                    "end": 1235
                },
                {
                    "start": 1236,
                    "end": 1497
                },
                {
                    "start": 1500,
                    "end": 1750
                },
                {
                    "start": 1751,
                    "end": 1832
                },
                {
                    "start": 1833,
                    "end": 1994
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.03338623046875
        },
        {
            "corpus_id": "265308588",
            "title": "Applications of Large Scale Foundation Models for Autonomous Driving",
            "text": "There is a power-law relationship [22] between model performance and each of the following three factors: the number of non-embedding model parameters N, the training dataset size in tokens D, and the amount of non-embedding compute C. There exists an optimal budget allocation between the model size and the amount of data processed in training. They demonstrate that within a fixed compute budget (the term \"compute budget\" refers to the total amount of computations), the optimal model performance is obtained by training very large models and stopping early before convergence. \n\nAnother scaling law [38] claims that given the compute budget, the number of data tokens processed in training should be scaled equally to the size of the model. They show that smaller models that are adequately trained can overperform undertrained large models. The above work summarizes the empirical laws for deciding the size of the dataset under a fixed budget. \n\nData parallelism [47] distribute the whole training corpus into multiple GPUs with replicated model parameters and states. Data parallel running techniques can be split into two categories: asynchronous and synchronous data parallelism. \n\nAll-gather and all-reduce communication patterns are often used in data parallelism. All-gather patterns let every processor communicates its data to every other processor. All-reduce patterns are a layer on top of all-gather combining aggregation with summing or averaging. \n\nThe well-known asynchronous method is Parameter Server, where one server saves a baseline set of parameters while distributed workers keep model replicas that train on different mini-batches. The popular synchronous data parallelism method is Distributed Data Parallelism (DDP). DDP clones a model and allocates copies to m different workers. An initial \"global minibatch\" is used, then split evenly across the replicas to make local gradient updates. These gradients are then aggregated across replicas to generate an entire update, typically using an all-reduce communication pattern. \n\nModel parallelism [47] is the technique of splitting, or sharding, a neural architecture graph into subgraphs, and each subgraph, or model shard, is assigned to an individual GPU. These shards might correspond to groups of stacked layers in a feedforward network.",
            "score": 0.4444787588889456,
            "section_title": "Fig. 4. LLaMA 2 Training [59]",
            "char_start_offset": 8180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 235
                },
                {
                    "start": 236,
                    "end": 346
                },
                {
                    "start": 347,
                    "end": 581
                },
                {
                    "start": 584,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 846
                },
                {
                    "start": 847,
                    "end": 950
                },
                {
                    "start": 953,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1189
                },
                {
                    "start": 1192,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1466
                },
                {
                    "start": 1469,
                    "end": 1660
                },
                {
                    "start": 1661,
                    "end": 1747
                },
                {
                    "start": 1748,
                    "end": 1811
                },
                {
                    "start": 1812,
                    "end": 1920
                },
                {
                    "start": 1921,
                    "end": 2055
                },
                {
                    "start": 2058,
                    "end": 2237
                },
                {
                    "start": 2238,
                    "end": 2321
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.234619140625
        },
        {
            "corpus_id": "273695905",
            "title": "Does equivariance matter at scale?",
            "text": "Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models.",
            "score": 0.44427918454218507,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1104736328125
        },
        {
            "corpus_id": "238857020",
            "title": "Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers",
            "text": "Recently there has been a gain in interest in studying the effects of scale in machine learning, although only few studies have been done so far. Hestness et al. (2017) are the first to study the manner in which deep learning scales predictably empirically via a power law functional form. They specifically study how loss scales with respect to training dataset size on language modelling, image classification, and speech recognition tasks. Zhao et al. (2018) perform multiple experiments where they isolate features such as number of element and color and check if GANs (Goodfellow et al., 2014) and VAEs (Kingma & Welling, 2013) manage to generalize or not. For example, when training with images containing always three objects, they show that generative models do generate samples containing fewer, equal and more objects. They show the same thing happening for color and when mixing these features. They find that when the number of combinations is low in the training set, the model can simply learn to memorize them. When they increase the number of combinations though, that's when the model starts to generate new combinations. \n\nMore recently, Kaplan et al. (2020) study scaling laws for Transformer (Vaswani et al., 2017) language models. They find that performance has a power law relationship with the dataset size, model size and amount of training individually when not bottlenecked by the other two parameters. Additionally, for an equal number of parameters, tuning within a reasonable range the Transformer's depth versus width has little impact on performance. They also demonstrate that overfitting follows a simple ratio between model and dataset size. They show that larger models are more sample efficient and that when training compute is on a budget, but model and dataset size is not, optimal performance is obtained by training very large models and early stopping, rather than training smaller models to convergence. They also find that, when out-of-distribution, the language modelling performance correlates with the validation performance of the training problem, offset by a constant. \n\nSimilarly, Brown et al. (2020) train language models of various sizes, then evaluate them on multiple downstream natural language processing tasks without any fine-tuning and restrictions on the amount of training data or training compute budget. They show that performance scales in relationship with model size.",
            "score": 0.4440647748090263,
            "section_title": "RELATED WORK",
            "char_start_offset": 16403,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 145
                },
                {
                    "start": 146,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 442
                },
                {
                    "start": 443,
                    "end": 661
                },
                {
                    "start": 662,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 905
                },
                {
                    "start": 906,
                    "end": 1025
                },
                {
                    "start": 1026,
                    "end": 1138
                },
                {
                    "start": 1141,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1428
                },
                {
                    "start": 1429,
                    "end": 1581
                },
                {
                    "start": 1582,
                    "end": 1675
                },
                {
                    "start": 1676,
                    "end": 1946
                },
                {
                    "start": 1947,
                    "end": 2118
                },
                {
                    "start": 2121,
                    "end": 2367
                },
                {
                    "start": 2368,
                    "end": 2434
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1044921875
        },
        {
            "corpus_id": "276618147",
            "title": "(Mis)Fitting: A Survey of Scaling Laws",
            "text": "We provide one possible set of responses to our checklist, reflective of some recommendations enumerated above, and loosely based on Hoffmann et al. (2022). These answers roughly correspond to a subset of the experiments we run in \u00a77. Ours (all checkpoints) (a) \u00a73, \u00a77.1 Using data from both Besiroglu et al. (2024) (left) and our own models (right), we compare the effects of fitting to the power law form used in Approach 3 of Hoffmann et al. (2022) with the variant used by Muennighoff et al. (2024), which assumes that the exponents \u03b1, \u03b2 are equal -equivalently, that N * (C) and D * (C) scale about linearly with each other. When using only the performance of final checkpoints from both Besiroglu et al. (2024) and our own experiments, taking this assertion results in a law much closer to the one reported by Hoffmann et al. (2022). On our own models, we also show results when using the IsoFLOP approach from Hoffmann et al. (2022). As we are using only results from final model checkpoints, the size of the data input to the IsoFLOP approach in this case is reduced. Ours (all checkpoints) (b) \u00a74, \u00a77.2 With our models, we simulate the effects of not sweeping the learning rate. As a baseline, (1) we sweep at each (N , D) pair for the optimal learning rate over a range of values at most a multiple of 2 apart. Next, (2) we use a learning rate of 1e-3 for all N , the optimal for our 1 billion parameter models, and do the same for (3) 2e-3 and (4) 4e-3, which is optimal for our 12 million parameter models. Lastly, we use all models across all learning rates at the same N and D. Results vary dramatically across these settings.",
            "score": 0.4439982908604859,
            "section_title": "D.1 EXAMPLE CHECKLIST",
            "char_start_offset": 51011,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 156
                },
                {
                    "start": 157,
                    "end": 234
                },
                {
                    "start": 235,
                    "end": 629
                },
                {
                    "start": 630,
                    "end": 839
                },
                {
                    "start": 840,
                    "end": 940
                },
                {
                    "start": 941,
                    "end": 1075
                },
                {
                    "start": 1076,
                    "end": 1111
                },
                {
                    "start": 1112,
                    "end": 1187
                },
                {
                    "start": 1188,
                    "end": 1320
                },
                {
                    "start": 1321,
                    "end": 1518
                },
                {
                    "start": 1519,
                    "end": 1640
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0252227783203125
        },
        {
            "corpus_id": "270391350",
            "title": "Scaling Laws in Linear Regression: Compute, Parameters, and Data",
            "text": "We emphasize that the scaling law bound in Theorem 4.1 holds for every M, N \u2265 1. We also remark that the sum of approximization and bias errors dominates ER M (v N ) \u2212 \u03c3 2 , whereas the variance error is of strict higher order in terms of both M and N , and is thus disappeared in the population risk bound. \n\nOptimal stepsize. Based on the tight scaling law in Theorem 4.1, we can calculate the optimal stepsize that minimizes the risk. Specifically, the optimal stepsize is \u03b3 \u2242 1 when N eff \u2272 M a and can be anything such that M a /N eff \u2272 \u03b3 \u2272 1 when N eff \u2273 M a . In both cases, choosing \u03b3 \u2242 1 is optimal. When the sample size is large such that N eff \u2273 M a , the optimal stepsize is relatively robust and can be chosen from a range. \n\nAllocation of data and model sizes. Following Hoffmann et al. (2022), we measure the compute complexity by M N as (SGD) queries M -dimensional gradients for N times. Given a total compute budget of M N = C, from Theorem 6.1 and N eff := N/ log(N ), we see that the best population risk is achieved by setting \u03b3 = \u0398(1), M = \u0398(C 1/(a+1) ), and N = \u0398(C a/(a+1) ). Our theory suggests setting a data size slightly larger than the model size when the compute budget is the bottleneck. \n\nComparison with (Bordelon et al., 2024). The work by Bordelon et al. (2024) considered the scaling law of batch gradient descent (or gradient flow) on a teacher-student model (see their equation ( 14)). Their teacher-student model can be viewed as our sketched linear regression model. However, we consider one-pass SGD, therefore in our setting the number of gradient steps is equivalent to the data size. When we equalize the number of gradient steps and the data size in their equation ( 14) and set the parameter prior as Assumption 1C, their prediction is consistent with ours.",
            "score": 0.4439982908604859,
            "section_title": "Scaling laws",
            "char_start_offset": 17812,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 80
                },
                {
                    "start": 81,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 327
                },
                {
                    "start": 328,
                    "end": 437
                },
                {
                    "start": 438,
                    "end": 566
                },
                {
                    "start": 567,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 736
                },
                {
                    "start": 739,
                    "end": 774
                },
                {
                    "start": 775,
                    "end": 904
                },
                {
                    "start": 905,
                    "end": 1099
                },
                {
                    "start": 1100,
                    "end": 1218
                },
                {
                    "start": 1221,
                    "end": 1261
                },
                {
                    "start": 1262,
                    "end": 1423
                },
                {
                    "start": 1424,
                    "end": 1506
                },
                {
                    "start": 1507,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1803
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0360107421875
        },
        {
            "corpus_id": "273877632",
            "title": "Scaling Laws for Precision",
            "text": "Size N is Constrained Minimizing L(D, P ) with N fixed, subject to C \u221d N DP . A common use case in practice is to train a suite of models of various sizes on similar data. The Llama-3 and Gemma-2 series [Dubey et al., 2024, Team et al., 2024] are examples. In this setting, N is fixed in advance and only D, P are jointly optimized. Surprisingly, our scaling laws predict that models of differing sizes should not necessarily be trained in the same precision, and that compute-optimal precision scales as P * (C) \u221d log C. Since N is held constant and we show in Appendix E that log C \u2248 log D in proportion, we can write P * (C) \u221d log(D/N ). The intuition for this is that, for a fixed N , precision acts as a new lever to bring highly overtrained models closer to pretraining optimality 3 by reducing D/N eff . \n\nFinding 3. When N, D, P are optimized jointly, compute-optimal pretraining precision is independent of compute. 16-bit has many unnecessary bits, and 4-bit requires increasing the model size disproportionately to maintain loss scaling. Our fits imply that 7-8 bits are compute-optimal. In contrast, when N is fixed in advance, such as when training a model family on similar data, P * (C) \u221d log C. This suggests that for models that will be significantly overtrained, higher precision during training may be compute-optimal. Two competing effects at play during post-train quantization. Intuitively, training any of P w , P a , P kv in low precision forces the model to learn weights that are robust to \"quantization noise,\" so they degrade less under PTQ. But the reduced N \u2192 N eff implies that models trained in low precision will degrade more because \u03b4 PTQ increases with N \u2212\u03b3 N (Section 3). We call this second effect the \"overtraining\" effect. In practice, the first \"robustification\" effect is larger, so that models trained in lower precision overall degrade less when post-train quantized.",
            "score": 0.4439982908604859,
            "section_title": "But Compute-Optimal Pretraining Precision Can Increase in Compute if Model",
            "char_start_offset": 21258,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 77
                },
                {
                    "start": 78,
                    "end": 171
                },
                {
                    "start": 172,
                    "end": 256
                },
                {
                    "start": 257,
                    "end": 332
                },
                {
                    "start": 333,
                    "end": 640
                },
                {
                    "start": 641,
                    "end": 810
                },
                {
                    "start": 813,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1210
                },
                {
                    "start": 1211,
                    "end": 1337
                },
                {
                    "start": 1338,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1569
                },
                {
                    "start": 1570,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1761
                },
                {
                    "start": 1762,
                    "end": 1910
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0889892578125
        },
        {
            "corpus_id": "262152780",
            "title": "Neural scaling of deep chemical models",
            "text": "For large language and computer vision models trained to convergence with sufficient model parameters and/or data, performance is characterized by empirical scaling laws where the loss scales as a power law 18 of the form \n\nfor coefficient \u03b1, scaling exponent \u03b2 and resource R. R is the number of model parameters, dataset size or compute. \u03b2 measures the slope of the power law and indicates the scaling efficiency of the model with respect to a scaling factor, R. The power-law trends break down in 'resolution limited' regimes 34 , indicating that the model (dataset) size is insufficient for the given amount of data (model parameters). \n\nNeural scaling presents a best-case scenario for model pre-training loss improvements with increasing resources, and allows for optimal allocation of fixed budgets, for example, to decide whether longer training, more data or larger models will be most efficient for improving pre-training loss. Comparing neural-scaling exponents also provides a fundamental metric for measuring resource efficiency across model architectures. Investigations into neural scaling in the NLP domain have revealed general conclusions about overfitting, sensitivity to architectural choices, transfer learning and sample efficiency 18 . These factors are equally or more important in scientific deep learning applications, where rapid advances are being made in specialized architecture development, and it is often unclear how architectures will perform beyond the small benchmark datasets that are commonly available in scientific settings.",
            "score": 0.44392037677495677,
            "section_title": "Neural scaling",
            "char_start_offset": 24239,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 221
                },
                {
                    "start": 224,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 639
                },
                {
                    "start": 642,
                    "end": 937
                },
                {
                    "start": 938,
                    "end": 1069
                },
                {
                    "start": 1070,
                    "end": 1258
                },
                {
                    "start": 1259,
                    "end": 1564
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.076416015625
        },
        {
            "corpus_id": "272525111",
            "title": "Optimization Hyper-parameter Laws for Large Language Models",
            "text": "Large Language Models have driven significant AI advancements, yet their training is resource-intensive and highly sensitive to hyper-parameter selection. While scaling laws provide valuable guidance on model size and data requirements, they fall short in choosing dynamic hyper-parameters, such as learning-rate (LR) schedules, that evolve during training. To bridge this gap, we present Optimization Hyper-parameter Laws (Opt-Laws), a framework that effectively captures the relationship between hyper-parameters and training outcomes, enabling the pre-selection of potential optimal schedules. Grounded in stochastic differential equations, Opt-Laws introduce novel mathematical interpretability and offer a robust theoretical foundation for some popular LR schedules. Our extensive validation across diverse model sizes and data scales demonstrates Opt-Laws' ability to accurately predict training loss and identify optimal LR schedule candidates in pre-training, continual training, and fine-tuning scenarios. This approach significantly reduces computational costs while enhancing overall model performance.",
            "score": 0.4436594278536846,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.054290771484375
        },
        {
            "corpus_id": "259243822",
            "title": "Scaling MLPs: A Tale of Inductive Bias",
            "text": "One of the key mysteries in deep learning is that networks tend to improve in terms of generalization when compute, in the form of parameter count and dataset size, is scaled up. Recently it has been observed in several works that the benefits of scale are highly predictable, i.e. generalization performance exhibits a power-law structure when plotted against compute measured in FLOPS (Rosenfeld et al., 2020;Hestness et al., 2017Hestness et al., , 2019;;Kaplan et al., 2020;Zhai et al., 2022). The functional form has recently been further refined (Caballero et al., 2023). The predictable nature of test performance has even been leveraged to estimate the optimal model before training (Hoffmann et al., 2022;OpenAI, 2023). In order to understand this important characteristic of deep learning theoretically, it is important to analyze whether MLPs exhibit similar properties. \n\nFigure 5: Test error (in %) on CIFAR10 (left) and ImageNet1k (right) when linearly transferred as a function of PFLOPS, measured according to Eq.( 4), on a log-log scale. \n\nwhere FLOP(f ) denotes the number of FLOPs needed to complete the forward pass of f for a single example. We note that the number of parameters P present in f enters this equation implicitly in the form of FLOP(f ) \u221d P . Observe that a given level of compute can be achieved in different ways, i.e. using more parameters P , training on more examples N , or training for a longer time T . When allocating a given level of compute optimally, it is observed that for convolutional and transformer-based architectures, the test error E(C) as a function of compute behaves as a power-law \n\nwhere a, b, E \u221e \u2208 R + and \u03b1 > 0 is the scaling coefficient determining the rate of decay. E \u221e denotes the irreducible error, i.e. even if infinite compute were employed, the performance remains imperfect. \n\nThe test error can be measured upstream (i.e. on the pre-training task) or downstream when fine-tuning on a different task.",
            "score": 0.44358426284100594,
            "section_title": "Scaling Laws",
            "char_start_offset": 23082,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 178
                },
                {
                    "start": 179,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 576
                },
                {
                    "start": 577,
                    "end": 727
                },
                {
                    "start": 728,
                    "end": 880
                },
                {
                    "start": 883,
                    "end": 1053
                },
                {
                    "start": 1056,
                    "end": 1161
                },
                {
                    "start": 1162,
                    "end": 1276
                },
                {
                    "start": 1277,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1639
                },
                {
                    "start": 1642,
                    "end": 1731
                },
                {
                    "start": 1732,
                    "end": 1846
                },
                {
                    "start": 1849,
                    "end": 1972
                }
            ],
            "ref_mentions": [
                {
                    "start": 387,
                    "end": 411,
                    "matchedPaperCorpusId": "203592013"
                },
                {
                    "start": 477,
                    "end": 495,
                    "matchedPaperCorpusId": "235367962"
                },
                {
                    "start": 551,
                    "end": 575,
                    "matchedPaperCorpusId": "253117181"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1649169921875
        },
        {
            "corpus_id": "273162606",
            "title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
            "text": "To explore the potentials and trends of the scaling with rollouts in inference-time, we depict the solved rate of problems with rollouts in three benchmarks with different difficulty levels. Analyzing the performance alongside Figure 3, the increment in the number of rollouts consistently enhances  (Brown et al., 2024) has demonstrated its effectiveness in enhancing the probability of finding the correct answers. Self-Consistency (Wang et al., 2022) samples a complete path each time while tree search methods like Tree-of-Thought (ToT) (Yao et al., 2023) and Monte Carlo Tree Search (MCTS) (Chen et al., 2024a,b;Luo et al., 2024b;Feng et al., 2023;Xie et al., 2024;Xu, 2023;Liu et al., 2023;Tian et al., 2024;Ding et al., 2023) extend multiple steps to optimize step answers and ultimately obtain the optimal solution. Additionally, Self-Refine (Madaan et al., 2023a) method has become a recent focus. Self-verification (Gero et al., 2023;Weng et al., 2022) and rStar (Qi et al., 2024) utilize the inherent capabilities of the model to iteratively explore and refine answers. However, the performance of Self-Refine is typically constrained by the inherent capabilities of the model, especially for small language models (SLMs) with significantly weaker Self-Refine abilities (Madaan et al., 2023b). Zhang et al. (2024b) suggests that the mathematical reasoning abilities of LLMs can be enhanced by treating the refinement process as a directed acyclic graph (DAG) through multi-agent collaboration. In our approach, we combine MCTS with Self-Refine to explore potential solutions and a global win-loss matrix is then constructed in the form of a directed graph to calculate the final quantile scores.",
            "score": 0.4430972472285383,
            "section_title": "Scaling Study",
            "char_start_offset": 21622,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 190
                },
                {
                    "start": 191,
                    "end": 416
                },
                {
                    "start": 417,
                    "end": 823
                },
                {
                    "start": 824,
                    "end": 906
                },
                {
                    "start": 907,
                    "end": 1080
                },
                {
                    "start": 1081,
                    "end": 1304
                },
                {
                    "start": 1305,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1706
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0095977783203125
        },
        {
            "corpus_id": "273228196",
            "title": "Scaling Laws Across Model Architectures: A Comparative Analysis of Dense and MoE Models in Large Language Models",
            "text": "The architecture of all models (including Dense Models and MoE Models) are shown in table 3 and  table 4, respectively. The number of layers, attention heads, hidden dimensions, and other relevant details are listed in the tables. \n\nDuring training, we employed the AdamW optimizer with parameters \u03b2 1 = 0.9 and \u03b2 2 = 0.95 for all models. Following the Chinchilla law (Hoffmann et al., 2022), we established a maximum learning rate of 1.5 \u00d7 10 \u22123 for smaller models and 2 \u00d7 10 \u22124 for larger ones. A cosine scheduler with a 10x learning rate decay was implemented throughout the training process. We applied Gaussian smoothing with a 10-step window length to enhance the training curve. \n\nSpecifically, we identified the best performance values within our hyperparameter range. The range of hyperparameter settings, including batch size and learning rate, was carefully selected for each model size to ensure optimal performance within the designated FLOP budget. Our observations indicate that performance tends to converge to optimal values around the neighborhood of the best settings, as illustrated in Figure 7. The dataset we used for the training of Dense Models and MoE Models is The Pile (Gao et al., 2020), which is a 825 GiB English text corpus consisting of 22 high-quality subsets. Due to the significant costs associated with training process, the scaling laws of large language models (LLMs) is crucial. Some previous studies (Kaplan et al., 2020;Bahri et al., 2021) have proposed a power-law relationship between loss and various factors like the number of non-embedding parameters, training tokens, and compute budget across different magnitudes. Notably, Kaplan et al. (2020) found that increasing the model size by 8 times only requires a roughly 5x increase in data to avoid penalties. In contrast to earlier findings, Hoffmann et al. ( 2022) implements optimized training configurations, which include the use of training tokens and learning rate schedules, and recommends scaling training tokens in proportion to model size. Additionally, research by Bi et al. (2024) explores the scaling laws of batch size and learning rate in relation to model scale (non-embedding FLOPs per token), offering a more precise estimation.",
            "score": 0.44262978836438255,
            "section_title": "A Experimental Settings",
            "char_start_offset": 19945,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 119
                },
                {
                    "start": 120,
                    "end": 230
                },
                {
                    "start": 233,
                    "end": 338
                },
                {
                    "start": 339,
                    "end": 496
                },
                {
                    "start": 497,
                    "end": 595
                },
                {
                    "start": 596,
                    "end": 685
                },
                {
                    "start": 688,
                    "end": 776
                },
                {
                    "start": 777,
                    "end": 962
                },
                {
                    "start": 963,
                    "end": 1115
                },
                {
                    "start": 1116,
                    "end": 1293
                },
                {
                    "start": 1294,
                    "end": 1417
                },
                {
                    "start": 1418,
                    "end": 1662
                },
                {
                    "start": 1663,
                    "end": 1804
                },
                {
                    "start": 1805,
                    "end": 2045
                },
                {
                    "start": 2046,
                    "end": 2242
                }
            ],
            "ref_mentions": [
                {
                    "start": 368,
                    "end": 391,
                    "matchedPaperCorpusId": "246473179"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.07611083984375
        },
        {
            "corpus_id": "39487",
            "title": "Length bias in Encoder Decoder Models and a Case for Global Conditioning",
            "text": "During training we use maximum likelihood to estimate \u03b8 given a large set of valid input-output pairs {(x 1 , y 1 ), . . . , (x N , y N )} where each y i belongs to Y which in general is much larger than W. Our main challenge during training is that Y is intractably large for computing Z. We decompose Z as \n\nand then resort to estimating the last term using importance sampling. Constructing a high quality proposal distribution over Y /y is difficult in its own right, so in practice, we make the following approximations. We extract the most common T sequences across a data set into a pool of negative examples. We estimate the empirical prior probability of the sequences in that pool, Q(y), and then draw k samples from this distribution. We take care to remove the true sequence from this distribution so as to remove the need to estimate its prior probability. \n\nDuring inference, given an input x we need to find argmax y\u2208W s(y|x, \u03b8). This task can be performed efficiently in our network because the vectors v y for the sequences y in the whitelist W can be precomputed. Given an input x, we compute v x and take dot-product with the pre-computed vectors to find the highest scoring response. This gives us the optimal response. When W is very large, we can obtain an approximate solution by indexing the vectors v y of W using recent methods specifically designed for dot-product based retrieval (Guo et al., 2016).",
            "score": 0.4424501731439011,
            "section_title": "Training and Inference",
            "char_start_offset": 13237,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 122
                },
                {
                    "start": 123,
                    "end": 289
                },
                {
                    "start": 290,
                    "end": 307
                },
                {
                    "start": 310,
                    "end": 380
                },
                {
                    "start": 381,
                    "end": 525
                },
                {
                    "start": 526,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 745
                },
                {
                    "start": 746,
                    "end": 869
                },
                {
                    "start": 872,
                    "end": 944
                },
                {
                    "start": 945,
                    "end": 1081
                },
                {
                    "start": 1082,
                    "end": 1203
                },
                {
                    "start": 1204,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1427
                }
            ],
            "ref_mentions": [
                {
                    "start": 1408,
                    "end": 1426,
                    "matchedPaperCorpusId": "2863491"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.003124237060546875
        },
        {
            "corpus_id": "276408405",
            "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps through Fill-in-the-Middle Task",
            "text": "Just as the scaling law in model training applies, there is also a scaling law for LLMs during testtime. The former improves the model's reasoning ability by providing more training data (Hoffmann et al., 2022), while the latter increases the model's computational load during inference to enhance calculation accuracy, thereby improving performance (Brown et al., 2024;Snell et al., 2024). \n\nExpanding reasoning steps is one way to enhance the test-time computation of LLMs. By generating more detailed reasoning steps during inference, the model's reasoning performance can be improved. \n\nThere are several ways to expand reasoning steps. \n\nFor example, in a training-free approach, prompts like Chain-of-Thought (Wei et al., 2023) can guide the model to perform more detailed reasoning. Using self-consistency (Wang et al., 2023b) to perform multiple reasoning paths and vote on the most consistent answers is another option. Additionally, methods like tree-search combined with a verifier can be used to select the optimal reasoning path (Chen et al., 2024;Feng et al., 2024;Guan et al., 2025). On the other hand, trainingbased approaches involve transforming training data into more detailed steps (Jin et al., 2024;Ying et al., 2024) or incorporating behaviors like planning (Wang et al., 2023a) and self-correction (Yan et al., 2024), which can increase the model's computation during test-time, thus improving reasoning performance.",
            "score": 0.44205053103697023,
            "section_title": "Expansion of Reasoning Steps",
            "char_start_offset": 7184,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 104
                },
                {
                    "start": 105,
                    "end": 390
                },
                {
                    "start": 393,
                    "end": 475
                },
                {
                    "start": 476,
                    "end": 588
                },
                {
                    "start": 591,
                    "end": 640
                },
                {
                    "start": 643,
                    "end": 789
                },
                {
                    "start": 790,
                    "end": 928
                },
                {
                    "start": 929,
                    "end": 1098
                },
                {
                    "start": 1099,
                    "end": 1440
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0457763671875
        },
        {
            "corpus_id": "270226169",
            "title": "Zyda: A 1.3T Dataset for Open Language Modeling",
            "text": "Over the last five years, large language models (LLMs) have been undergoing an extremely rapid growth in scale, cost, and capabilities (Vaswani et al., 2017;Radford et al., 2019;Brown et al., 2020;Team et al., 2023;Achiam et al., 2023;Sevilla et al., 2022). This development was fueled by the LLM scaling laws (Hestness et al., 2017;Kaplan et al., 2020;Hoffmann et al., 2022) that established a relationship between the attainable loss and model size, dataset size and compute budget based on systematic experiments. Highlighting how performance increases with model size, these scaling laws provide guidance for how to optimally allocate resources for model size and dataset size given a fixed compute budget and provide concrete and fairly accurate predictions about the final loss and downstream capabilities of these models. Specifically, the \"Chinchilla scaling laws\" (Hoffmann et al., 2022) suggest that equal scaling of parameters and data is required to train a \"compute-optimal\" model. However, as models become increasingly widely deployed, the majority of the total FLOPs are spent in inference, and not in pretraining. Therefore, the focus has been shifting towards \"inference-optimal\" models, which are much smaller and are trained on significantly more tokens than the Chinchilla scaling laws would recommend (Touvron et al., 2023;Jiang et al., 2023). These smaller models require significantly fewer forward-pass FLOPs and significantly less GPU VRAM for inference, which has caused them to become extremely important in the open-source LLM community, where the ability of a model to fit inside the VRAM of consumer GPUs is extremely important. \n\nThese trends have resulted in a significant increase in the ratio of total number of training tokens to model parameter count. State-of-the-art LLMs went from a 300B:175B ratio with GPT3 and a 540B:760B ratio with PaLM to a 12T:132B ratio with DBRX (MosaicML, 2024) and a 15T:8B ratio with Llama3.",
            "score": 0.44184693095055017,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 257
                },
                {
                    "start": 258,
                    "end": 516
                },
                {
                    "start": 517,
                    "end": 828
                },
                {
                    "start": 829,
                    "end": 994
                },
                {
                    "start": 995,
                    "end": 1130
                },
                {
                    "start": 1131,
                    "end": 1365
                },
                {
                    "start": 1366,
                    "end": 1659
                },
                {
                    "start": 1662,
                    "end": 1788
                },
                {
                    "start": 1789,
                    "end": 1959
                }
            ],
            "ref_mentions": [
                {
                    "start": 135,
                    "end": 157,
                    "matchedPaperCorpusId": "13756489"
                },
                {
                    "start": 157,
                    "end": 178,
                    "matchedPaperCorpusId": "160025533"
                },
                {
                    "start": 178,
                    "end": 197,
                    "matchedPaperCorpusId": "218971783"
                },
                {
                    "start": 235,
                    "end": 256,
                    "matchedPaperCorpusId": "246822642"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.089111328125
        },
        {
            "corpus_id": "252683098",
            "title": "Scaling Laws for a Multi-Agent Reinforcement Learning Model",
            "text": "Our study constitutes, to our knowledge, the first investigation of power-law scaling phenomena for a MARL algorithm. Measuring the performance of the AlphaZero algorithm using Elo rating, we follow a similar path as Kaplan et al. (2020) by providing evidence of power-law Figure 1: Left: Optimal number of neural network parameters for different amounts of available compute. The optimal agent size scales for both Connect Four and Pentago as a single power law with available compute. The predicted slope \u03b1 opt C = \u03b1 C /\u03b1 N of Eq. ( 7) matches the observed data, where \u03b1 C and \u03b1 N are the compute and model size scaling exponents, respectively. See Table 3 for the numerical values. Right: The same graph zoomed out to include the resources used to create AlphaZero (Silver et al., 2017a) and AlphaGoZero (Silver et al., 2017b). These models stand well bellow the optimal trend for Connect Four and Pentago. \n\nscaling of playing strength with model size and compute, as well as a power law of optimal model size with respect to available compute. Focusing on AlphaZero-agents that are guided by neural nets with fully connected layers, we test our hypothesis on two popular board games: Connect Four and Pentago. These games are selected for being different from each other with respect to branching factors and game lengths. \n\nUsing the Bradley-Terry model definition of playing strength (Bradley & Terry, 1952), we start by showing that playing strength scales as a power law with neural network size when models are trained until convergence in the limit of abundant compute. We find that agents trained on Connect Four and Pentago scale with similar exponents. \n\nIn a second step we investigate the trade-off between model size and compute. Similar to scaling observed in the game Hex (Jones, 2021), we observe power-law scaling when compute is limited, again with similar exponents for Connect Four and Pentago. Finally we utilize these two scaling laws to find a scaling law for the optimal model size given the amount of compute available, as shown in Fig. 1. We find that the optimal neural network size scales as a power law with compute, with an exponent that can be derived from the individual size-scaling and compute-scaling exponents.",
            "score": 0.4415990251755864,
            "section_title": "INTRODUCTION",
            "char_start_offset": 1868,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 117
                },
                {
                    "start": 118,
                    "end": 376
                },
                {
                    "start": 377,
                    "end": 486
                },
                {
                    "start": 487,
                    "end": 646
                },
                {
                    "start": 647,
                    "end": 684
                },
                {
                    "start": 685,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 909
                },
                {
                    "start": 912,
                    "end": 1048
                },
                {
                    "start": 1049,
                    "end": 1214
                },
                {
                    "start": 1215,
                    "end": 1327
                },
                {
                    "start": 1330,
                    "end": 1580
                },
                {
                    "start": 1581,
                    "end": 1666
                },
                {
                    "start": 1669,
                    "end": 1746
                },
                {
                    "start": 1747,
                    "end": 1918
                },
                {
                    "start": 1919,
                    "end": 2068
                },
                {
                    "start": 2069,
                    "end": 2250
                }
            ],
            "ref_mentions": [
                {
                    "start": 1391,
                    "end": 1414,
                    "matchedPaperCorpusId": "121987403"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0885009765625
        },
        {
            "corpus_id": "267782738",
            "title": "Navigating Scaling Laws: Compute Optimality in Adaptive Model Training",
            "text": "We fit functions of the form \n\nSimilar to previous work, we resample points to be almost equidistant in the log-domain, in terms of FLOPs. We minimize different initializations using the minimize function in scipy (Virtanen et al., 2020), and choose the one that leads to the smallest error. The function to minimize is based on the Huber loss with \u03b4 = 1e \u22123 . \n\nB.5 SEC. 5 DETAILS In Sec. 5, we train Llama (Touvron et al., 2023a;b)   Extrapolating to longer contexts is a very active area of research with exciting work published recently (Qin et al., 2023;Ruoss et al., 2023;Press et al., 2021;Peng et al., 2023). In our case, we are training using RoPE positional encodings (Su et al., 2024), which are known to extrapolate easier compared to other ones, such as absolute positional encodings. \n\nThe exact number for the most compute-intensive point in Fig. 9 is 17825792000 tokens. The exact model size including the embedding parameters is 137841408 parameters. That leads to an approximate token per parameter value of 129.3, past the optimal \"Chinchilla\" point, leading thus to model that are more inference efficient. \n\nB.6 SEC. 6.2 DETAILS In Sec. 6.2, we train ViT models using the same hyperparameters found as described above. For the batch-size experiments, we train V384-20/12 models with batch size in {256, 2048} and navigate across the scaling laws corresponding to the same values. For the different training objective experiments, we train a V384-20/12 model using either supervised training or distillation from a powerful teacher. We use as a teacher a pre-trained V640-10/12 model and train using only distillation loss as in Hinton et al. (2015), with a temperature of T = 2. When calculating the FLOPs of the single step, we include the FLOPs of the teacher only for the forward pass.",
            "score": 0.44158573638185245,
            "section_title": "B.4 SCALING LAWS",
            "char_start_offset": 30063,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 28
                },
                {
                    "start": 31,
                    "end": 138
                },
                {
                    "start": 139,
                    "end": 291
                },
                {
                    "start": 292,
                    "end": 360
                },
                {
                    "start": 363,
                    "end": 616
                },
                {
                    "start": 617,
                    "end": 797
                },
                {
                    "start": 800,
                    "end": 886
                },
                {
                    "start": 887,
                    "end": 967
                },
                {
                    "start": 968,
                    "end": 1126
                },
                {
                    "start": 1129,
                    "end": 1137
                },
                {
                    "start": 1138,
                    "end": 1157
                },
                {
                    "start": 1158,
                    "end": 1239
                },
                {
                    "start": 1240,
                    "end": 1400
                },
                {
                    "start": 1401,
                    "end": 1552
                },
                {
                    "start": 1553,
                    "end": 1699
                },
                {
                    "start": 1700,
                    "end": 1809
                }
            ],
            "ref_mentions": [
                {
                    "start": 214,
                    "end": 237,
                    "matchedPaperCorpusId": "198229805"
                },
                {
                    "start": 678,
                    "end": 695,
                    "matchedPaperCorpusId": "233307138"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.06646728515625
        },
        {
            "corpus_id": "276259426",
            "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
            "text": "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models. \n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law f r (x) = \u03b2 r x \u2212cr + \u03b5 r , then c r increases with r while \u03b5 r decreases. Furthermore, the coefficient \u03b2 r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2 r , its faster convergence (c r ) and lower asymptotic limit (\u03b5 r ) will eventually lead to superior performance. In other words, using the higher recursion level is advantageous eventually, which is consistent with the experimental results. To quantify this more explicitly, we train language models with four signatures A r B: r \u2208 {1, 2, 3, 4}. Then, we plot the optimal value of r against training compute. As shown in Figure 5b, the optimal value of r monotonically increases with training compute, in agreement with earlier results. Also, smaller models benefit more from RINS. \n\nAdding Linear Adapters. Earlier in Section 4, we showed that enabling stochastic RINS during training exhibits a tradeoff between worst-case and best-case performance, depending on whether or not RINS is applied at inference time. Next, we introduce a additional improvement: when a maximum of r recursion rounds are used in stochastic RINS, we add r lightweight, linear adapters (i.e. linear projection layers) to the output before the projection head. The choice of which adapter to apply is a function of how many recursion rounds are used. Specifically, if signature A r B is used, the r-th adapter is applied. Empirically, this introduces < 1% more parameters and has a negligible Figure 6: LEFT 2 PLOTS: y-axis corresponds to performance when RINS is enabled during training but disabled at inference time in 600M-parameter LMs.",
            "score": 0.4413322177305046,
            "section_title": "Multimodal Systems",
            "char_start_offset": 22964,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 88
                },
                {
                    "start": 89,
                    "end": 248
                },
                {
                    "start": 251,
                    "end": 286
                },
                {
                    "start": 287,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 538
                },
                {
                    "start": 539,
                    "end": 768
                },
                {
                    "start": 769,
                    "end": 896
                },
                {
                    "start": 897,
                    "end": 1001
                },
                {
                    "start": 1002,
                    "end": 1064
                },
                {
                    "start": 1065,
                    "end": 1192
                },
                {
                    "start": 1193,
                    "end": 1237
                },
                {
                    "start": 1240,
                    "end": 1263
                },
                {
                    "start": 1264,
                    "end": 1470
                },
                {
                    "start": 1471,
                    "end": 1625
                },
                {
                    "start": 1626,
                    "end": 1693
                },
                {
                    "start": 1694,
                    "end": 1783
                },
                {
                    "start": 1784,
                    "end": 1854
                },
                {
                    "start": 1855,
                    "end": 2074
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.04345703125
        },
        {
            "corpus_id": "273549959",
            "title": "Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models",
            "text": "Generation and obtaining reward signal can be fundamentally slower than inference. In the classic RLHF setup, generation is autoregressive and scales linearly with the length of the response to generate, whereas reward model inference can be constant. Recent work shows that reward may require human labelling (Llama Team, 2024), output chain-of-thought reasoning (Zhang et al., 2024;Ankner et al., 2024), or executing external tools such as Learn verifiers (Google Deepmind, 2024). In this scenario, we have extra training compute cycles and ask the question, \"is it useful to train more on existing data?\". Following previous work with PPO (Ouyang et al., 2022), we experiment with taking multiple updates on the same batch of generated data i.e. \"ppo epochs\" (Schulman et al., 2015). In our asynchronous TLDR setup, we generate N = 1 mini-batches and perform T = 1, 2, 3 updates per mini-batch. \n\nWe plot results across different scales in Figure 7 (left). At 410m and 1B scales, models achieve a higher win-rate for the same number of generated samples, showing that multiple updates make training more sample efficient. This means that extra training time can be used to increase winrate. But measuring the final points on the pareto frontier in Figure 7 (right), we find that increasing updates per mini-batch also increases drift in terms of KL. Therefore, in generation-bound scenarios, multiple updates may increase the win-rate with the same compute-time but incurs higher KL.",
            "score": 0.44122769406402285,
            "section_title": "GENERATION-BOUND RLHF",
            "char_start_offset": 15711,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 82
                },
                {
                    "start": 83,
                    "end": 251
                },
                {
                    "start": 252,
                    "end": 482
                },
                {
                    "start": 483,
                    "end": 608
                },
                {
                    "start": 609,
                    "end": 786
                },
                {
                    "start": 787,
                    "end": 897
                },
                {
                    "start": 900,
                    "end": 959
                },
                {
                    "start": 960,
                    "end": 1124
                },
                {
                    "start": 1125,
                    "end": 1193
                },
                {
                    "start": 1194,
                    "end": 1352
                },
                {
                    "start": 1353,
                    "end": 1486
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.00760650634765625
        },
        {
            "corpus_id": "276107826",
            "title": "Sample, Scrutinize and Scale: Effective Inference-Time Search by Scaling Verification",
            "text": "Specifically, we consider the case where models must self-verify their responses to select the best answer, and do not make the strong assumption that one can access groundtruth answers or symbolic systems that exactly verify correctness. In this setup, we address the question: what test-time scaling trends emerge as we scale both the number of sampled responses and verification capabilities? In particular, what are the limits of scaling this simple sampling-based search paradigm and how much does one need to continuously scale verification capability as one scales up search? \n\nOur findings. We first identify scaling trends demonstrating that reasoning performance continues to improve with sampling-based search even as test-time compute is scaled well beyond the point where the performance of self-consistency [Wang et al., 2023] saturates. At sufficient scale, even our minimalist implementation provides a significant leap in reasoning accuracy, lifting Gemini v1.5 Pro performance beyond o1-Preview, and Gemini v1.5 Flash beyond Gemini v1.5 Pro, on reasoning benchmarks such as LiveBench [White et al., 2024] and the AIME [MAA, 2024], exhibiting sustained power-law scaling on the latter. This not only highlights the importance of sampling-based search for scaling capability, but also suggests the utility of sampling-based search as a simple baseline on which to compare other test-time compute scaling strategies and measure genuine improvements in models' search capabilities. \n\nWe then attribute much of the strong scaling trends of sampling-based search to an implicit scaling phenomenon. Contrary to the intuition that sampling more responses should impose a greater burden on the verifier and reduce verification accuracy, we observe that scaling sampling indirectly enhances verification accuracy. At a high-level, this is because well-written responses are easier to verify than poorly written responses, and scaling sampling widens the pool of well-written candidates. \n\nWe further identify two effective strategies for scaling verification capabilities using test-time compute: (1) directly comparing candidate responses and (2) task-specific rewriting of candidate responses. The former mitigates a core weakness of language models, which struggle to identify mistakes and hallucinations unless given their locations [Tyen et al., 2024], by leveraging the fact that differences between candidate responses provide a strong signal for where errors might be located.",
            "score": 0.44095276103057013,
            "section_title": "Introduction",
            "char_start_offset": 1893,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 238
                },
                {
                    "start": 239,
                    "end": 395
                },
                {
                    "start": 396,
                    "end": 582
                },
                {
                    "start": 585,
                    "end": 598
                },
                {
                    "start": 599,
                    "end": 851
                },
                {
                    "start": 852,
                    "end": 1202
                },
                {
                    "start": 1203,
                    "end": 1495
                },
                {
                    "start": 1498,
                    "end": 1609
                },
                {
                    "start": 1610,
                    "end": 1821
                },
                {
                    "start": 1822,
                    "end": 1994
                },
                {
                    "start": 1997,
                    "end": 2203
                },
                {
                    "start": 2204,
                    "end": 2492
                }
            ],
            "ref_mentions": [
                {
                    "start": 821,
                    "end": 840,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.039276123046875
        },
        {
            "corpus_id": "278714770",
            "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning",
            "text": "Test-time scaling (TTS) has emerged as a pivotal strategy in enhancing the performance of LLMs by allocating additional computational resources during inference. This approach shifts the traditional emphasis from extensive pretraining to optimizing inference-time computation, enabling models to tackle complex tasks more effectively. Following Muennighoff et al. [2025] and Zhang et al. [2025], we classify test-time scaling methods into: (1) Parallel Scaling [Wang et al., 2023, Brown et al., 2024, Snell et al., 2024, Liu et al., 2025], where parallel computes multiple reasoning chains independently, (2) Sequential Scaling [Madaan et al., 2023, Chen et al., 2024, Muennighoff et al., 2025], where computes a longer reasoning chain and generates the chain sequentially, and (3) Hybrid Scaling [Yao et al., 2023, Gandhi et al., 2024, Wang et al., 2025], where combines the parallel scaling and sequential scaling methods. In this paper, we mainly focus on parallel test-time scaling, which can be adopt on large-scale LLMs efficiently. \n\nAs conclued by Zhang et al. [2025], parallel scaling improves test-time performance by generating multiple reasoning chains in parallel, and then aggregating them together to the final answer. Early evidence that sampling multiple reasoning chains and voting improves robustness came from Self-Consistency (SC) [Wang et al., 2023], inspiring subsequent studies on how many chains to sample for a fixed compute envelope [Snell et al., 2024]. Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]. Despite the effectiveness of these approaches, the majority of existing parallel test-time scaling methods rely on discrete token-by-token generation, which imposes inherent constraints and limits their expressiveness.",
            "score": 0.44042419639495944,
            "section_title": "Test-Time Scaling",
            "char_start_offset": 5749,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 161
                },
                {
                    "start": 162,
                    "end": 334
                },
                {
                    "start": 335,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1038
                },
                {
                    "start": 1041,
                    "end": 1233
                },
                {
                    "start": 1234,
                    "end": 1481
                },
                {
                    "start": 1482,
                    "end": 1695
                },
                {
                    "start": 1696,
                    "end": 1914
                }
            ],
            "ref_mentions": [
                {
                    "start": 461,
                    "end": 479,
                    "matchedPaperCorpusId": "247595263"
                },
                {
                    "start": 628,
                    "end": 648,
                    "matchedPaperCorpusId": "257900871"
                },
                {
                    "start": 648,
                    "end": 667,
                    "matchedPaperCorpusId": "258059885"
                },
                {
                    "start": 797,
                    "end": 814,
                    "matchedPaperCorpusId": "258762525"
                },
                {
                    "start": 1352,
                    "end": 1371,
                    "matchedPaperCorpusId": "247595263"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.0633544921875
        },
        {
            "corpus_id": "276558383",
            "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale",
            "text": "Large Language Models (LLM) are becoming pervasive in many aspects of life. The performance of these models are dictated by several factors including the model architecture, model size, training data size as well as training data quality. \n\nHow much data should one use to train an LLM of certain size? The answer is typically governed by scaling laws -empirical formulas that estimate optimal models sizes and data sizes for a given compute budget. For instance, the widely adopted Chinchilla law [1] suggested a compute optimal token-to-parameter-ratio of roughly 20. However, recent state-of-the-art LLMs have been trained on far more data than what the scaling laws would deem as optimal. For instance, Llama3 family of models are trained on 15 trillion (15T) tokens (compared to 1.8T tokens for Llama2) [2,3], Gemma2 family of models are trained on 13T tokens [4], and Granite-3.0 family of models are trained on 12T tokens [5]. At the time of writing of this paper, the pre-training datasets for leading LLMs, such as Llama3 [2] and Mixtral [6], remain inaccessible to the public, with limited information available on their creation process. \n\nFigure 1: GneissWeb (\u223c10T tokens) outperforms state-of-the-art open-source datasets with 5T+ tokens. Specifically, we compare average scores on a set of 11 tasks with 18 variants (zero-shot and few-shot) for 1.4B parameter models (left) and 7B parameter models (right), trained on 350B tokens. We also compare with state-of-the-art existing models of roughly 1B parameter size. Models trained on GneissWeb (green) achieve higher performance than the models trained on other datasets (circles) and existing models (crosses). \n\nOpacity of the datasets used to train leading LLMs, has motivated the development of several open-source datasets [7,8,9,10,11].",
            "score": 0.440285443847689,
            "section_title": "Introduction",
            "char_start_offset": 15,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 75
                },
                {
                    "start": 76,
                    "end": 238
                },
                {
                    "start": 241,
                    "end": 302
                },
                {
                    "start": 303,
                    "end": 449
                },
                {
                    "start": 450,
                    "end": 569
                },
                {
                    "start": 570,
                    "end": 692
                },
                {
                    "start": 693,
                    "end": 885
                },
                {
                    "start": 886,
                    "end": 933
                },
                {
                    "start": 934,
                    "end": 1148
                },
                {
                    "start": 1151,
                    "end": 1251
                },
                {
                    "start": 1252,
                    "end": 1444
                },
                {
                    "start": 1445,
                    "end": 1528
                },
                {
                    "start": 1529,
                    "end": 1674
                },
                {
                    "start": 1677,
                    "end": 1805
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.017303466796875
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "To understand how to compute-optimally scale search methods, we now conduct a difficulty bin analysis. Specifically, we compare beam-search ( = 4) against best-of-N. \n\nIn Figure 3 (right) we see that while in aggregate, beam search and best-of-N perform similarly with a high generation budget, evaluating their efficacy over difficulty bins reveals very different trends. On the easy questions (levels 1 and 2), the stronger optimizer of the two approaches, beam search, degrades performance as the generation budget increases, suggesting signs of exploitation of the PRM signal. In contrast, on the harder questions (levels 3 and 4), beam search consistently outperforms best-of-N. On the most difficult questions (level 5), no method makes much meaningful progress. \n\nThese findings match intuition: we might expect that on the easy questions, the verifier will make mostly correct assessments of correctness. Therefore, by applying further optimization via beam search, we only further amplify any spurious features learned by the verifier, causing performance degredation. On the more difficult questions, the base model is much less likely to sample the correct answer in the first place, so search can serve to help guide the model towards producing the correct answer more often. . \"Computeoptimal oracle\" refers to using oracle difficulty bins derived from the groundtruth correctness information, and \"computeoptimal predicted\" refers to using the PRM's predictions to generate difficulty bins. Observe that the curves with either type of difficulty bins largely overlap with each other. \n\nCompute-optimal search. Given the above results, it is clear that question difficulty can be a useful statistic to predict the optimal search strategy to use at a given compute budget. Additionally, the best choice of search strategy can vary drastically as a function of this difficulty statistic. We therefore visualize the \"compute-optimal\" scaling trend, as represented by the best performing search strategy at each difficulty level in Figure 4. We see that in the low generation budget regime, using both the oracle and predicted difficulty, compute-optimal scaling can nearly outperform best-of-N using up to 4x less test-time compute (e.g. 16 verses 64 generations).",
            "score": 0.44023533538733584,
            "section_title": "Which problems does search improve?",
            "char_start_offset": 28274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 102
                },
                {
                    "start": 103,
                    "end": 165
                },
                {
                    "start": 168,
                    "end": 372
                },
                {
                    "start": 373,
                    "end": 580
                },
                {
                    "start": 581,
                    "end": 683
                },
                {
                    "start": 684,
                    "end": 768
                },
                {
                    "start": 771,
                    "end": 912
                },
                {
                    "start": 913,
                    "end": 1077
                },
                {
                    "start": 1078,
                    "end": 1289
                },
                {
                    "start": 1290,
                    "end": 1504
                },
                {
                    "start": 1505,
                    "end": 1597
                },
                {
                    "start": 1600,
                    "end": 1623
                },
                {
                    "start": 1624,
                    "end": 1784
                },
                {
                    "start": 1785,
                    "end": 1898
                },
                {
                    "start": 1899,
                    "end": 2247
                },
                {
                    "start": 2248,
                    "end": 2274
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.049774169921875
        },
        {
            "corpus_id": "277104276",
            "title": "MetaScale: Test-Time Scaling with Evolving Meta-Thoughts",
            "text": "Baseline Methods For each task, we compare METASCALE to four baseline inference methods with the same reward model and scaling budget if applicable. \n\n1-Pass, where the target LLM performs a single forward pass with greedy decoding where the temperature is fixed to 0. Chain-of-Thought (CoT, Wei et al. 2022), where the target LLM is prompted to generate a reasoning process towards the final solution for the given task. This is also performed in a greedy decoding manner with the temperature set as 0. \n\nBest-of-N (Brown et al., 2024) for each sampling. Temperature of 0.6 is applied. \n\nModels We use one state-of-the-art LLM, GPT-4o (gpt-4o-0806) (Achiam et al., 2023), and one open-source LLM Llama-3.1-8B-Instruct (Dubey et al., 2024) for evaluation. Skywork-Reward-Llama-3.1-8B-v0.2 (Liu et al., 2024) is directly adopted as the reward model for assigning reward scores to candidate responses, based on which the optimal response is chosen as the final answer. To ensure fair comparisons, we use consistent inference settings across models and maintain the same sampling budgets. \n\nImplementation Details Due to limited computational resources and high API costs, we randomly sample 64 testing instances from each benchmark to create new subsampled evaluation sets, which are used consistently across all methods. To precisely evaluate the effectiveness of different testtime compute scaling strategies without interference from demonstrations, we prompt LLMs in the zero-shot setting on all studied tasks. and GSM8K, respectively. It also surpasses the standard Best-of-N method by 8.97%, 6.24%, and 3.12%, respectively. Further, GPT-4o outperforms o1-mini (Jaech et al., 2024) with METASCALE under style control (Tab. 2). Similar improvements are observed when using Llama-3.1-8B-Instrct as the base model.",
            "score": 0.4400312907737962,
            "section_title": "Experimental Setup",
            "char_start_offset": 13976,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 148
                },
                {
                    "start": 151,
                    "end": 421
                },
                {
                    "start": 422,
                    "end": 503
                },
                {
                    "start": 506,
                    "end": 555
                },
                {
                    "start": 556,
                    "end": 586
                },
                {
                    "start": 589,
                    "end": 718
                },
                {
                    "start": 719,
                    "end": 755
                },
                {
                    "start": 756,
                    "end": 788
                },
                {
                    "start": 789,
                    "end": 966
                },
                {
                    "start": 967,
                    "end": 1085
                },
                {
                    "start": 1088,
                    "end": 1319
                },
                {
                    "start": 1320,
                    "end": 1512
                },
                {
                    "start": 1513,
                    "end": 1537
                },
                {
                    "start": 1538,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1725
                },
                {
                    "start": 1726,
                    "end": 1729
                },
                {
                    "start": 1730,
                    "end": 1814
                }
            ],
            "ref_mentions": [
                {
                    "start": 292,
                    "end": 308,
                    "matchedPaperCorpusId": "246411621"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.01094818115234375
        },
        {
            "corpus_id": "255185900",
            "title": "Cramming: Training a Language Model on a Single GPU in One Day",
            "text": "The most obvious way to efficiently scale down training is by modifying the model architecture; intuitively, it seems likely that smaller/lower capacity models will be optimal in the cramming regime. \n\nIn this section, we study the relationship between model type and training efficiency. We see that scaling laws create a strong barrier to scaling down. Per-token efficiency of training depends strongly on model size, but not transformer type. Furthermore, smaller models learn less efficiently, and this largely mitigates any throughput gains. Fortunately, the fact that training efficiency is nearly constant across models of the same size means that we can boost performance by finding architecture modifications that speed up gradient computation while keeping the parameter count nearly constant. This makes architecture selection fairly straightforward as we can make design choices based primarily on how they affect computation time for a single gradient step. \n\nScaling laws hold in the low-resource regime A large corpus of research in recent years has developed architectural improvements to speed up the original transformer. Many of these methods have not been found to improve training for the large-scale T5 architecture Narang et al. (2021); Tay et al. (2022a). But, in the low compute setting where data throughput is of utmost importance, maybe this is the way forward? Scaling laws have been observed by Kaplan et al. (2020) in the highresource regime, and seem to hold strongly in the limit as resources grow. Surprisingly, these laws also hold in the limit of extreme compute down-scaling, and they create a barrier to low-cost training. \n\nWe exemplify the effect of scaling laws for many transformer variants from the literature in Figure 1, where we train each architecture variant with optimized training hyperparameters as described below in Section 4.3. We apply these architecture variants to a shared baseline model that incorporates",
            "score": 0.4399504291893912,
            "section_title": "MODIFYING THE ARCHITECTURE",
            "char_start_offset": 15180,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 199
                },
                {
                    "start": 202,
                    "end": 288
                },
                {
                    "start": 289,
                    "end": 354
                },
                {
                    "start": 355,
                    "end": 445
                },
                {
                    "start": 446,
                    "end": 546
                },
                {
                    "start": 547,
                    "end": 803
                },
                {
                    "start": 804,
                    "end": 970
                },
                {
                    "start": 973,
                    "end": 1139
                },
                {
                    "start": 1140,
                    "end": 1279
                },
                {
                    "start": 1280,
                    "end": 1389
                },
                {
                    "start": 1390,
                    "end": 1531
                },
                {
                    "start": 1532,
                    "end": 1660
                },
                {
                    "start": 1663,
                    "end": 1881
                },
                {
                    "start": 1882,
                    "end": 1963
                }
            ],
            "ref_mentions": [
                {
                    "start": 1238,
                    "end": 1258,
                    "matchedPaperCorpusId": "232035936"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.190673828125
        },
        {
            "corpus_id": "250607724",
            "title": "ScaleNet: Searching for the Model to Scale",
            "text": "Inspired by ancestral sampling [2], we propose a hierarchical sampling strategy (HSS) that splits the search space into partitions and the sampling is implemented respectively. The search space and the sampling distribution of the super-supernet for FLOPs are carefully designed according to the budgets of various scaling stages and undertake a multi-modal form distribution. \n\nSecondly, considering that our goal is to find a base model architecture with the strongest scaling capability (instead of the best performance) and its corresponding optimal scaling strategies, we propose a joint search for them, dubbed Markov chain-based evolution algorithm (MCEA), by iteratively and interactively optimizing both of them. After obtaining the searched scaling strategies, we model the trends of depth, width, and resolution, respectively, and generalize them to develop even larger models. We theoretically derive a group of generalization functions in the three dimensions for larger-scale architectures, with which moderate performance can be actually achieved. \n\nThe contributions of this paper are four-fold: \n\n-We propose ScaleNet to jointly search the base model and a group of the scaling strategies based on one-shot NAS framework. The scaling strategies of larger scales are generalized by the searched ones with our theoretically derived generalization functions. -We carefully design the search space and a multi-modal distribution for FLOPs budgets for hierarchical sampling strategy (HSS) in the one-shot NAS-based scaling search algorithm to enhance the training sufficiency of paths in super-supernet. In each iteration, an evolution procedure with crossover-mutation and evaluation is undertaken for searching the optimal base model or scaling strategies based on the search space. Finally, after obtaining the optimal ones, we generalize them to larger-scale architectures by the estimations of the trends of depth, width, and resolution, respectively (the right box). All the obtained architectures will be applied for retraining and inference. \n\npost-processing. However, the main drawback of these methods is that they only searched for a model family by training a joint or even a group of independent supernet(s), but did not analyze the structural relationship and explicit scaling strategies between the architectures with different budgets in the model family. \n\nIt is difficult and even infeasible to extend the scaling strategies to larger scales.",
            "score": 0.43981845632851413,
            "section_title": "Introduction",
            "char_start_offset": 3597,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 176
                },
                {
                    "start": 177,
                    "end": 376
                },
                {
                    "start": 379,
                    "end": 721
                },
                {
                    "start": 722,
                    "end": 888
                },
                {
                    "start": 889,
                    "end": 1062
                },
                {
                    "start": 1065,
                    "end": 1111
                },
                {
                    "start": 1114,
                    "end": 1238
                },
                {
                    "start": 1239,
                    "end": 1372
                },
                {
                    "start": 1373,
                    "end": 1615
                },
                {
                    "start": 1616,
                    "end": 1796
                },
                {
                    "start": 1797,
                    "end": 1984
                },
                {
                    "start": 1985,
                    "end": 2061
                },
                {
                    "start": 2064,
                    "end": 2080
                },
                {
                    "start": 2081,
                    "end": 2384
                },
                {
                    "start": 2387,
                    "end": 2473
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.029815673828125
        },
        {
            "corpus_id": "271719990",
            "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
            "text": "On the other hand, with more difficult problems that may require searching over many different high-level approaches to solving the problem, re-sampling new responses independently in parallel or deploying tree-search against a process-based reward model is likely a more effective way to use test-time computation. This finding illustrates the need to deploy an adaptive \"compute-optimal\" strategy for scaling test-time compute, wherein the specific approach for utilizing test-time compute is selected depending on the prompt, so as to make the best use of additional computation. We also show that a notion of question difficulty (Section 4) from the perspective of the base LLM can be used to predict the efficacy of test-time computation, enabling us to practically instantiate this 'compute-optimal' strategy given a prompt. By appropriately allocating test-time compute in this way, we are able to greatly improve test-time compute scaling, surpassing the performance of a best-of-N baseline while only using about 4x less computation with both revisions and search (Sections 5 and 6). \n\nUsing our improved test-time compute scaling strategy, we then aim to understand to what extent test-time computation can effectively substitute for additional pretraining. We conduct a FLOPs-matched comparison between a smaller model with additional test-time compute and pretraining a 14x larger model. We find that on easy and intermediate questions, and even hard questions (depending on the specific conditions on the pretraining and inference workload), additional test-time compute is often preferable to scaling pretraining. This finding suggests that rather than focusing purely on scaling pretraining, in some settings it is be more effective to pretrain smaller models with less compute, and then apply test-time compute to improve model outputs. That said, with the most challenging questions, we observe very little benefits from scaling up test-time compute. Instead, we find that on these questions, it is more effective to make progress by applying additional pretraining compute, demonstrating that current approaches to scaling test-time compute may not be 1-to-1 exchangeable with scaling pretraining.",
            "score": 0.4397508844210022,
            "section_title": "Test-time Search Against a PRM Verifier",
            "char_start_offset": 5274,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 315
                },
                {
                    "start": 316,
                    "end": 582
                },
                {
                    "start": 583,
                    "end": 830
                },
                {
                    "start": 831,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1267
                },
                {
                    "start": 1268,
                    "end": 1399
                },
                {
                    "start": 1400,
                    "end": 1627
                },
                {
                    "start": 1628,
                    "end": 1852
                },
                {
                    "start": 1853,
                    "end": 1967
                },
                {
                    "start": 1968,
                    "end": 2215
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.10595703125
        },
        {
            "corpus_id": "256416224",
            "title": "Scaling laws for single-agent reinforcement learning",
            "text": "Recent work has shown that, in generative modeling, cross-entropy loss improves smoothly with model size and training compute, following a power law plus constant scaling law. One challenge in extending these results to reinforcement learning is that the main performance objective of interest, mean episode return, need not vary smoothly. To overcome this, we introduce *intrinsic performance*, a monotonic function of the return defined as the minimum compute required to achieve the given return across a family of models of different sizes. We find that, across a range of environments, intrinsic performance scales as a power law in model size and environment interactions. Consequently, as in generative modeling, the optimal model size scales as a power law in the training compute budget. Furthermore, we study how this relationship varies with the environment and with other properties of the training setup. In particular, using a toy MNIST-based environment, we show that varying the\"horizon length\"of the task mostly changes the coefficient but not the exponent of this relationship.",
            "score": 0.4391762891473478,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.227783203125
        },
        {
            "corpus_id": "270045432",
            "title": "Scaling Laws for Discriminative Classification in Large Language Models",
            "text": "Given our two-stage training pipeline, our goal in studying scaling laws is to predict how well a model adapted to a domain using a language modeling objective (stage 1) will perform when discriminatively fine-tuned as a classifier (stage 2).We evaluate this scaling behavior using various metrics that offer insights into performance, efficiency, and behavior across different scales, including FLOPs (Floating Point Operations): The number of floating point operations executed during the language modeling stage.It provides insights into the computational complexity of the LLM and how it scales with model and dataset size.\n\nLanguage Modeling Loss: The cross entropy between the model's predicted next token and the actual next token in training data, directly optimized by the language modeling objective.Monitoring the loss ensures that the model is converging toward an optimal language model of the data.Figure 3 depicts properties of the scaling laws observed in our experiments.In Figure 3 (a) we plot classification loss as a function of language modeling FLOPs for each of the model sizes we adapted.We observe overlap across the three model sizes tested such that for a given compute budget such as 10 18 FLOPs, larger models exhibit lower training loss, suggesting their ability to learn complex patterns and structure within the training data.While we do not fit a formal scaling law here, the observation is consistent with compute optimal language modeling [16,18] applied to discriminative finetuning.\n\nIn Figure 3 (b), we plot the classification loss as a function of number of tokens seen during our domain adaptation stage.We observe scaling as a function of number of training tokens with larger models exhibiting lower classification loss for the same number of training tokens.We also observe a linear relationship between supervised classification loss and the number of training tokens across model sizes.These results suggest that even the largest model we trained will still benefit from more data.We find this encouraging because it means we are able to produce better classifiers when discriminatively fine-tuned, despite our practical parameterlimitations [15,18].\n\nWe relate language modeling loss to classification loss in Figure 3  (c).We observe a linear relationship between language modeling loss during domain adaptation and classification loss during discriminative fine-tuning across model sizes.",
            "score": 0.4391218328992924,
            "section_title": "EXPERIMENTS 5.1 Offline Training and Evaluation",
            "char_start_offset": 19942,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 242
                },
                {
                    "start": 242,
                    "end": 515
                },
                {
                    "start": 515,
                    "end": 627
                },
                {
                    "start": 629,
                    "end": 810
                },
                {
                    "start": 810,
                    "end": 912
                },
                {
                    "start": 912,
                    "end": 988
                },
                {
                    "start": 988,
                    "end": 1112
                },
                {
                    "start": 1112,
                    "end": 1358
                },
                {
                    "start": 1358,
                    "end": 1519
                },
                {
                    "start": 1521,
                    "end": 1644
                },
                {
                    "start": 1644,
                    "end": 1801
                },
                {
                    "start": 1801,
                    "end": 1931
                },
                {
                    "start": 1931,
                    "end": 2026
                },
                {
                    "start": 2026,
                    "end": 2195
                },
                {
                    "start": 2197,
                    "end": 2270
                },
                {
                    "start": 2270,
                    "end": 2436
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.033782958984375
        },
        {
            "corpus_id": "277595998",
            "title": "Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling Law for Reasoning",
            "text": "We empirically investigate the relation between the optimal model and the graph search entropy by plotting them against each other in Figure 4, and perform linear regression. The optimal model sizes are obtained from the synthetic experiments conducted in the ablation studies. In the ablation studies we only report the results for exponentially increasing model sizes for clarity. In this study to better capture the optimal model size, we make the model sizes near the optimal model size more fine-grain. In all experiments, we keep the training hyperparameter the same, with 10k train steps. \n\nWe find a strong linear relation between the optimal model size and the graph search entropy with R 2 = 0.85. Note that there are a few sources of noise for locating the optimal model size for a specific knowledge graph. First, we only train language model with selected sizes due to compute and time limitations, and the quantization of the model size would disrupt the smoothness of the scaling law. Second, the exact location of the optimal model size is dependent on the training steps, which we did not thoroughly traverse but choose to inspect at the training step 10k. \n\nAfter fitting a linear regression line using the data from our synthetic experiments, we check the validity of this empirical scaling law against our real-world knowledge graph, FB15K-237. We calculate the graph search entropy for FB15K-237, and find the predicted optimal model size is very close to the observed optimal model size, shown as a green dot in Figure 4. \n\nFrom our scaling law, we can see that roughly 124 additional parameters in the optimal model size are required per 1-bit entropy increase in the knowledge graph. That is a language model can only reliably (not perfectly) reason over 0.008 bit information per parameter. This is very different from the knowledge capacity scaling law concluded by Allen-Zhu & Li (2025), which shows that the language model can store 2 bits of knowledge per parameter. We think this discrepancy is due to two reasons: first, our scaling law is not only about memorizing the knowledge, but also about reasoning over the learned knowledge, which is significantly harder. Second, the way we compute the graph search entropy is fundamentally different from the way Allen-Zhu & Li (2025) computes the knowledge entropy.",
            "score": 0.43898437086668357,
            "section_title": "Optimal Model Size v.s. Graph Search Entropy",
            "char_start_offset": 21715,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 174
                },
                {
                    "start": 175,
                    "end": 277
                },
                {
                    "start": 278,
                    "end": 382
                },
                {
                    "start": 383,
                    "end": 507
                },
                {
                    "start": 508,
                    "end": 595
                },
                {
                    "start": 598,
                    "end": 707
                },
                {
                    "start": 708,
                    "end": 818
                },
                {
                    "start": 819,
                    "end": 999
                },
                {
                    "start": 1000,
                    "end": 1173
                },
                {
                    "start": 1176,
                    "end": 1364
                },
                {
                    "start": 1365,
                    "end": 1543
                },
                {
                    "start": 1546,
                    "end": 1707
                },
                {
                    "start": 1708,
                    "end": 1815
                },
                {
                    "start": 1816,
                    "end": 1995
                },
                {
                    "start": 1996,
                    "end": 2195
                },
                {
                    "start": 2196,
                    "end": 2341
                }
            ],
            "ref_mentions": [
                {
                    "start": 1892,
                    "end": 1913,
                    "matchedPaperCorpusId": "269005957"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.043121337890625
        },
        {
            "corpus_id": "253553203",
            "title": "Galactica: A Large Language Model for Science",
            "text": "The idea of \"scaling laws\" was put forward by , who demonstrated evidence that loss scales as a power-law with model size, dataset size, and the amount of training compute. The focus was on upstream perplexity, and work by Tay et al. (2022a) showed that this does not always correlate with downstream performance. Hoffmann et al. (2022) presented new analysis taking into account the optimal amount of data, and suggested that existing language models were undertrained: \"Chinchilla scaling laws\". This work did not take into the account of fresh versus repeated tokens. In this work, we show that we can improve upstream and downstream performance by training on repeated tokens.",
            "score": 0.43861957524257267,
            "section_title": "Scaling Laws",
            "char_start_offset": 2912,
            "sentence_offsets": [],
            "ref_mentions": [
                {
                    "start": 314,
                    "end": 336,
                    "matchedPaperCorpusId": "247778764"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.23876953125
        },
        {
            "corpus_id": "277467322",
            "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
            "text": "Can training-time choices like SFT and RL affect the effectiveness of TTS? A: Yes, training-time strategies often shape the ceiling of what TTS methods can achieve at inference. For example, SFT can provide strong reasoning priors that improve the stability and quality of scaling strategies like self-consistency or verifier-based search. On the other hand, RL-based fine-tuning can explicitly incentivize concise and correct reasoning chains, reducing overthinking or incoherent outputs at test time. Both approaches can also be combined: SFT initializes the policy, while RL further sharpens it-this is the strategy behind systems like DeepSeek-R1. Alternatively, RL-trained models can be distilled back into SFT models to reduce inference-time costs without losing the benefits of RL optimization. \n\nQ: How can we improve the efficiency of TTS in multi-turn setups? A: Improving TTS efficiency involves both structural design and learned behaviors: (1) Model-based: Fine-tune models to generate more concise and purposeful reasoning traces. Reinforcement learning (RL) or reward-guided training can penalize overlong or redundant outputs while preserving correctness. Distillation from overthinking-prone models into lighter, more efficient ones can further reduce reasoning overhead. (2) Output-based: Apply early stopping strategies based on verifier confidence or answer agreement to terminate reasoning once sufficient certainty is achieved. This avoids unnecessary continuation in well-understood problems. (3) Prompt-based: Use prompt-level controls-such as token budgets, step limits, or task difficulty cues-to guide the model toward more targeted and efficient reasoning paths at inference. These strategies together help reduce computational cost while maintaining strong solution quality during TTS. \n\nQ: What are some representative or widely-used TTS methods that can serve as baselines? A: Parallel-Self-Consistency, Best-of-N; Sequential-STaR, Self-Refine, PRM; Hybrid-MCTS, ToT; Internal-Distilled-R1, R1. \n\nQ: Is there an optimal go-to solution so far? A: No free lunch. Optimal computing is often dependent on the hardness and openness of the question.",
            "score": 0.43852001599189716,
            "section_title": "Q:",
            "char_start_offset": 69139,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 177
                },
                {
                    "start": 178,
                    "end": 339
                },
                {
                    "start": 340,
                    "end": 502
                },
                {
                    "start": 503,
                    "end": 651
                },
                {
                    "start": 652,
                    "end": 801
                },
                {
                    "start": 804,
                    "end": 869
                },
                {
                    "start": 870,
                    "end": 1044
                },
                {
                    "start": 1045,
                    "end": 1171
                },
                {
                    "start": 1172,
                    "end": 1288
                },
                {
                    "start": 1289,
                    "end": 1449
                },
                {
                    "start": 1450,
                    "end": 1515
                },
                {
                    "start": 1516,
                    "end": 1703
                },
                {
                    "start": 1704,
                    "end": 1814
                },
                {
                    "start": 1817,
                    "end": 1904
                },
                {
                    "start": 1905,
                    "end": 2025
                },
                {
                    "start": 2028,
                    "end": 2073
                },
                {
                    "start": 2074,
                    "end": 2091
                },
                {
                    "start": 2092,
                    "end": 2174
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.015960693359375
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "1. \"How does the sparsity level impact the scaling laws of the relationship between N and C for training-compute optimal models?\" To address this question in \u00a72.1, we fix S and vary N , studying how optimal N and N a change for different values of S:  2. \"Is there an optimal balance between total number of parameters and the sparsity level under fixed training-compute budget?\" To address this question in \u00a72.2, we fix N and vary S, studying how optimal S changes across different values of N : \n\nAs the first step, considering a fixed training compute budget C, we fit a 3D surface, referred to as the IsoFLOP surface, in Figure 1a, using a polynomial function, following approach II of Hoffmann et al. (2022). Compared to Hoffmann et al. (2022) we include the sparsity variable and fit a single 3d IsoFLOP surface across all data points, rather than fitting separate 2d IsoFLOP curves for fixed sparsity levels or model sizes. We conducted a grid search to determine the optimal polynomial degree for N , S, and the interaction term N \u00d7 S, finding that a degree of (2, 2, 2) resulted in the lowest cross-validation error. Both N and S are in log space (see Appendix B for more details). Figure 1b illustrates the goodness of fit, demonstrating a strong correlation and low predictive error. \n\nAs seen in Figure 1a, the IsoFLOP surface plot is parabolic along model size, suggesting that the findings of Hoffmann et al. (2022) extend to MoEs across different sparsity levels, i.e., L(N ; C, S) is parabolic, with its optimal solution located at the turning point. When considering the total number of parameters N , the optimal value increases as the sparsity level increases, while for the active number of parameters N a the optimal value decreases with the sparsity level. This indicates that by increasing the sparsity level the training compute optimal models are larger but have fewer FLOPs per example, i.e., lower inference cost. Moreover, along sparsity, the pretraining loss decreases monotonically, indicating that, for the same compute budget, sparser models achieve better pretraining performance.",
            "score": 0.4379753213443166,
            "section_title": "The Interplay between Model Parameters and Sparsity in MoEs",
            "char_start_offset": 6884,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 129
                },
                {
                    "start": 130,
                    "end": 379
                },
                {
                    "start": 380,
                    "end": 496
                },
                {
                    "start": 499,
                    "end": 713
                },
                {
                    "start": 714,
                    "end": 930
                },
                {
                    "start": 931,
                    "end": 1125
                },
                {
                    "start": 1126,
                    "end": 1190
                },
                {
                    "start": 1191,
                    "end": 1294
                },
                {
                    "start": 1297,
                    "end": 1566
                },
                {
                    "start": 1567,
                    "end": 1778
                },
                {
                    "start": 1779,
                    "end": 1940
                },
                {
                    "start": 1941,
                    "end": 2113
                }
            ],
            "ref_mentions": [
                {
                    "start": 690,
                    "end": 712,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 726,
                    "end": 748,
                    "matchedPaperCorpusId": "258509679"
                },
                {
                    "start": 1407,
                    "end": 1429,
                    "matchedPaperCorpusId": "258509679"
                }
            ],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.242431640625
        },
        {
            "corpus_id": "275789885",
            "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models",
            "text": "In this paper, we try to understand the optimal trade-off between parameters and compute per example to optimally increase the model capacity. Our experiments and analysis, underscores the role of sparsity, as a factor that controls the balance between total number of parameters and FLOPs per example, in the scaling laws for Mixture-of-Expert Transformers (MoEs), showing that the most efficient model configuration depends on balancing model size, training compute, and sparsity level. \n\nThe optimal recipe for balancing FLOPs per example and parameter count in MoEs depends on the objective as well other resource constraints. Our findings indicate that sparsity, as a knob that controls FLOPs per example in MoEs, is a powerful mechanism for optimizing model performance under constrained training compute budgets. By balancing the total number of parameters, compute, and sparsity, MoEs can be scaled more effectively. These insights provide valuable guidance for scaling language models, especially for MoEs, where the trade-offs between parameters and FLOPs must be carefully managed. \n\nMoEs were originally introduced to allow increasing model capacity without a significant increase in inference cost. Our experiments show that under fixed total training compute budget increasing sparsity in MoEs leads to smaller FLOPs per example, higher number of parameters, and lower pretraining loss simultaneously. In other words, in the context of MoEs, if there are no constraints on the total number of parameters, increasing the capacity of the model through parameter count seem to be the optimal strategy if lower pretraining loss is the main goal. On the other hand, when comparing how well the pretraining performance transfers to various downstream tasks, denser models seem to be better on certain types of task that potentially rely on deeper processing of the input vs the knowledge stored in the parameters of the model. This potentially signals the importance of the role of FLOPs per example in increasing the capacity of the model during inference. \n\nOur experiments demonstrate that Mixture-of-Experts (MoE) models use Chain-of-Thought (CoT) prompting more effectively than dense models, achieving better performance when allocated additional computational resources during inference. This observation reveals an interesting direction to improve the performance efficiency of MoEs at inference time.",
            "score": 0.43759376725346566,
            "section_title": "Conclusion",
            "char_start_offset": 27713,
            "sentence_offsets": [
                {
                    "start": 0,
                    "end": 142
                },
                {
                    "start": 143,
                    "end": 488
                },
                {
                    "start": 491,
                    "end": 630
                },
                {
                    "start": 631,
                    "end": 819
                },
                {
                    "start": 820,
                    "end": 924
                },
                {
                    "start": 925,
                    "end": 1092
                },
                {
                    "start": 1095,
                    "end": 1211
                },
                {
                    "start": 1212,
                    "end": 1415
                },
                {
                    "start": 1416,
                    "end": 1655
                },
                {
                    "start": 1656,
                    "end": 1934
                },
                {
                    "start": 1935,
                    "end": 2065
                },
                {
                    "start": 2068,
                    "end": 2302
                },
                {
                    "start": 2303,
                    "end": 2417
                }
            ],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1331787109375
        },
        {
            "corpus_id": "276961579",
            "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
            "text": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget.",
            "score": 0.43701875225945785,
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "pdf_hash": "",
            "stype": "vespa",
            "rerank_score": 0.1693115234375
        },
        {
            "paperId": "89002efea1f669b82215dd7cff4d16287f62d891",
            "corpusId": 276647485,
            "title": "Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 84,
            "citationCount": 8,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2502.20379, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2093408777",
                    "name": "Shalev Lifshitz"
                },
                {
                    "authorId": "2268166612",
                    "name": "Sheila A. McIlraith"
                },
                {
                    "authorId": "2347681184",
                    "name": "Yilun Du"
                }
            ],
            "abstract": "By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.",
            "corpus_id": "276647485",
            "text": "By utilizing more computational resources at test-time, large language models (LLMs) can improve without additional training. One common strategy uses verifiers to evaluate candidate outputs. In this work, we propose a novel scaling dimension for test-time compute: scaling the number of verifiers. We introduce Multi-Agent Verification (MAV) as a test-time compute paradigm that combines multiple verifiers to improve performance. We propose using Aspect Verifiers (AVs), off-the-shelf LLMs prompted to verify different aspects of outputs, as one possible choice for the verifiers in a MAV system. AVs are a convenient building block for MAV since they can be easily combined without additional training. Moreover, we introduce BoN-MAV, a simple multi-agent verification algorithm that combines best-of-n sampling with multiple verifiers. BoN-MAV demonstrates stronger scaling patterns than self-consistency and reward model verification, and we demonstrate both weak-to-strong generalization, where combining weak verifiers improves even stronger LLMs, and self-improvement, where the same base model is used to both generate and verify outputs. Our results establish scaling the number of verifiers as a promising new dimension for improving language model performance at test-time.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.04248046875
        },
        {
            "paperId": "67aea1062af04099dbf8600efef266d4c53d098d",
            "corpusId": 279260725,
            "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report",
            "venue": "",
            "year": 2025,
            "referenceCount": 46,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.08228, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2366157284",
                    "name": "Mustafa Baniodeh"
                },
                {
                    "authorId": "2957685",
                    "name": "Kratarth Goel"
                },
                {
                    "authorId": "2295512886",
                    "name": "Scott Ettinger"
                },
                {
                    "authorId": "2331449646",
                    "name": "Carlos Fuertes"
                },
                {
                    "authorId": "2323369278",
                    "name": "Ari Seff"
                },
                {
                    "authorId": "2366143602",
                    "name": "Tim Shen"
                },
                {
                    "authorId": "40273102",
                    "name": "Cole Gulino"
                },
                {
                    "authorId": "2366629096",
                    "name": "Chenjie Yang"
                },
                {
                    "authorId": "3451901",
                    "name": "Ghassen Jerfel"
                },
                {
                    "authorId": "51221461",
                    "name": "Dokook Choe"
                },
                {
                    "authorId": "2367634427",
                    "name": "Rui Wang"
                },
                {
                    "authorId": "1833585",
                    "name": "V. Kallem"
                },
                {
                    "authorId": "2366163807",
                    "name": "Sergio Casas"
                },
                {
                    "authorId": "2366163750",
                    "name": "Rami Al-Rfou"
                },
                {
                    "authorId": "2328310625",
                    "name": "Benjamin Sapp"
                },
                {
                    "authorId": "1881964177",
                    "name": "Drago Anguelov"
                }
            ],
            "abstract": "We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.",
            "corpus_id": "279260725",
            "text": "We study the empirical scaling laws of a family of encoder-decoder autoregressive transformer models on the task of joint motion forecasting and planning in the autonomous driving domain. Using a 500 thousand hours driving dataset, we demonstrate that, similar to language modeling, model performance improves as a power-law function of the total compute budget, and we observe a strong correlation between model training loss and model evaluation metrics. Most interestingly, closed-loop metrics also improve with scaling, which has important implications for the suitability of open-loop metrics for model development and hill climbing. We also study the optimal scaling of the number of transformer parameters and the training data size for a training compute-optimal model. We find that as the training compute budget grows, optimal scaling requires increasing the model size 1.5x as fast as the dataset size. We also study inference-time compute scaling, where we observe that sampling and clustering the output of smaller models makes them competitive with larger models, up to a crossover point beyond which a larger models becomes more inference-compute efficient. Overall, our experimental results demonstrate that optimizing the training and inference-time scaling properties of motion forecasting and planning models is a key lever for improving their performance to address a wide variety of driving scenarios. Finally, we briefly study the utility of training on general logged driving data of other agents to improve the performance of the ego-agent, an important research area to address the scarcity of robotics data for large capacity models training.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.275146484375
        },
        {
            "paperId": "8b8555d4b109395d937f8b20deaee76ebaa83d73",
            "corpusId": 279244494,
            "title": "Kinetics: Rethinking Test-Time Scaling Laws",
            "venue": "",
            "year": 2025,
            "referenceCount": 102,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2506.05333, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2003376054",
                    "name": "Ranajoy Sadhukhan"
                },
                {
                    "authorId": "2300556424",
                    "name": "Zhuo Chen"
                },
                {
                    "authorId": "2366124482",
                    "name": "Haizhong Zheng"
                },
                {
                    "authorId": "2364951887",
                    "name": "Yang Zhou"
                },
                {
                    "authorId": "2268272",
                    "name": "Emma Strubell"
                },
                {
                    "authorId": "2301065578",
                    "name": "Beidi Chen"
                }
            ],
            "abstract": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
            "corpus_id": "279244494",
            "text": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential and increasingly important with more computing invested, for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as a function of computation, and continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.2130126953125
        },
        {
            "paperId": "1ef101c16acbb522964aa77353692dcf69da6e68",
            "corpusId": 273821874,
            "title": "Language Models and Cycle Consistency for Self-Reflective Machine Translation",
            "venue": "arXiv.org",
            "year": 2024,
            "referenceCount": 23,
            "citationCount": 0,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2411.02791, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "3383269",
                    "name": "Jianqiao Wangni"
                }
            ],
            "abstract": "This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A to a target language B, and subsequently translate these candidates back to the original language A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law and test-time computation scaling law. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.",
            "corpus_id": "273821874",
            "text": "This paper introduces a novel framework that leverages large language models (LLMs) for machine translation (MT). We start with one conjecture: an ideal translation should contain complete and accurate information for a strong enough LLM to recover the original sentence. We generate multiple translation candidates from a source language A to a target language B, and subsequently translate these candidates back to the original language A. By evaluating the cycle consistency between the original and back-translated sentences using metrics such as token-level precision and accuracy, we implicitly estimate the translation quality in language B, without knowing its ground-truth. This also helps to evaluate the LLM translation capability, only with monolingual corpora. For each source sentence, we identify the translation candidate with optimal cycle consistency with the original sentence as the final answer. Our experiments demonstrate that larger LLMs, or the same LLM with more forward passes during inference, exhibit increased cycle consistency, aligning with the LLM model size scaling law and test-time computation scaling law. This work provide methods for, 1) to implicitly evaluate translation quality of a sentence in the target language, 2), to evaluate capability of LLM for any-to-any-language translation, and 3), how to generate a better translation for a specific LLM.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.00821685791015625
        },
        {
            "paperId": "82a82bbb75b08c3de78f7cebd52e1c3c38d515b2",
            "corpusId": 277065878,
            "title": "Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition",
            "venue": "arXiv.org",
            "year": 2025,
            "referenceCount": 106,
            "citationCount": 5,
            "influentialCitationCount": 0,
            "isOpenAccess": false,
            "openAccessPdf": {
                "url": "",
                "status": null,
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://arxiv.org/abs/2503.12303, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2351433544",
                    "name": "Xiaoying Zhang"
                },
                {
                    "authorId": "2350475912",
                    "name": "Da Peng"
                },
                {
                    "authorId": "2336012888",
                    "name": "Yipeng Zhang"
                },
                {
                    "authorId": "2292210274",
                    "name": "Zonghao Guo"
                },
                {
                    "authorId": "2350872790",
                    "name": "Chengyue Wu"
                },
                {
                    "authorId": "2350519623",
                    "name": "Chi Chen"
                },
                {
                    "authorId": "2350513176",
                    "name": "Wei Ke"
                },
                {
                    "authorId": "2284066927",
                    "name": "Helen Meng"
                },
                {
                    "authorId": "2350529146",
                    "name": "Maosong Sun"
                }
            ],
            "abstract": "Recent progress in (multimodal) large language models ((M)LLMs) has shifted focus from pre-training to inference-time computation and post-training optimization, largely due to concerns over the availability of high-quality human data. However, these strategies alone are insufficient to drive substantial model improvements. We argue that effective model advancement requires strong synergy among pre-training, inference-time computation, and post-training optimization. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework for constructing next-generation foundation MLLMs by imparting multimodal knowledge and enhancing systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description for step-by-step visual understanding and integrate structured Chain-of-Thought (CoT) reasoning to support in-depth multimodal reasoning. SIcog first equips a base model with systematic perception and reasoning using minimal external supervision. The enhanced models then generate candidate image captions and CoT reasoning responses for unlabeled images and image-question pairs across diverse tasks, which are filtered through a semantic-similarity-guided self-consistency mechanism. These high-quality, self-generated samples enable large-scale multimodal pre-training, creating a self-improvement loop. Experiments demonstrate SIcog's effectiveness in developing MLLMs with enhanced multimodal cognition. Using only 213K self-generated pre-training samples, SIcog achieves significant improvements, including +3.6% on MMStar and +3.5% on AI2D, outperforming previous pre-training approaches. When combined with post-training techniques for CoT reasoning, SIcog yields +9% gains on MMVet and +8.5% on ScienceQA.",
            "corpus_id": "277065878",
            "text": "Recent progress in (multimodal) large language models ((M)LLMs) has shifted focus from pre-training to inference-time computation and post-training optimization, largely due to concerns over the availability of high-quality human data. However, these strategies alone are insufficient to drive substantial model improvements. We argue that effective model advancement requires strong synergy among pre-training, inference-time computation, and post-training optimization. In this paper, we introduce Self-Improving cognition (SIcog), a self-learning framework for constructing next-generation foundation MLLMs by imparting multimodal knowledge and enhancing systematic cognitive capabilities through multimodal pre-training with self-generated data. Specifically, we propose Chain-of-Description for step-by-step visual understanding and integrate structured Chain-of-Thought (CoT) reasoning to support in-depth multimodal reasoning. SIcog first equips a base model with systematic perception and reasoning using minimal external supervision. The enhanced models then generate candidate image captions and CoT reasoning responses for unlabeled images and image-question pairs across diverse tasks, which are filtered through a semantic-similarity-guided self-consistency mechanism. These high-quality, self-generated samples enable large-scale multimodal pre-training, creating a self-improvement loop. Experiments demonstrate SIcog's effectiveness in developing MLLMs with enhanced multimodal cognition. Using only 213K self-generated pre-training samples, SIcog achieves significant improvements, including +3.6% on MMStar and +3.5% on AI2D, outperforming previous pre-training approaches. When combined with post-training techniques for CoT reasoning, SIcog yields +9% gains on MMVet and +8.5% on ScienceQA.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.006412506103515625
        },
        {
            "paperId": "dfb4a0245fac6ecab0720b99da98e130333e50bd",
            "corpusId": 3691454,
            "title": "Smartphone-Based System for Learning and Inferring Hearing Aid Settings.",
            "venue": "Journal of american academy of audiology",
            "year": 2016,
            "referenceCount": 50,
            "citationCount": 28,
            "influentialCitationCount": 1,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://europepmc.org/articles/pmc5266590?pdf=render",
                "status": "GREEN",
                "license": null,
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.3766/JAAA.15099?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.3766/JAAA.15099, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2973388",
                    "name": "Gabriel Aldaz"
                },
                {
                    "authorId": "6111193",
                    "name": "S. Puria"
                },
                {
                    "authorId": "7242868",
                    "name": "L. Leifer"
                }
            ],
            "abstract": "BACKGROUND\nPrevious research has shown that hearing aid wearers can successfully self-train their instruments' gain-frequency response and compression parameters in everyday situations. Combining hearing aids with a smartphone introduces additional computing power, memory, and a graphical user interface that may enable greater setting personalization. To explore the benefits of self-training with a smartphone-based hearing system, a parameter space was chosen with four possible combinations of microphone mode (omnidirectional and directional) and noise reduction state (active and off). The baseline for comparison was the \"untrained system,\" that is, the manufacturer's algorithm for automatically selecting microphone mode and noise reduction state based on acoustic environment. The \"trained system\" first learned each individual's preferences, self-entered via a smartphone in real-world situations, to build a trained model. The system then predicted the optimal setting (among available choices) using an inference engine, which considered the trained model and current context (e.g., sound environment, location, and time).\n\n\nPURPOSE\nTo develop a smartphone-based prototype hearing system that can be trained to learn preferred user settings. Determine whether user study participants showed a preference for trained over untrained system settings.\n\n\nRESEARCH DESIGN\nAn experimental within-participants study. Participants used a prototype hearing system-comprising two hearing aids, Android smartphone, and body-worn gateway device-for \u223c6 weeks.\n\n\nSTUDY SAMPLE\nSixteen adults with mild-to-moderate sensorineural hearing loss (HL) (ten males, six females; mean age = 55.5 yr). Fifteen had \u22656 mo of experience wearing hearing aids, and 14 had previous experience using smartphones.\n\n\nINTERVENTION\nParticipants were fitted and instructed to perform daily comparisons of settings (\"listening evaluations\") through a smartphone-based software application called Hearing Aid Learning and Inference Controller (HALIC). In the four-week-long training phase, HALIC recorded individual listening preferences along with sensor data from the smartphone-including environmental sound classification, sound level, and location-to build trained models. In the subsequent two-week-long validation phase, participants performed blinded listening evaluations comparing settings predicted by the trained system (\"trained settings\") to those suggested by the hearing aids' untrained system (\"untrained settings\").\n\n\nDATA COLLECTION AND ANALYSIS\nWe analyzed data collected on the smartphone and hearing aids during the study. We also obtained audiometric and demographic information.\n\n\nRESULTS\nOverall, the 15 participants with valid data significantly preferred trained settings to untrained settings (paired-samples t test). Seven participants had a significant preference for trained settings, while one had a significant preference for untrained settings (binomial test). The remaining seven participants had nonsignificant preferences. Pooling data across participants, the proportion of times that each setting was chosen in a given environmental sound class was on average very similar. However, breaking down the data by participant revealed strong and idiosyncratic individual preferences. Fourteen participants reported positive feelings of clarity, competence, and mastery when training via HALIC.\n\n\nCONCLUSIONS\nThe obtained data, as well as subjective participant feedback, indicate that smartphones could become viable tools to train hearing aids. Individuals who are tech savvy and have milder HL seem well suited to take advantages of the benefits offered by training with a smartphone.",
            "corpus_id": "3691454",
            "text": "BACKGROUND\nPrevious research has shown that hearing aid wearers can successfully self-train their instruments' gain-frequency response and compression parameters in everyday situations. Combining hearing aids with a smartphone introduces additional computing power, memory, and a graphical user interface that may enable greater setting personalization. To explore the benefits of self-training with a smartphone-based hearing system, a parameter space was chosen with four possible combinations of microphone mode (omnidirectional and directional) and noise reduction state (active and off). The baseline for comparison was the \"untrained system,\" that is, the manufacturer's algorithm for automatically selecting microphone mode and noise reduction state based on acoustic environment. The \"trained system\" first learned each individual's preferences, self-entered via a smartphone in real-world situations, to build a trained model. The system then predicted the optimal setting (among available choices) using an inference engine, which considered the trained model and current context (e.g., sound environment, location, and time).\n\n\nPURPOSE\nTo develop a smartphone-based prototype hearing system that can be trained to learn preferred user settings. Determine whether user study participants showed a preference for trained over untrained system settings.\n\n\nRESEARCH DESIGN\nAn experimental within-participants study. Participants used a prototype hearing system-comprising two hearing aids, Android smartphone, and body-worn gateway device-for \u223c6 weeks.\n\n\nSTUDY SAMPLE\nSixteen adults with mild-to-moderate sensorineural hearing loss (HL) (ten males, six females; mean age = 55.5 yr). Fifteen had \u22656 mo of experience wearing hearing aids, and 14 had previous experience using smartphones.\n\n\nINTERVENTION\nParticipants were fitted and instructed to perform daily comparisons of settings (\"listening evaluations\") through a smartphone-based software application called Hearing Aid Learning and Inference Controller (HALIC). In the four-week-long training phase, HALIC recorded individual listening preferences along with sensor data from the smartphone-including environmental sound classification, sound level, and location-to build trained models. In the subsequent two-week-long validation phase, participants performed blinded listening evaluations comparing settings predicted by the trained system (\"trained settings\") to those suggested by the hearing aids' untrained system (\"untrained settings\").\n\n\nDATA COLLECTION AND ANALYSIS\nWe analyzed data collected on the smartphone and hearing aids during the study. We also obtained audiometric and demographic information.\n\n\nRESULTS\nOverall, the 15 participants with valid data significantly preferred trained settings to untrained settings (paired-samples t test). Seven participants had a significant preference for trained settings, while one had a significant preference for untrained settings (binomial test). The remaining seven participants had nonsignificant preferences. Pooling data across participants, the proportion of times that each setting was chosen in a given environmental sound class was on average very similar. However, breaking down the data by participant revealed strong and idiosyncratic individual preferences. Fourteen participants reported positive feelings of clarity, competence, and mastery when training via HALIC.\n\n\nCONCLUSIONS\nThe obtained data, as well as subjective participant feedback, indicate that smartphones could become viable tools to train hearing aids. Individuals who are tech savvy and have milder HL seem well suited to take advantages of the benefits offered by training with a smartphone.",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.0012598037719726562
        },
        {
            "paperId": "cc94320876e159e9421a7e15e578cd4196b1b1cc",
            "corpusId": 229162762,
            "title": "IEEE Access Special Section Editorial: Scalable Deep Learning for Big Data",
            "venue": "IEEE Access",
            "year": 2020,
            "referenceCount": 1,
            "citationCount": 3,
            "influentialCitationCount": 0,
            "isOpenAccess": true,
            "openAccessPdf": {
                "url": "https://doi.org/10.1109/access.2020.3041166",
                "status": "GOLD",
                "license": "CCBY",
                "disclaimer": "Notice: Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ACCESS.2020.3041166?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ACCESS.2020.3041166, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."
            },
            "authors": [
                {
                    "authorId": "2273607213",
                    "name": "Liangxiu Han"
                },
                {
                    "authorId": "2270814943",
                    "name": "Daoqiang Zhang"
                },
                {
                    "authorId": "2269819182",
                    "name": "Omer F. Rana"
                },
                {
                    "authorId": "2269842228",
                    "name": "Yi Pan"
                },
                {
                    "authorId": "2323517439",
                    "name": "Sohail Jabbar"
                },
                {
                    "authorId": "38383296",
                    "name": "Mazin S. Yousif"
                },
                {
                    "authorId": "2131706",
                    "name": "Moayad Aloqaily"
                }
            ],
            "abstract": "Deep learning (DL) has emerged as a key application exploiting the increasing computational power in systems such asGPUs,multicore processors, Systems-on-Chip (SoC), and distributed clusters. It has also attracted much attention in discovering correlation patterns in data in an unsupervised manner and has been applied in various domains including speech recognition, image classification, natural language processing, and computer vision. Unlike traditional machine learning (ML) approaches, DL also enables dynamic discovery of features from data. In addition, now, a number of commercial vendors also offer accelerators for deep learning systems (such as Nvidia, Intel, and Huawei). Typical deep neural networks (DNNs) require large amounts of data to learn parameters (often reachingmillions), which is a computationally intensive process requiring significant time to train a model. As the data size increases and as deep learningmodels becomemore complex, it requires more computing power and memory to train an accurate model in a timely manner. Despite existing efforts in training and inference of deep learning models to increase concurrency, many existing training algorithms for deep learning are notoriously difficult to scale and parallelize due to inherent algorithmic interdependencies and the training data. Therefore, there is a need to develop efficient and scalable deep learning frameworks suitable for big data processing and analysis. This Special Section of IEEE ACCESS on scalable deep learning for big data brings together research contributions from academia and industry which address key challenges in big data processing and analysis using scalable deep learning. The Call for Papers attracted a lot of attention from the scientific community and received 92 submissions, out of which 28 articles were accepted for inclusion in the Special Section after a thorough review process. Each submission was reviewed by at least two independent referees. The accepted articles cover a range of topics relevant to this Special Section including algorithmic developments to support applications in different domains (such as healthcare, transportation, agriculture, power system, flood monitoring, mine gas monitoring, social networks, cyber security, software engineering, etc.) and using various types of big data (text, images, videos, time-series data, etc.). In the article \u2018\u2018Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of deep neural networks,\u2019\u2019 by Cho et al., the authors describe a simple yet robust algorithm for DNN hyperparameter optimization\u2014DEEP-BO (diversified, early terminationenabled, and parallel Bayesian optimization). When evaluated over six DNN benchmarks, DEEP-BO mostly outperformed well-known solutions including GP-Hedge, BOHB, and the speed-up variants that use median stopping rule or learning curve extrapolation. In the article \u2018\u2018GAN-knowledge distillation for one-stage object detection,\u2019\u2019 by Wang et al., the authors propose a clean and effective knowledge distillation method called generative adversarial networks\u2013knowledge distillation (GAN-KD) for one-stage object detection. The feature maps generated by teacher and student network are employed as true and fake samples, respectively, and generate adversarial training to improve the performance of the student network in one-stage object detection. In the article \u2018\u2018Attention-based dense decoding network for monocular depth estimation,\u2019\u2019 by Wang et al., the authors describe a novel encoder\u2013decoder attention-dense decoding network to solve the local depth detail loss caused by convolution stacking. It takes advantage of the channel\u2013spatial attention module, which captures the dependence between different channels and spatial locations by self-attentions. It also introduces a dense decoding module to capture more massive and denser attention features. A distance-aware loss function that pays more attention to long-distance objects (i.e., objects that are further away in an image) is also introduced. In the article \u2018\u2018R3MR: Region growing based 3D mesh reconstruction for big data platform,\u2019\u2019 by Li et al., a novel region growing based 3-D mesh reconstruction method for a big data platform is introduced. This approach reconstructs the classified data points from simple to complex by the rational principle of optimal selection and the use of the inner edge adjacency list. Experimental results show that the proposed method can accurately reconstruct the surface shape of the point cloud model and could reflect the detailed features of the model more naturally. The article by Yasir et al., \u2018\u2018TRICE: Mining frequent itemsets by iterative TRimmed transaction LattICE in sparse big data,\u2019\u2019 introduces a novel method to mine frequent itemsets by iterative TRimmed transaction lattICE (TRICE) for processing and analyzing sparse data sets. The proposed method shows better performance than the existing",
            "corpus_id": "229162762",
            "text": "Deep learning (DL) has emerged as a key application exploiting the increasing computational power in systems such asGPUs,multicore processors, Systems-on-Chip (SoC), and distributed clusters. It has also attracted much attention in discovering correlation patterns in data in an unsupervised manner and has been applied in various domains including speech recognition, image classification, natural language processing, and computer vision. Unlike traditional machine learning (ML) approaches, DL also enables dynamic discovery of features from data. In addition, now, a number of commercial vendors also offer accelerators for deep learning systems (such as Nvidia, Intel, and Huawei). Typical deep neural networks (DNNs) require large amounts of data to learn parameters (often reachingmillions), which is a computationally intensive process requiring significant time to train a model. As the data size increases and as deep learningmodels becomemore complex, it requires more computing power and memory to train an accurate model in a timely manner. Despite existing efforts in training and inference of deep learning models to increase concurrency, many existing training algorithms for deep learning are notoriously difficult to scale and parallelize due to inherent algorithmic interdependencies and the training data. Therefore, there is a need to develop efficient and scalable deep learning frameworks suitable for big data processing and analysis. This Special Section of IEEE ACCESS on scalable deep learning for big data brings together research contributions from academia and industry which address key challenges in big data processing and analysis using scalable deep learning. The Call for Papers attracted a lot of attention from the scientific community and received 92 submissions, out of which 28 articles were accepted for inclusion in the Special Section after a thorough review process. Each submission was reviewed by at least two independent referees. The accepted articles cover a range of topics relevant to this Special Section including algorithmic developments to support applications in different domains (such as healthcare, transportation, agriculture, power system, flood monitoring, mine gas monitoring, social networks, cyber security, software engineering, etc.) and using various types of big data (text, images, videos, time-series data, etc.). In the article \u2018\u2018Basic enhancement strategies when using Bayesian optimization for hyperparameter tuning of deep neural networks,\u2019\u2019 by Cho et al., the authors describe a simple yet robust algorithm for DNN hyperparameter optimization\u2014DEEP-BO (diversified, early terminationenabled, and parallel Bayesian optimization). When evaluated over six DNN benchmarks, DEEP-BO mostly outperformed well-known solutions including GP-Hedge, BOHB, and the speed-up variants that use median stopping rule or learning curve extrapolation. In the article \u2018\u2018GAN-knowledge distillation for one-stage object detection,\u2019\u2019 by Wang et al., the authors propose a clean and effective knowledge distillation method called generative adversarial networks\u2013knowledge distillation (GAN-KD) for one-stage object detection. The feature maps generated by teacher and student network are employed as true and fake samples, respectively, and generate adversarial training to improve the performance of the student network in one-stage object detection. In the article \u2018\u2018Attention-based dense decoding network for monocular depth estimation,\u2019\u2019 by Wang et al., the authors describe a novel encoder\u2013decoder attention-dense decoding network to solve the local depth detail loss caused by convolution stacking. It takes advantage of the channel\u2013spatial attention module, which captures the dependence between different channels and spatial locations by self-attentions. It also introduces a dense decoding module to capture more massive and denser attention features. A distance-aware loss function that pays more attention to long-distance objects (i.e., objects that are further away in an image) is also introduced. In the article \u2018\u2018R3MR: Region growing based 3D mesh reconstruction for big data platform,\u2019\u2019 by Li et al., a novel region growing based 3-D mesh reconstruction method for a big data platform is introduced. This approach reconstructs the classified data points from simple to complex by the rational principle of optimal selection and the use of the inner edge adjacency list. Experimental results show that the proposed method can accurately reconstruct the surface shape of the point cloud model and could reflect the detailed features of the model more naturally. The article by Yasir et al., \u2018\u2018TRICE: Mining frequent itemsets by iterative TRimmed transaction LattICE in sparse big data,\u2019\u2019 introduces a novel method to mine frequent itemsets by iterative TRimmed transaction lattICE (TRICE) for processing and analyzing sparse data sets. The proposed method shows better performance than the existing",
            "section_title": "abstract",
            "char_start_offset": 0,
            "sentence_offsets": [],
            "ref_mentions": [],
            "score": 0.0,
            "stype": "public_api",
            "pdf_hash": "",
            "rerank_score": 0.020416259765625
        }
    ],
    "quotes": {
        "cost": 0.015005999999999999,
        "quotes": [
            {
                "idx": 0,
                "key": "[270764838 | Porian et al. | 2024 | Citations: 26]",
                "snippets": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related work",
                        "pdf_hash": "",
                        "start": 0,
                        "end": 362,
                        "sentence_offsets": [
                            {
                                "start": 0,
                                "end": 88
                            },
                            {
                                "start": 89,
                                "end": 362
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."
                    }
                ]
            },
            {
                "idx": 1,
                "key": "[275336968 | Lu | 2025 | Citations: 1]",
                "snippets": "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time.",
                "model": "claude-3-7-sonnet-20250219",
                "inline_citations": {},
                "metadata": [
                    {
                        "section_title": "Related Work",
                        "pdf_hash": "",
                        "start": 852,
                        "end": 1173,
                        "sentence_offsets": [
                            {
                                "start": 852,
                                "end": 1081
                            },
                            {
                                "start": 1082,
                                "end": 1173
                            }
                        ],
                        "ref_mentions": [],
                        "quote": "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                    }
                ]
            }
        ]
    },
    "cluster": {
        "cost": 0.011853,
        "cot": "To answer this query about optimal scaling laws for inference time compute, I need to organize the quotes that discuss how researchers have approached optimality for inference computation, as contrasted with training computation.\n\nLooking at the user query, they're asking about whether research has been done on optimality for inference time compute, similar to how scaling laws have been established for training compute. They specifically mention self-consistency as an example where sampling multiple outputs may be more effective than using a stronger model.\n\nLet me plan the dimensions:\n\n1. \"Introduction to Scaling Laws\" - This would provide background on scaling laws in general, focusing on how they've traditionally been applied to training compute. This helps set the context for the more specific inference-time optimality discussion. This will be a synthesis format to provide a coherent explanation of the concept.\n\n2. \"Research on Inference-Time Compute Optimality\" - This will cover the specific research mentioned in the quotes about inference-time compute optimization. Both quotes discuss research in this area, mentioning works by Sardana and Frankle, Sardana et al., and Snell et al. This will be in synthesis format to provide a coherent explanation of how researchers have approached this problem.\n\n3. \"Trade-offs Between Model Size and Inference Efficiency\" - This dimension focuses on the specific finding mentioned in the quotes about how considering inference costs can push optimization toward smaller models, which relates to the user's question about whether using a stronger model or multiple samples from a smaller model is more effective. This will be in synthesis format to provide a coherent analysis of these trade-offs.",
        "plan": {
            "Introduction to Scaling Laws (synthesis)": [],
            "Research on Inference-Time Compute Optimality (synthesis)": [
                0,
                1
            ],
            "Trade-offs Between Model Size and Inference Efficiency (synthesis)": [
                0
            ]
        },
        "model": "claude-3-7-sonnet-20250219"
    },
    "summary": {
        "sections": [
            {
                "title": "Introduction to Scaling Laws",
                "tldr": "Scaling laws describe how model performance improves with increased training compute, dataset size, and model parameters. While these laws are well-established for training, less research exists on optimal compute allocation during inference time. (LLM Memory)",
                "text": "\nScaling laws in machine learning have become a foundational concept for understanding how to build more capable AI systems. These mathematical relationships describe how model performance improves as we increase resources like training compute, dataset size, and model parameters. Research from OpenAI, DeepMind, and other labs has established that performance often follows predictable power-law relationships with these resources. For example, language model performance typically improves as a power-law of compute with exponents around 0.5, meaning that doubling compute leads to a fixed improvement in performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nWhile these scaling laws for training are well-documented, your query correctly identifies an important gap in the literature: optimal scaling laws for inference-time compute are less thoroughly explored. The training-focused scaling laws help us understand how to allocate resources during model development, but they don't necessarily tell us the most efficient ways to use these models after they're trained. This distinction is crucial because inference optimization involves different constraints and objectives than training optimization. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">",
                "citations": [],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Research on Inference-Time Compute Optimality",
                "tldr": "Recent research has begun addressing the gap in understanding optimal compute allocation during inference. Some researchers have specifically developed models that balance efficiency across both training and deployment phases. (2 sources)",
                "text": "\nWhile scaling laws for training are well-established, researchers have only recently begun to systematically study how to optimize compute resources during model inference. This emerging line of research recognizes that deployment efficiency is just as important as training efficiency, especially as models become more widely used.\n\nOne notable contribution comes from Sardana and Frankle, who extended scaling law research to account for inference costs. Their work showed that considering deployment compute naturally shifts optimal design choices toward smaller models, challenging the prevailing focus on ever-larger architectures <Paper corpusId=\"270764838\" paperTitle=\"(Porian et al., 2024)\" isShortName></Paper>. This represents an important recognition that the most compute-efficient model for training may not be the most efficient during deployment.\n\nBuilding on this foundation, Sardana and colleagues proposed methods specifically designed to balance efficiency across both training and deployment phases. Their approach suggests that smaller models trained on much larger datasets (potentially including synthetic data) can achieve better overall efficiency than models optimized solely for training <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. In parallel, Snell and collaborators focused specifically on test-time compute optimization strategies, further expanding our understanding of inference efficiency <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>.\n\nThese research efforts highlight a growing recognition that optimal resource allocation during inference requires different considerations than during training. As models continue to be deployed in increasingly diverse settings with varying resource constraints, understanding these inference-time scaling laws becomes increasingly important.",
                "citations": [
                    {
                        "id": "(Porian et al., 2024)",
                        "snippets": [
                            "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."
                        ],
                        "paper": {
                            "corpus_id": 270764838,
                            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                            "authors": [
                                {
                                    "authorId": "2308470091",
                                    "name": "Tomer Porian"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "2191688",
                                    "name": "J. Jitsev"
                                },
                                {
                                    "authorId": "2253541812",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.54443359375
                    },
                    {
                        "id": "(Lu, 2025)",
                        "snippets": [
                            "Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."
                        ],
                        "paper": {
                            "corpus_id": 275336968,
                            "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws",
                            "authors": [
                                {
                                    "authorId": "2338865687",
                                    "name": "Chien-Ping Lu"
                                }
                            ],
                            "year": 2025,
                            "venue": "arXiv.org",
                            "n_citations": 1
                        },
                        "score": 0.705078125
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            },
            {
                "title": "Trade-offs Between Model Size and Inference Efficiency",
                "tldr": "Research reveals that optimal model design must balance training efficiency with inference costs, often favoring smaller models when deployment is considered. This challenges the prevailing trend toward ever-larger models and highlights the importance of considering the full lifecycle computational costs. (1 source)",
                "text": "\nThe relationship between model size and inference efficiency presents critical trade-offs that practitioners must navigate. While the scaling laws literature has predominantly focused on optimizing training compute, recent work has begun examining how considering inference costs changes the optimal allocation of computational resources.\n\nA significant insight from this emerging research is that accounting for inference costs naturally shifts optimal design choices toward smaller model architectures. Sardana and Frankle's work demonstrates that when deployment compute is factored into the equation, the compute-optimal regime changes substantially <Paper corpusId=\"270764838\" paperTitle=\"(Porian et al., 2024)\" isShortName></Paper>. This finding challenges the common practice of building increasingly larger models and suggests that medium-scale models may actually represent better overall efficiency when their full lifecycle computational costs are considered.\n\nThis perspective is particularly relevant as models transition from research environments to production deployments where inference costs accumulate over millions or billions of queries. In these settings, slightly higher training costs may be justified if they result in models that are significantly more efficient during deployment. The field is gradually recognizing that optimizing solely for training efficiency - as traditional scaling laws suggest - may lead to suboptimal designs when inference efficiency is a priority <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThese findings have important implications for model development strategies, especially as AI systems become more widely deployed in resource-constrained environments. Rather than simply scaling up model size according to training-oriented scaling laws, developers may achieve better overall efficiency by carefully balancing the trade-offs between model size, training compute, and inference requirements.",
                "citations": [
                    {
                        "id": "(Porian et al., 2024)",
                        "snippets": [
                            "Recent work also studies compute-bounded scaling laws beyond the compute-optimal regime. Informed by the increasingly common practice of training medium-scale models beyond compute optimality [e.g., 55, 56, 29], Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."
                        ],
                        "paper": {
                            "corpus_id": 270764838,
                            "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models",
                            "authors": [
                                {
                                    "authorId": "2308470091",
                                    "name": "Tomer Porian"
                                },
                                {
                                    "authorId": "52193502",
                                    "name": "Mitchell Wortsman"
                                },
                                {
                                    "authorId": "2191688",
                                    "name": "J. Jitsev"
                                },
                                {
                                    "authorId": "2253541812",
                                    "name": "Ludwig Schmidt"
                                },
                                {
                                    "authorId": "2444742",
                                    "name": "Y. Carmon"
                                }
                            ],
                            "year": 2024,
                            "venue": "Neural Information Processing Systems",
                            "n_citations": 26
                        },
                        "score": 0.54443359375
                    }
                ],
                "format": "synthesis",
                "table": null,
                "model": "claude-3-7-sonnet-20250219"
            }
        ],
        "cost": 0.028377
    }
}