{"better_query": "What are the most effective methods for improving language model calibration across different knowledge domains, and how do techniques such as multicalibration and few-shot recalibration address domain-specific miscalibration?", "better_answer": {"sections": [{"title": "Introduction to Language Model Calibration", "tldr": "Language model calibration refers to how well a model's confidence in its predictions aligns with its actual accuracy. Well-calibrated models are crucial for reliable decision-making in high-stakes domains like healthcare and legal applications. (LLM Memory)", "text": "\nLanguage model calibration measures the alignment between a model's confidence in its predictions and the actual probability of those predictions being correct. A perfectly calibrated model should be 90% accurate when it expresses 90% confidence in its answers. However, large language models (LLMs) often suffer from overconfidence, where they express high certainty even when incorrect, or underconfidence, where they express low certainty despite being correct. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nCalibration is particularly important for practical applications of language models where reliability and trustworthiness are critical. In high-stakes domains such as healthcare diagnostics, legal reasoning, or financial advising, poor calibration can lead to dangerous outcomes when models express unwarranted confidence in incorrect information. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe calibration problem becomes more complex across different knowledge domains. Models may be well-calibrated on common knowledge but poorly calibrated on specialized domains, leading to domain-specific calibration gaps. This occurs because pre-training data often contains imbalanced representation of different domains, resulting in varying levels of model confidence that don't accurately reflect its true capabilities in those areas. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has identified several factors affecting calibration quality, including model size, training methods, and evaluation contexts. Larger models tend to be better calibrated than smaller ones, though they may still exhibit significant calibration errors when venturing into specialized domains or novel tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Common Calibration Methods for Language Models", "tldr": "Common calibration methods for language models include post-hoc techniques like temperature scaling, training modifications like label smoothing, and feature preservation approaches that maintain pre-trained model characteristics during fine-tuning. (9 sources)", "text": "\nSeveral established methods have emerged to address calibration challenges in language models. One of the most widely used post-hoc calibration techniques is **temperature scaling**, which applies a single learnable parameter to scale model logits before the softmax function, effectively softening overconfident predictions <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. Temperature scaling has consistently proven effective for in-domain calibration and is recommended as a reliable post-hoc recalibration technique <Paper corpusId=\"247613322\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper>.\n\nAnother effective approach is **label smoothing**, which penalizes low entropy distributions by assigning a fixed probability to the true label and distributing remaining probability mass across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Label smoothing prevents networks from becoming overconfident and has been shown to improve both generalization and model calibration <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>. This technique is particularly beneficial in out-of-domain settings where models tend to be poorly calibrated <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nMore recent research has identified that **preserving pre-trained features** can significantly improve calibration of fine-tuned language models. Several methods that maintain the connection to pre-trained weights have demonstrated superior calibration performance, including:\n\n1. **Parameter-efficient tuning** methods that modify only a small subset of model parameters while keeping most pre-trained weights frozen <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>\n\n2. **Pre-trained weight decay** that regularizes fine-tuned weights to stay closer to their pre-trained values\n\n3. **Mixout**, which stochastically mixes the parameters of the pre-trained and fine-tuned models, regularizing learning to minimize deviation from the pre-trained model <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>\n\nThese feature preservation approaches are particularly effective for out-of-domain calibration scenarios, where maintaining connection to the pre-trained model helps prevent overconfidence <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nFor resource-constrained scenarios, **few-shot learning** has emerged as a surprisingly effective calibration technique. By fine-tuning on a small number of examples in a target domain or language, models can significantly improve their calibration in that specific context <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper> <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.\n\nModel selection also impacts calibration quality, with larger pre-trained language models generally exhibiting better calibration than smaller ones. Among model architectures, ELECTRA has been identified as particularly effective for well-calibrated predictions <Paper corpusId=\"247613322\" paperTitle=\"(Xiao et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Xiao et al., 2022)", "paper": {"corpus_id": 247613322, "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yuxin Xiao", "authorId": "120727449"}, {"name": "Paul Pu Liang", "authorId": "28130078"}, {"name": "Umang Bhatt", "authorId": "32326200"}, {"name": "W. Neiswanger", "authorId": "2934259"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}], "n_citations": 96}, "snippets": ["In this paper, we contribute a comprehensive analysis on how to reduce calibration error in a PLMbased pipeline. We establish four key considerations behind the pipeline and compare a broad range of prevalent options for each consideration", "Based on our large-scale systematic analysis, we recommend the following:\n\n1. Use ELECTRA for PLM encoding.\n\n2. Use larger PLMs if possible. 3. Use Temp Scaling for post hoc recalibration. 4. Use Focal Loss during the fine-tuning stage."], "score": 0.9482421875}, {"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 253098773, "title": "On the Calibration of Massively Multilingual Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning."], "score": 0.93017578125}, {"id": "(Muller et al., 2019)", "paper": {"corpus_id": 174802983, "title": "When Does Label Smoothing Help?", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael M\u00fcller", "authorId": "2114054259"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 1954}, "snippets": ["The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."], "score": 0.0}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}, {"id": "(Houlsby et al., 2019)", "paper": {"corpus_id": 59599816, "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "N. Houlsby", "authorId": "2815290"}, {"name": "A. Giurgiu", "authorId": "1911881"}, {"name": "Stanislaw Jastrzebski", "authorId": "40569328"}, {"name": "Bruna Morrone", "authorId": "68973833"}, {"name": "Quentin de Laroussilhe", "authorId": "51985388"}, {"name": "Andrea Gesmundo", "authorId": "2813347"}, {"name": "Mona Attariyan", "authorId": "2809991"}, {"name": "S. Gelly", "authorId": "1802148"}], "n_citations": 4518}, "snippets": ["Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."], "score": 0.0}, {"id": "(Lee et al., 2019)", "paper": {"corpus_id": 202750126, "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Cheolhyoung Lee", "authorId": "81275395"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Wanmo Kang", "authorId": "1742677"}], "n_citations": 209}, "snippets": ["In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."], "score": 0.0}, {"id": "(He et al., 2023)", "paper": {"corpus_id": 258967945, "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guande He", "authorId": "2218509878"}, {"name": "Jianfei Chen", "authorId": "2276707"}, {"name": "Jun Zhu", "authorId": "2155220672"}], "n_citations": 22}, "snippets": ["Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."], "score": 0.96484375}, {"id": "(Lauscher et al., 2020)", "paper": {"corpus_id": 226262344, "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Anne Lauscher", "authorId": "29891652"}, {"name": "Vinit Ravishankar", "authorId": "24881798"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "Goran Glavas", "authorId": "2472657"}], "n_citations": 315}, "snippets": ["Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."], "score": 0.0}], "table": null}, {"title": "Regularization and Feature Preservation Approaches", "tldr": "Regularization approaches can significantly improve language model calibration by preventing overconfidence and maintaining connections to pre-trained features. Techniques like on-manifold and off-manifold regularization, parameter-efficient tuning, and mixout have proven particularly effective for addressing domain-specific calibration challenges. (4 sources)", "text": "\nBuilding on traditional calibration methods, regularization approaches have emerged as powerful techniques for addressing calibration challenges in language models. These methods work by constraining the model during fine-tuning to prevent overconfident predictions and preserve useful features from pre-training.\n\nData augmentation-based regularization has shown promising results for improving calibration. Kong et al. proposed a dual regularization approach that uses pseudo samples both on and off the data manifold. Their method employs on-manifold regularization by interpolating training data and labels along directions learned from hidden feature space, introducing a smoothness constraint that improves calibration for in-distribution data. Complementing this, they use off-manifold regularization by adding perturbations that point outward from the data manifold and penalizing overconfident predictions on these samples <Paper corpusId=\"222327644\" paperTitle=\"(Kong et al., 2020)\" isShortName></Paper>.\n\nFeature preservation approaches have proven particularly effective for maintaining calibration during fine-tuning. He et al. observed that preserving pre-trained features helps calibrate fine-tuned language models, especially in out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>. Their research validated that several methods designed to maintain connections to pre-trained models significantly improve calibration:\n\n1. **Parameter-efficient tuning** methods, which modify only a small subset of model parameters while keeping most pre-trained weights frozen, have demonstrated strong calibration performance <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>.\n\n2. **Pre-trained weight decay** regularizes fine-tuned weights to stay closer to their initial pre-trained values, helping prevent overconfidence <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\n3. **Mixout** stochastically mixes parameters from pre-trained and fine-tuned models during training, effectively regularizing learning to minimize deviation from the pre-trained model <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.\n\nThe effectiveness of these feature preservation approaches stems from the observation that pre-trained language models are often well-calibrated on their masked language modeling (MLM) objectives, but lose this calibration when fine-tuned without proper regularization. By maintaining consistency between pre-trained and fine-tuned models, these techniques can achieve lower expected calibration error while maintaining competitive accuracy on natural language understanding tasks <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nThese regularization and feature preservation approaches are particularly valuable for addressing domain-specific calibration challenges, as they help prevent the model from becoming overconfident when facing examples that differ from its training distribution.", "citations": [{"id": "(Kong et al., 2020)", "paper": {"corpus_id": 222327644, "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Lingkai Kong", "authorId": "2865034"}, {"name": "Haoming Jiang", "authorId": "5795999"}, {"name": "Yuchen Zhuang", "authorId": "8103389"}, {"name": "Jie Lyu", "authorId": "2053220976"}, {"name": "T. Zhao", "authorId": "36345161"}, {"name": "Chao Zhang", "authorId": "145657504"}], "n_citations": 26}, "snippets": ["We propose a regularization approach to addressing miscalibration for fine-tuning pre-trained language models from a data augmentation perspective. We propose two new regularizers using pseudo samples both on and off the data manifold to mitigate data scarcity and prevent over-confident predictions. Specifically, our method imposes two types of regularization for better calibration during fine-tuning: (1) On-manifold regularization: We first generate on-manifold samples by interpolating the training data and their corresponding labels along the direction learned from hidden feature space; training over such augmented on-manifold data introduces a smoothness constraint within the data manifold to improve the model calibration for in-distribution data.\n\n(2) Off-manifold regularization: We generate off-manifold samples by adding relatively large perturbations along the directions that point outward the data manifold; we penalize the negative entropy of the output distribution for such off-manifold samples"], "score": 0.92431640625}, {"id": "(He et al., 2023)", "paper": {"corpus_id": 258967945, "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guande He", "authorId": "2218509878"}, {"name": "Jianfei Chen", "authorId": "2276707"}, {"name": "Jun Zhu", "authorId": "2155220672"}], "n_citations": 22}, "snippets": ["Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."], "score": 0.96484375}, {"id": "(Houlsby et al., 2019)", "paper": {"corpus_id": 59599816, "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "N. Houlsby", "authorId": "2815290"}, {"name": "A. Giurgiu", "authorId": "1911881"}, {"name": "Stanislaw Jastrzebski", "authorId": "40569328"}, {"name": "Bruna Morrone", "authorId": "68973833"}, {"name": "Quentin de Laroussilhe", "authorId": "51985388"}, {"name": "Andrea Gesmundo", "authorId": "2813347"}, {"name": "Mona Attariyan", "authorId": "2809991"}, {"name": "S. Gelly", "authorId": "1802148"}], "n_citations": 4518}, "snippets": ["Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."], "score": 0.0}, {"id": "(Lee et al., 2019)", "paper": {"corpus_id": 202750126, "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Cheolhyoung Lee", "authorId": "81275395"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Wanmo Kang", "authorId": "1742677"}], "n_citations": 209}, "snippets": ["In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."], "score": 0.0}], "table": null}, {"title": "Domain-Specific Calibration Challenges", "tldr": "Language models often appear well-calibrated over broad distributions while hiding significant miscalibration within specific knowledge domains, creating critical reliability issues. These domain-specific calibration problems manifest as systematic overconfidence in some areas (like mathematics) and underconfidence in others (like history), requiring targeted solutions for each knowledge domain. (2 sources)", "text": "\nWhile language models may demonstrate good overall calibration metrics when evaluated broadly, recent research has revealed that this apparent calibration often masks serious miscalibration within specific knowledge domains. This phenomenon creates significant challenges for deploying language models in specialized contexts where reliability is crucial.\n\nLi et al. discovered that seemingly well-calibrated models frequently exhibit \"significant miscalibration within narrower slices\" of knowledge, where systematic overconfidence in certain domains (like mathematics) can balance out systematic underconfidence in others (like history), creating an illusion of good calibration when measured in aggregate <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This hidden domain-specific miscalibration poses substantial risks when models are deployed in specialized contexts, as the model's confidence scores may not accurately reflect its true likelihood of being correct in that particular knowledge area.\n\nThe challenge becomes more complex because different knowledge domains may require distinct calibration approaches. Traditional calibration methods that work well for general knowledge may fail to address the unique characteristics of specialized domains like medicine, law, or technical subjects. These domains often have their own linguistic patterns, specialized terminology, and reasoning structures that affect how language models perform and express confidence.\n\nDomain-specific calibration challenges are further complicated by the uneven representation of different knowledge areas in pre-training data, resulting in varying degrees of model expertise and confidence across domains. This imbalance can lead to systematic biases in calibration, where models consistently over- or under-estimate their capabilities in specific areas.\n\nTo address these fine-grained calibration issues, researchers have begun developing more targeted approaches. Detommaso et al. proposed \"multicalibration\" as a solution that aims to achieve calibration \"not just marginally, but simultaneously across various intersecting groupings of the data\" <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>. Their approach creates groupings for prompt/completion pairs through embedding space clustering and \"self-annotation,\" where the language model is queried about various aspects of the prompt to identify groups that correlate with correctness probability. This multicalibration approach has shown \"substantial improvements in fine-grained measures of both calibration and accuracy\" compared to existing methods that don't account for domain-specific characteristics <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)", "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct)."], "score": 0.9775390625}, {"id": "(Detommaso et al., 2024)", "paper": {"corpus_id": 269004786, "title": "Multicalibration for Confidence Scoring in LLMs", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gianluca Detommaso", "authorId": "2295667267"}, {"name": "Martin Bertran", "authorId": "2295665717"}, {"name": "Riccardo Fogliato", "authorId": "2295664744"}, {"name": "Aaron Roth", "authorId": "2295665299"}], "n_citations": 19}, "snippets": ["This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."], "score": 0.97119140625}], "table": null}, {"title": "Few-Shot Recalibration for Domain Adaptation", "tldr": "Few-shot recalibration techniques effectively address domain-specific calibration challenges by fine-tuning models with small amounts of domain-specific data. This approach is particularly valuable for adapting language models to specialized knowledge domains or resource-limited languages, providing significant calibration improvements with minimal additional training. (5 sources)", "text": "\nFew-shot recalibration has emerged as a powerful approach for adapting language models to specific domains or languages while improving their calibration. This method leverages a small number of examples from the target domain to adjust model confidence, addressing the domain-specific calibration challenges identified in previous sections.\n\nTraditional calibration techniques like temperature scaling, while effective for in-domain calibration, often struggle with domain adaptation challenges. Temperature scaling can be applied using either source language development data or target language development data (Self-TS), with the latter providing better calibration for cross-lingual transfer <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. However, these methods may not fully address the nuanced calibration issues that arise when transferring to different knowledge domains.\n\nFew-shot learning presents a surprisingly effective solution for improving calibration across domains. By fine-tuning on just a small number of examples from the target domain or language, models can significantly reduce domain shift and improve both performance and calibration <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This approach has proven particularly valuable for cross-lingual transfer, where fine-tuning on a few target-language instances consistently improves results across various tasks <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper>.\n\nRecent research has further refined this approach with \"slice-specific few-shot recalibration\" - a framework that addresses the fine-grained calibration issues hidden within broader distributions. This method uses a small number of unlabeled examples from a specific knowledge slice (e.g., mathematics, history) to recalibrate the language model specifically for that domain. For each slice, a separate recalibration model is trained to map the language model's confidence scores to domain-specific estimates of precision <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThis domain-specific recalibration approach is particularly valuable because language models often exhibit systemic overconfidence in some domains while showing underconfidence in others. These opposing miscalibrations can balance each other out in aggregate evaluations, creating an illusion of good overall calibration while hiding significant domain-specific problems <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of few-shot recalibration techniques is further enhanced when combined with regularization methods like label smoothing, which helps prevent overconfidence by penalizing low entropy distributions. Label smoothing has demonstrated competitive performance with temperature scaling, especially in out-of-domain settings where calibration challenges are more pronounced <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nFor practical applications, few-shot recalibration offers an efficient solution that balances calibration quality with computational efficiency. By requiring only a small number of examples from the target domain, this approach enables effective domain adaptation even in resource-constrained scenarios where extensive labeled data may not be available.", "citations": [{"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 253098773, "title": "On the Calibration of Massively Multilingual Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning."], "score": 0.93017578125}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Lauscher et al., 2020)", "paper": {"corpus_id": 226262344, "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Anne Lauscher", "authorId": "29891652"}, {"name": "Vinit Ravishankar", "authorId": "24881798"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "Goran Glavas", "authorId": "2472657"}], "n_citations": 315}, "snippets": ["Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)", "To recalibrate a model in these finer-grained settings, we propose slice-specific few-shot recalibration-a new framework that uses only a small number of unlabeled examples from a given slice to recalibrate the LM for that slice. Specifically, for a given LM, we train a separate recalibration model that takes a few unlabeled examples as input and outputs a curve that maps the LM's confidence scores to slice-specific estimates of precision (i.e. the percentage of examples above the given confidence score that will be correct)."], "score": 0.9775390625}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}], "table": null}, {"title": "Multicalibration for Reliable Confidence Scores", "tldr": "Multicalibration addresses fine-grained calibration challenges by ensuring language models are well-calibrated across multiple intersecting data groupings simultaneously, not just in aggregate. This approach creates domain-specific groupings through embedding space clustering and self-annotation techniques, providing more reliable confidence scores for specialized knowledge domains. (1 source)", "text": "\nTraditional calibration methods often focus on improving overall calibration across a broad distribution, which can mask significant miscalibration within specific knowledge domains or data slices. To address this limitation, multicalibration has emerged as a promising approach that ensures calibration across multiple intersecting groupings of data simultaneously, rather than just marginally across the entire distribution.\n\nDetommaso et al. propose a multicalibration framework specifically designed to yield interpretable and reliable confidence scores for large language model outputs. Their approach focuses on achieving calibration \"not just marginally, but simultaneously across various intersecting groupings of the data,\" ensuring that models are well-calibrated even within specific knowledge domains. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nA key innovation in this multicalibration approach is the method of forming meaningful groupings for prompt/completion pairs that correlate with correctness probability. The researchers employ two primary techniques to create these groupings:\n\n1. **Clustering within embedding space**: This technique identifies natural groupings of related prompts and completions based on their vector representations in the model's embedding space.\n\n2. **Self-annotation**: This novel approach queries the language model itself with yes-or-no questions about the prompt to identify characteristics that might correlate with correctness probability. For example, asking the model whether a prompt requires mathematical reasoning or historical knowledge helps create domain-specific groupings for targeted calibration. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nTo prevent overfitting, which is a common challenge in multicalibration methods, the researchers developed novel algorithmic variants that improve performance stability. These refined algorithms maintain the benefits of multicalibration while reducing the risk of overadjusting to specific data groupings. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nWhen evaluated across various question-answering datasets and different large language models, multicalibration consistently demonstrates \"substantial improvements in fine-grained measures of both calibration and accuracy\" compared to traditional calibration methods. These improvements are particularly significant for specialized knowledge domains where conventional calibration techniques often struggle to address domain-specific confidence biases. <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>\n\nThe multicalibration approach complements other calibration techniques discussed earlier, such as few-shot recalibration and regularization methods. While few-shot approaches adapt models to specific domains using limited examples, and regularization preserves beneficial pre-trained features, multicalibration ensures reliable confidence scores across multiple intersecting data groupings simultaneously. This comprehensive approach to calibration is particularly valuable for high-stakes applications where reliability within specific knowledge domains is critical.", "citations": [{"id": "(Detommaso et al., 2024)", "paper": {"corpus_id": 269004786, "title": "Multicalibration for Confidence Scoring in LLMs", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gianluca Detommaso", "authorId": "2295667267"}, {"name": "Martin Bertran", "authorId": "2295665717"}, {"name": "Riccardo Fogliato", "authorId": "2295664744"}, {"name": "Aaron Roth", "authorId": "2295665299"}], "n_citations": 19}, "snippets": ["This paper proposes the use of \"multicalibration\" to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."], "score": 0.97119140625}], "table": null}, {"title": "Prompt-Based Calibration Techniques", "tldr": "Prompt-based calibration techniques address language model confidence biases without requiring labeled data by analyzing model responses to \"content-free\" inputs. These methods, like contextual calibration, can significantly improve model accuracy and reduce prediction variance by counteracting inherent model biases toward certain answers. (2 sources)", "text": "\nPrompt-based calibration techniques offer effective approaches for improving language model calibration without requiring extensive labeled data, making them particularly valuable for zero-shot and few-shot learning scenarios. Unlike traditional calibration methods that depend on labeled examples, these techniques leverage the model's own responses to specially crafted prompts to identify and address confidence biases.\n\nA prominent approach in this category is contextual calibration, which addresses the inherent biases language models have toward certain predictions. Current language models often exhibit poor calibration with predicted probabilities subject to various forms of bias, such as favoring tokens that appear more frequently in pre-training data or tokens positioned near the end of a prompt <Paper corpusId=\"248524894\" paperTitle=\"(Smith et al., 2022)\" isShortName></Paper>. These biases can significantly impact the reliability of model predictions, especially when using prompts as labeling functions or when thresholding predictions based on confidence scores.\n\nContextual calibration works by estimating scaling weights from the model's predicted token probabilities when queried with \"content-free\" or null input instances. These null inputs - typically tokens like \"N/A\", \"\", or \"<|endoftext|>\" - serve as baselines that reveal the model's inherent biases without any meaningful content to influence predictions. By analyzing how the model responds to these null inputs, researchers can estimate calibration parameters that transform the model's raw predictions to be more balanced across possible answers <Paper corpusId=\"248524894\" paperTitle=\"(Smith et al., 2022)\" isShortName></Paper> <Paper corpusId=\"231979430\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>.\n\nThis approach is particularly valuable for addressing the instability observed in few-shot learning scenarios, where model accuracy can vary dramatically based on superficial factors such as prompt format, training example selection, or even the order of examples within a prompt. Research has shown that by estimating a model's bias toward each answer and fitting calibration parameters to create uniform predictions for content-free inputs, contextual calibration can substantially improve accuracy (by up to 30.0% absolute) and reduce variance across different prompt configurations <Paper corpusId=\"231979430\" paperTitle=\"(Zhao et al., 2021)\" isShortName></Paper>.\n\nThe effectiveness of prompt-based calibration methods stems from their ability to identify and counteract specific biases without requiring labeled data for the target task. This makes them especially suitable for zero-shot and few-shot settings where labeled examples are scarce or unavailable. By addressing the calibration challenges inherent in prompt-based approaches, these techniques enhance both the reliability and performance of language models across diverse knowledge domains.", "citations": [{"id": "(Smith et al., 2022)", "paper": {"corpus_id": 248524894, "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision", "year": 2022, "venue": "ACM / IMS Journal of Data Science", "authors": [{"name": "Ryan Smith", "authorId": "2165361707"}, {"name": "Jason Alan Fries", "authorId": "31592365"}, {"name": "Braden Hancock", "authorId": "34302368"}, {"name": "Stephen H. Bach", "authorId": "2870504"}], "n_citations": 56}, "snippets": ["We find that it is useful to improve the calibration of prompted labeling functions. Calibration is a measurement of how strongly a model's predicted probabilities correlate with observed accuracy, i.e., a predicted probability of p should be correct p \u2022 100% of the time. Current language models are not well-calibrated, with predicted probabilities subject to several forms of biasing, e.g., favoring tokens observed more during pretraining or tokens that appear near the end of a prompt [26; 68]. Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions. When using prompts as labelers, we may also want to threshold predictions to select only the most confident answers. Popular recalibration methods such as Platt and vector scaling [38; 25] require labeled data to learn a transformation of the model's predicted probabilities, creating challenges to directly applying these methods in zero-shot settings. Instead, we use contextual calibration (Zhao et al., 2021), where scaling weights are estimated from the predicted token probabilities of a prompt queried using \"content-free\" or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification. We use the tokens { N/A, , [MASK], <|endoftext|> } as our null inputs, using the average predicted probabilities per token to estimate our scaling weights for each prompt. The resulting transformation is then applied to each prompted labeling function's predictions."], "score": 0.91064453125}, {"id": "(Zhao et al., 2021)", "paper": {"corpus_id": 231979430, "title": "Calibrate Before Use: Improving Few-Shot Performance of Language Models", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tony Zhao", "authorId": "145914976"}, {"name": "Eric Wallace", "authorId": "145217343"}, {"name": "Shi Feng", "authorId": "2113511266"}, {"name": "D. Klein", "authorId": "38666915"}, {"name": "Sameer Singh", "authorId": "34650964"}], "n_citations": 1428}, "snippets": ["GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as \"N/A\". We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt."], "score": 0.0}], "table": null}], "cost": 0.315171}}
