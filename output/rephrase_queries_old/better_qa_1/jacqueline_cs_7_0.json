{"better_query": "How does the attention masking mechanism differ between causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder language models?", "better_answer": {"sections": [{"title": "Introduction: Fundamentals of Attention Masking", "tldr": "Attention masking is a key mechanism that controls which input tokens can be attended to when generating each output token in transformer-based language models. The three primary masking patterns\u2014fully-visible, causal, and causal with prefix\u2014fundamentally shape how different language model architectures process and generate text. (3 sources)", "text": "\nAttention masking serves as a critical component in transformer-based language models, determining the information flow within the self-attention mechanism. The self-attention operation takes a sequence as input and produces a new sequence of the same length, where each output element is computed as a weighted average of the input elements. The weights are determined by the self-attention mechanism based on the relationships between the elements, and the attention mask constrains which input elements can be attended to when generating each output element <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nThere are three fundamental attention masking patterns that distinguish different language model architectures. First, the fully-visible mask allows the self-attention mechanism to attend to the entire input sequence when generating any output element. This pattern is visualized as a fully populated matrix and is typically used when processing a prefix or context that will later inform predictions <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. Second, the causal mask implements unidirectional attention by preventing an output element from attending to any future input elements. Formally, this is represented as a lower triangular binary matrix where m_ij = 0 for all i < j, ensuring that tokens can only attend to themselves and previous tokens in the sequence <Paper corpusId=\"264805730\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. Finally, the causal mask with prefix combines these approaches, applying fully-visible masking to a portion of the input sequence while maintaining causal masking for the remainder <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nThese attention masking patterns are visually represented as matrices where dark cells indicate allowed attention connections and light cells indicate restricted connections. The pattern choice significantly impacts model behavior\u2014fully-visible masking allows bidirectional context awareness, causal masking enforces left-to-right generation by preventing \"seeing into the future,\" and the hybrid approach with a prefix enables special treatment of contextual information <Paper corpusId=\"268041362\" paperTitle=\"(Yi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern", "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."], "score": 0.91650390625}, {"id": "(Gong et al., 2023)", "paper": {"corpus_id": 264805730, "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhuocheng Gong", "authorId": "2165228008"}, {"name": "Jiahao Liu", "authorId": "2261393008"}, {"name": "Qifan Wang", "authorId": "2261393439"}, {"name": "Jingang Wang", "authorId": "2258759716"}, {"name": "Xunliang Cai", "authorId": "2259620212"}, {"name": "Dongyan Zhao", "authorId": "2253232138"}, {"name": "Rui Yan", "authorId": "2249533146"}], "n_citations": 2}, "snippets": ["Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j."], "score": 0.6669921875}, {"id": "(Yi et al., 2024)", "paper": {"corpus_id": 268041362, "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zihao Yi", "authorId": "2287925430"}, {"name": "Jiarui Ouyang", "authorId": "2287922728"}, {"name": "Yuwen Liu", "authorId": "2288039936"}, {"name": "Tianhao Liao", "authorId": "2287923878"}, {"name": "Zhe Xu", "authorId": "2288033664"}, {"name": "Ying Shen", "authorId": "2288065597"}], "n_citations": 72}, "snippets": ["Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)"], "score": 0.7626953125}], "table": null}, {"title": "Causal Decoder-Only Models", "tldr": "Causal decoder-only models employ a unidirectional attention mask that restricts each token to attend only to itself and previous tokens in the sequence, enforcing strict left-to-right generation. This masking pattern, implemented as a lower triangular matrix, is fundamental to popular models like GPT and LLaMA, ensuring that information flows only from past to future tokens. (11 sources)", "text": "\nCausal decoder-only models represent one of the most prevalent architectures in modern large language models, including widely used systems like GPT and LLaMA <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. The defining characteristic of these models is their attention masking mechanism, which implements a strict unidirectional information flow. This causal attention mask ensures that each token in the sequence can only attend to preceding tokens and itself, but never to future tokens <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>.\n\nFormally, the causal attention mask is implemented as a lower triangular binary matrix M \u2208 {0, 1}^(n\u00d7n), where n represents the total sequence length. Each element m_ij in this matrix indicates whether the j-th token can attend to the i-th token, with m_ij = 0 for all i < j <Paper corpusId=\"264805730\" paperTitle=\"(Gong et al., 2023)\" isShortName></Paper>. In practical implementations, this is often achieved by setting the lower triangular part of the attention matrix to 0 and all other elements to \u2212\u221e, effectively preventing any attention to future tokens <Paper corpusId=\"270214176\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nThe causal masking pattern has important implications for how these models process and generate text. Unlike encoder-decoder or prefix decoder architectures, causal decoder-only models treat all tokens equivalently\u2014there is no separate mechanism for processing input and output sequences differently <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This uniform processing approach means that conditioning is based solely on past tokens, with the model predicting the next token in the sequence autoregressively <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>.\n\nA key consequence of this architecture is that in causal decoder-only models, only the last token in a sequence contains information from the entire input, as each token can only attend to what came before it <Paper corpusId=\"271903802\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This property shapes how these models must be used in practice, particularly for tasks requiring bidirectional context understanding.\n\nThe causal attention mechanism provides advantages for autoregressive text generation by maintaining the integrity of the prediction task\u2014each token must be predicted without \"peeking\" at future information <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. Additionally, there are technical benefits to this approach: the attention matrix in causal attention masks is guaranteed to be full-rank, potentially providing greater expressiveness compared to some bidirectional attention mechanisms that may suffer from rank collapse issues <Paper corpusId=\"273233776\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"232134936\" paperTitle=\"(Dong et al., 2021)\" isShortName></Paper>.\n\nFor specialized applications, variations on the standard causal attention mask have been developed. For instance, some approaches implement cascaded attention masking schemes that preserve causal attention within blocks of tokens while allowing controlled attention patterns between blocks <Paper corpusId=\"272593363\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>. These modifications demonstrate how the fundamental causal masking pattern can be adapted for particular use cases while maintaining its essential unidirectional character.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 272423598, "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingze Gao", "authorId": "2319805585"}, {"name": "Jingyu Liu", "authorId": "2302790279"}, {"name": "Mingda Li", "authorId": "2302785092"}, {"name": "Jiangtao Xie", "authorId": "2319964588"}, {"name": "Qingbin Liu", "authorId": "2258682951"}, {"name": "Bo Zhao", "authorId": "2304448412"}, {"name": "Xi Chen", "authorId": "2302990371"}, {"name": "Hui Xiong", "authorId": "2319814814"}], "n_citations": 2}, "snippets": ["In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."], "score": 0.70947265625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.654296875}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.6611328125}, {"id": "(Gong et al., 2023)", "paper": {"corpus_id": 264805730, "title": "Improving Input-label Mapping with Demonstration Replay for In-context Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhuocheng Gong", "authorId": "2165228008"}, {"name": "Jiahao Liu", "authorId": "2261393008"}, {"name": "Qifan Wang", "authorId": "2261393439"}, {"name": "Jingang Wang", "authorId": "2258759716"}, {"name": "Xunliang Cai", "authorId": "2259620212"}, {"name": "Dongyan Zhao", "authorId": "2253232138"}, {"name": "Rui Yan", "authorId": "2249533146"}], "n_citations": 2}, "snippets": ["Causal Attention Mask Practically, a causal attention mask is used to implement causal attention, which guarantees unidirectionality by masking all right-to-left attention connections and only allowing right-to-left connections. Formally, the attention mask is a binary-valued matrix M \u2208 {0, 1} n\u00d7n , where n is the total sequence length. \n\nThe element m ij in M indicates whether the j-th token in the sequence can attend to the i-th token, with a value of 1 for yes and 0 for no. Therefore, the causal attention mask matrix is a lower triangular matrix where m ij = 0, \u2200i < j."], "score": 0.6669921875}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 270214176, "title": "A Survey on Large Language Models for Code Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Juyong Jiang", "authorId": "2294682530"}, {"name": "Fan Wang", "authorId": "2304542351"}, {"name": "Jiasi Shen", "authorId": "2305041631"}, {"name": "Sungju Kim", "authorId": "2304525068"}, {"name": "Sunghun Kim", "authorId": "2257349580"}], "n_citations": 197}, "snippets": ["The conditional probability   (  |x < )) is modeled by adding a causal attention mask to the multi-head self-attention matrix of each Transformer block.To be specific, causal attention masking is implemented by setting the lower triangular part of the matrix to 0 and the remaining elements to \u2212\u221e, ensuring that each token   attends only to its predecessors and itself.On the contrary, in encoder-decoder LLMs, a pivot token   is randomly selected in a sequence of tokens and then regarding the context before it as the source sequence x  = { 1 , . . .,   } of the encoder and the sequence after it as the target output x  = { +1 , . . .,   } of decoder."], "score": 0.53857421875}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."], "score": 0.765625}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."], "score": 0.6494140625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 271903802, "title": "WPN: An Unlearning Method Based on N-pair Contrastive Learning in Language Models", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Guitao Chen", "authorId": "2316443362"}, {"name": "Yunshen Wang", "authorId": "2316551557"}, {"name": "Hongye Sun", "authorId": "2316544717"}, {"name": "Guang Chen", "authorId": "2316516363"}], "n_citations": 1}, "snippets": ["In the decoder-only autoregressive model architecture, due to the use of the causal attention masking mechanism, each token only pays attention to the tokens before it. Therefore, only the last token contains the information of the entire sentence."], "score": 0.79248046875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 273233776, "title": "LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhe Li", "authorId": "2325264641"}, {"name": "Weihao Yuan", "authorId": "2268513823"}, {"name": "Yisheng He", "authorId": "2281417686"}, {"name": "Lingteng Qiu", "authorId": "73555520"}, {"name": "Shenhao Zhu", "authorId": "2325327454"}, {"name": "Xiaodong Gu", "authorId": "2268724718"}, {"name": "Weichao Shen", "authorId": "2323499860"}, {"name": "Yuan Dong", "authorId": "2218391810"}, {"name": "Zilong Dong", "authorId": "2268646063"}, {"name": "Laurence T. Yang", "authorId": "2261796256"}], "n_citations": 10}, "snippets": ["Unlike the bidirectional attention mask in MoMask (Guo et al., 2023), we employ a causal attention mask for autoregressive mask prediction tasks. Currently, transformer-based motion generation models (Guo et al., 2023;(Zhang et al., 2023) commonly utilize bidirectional attention masks, which correspond to encoder-only model architectures. However, during training, the bidirectional attention mask allows the model to simultaneously rely on contextual information, simplifying the mask prediction task and diminishing the model's generative capacity. \n\nIn addition, this bidirectional masking leads to rank collapse. The attention matrix generated by a bidirectional attention mask typically arises from the product of a low-rank decomposed matrix and a softmax function; specifically, it results from multiplying an n \u00d7 d matrix with a d \u00d7 n matrix before applying softmax (where n \u226b d). This form of attention matrix suffers from reduced expressiveness due to low-rank issues (Dong et al., 2021). In contrast, the attention matrix for a causal attention mask is a lower triangular matrix, with its determinant equal to the product of its diagonal elements. Due to the presence of softmax, all diagonal elements must be positive, ensuring that its determinant is also positive. Consequently, the attention matrix of the causal attention mask (decoder-only architecture) is guaranteed to be full-rank, providing greater expressiveness."], "score": 0.78857421875}, {"id": "(Dong et al., 2021)", "paper": {"corpus_id": 232134936, "title": "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Yihe Dong", "authorId": "145595795"}, {"name": "Jean-Baptiste Cordonnier", "authorId": "51440515"}, {"name": "Andreas Loukas", "authorId": "1966031"}], "n_citations": 387}, "snippets": ["Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards\"token uniformity\". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures."], "score": 0.0}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 272593363, "title": "STORE: Streamlining Semantic Tokenization and Generative Recommendation with A Single LLM", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qijiong Liu", "authorId": "150270469"}, {"name": "Jieming Zhu", "authorId": "2290237904"}, {"name": "Lu Fan", "authorId": "2147259441"}, {"name": "Zhou Zhao", "authorId": "2265936086"}, {"name": "Xiao-Ming Wu", "authorId": "2187512110"}], "n_citations": 5}, "snippets": ["Decoder-only large language models typically employ a causal attention mask, ensuring that each token in a sequence can only attend to preceding tokens and itself, but not to future tokens. This conventional approach is unsuitable for our scenario where only the outputs of the token block (and subsequently the placeholder block) are permitted to generate the task output. Therefore, we introduce a cascaded attention masking scheme that includes both inner-block and inter-block masking.\n\nAs illustrated in the diagonal of Figure 4, inner-block masking consistently enforces causal attention to preserve sequential knowledge comprehension. Conversely, inter-block masking can be configured as either full or empty attention: the content block fully attends to the token block, and the placeholder block fully attends to the task block. Attention between other blocks is prohibited and set to empty."], "score": 0.833984375}], "table": null}, {"title": "Non-Causal (Prefix) Decoder-Only Models", "tldr": "Non-causal (prefix) decoder-only models employ a hybrid attention masking approach that allows bidirectional attention over designated prefix tokens while maintaining unidirectional attention for generated tokens. This architecture combines the comprehensive context understanding of encoder models with the autoregressive generation capabilities of causal decoder models. (10 sources)", "text": "\nPrefix decoder-only models represent an architectural variation that modifies the attention masking mechanism used in standard causal decoders <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This hybrid approach enables bidirectional attention over a designated portion of the input sequence (the prefix) while maintaining unidirectional attention for the remainder of the sequence <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. The design was notably explored in the T5 framework as \"PrefixLM,\" offering a middle ground between fully-visible and strictly causal masking patterns <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nThe attention masking pattern in prefix decoder models can be visualized as a matrix with two distinct regions: the prefix region employs fully-visible masking (allowing bidirectional attention), while the remaining sequence uses causal masking <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This configuration enables the model to build richer non-causal representations of the input or conditioning text <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. For example, when translating an English sentence to French, the model would apply fully-visible masking to the prefix (\"translate English to French: I am doing well. Target:\"), followed by causal masking while predicting the target (\"je vais bien\") <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nUnlike causal decoder-only models that use a targets-only paradigm, prefix models employ an input-to-target paradigm <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This distinction is crucial as it allows the model to process the prefix information comprehensively before generating the output sequence. The bidirectional attention over the prefix enables the model to encode contextual information in a manner similar to an encoder, while the causal masking for subsequent tokens preserves the autoregressive nature required for text generation <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of the prefix decoder architecture is that it combines the benefits of both encoder-decoder and causal decoder-only approaches <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. By allowing bidirectional attention for the prefix tokens, these models can develop a more comprehensive understanding of the input context <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>. At the same time, the causal masking for the output portion maintains the model's ability to generate text autoregressively <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This makes prefix decoder models particularly well-suited for tasks that require both deep understanding of input context and controlled text generation.", "citations": [{"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."], "score": 0.765625}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."], "score": 0.6494140625}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271600495, "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingcong Lu", "authorId": "2314473248"}, {"name": "Jiangcai Zhu", "authorId": "2314649002"}, {"name": "Wang Hao", "authorId": "2314113733"}, {"name": "Zheng Li", "authorId": "2314323587"}, {"name": "Shusheng Zhang", "authorId": "2314311430"}, {"name": "Kailai Shao", "authorId": "2314110211"}, {"name": "Chao Chen", "authorId": "2314192630"}, {"name": "Nan Li", "authorId": "2314343132"}, {"name": "Feng Wang", "authorId": "2324104105"}, {"name": "Xin Lu", "authorId": "2324103820"}], "n_citations": 0}, "snippets": ["Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)", "based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself", "Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."], "score": 0.744140625}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern", "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."], "score": 0.91650390625}, {"id": "(Katz et al., 2024)", "paper": {"corpus_id": 274992300, "title": "Segment-Based Attention Masking for GPTs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shahar Katz", "authorId": "121254633"}, {"name": "Liran Ringel", "authorId": "2186740854"}, {"name": "Yaniv Romano", "authorId": "2335566528"}, {"name": "Lior Wolf", "authorId": "2284763723"}], "n_citations": 1}, "snippets": ["Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering", "Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures", "The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder."], "score": 0.79931640625}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence."], "score": 0.97265625}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.6611328125}, {"id": "(Ewer et al., 2024)", "paper": {"corpus_id": 273025546, "title": "ENTP: Encoder-only Next Token Prediction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ethan Ewer", "authorId": "2323781863"}, {"name": "Daewon Chae", "authorId": "2253659910"}, {"name": "Thomas Zeng", "authorId": "2323820473"}, {"name": "Jinkyu Kim", "authorId": "2323851531"}, {"name": "Kangwook Lee", "authorId": "2323790154"}], "n_citations": 4}, "snippets": ["In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants)."], "score": 0.57177734375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.654296875}, {"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."], "score": 0.92333984375}], "table": null}, {"title": "Encoder-Decoder Models", "tldr": "Encoder-decoder models employ distinct attention masking mechanisms in each component: fully-visible (bidirectional) attention in the encoder and a combination of causal self-attention and cross-attention in the decoder. This architecture allows for comprehensive bidirectional understanding of input sequences while maintaining controlled autoregressive generation of output sequences. (13 sources)", "text": "\nThe original Transformer architecture introduced by Vaswani et al. represents the foundational encoder-decoder model design, featuring two separate stacks of Transformer blocks\u2014the encoder and the decoder\u2014each with distinct attention masking patterns <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. In this architecture, attention mechanisms are applied in three distinct places: bidirectional self-attention in the encoder, causal self-attention in the decoder, and cross-attention between encoder and decoder <Paper corpusId=\"253080830\" paperTitle=\"(Gao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>.\n\nThe encoder component employs fully-visible (bidirectional) attention masking, which allows each token to attend to all other tokens in the input sequence <Paper corpusId=\"268201845\" paperTitle=\"(Chin et al., 2024)\" isShortName></Paper>. This bidirectional attention enables the encoder to build comprehensive representations of the input by capturing dependencies in both forward and backward directions <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. The fully-visible mask in the encoder is fundamental to models like BERT that are designed for understanding tasks rather than generation, as it allows for complete contextual awareness <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nIn contrast, the decoder component implements two different types of attention mechanisms. First, it uses masked multi-head self-attention with a causal mask that prevents each position from attending to future positions <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. This causal masking ensures that when predicting the output sequence, each token can only attend to preceding tokens and itself, maintaining the autoregressive nature of text generation <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Second, the decoder incorporates cross-attention layers that allow it to attend to the encoder's output representations <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. This cross-attention mechanism enables each token in the decoder to access information from all tokens in the encoded input sequence, regardless of position <Paper corpusId=\"253080830\" paperTitle=\"(Gao et al., 2022)\" isShortName></Paper>.\n\nThe distinct masking patterns in encoder-decoder models offer specific advantages for various NLP tasks. The bidirectional attention in the encoder allows for rich contextual representations of the input sequence, while the causal masking in the decoder self-attention ensures proper autoregressive generation <Paper corpusId=\"247618909\" paperTitle=\"(Hua et al., 2022)\" isShortName></Paper>. This combination makes encoder-decoder models particularly effective for sequence-to-sequence tasks like machine translation, where understanding the entire source sentence is crucial before generating the target translation <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>.\n\nSome variations of encoder-decoder architectures have been explored to enhance their capabilities. For instance, alternative causal masking strategies have been proposed for specific applications, such as in speech processing, where speech tokens are allowed to attend to all other speech tokens in the sequence while maintaining causal masking for text tokens <Paper corpusId=\"275336136\" paperTitle=\"(Lam et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259501685\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. These modifications demonstrate the flexibility of the encoder-decoder architecture to accommodate different attention patterns for specialized tasks.\n\nWhile encoder-decoder models have been foundational in NLP development, their complexity and computational requirements have led to the rise of decoder-only architectures for many text generation tasks <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. Nevertheless, the encoder-decoder approach remains valuable for tasks that benefit from its clear separation of bidirectional understanding and unidirectional generation components <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.6611328125}, {"id": "(Gao et al., 2022)", "paper": {"corpus_id": 253080830, "title": "Is Encoder-Decoder Redundant for Neural Machine Translation?", "year": 2022, "venue": "AACL", "authors": [{"name": "Yingbo Gao", "authorId": "66122912"}, {"name": "Christian Herold", "authorId": "1474566597"}, {"name": "Zijian Yang", "authorId": "48598969"}, {"name": "H. Ney", "authorId": "145322333"}], "n_citations": 4}, "snippets": ["In the original Transformer (Vaswani et al., 2017) model, the attention mechanism is used in three places, namely, a J \u00d7 J encoder self attention matrix, a I \u00d7 I decoder self attention matrix and a J \u00d7 I encoder-decoder cross attention matrix. As shown in Fig. 1, they correspond to matrices C, B and D respectively. The attention masks in B and D are straightforward. The triangular attention mask in the B matrix needs to be causal by definition, because otherwise target positions may attend to future positions and cheat. The attention mask in D needs to be full, because we want each target position to be able to look at each source position so that there is no information loss. However, the attention mask in C is how some of the previous In our case, we consider both the triangular and full attention mask patterns for C, because both have good intuitions. The triangular mask is closer to the original objective of learning the joint distribution P (f J 1 , e I 1 ), while the full mask enables better information flow because early source positions also have access to future source positions to come up with better hidden representations."], "score": 0.56640625}, {"id": "(Vaswani et al., 2017)", "paper": {"corpus_id": 13756489, "title": "Attention is All you Need", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Niki Parmar", "authorId": "3877127"}, {"name": "Jakob Uszkoreit", "authorId": "39328010"}, {"name": "Llion Jones", "authorId": "145024664"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "I. Polosukhin", "authorId": "3443442"}], "n_citations": 132444}, "snippets": ["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."], "score": 0.0}, {"id": "(Chin et al., 2024)", "paper": {"corpus_id": 268201845, "title": "Learning to Deliver: a Foundation Model for the Montreal Capacitated Vehicle Routing Problem", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Samuel J. K. Chin", "authorId": "2289612023"}, {"name": "Matthias Winkenbach", "authorId": "2289612073"}, {"name": "Akash Srivastava", "authorId": "2289782615"}], "n_citations": 0}, "snippets": ["Second, they pursue an LM objective analogous to what we described in Section 4.1. As this models a conditional probability, a causal mask is used in the attention mechanism, such that a token at any given position can only view previous tokens and not future tokens", "First, they pursue a denoising objective (see, (Devlin et al., 2019) for which the inputs to the model are randomly masked, corrupted, or left unedited. Here, masked means that a placeholder token that is not a word is put at the corresponding position, while corrupted means that a random word is put at that position. Note that the denoising objective relies on an encoder-only architecture, which contains a fully-visible mask for the attention mechanism. \n\nHere, all tokens in the input are connected to each other."], "score": 0.51171875}, {"id": "(Busto-Castineira et al., 2024)", "paper": {"corpus_id": 270832367, "title": "Predictability and Causality in Spanish and English Natural Language Generation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Andrea Busto-Casti\u00f1eira", "authorId": "2222734467"}, {"name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o", "authorId": "2323809078"}, {"name": "Silvia Garc\u00eda-M\u00e9ndez", "authorId": "1405165681"}, {"name": "Francisco de Arriba-P\u00e9rez", "authorId": "2034282614"}], "n_citations": 1}, "snippets": ["While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder.\n\nEven though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]."], "score": 0.505859375}, {"id": "(Katz et al., 2024)", "paper": {"corpus_id": 274992300, "title": "Segment-Based Attention Masking for GPTs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shahar Katz", "authorId": "121254633"}, {"name": "Liran Ringel", "authorId": "2186740854"}, {"name": "Yaniv Romano", "authorId": "2335566528"}, {"name": "Lior Wolf", "authorId": "2284763723"}], "n_citations": 1}, "snippets": ["Encoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering", "Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures", "The most closely related work to our approach, PrefixLM, was explored in the T5 framework (Raffel et al., 2019). PrefixLM operates within a unified decoder-only architecture but enables bidirectional attention over a designated prefix of the input sequence while maintaining causal attention for the remainder."], "score": 0.79931640625}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."], "score": 0.765625}, {"id": "(Hua et al., 2022)", "paper": {"corpus_id": 247618909, "title": "Self-supervision through Random Segments with Autoregressive Coding (RandSAC)", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tianyu Hua", "authorId": "1419971650"}, {"name": "Yonglong Tian", "authorId": "2476765"}, {"name": "Sucheng Ren", "authorId": "1823941979"}, {"name": "Hang Zhao", "authorId": "2146231364"}, {"name": "L. Sigal", "authorId": "144398147"}], "n_citations": 16}, "snippets": ["The decoder of a transformer masks the internal attention matrix with a causal mask and predicts the target sequence X tgt = (x 2 ,", ", x n ) autoregressively", "In the encoder, causal source mask enables a given segment to only attend over preceding segments and the tokens within itself. The decoder, given the position of tokens (i.e., target queries), predicts tokens within each segment conditioned on encoded previous segments (enabled by the memory mask)."], "score": 0.59228515625}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern", "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."], "score": 0.91650390625}, {"id": "(Lam et al., 2025)", "paper": {"corpus_id": 275336136, "title": "Prepending or Cross-Attention for Speech-to-Text? An Empirical Comparison", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Tsz Kin Lam", "authorId": "46211375"}, {"name": "Marco Gaido", "authorId": "1736801422"}, {"name": "Sara Papi", "authorId": "2006601535"}, {"name": "L. Bentivogli", "authorId": "2486762"}, {"name": "Barry Haddow", "authorId": "2322986291"}], "n_citations": 0}, "snippets": ["In standard settings, causal masking is also applied in the DFP models where both previous tokens Y and the input audio representation X are masked. Therefore, the decoder self-attentions implement the above masking strategy on the concatenated sequence X \u2225 Y 0,", ",i\u22121 (Figure 1b and 3 We use prepending and concatenation interchangeably. 1c). Recent works (Wu et al., 2023) propose an alternative solution for causal masking, where only the previous tokens are masked while each element of the speech sequence can attend to each other. In this case, the causal mask M becomes: \n\nwhere N is the length of the speech sequence X. This enables speech tokens to attend to all other speech tokens, including subsequent ones, in the decoder self-attention layers, as it happens in the self-attention of the speech encoders in encoderdecoder models."], "score": 0.68798828125}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 259501685, "title": "On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration", "year": 2023, "venue": "Automatic Speech Recognition & Understanding", "authors": [{"name": "Jian Wu", "authorId": "97569165"}, {"name": "Yashesh Gaur", "authorId": "2334648"}, {"name": "Zhuo Chen", "authorId": "145718850"}, {"name": "Long Zhou", "authorId": "2135918679"}, {"name": "Yilun Zhu", "authorId": "2117870253"}, {"name": "Tianrui Wang", "authorId": "2118915113"}, {"name": "Jinyu Li", "authorId": "152319568"}, {"name": "Shujie Liu", "authorId": "2107983441"}, {"name": "Bo Ren", "authorId": "2121381699"}, {"name": "Linquan Liu", "authorId": "2049319"}, {"name": "Yu Wu", "authorId": "49176273"}], "n_citations": 136}, "snippets": ["Large language models (LLMs) have achieved remarkable success in the field of natural language processing, enabling better human-computer interaction using natural language. However, the seamless integration of speech signals into LLMs has not been explored well. The \"decoder-only\" architecture has also not been well studied for speech processing tasks. In this research, we introduce Speech-LLaMA, a novel approach that effectively incorporates acoustic information into text-based large language models. Our method leverages Connectionist Temporal Classification and a simple audio encoder to map the compressed acoustic features to the continuous semantic space of the LLM. In addition, we further probe the decoder-only architecture for speech-to-text tasks by training a smaller scale randomly initialized speech-LLaMA model from speech-text paired data alone. We conduct experiments on multilingual speech-to-text translation tasks and demonstrate a significant improvement over strong baselines, highlighting the potential advantages of decoder-only models for speech-to-text conversion."], "score": 0.0}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens.\n\nThe decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction."], "score": 0.6494140625}], "table": null}, {"title": "Comparative Analysis of Attention Masking Mechanisms", "tldr": "The three language model architectures\u2014causal decoder-only, non-causal (prefix) decoder-only, and encoder-decoder\u2014each employ distinctive attention masking patterns that fundamentally shape their capabilities and limitations. These architectural differences determine how models process context, balance understanding with generation, and handle various natural language processing tasks. (12 sources)", "text": "\nWhen comparing the attention masking mechanisms across the three major language model architectures, several key distinctions emerge that influence their performance characteristics and optimal use cases. The most fundamental difference lies in how information flows through each architecture, which is directly controlled by the attention masking patterns <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper> <Paper corpusId=\"268041362\" paperTitle=\"(Yi et al., 2024)\" isShortName></Paper>.\n\nCausal decoder-only models implement strictly unidirectional attention, where each token can only attend to itself and previous tokens in the sequence. This architecture processes all tokens equivalently through a single masking pattern, making it conceptually simpler but limited in its ability to build comprehensive bidirectional representations <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. In contrast, non-causal (prefix) decoder-only models offer a middle ground by employing a hybrid approach\u2014bidirectional attention for prefix tokens and unidirectional attention for generated tokens\u2014combining aspects of both understanding and generation within a single architecture <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nEncoder-decoder models represent the most compartmentalized approach, with distinct components and masking patterns for understanding (encoder with fully-visible masking) and generation (decoder with causal self-attention and cross-attention) <Paper corpusId=\"273025546\" paperTitle=\"(Ewer et al., 2024)\" isShortName></Paper>. This clear separation allows encoder-decoder models to develop rich bidirectional representations of input sequences while maintaining controlled autoregressive generation <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nFrom a technical perspective, these architectural differences impact model performance characteristics. Causal attention in decoder-only models creates a pattern of cumulative quantization errors that can affect generation quality, as errors propagate through the sequence <Paper corpusId=\"260886785\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. Additionally, the choice of attention mask directly influences a model's suitability for specific tasks. For instance, fully-visible masking enables comprehensive contextual understanding beneficial for tasks like question answering, while causal masking is essential for autoregressive text generation <Paper corpusId=\"263831258\" paperTitle=\"(Woo et al., 2023)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>.\n\nThe evolution of these architectures reflects a growing understanding of how attention masking can be optimized for different applications. The T5 framework's introduction of \"PrefixLM\" demonstrated that hybrid approaches can combine the strengths of different masking patterns <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. By allowing bidirectional attention over a designated prefix while maintaining causal attention for generation, prefix models offer a balance between the comprehensive context understanding of encoder models and the autoregressive generation capabilities of decoder models <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThe practical implications of these architectural differences become evident in real-world applications. For instance, in translation tasks, prefix decoder-only models can apply fully-visible masking to the source text (e.g., \"translate English to French: I am doing well. Target:\") followed by causal masking for generating the target language output (\"je vais bien\") <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. This capability to process different segments of the input with distinct attention patterns provides greater flexibility than the uniform approach of causal decoder-only models.\n\nThese architectural distinctions have shaped the evolution of modern language models, with each approach finding its niche in the broader landscape of natural language processing. While encoder-decoder models were the original Transformer architecture, the simplicity and scaling properties of decoder-only models have made them dominant in recent large language model development <Paper corpusId=\"248118752\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. Nevertheless, the diversity of attention masking mechanisms continues to play a crucial role in addressing the varied requirements of different natural language processing tasks <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["A major distinguishing factor for different architectures is the \"mask\" used by different attention mechanisms in the model. Recall that the self-attention operation in a Transformer takes a sequence as input and outputs a new sequence of the same length. Each entry of the output sequence is produced by computing a weighted average of entries of the input sequence. Specifically, let y i refer to the ith element of the output sequence and x j refer to the jth entry of the input sequence. y i is computed as j w i,j x j , where w i,j is the scalar weight produced by the self-attention mechanism as a function of x i and x j . The attention mask is then used to zero out certain weights in order to constrain which entries of the input can be attended to at a given output timestep. Diagrams of the masks we will consider are shown in Figure 3. For example, the causal mask (Figure 3, middle) sets any w i,j to zero if j > i.\n\nThe encoder uses a \"fully-visible\" attention mask. Fully-visible masking allows a selfattention mechanism to attend to any entry of the input when producing each entry of its output. We visualize this masking pattern in Figure 3, left. This form of masking is appropriate when attending over a \"prefix\", i.e. some context provided to the model that is later used when making predictions. BERT (Devlin et al., 2018) also uses a fully-visible masking pattern", "Left: A fully-visible mask allows the self-attention mechanism to attend to the full input at every output timestep. Middle: A causal mask prevents the ith output element from depending on any input elements from \"the future\". Right: Causal masking with a prefix allows the self-attention mechanism to use fully-visible masking on a portion of the input sequence.\n\nThe self-attention operations in the Transformer's decoder use a \"causal\" masking pattern. When producing the ith entry of the output sequence, causal masking prevents the model from attending to the jth entry of the input sequence for j > i. This is used during training so that the model can't \"see into the future\" as it produces its output."], "score": 0.91650390625}, {"id": "(Yi et al., 2024)", "paper": {"corpus_id": 268041362, "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zihao Yi", "authorId": "2287925430"}, {"name": "Jiarui Ouyang", "authorId": "2287922728"}, {"name": "Yuwen Liu", "authorId": "2288039936"}, {"name": "Tianhao Liao", "authorId": "2287923878"}, {"name": "Zhe Xu", "authorId": "2288033664"}, {"name": "Ying Shen", "authorId": "2288065597"}], "n_citations": 72}, "snippets": ["Fig. 1. The matrix comparison of attention mask patterns between decoder-only and encoder-decoder architectures. The matrix uses dark cells to allow for self-attention of input elements  at the output time step , while light cells restrict this attention. The left panel represents the full input attention, the middle panel refers to preventing future input reliance, and the right panel combines causal masking with a prefix for partial input sequence fully-visible masking. (Raffel et al., 2019)"], "score": 0.7626953125}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248118752, "title": "What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Thomas Wang", "authorId": "2135734748"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Daniel Hesslow", "authorId": "80424302"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Julien Launay", "authorId": "143945447"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 175}, "snippets": ["A major difference between these architectures is the masking pattern applied to the provided inputs, which act as contextual information for the model to make a prediction. Figure 2 showcases the attention masking patterns in the three architectural variants we consider.\n\nThe self-attention layers in the decoder utilize a causal masking pattern that prevents the model from attending to future tokens when predicting the output sequence (see Figure 2, on the right).\n\nCausal decoder-only. Although the encoder-decoder is the original Transformer variant, most recent LLMs use a decoder-only architecture. These models can be trained as a traditional language model (i.e. to predict the next token in a sequence). Decoder-only models have no independent means of processing or representing the input sequence and target sequence differently-all tokens are processed in an equivalent fashion, and, because of the causal masking pattern, conditioning is simply based on past tokens (see Figure 2, on the left).\n\nNon-causal decoder-only. To allow decoder-only models to build richer non-causal representations of the input/conditioning text, it has been proposed to simply modify the attention mask used. Specifically, the self-attention masking pattern can be changed so that the region of the input sequence corresponding to conditioning information has a non-causal mask (i.e. attention in this region is not restricted to past tokens, see middle of Figure 2), as in the encoder of an encoder-decoder architecture."], "score": 0.765625}, {"id": "(Ewer et al., 2024)", "paper": {"corpus_id": 273025546, "title": "ENTP: Encoder-only Next Token Prediction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ethan Ewer", "authorId": "2323781863"}, {"name": "Daewon Chae", "authorId": "2253659910"}, {"name": "Thomas Zeng", "authorId": "2323820473"}, {"name": "Jinkyu Kim", "authorId": "2323851531"}, {"name": "Kangwook Lee", "authorId": "2323790154"}], "n_citations": 4}, "snippets": ["In contrast, the causal decoder-only model (Brown et al., 2020)(Chowdhery et al., 2022) uses only the Transformer decoder and applies causal attention to all tokens to perform nexttoken prediction, ensuring that each token attends only to previous tokens. The prefix decoder-only model (Raffel et al., 2019)Wu et al., 2021) is similar to the causal decoder-only model but differs in that it applies non-causal attention (i.e., full self-attention) to the input sequence (see Figure 8 for visualizations of the attention patterns in these variants)."], "score": 0.57177734375}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271600495, "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingcong Lu", "authorId": "2314473248"}, {"name": "Jiangcai Zhu", "authorId": "2314649002"}, {"name": "Wang Hao", "authorId": "2314113733"}, {"name": "Zheng Li", "authorId": "2314323587"}, {"name": "Shusheng Zhang", "authorId": "2314311430"}, {"name": "Kailai Shao", "authorId": "2314110211"}, {"name": "Chao Chen", "authorId": "2314192630"}, {"name": "Nan Li", "authorId": "2314343132"}, {"name": "Feng Wang", "authorId": "2324104105"}, {"name": "Xin Lu", "authorId": "2324103820"}], "n_citations": 0}, "snippets": ["Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021)", "based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself", "Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."], "score": 0.744140625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["The Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices. The figure illustrates a comparison of mask configurations between the Encoderdecoder and Decoder-only architectures (including Casual Decoder and Prefix Decoder).\n\nThe Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.654296875}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["For example, to translate an English sentence \"I am doing well\" to French, the model would apply a fully visible mask to the prefix \"translate English to French: I am doing well. Target:\", followed by causal masking while predicting the target \"je vais bien\". Also, unlike causal language models where the targets-only paradigm is used, the prefix language model uses the input-to-target paradigm. Both causal and prefix model architectures are autoregressive as the objective is to predict the next token. However, the causal model uses a unidirectional attention mask, while the prefix model modifies the masking mechanism to employ bidirectional attention over prefix tokens. Figure 4 demonstrates the mechanism of the above architectures. The lines represent the attention visibility. Dark lines represent the fully visible masking (bidirectional attention), and light gray lines represent causal masking (unidirectional attention). As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence."], "score": 0.97265625}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 260886785, "title": "Token-Scaled Logit Distillation for Ternary Weight Generative Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Minsoo Kim", "authorId": "2141320070"}, {"name": "Sihwa Lee", "authorId": "2144376191"}, {"name": "Janghwan Lee", "authorId": "2265920992"}, {"name": "S. Hong", "authorId": "2158125346"}, {"name": "Duhyeuk Chang", "authorId": "2180828053"}, {"name": "Wonyong Sung", "authorId": "66936521"}, {"name": "Jungwook Choi", "authorId": "2506452"}], "n_citations": 15}, "snippets": ["Cumulative Quantization Errors in Causal Attention. Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for text generation tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedure of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to Q, K, and V computation is uniformly spread because of the simultaneous computing nature inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation."], "score": 0.5078125}, {"id": "(Woo et al., 2023)", "paper": {"corpus_id": 263831258, "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Gerald Woo", "authorId": "151488390"}, {"name": "Chenghao Liu", "authorId": "2257133300"}, {"name": "Akshat Kumar", "authorId": "2257090206"}, {"name": "Doyen Sahoo", "authorId": "36187119"}], "n_citations": 14}, "snippets": ["Causal attention masks can be used to differentiate between encoding and decoding, i.e. full attention for encoding and causal attention for decoding. (Dong et al., 2019) introduced various attention masking strategies for a unified Transformer architecture in the context of NLP. While the various masking strategies correspond to different downstream tasks in natural language processing (e.g. full attention/bidirectional encoding for extractive question answering and full causal/unidirectional decoding for long text generation), it is unclear which paradigm time series forecasting fits in. On the one hand, we could argue that past time steps should not attend to future time steps, on the other hand, attending to future time steps could help in extracting seasonal information for example."], "score": 0.56689453125}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 147704286, "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Li Dong", "authorId": "145307652"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Wenhui Wang", "authorId": "51456429"}, {"name": "Furu Wei", "authorId": "49807919"}, {"name": "Xiaodong Liu", "authorId": "46522098"}, {"name": "Yu Wang", "authorId": "72682749"}, {"name": "Jianfeng Gao", "authorId": "1800422"}, {"name": "M. Zhou", "authorId": "143849609"}, {"name": "H. Hon", "authorId": "145058181"}], "n_citations": 1560}, "snippets": ["This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."], "score": 0.0}, {"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2019), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is preva-lent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."], "score": 0.92333984375}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}], "table": null}], "cost": 0.37867799999999996}}
