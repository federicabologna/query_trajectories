{"better_query": "What are the main methodologies used in membership inference attacks on machine learning models, and how do black-box and white-box approaches differ?", "better_answer": {"sections": [{"title": "Introduction to Membership Inference Attacks", "tldr": "Membership inference attacks aim to determine whether a specific data record was used to train a machine learning model, posing significant privacy risks. These attacks exploit differences in model behavior when processing seen versus unseen data, with approaches ranging from shadow model training to confidence score analysis. (15 sources)", "text": "\nMembership inference attacks (MIAs) represent a significant privacy concern in machine learning, as they allow adversaries to determine whether a specific data record was part of a model's training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. These attacks exploit a fundamental observation: machine learning models often behave differently on data they were trained on versus data they encounter for the first time, with overfitting being a common but not the only reason for this difference <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>.\n\nThe core objective of a membership inference attack is to recognize distinctions in a target model's behavior that can differentiate between members and non-members of the training dataset <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. This type of attack can be conducted in either black-box or white-box settings. In the black-box setting, the adversary only has query access to the model through a prediction API, while in the white-box setting, the attacker has full access to the model's internal structure <Paper corpusId=\"227227868\" paperTitle=\"(Hidano et al., 2020)\" isShortName></Paper> <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper> <Paper corpusId=\"67855651\" paperTitle=\"(Jayaraman et al., 2019)\" isShortName></Paper>.\n\nThe most established approach to membership inference, introduced by Shokri et al., involves training multiple \"shadow models\" designed to mimic the target model's behavior <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. These shadow models are used to generate labeled examples (member vs. non-member), which are then used to train an \"attack model\" - a classifier that can distinguish between the predictions made on training data versus predictions on unseen data <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"231861713\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>. Later research by Salem et al. demonstrated that even a single shadow model can be sufficient for an effective attack <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper> <Paper corpusId=\"236956533\" paperTitle=\"(Bagmar et al., 2021)\" isShortName></Paper>.\n\nAlternative approaches include confidence-thresholding methods, where the prediction confidence of the target model is compared to a threshold - with higher confidence potentially indicating membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper>. More recent work has explored label-only membership inference attacks that don't require confidence scores, instead relying on the robustness of model predictions under perturbations <Paper corpusId=\"251765197\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\nThe vulnerability to membership inference attacks is often linked to overfitting, as models that memorize their training data rather than learning generalizable patterns create distinguishable differences in their behavior on seen versus unseen examples <Paper corpusId=\"199546273\" paperTitle=\"(Hilprecht et al., 2019)\" isShortName></Paper>. However, research has shown that even well-generalized models can be susceptible to these attacks <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>.\n\nThe potential privacy risks posed by membership inference attacks are particularly concerning in domains involving sensitive data, such as healthcare analytics, where revealing that an individual's data was used to train a model could expose private information <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper> <Paper corpusId=\"202888587\" paperTitle=\"(Aivodji et al., 2019)\" isShortName></Paper>. Recent advances have even demonstrated the possibility of performing membership inference attacks with only black-box access to target models, further highlighting the breadth of this privacy concern <Paper corpusId=\"264179931\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"50778569\" paperTitle=\"(Truex et al., 2018)\" isShortName></Paper>.", "citations": [{"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members."], "score": 0.96923828125}, {"id": "(Yeom et al., 2017)", "paper": {"corpus_id": 2656445, "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting", "year": 2017, "venue": "IEEE Computer Security Foundations Symposium", "authors": [{"name": "Samuel Yeom", "authorId": "26378728"}, {"name": "Irene Giacomelli", "authorId": "3025831"}, {"name": "Matt Fredrikson", "authorId": "2623167"}, {"name": "S. Jha", "authorId": "1680133"}], "n_citations": 1133}, "snippets": ["Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."], "score": 0.0}, {"id": "(Hidano et al., 2020)", "paper": {"corpus_id": 227227868, "title": "TransMIA: Membership Inference Attacks Using Transfer Shadow Training", "year": 2020, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Seira Hidano", "authorId": "3222644"}, {"name": "Yusuke Kawamoto", "authorId": "49224245"}, {"name": "Takao Murakami", "authorId": "2116295"}], "n_citations": 13}, "snippets": ["We deal with two approaches to constructing a membership inference adversary: the learning-based approach [14] and the entropy-based approach [24]. The former constructs an adversary A as a classification model obtained by supervised learning using a dataset other than D train.\n\nThe membership inference attack exploits a different behavior of the model f when a given data point (x, y) has been used to train f. To build an adversary A against f, we attempt to learn some statistical relevance between the distribution f(x) of confidence values and the membership (x, y) \u2208 D train.\n\nIn this attack, the adversary A is provided access to the model f and some dataset disjoint from the training dataset D train. An adversary is said to have black-box access to f if it can query data x to f and obtain their prediction vectors f(x). In contrast, white-box access to f allows the adversary to obtain the internal structure of f itself (e.g., the weights of the connections between nodes when f is a neural network)."], "score": 0.97509765625}, {"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Jayaraman et al., 2019)", "paper": {"corpus_id": 67855651, "title": "When Relaxations Go Bad: \"Differentially-Private\" Machine Learning", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Bargav Jayaraman", "authorId": "2348109"}, {"name": "David Evans", "authorId": "145685504"}], "n_citations": 7}, "snippets": ["Membership inference attacks can either be completely black-box where an attacker only has query access to the target model (Shokri et al., 2016), or can assume that the attacker has full white-box access to the target model, along with some auxillary information (Yeom et al., 2017). The first membership inference attack on machine learning was proposed by Shokri et al. (Shokri et al., 2016). They consider an attacker who can query the target model in a black-box way to obtain confidence scores for the queried input. The attacker tries to exploit the confidence score to determine whether the query input was present in the training data. Their attack method involves first training shadow models on a labelled data set, which can be generated either via black-box queries to the target model or through assumptions about the underlying distribution of training set. The attacker then trains an attack model using the shadow models to distinguish whether or not an input record is in the shadow training set. Finally, the attacker makes API calls to the target model to obtain confidence scores for each given input record and infers whether or not the input was part of the target model's training set. The inference model distinguishes between the target model's predictions for inputs that are in its training set and those it did not train on. The key assumption is that the confidence score of the target model is higher for the training instances than it would be for arbitrary instances not present in the training set. This can be due to the generalization gap, which is prominent in models that overfit to training data. \n\nA more targeted approach was proposed by Long et al. [44] where the shadow models are trained with and without a targeted input record t. At inference time, the attacker can check if the input record t was present in the training set of target model. This approach tests the membership of a specific record more accurately than Shokri et al.'s approach [62]."], "score": 0.9716796875}, {"id": "(Song et al., 2019)", "paper": {"corpus_id": 165163934, "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples", "year": 2019, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 244}, "snippets": ["Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training."], "score": 0.96826171875}, {"id": "(He et al., 2021)", "paper": {"corpus_id": 231861713, "title": "Node-Level Membership Inference Attacks Against Graph Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Rui Wen", "authorId": "2054749404"}, {"name": "Yixin Wu", "authorId": "2127727861"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yun Shen", "authorId": "2117688523"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 98}, "snippets": ["Membership inference attacks aim at inferring membership of individual training sam-ples of a target model to which an adversary has black-box access through a prediction API (Carlini et al., 2018)9,19,28,(Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017)", "Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models in the black-box setting. The authors provide a general formulation of membership inference attack whereas the adversary trains multiple shadow models to mimic the target model's behavior with certain background knowledge of training data and leverages many attack models to conduct the attack", "Nasr et al. (Nasr et al., 2018) conduct a comprehensive study for membership inference attacks in both blackbox and white-box settings."], "score": 0.9794921875}, {"id": "(Salem et al., 2018)", "paper": {"corpus_id": 46933970, "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models", "year": 2018, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "A. Salem", "authorId": "66697271"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mathias Humbert", "authorId": "144887171"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "M. Backes", "authorId": "144588806"}], "n_citations": 950}, "snippets": ["Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."], "score": 0.0}, {"id": "(Bagmar et al., 2021)", "paper": {"corpus_id": 236956533, "title": "Membership Inference Attacks on Lottery Ticket Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Aadesh Bagmar", "authorId": "1693718757"}, {"name": "Shishira R. Maiya", "authorId": "51469126"}, {"name": "Shruti Bidwalka", "authorId": "2122929492"}, {"name": "A. Deshpande", "authorId": "144520191"}], "n_citations": 5}, "snippets": ["Membership inference attacks (MIA) aim to identify whether a data sample was used to train a machine learning model or not. These attacks have been successfully carried out on centralized supervised learning and unsupervised learning models and also distributed learning based Federated Learning models (Hu et al., 2021).\n\nThese attacks work even if the attacker does not have access to the original training data that was used to train the target model. Shokri et al. (2017) describe a method wherein they train multiple \"shadow models\" that mimic the behaviour of the target model. This is a type of a white-box attack where the architecture of the targeted model and the training dataset membership of this shadow model is known. Salem et al. (2018) showed that a single shadow network is sufficient too.\n\nMembership inference attacks have been studied extensively (Shokri et al., 2017;Nasr et al., 2018;Li & Zhang, 2020) and across different domains (Danhier et al., 2020;Salem et al., 2018;Liu et al., 2019;He et al., 2020). Different types of attacks including neural network based and metric based have been proposed and researchers have shown successful black box and white box approaches."], "score": 0.96728515625}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 251765197, "title": "Auditing Membership Leakages of Multi-Exit Networks", "year": 2022, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Yiyong Liu", "authorId": "2182511319"}, {"name": "Xinlei He", "authorId": "2116553732"}, {"name": "Ning Yu", "authorId": "145648201"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yang Zhang", "authorId": "1698138"}], "n_citations": 34}, "snippets": ["Membership inference is one of the major methods to evaluate privacy risks of machine learning models [26,29](Leino et al., 2019)(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Song et al., 2019)(Yeom et al., 2017). Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against ML models. They train multiple attack models using a dataset constructed from multiple shadow models. These attack models take the posterior of the target sample as input and predict its membership status, i.e., member or non-member. Then Salem et al. (Salem et al., 2018) propose a model and data-independent membership inference attack by gradually relaxing the assumption made by Shokri et al. (Shokri et al., 2016). Later, Nasr et al. (Nasr et al., 2018) focus on the privacy risk in centralized and federated learning scenarios, and conduct extensive experiments under both black-box and whitebox settings. Song et al. (Song et al., 2019) study the relationship between adversarial examples and the privacy risk caused by membership inference attacks. Li and Zhang (Li et al., 2020) and Choquette-Choo et al. (Choquette-Choo et al., 2020) propose the label-only membership inference attack by changing the predicted labels of the target model, then measuring the magnitude of the perturbation. If the magnitude of the perturbation is larger than a predefined threshold, the adversary considers the data sample as a member and vice versa."], "score": 0.9677734375}, {"id": "(Choquette-Choo et al., 2020)", "paper": {"corpus_id": 220831381, "title": "Label-Only Membership Inference Attacks", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Christopher A. Choquette-Choo", "authorId": "1415982317"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Nicolas Papernot", "authorId": "1967156"}], "n_citations": 516}, "snippets": ["Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."], "score": 0.0}, {"id": "(Hilprecht et al., 2019)", "paper": {"corpus_id": 199546273, "title": "Monte Carlo and Reconstruction Membership Inference Attacks against Generative Models", "year": 2019, "venue": "Proceedings on Privacy Enhancing Technologies", "authors": [{"name": "Benjamin Hilprecht", "authorId": "81786870"}, {"name": "Martin H\u00e4rterich", "authorId": "2736329"}, {"name": "Daniel Bernau", "authorId": "13047311"}], "n_citations": 191}, "snippets": ["The goal of membership inference (MI) is to gather evidence whether a specific record or a set of records belongs to the training dataset of a given machine learning model. MI thus represents an approach for measuring how much a model leaks about individual records of a population. The success rates of MI attacks against a model are tightly linked to overfitting (i.e., the generalization error [30]). The poorer a model generalizes the more specificities it contains about individual training data records. ) The choice of the attack determines the requirements on the information that is available to the actor. The MC attack requires samples drawn from the generative model while the Reconstruction attack has to be able to evaluate the generative model."], "score": 0.970703125}, {"id": "(Aivodji et al., 2019)", "paper": {"corpus_id": 202888587, "title": "GAMIN: An Adversarial Approach to Black-Box Model Inversion", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "U. A\u00efvodji", "authorId": "40907220"}, {"name": "S. Gambs", "authorId": "1777382"}, {"name": "Timon Ther", "authorId": "1388769197"}], "n_citations": 42}, "snippets": ["Membership attacks against machine learning models have been introduced by Shokri, Stronati, Song and Shmatikov (Shokri et al., 2016). Given a data record d and a trained model M trained over a training dataset D M train , a membership inference attack consist in trying to evaluate if d \u2208 D M train . For instance, the authors demonstrated in 2017 the possibility for an adversary to assess the presence of a given individual in hospital datasets in a true black-box setting, highlighting the potential privacy damage this type of attack can cause. This type of attack exploits the fact that machine learning models may be subject to overfitting (i.e, being significantly more accurate at predicting outputs for the training data than predicting outputs for the test data). The attack involves training multiple shadow models, each using the same machine learning technique as that of the target model, and using a dataset similar to that of the target model. However, this is done by explicitly labeling predictions vectors on its training set and its test set differently. Finally, a classifier is trained to distinguish training data from test data. Membership attacks have also been studied by Melis, Song, de Cristofaro and Shmatikov (Melis et al., 2018) in the context of collaborative learning, in which the authors showed that the interactive nature of the collaboration can be exploited by a participant to conduct a membership attack on other participants' training sets."], "score": 0.96875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 264179931, "title": "Gradient-Leaks: Enabling Black-Box Membership Inference Attacks Against Machine Learning Models", "year": 2024, "venue": "IEEE Transactions on Information Forensics and Security", "authors": [{"name": "Gaoyang Liu", "authorId": "2019143562"}, {"name": "Tianlong Xu", "authorId": "2260601109"}, {"name": "Rui Zhang", "authorId": "2118403448"}, {"name": "Zixiong Wang", "authorId": "2259065760"}, {"name": "Chen Wang", "authorId": "40614774"}, {"name": "Ling Liu", "authorId": "2213648984"}], "n_citations": 11}, "snippets": ["In this paper, we present Gradient-Leaks as the first evidence showcasing the possibility of performing membership inference attacks (MIAs), with mere black-box access, which aim to determine whether a data record was utilized to train a given target ML model or not. The key idea of Gradient-Leaks is to construct a local ML model around the given record which locally approximates the target model's prediction behavior."], "score": 0.96728515625}, {"id": "(Truex et al., 2018)", "paper": {"corpus_id": 50778569, "title": "Towards Demystifying Membership Inference Attacks", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Stacey Truex", "authorId": "25121568"}, {"name": "Ling Liu", "authorId": "46458150"}, {"name": "M. E. Gursoy", "authorId": "2327300"}, {"name": "Lei Yu", "authorId": "2112532900"}, {"name": "Wenqi Wei", "authorId": "47747953"}], "n_citations": 112}, "snippets": ["Most existing membership inference attacks similarly attack deep learning models, utilizing deep neural networks (DNNs) for training both the target model under attack and the attack model [6,18,28](Shokri et al., 2016). However, membership inference attacks are different from adversarial examples with respect to both attack generation process and adverse effect of attacks and represent two different classes of security and privacy intrusion problems under the general umbrella of adversarial machine learning", "we investigate membership inference attacks under the black-box access scenario in which an adversary may probe the prediction API with input and receive the prediction output from the privately trained model."], "score": 0.96875}], "table": null}, {"title": "Black-Box Methodologies", "tldr": "Black-box membership inference attacks operate without access to the target model's internal structure, using only the model's outputs to determine membership status. These methodologies range from shadow model training approaches to metric-based techniques that analyze prediction confidence, with recent innovations focusing on minimizing assumptions and resource requirements. (10 sources)", "text": "\nBlack-box membership inference attacks operate in settings where the adversary has limited access to the target model, typically only being able to query the model and observe its outputs without knowledge of its internal parameters or architecture. Several key methodologies have been developed for this scenario:\n\n## Shadow Model Training Approaches\n\nThe most established black-box methodology, introduced by Shokri et al., uses multiple \"shadow models\" to mimic the behavior of the target model <Paper corpusId=\"10488675\" paperTitle=\"(Shokri et al., 2016)\" isShortName></Paper>. In this approach:\n\n1. The attacker trains several shadow models on datasets that attempt to replicate the distribution of the target model's training data\n2. For each shadow model, the attacker collects outputs on both training data (members) and test data (non-members)\n3. These outputs, labeled as either member or non-member, are used to train an \"attack model\" (typically a binary classifier)\n4. The attack model then predicts membership status for any data point queried against the target model\n\nThis approach was later refined by Salem et al., who demonstrated that even a single shadow model can be sufficient to mount effective attacks, significantly reducing the computational burden <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>. This relaxation of assumptions makes membership inference attacks more broadly applicable and less resource-intensive <Paper corpusId=\"91184074\" paperTitle=\"(Salem et al., 2019)\" isShortName></Paper>.\n\n## Metric-Based Approaches\n\nAs an alternative to training shadow models, researchers have developed simpler techniques that rely on analyzing specific metrics from model outputs:\n\n1. **Confidence thresholding**: This approach compares the prediction confidence of the target model to a threshold, with higher confidence potentially indicating membership <Paper corpusId=\"165163934\" paperTitle=\"(Song et al., 2019)\" isShortName></Paper> <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. This method can achieve membership inference accuracy similar to more complex approaches.\n\n2. **Entropy-based methods**: These techniques use the prediction entropy as a measure to distinguish between members and non-members <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n3. **Ranking-based methods**: Salem et al. demonstrated that simply ranking the elements in the confidence score vectors before inputting them to the attack model could improve the inference accuracy <Paper corpusId=\"46933970\" paperTitle=\"(Salem et al., 2018)\" isShortName></Paper>.\n\n## Data-Independent Approaches\n\nMore recent research has focused on making black-box attacks less dependent on assumptions about data and model architecture:\n\n1. **BlindMI**: Hui et al. developed an approach that extracts membership semantics without shadow models through \"differential comparison,\" where a dataset of non-members is generated through transformations, and target samples are moved between datasets to detect membership status <Paper corpusId=\"230523638\" paperTitle=\"(Hui et al., 2021)\" isShortName></Paper>.\n\n2. **Metric benchmarking**: Song et al. proposed a systematic way to benchmark membership inference risks by comparing different non-neural network based inference attacks and introduced a \"privacy risk score\" to measure individual samples' likelihood of being training members <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n## Label-Only Attacks\n\nA significant advancement in black-box methodologies are attacks that work with only the predicted labels rather than confidence scores:\n\n1. **Decision-based attacks**: Li et al. demonstrated that even when a model only exposes the final predicted label (not confidence scores), membership can still be inferred through techniques like transfer attacks and boundary attacks <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n2. **Robustness-based approaches**: Choquette-Choo et al. introduced attacks that evaluate the robustness of a model's predicted labels under perturbations to obtain membership signals <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\nThese various black-box methodologies highlight how membership information can leak through a model's outputs in multiple ways, even with minimal access to the model itself <Paper corpusId=\"232233426\" paperTitle=\"(Hu et al., 2021)\" isShortName></Paper>. The evolution of these techniques has progressively relaxed the assumptions required for successful attacks, making them increasingly practical and concerning from a privacy perspective.", "citations": [{"id": "(Shokri et al., 2016)", "paper": {"corpus_id": 10488675, "title": "Membership Inference Attacks Against Machine Learning Models", "year": 2016, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "R. Shokri", "authorId": "2520493"}, {"name": "M. Stronati", "authorId": "34828439"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 4160}, "snippets": ["Our membership inference attack exploits the observation that machine learning models often behave differently on the data that they were trained on versus the data that they \"see\" for the first time. Overfitting is a common reason but not the only one (see Section VII). The objective of the attacker is to construct an attack model that can recognize such differences in the target model's behavior and use them to distinguish members from non-members of the target model's training dataset based solely on the target model's output.\n\nOur attack model is a collection of models, one for each output class of the target model. This increases accuracy of the attack because the target model produces different distributions over its output classes depending on the input's true class.\n\nTo train our attack model, we build multiple \"shadow\" models intended to behave similarly to the target model. In contrast to the target model, we know the ground truth for each shadow model, i.e., whether a given record was in its training dataset or not. Therefore, we can use supervised training on the inputs and the corresponding outputs (each labeled \"in\" or \"out\") of the shadow models to teach the attack model how to distinguish the shadow models' outputs on members of their training datasets from their outputs on non-members."], "score": 0.96923828125}, {"id": "(Salem et al., 2018)", "paper": {"corpus_id": 46933970, "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models", "year": 2018, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "A. Salem", "authorId": "66697271"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mathias Humbert", "authorId": "144887171"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "M. Backes", "authorId": "144588806"}], "n_citations": 950}, "snippets": ["Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model."], "score": 0.0}, {"id": "(Salem et al., 2019)", "paper": {"corpus_id": 91184074, "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning", "year": 2019, "venue": "USENIX Security Symposium", "authors": [{"name": "A. Salem", "authorId": "66697271"}, {"name": "Apratim Bhattacharyya", "authorId": "3407762"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Mario Fritz", "authorId": "1739548"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 257}, "snippets": ["Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data (Hagestedt et al., 2019)[21] and location data (Pyrgelis et al., 2017)[37]. Shokri et al. (Shokri et al., 2016) propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models [19,27,28](Nasr et al., 2018)(Nasr et al., 2018)(Salem et al., 2018)42,(Yeom et al., 2017)."], "score": 0.97216796875}, {"id": "(Song et al., 2019)", "paper": {"corpus_id": 165163934, "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples", "year": 2019, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 244}, "snippets": ["Membership inference attacks aim to determine whether a given data point was used to train the model or not (Hayes et al., 2017)32,(Nasr et al., 2018)(Salem et al., 2018)(Shokri et al., 2016)(Yeom et al., 2017). The attack poses a serious privacy risk to the individuals whose data is used for model training, for example in the setting of health analytics. Shokri et al. (Shokri et al., 2016) design a membership inference attack method based on training an inference model to distinguish between predictions on training set members versus non-members. To train the inference model, they introduce the shadow training technique: (1) the adversary first trains multiple \"shadow models\" which simulate the behavior of the target model, (2) based on the shadow models' outputs on their own training and test examples, the adversary obtains a labeled (member vs non-member) dataset, and (3) finally trains the inference model as a neural network to perform membership inference attack against the target model. The input to the inference model is the prediction vector of the target model on a target data record. A simpler inference model, such as a linear classifier, can also distinguish significantly vulnerable members from non-members. Yeom et al. (Yeom et al., 2017) suggest comparing the prediction confidence value of a target example with a threshold (learned for example through shadow training). Large confidence indicates membership. Their results show that such a simple confidence-thresholding method is reasonably effective and achieves membership inference accuracy close to that of a complex neural network classifier learned from shadow training."], "score": 0.96826171875}, {"id": "(Yeom et al., 2017)", "paper": {"corpus_id": 2656445, "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting", "year": 2017, "venue": "IEEE Computer Security Foundations Symposium", "authors": [{"name": "Samuel Yeom", "authorId": "26378728"}, {"name": "Irene Giacomelli", "authorId": "3025831"}, {"name": "Matt Fredrikson", "authorId": "2623167"}, {"name": "S. Jha", "authorId": "1680133"}], "n_citations": 1133}, "snippets": ["Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."], "score": 0.0}, {"id": "(Song et al., 2020)", "paper": {"corpus_id": 214623088, "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 375}, "snippets": ["In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018)."], "score": 0.97607421875}, {"id": "(Hui et al., 2021)", "paper": {"corpus_id": 230523638, "title": "Practical Blind Membership Inference Attack via Differential Comparisons", "year": 2021, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Bo Hui", "authorId": "2000495031"}, {"name": "Yuchen Yang", "authorId": "46285766"}, {"name": "Haolin Yuan", "authorId": "2114128628"}, {"name": "P. Burlina", "authorId": "1765936"}, {"name": "N. Gong", "authorId": "144516687"}, {"name": "Yinzhi Cao", "authorId": "3139121"}], "n_citations": 122}, "snippets": ["Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses."], "score": 0.0}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 237563320, "title": "Membership Leakage in Label-Only Exposures", "year": 2020, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 246}, "snippets": ["Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)", "Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack."], "score": 0.97900390625}, {"id": "(Choquette-Choo et al., 2020)", "paper": {"corpus_id": 220831381, "title": "Label-Only Membership Inference Attacks", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Christopher A. Choquette-Choo", "authorId": "1415982317"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Nicolas Papernot", "authorId": "1967156"}], "n_citations": 516}, "snippets": ["Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."], "score": 0.0}, {"id": "(Hu et al., 2021)", "paper": {"corpus_id": 232233426, "title": "Membership Inference Attacks on Machine Learning: A Survey", "year": 2021, "venue": "ACM Computing Surveys", "authors": [{"name": "Hongsheng Hu", "authorId": "2109102414"}, {"name": "Z. Salcic", "authorId": "2175708"}, {"name": "Lichao Sun", "authorId": "46732871"}, {"name": "G. Dobbie", "authorId": "152945656"}, {"name": "P. Yu", "authorId": "2721708"}, {"name": "Xuyun Zhang", "authorId": "2356672360"}], "n_citations": 440}, "snippets": ["Machine learning (ML) models have been widely applied to various applications, including image classification, text generation, audio recognition, and graph data analysis. However, recent studies have shown that ML models are vulnerable to membership inference attacks (MIAs), which aim to infer whether a data record was used to train a target model or not. MIAs on ML models can directly lead to a privacy breach. For example, via identifying the fact that a clinical record that has been used to train a model associated with a certain disease, an attacker can infer that the owner of the clinical record has the disease with a high chance. In recent years, MIAs have been shown to be effective on various ML models, e.g., classification models and generative models. Meanwhile, many defense methods have been proposed to mitigate MIAs. Although MIAs on ML models form a newly emerging and rapidly growing research area, there has been no systematic survey on this topic yet. In this article, we conduct the first comprehensive survey on membership inference attacks and defenses. We provide the taxonomies for both attacks and defenses, based on their characterizations, and discuss their pros and cons. Based on the limitations and gaps identified in this survey, we point out several promising future research directions to inspire the researchers who wish to follow this area. This survey not only serves as a reference for the research community but also provides a clear description for researchers outside this research domain. To further help the researchers, we have created an online resource repository, which we will keep updated with future relevant work. Interested readers can find the repository at https://github.com/HongshengHu/membership-inference-machine-learning-literature."], "score": 0.0}], "table": null}, {"title": "White-Box Methodologies", "tldr": "White-box membership inference attacks leverage full access to the target model's internal architecture and parameters to determine training data membership. These methodologies exploit gradient information, activation patterns, and loss characteristics to achieve higher precision than black-box approaches, particularly when models appear to generalize well. (11 sources)", "text": "\nWhite-box membership inference attacks operate with full access to a target model's internal structure, enabling more powerful attacks compared to black-box approaches. These methodologies leverage various internal signals to determine whether a specific data point was used during training:\n\n## Gradient-Based Approaches\n\nWhite-box attacks commonly exploit the gradients of the model with respect to its parameters:\n\n1. **Gradient analysis**: Nasr et al. introduced techniques that analyze how the stochastic gradient descent algorithm behaves differently on training versus non-training data <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. The authors showed that a straightforward extension of black-box attacks to the white-box setting (by analyzing activation function outputs) is ineffective, and instead developed algorithms that exploit specific gradient-based vulnerability patterns.\n\n2. **Loss gradient exploitation**: Leino et al. demonstrated how analyzing gradients reveals that models use features in idiosyncratic ways for training data, providing strong membership signals <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>. This approach can detect membership even when a model's black-box behavior appears to generalize well.\n\n3. **Parameter-based membership scoring**: Sablayrolles et al. developed theoretical foundations showing that optimal membership inference can be approximated through model parameters' distribution analysis <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper>.\n\n## Feature Representation Analysis\n\nWhite-box attacks can also exploit how models process and represent features:\n\n1. **Intermediate layer analysis**: Multiple researchers have investigated using the outputs of intermediate layers (feature representations) to detect differences in how models process training versus non-training data <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248810845\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>.\n\n2. **Adversarial distance**: Some attacks measure the distance needed to create adversarial examples for input samples, with training samples typically requiring larger perturbations <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper>.\n\n## Loss-Based Mechanisms\n\nThe loss function provides particularly strong signals in white-box settings:\n\n1. **Direct loss comparison**: Yeom et al. showed that simply comparing the loss of a model on a given sample against a threshold can effectively distinguish between members and non-members <Paper corpusId=\"2656445\" paperTitle=\"(Yeom et al., 2017)\" isShortName></Paper>. While this can be done in black-box settings too, white-box access provides additional loss information.\n\n2. **Training trajectory analysis**: Recent approaches by Liu et al. analyze how the loss of samples evolves throughout training, providing an enhanced membership signal <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\n## Advantages of White-Box Approaches\n\nWhite-box attacks consistently demonstrate several advantages over black-box methods:\n\n1. **Higher precision**: Leino et al. showed that white-box attacks can be calibrated for high precision, making them effective at confidently identifying positive membership when black-box attacks cannot <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n2. **Effectiveness on well-generalized models**: Nasr et al. demonstrated that even well-generalized models that resist black-box attacks remain vulnerable to white-box methods <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. This suggests that model memorization occurs even without overt overfitting.\n\n3. **Attack strength**: Multiple studies confirm that white-box attacks generally outperform black-box attacks in terms of accuracy and confidence <Paper corpusId=\"239016142\" paperTitle=\"(Tang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"246706163\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper>.\n\nWhile some research suggests that under certain assumptions about model parameter distribution, theoretical optimal black-box attacks can match white-box performance <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256868849\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>, empirical results consistently show that white-box attacks remain more powerful in practical scenarios, particularly when models appear to generalize well or when high-confidence membership inference is required.", "citations": [{"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Leino et al., 2019)", "paper": {"corpus_id": 195699554, "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference", "year": 2019, "venue": "USENIX Security Symposium", "authors": [{"name": "Klas Leino", "authorId": "35802340"}, {"name": "Matt Fredrikson", "authorId": "2623167"}], "n_citations": 272}, "snippets": ["Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data."], "score": 0.9677734375}, {"id": "(Sablayrolles et al., 2019)", "paper": {"corpus_id": 174799799, "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexandre Sablayrolles", "authorId": "3469062"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "C. Schmid", "authorId": "2462253"}, {"name": "Y. Ollivier", "authorId": "1734570"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}], "n_citations": 368}, "snippets": ["Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet."], "score": 0.0}, {"id": "(Grosso et al., 2022)", "paper": {"corpus_id": 247595200, "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Ganesh Del Grosso", "authorId": "2033372042"}, {"name": "Hamid Jalalzai", "authorId": "52195298"}, {"name": "Georg Pichler", "authorId": "134026950"}, {"name": "C. Palamidessi", "authorId": "1722055"}, {"name": "P. Piantanida", "authorId": "1743922"}], "n_citations": 22}, "snippets": ["The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."], "score": 0.97265625}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 248810845, "title": "Evaluating Membership Inference Through Adversarial Robustness", "year": 2022, "venue": "Computer/law journal", "authors": [{"name": "Zhaoxi Zhang", "authorId": "2156121564"}, {"name": "Leo Yu Zhang", "authorId": "2248789322"}, {"name": "Xufei Zheng", "authorId": "2110300762"}, {"name": "Bilal Hussain Abbasi", "authorId": "2165378948"}, {"name": "Shengshan Hu", "authorId": "3115562"}], "n_citations": 16}, "snippets": ["There are two types of attack settings: blackbox and white-box. In the black-box setting, attackers can only access the output of target model. In this type of attack, the most famous technique is to first train a shadow model, which essentially duplicates the functionality of the target model, and then perform inference attack on the shadow model [17]. In contrast, in the white-box setting, attackers can also access internal details of the target models [19,20,21,22]. Typically, the white-box attack is stronger than the black-box. This is due to the fact that adversary has access to model parameters and neuron activations of the model in such attacks. However, black-box attacks can also perform well given that the attack is designed carefully and systematically. For instance, the work [22] showed the performance of black-box attacks is close to white-box attacks under some attack settings.\n\nRecent research suggests that membership inference attacks can be linked with the well-known phenomenon in deep learning: model overfitting [18]. The rationale behind this is overfitted model can lead to significant differences between members and non-members under a variety of measurements. For example, overfitted model prefer higher confidence score, lower entropy, and smaller values of loss function for member examples. It is easy for the attacker to make use of such significant differences to differentiate members and non-member, which lead to the popularity of metric-based inference attacks [17,18]22]."], "score": 0.97021484375}, {"id": "(Yeom et al., 2017)", "paper": {"corpus_id": 2656445, "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting", "year": 2017, "venue": "IEEE Computer Security Foundations Symposium", "authors": [{"name": "Samuel Yeom", "authorId": "26378728"}, {"name": "Irene Giacomelli", "authorId": "3025831"}, {"name": "Matt Fredrikson", "authorId": "2623167"}, {"name": "S. Jha", "authorId": "1680133"}], "n_citations": 1133}, "snippets": ["Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks."], "score": 0.0}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 251953448, "title": "Membership Inference Attacks by Exploiting Loss Trajectory", "year": 2022, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Yiyong Liu", "authorId": "2182511319"}, {"name": "Zhengyu Zhao", "authorId": "2277275"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 111}, "snippets": ["Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."], "score": 0.9716796875}, {"id": "(Tang et al., 2021)", "paper": {"corpus_id": 239016142, "title": "Mitigating Membership Inference Attacks by Self-Distillation Through a Novel Ensemble Architecture", "year": 2021, "venue": "USENIX Security Symposium", "authors": [{"name": "Xinyu Tang", "authorId": "2048002984"}, {"name": "Saeed Mahloujifar", "authorId": "2364685227"}, {"name": "Liwei Song", "authorId": "144173853"}, {"name": "Virat Shejwalkar", "authorId": "148318826"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Amir Houmansadr", "authorId": "1972973"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 80}, "snippets": ["Membership inference attacks are usually studied in a black-box manner [32,41,43]: an attacker either leverages the shadow training technique or utilizes knowledge of partial membership information of training set. Most MIAs are direct single-query attacks [46,47,54,55]. A more recent line of MIA research has considered indirect multi-query attacks which leverage multiple queries around the target sample to extract additional information [8,20,28]29]", "Another threat model for MIAs is that of a white-box setting, i.e., attacker has full access to the model (Leino et al., 2019)(Nasr et al., 2018), which can exploit model parameters to infer membership information."], "score": 0.970703125}, {"id": "(Zhang et al._1, 2022)", "paper": {"corpus_id": 248870291, "title": "Black-box based limited query membership inference attack", "year": 2022, "venue": "IEEE Access", "authors": [{"name": "Yu Zhang", "authorId": "2153635068"}, {"name": "Huaping Zhou", "authorId": "2146383582"}, {"name": "Pengyan Wang", "authorId": "2297143623"}, {"name": "Gaoming Yang", "authorId": "2105622"}], "n_citations": 3}, "snippets": ["Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability."], "score": 0.98193359375}, {"id": "(Zhou et al., 2022)", "paper": {"corpus_id": 246706163, "title": "PPA: Preference Profiling Attack Against Federated Learning", "year": 2022, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Chunyi Zhou", "authorId": "1845880105"}, {"name": "Yansong Gao", "authorId": "39922366"}, {"name": "Anmin Fu", "authorId": "2068511826"}, {"name": "Kai Chen", "authorId": "2157740727"}, {"name": "Zhiyang Dai", "authorId": "151498397"}, {"name": "Zhi Zhang", "authorId": "2116763991"}, {"name": "Minhui Xue", "authorId": "2837434"}, {"name": "Yuqing Zhang", "authorId": "2155342827"}], "n_citations": 23}, "snippets": ["The membership inference attack (Shokri et al., 2016) proposed by Shokri et al. constructs shadow models by imitating the behavior of target model, and then trains the attack model according to their outputs, which can infer the existence of a specific data record in the training set. Salem et al. (Salem et al., 2018) optimized the attack by decreasing the number of shadow models from n to 1. Nasr et al. (Nasr et al., 2018) designed a white-box membership inference attack against centralized and FL by exploiting the vulnerability of stochastic gradient descent algorithm. Zari et al. [57] also demonstrated the passive membership inference attack in FL. Chen et al. (Chen et al., 2019) provided a generic membership inference attack to attack the deep generative models and judged whether the image belongs to the victim's training set by devising a calibration technique. Leino et al. (Leino et al., 2019) utilized the model overfitting impact to design a white-box membership inference attack, and demonstrated that this attack outperforms prior black-box methods. Pyrgelis et al. (Pyrgelis et al., 2017) focused on the feasibility of membership inference attacks on aggregate location time-series, and used adversarial tasks based on game theory to infer membership information on location information. Some membership inference attacks (Hu et al., 2021), (Hayes et al., 2017), (Hilprecht et al., 2019) attacked generative model under the white-box and blackbox settings."], "score": 0.97412109375}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 256868849, "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Derui Zhu", "authorId": "47770210"}, {"name": "Dingfan Chen", "authorId": "153642281"}, {"name": "Jens Grossklags", "authorId": "1718732"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 14}, "snippets": ["Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution."], "score": 0.98046875}], "table": null}, {"title": "Comparing Black-Box and White-Box Approaches", "tldr": "Black-box and white-box membership inference attacks differ fundamentally in their access requirements, methodologies, and effectiveness, with white-box approaches generally achieving higher attack precision when models appear well-generalized. The access-accuracy tradeoff between these approaches has important implications for privacy risk assessment and defense development. (9 sources)", "text": "\nMembership inference attacks can be categorized into black-box and white-box approaches, which differ significantly in their assumptions, methodologies, and effectiveness. Understanding these differences is crucial for properly assessing privacy risks and developing appropriate defenses.\n\n## Access Requirements\n\nThe fundamental distinction between these approaches lies in the adversary's access level to the target model:\n\n- **Black-box attacks** operate with limited access, typically allowing the adversary to only query the model and observe its outputs without knowledge of internal parameters or architecture <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper>. This represents a more realistic threat model in deployed machine learning services.\n\n- **White-box attacks** assume full access to the target model's internal structure, parameters, and architecture <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper>. This provides the adversary with a more comprehensive view of the model's behavior.\n\n## Attack Effectiveness\n\nResearch consistently shows differences in the effectiveness of these approaches:\n\n1. **Attack strength**: White-box attacks generally outperform black-box attacks in terms of accuracy and confidence <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>. Multiple studies confirm that white-box membership inference has stronger attack capabilities than black-box approaches <Paper corpusId=\"248870291\" paperTitle=\"(Zhang et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"268819379\" paperTitle=\"(Alshantti et al., 2024)\" isShortName></Paper>.\n\n2. **Well-generalized models**: One of the most significant differences is that white-box attacks remain effective even when models appear to generalize well. Nasr et al. demonstrated that even well-generalized models that resist black-box attacks remain vulnerable to white-box methods <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper> <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This suggests that model memorization occurs even without overt overfitting.\n\n3. **High-precision identification**: Leino et al. showed that white-box attacks can be calibrated for high precision, making them effective at confidently identifying positive membership when black-box attacks cannot <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n4. **Performance gap**: In generative models, Chen et al. demonstrated that full white-box membership inference attacks are consistently more effective than grey-box and black-box attacks <Paper corpusId=\"268819379\" paperTitle=\"(Alshantti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221203089\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper>.\n\n## Theoretical Considerations\n\nDespite the empirical advantage of white-box attacks, some researchers have explored theoretical boundaries between these approaches:\n\n1. **Theoretical equivalence**: Sablayrolles et al. developed theoretical foundations suggesting that under certain assumptions about model parameter distribution, optimal black-box attacks can approximate the performance of white-box attacks <Paper corpusId=\"174799799\" paperTitle=\"(Sablayrolles et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256868849\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n2. **Empirical reality**: Despite this theoretical equivalence, empirical results consistently show that white-box attacks remain more powerful in practical scenarios <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n## Practical Implications\n\nThe differences between black-box and white-box approaches have important implications for privacy protection:\n\n1. **Defense evaluation**: When evaluating privacy defenses, considering both attack types provides a more comprehensive privacy assessment. Many defenses that appear effective against black-box attacks fail against white-box attacks <Paper corpusId=\"195699554\" paperTitle=\"(Leino et al., 2019)\" isShortName></Paper>.\n\n2. **Risk assessment**: Black-box attacks represent more realistic threats in deployed machine learning services, where users typically have limited access. However, white-box attacks show the upper bound of privacy risk, particularly important for sensitive applications <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\n3. **Defense approaches**: Effective defenses must consider both attack types. For instance, Chen et al. found that differential privacy and strong L2 regularization are among the few defenses effective against both black-box and white-box attacks <Paper corpusId=\"259342605\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nThe evolution of both attack types has led to increasingly sophisticated approaches, with black-box attacks becoming more effective with fewer assumptions and white-box attacks exploiting increasingly subtle signals from model internals. This ongoing arms race underscores the importance of robust privacy protection mechanisms in machine learning systems that can withstand both attack vectors.", "citations": [{"id": "(Zhang et al._1, 2022)", "paper": {"corpus_id": 248870291, "title": "Black-box based limited query membership inference attack", "year": 2022, "venue": "IEEE Access", "authors": [{"name": "Yu Zhang", "authorId": "2153635068"}, {"name": "Huaping Zhou", "authorId": "2146383582"}, {"name": "Pengyan Wang", "authorId": "2297143623"}, {"name": "Gaoming Yang", "authorId": "2105622"}], "n_citations": 3}, "snippets": ["Membership inference attacks are divided into white-box attacks [11](Nasr et al., 2018)(Leino et al., 2019)(Hayes et al., 2017) and blackbox attacks (Yurtsever et al., 2019)(Shokri et al., 2016)(Chen et al., 2019) according to background knowledge. In the case of a black-box attack, the attacker does not know the structure and network of the target model, and can only predict the result of the input data by interacting with the machine learning algorithm. On the contrary, the attacker can fully access all the structure and parameters of the target model under the white box attack, which has a very strong attack capability."], "score": 0.98193359375}, {"id": "(Grosso et al., 2022)", "paper": {"corpus_id": 247595200, "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Ganesh Del Grosso", "authorId": "2033372042"}, {"name": "Hamid Jalalzai", "authorId": "52195298"}, {"name": "Georg Pichler", "authorId": "134026950"}, {"name": "C. Palamidessi", "authorId": "1722055"}, {"name": "P. Piantanida", "authorId": "1743922"}], "n_citations": 22}, "snippets": ["The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."], "score": 0.97265625}, {"id": "(Leino et al., 2019)", "paper": {"corpus_id": 195699554, "title": "Stolen Memories: Leveraging Model Memorization for Calibrated White-Box Membership Inference", "year": 2019, "venue": "USENIX Security Symposium", "authors": [{"name": "Klas Leino", "authorId": "35802340"}, {"name": "Matt Fredrikson", "authorId": "2623167"}], "n_citations": 272}, "snippets": ["Training data membership inference attacks aim to determine whether a given data point was present in the training data used to build a model. This is a privacy threat in itself, but membership inference vulnerability has come to be seen as a more general indicator of whether a model leaks private information [26,37,47], and is closely related to the guarantee provided by differential privacy [25].\n\nTo date, most membership inference attacks follow the socalled shadow model (shadow-bb) approach [37]. Briefly, in the shadow model approach, membership inference is cast as a supervised learning problem, where the adversary is given a data point and its true label, and aims to predict a binary label indicating membership status. To do so, the adversary trains a set of shadow models to replicate the functionality of the target model, and trains an attack model from training data derived from the shadow models' outputs on points used to train the shadow models and points not previously seen by the shadow models. Subsequently, this attack was extended to the white-box setting [32] by including activation and gradient information obtained from the target model as features for the attack model. However, because gradient information may be very specific to a particular model, this white-box attack does not use shadow models and instead assumes that the adversary already knows a significant portion of the target model's training data."], "score": 0.9677734375}, {"id": "(Alshantti et al., 2024)", "paper": {"corpus_id": 268819379, "title": "Privacy Re\u2010Identification Attacks on Tabular GANs", "year": 2024, "venue": "Security and Privacy", "authors": [{"name": "Abdallah Alshantti", "authorId": "2145327284"}, {"name": "Adil Rasheed", "authorId": "2265981625"}, {"name": "Frank Westad", "authorId": "2243101143"}], "n_citations": 4}, "snippets": ["Membership inference attacks (MIAs) were first devised by Shokri et al. (2017), in which classification models are targeted in a black-box setting. In MIA, an attacker is provided with a query dataset from an unknown source and attempts to identify the data records that were used for training a machine learning model. Whereas, in white-box membership inference attacks the perpetrator has access to the internals of the training model and uses this knowledge to make better-informed decisions about the membership of the records in the query set. It has been demonstrated that white-box MIAs on a neural network's stochastic gradient descent optimizer are far more powerful than the standard black-box attacks (Nasr et al., 2019). Moreover, membership inference is increasingly explored in the federated learning domain, in which a model is trained in a decentralised manner by several actors (Melis et al., 2019). Federated learning can however introduce data leakages which add up to the privacy concerns. In addition, it has been observed that while overfitting does contribute to the data leakage (Shokri et al., 2017), it was also shown that a wellgeneralisable model is still largely susceptible to effective membership attacks (Yeom et al., 2017; Long et al., 2018), thus motivating for further studies on membership inference attacks.\n\nMore recently, the application of membership inference attacks has also been extended to generative models. Hayes et al. (2019) presented the first study of membership inference attacks on GANs whereby it was found that white-box attacks can exploit the overfitting in generative models, thus shedding light on the magnitude of privacy leakage issue in generative applications. The membership inference attacks are further bolstered by Hilprecht et al. (2019), who formulated a new type of MIAs based on Monte Carlo and demonstrated their successfulness against GAN models. Meanwhile, Chen et al. (2020b) comprehensively studied MIAs on various GAN implementations and demonstrated that full white-box MIAs are persistently more effective than grey-box and black-box attacks."], "score": 0.966796875}, {"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 259342605, "title": "Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction", "year": 2023, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Zitao Chen", "authorId": "1583483502"}, {"name": "K. Pattabiraman", "authorId": "1715185"}], "n_citations": 24}, "snippets": ["Depending on the adversary capabilities, MIAs can be divided into black-box (Shokri et al., 2016), (Yeom et al., 2017), [17], (Carlini et al., 2021), (Song et al., 2020), (Choquette-Choo et al., 2020), [47], (Li et al., 2020) and white-box attacks (Leino et al., 2019), (Jayaraman et al., 2020), (Nasr et al., 2018). The former has access only into the output of the target model while the latter has visibility into information such as the internal model gradients to facilitate membership inference. Black-box MIA assumes a more realistic adversary, and hence is hence widely adopted in prior defense studies (Jia et al., 2019), (Tang et al., 2021), (Nasr et al., 2018) (and in HAMP). Such attacks can be mounted by either shadow-training (Shokri et al., 2016), (Nasr et al., 2018), (Yeom et al., 2017) or computing statistical metrics based on the partial knowledge of the private dataset (Song et al., 2020), (Choquette-Choo et al., 2020), (Li et al., 2020). Many of those attacks require full or partial access to the output scores by the model, and may be defeated if the model only reveals the prediction label. This motivates a new class of attacks called, label-only attacks, which can be launched either with (Choquette-Choo et al., 2020) or without (Li et al., 2020) partial knowledge of the membership information."], "score": 0.974609375}, {"id": "(Chen et al., 2019)", "paper": {"corpus_id": 221203089, "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models", "year": 2019, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Dingfan Chen", "authorId": "153642281"}, {"name": "Ning Yu", "authorId": "2052212417"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 406}, "snippets": ["Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data)."], "score": 0.0}, {"id": "(Sablayrolles et al., 2019)", "paper": {"corpus_id": 174799799, "title": "White-box vs Black-box: Bayes Optimal Strategies for Membership Inference", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alexandre Sablayrolles", "authorId": "3469062"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "C. Schmid", "authorId": "2462253"}, {"name": "Y. Ollivier", "authorId": "1734570"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}], "n_citations": 368}, "snippets": ["Membership inference determines, given a sample and trained parameters of a machine learning model, whether the sample was part of the training set. In this paper, we derive the optimal strategy for membership inference with a few assumptions on the distribution of the parameters. We show that optimal attacks only depend on the loss function, and thus black-box attacks are as good as white-box attacks. As the optimal strategy is not tractable, we provide approximations of it leading to several inference methods, and show that existing membership inference methods are coarser approximations of this optimal strategy. Our membership attacks outperform the state of the art in various settings, ranging from a simple logistic regression to more complex architectures and datasets, such as ResNet-101 and Imagenet."], "score": 0.0}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 256868849, "title": "Data Forensics in Diffusion Models: A Systematic Analysis of Membership Privacy", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Derui Zhu", "authorId": "47770210"}, {"name": "Dingfan Chen", "authorId": "153642281"}, {"name": "Jens Grossklags", "authorId": "1718732"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 14}, "snippets": ["Membership Inference Attack (MIA) was first introduced by Shokri et al. (Shokri et al., 2016). It focuses on attacking classification models in a black-box setting, where the attacker has access to the victim model's full response, including confidence scores for all classes, for a given query sample as input. Existing works have developed various approaches in attacking both white-box (Nasr et al., 2018), [28] as well as black-box (Shokri et al., 2016), (Sablayrolles et al., 2019), (Yeom et al., 2017), (Salem et al., 2018), (Song et al., 2020) classification models. Black-box MIAs typically involve training shadow models to extract member and non-member characteristics or utilizing easily accessible information such as losses as the membership indicator. White-box attacks, on the other hand, utilize the target model's internals (e.g., sample gradients) to construct membership scores. In particular, it has been shown that the sample loss generally can serve as a discriminative signal that tells apart members from non-members (Yeom et al., 2017). Sablayrolles et al. (Sablayrolles et al., 2019) further showed that black-box attacks can approximate the performance of white-box MIA under certain assumptions on the model parameter distribution."], "score": 0.98046875}], "table": null}, {"title": "Recent Developments and Variants", "tldr": "Membership inference attacks have evolved significantly with innovations in label-only approaches that operate without confidence scores, multi-query strategies that leverage multiple data points, and specialized attacks against complex systems like generative models and federated learning. These developments have progressively reduced the assumptions required by attackers while improving attack performance, highlighting the persistent and growing nature of privacy risks in machine learning systems. (14 sources)", "text": "\nMembership inference attacks continue to evolve, with researchers developing increasingly sophisticated approaches that operate under more realistic constraints while achieving better performance. Several notable recent developments have expanded the scope and effectiveness of these attacks:\n\n## Label-Only Approaches\n\nA significant advancement in membership inference has been the development of label-only attacks that operate without access to confidence scores:\n\n1. **Decision-based attacks**: Li and Zhang introduced attacks that can determine membership status using only the predicted labels rather than confidence scores <Paper corpusId=\"237563320\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. These decision-based approaches include transfer attacks and boundary attacks that can sometimes outperform traditional score-based methods.\n\n2. **Perturbation-based methods**: Choquette-Choo et al. demonstrated that label-only membership inference attacks can evaluate the robustness of model predictions under perturbations to obtain membership signals <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>. These attacks break many defense mechanisms that rely on \"confidence masking\" while leaving predicted labels unchanged.\n\n3. **Effectiveness against defenses**: Label-only attacks have proven remarkably effective against various defensive strategies, with research showing that only differential privacy and strong L2 regularization consistently thwart these attacks <Paper corpusId=\"220831381\" paperTitle=\"(Choquette-Choo et al., 2020)\" isShortName></Paper>.\n\n## Multi-Query Strategies\n\nRecent research has explored more sophisticated query strategies that leverage multiple related data points:\n\n1. **Differential comparison**: Hui et al. introduced BlindMI, which uses a novel differential comparison approach to extract membership semantics without shadow models <Paper corpusId=\"230523638\" paperTitle=\"(Hui et al., 2021)\" isShortName></Paper>. The method generates non-member datasets through transformations and then measures how samples affect set distances when moved between datasets.\n\n2. **Training trajectory analysis**: Liu et al. developed TrajectoryMIA, which exploits membership signals generated throughout a model's training process instead of relying solely on the final model <Paper corpusId=\"251953448\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This approach has demonstrated significantly higher true-positive rates at low false-positive rates compared to existing methods.\n\n3. **Multiple model comparisons**: Carlini et al. created the Likelihood Ratio Attack (LiRA) that carefully combines multiple techniques to achieve substantially more powerful attacks at low false-positive rates <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\n## Attacks in Specialized Domains\n\nMembership inference has expanded beyond traditional supervised learning to specialized domains:\n\n1. **Generative models**: Researchers have demonstrated that generative models, including GANs and diffusion models, are also vulnerable to membership inference attacks <Paper corpusId=\"221203089\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper> <Paper corpusId=\"52211986\" paperTitle=\"(Hayes et al., 2017)\" isShortName></Paper>. White-box attacks on generative models have been shown to be consistently more effective than grey-box and black-box approaches.\n\n2. **Federated learning**: Nasr et al. investigated membership inference in federated learning settings, showing how adversarial participants can successfully run active membership inference attacks against other participants <Paper corpusId=\"133091488\" paperTitle=\"(Nasr et al., 2018)\" isShortName></Paper>. Melis et al. further demonstrated that model updates in collaborative learning leak unintended information about participants' training data <Paper corpusId=\"53099247\" paperTitle=\"(Melis et al., 2018)\" isShortName></Paper>.\n\n3. **Large language models**: Recent work has expanded membership inference to complex models like large language models, with researchers developing attacks that rely solely on generated text outputs <Paper corpusId=\"272367776\" paperTitle=\"(Wen et al., 2024)\" isShortName></Paper>.\n\n## Active and Adversarial Approaches\n\nMore recent attacks have incorporated active techniques where the adversary takes a more proactive role:\n\n1. **Data poisoning for enhanced inference**: Tram\u00e8r et al. demonstrated that an adversary who can poison even a small fraction (<0.1%) of the training dataset can boost inference attack performance by 1-2 orders of magnitude <Paper corpusId=\"247922814\" paperTitle=\"(Tramer et al., 2022)\" isShortName></Paper>.\n\n2. **Adversarial examples for membership signals**: Grosso et al. developed an approach that measures the magnitude of perturbation needed to create adversarial examples as a signal of training membership <Paper corpusId=\"247595200\" paperTitle=\"(Grosso et al., 2022)\" isShortName></Paper>.\n\n3. **Targeted attacks**: Long et al. introduced targeted membership inference attacks that focus on identifying specific vulnerable records rather than attacking indiscriminately, showing that even when aggregate attack precision is near the baseline, individual records can be highly vulnerable <Paper corpusId=\"226266600\" paperTitle=\"(Long et al., 2020)\" isShortName></Paper>.\n\n## Evaluation Methodologies\n\nResearchers have also improved how membership inference attacks are evaluated:\n\n1. **Privacy risk scoring**: Song et al. introduced a more fine-grained privacy analysis approach using a privacy risk score metric that measures an individual sample's likelihood of being a training member <Paper corpusId=\"214623088\" paperTitle=\"(Song et al., 2020)\" isShortName></Paper>.\n\n2. **Evaluation at low false-positive rates**: Carlini et al. argued that attacks should be evaluated by their true-positive rate at low false-positive rates (e.g., \u22640.1%), rather than using average-case accuracy metrics that fail to characterize whether the attack can confidently identify members <Paper corpusId=\"244920593\" paperTitle=\"(Carlini et al., 2021)\" isShortName></Paper>.\n\nThese recent developments highlight the dynamic nature of the field, with researchers continually reducing the assumptions required by attackers while improving attack performance. This ongoing evolution underscores the persistent privacy risks in machine learning systems and the need for robust defenses that can withstand increasingly sophisticated attacks.", "citations": [{"id": "(Li et al., 2020)", "paper": {"corpus_id": 237563320, "title": "Membership Leakage in Label-Only Exposures", "year": 2020, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 246}, "snippets": ["Shokri et al. (Shokri et al., 2016) present the first membership inference attack against machine learning models. The general idea behind this attack is to use multiple shadow models to generate data to train multiple attack models (one for each class). These attack models take the target sample's confidence scores as input and output its membership status, i.e., member or non-member. Salem et al. (Salem et al., 2018) later present another attack by gradually relaxing the assumptions made by Shokri et al. (Shokri et al., 2016) achieving a model and data independent membership inference. In addition, there are several other subsequent score-based membership inference attacks (Hui et al., 2021)(Li et al., 2020)35,(Song et al., 2019)(Yeom et al., 2017)", "Existing membership inference attacks leverage the confidence scores returned by the model as their inputs (score-based attacks). However, these attacks can be easily mitigated if the model only exposes the predicted label, i.e., the final model decision. In this paper, we propose decision-based membership inference attacks and demonstrate that label-only exposures are also vulnerable to membership leakage. In particular, we develop two types of decision-based attacks, namely transfer attack and boundary attack."], "score": 0.97900390625}, {"id": "(Choquette-Choo et al., 2020)", "paper": {"corpus_id": 220831381, "title": "Label-Only Membership Inference Attacks", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Christopher A. Choquette-Choo", "authorId": "1415982317"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Nicolas Papernot", "authorId": "1967156"}], "n_citations": 516}, "snippets": ["Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees."], "score": 0.0}, {"id": "(Hui et al., 2021)", "paper": {"corpus_id": 230523638, "title": "Practical Blind Membership Inference Attack via Differential Comparisons", "year": 2021, "venue": "Network and Distributed System Security Symposium", "authors": [{"name": "Bo Hui", "authorId": "2000495031"}, {"name": "Yuchen Yang", "authorId": "46285766"}, {"name": "Haolin Yuan", "authorId": "2114128628"}, {"name": "P. Burlina", "authorId": "1765936"}, {"name": "N. Gong", "authorId": "144516687"}, {"name": "Yinzhi Cao", "authorId": "3139121"}], "n_citations": 122}, "snippets": ["Membership inference (MI) attacks affect user privacy by inferring whether given data samples have been used to train a target learning model, e.g., a deep neural network. There are two types of MI attacks in the literature, i.e., these with and without shadow models. The success of the former heavily depends on the quality of the shadow model, i.e., the transferability between the shadow and the target; the latter, given only blackbox probing access to the target model, cannot make an effective inference of unknowns, compared with MI attacks using shadow models, due to the insufficient number of qualified samples labeled with ground truth membership information. \nIn this paper, we propose an MI attack, called BlindMI, which probes the target model and extracts membership semantics via a novel approach, called differential comparison. The high-level idea is that BlindMI first generates a dataset with nonmembers via transforming existing samples into new samples, and then differentially moves samples from a target dataset to the generated, non-member set in an iterative manner. If the differential move of a sample increases the set distance, BlindMI considers the sample as non-member and vice versa. \nBlindMI was evaluated by comparing it with state-of-the-art MI attack algorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20% when compared to state-of-the-art on some datasets, such as Purchase-50 and Birds-200, in the blind setting where the adversary does not know the target model's architecture and the target dataset's ground truth labels. We also show that BlindMI can defeat state-of-the-art defenses."], "score": 0.0}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 251953448, "title": "Membership Inference Attacks by Exploiting Loss Trajectory", "year": 2022, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Yiyong Liu", "authorId": "2182511319"}, {"name": "Zhengyu Zhao", "authorId": "2277275"}, {"name": "M. Backes", "authorId": "144588806"}, {"name": "Yang Zhang", "authorId": "2145954003"}], "n_citations": 111}, "snippets": ["Machine learning models are vulnerable to membership inference attacks in which an adversary aims to predict whether or not a particular sample was contained in the target model's training dataset. Existing attack methods have commonly exploited the output information (mostly, losses) solely from the given target model. As a result, in practical scenarios where both the member and non-member samples yield similarly small losses, these methods are naturally unable to differentiate between them. To address this limitation, in this paper, we propose a new attack method, called TrajectoryMIA, which can exploit the membership information from the whole training process of the target model for improving the attack performance. To mount the attack in the common black-box setting, we leverage knowledge distillation, and represent the membership information by the losses evaluated on a sequence of intermediate models at different distillation epochs, namely distilled loss trajectory, together with the loss from the given target model. Experimental results over different datasets and model architectures demonstrate the great advantage of our attack in terms of different metrics. For example, on CINIC-10, our attack achieves at least 6 times higher true-positive rate at a low false-positive rate of 0.1% than existing methods. Further analysis demonstrates the general effectiveness of our attack in more strict scenarios."], "score": 0.9716796875}, {"id": "(Carlini et al., 2021)", "paper": {"corpus_id": 244920593, "title": "Membership Inference Attacks From First Principles", "year": 2021, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Steve Chien", "authorId": "2059189068"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Shuang Song", "authorId": "144206374"}, {"name": "A. Terzis", "authorId": "1763579"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}], "n_citations": 708}, "snippets": ["A membership inference attack allows an adversary to query a trained machine learning model to predict whether or not a particular example was contained in the model\u2019s training dataset. These attacks are currently evaluated using average-case \"accuracy\" metrics that fail to characterize whether the attack can confidently identify any members of the training set. We argue that attacks should instead be evaluated by computing their true-positive rate at low (e.g., \u2264 0.1%) false-positive rates, and find most prior attacks perform poorly when evaluated in this way. To address this we develop a Likelihood Ratio Attack (LiRA) that carefully combines multiple ideas from the literature. Our attack is $10\\times$ more powerful at low false-positive rates, and also strictly dominates prior attacks on existing metrics."], "score": 0.0}, {"id": "(Chen et al., 2019)", "paper": {"corpus_id": 221203089, "title": "GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models", "year": 2019, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Dingfan Chen", "authorId": "153642281"}, {"name": "Ning Yu", "authorId": "2052212417"}, {"name": "Yang Zhang", "authorId": "2145954003"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 406}, "snippets": ["Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data)."], "score": 0.0}, {"id": "(Hayes et al., 2017)", "paper": {"corpus_id": 52211986, "title": "LOGAN: Membership Inference Attacks Against Generative Models", "year": 2017, "venue": "Proceedings on Privacy Enhancing Technologies", "authors": [{"name": "Jamie Hayes", "authorId": "9200194"}, {"name": "Luca Melis", "authorId": "145557680"}, {"name": "G. Danezis", "authorId": "1722262"}, {"name": "Emiliano De Cristofaro", "authorId": "1728207"}], "n_citations": 515}, "snippets": ["Abstract Generative models estimate the underlying distribution of a dataset to generate realistic samples according to that distribution. In this paper, we present the first membership inference attacks against generative models: given a data point, the adversary determines whether or not it was used to train the model. Our attacks leverage Generative Adversarial Networks (GANs), which combine a discriminative and a generative model, to detect overfitting and recognize inputs that were part of training datasets, using the discriminator\u2019s capacity to learn statistical differences in distributions. We present attacks based on both white-box and black-box access to the target model, against several state-of-the-art generative models, over datasets of complex representations of faces (LFW), objects (CIFAR-10), and medical images (Diabetic Retinopathy). We also discuss the sensitivity of the attacks to different training parameters, and their robustness against mitigation strategies, finding that defenses are either ineffective or lead to significantly worse performances of the generative models in terms of training stability and/or sample quality."], "score": 0.0}, {"id": "(Nasr et al., 2018)", "paper": {"corpus_id": 133091488, "title": "Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Milad Nasr", "authorId": "3490923"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Amir Houmansadr", "authorId": "1972973"}], "n_citations": 1452}, "snippets": ["Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies."], "score": 0.0}, {"id": "(Melis et al., 2018)", "paper": {"corpus_id": 53099247, "title": "Exploiting Unintended Feature Leakage in Collaborative Learning", "year": 2018, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Luca Melis", "authorId": "145557680"}, {"name": "Congzheng Song", "authorId": "3469125"}, {"name": "Emiliano De Cristofaro", "authorId": "1728207"}, {"name": "Vitaly Shmatikov", "authorId": "1723945"}], "n_citations": 1482}, "snippets": ["Collaborative machine learning and related techniques such as federated learning allow multiple participants, each with his own training dataset, to build a joint model by training locally and periodically exchanging model updates. We demonstrate that these updates leak unintended information about participants' training data and develop passive and active inference attacks to exploit this leakage. First, we show that an adversarial participant can infer the presence of exact data points -- for example, specific locations -- in others' training data (i.e., membership inference). Then, we show how this adversary can infer properties that hold only for a subset of the training data and are independent of the properties that the joint model aims to capture. For example, he can infer when a specific person first appears in the photos used to train a binary gender classifier. We evaluate our attacks on a variety of tasks, datasets, and learning configurations, analyze their limitations, and discuss possible defenses."], "score": 0.0}, {"id": "(Wen et al., 2024)", "paper": {"corpus_id": 272367776, "title": "Membership Inference Attacks Against In-Context Learning", "year": 2024, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Rui Wen", "authorId": "2054749404"}, {"name": "Zheng Li", "authorId": "2146247989"}, {"name": "Michael Backes", "authorId": "2257034706"}, {"name": "Yang Zhang", "authorId": "2257291195"}], "n_citations": 14}, "snippets": ["Adapting Large Language Models (LLMs) to specific tasks introduces concerns about computational efficiency, prompting an exploration of efficient methods such as In-Context Learning (ICL). However, the vulnerability of ICL to privacy attacks under realistic assumptions remains largely unexplored. In this work, we present the first membership inference attack tailored for ICL, relying solely on generated texts without their associated probabilities. We propose four attack strategies tailored to various constrained scenarios and conduct extensive experiments on four popular large language models. Empirical results show that our attacks can accurately determine membership status in most cases, e.g., 95% accuracy advantage against LLaMA, indicating that the associated risks are much higher than those shown by existing probability-based attacks. Additionally, we propose a hybrid attack that synthesizes the strengths of the aforementioned strategies, achieving an accuracy advantage of over 95% in most cases. Furthermore, we investigate three potential defenses targeting data, instruction, and output. Results demonstrate combining defenses from orthogonal dimensions significantly reduces privacy leakage and offers enhanced privacy assurances."], "score": 0.0}, {"id": "(Tramer et al., 2022)", "paper": {"corpus_id": 247922814, "title": "Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets", "year": 2022, "venue": "Conference on Computer and Communications Security", "authors": [{"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "R. Shokri", "authorId": "2520493"}, {"name": "Ayrton San Joaquin", "authorId": "2161243047"}, {"name": "Hoang M. Le", "authorId": "2145242672"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Sanghyun Hong", "authorId": "2111053680"}, {"name": "Nicholas Carlini", "authorId": "2483738"}], "n_citations": 123}, "snippets": ["We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data. Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8\u00d7 more precise inference on all other users' otherwise-private data points. Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data."], "score": 0.0}, {"id": "(Grosso et al., 2022)", "paper": {"corpus_id": 247595200, "title": "Leveraging Adversarial Examples to Quantify Membership Information Leakage", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Ganesh Del Grosso", "authorId": "2033372042"}, {"name": "Hamid Jalalzai", "authorId": "52195298"}, {"name": "Georg Pichler", "authorId": "134026950"}, {"name": "C. Palamidessi", "authorId": "1722055"}, {"name": "P. Piantanida", "authorId": "1743922"}], "n_citations": 22}, "snippets": ["The Softmax Response, Modified Entropy and Loss strategies are black-box strategies, since the attacker only requires access to the target sample, its label and the output of the model (either the logits or Softmax response of the model). On the other hand, the Gradient Norm and Adversarial Distance strategies are white-box, as the attacker requires access to the model parameters in order to compute gradients of the loss function. In addition to white-box access to the target model, for some of the strategies we consider, the attacker requires its own training set of samples, including a subset of the training set used by the target model. This is the case for the Grad w, Grad x, Intermediate Outputs (Int. Outs) and the White-Box (WB) attacker."], "score": 0.97265625}, {"id": "(Long et al., 2020)", "paper": {"corpus_id": 226266600, "title": "A Pragmatic Approach to Membership Inferences on Machine Learning Models", "year": 2020, "venue": "European Symposium on Security and Privacy", "authors": [{"name": "Yunhui Long", "authorId": "3147214"}, {"name": "Lei Wang", "authorId": "2152507640"}, {"name": "Diyue Bu", "authorId": "3203018"}, {"name": "Vincent Bindschaedler", "authorId": "3094927"}, {"name": "Xiaofeng Wang", "authorId": "50141047"}, {"name": "Haixu Tang", "authorId": "2112389071"}, {"name": "Carl A. Gunter", "authorId": "1785347"}, {"name": "Kai Chen", "authorId": "145126969"}], "n_citations": 83}, "snippets": ["Membership Inference Attacks (MIAs) aim to determine the presence of a record in a machine learning model's training data by querying the model. Recent work has demonstrated the effectiveness of MIA on various machine learning models and corresponding defenses have been proposed. However, both attacks and defenses have focused on an adversary that indiscriminately attacks all the records without regard to the cost of false positives or negatives. In this work, we revisit membership inference attacks from the perspective of a pragmatic adversary who carefully selects targets and make predictions conservatively. We design a new evaluation methodology that allows us to evaluate the membership privacy risk at the level of individuals and not only in aggregate. We experimentally demonstrate that highly vulnerable records exist even when the aggregate attack precision is close to 50% (baseline). Specifically, on the MNIST dataset, our pragmatic adversary achieves a precision of 95.05% whereas the prior attack only achieves a precision of 51.7%."], "score": 0.0}, {"id": "(Song et al., 2020)", "paper": {"corpus_id": 214623088, "title": "Systematic Evaluation of Privacy Risks of Machine Learning Models", "year": 2020, "venue": "USENIX Security Symposium", "authors": [{"name": "Liwei Song", "authorId": "144173853"}, {"name": "Prateek Mittal", "authorId": "143615345"}], "n_citations": 375}, "snippets": ["In this paper, we focus on the membership inference attack, where the adversary aims to guess whether an input sample was used to train the target machine learning model or not (Shokri et al., 2016)(Yeom et al., 2017). It poses a severe privacy risk as the membership can reveal an individual's sensitive information [3](Pyrgelis et al., 2017). For example, participation in a hospital's health analytic training set means that an individual was once a patient in that hospital. Shokri et al. (Shokri et al., 2016) conducted membership inference attacks against machine learning classifiers in the black-box manner, where the adversary only observes prediction outputs of the target model. They formalize the attack as a classification problem and train dedicated neural network (NN) classifiers to distinguish between training members and non-members. The research community has since extended the idea of membership inference attacks to generative models (Chen et al., 2019)(Hayes et al., 2017)(Hilprecht et al., 2019)(Wu et al., 2019), to differentially private models (Jayaraman et al., 2019)(Rahman et al., 2018), to decentralized settings where the models are trained across multiple users without sharing their data (Melis et al., 2018)(Nasr et al., 2018), and to white-box settings where the adversary also has the access to the target model's architecture and weights (Nasr et al., 2018)."], "score": 0.97607421875}], "table": null}], "cost": 1.00932}}
