{"better_query": "What is the impact of 4-bit quantization on neural network inference accuracy compared to 8-bit and 16-bit quantization?", "better_answer": {"sections": [{"title": "Introduction to Neural Network Quantization", "tldr": "Neural network quantization reduces model precision from higher-bit floating-point formats to lower-bit integer representations to improve inference efficiency. This process involves converting weights and activations to lower precision formats like 16-bit, 8-bit, or 4-bit to reduce memory requirements and computational costs. (LLM Memory)", "text": "\nNeural network quantization is a technique that reduces the numerical precision of a model's parameters (weights) and activations from high-precision floating-point formats (typically 32-bit) to lower-precision representations. This process helps in decreasing the memory footprint, power consumption, and computational requirements of neural networks while attempting to maintain acceptable accuracy levels. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn standard neural networks, weights and activation values are typically stored as 32-bit floating-point numbers (FP32), which provide high precision but require significant memory and computational resources. Quantization converts these values to lower precision formats such as 16-bit (FP16 or INT16), 8-bit (INT8), or 4-bit (INT4) representations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe quantization process involves mapping the continuous range of floating-point values to a discrete set of values that can be represented with fewer bits. This mapping can be uniform (equal spacing between quantized values) or non-uniform (varying spacing based on the distribution of values). The process typically includes steps like calibration, where the range of values is analyzed to determine optimal scaling factors, and actual conversion to the target precision. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nQuantization can be applied at different stages: post-training quantization (PTQ) occurs after a model has been fully trained at high precision, while quantization-aware training (QAT) incorporates the effects of quantization during the training process to help the model adapt to reduced precision. Each approach offers different trade-offs between implementation complexity and final model accuracy. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Accuracy Impact of Different Quantization Levels", "tldr": "The accuracy impact of 4-bit quantization is significantly more pronounced than 8-bit or 16-bit quantization across most neural network architectures. While 8-bit and 16-bit quantization typically maintain accuracy within 1% of full precision models, 4-bit quantization can cause accuracy drops ranging from 1-12% depending on network architecture, dataset complexity, and whether mitigation techniques are employed. (18 sources)", "text": "\nNeural network quantization demonstrates varying degrees of accuracy impact depending on the precision level used. The most commonly studied quantization levels are 16-bit, 8-bit, and 4-bit, with each showing distinct patterns of accuracy degradation compared to full-precision (32-bit) models.\n\nAt the highest quantization level, 16-bit precision generally preserves model accuracy almost perfectly, with minimal to no measurable degradation in performance across various architectures and tasks <Paper corpusId=\"235166247\" paperTitle=\"(Shomron et al., 2021)\" isShortName></Paper> <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>. This makes 16-bit quantization a safe choice when balancing efficiency and accuracy.\n\nThe 8-bit quantization level has become widely adopted in real-world implementations, as it consistently maintains accuracy very close to full-precision models. Multiple studies confirm that 8-bit quantized networks typically experience only 0.1-0.5% accuracy drop compared to their full-precision counterparts <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper> <Paper corpusId=\"247595112\" paperTitle=\"(Nagel et al., 2022)\" isShortName></Paper>. This negligible accuracy loss explains why 8-bit inference has been extensively deployed in commercial GPUs and NPUs <Paper corpusId=\"220891847\" paperTitle=\"(Cho et al., 2020)\" isShortName></Paper> <Paper corpusId=\"213621056\" paperTitle=\"(Medina et al., 2020)\" isShortName></Paper> <Paper corpusId=\"4202768\" paperTitle=\"(Jouppi et al., 2017)\" isShortName></Paper>. Krishnamoorthi observed that \"at 8-bits of precision, post training quantization schemes provide close to floating point accuracy\" <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe transition to 4-bit quantization, however, reveals more significant accuracy challenges. While 4-bit quantization offers substantial memory and computational benefits, it typically introduces noticeable accuracy degradation compared to higher precision formats. The magnitude of this degradation varies considerably based on several factors:\n\n1. **Network Architecture**: Deeper networks generally experience more severe accuracy loss with 4-bit quantization. For instance, VGG-16 demonstrates a dramatic drop from 93.3% accuracy at full precision to approximately 20% when using 4-bit weights <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. The accuracy degradation becomes more pronounced as network depth increases.\n\n2. **Dataset Complexity**: Networks trained on simpler datasets like MNIST or CIFAR-10 tend to be more resilient to 4-bit quantization than those trained on complex datasets like ImageNet. Guo et al. found no quantization loss on simple datasets but observed approximately 12% performance loss on ImageNet <Paper corpusId=\"235266137\" paperTitle=\"(Guo et al., 2021)\" isShortName></Paper>.\n\n3. **Quantization Approach**: When comparing different quantization techniques, the accuracy impact varies significantly. Post-training quantization (PTQ) to 4-bit precision without additional techniques typically causes more substantial accuracy drops than quantization-aware training (QAT) <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>. Several studies report accuracy drops of 2-5% across various models when moving from 8-bit to 4-bit weight quantization <Paper corpusId=\"261049574\" paperTitle=\"(Frumkin et al., 2023)\" isShortName></Paper>.\n\nFor specific model families, the impact of 4-bit quantization presents interesting patterns. Large language models (LLMs) appear to maintain most of their emergent abilities at 4-bit precision, showing only minimal degradation in reasoning and instruction-following tasks <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>. However, when quantization precision drops to 2-bit, performance typically degrades to near-random levels.\n\nThe accuracy impact also depends on the specific layer being quantized. For example, in SqueezeNet, 4-bit quantization of weights in the 'fire6-expand3x3' layer caused accuracy to drop to 42.5%, while the same quantization applied to the 'fire9-expand3x3' layer resulted in only a 5% accuracy reduction <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\nVarious studies have quantified the typical accuracy loss with 4-bit quantization:\n- McKinstry et al. found that 4-bit networks required significantly longer fine-tuning compared to 8-bit networks to match or exceed pretrained floating-point accuracy <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n- Wei et al. observed a maximum accuracy loss of 4.0% for 4-bit quantized models, with higher variance in training stability compared to higher-precision models <Paper corpusId=\"73479694\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.\n- Trusov et al. reported that 4-bit quantization resulted in 95.0% accuracy compared to 95.4% for 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>.\n- Tumialis et al. found that 4-bit networks performed approximately four percentage points worse than full-precision networks <Paper corpusId=\"275038383\" paperTitle=\"(Tumialis et al., 2024)\" isShortName></Paper>.\n\nIn quantitative terms, the relative root mean square error (RRMSE) when decreasing from 8-bit to 4-bit uniform quantization increases by factors of 17.2x and 16x for different network groups, highlighting the increased noise introduced at lower precision <Paper corpusId=\"259274857\" paperTitle=\"(Sabri et al., 2023)\" isShortName></Paper>.\n\nOverall, the empirical evidence demonstrates a clear pattern: while 8-bit and 16-bit quantization can be applied with minimal accuracy impact across most network architectures, 4-bit quantization introduces more significant accuracy challenges that may require additional techniques to mitigate, especially for complex models and datasets.", "citations": [{"id": "(Shomron et al., 2021)", "paper": {"corpus_id": 235166247, "title": "Post-Training Sparsity-Aware Quantization", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Gil Shomron", "authorId": "19190632"}, {"name": "F. Gabbay", "authorId": "3147903"}, {"name": "Samer Kurzum", "authorId": "2104448639"}, {"name": "U. Weiser", "authorId": "1948330"}], "n_citations": 34}, "snippets": ["Mapping FP32 models to INT8 using uniform PTQ yields models with negligible accuracy degradation; however, reducing precision below 8 bits with PTQ is challenging, as accuracy degradation becomes noticeable, due to the increase in quantization noise."], "score": 0.96826171875}, {"id": "(Yee et al., 2020)", "paper": {"corpus_id": 225583435, "title": "Face Recognition and Machine Learning at the Edge", "year": 2020, "venue": "IOP Conference Series: Materials Science and Engineering", "authors": [{"name": "Joanne Ling Sin Yee", "authorId": "2004954858"}, {"name": "U. U. Sheikh", "authorId": "2412102"}, {"name": "M. Mokji", "authorId": "1951977"}, {"name": "S. Rahman", "authorId": "2111988471"}], "n_citations": 2}, "snippets": ["For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."], "score": 0.98193359375}, {"id": "(Gennari et al., 2019)", "paper": {"corpus_id": 57573783, "title": "DSConv: Efficient Convolution Operator", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Marcelo Gennari", "authorId": "2065676996"}, {"name": "Roger Fawcett", "authorId": "144924636"}, {"name": "V. Prisacariu", "authorId": "2824784"}], "n_citations": 67}, "snippets": ["The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture", "Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted."], "score": 0.9765625}, {"id": "(Nagel et al., 2022)", "paper": {"corpus_id": 247595112, "title": "Overcoming Oscillations in Quantization-Aware Training", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Markus Nagel", "authorId": "41229153"}, {"name": "Marios Fournarakis", "authorId": "2090358941"}, {"name": "Yelysei Bondarenko", "authorId": "2112207572"}, {"name": "Tijmen Blankevoort", "authorId": "83133279"}], "n_citations": 108}, "snippets": ["Despite its clear power and latency benefits, quantization comes at the cost of added noise due to the reduced precision. However, researchers in recent years have shown that neural networks are robust to this noise and can be quantized to 8-bits with a minimal drop in accuracy using post-training quantization techniques (PTQ) (Nagel et al., 2019)(Banner et al., 2018)Nahshan et al., 2020). PTQ can be very efficient and, generally, only requires access to a small calibration dataset, but suffers when applied to low-bit quantization (\u2264 4-bits) of neural networks."], "score": 0.9580078125}, {"id": "(Cho et al., 2020)", "paper": {"corpus_id": 220891847, "title": "McDRAM v2: In-Dynamic Random Access Memory Systolic Array Accelerator to Address the Large Model Problem in Deep Neural Networks on the Edge", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "Seunghwan Cho", "authorId": "2111878754"}, {"name": "Haerang Choi", "authorId": "1716222897"}, {"name": "Eunhyeok Park", "authorId": "2292315"}, {"name": "Hyunsung Shin", "authorId": "10307183"}, {"name": "Sungjoo Yoo", "authorId": "1808405"}], "n_citations": 41}, "snippets": ["For CNNs, inferences with int8 precision has been widely adopted in an increasing number of real-world GPUs [17], [34]/NPUs (Medina et al., 2020), (Jouppi et al., 2017), as int8 precision has been shown to be sufficient for GEMM operation in the inference of various CNNs in recent studies. It has also been found that sub-8-bit quantization of the inference of a CNN does not lead to significant accuracy loss. For example, with the ImageNet classification dataset (Deng et al., 2009), [38] reports an activation 4-bit and weight 4-bit quantization of an ResNet-50 (He et al., 2015) with a 0.2% top-1 accuracy loss compared to the case of FP32, in which an activation 2-bit and weight 2-bit quantization was done and a 3.2% top-1 accuracy loss was obtained."], "score": 0.962890625}, {"id": "(Medina et al., 2020)", "paper": {"corpus_id": 213621056, "title": "Habana Labs Purpose-Built AI Inference and Training Processor Architectures: Scaling AI Training Systems Using Standard Ethernet With Gaudi Processor", "year": 2020, "venue": "IEEE Micro", "authors": [{"name": "Eitan Medina", "authorId": "1379959105"}, {"name": "Eran Dagan", "authorId": "1581369413"}], "n_citations": 37}, "snippets": ["The growing computational requirements of AI applications are challenging today's general-purpose CPU and GPU architectures and driving the need for purpose-built, programmable AI solutions. Habana Labs designed its Goya processor to meet the high throughput/low latency demands of Inference workloads, and its Gaudi processor for throughput combined with massive scale up and scale out capability needed to speed training workloads efficiently. To address the need for scaling training, Habana is the first AI chip developer to integrate standard Ethernet onto a training processor."], "score": 0.0}, {"id": "(Jouppi et al., 2017)", "paper": {"corpus_id": 4202768, "title": "In-datacenter performance analysis of a tensor processing unit", "year": 2017, "venue": "International Symposium on Computer Architecture", "authors": [{"name": "Norman P. Jouppi", "authorId": "2360527413"}, {"name": "C. Young", "authorId": "39660914"}, {"name": "Nishant Patil", "authorId": "2056800684"}, {"name": "David Patterson", "authorId": "2265549299"}, {"name": "Gaurav Agrawal", "authorId": "2265528597"}, {"name": "Raminder Bajwa", "authorId": "2265528365"}, {"name": "Sarah Bates", "authorId": "2265528604"}, {"name": "Suresh Bhatia", "authorId": "2265528570"}, {"name": "Nan Boden", "authorId": "2265528912"}, {"name": "Al Borchers", "authorId": "2265529409"}, {"name": "Rick Boyle", "authorId": "2265529523"}, {"name": "Pierre-luc Cantin", "authorId": "2265531532"}, {"name": "Clifford Chao", "authorId": "2265528729"}, {"name": "Chris Clark", "authorId": "2265550963"}, {"name": "Jeremy Coriell", "authorId": "2265528756"}, {"name": "Mike Daley", "authorId": "2265529003"}, {"name": "Matt Dau", "authorId": "2265528872"}, {"name": "Jeffrey Dean", "authorId": "2265529729"}, {"name": "Ben Gelb", "authorId": "2265528537"}, {"name": "Taraneh Ghaemmaghami", "authorId": "1398213038"}, {"name": "Rajendra Gottipati", "authorId": "2265528754"}, {"name": "William Gulland", "authorId": "2265528752"}, {"name": "Robert Hagmann", "authorId": "2265528749"}, {"name": "C. Richard Ho", "authorId": "2361904494"}, {"name": "Doug Hogberg", "authorId": "2265528842"}, {"name": "John Hu", "authorId": "2265596858"}, {"name": "R. Hundt", "authorId": "1701671"}, {"name": "Dan Hurt", "authorId": "2265528853"}, {"name": "Julian Ibarz", "authorId": "2265529394"}, {"name": "A. Jaffey", "authorId": "10433955"}, {"name": "Alek Jaworski", "authorId": "2265529292"}, {"name": "Alexander Kaplan", "authorId": "2265551723"}, {"name": "Harshit Khaitan", "authorId": "2265529388"}, {"name": "Daniel Killebrew", "authorId": "35002044"}, {"name": "Andy Koch", "authorId": "2265529856"}, {"name": "Naveen Kumar", "authorId": "2265642844"}, {"name": "Steve Lacy", "authorId": "2265529386"}, {"name": "James Laudon", "authorId": "2258707080"}, {"name": "James Law", "authorId": "2265529416"}, {"name": "Diemthu Le", "authorId": "2265529515"}, {"name": "Chris Leary", "authorId": "2265529268"}, {"name": "Zhuyuan Liu", "authorId": "2265576530"}, {"name": "Kyle Lucke", "authorId": "2265532187"}, {"name": "Alan Lundin", "authorId": "2265529249"}, {"name": "Gordon MacKean", "authorId": "2265528983"}, {"name": "Adriana Maggiore", "authorId": "2265528640"}, {"name": "Maire Mahony", "authorId": "2265529551"}, {"name": "Kieran Miller", "authorId": "2265619145"}, {"name": "R. Nagarajan", "authorId": "1395811464"}, {"name": "Ravi Narayanaswami", "authorId": "1398213032"}, {"name": "Ray Ni", "authorId": "2265529536"}, {"name": "Kathy Nix", "authorId": "2265531761"}, {"name": "Thomas Norrie", "authorId": "3222376"}, {"name": "Mark Omernick", "authorId": "3175815"}, {"name": "Narayana Penukonda", "authorId": "1398212990"}, {"name": "Andy Phelps", "authorId": "2265528744"}, {"name": "Jonathan Ross", "authorId": "2265552909"}, {"name": "Matt Ross", "authorId": "2265552588"}, {"name": "Amir Salek", "authorId": "2265528580"}, {"name": "Emad Samadiani", "authorId": "9364218"}, {"name": "Chris Severn", "authorId": "2265528562"}, {"name": "Gregory Sizikov", "authorId": "2265528974"}, {"name": "Matthew Snelham", "authorId": "2265528625"}, {"name": "Jed Souter", "authorId": "2265529370"}, {"name": "Dan Steinberg", "authorId": "2265532375"}, {"name": "Andy Swing", "authorId": "1394189636"}, {"name": "Mercedes Tan", "authorId": "2265616791"}, {"name": "Gregory Thorson", "authorId": "2265532008"}, {"name": "Bo Tian", "authorId": "2265529994"}, {"name": "Horia Toma", "authorId": "2265529013"}, {"name": "Erick Tuttle", "authorId": "2265528972"}, {"name": "Vijay Vasudevan", "authorId": "2265528643"}, {"name": "Richard Walter", "authorId": "2265529413"}, {"name": "Walter Wang", "authorId": "2265617722"}, {"name": "Eric Wilcox", "authorId": "2265529247"}, {"name": "Doe Hyun Yoon", "authorId": "2360529871"}], "n_citations": 4643}, "snippets": ["Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X\u201330X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X\u201380X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU."], "score": 0.0}, {"id": "(Krishnamoorthi, 2018)", "paper": {"corpus_id": 49356451, "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Raghuraman Krishnamoorthi", "authorId": "2065915235"}], "n_citations": 1021}, "snippets": ["We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations", "It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments."], "score": 0.98876953125}, {"id": "(Charan et al., 2020)", "paper": {"corpus_id": 218788236, "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution", "year": 2020, "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": [{"name": "Gouranga Charan", "authorId": "91781917"}, {"name": "Abinash Mohanty", "authorId": "2942785"}, {"name": "Xiaocong Du", "authorId": "3457252"}, {"name": "Gokul Krishnan", "authorId": "144828251"}, {"name": "R. Joshi", "authorId": "145553390"}, {"name": "Yu Cao", "authorId": "1965873861"}], "n_citations": 30}, "snippets": ["We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."], "score": 0.9853515625}, {"id": "(Guo et al., 2021)", "paper": {"corpus_id": 235266137, "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Qingyu Guo", "authorId": "2892749"}, {"name": "Yuan Wang", "authorId": "2156166711"}, {"name": "Xiaoxin Cui", "authorId": "39658679"}], "n_citations": 2}, "snippets": ["We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios."], "score": 0.9814453125}, {"id": "(Frumkin et al., 2023)", "paper": {"corpus_id": 261049574, "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "N. Frumkin", "authorId": "2094690757"}, {"name": "Dibakar Gope", "authorId": "2193705"}, {"name": "D. Marculescu", "authorId": "92419662"}], "n_citations": 17}, "snippets": ["Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models."], "score": 0.9658203125}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 259937594, "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Peiyu Liu", "authorId": "2108129670"}, {"name": "Zikang Liu", "authorId": "2119618242"}, {"name": "Ze-Feng Gao", "authorId": "9136116"}, {"name": "Dawei Gao", "authorId": "2162036220"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Bolin Ding", "authorId": "1696332"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 35}, "snippets": ["Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K."], "score": 0.98291015625}, {"id": "(Berti et al., 2025)", "paper": {"corpus_id": 276903421, "title": "Emergent Abilities in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leonardo Berti", "authorId": "2229196235"}, {"name": "Flavio Giorgi", "authorId": "2325903265"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 6}, "snippets": ["At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."], "score": 0.96875}, {"id": "(McKinstry et al., 2018)", "paper": {"corpus_id": 52197199, "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "J. McKinstry", "authorId": "46571359"}, {"name": "S. K. Esser", "authorId": "2357931"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "John V. Arthur", "authorId": "2248110488"}, {"name": "Izzet B. Yildiz", "authorId": "3121907"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 94}, "snippets": ["FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."], "score": 0.97265625}, {"id": "(Wei et al., 2019)", "paper": {"corpus_id": 73479694, "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications", "year": 2019, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Xin Wei", "authorId": "1382778048"}, {"name": "Wenchao Liu", "authorId": "2109188137"}, {"name": "Lei Chen", "authorId": "2146072742"}, {"name": "Long Ma", "authorId": "145044878"}, {"name": "He Chen", "authorId": "92779203"}, {"name": "Zhuang Yin", "authorId": "2199301922"}], "n_citations": 28}, "snippets": ["The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance."], "score": 0.97509765625}, {"id": "(Trusov et al., 2020)", "paper": {"corpus_id": 221655691, "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices", "year": 2020, "venue": "International Conference on Pattern Recognition", "authors": [{"name": "A. Trusov", "authorId": "1441088616"}, {"name": "E. Limonova", "authorId": "37239110"}, {"name": "Dmitry Slugin", "authorId": "89361905"}, {"name": "D. Nikolaev", "authorId": "35331830"}, {"name": "V. Arlazarov", "authorId": "35343060"}], "n_citations": 17}, "snippets": ["4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."], "score": 0.98291015625}, {"id": "(Tumialis et al., 2024)", "paper": {"corpus_id": 275038383, "title": "The Impact of 8- and 4-Bit Quantization on the Accuracy and Silicon Area Footprint of Tiny Neural Networks", "year": 2024, "venue": "Electronics", "authors": [{"name": "Pawe\u0142 Tumialis", "authorId": "2337212061"}, {"name": "Marcel Skierkowski", "authorId": "2337207657"}, {"name": "Jakub Przychodny", "authorId": "2337211715"}, {"name": "Pawe\u0142 Obszarski", "authorId": "2337209535"}], "n_citations": 1}, "snippets": ["Thanks to this, the accuracy of the 4-bit networks did not differ significantly from the 8-bit ones and their results were approximately four percentage points worse than those of the full precision networks."], "score": 0.96728515625}, {"id": "(Sabri et al., 2023)", "paper": {"corpus_id": 259274857, "title": "ReDy: A Novel ReRAM-centric Dynamic Quantization Approach for Energy-efficient CNN Inference", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Mohammad Sabri", "authorId": "2058852328"}, {"name": "Marc Riera", "authorId": "143640059"}, {"name": "Antonio Gonz'alez", "authorId": "2220761969"}], "n_citations": 2}, "snippets": ["Similarly, when decreasing the numerical precision of the uniform quantization to 4-bits, the RRMSE increases to 28.81% and 7.89% for groups (a) and (b) respectively, which corresponds to an increment of 17.2x and 16x respect the 8-bit uniform quantization."], "score": 0.96533203125}], "table": null}, {"title": "Factors Affecting Quantization Impact", "tldr": "Several key factors determine how severely 4-bit quantization affects neural network accuracy, including network architecture complexity, layer position within the network, and whether weights or activations are being quantized. (9 sources)", "text": "\nThe impact of quantization on neural network accuracy varies significantly based on multiple factors that must be considered when implementing lower-precision models. Understanding these factors is crucial for predicting and mitigating potential accuracy degradation, particularly at 4-bit precision levels.\n\n## Network Architecture and Complexity\nThe architecture and depth of a neural network significantly influence its resilience to quantization. Deeper networks generally experience more severe accuracy degradation when quantized to lower bit-widths. As demonstrated in research by Charan et al., \"The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to ~20% in 4-bit (INT4) quantization\" <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This dramatic reduction in accuracy highlights how architectural complexity can amplify quantization effects.\n\n## Layer Position and Sensitivity\nThe position of layers within a network plays a critical role in quantization sensitivity. Studies have shown that early layers are particularly sensitive to quantization errors. Caldeira et al. found that \"model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network\" <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>. This insight has led to the development of mixed-precision approaches that apply higher precision to sensitive layers while using lower precision for less sensitive layers.\n\nPosso et al. further confirmed that \"the binarization of early layers diminishes the generalization capabilities of the neural network\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>, suggesting that maintaining higher precision in early layers is crucial for preserving model performance.\n\n## Differential Sensitivity: Weights vs. Activations\nA particularly important insight is that weights and activations respond differently to quantization. Multiple studies have demonstrated that activations are generally more sensitive to precision reduction than weights. Posso et al. observed that \"activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>.\n\nThis differential sensitivity has led to asymmetric quantization approaches. Pasricha noted that \"Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters\" <Paper corpusId=\"271571530\" paperTitle=\"(Pasricha, 2024)\" isShortName></Paper>. In an analysis where weights were restricted to binary values but activation precision was varied, researchers found that \"accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated,\" leading to the conclusion that \"4-bit activations... allowed reducing bitwidths without significantly reducing model accuracy\" <Paper corpusId=\"271571530\" paperTitle=\"(Pasricha, 2024)\" isShortName></Paper> <Paper corpusId=\"235795063\" paperTitle=\"(Sunny et al., 2021)\" isShortName></Paper>.\n\nRecent work on large language models supports this observation, with Lang et al. reporting that \"LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision\" <Paper corpusId=\"273821224\" paperTitle=\"(Lang et al., 2024)\" isShortName></Paper>.\n\n## Task-Specific Sensitivity\nThe specific task a model performs can influence its quantization sensitivity. For instance, Posso et al. found that \"quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy\" <Paper corpusId=\"271050128\" paperTitle=\"(Posso et al., 2024)\" isShortName></Paper>, suggesting that certain predictive tasks may be more resilient to quantization than others.\n\nIn facial recognition applications, Caldeira et al. observed extreme performance variance at 4-bit precision: \"a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization\" <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>. This highlights how task-specific factors can dramatically influence quantization tolerance.\n\n## Information Bottleneck Effects\nFrom an information theory perspective, quantization creates an information bottleneck. Lorenzen et al. observed that \"For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly... which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network\" <Paper corpusId=\"235623747\" paperTitle=\"(Lorenzen et al., 2021)\" isShortName></Paper>. This explains why some models experience catastrophic accuracy drops below certain precision thresholds.\n\n## Error Propagation Characteristics\nInterestingly, quantization can have unexpected effects on error resilience in certain hardware implementations. Xiao et al. found that \"the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network\" because \"The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next\" <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This demonstrates how quantization can sometimes provide beneficial properties in specific hardware contexts, despite reducing overall precision.\n\nUnderstanding these factors helps explain why \"Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation\" <Paper corpusId=\"211146310\" paperTitle=\"(Shkolnik et al., 2020)\" isShortName></Paper>, and provides guidance for developing effective mitigation strategies.", "citations": [{"id": "(Charan et al., 2020)", "paper": {"corpus_id": 218788236, "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution", "year": 2020, "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": [{"name": "Gouranga Charan", "authorId": "91781917"}, {"name": "Abinash Mohanty", "authorId": "2942785"}, {"name": "Xiaocong Du", "authorId": "3457252"}, {"name": "Gokul Krishnan", "authorId": "144828251"}, {"name": "R. Joshi", "authorId": "145553390"}, {"name": "Yu Cao", "authorId": "1965873861"}], "n_citations": 30}, "snippets": ["We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."], "score": 0.9853515625}, {"id": "(Caldeira et al., 2024)", "paper": {"corpus_id": 267035013, "title": "Model Compression Techniques in Biometrics Applications: A Survey", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Eduarda Caldeira", "authorId": "2219025387"}, {"name": "Pedro C. Neto", "authorId": "2044499858"}, {"name": "Marco Huber", "authorId": "1516862195"}, {"name": "N. Damer", "authorId": "2265721"}, {"name": "Ana F. Sequeira", "authorId": "145279873"}], "n_citations": 10}, "snippets": ["The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models", ".When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly."], "score": 0.97607421875}, {"id": "(Posso et al., 2024)", "paper": {"corpus_id": 271050128, "title": "Real-Time Spacecraft Pose Estimation Using Mixed-Precision Quantized Neural Network on COTS Reconfigurable MPSoC", "year": 2024, "venue": "IEEE International New Circuits and Systems Conference", "authors": [{"name": "Julien Posso", "authorId": "2164124197"}, {"name": "Guy Bois", "authorId": "2259651671"}, {"name": "Yvon Savaria", "authorId": "2246316919"}], "n_citations": 0}, "snippets": ["Further experiments have revealed that activations are notably more sensitive to low-bit quantization, with quantization errors below 4 bits frequently causing orientation decoding errors and disrupting the neural network's training process", "Our findings also indicate that the binarization of early layers diminishes the generalization capabilities of the neural network", "Therefore, quantization below 8 bits has a more pronounced impact on orientation accuracy than on position accuracy."], "score": 0.95703125}, {"id": "(Pasricha, 2024)", "paper": {"corpus_id": 271571530, "title": "Optical Computing for Deep Neural Network Acceleration: Foundations, Recent Developments, and Emerging Directions", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "S. Pasricha", "authorId": "1681003"}], "n_citations": 0}, "snippets": ["Studies have shown that activations are more susceptible to bitwidth reduction than weight parameters. Therefore, an alternative for increasing inference accuracy is to not binarize activations, while keeping the weights binarized. To determine the appropriate activation bitwidth (precision), an analysis was performed in (Sunny et al., 2021), where weights were restricted to binary (1-bit) values, but the bit precision level of the activations was altered from 1-bit to 16-bits, across various CNN models and learning tasks. During CNN model training, weights were binarized during the forward and backward propagations but not during the parameter update step, because keeping high precision weights during the updates is necessary for stochastic gradient descent (SGD) to work efficiently. After training, all weights were in binary format, while the precision of input activations was varied. Figure 9(a) shows the results of varying activation bitwidth across four different CNN models and their datasets. It was observed that the accuracy had notable change initially as activations bits were increased, but this gain in accuracy soon saturated. Based on the results, 1-bit weights were considered along with 4-bit activations, which allowed reducing bitwidths without significantly reducing model accuracy."], "score": 0.953125}, {"id": "(Sunny et al., 2021)", "paper": {"corpus_id": 235795063, "title": "ROBIN: A Robust Optical Binary Neural Network Accelerator", "year": 2021, "venue": "ACM Transactions on Embedded Computing Systems", "authors": [{"name": "Febin P. Sunny", "authorId": "1507532841"}, {"name": "Asif Mirza", "authorId": "1752857527"}, {"name": "Mahdi Nikdast", "authorId": "2260207"}, {"name": "S. Pasricha", "authorId": "1681003"}], "n_citations": 35}, "snippets": ["Domain specific neural network accelerators have garnered attention because of their improved energy efficiency and inference performance compared to CPUs and GPUs. Such accelerators are thus well suited for resource-constrained embedded systems. However, mapping sophisticated neural network models on these accelerators still entails significant energy and memory consumption, along with high inference time overhead. Binarized neural networks (BNNs), which utilize single-bit weights, represent an efficient way to implement and deploy neural network models on accelerators. In this paper, we present a novel optical-domain BNN accelerator, named ROBIN, which intelligently integrates heterogeneous microring resonator optical devices with complementary capabilities to efficiently implement the key functionalities in BNNs. We perform detailed fabrication-process variation analyses at the optical device level, explore efficient corrective tuning for these devices, and integrate circuit-level optimization to counter thermal variations. As a result, our proposed ROBIN architecture possesses the desirable traits of being robust, energy-efficient, low latency, and high throughput, when executing BNN models. Our analysis shows that ROBIN can outperform the best-known optical BNN accelerators and many electronic accelerators. Specifically, our energy-efficient ROBIN design exhibits energy-per-bit values that are \u223c4 \u00d7 lower than electronic BNN accelerators and \u223c933 \u00d7 lower than a recently proposed photonic BNN accelerator, while a performance-efficient ROBIN design shows \u223c3 \u00d7 and \u223c25 \u00d7 better performance than electronic and photonic BNN accelerators, respectively."], "score": 0.0}, {"id": "(Lang et al., 2024)", "paper": {"corpus_id": 273821224, "title": "A Comprehensive Study on Quantization Techniques for Large Language Models", "year": 2024, "venue": "2024 4th International Conference on Artificial Intelligence, Robotics, and Communication (ICAIRC)", "authors": [{"name": "Jiedong Lang", "authorId": "2329225877"}, {"name": "Zhehao Guo", "authorId": "2329318690"}, {"name": "Shuyu Huang", "authorId": "2329202190"}], "n_citations": 12}, "snippets": ["LLM-QAT demonstrates better accuracy with a configuration of 4-bit weights, 4-bit KV caches, and 8-bit activations, compared to a uniform 4-bit setting across all precision [35]."], "score": 0.9541015625}, {"id": "(Lorenzen et al., 2021)", "paper": {"corpus_id": 235623747, "title": "Information Bottleneck: Exact Analysis of (Quantized) Neural Networks", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "S. Lorenzen", "authorId": "2048034204"}, {"name": "C. Igel", "authorId": "1748824"}, {"name": "M. Nielsen", "authorId": "2069766353"}], "n_citations": 18}, "snippets": ["For the 4-bit networks, as each neuron can take only 2^4 = 16 different values, the total number of possible states per layer decreases significantly in this setting, and as expected we see lower overall MI measurements", "However, the network accuracy has now degraded compared to the non-quantized networks (see Supplementary Material F), which indicates that the binning used in the estimation of the MI in previous experiments has discarded information vital to the network."], "score": 0.95458984375}, {"id": "(Xiao et al., 2021)", "paper": {"corpus_id": 237416732, "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]", "year": 2021, "venue": "IEEE Circuits and Systems Magazine", "authors": [{"name": "T. Xiao", "authorId": "2061135402"}, {"name": "Ben Feinberg", "authorId": "153505332"}, {"name": "C. Bennett", "authorId": "2464307"}, {"name": "V. Prabhakar", "authorId": "35839928"}, {"name": "Prashant Saxena", "authorId": "144956677"}, {"name": "V. Agrawal", "authorId": "1578156774"}, {"name": "S. Agarwal", "authorId": "3491847"}, {"name": "M. Marinella", "authorId": "30782065"}], "n_citations": 37}, "snippets": ["Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity."], "score": 0.974609375}, {"id": "(Shkolnik et al., 2020)", "paper": {"corpus_id": 211146310, "title": "Robust Quantization: One Model to Rule Them All", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Moran Shkolnik", "authorId": "2065832429"}, {"name": "Brian Chmiel", "authorId": "104387774"}, {"name": "Ron Banner", "authorId": "2607278"}, {"name": "Gil Shomron", "authorId": "19190632"}, {"name": "Yury Nahshan", "authorId": "51498210"}, {"name": "A. Bronstein", "authorId": "49791556"}, {"name": "U. Weiser", "authorId": "1948330"}], "n_citations": 76}, "snippets": ["Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods."], "score": 0.9599609375}], "table": null}, {"title": "Mitigation Techniques for 4-bit Quantization", "tldr": "Multiple techniques have been developed to mitigate the accuracy degradation in 4-bit quantization, including quantization-aware training, mixed precision approaches, and adapter fine-tuning. These methods can significantly reduce or even eliminate the accuracy gap between 4-bit and higher precision models. (10 sources)", "text": "\nNeural network quantization to 4-bit precision can cause significant accuracy degradation when applied naively, but several effective mitigation techniques have been developed to address this challenge. These approaches enable the use of 4-bit precision while maintaining accuracy comparable to higher-precision models.\n\n## Quantization-Aware Training (QAT)\n\nQuantization-aware training has emerged as one of the most effective techniques for mitigating accuracy loss at 4-bit precision. Unlike post-training quantization (PTQ), QAT incorporates quantization effects during the training process, allowing the network to adapt to lower precision. Krishnamoorthi found that most networks can obtain accuracies within 5% of 8-bit quantization through fine-tuning with 4-bit weights while keeping activations at 8-bit precision <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe Factorized Asymmetric Quantization (FAQ) method demonstrated particularly impressive results, with FAQ-trained 4-bit networks outperforming all comparable quantization methods. While these networks required significantly longer fine-tuning (approximately 110 epochs) compared to 8-bit networks, they eventually matched or exceeded the accuracy of pretrained floating-point models <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n\nMore recent approaches like LLM-QAT (Large Language Model Quantization-Aware Training) have been specifically designed for language models, showing that a 4-8-4 configuration (4-bit weights, 8-bit activations, 4-bit KV cache) of a 30B parameter model can outperform an 8-bit 13B model <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\n## Mixed Precision Approaches\n\nMixed precision quantization applies different bit-widths to different parts of the network, strategically preserving higher precision where it matters most:\n\n1. **Layer-specific quantization**: Keeping the first and last layers at higher precision while using 4-bit for intermediate layers has proven effective. Abdolrashidi et al. found that \"models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit\" <Paper corpusId=\"234337594\" paperTitle=\"(Abdolrashidi et al., 2021)\" isShortName></Paper>.\n\n2. **Parameter-specific quantization**: Maintaining higher precision for activations while using lower precision for weights can be beneficial. This asymmetric approach leverages the insight that activations are typically more sensitive to quantization than weights.\n\n## Advanced Optimization Techniques\n\nSeveral specialized optimization techniques have been developed specifically for 4-bit quantization:\n\n1. **Parameter Stateless Gradient Descent (PSGD)**: This method has demonstrated remarkable improvements for 4-bit models, outperforming alternative approaches by approximately 8% on ImageNet <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>.\n\n2. **Block-wise Quantization**: Dettmers et al. found that splitting parameters into small independently quantized blocks significantly improves the bit-level scaling trade-off for 4-bit models <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>.\n\n3. **Dynamic Bit-width (DyBit)**: This approach dynamically adjusts quantization precision and has achieved 1.97% higher ImageNet inference accuracy compared to previous state-of-the-art methods at 4-bit quantization <Paper corpusId=\"257205844\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\n## Adapter Fine-Tuning\n\nFor large language models, adapter fine-tuning has emerged as a particularly effective approach. This technique involves adding small trainable modules to an already quantized model and fine-tuning only these adapters.\n\nResearch by Dettmers et al. demonstrated that 4-bit adapter fine-tuning can fully recover the performance lost due to quantization in models ranging from 125M to 3B parameters. Their experiments on GLUE and Super-NaturalInstructions datasets showed that \"16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline\" <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\n## Combining Techniques\n\nThe most successful approaches often combine multiple mitigation strategies. For example, LLM-QAT leverages both quantization-aware training and mixed precision (4-bit weights, 8-bit activations) to achieve optimal results <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nThese various mitigation techniques have collectively transformed 4-bit quantization from a significant compromise on accuracy to a viable option that \"strikes the best balance between minimal computing/memory complexity and minimal model accuracy degradation\" <Paper corpusId=\"236087695\" paperTitle=\"(Chin et al., 2021)\" isShortName></Paper>. As noted by Shkolnik et al., while \"naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation,\" the various mitigation techniques discussed above have substantially addressed this challenge <Paper corpusId=\"211146310\" paperTitle=\"(Shkolnik et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Krishnamoorthi, 2018)", "paper": {"corpus_id": 49356451, "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Raghuraman Krishnamoorthi", "authorId": "2065915235"}], "n_citations": 1021}, "snippets": ["We note that at 8-bits of precision, post training quantization schemes provide close to floating point accuracy. In order to better understand the benefits of quantization aware training, we perform experiments to assess performance at 4 bit quantization for weights and activations", "It is interesting to see that for most networks, one can obtain accuracies within 5% of 8-bit quantization with fine tuning 4 bit weights (column 4 of Table 5). The improvements due to fine tuning are also more apparent at 4 bits. Note that activations are quantized to 8-bits in these experiments."], "score": 0.98876953125}, {"id": "(McKinstry et al., 2018)", "paper": {"corpus_id": 52197199, "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "J. McKinstry", "authorId": "46571359"}, {"name": "S. K. Esser", "authorId": "2357931"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "John V. Arthur", "authorId": "2248110488"}, {"name": "Izzet B. Yildiz", "authorId": "3121907"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 94}, "snippets": ["FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."], "score": 0.97265625}, {"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 258959117, "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zechun Liu", "authorId": "2109370860"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Changsheng Zhao", "authorId": "2112729504"}, {"name": "Ernie Chang", "authorId": "48025720"}, {"name": "Pierre Stock", "authorId": "37502184"}, {"name": "Yashar Mehdad", "authorId": "2121361882"}, {"name": "Yangyang Shi", "authorId": "152345059"}, {"name": "Raghuraman Krishnamoorthi", "authorId": "2065915235"}, {"name": "Vikas Chandra", "authorId": "144137037"}], "n_citations": 209}, "snippets": ["First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B."], "score": 0.9541015625}, {"id": "(Abdolrashidi et al., 2021)", "paper": {"corpus_id": 234337594, "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit", "year": 2021, "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "AmirAli Abdolrashidi", "authorId": "1938271"}, {"name": "Lisa Wang", "authorId": "2108552747"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "J. Malmaud", "authorId": "3274291"}, {"name": "Oleg Rybakov", "authorId": "145573927"}, {"name": "Chas Leichner", "authorId": "108381331"}, {"name": "Lukasz Lew", "authorId": "2065594000"}], "n_citations": 36}, "snippets": ["In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models", "Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks."], "score": 0.98583984375}, {"id": "(Kim et al., 2020)", "paper": {"corpus_id": 218862856, "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Jangho Kim", "authorId": "49476045"}, {"name": "Kiyoon Yoo", "authorId": "1713608836"}, {"name": "Nojun Kwak", "authorId": "3160425"}], "n_citations": 7}, "snippets": ["On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated."], "score": 0.98046875}, {"id": "(Dettmers et al., 2022)", "paper": {"corpus_id": 254853733, "title": "The case for 4-bit precision: k-bit Inference Scaling Laws", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 234}, "snippets": ["We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy."], "score": 0.982421875}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 257205844, "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference", "year": 2023, "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": [{"name": "Jiajun Zhou", "authorId": null}, {"name": "Jiajun Wu", "authorId": "2110436131"}, {"name": "Yizhao Gao", "authorId": "1939358"}, {"name": "Yuhao Ding", "authorId": "40879066"}, {"name": "Chaofan Tao", "authorId": "144259094"}, {"name": "Bo Li", "authorId": "143771569"}, {"name": "Fengbin Tu", "authorId": "1910352"}, {"name": "Kwang-Ting Cheng", "authorId": "145210800"}, {"name": "Hayden Kwok-Hay So", "authorId": "1710259"}, {"name": "Ngai Wong", "authorId": "1873081"}], "n_citations": 7}, "snippets": ["Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization"], "score": 0.96435546875}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization."], "score": 0.955078125}, {"id": "(Chin et al., 2021)", "paper": {"corpus_id": 236087695, "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Hsu-Hsun Chin", "authorId": "1597397664"}, {"name": "R. Tsay", "authorId": "1769065"}, {"name": "Hsin-I Wu", "authorId": "120155431"}], "n_citations": 5}, "snippets": ["The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation."], "score": 0.9677734375}, {"id": "(Shkolnik et al., 2020)", "paper": {"corpus_id": 211146310, "title": "Robust Quantization: One Model to Rule Them All", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Moran Shkolnik", "authorId": "2065832429"}, {"name": "Brian Chmiel", "authorId": "104387774"}, {"name": "Ron Banner", "authorId": "2607278"}, {"name": "Gil Shomron", "authorId": "19190632"}, {"name": "Yury Nahshan", "authorId": "51498210"}, {"name": "A. Bronstein", "authorId": "49791556"}, {"name": "U. Weiser", "authorId": "1948330"}], "n_citations": 76}, "snippets": ["Naively quantizing a single-precision floating point (FP32) model to 4 bits (INT4) or lower usually incurs a significant accuracy degradation. Many studies have tried to mitigate this accuracy decrease by offering different quantization methods."], "score": 0.9599609375}], "table": null}, {"title": "Trade-offs and Benefits of 4-bit Quantization", "tldr": "4-bit quantization offers substantial model size reduction and computational efficiency gains compared to 8-bit and 16-bit quantization, with models typically 2-13x smaller than full precision counterparts. Despite modest accuracy drops, 4-bit models often provide the optimal balance between efficiency and performance, making them increasingly preferable for deployment in resource-constrained environments. (9 sources)", "text": "\nNeural network quantization presents a fundamental trade-off between model precision and computational efficiency. The 4-bit quantization level offers distinct advantages that make it increasingly attractive for many real-world applications, despite the accuracy challenges discussed in previous sections.\n\n## Memory and Storage Benefits\n\nOne of the most significant advantages of 4-bit quantization is the dramatic reduction in model size. Wang et al. demonstrated that 4-bit quantized networks can be approximately 13 times smaller than their full-precision counterparts <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This substantial size reduction enables the deployment of larger model architectures on memory-constrained devices that would otherwise be unable to accommodate them.\n\n4-bit quantization also introduces increased weight sparsity, with Wang et al. reporting that 4-bit weight quantized networks can achieve more than 38% zero parameters, significantly higher than the 2.65% observed in 8-bit models <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This sparsity can be leveraged to further optimize storage and computation.\n\n## Computational Efficiency\n\nThe computational advantages of 4-bit quantization extend beyond mere storage benefits. Trusov et al. found that 4-bit quantization delivered a 48% overall inference speedup compared to full-precision models, outperforming the 39% speedup achieved with 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>. This additional performance gain can be critical for applications where inference latency is a primary concern.\n\nChudakov et al. confirmed that \"the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference\" compared to the current 8-bit standard <Paper corpusId=\"245351885\" paperTitle=\"(Chudakov et al., 2021)\" isShortName></Paper>.\n\n## The Efficiency-Accuracy Balance\n\nResearch increasingly suggests that 4-bit quantization represents an optimal trade-off point between efficiency and accuracy for many applications. Chin et al. concluded that \"4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation\" <Paper corpusId=\"236087695\" paperTitle=\"(Chin et al., 2021)\" isShortName></Paper>. This observation is particularly significant for deployment scenarios where both computational resources and accuracy requirements must be carefully balanced.\n\nAbdolrashidi et al. conducted a comprehensive analysis of quantization at different precision levels and found that \"models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models,\" leading them to suggest that \"4-bit may be a preferred numeric format for quantizing neural networks\" <Paper corpusId=\"234337594\" paperTitle=\"(Abdolrashidi et al., 2021)\" isShortName></Paper>.\n\nFor large language models specifically, Liu et al. found that \"4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size,\" with specific examples showing that \"a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B\" <Paper corpusId=\"258959117\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. This suggests that the benefits of 4-bit quantization extend even to state-of-the-art language models when proper quantization techniques are applied.\n\nDettmers et al. reinforced this finding through extensive research on parameter quantization, concluding that \"4-bit precision is almost universally optimal for total model bits and zero-shot accuracy\" <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>. This provides strong evidence that 4-bit quantization represents a sweet spot in the precision-efficiency trade-off curve.\n\n## Hardware Error Resilience\n\nInterestingly, 4-bit quantization can provide unexpected benefits in certain hardware implementations. Xiao et al. discovered that \"the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network\" in analog computing systems <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This increased resilience stems from the activation quantization, as \"the large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next\" <Paper corpusId=\"237416732\" paperTitle=\"(Xiao et al., 2021)\" isShortName></Paper>. This additional benefit could be particularly valuable for emerging hardware architectures where error tolerance is a concern.\n\n## Channel-Wise Optimization\n\nRecent research has explored mixed-precision approaches that selectively apply different bit-widths at the channel level. Motetti et al. found that \"4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less than twice the 2-bit ones while providing significant accuracy advantages\" <Paper corpusId=\"270870579\" paperTitle=\"(Motetti et al., 2024)\" isShortName></Paper>. This suggests that 4-bit precision provides a favorable return on investment in terms of the computational cost versus accuracy benefit, particularly compared to 2-bit precision which offers minimal representational capacity.\n\nIn summary, while 4-bit quantization introduces more significant accuracy challenges than 8-bit or 16-bit alternatives, its substantial benefits in terms of memory reduction, computational efficiency, and overall deployment practicality make it an increasingly attractive option for many applications. With appropriate mitigation techniques, 4-bit quantization can deliver an optimal balance between model efficiency and performance across a wide range of neural network architectures and tasks.", "citations": [{"id": "(Wang et al., 2021)", "paper": {"corpus_id": 244531228, "title": "LiteEdge: Lightweight Semantic Edge Detection Network", "year": 2021, "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "authors": [{"name": "Hao Wang", "authorId": null}, {"name": "Hasan Al-Banna Mohamed", "authorId": "118657651"}, {"name": "Zuowen Wang", "authorId": "15678675"}, {"name": "Bodo Rueckauer", "authorId": "4765464"}, {"name": "Shih-Chii Liu", "authorId": "1704961"}], "n_citations": 3}, "snippets": ["The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."], "score": 0.9599609375}, {"id": "(Trusov et al., 2020)", "paper": {"corpus_id": 221655691, "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices", "year": 2020, "venue": "International Conference on Pattern Recognition", "authors": [{"name": "A. Trusov", "authorId": "1441088616"}, {"name": "E. Limonova", "authorId": "37239110"}, {"name": "Dmitry Slugin", "authorId": "89361905"}, {"name": "D. Nikolaev", "authorId": "35331830"}, {"name": "V. Arlazarov", "authorId": "35343060"}], "n_citations": 17}, "snippets": ["4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."], "score": 0.98291015625}, {"id": "(Chudakov et al., 2021)", "paper": {"corpus_id": 245351885, "title": "Quantization noise in low bit quantization and iterative adaptation to quantization noise in quantizable neural networks", "year": 2021, "venue": "Journal of Physics: Conference Series", "authors": [{"name": "D. Chudakov", "authorId": "2146460322"}, {"name": "A. Goncharenko", "authorId": "144749173"}, {"name": "S. Alyamkin", "authorId": "81455739"}, {"name": "A. Densidov", "authorId": "2146460282"}], "n_citations": 0}, "snippets": ["At the moment, the standard is 8-bit uniform quantization. Nevertheless, the use of uniform low-bit quantization (4- and 6-bit quantization) has significant advantages in speed and resource requirements for inference."], "score": 0.9658203125}, {"id": "(Chin et al., 2021)", "paper": {"corpus_id": 236087695, "title": "A High-Performance Adaptive Quantization Approach for Edge CNN Applications", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Hsu-Hsun Chin", "authorId": "1597397664"}, {"name": "R. Tsay", "authorId": "1769065"}, {"name": "Hsin-I Wu", "authorId": "120155431"}], "n_citations": 5}, "snippets": ["The experimental results from recent research [17] [18] show a comparison of model accuracy under different bit-width quantization in the image classification task, and the results support that the 4-bit quantization models strike the best balance between minimal computing/memory complexity and minimal model accuracy degradation."], "score": 0.9677734375}, {"id": "(Abdolrashidi et al., 2021)", "paper": {"corpus_id": 234337594, "title": "Pareto-Optimal Quantized ResNet Is Mostly 4-bit", "year": 2021, "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "AmirAli Abdolrashidi", "authorId": "1938271"}, {"name": "Lisa Wang", "authorId": "2108552747"}, {"name": "Shivani Agrawal", "authorId": "3504647"}, {"name": "J. Malmaud", "authorId": "3274291"}, {"name": "Oleg Rybakov", "authorId": "145573927"}, {"name": "Chas Leichner", "authorId": "108381331"}, {"name": "Lukasz Lew", "authorId": "2065594000"}], "n_citations": 36}, "snippets": ["In this work, we analyzed how quantization at different precisions influences the compute cost-quality Pareto curves on ResNet models", "Additionally, models in 4-bit (with first and last layers in 8-bit) offer a consistent advantage over 8-bit, using both linear and quadratic cost models. This observation suggests that 4-bit may be a preferred numeric format for quantizing neural networks."], "score": 0.98583984375}, {"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 258959117, "title": "LLM-QAT: Data-Free Quantization Aware Training for Large Language Models", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zechun Liu", "authorId": "2109370860"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Changsheng Zhao", "authorId": "2112729504"}, {"name": "Ernie Chang", "authorId": "48025720"}, {"name": "Pierre Stock", "authorId": "37502184"}, {"name": "Yashar Mehdad", "authorId": "2121361882"}, {"name": "Yangyang Shi", "authorId": "152345059"}, {"name": "Raghuraman Krishnamoorthi", "authorId": "2065915235"}, {"name": "Vikas Chandra", "authorId": "144137037"}], "n_citations": 209}, "snippets": ["First, 8-bit quantization should be preferred over smaller full precision models, and PTQ methods are sufficient for this case. An 8-8-8 30B quantized model outperforms a 13B model of similar size, and should have lower latency and higher throughput in practice. This also holds for an 8-bit 13B model compared with a 16-bit 7B model. Furthermore, 4-bit models quantized using LLM-QAT should be preferred over 8-bit models of similar size. For instance a 4-8-4 LLM-QAT 30B outperforms an 8-bit LLaMA-13B, and a 4-8-8 LLM-QAT 13B is better than an 8-bit LLaMA-7B."], "score": 0.9541015625}, {"id": "(Dettmers et al., 2022)", "paper": {"corpus_id": 254853733, "title": "The case for 4-bit precision: k-bit Inference Scaling Laws", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 234}, "snippets": ["We find that it is challenging to improve the bit-level scaling trade-off, with the only improvements being the use of a small block size -- splitting the parameters into small independently quantized blocks -- and the quantization data type being used (e.g., Int vs Float). Overall, our findings show that {4-bit} precision is almost universally optimal for total model bits and zero-shot accuracy."], "score": 0.982421875}, {"id": "(Xiao et al., 2021)", "paper": {"corpus_id": 237416732, "title": "On the Accuracy of Analog Neural Network Inference Accelerators [Feature]", "year": 2021, "venue": "IEEE Circuits and Systems Magazine", "authors": [{"name": "T. Xiao", "authorId": "2061135402"}, {"name": "Ben Feinberg", "authorId": "153505332"}, {"name": "C. Bennett", "authorId": "2464307"}, {"name": "V. Prabhakar", "authorId": "35839928"}, {"name": "Prashant Saxena", "authorId": "144956677"}, {"name": "V. Agrawal", "authorId": "1578156774"}, {"name": "S. Agarwal", "authorId": "3491847"}, {"name": "M. Marinella", "authorId": "30782065"}], "n_citations": 37}, "snippets": ["Fig. 19(a) shows that the 4-bit model is substantially more resilient to state-independent errors than the 8-bit network. This results entirely from activation quantization, and not weight quantization. The large separation between the 16 activation levels effectively cuts off the propagation of accumulated cell errors from one layer to the next. The same cell error results in a smaller dot product error on average. Fig. 19(b) shows that the 4-bit network is also more resilient to state-proportional error, but here the benefit is smaller. This can be explained by the different weight value distributions of the two networks. With only 16 levels, the distribution of the 4-bit weights cannot peak as sharply at zero as the 8-bit weights, which have 256 levels. As a result, the memory cells in the 4-bit network have a significantly higher average conductance (7.52% of G max ) than the cells in the 8-bit network (1.95% of G max ). Thus, deeply quantized weights can actually be harmful for analog systems, as it leads to a higher state-proportional error per cell. This is ultimately outweighed by the benefit of quantized activations, so there is a net improvement in error sensitivity."], "score": 0.974609375}, {"id": "(Motetti et al., 2024)", "paper": {"corpus_id": 270870579, "title": "Joint Pruning and Channel-Wise Mixed-Precision Quantization for Efficient Deep Neural Networks", "year": 2024, "venue": "IEEE transactions on computers", "authors": [{"name": "Beatrice Alessandra Motetti", "authorId": "2223967064"}, {"name": "Matteo Risso", "authorId": "2114394120"}, {"name": "Alessio Burrello", "authorId": "51091982"}, {"name": "Enrico Macii", "authorId": "2231805248"}, {"name": "M. Poncino", "authorId": "1695243"}, {"name": "D. J. Pagliari", "authorId": "36204425"}], "n_citations": 4}, "snippets": ["In \"Medium\" models (central plot), very few parameters, if any, are quantized at 2 bits, since it is the least interesting precision due to the low representational capacity, but not negligible cost. In contrast, 4-bit channels represent a significant component, especially for the size and NE16 regularizers, costing less then twice the 2-bit ones while providing significant accuracy advantages."], "score": 0.951171875}], "table": null}, {"title": "Specific Application Cases and Performance Results", "tldr": "4-bit quantization impacts different neural network models and applications with varying degrees of accuracy loss. While many simple tasks and networks maintain good performance with 4-bit precision, complex models and tasks can experience significant degradation, though the efficiency benefits often justify the tradeoff. (18 sources)", "text": "\nHere are key empirical findings from various studies on 4-bit quantization across different application domains:\n\n## Image Classification Networks\n- **ResNet-34**: Achieved 74.52% top-1 accuracy with 4-bit weights and activations, surprisingly outperforming the 32-bit version (73.59%) <Paper corpusId=\"231699188\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"3455952\" paperTitle=\"(Mishra et al., 2017)\" isShortName></Paper>.\n \n- **ImageNet Models**: DyBit quantization technique achieved 1.97% higher accuracy than other state-of-the-art methods at 4-bit precision <Paper corpusId=\"257205844\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\n- **General CNN Models**: Moving from 8-bit to 4-bit weight quantization typically results in 2-5% accuracy degradation across various image classification models <Paper corpusId=\"261049574\" paperTitle=\"(Frumkin et al., 2023)\" isShortName></Paper>.\n\n- **Compressed Networks**: 4-bit quantized networks can be approximately 13 times smaller than full-precision counterparts with over 38% zero parameters, significantly higher than the 2.65% observed in 8-bit models <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\n## Specific Model Architectures\n- **SqueezeNet**: When 4-bit quantization was applied to different layers, accuracy impacts varied dramatically:\n - 'fire6-expand3x3' layer: Accuracy dropped to 42.5%\n - 'fire8-expand3x3' layer: Accuracy dropped to 72.5% \n - 'fire9-expand3x3' layer: Accuracy dropped to 85% (recommended layer for quantization)\n - No accuracy changes were observed with 8-bit and 16-bit quantization on any layer <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\n- **Spiking Neural Networks (SNNs)**: Demonstrated varying robustness to 4-bit quantization based on dataset complexity:\n - MNIST/CIFAR10: Minimal accuracy loss (0.22-1.75%)\n - ImageNet: Significant accuracy loss (8.8%)\n - Conclusion: \"Weights would better be encoded in 4-bit or over\" for larger networks <Paper corpusId=\"221103917\" paperTitle=\"(Zhou et al., 2019)\" isShortName></Paper>.\n\n- **Simple Classification Models**: On straightforward datasets like MNIST, CIFAR10, and CIFAR100, 4-bit quantization showed no measurable accuracy loss, while on complex datasets like ImageNet, it caused approximately 12% performance loss <Paper corpusId=\"235266137\" paperTitle=\"(Guo et al., 2021)\" isShortName></Paper>.\n\n## Face Recognition Systems\n- **MobileFaceNet**: Demonstrated extreme sensitivity to bit reduction, with accuracy in identification tasks dropping dramatically from 99.80% with 6-bit quantization to just 5.53% with 4-bit quantization <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>.\n\n- **Mixed Precision Quantization (MPQ)**: Using 8-bit for initial layers, 4-bit for middle layers, and 2-bit for final layers reduced computational footprint by 90% with only 0.68 percentage point performance drop on LFW dataset <Paper corpusId=\"267035013\" paperTitle=\"(Caldeira et al., 2024)\" isShortName></Paper>.\n\n## Language Models\n- **Large Language Models (LLMs)**: Emergent abilities (in-context learning, chain-of-thought reasoning, instruction following) were minimally affected by 4-bit quantization:\n - 4-bit and 8-bit results were very similar to 16-bit floating-point performance\n - 2-bit quantization caused results to drop to near-random levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n- **Adapter Fine-tuning**: 4-bit adapter fine-tuning fully recovered performance lost due to quantization in models ranging from 125M to 3B parameters on GLUE and Super-NaturalInstructions datasets <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\n## Hardware Implementation Results\n- **NeuRRAM Hardware**: Successfully implemented models with inputs to all convolutional and fully connected layers quantized to 4-bit or below, noting that accuracy could be improved by using higher precision for sensitive input and output layers <Paper corpusId=\"251644990\" paperTitle=\"(Wan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>.\n\n- **Speed and Efficiency Metrics**: 4-bit quantization delivered 48% overall inference speedup compared to full-precision models, outperforming the 39% speedup achieved with 8-bit quantization, while maintaining 95.0% accuracy compared to 95.4% with 8-bit quantization <Paper corpusId=\"221655691\" paperTitle=\"(Trusov et al., 2020)\" isShortName></Paper>.\n\n## Other Network Types\n- **Ternary Shift Networks**: Demonstrated remarkable resilience to quantization, with only 0.35% accuracy loss when moving from 4-bit (98.76% accuracy) to 2-bit precision (98.41% accuracy) <Paper corpusId=\"250607637\" paperTitle=\"(Avila et al., 2022)\" isShortName></Paper>.\n\n- **General CNN Compression**: Compression down to 4-bits resulted in accuracy drops of only 1-2% depending on architecture, with significant compression rates (5x for convolutions with 256 channel inputs) <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper>. Models using 8-bit precision typically lose only around 0.1% accuracy, while 4-bit models show 1-3% accuracy drops without retraining <Paper corpusId=\"57573783\" paperTitle=\"(Gennari et al., 2019)\" isShortName></Paper> <Paper corpusId=\"51610353\" paperTitle=\"(Gysel et al., 2018)\" isShortName></Paper>.\n\n- **4-bit Quantized Classification Models**: Showed maximum accuracy loss of 4.0% with higher training stability variance compared to higher-precision models, requiring multiple training runs to achieve satisfactory performance <Paper corpusId=\"73479694\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Liang et al., 2021)", "paper": {"corpus_id": 231699188, "title": "Pruning and Quantization for Deep Neural Network Acceleration: A Survey", "year": 2021, "venue": "Neurocomputing", "authors": [{"name": "Tailin Liang", "authorId": "123804153"}, {"name": "C. Glossner", "authorId": "145073018"}, {"name": "Lei Wang", "authorId": "66148232"}, {"name": "Shaobo Shi", "authorId": "2072685048"}], "n_citations": 700}, "snippets": ["ResNet-34 with 4-bit weights and activation obtained 74.52% top-1 accuracy while the 32-bit version is 73.59% (Mishra et al., 2017)"], "score": 0.9609375}, {"id": "(Mishra et al., 2017)", "paper": {"corpus_id": 3455952, "title": "WRPN: Wide Reduced-Precision Networks", "year": 2017, "venue": "International Conference on Learning Representations", "authors": [{"name": "Asit K. Mishra", "authorId": "35769149"}, {"name": "Eriko Nurvitadhi", "authorId": "145870092"}, {"name": "Jeffrey J. Cook", "authorId": "2115139013"}, {"name": "Debbie Marr", "authorId": "33027790"}], "n_citations": 267}, "snippets": ["For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks. Activation maps, however, occupy a large memory footprint during both the training and inference step when using mini-batches of inputs. One way to reduce this large memory footprint is to reduce the precision of activations. However, past works have shown that reducing the precision of activations hurts model accuracy. We study schemes to train networks from scratch using reduced-precision activations without hurting accuracy. We reduce the precision of activation maps (along with model parameters) and increase the number of filter maps in a layer, and find that this scheme matches or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly improve the execution efficiency (e.g. reduce dynamic memory footprint, memory bandwidth and computational energy) and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN - wide reduced-precision networks. We report results and show that WRPN scheme is better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks."], "score": 0.0}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 257205844, "title": "DyBit: Dynamic Bit-Precision Numbers for Efficient Quantized Neural Network Inference", "year": 2023, "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems", "authors": [{"name": "Jiajun Zhou", "authorId": null}, {"name": "Jiajun Wu", "authorId": "2110436131"}, {"name": "Yizhao Gao", "authorId": "1939358"}, {"name": "Yuhao Ding", "authorId": "40879066"}, {"name": "Chaofan Tao", "authorId": "144259094"}, {"name": "Bo Li", "authorId": "143771569"}, {"name": "Fengbin Tu", "authorId": "1910352"}, {"name": "Kwang-Ting Cheng", "authorId": "145210800"}, {"name": "Hayden Kwok-Hay So", "authorId": "1710259"}, {"name": "Ngai Wong", "authorId": "1873081"}], "n_citations": 7}, "snippets": ["Experimental results demonstrate that the ImageNet inference accuracy via DyBit is 1.97% higher than the state-of-the-art at 4-bit quantization"], "score": 0.96435546875}, {"id": "(Frumkin et al., 2023)", "paper": {"corpus_id": 261049574, "title": "Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "N. Frumkin", "authorId": "2094690757"}, {"name": "Dibakar Gope", "authorId": "2193705"}, {"name": "D. Marculescu", "authorId": "92419662"}], "n_citations": 17}, "snippets": ["Moving from 8-bit to 4-bit weight quantization, we see an accuracy degradation of about 2 \u2212 5% across all models."], "score": 0.9658203125}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 244531228, "title": "LiteEdge: Lightweight Semantic Edge Detection Network", "year": 2021, "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "authors": [{"name": "Hao Wang", "authorId": null}, {"name": "Hasan Al-Banna Mohamed", "authorId": "118657651"}, {"name": "Zuowen Wang", "authorId": "15678675"}, {"name": "Bodo Rueckauer", "authorId": "4765464"}, {"name": "Shih-Chii Liu", "authorId": "1704961"}], "n_citations": 3}, "snippets": ["The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."], "score": 0.9599609375}, {"id": "(Yee et al., 2020)", "paper": {"corpus_id": 225583435, "title": "Face Recognition and Machine Learning at the Edge", "year": 2020, "venue": "IOP Conference Series: Materials Science and Engineering", "authors": [{"name": "Joanne Ling Sin Yee", "authorId": "2004954858"}, {"name": "U. U. Sheikh", "authorId": "2412102"}, {"name": "M. Mokji", "authorId": "1951977"}, {"name": "S. Rahman", "authorId": "2111988471"}], "n_citations": 2}, "snippets": ["For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."], "score": 0.98193359375}, {"id": "(Zhou et al., 2019)", "paper": {"corpus_id": 221103917, "title": "Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Shibo Zhou", "authorId": "81622425"}, {"name": "Li Xiaohua", "authorId": "4107514"}, {"name": "Ying Chen", "authorId": "2118426969"}, {"name": "S. T. Chandrasekaran", "authorId": "30908225"}, {"name": "A. Sanyal", "authorId": "9442413"}], "n_citations": 96}, "snippets": ["For weight quantization, the weights were quantized to 32-bit, 8-bit, 4-bit, and 2-bit words. Thanks to our easy training models, we could retrain the deep SNNs simply following the procedure developed for conventional CNN quantization (Li, Zhang, and Liu 2016;(Rastegari et al., 2016). Specifically, the forward inference used quantized weights while the backward gradient propagation used full-precision weights. We first trained with 32-bit quantization. After the training converged, we applied 8-bit quantization and retrain the SNNs. This procedure was repeated until the 2-bit quantization. Table 4 shows the testing accuracy under weight quantization. For the models over MNIST, CIFAR10 and Ima-geNet, weight quantization caused the worst accuracy loss of 0.22%, 1.75%, and 8.8%, respectively. As comparison, we listed two typical CNN weight quantization results (Cheng et al. 2018;(Zhang et al., 2018), which indicated a similar performance degradation pace. The results demonstrated that the SNN models were robust to weight quantization for relatively small datasets and small networks. For larger networks, the weights would better be encoded in 4-bit or over."], "score": 0.96435546875}, {"id": "(Guo et al., 2021)", "paper": {"corpus_id": 235266137, "title": "Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Qingyu Guo", "authorId": "2892749"}, {"name": "Yuan Wang", "authorId": "2156166711"}, {"name": "Xiaoxin Cui", "authorId": "39658679"}], "n_citations": 2}, "snippets": ["We tried 4-bit quantization on some simple datasets(MNIST, CIFAR10, CIFAR100), and there was no quantization loss. We also tried 4-bit quantization on big dataset(ImageNet). The experiment result shows that there is about 12% performance loss, but due to the inference efficiency of 4-bit quantization, this loss may be tolerable in some application scenarios."], "score": 0.9814453125}, {"id": "(Caldeira et al., 2024)", "paper": {"corpus_id": 267035013, "title": "Model Compression Techniques in Biometrics Applications: A Survey", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Eduarda Caldeira", "authorId": "2219025387"}, {"name": "Pedro C. Neto", "authorId": "2044499858"}, {"name": "Marco Huber", "authorId": "1516862195"}, {"name": "N. Damer", "authorId": "2265721"}, {"name": "Ana F. Sequeira", "authorId": "145279873"}], "n_citations": 10}, "snippets": ["The authors prove that model performance suffers a bigger impact when the first layers are quantized due to the forward propagation of the performed approximations, allowing for stronger quantization when progressing toward the last layers of the network. The final MPQ framework comprised 8, 4 and 2-bit quantization for the initial, middle and final layers, respectively, resulting in a network with only 10% of the computational footprint of the original model and a performance drop of 0.68 percentual points on LFW. Thus, MPQ can be extremely useful in establishing a compromise between achieving good performances and having computationally efficient models", ".When b = 8 or b = 6, the performance degradation was small in the identification and verification tasks and, in some cases, the quantized models even slightly surpassed the baseline. For b = 4, the performance suffered an extreme drop. As an example, a MobileFaceNet trained in the identification task suffered a reduction in accuracy from 99.80% in 6-bit quantization to 5.53% in 4-bit quantization. Hence, in this experimental setup, 4-bit quantization is not enough to achieve good performance, showing that this precision level does not allow the model to learn how to encode the relevant features properly."], "score": 0.97607421875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 259937594, "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Peiyu Liu", "authorId": "2108129670"}, {"name": "Zikang Liu", "authorId": "2119618242"}, {"name": "Ze-Feng Gao", "authorId": "9136116"}, {"name": "Dawei Gao", "authorId": "2162036220"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Bolin Ding", "authorId": "1696332"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 35}, "snippets": ["Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K."], "score": 0.98291015625}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["Recent findings have established that 4-bit quantization for inference is possible, but leads to performance degradation relative to 16-bit [13]18]. This raises the crucial question of whether the lost performance can be recovered by conducting 4-bit adapter finetuning. We test this for two setups. The first focuses on a comparison with full 16-bit finetuning of RoBERTA and T5 models sized 125M to 3B parameters on GLUE and the Super-NaturalInstructions dataset. Results are shown in Table 3. In both datasets, we observe that 16-bit, 8-bit, and 4-bit adapter methods replicate the performance of the fully finetuned 16-bit baseline. This suggests that the performance lost due to the imprecise quantization can be fully recovered through adapter finetuning after quantization."], "score": 0.955078125}, {"id": "(Wan et al., 2022)", "paper": {"corpus_id": 251644990, "title": "A compute-in-memory chip based on resistive random-access memory", "year": 2022, "venue": "Nature", "authors": [{"name": "W. Wan", "authorId": "3408296"}, {"name": "R. Kubendran", "authorId": "3459360"}, {"name": "Clemens J. S. Schaefer", "authorId": "1592769267"}, {"name": "S. Eryilmaz", "authorId": "32842496"}, {"name": "Wenqiang Zhang", "authorId": "1780073248"}, {"name": "Dabin Wu", "authorId": "1637988624"}, {"name": "S. Deiss", "authorId": "47429806"}, {"name": "Priyanka Raina", "authorId": "50501653"}, {"name": "H. Qian", "authorId": "72601203"}, {"name": "B. Gao", "authorId": "152632389"}, {"name": "Siddharth Joshi", "authorId": "145940386"}, {"name": "Huaqiang Wu", "authorId": "13135960"}, {"name": "H. P. Wong", "authorId": "144175897"}, {"name": "G. Cauwenberghs", "authorId": "2702388"}], "n_citations": 503}, "snippets": ["To efficiently implement the models on NeuRRAM, inputs to all convolutional and fully connected layers are quantized to 4-bit or below. The input bit-precisions of all the models are summarized in Table 1. We perform the quantized training using the parameterized clipping activation technique 46. The accuracies of some of our quantized models are lower than that of the state-of-the-art quantized model because we apply <4-bit quantization to the most sensitive input and output layers of the neural networks, which have been reported to cause large accuracy degradation and are thus often excluded from low-precision quantization 46,54. To obtain better accuracy for quantized models, one can use higher precision for sensitive input and output layers, apply more advanced quantization techniques, and use more optimized data preprocessing, data augmentation and regularization techniques during training."], "score": 0.9560546875}, {"id": "(Esser et al., 2019)", "paper": {"corpus_id": 67788003, "title": "Learned Step Size Quantization", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "S. K. Esser", "authorId": "2357931"}, {"name": "J. McKinstry", "authorId": "46571359"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 810}, "snippets": ["Deep networks run with low precision operations at inference time offer power and space advantages over high precision alternatives, but need to overcome the challenge of maintaining high accuracy as precision decreases. Here, we present a method for training such networks, Learned Step Size Quantization, that achieves the highest accuracy to date on the ImageNet dataset when using models, from a variety of architectures, with weights and activations quantized to 2-, 3- or 4-bits of precision, and that can train 3-bit models that reach full precision baseline accuracy. Our approach builds upon existing methods for learning weights in quantized networks by improving how the quantizer itself is configured. Specifically, we introduce a novel means to estimate and scale the task loss gradient at each weight and activation layer's quantizer step size, such that it can be learned in conjunction with other network parameters. This approach works using different levels of precision as needed for a given system and requires only a simple modification of existing training code."], "score": 0.0}, {"id": "(Trusov et al., 2020)", "paper": {"corpus_id": 221655691, "title": "Fast Implementation of 4-bit Convolutional Neural Networks for Mobile Devices", "year": 2020, "venue": "International Conference on Pattern Recognition", "authors": [{"name": "A. Trusov", "authorId": "1441088616"}, {"name": "E. Limonova", "authorId": "37239110"}, {"name": "Dmitry Slugin", "authorId": "89361905"}, {"name": "D. Nikolaev", "authorId": "35331830"}, {"name": "V. Arlazarov", "authorId": "35343060"}], "n_citations": 17}, "snippets": ["4-bit quantization gives 95.0% accuracy and 48% overall inference speedup, while an 8-bit quantized network gives 95.4% accuracy and 39% speedup."], "score": 0.98291015625}, {"id": "(Avila et al., 2022)", "paper": {"corpus_id": 250607637, "title": "Low-bit Shift Network for End-to-End Spoken Language Understanding", "year": 2022, "venue": "Interspeech", "authors": [{"name": "Anderson R. Avila", "authorId": "36941802"}, {"name": "Khalil Bibi", "authorId": "2337172863"}, {"name": "Ruizhi Yang", "authorId": "2115428434"}, {"name": "Xinlin Li", "authorId": "2108184905"}, {"name": "Chao Xing", "authorId": "2064677272"}, {"name": "Xiao Chen", "authorId": "2135917026"}], "n_citations": 4}, "snippets": ["Quantization had a mild impact on the overall performance. For integer quantization, for example, quantizing with 16 bits showed to be slightly benefitial compared to quantizatizing with 2 bits with latter dropping accuracy only 0.06 % on the test set. Similar trend was found for the ternary shift network. While quantizing with 4 bits gives 98.76 % accuracy, quantizing with 2 bits provides 98.41 % accuracy, representing a decay of only 0.35 %."], "score": 0.97998046875}, {"id": "(Gennari et al., 2019)", "paper": {"corpus_id": 57573783, "title": "DSConv: Efficient Convolution Operator", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Marcelo Gennari", "authorId": "2065676996"}, {"name": "Roger Fawcett", "authorId": "144924636"}, {"name": "V. Prisacariu", "authorId": "2824784"}], "n_citations": 67}, "snippets": ["The results also shows that compression down to 4-bits (which in convolutions with channel size input of 256 would yield a 5x compression rate) results in an accuracy drop of only 1% to 2% depending on the architecture", "Also as known in previous research papers [26](Gysel et al., 2018), models of 8/8 bits lose only around 0.1% accuracy. For models of 5/5 and 4/4, we get a drop of 1% to 3% in accuracy. To our knowledge, this is the smallest bit-width for fast inference that has been reported when models are neither retrained nor adapted."], "score": 0.9765625}, {"id": "(Gysel et al., 2018)", "paper": {"corpus_id": 51610353, "title": "Ristretto: A Framework for Empirical Study of Resource-Efficient Inference in Convolutional Neural Networks", "year": 2018, "venue": "IEEE Transactions on Neural Networks and Learning Systems", "authors": [{"name": "Philipp Gysel", "authorId": "2656137"}, {"name": "J. Pimentel", "authorId": "34585064"}, {"name": "Mohammad Motamedi", "authorId": "50789183"}, {"name": "S. Ghiasi", "authorId": "2034444"}], "n_citations": 229}, "snippets": ["Convolutional neural networks (CNNs) have led to remarkable progress in a number of key pattern recognition tasks, such as visual scene understanding and speech recognition, that potentially enable numerous applications. Consequently, there is a significant need to deploy trained CNNs to resource-constrained embedded systems. Inference using pretrained modern deep CNNs, however, requires significant system resources, including computation, energy, and memory space. To enable efficient implementation of trained CNNs, a viable approach is to approximate the network with an implementation-friendly model with only negligible degradation in classification accuracy. We present Ristretto, a CNN approximation framework that enables empirical investigation of the tradeoff between various number representation and word width choices and the classification accuracy of the model. Specifically, Ristretto analyzes a given CNN with respect to numerical range required to represent weights, activations, and intermediate results of convolutional and fully connected layers, and subsequently, it simulates the impact of reduced word width or lower precision arithmetic operators on the model accuracy. Moreover, Ristretto can fine-tune a quantized network to further improve its classification accuracy under a given number representation and word width configuration. Given a maximum classification accuracy degradation tolerance of 1%, we use Ristretto to demonstrate that three ImageNet networks can be condensed to use 8-bit dynamic fixed point for network weights and activations. Ristretto is available as a popular open-source software project1 and has already been viewed over 1 000 times on Github as of the submission of this brief.1https://github.com/pmgysel/caffe"], "score": 0.0}, {"id": "(Wei et al., 2019)", "paper": {"corpus_id": 73479694, "title": "FPGA-Based Hybrid-Type Implementation of Quantized Neural Networks for Remote Sensing Applications", "year": 2019, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Xin Wei", "authorId": "1382778048"}, {"name": "Wenchao Liu", "authorId": "2109188137"}, {"name": "Lei Chen", "authorId": "2146072742"}, {"name": "Long Ma", "authorId": "145044878"}, {"name": "He Chen", "authorId": "92779203"}, {"name": "Zhuang Yin", "authorId": "2199301922"}], "n_citations": 28}, "snippets": ["The classification accuracy of the 4-bit quantized model has the maximum loss of 4.0%. The other quantized models obtain satisfying performance and the average results are all around 97.2%, with a 1.2% accuracy degradation, compared to floating-point types. It can be seen that the classification accuracy degradation is reduced as the bit-width increases, which may be due to the increased quantization resolution resulting in a computing error reduction. However, the performance can hardly improve after reaching the maximum as the inherent redundancy within the network is limited. The standards deviations of 4-bit and 6-bit quantized models are higher than the others. This means that the stability of training is not good when adopting a low bit-width quantization scheme and multiple trainings are required to obtain a satisfying performance."], "score": 0.97509765625}], "table": null}], "cost": 0.504324}}
