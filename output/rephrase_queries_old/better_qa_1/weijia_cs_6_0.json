{"better_query": "What are the primary differences between sparse retrieval models like BM25 and dense retrieval models such as DPR or SentenceTransformer when used in retrieval-augmented language models?", "better_answer": {"sections": [{"title": "Introduction: Retrieval Models in RAG Systems", "tldr": "Retrieval-Augmented Generation (RAG) systems use different retrieval models to fetch relevant information before generating responses. Sparse retrieval models like BM25 and dense retrieval models like DPR represent fundamentally different approaches to finding relevant context for language models. (LLM Memory)", "text": "\nRetrieval-Augmented Generation (RAG) systems enhance language model outputs by retrieving relevant information from external knowledge sources before generating responses. At the core of these systems are retrieval models that determine which pieces of information are most relevant to a given query. These retrieval models generally fall into two main categories: sparse retrieval models and dense retrieval models. Sparse retrieval models, such as BM25, rely on term-based matching and represent documents as high-dimensional, sparse vectors where most elements are zero. In contrast, dense retrieval models like Dense Passage Retrieval (DPR) and SentenceTransformer encode semantic meaning into dense vector representations where all dimensions potentially contain information. Understanding these different retrieval approaches is crucial for developing effective RAG systems, as the choice of retrieval model significantly impacts the quality and relevance of information provided to the language model for response generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Representation Approach: Sparse vs. Dense Vectors", "tldr": "Sparse retrieval models like BM25 represent documents and queries as high-dimensional vectors with mostly zero values corresponding to vocabulary terms, while dense retrieval models like DPR use lower-dimensional continuous vectors that encode semantic meaning. (11 sources)", "text": "\nAt their core, sparse and dense retrieval models differ fundamentally in how they represent textual information. Sparse retrieval models, such as BM25, represent documents and queries as high-dimensional, sparse vectors where dimensions correspond to the vocabulary terms in the collection <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>. In these representations, most elements are zero (due to non-occurring terms), and the non-zero weights indicate the importance of each term, typically calculated using statistical methods <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper>. This approach follows the bag-of-words assumption, treating text as a multiset of words while ignoring grammar and word order <Paper corpusId=\"275119098\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models like Dense Passage Retrieval (DPR) and SentenceTransformer encode text into fixed-size, low-dimensional continuous vector representations <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. These dense vectors typically range from hundreds to thousands of dimensions\u2014significantly fewer than sparse representations\u2014where all dimensions potentially contain information <Paper corpusId=\"267328301\" paperTitle=\"(Giamphy et al., 2023)\" isShortName></Paper>. As Lin points out, both sparse and dense retrieval models can be viewed as parametric variations of a bi-encoder architecture, with the primary difference being the basis of the representation vector: lexical space versus semantic space <Paper corpusId=\"266230831\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>.\n\nAnother important distinction is that encoders in sparse retrieval only produce non-negative weights, whereas dense encoders have no such constraint <Paper corpusId=\"257585074\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>. This constraint in sparse models arises from their reliance on inverted indexing software stacks built for traditional lexical search, where weights are always non-negative term frequencies.\n\nThe representation difference leads to different indexing approaches. Sparse retrieval typically leverages inverted indices, where each term maps to a list of documents containing that term <Paper corpusId=\"268091298\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Dense retrieval, however, requires approximate nearest neighbor (ANN) search algorithms to efficiently find similar vectors in the embedding space <Paper corpusId=\"247292113\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper>. This architectural difference affects not only how information is represented but also how it is stored, indexed, and retrieved in practical applications.", "citations": [{"id": "(Piktus et al., 2021)", "paper": {"corpus_id": 245334864, "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Aleksandra Piktus", "authorId": "120174856"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Dmytro Okhonko", "authorId": "113568063"}, {"name": "Samuel Broscheit", "authorId": "2966239"}, {"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Barlas Ouguz", "authorId": "1628391446"}, {"name": "Edouard Grave", "authorId": "3024698"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}, {"name": "Sebastian Riedel", "authorId": "48662861"}], "n_citations": 66}, "snippets": ["We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval."], "score": 0.93408203125}, {"id": "(Robertson et al., 2009)", "paper": {"corpus_id": 207178704, "title": "The Probabilistic Relevance Framework: BM25 and Beyond", "year": 2009, "venue": "Foundations and Trends in Information Retrieval", "authors": [{"name": "S. Robertson", "authorId": "144430625"}, {"name": "H. Zaragoza", "authorId": "2833561"}], "n_citations": 3760}, "snippets": ["The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."], "score": 0.0}, {"id": "(Penha et al., 2022)", "paper": {"corpus_id": 248366550, "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Gustavo Penha", "authorId": "145579682"}, {"name": "C. Hauff", "authorId": "2731925"}], "n_citations": 0}, "snippets": ["An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario."], "score": 0.9521484375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 275119098, "title": "On the Robustness of Generative Information Retrieval Models", "year": 2024, "venue": "European Conference on Information Retrieval", "authors": [{"name": "Yuansan Liu", "authorId": "2143860482"}, {"name": "Ruqing Zhang", "authorId": "2109960367"}, {"name": "Jiafeng Guo", "authorId": "70414094"}, {"name": "Changjiang Zhou", "authorId": "2287815279"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Xueqi Cheng", "authorId": "2244825947"}], "n_citations": 4}, "snippets": ["Sparse retrieval models build representations of queries and documents based on the bag-of-words (BoW) assumption [55], where each text is treated as a multiset of its words, ignoring grammar and word order [13,41]. During the past decades, we have witnessed sparse retrieval models going through quick algorithmic shifts from early heuristic models [43], vector space models [43], to probabilistic models [40,41]. BM25 [42], as a representative of probabilistic models, is widely used for its efficiency while guaranteeing retrieval performance.\n\nWith the development of deep learning, many researchers have turned to dense retrieval models [19,20,56], which have been proven to be effective in capturing latent semantics and extracting effective features. Dense retrieval models typically adopt a bi-encoder architecture to encode queries and documents into low-dimension embeddings and use embedding similarities as estimated relevance scores for effective retrieval [13]. Karpukhin et al. [19] were pioneers in discovering that fine-tuning BERT to learn effective dense representations, called DPR, outperforms traditional retrieval methods like BM25."], "score": 0.96630859375}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Giamphy et al., 2023)", "paper": {"corpus_id": 267328301, "title": "A Quantitative Analysis of Noise Impact on Document Ranking", "year": 2023, "venue": "IEEE International Conference on Systems, Man and Cybernetics", "authors": [{"name": "Edward Giamphy", "authorId": "2181131501"}, {"name": "K\u00e9vin Sanchis", "authorId": "2212071788"}, {"name": "G. Dashyan", "authorId": "144101598"}, {"name": "Jean-Loup Guillaume", "authorId": "2281888390"}, {"name": "Ahmed Hamdi", "authorId": "2924500"}, {"name": "Lilian Sanselme", "authorId": "2281878315"}, {"name": "Antoine Doucet", "authorId": "2241553562"}], "n_citations": 2}, "snippets": ["BM25 is a sparse representation model that computes a score based on the frequency of the query terms in the document, as well as their inverse document frequency.On the other hand, DistilBERT is a state-of-the-art language model that uses a dense representation approach based on deep neural networks", "Unlike BM25, DistilBERT generates dense representations that encode semantic and syntactic information that can capture complex relationships between words and phrases.\n\nSparse and dense representations diverge in the way they encode the queries and documents.As suggested by their name, sparse (resp.dense) representation models encode the documents as sparse (resp.dense) vectors."], "score": 0.892578125}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 266230831, "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval", "year": 2023, "venue": "Text Retrieval Conference", "authors": [{"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "David Alfonso-Hermelo", "authorId": "1419474794"}, {"name": "Vitor Jeronymo", "authorId": "2274328833"}, {"name": "Ehsan Kamalloo", "authorId": "2023642"}, {"name": "Carlos Lassance", "authorId": "2131640257"}, {"name": "Rodrigo Nogueira", "authorId": "2274330487"}, {"name": "Odunayo Ogundepo", "authorId": "2166106776"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Jheng-Hong Yang", "authorId": "2109723027"}, {"name": "Xinyu Crystina Zhang", "authorId": "2118895402"}], "n_citations": 5}, "snippets": ["Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models."], "score": 0.9072265625}, {"id": "(Lin, 2021)", "paper": {"corpus_id": 238259539, "title": "A proposed conceptual framework for a representational approach to information retrieval", "year": 2021, "venue": "SIGIR Forum", "authors": [{"name": "Jimmy J. Lin", "authorId": "145580839"}], "n_citations": 53}, "snippets": ["Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation."], "score": 0.935546875}, {"id": "(Nguyen et al., 2023)", "paper": {"corpus_id": 257585074, "title": "A Unified Framework for Learned Sparse Retrieval", "year": 2023, "venue": "European Conference on Information Retrieval", "authors": [{"name": "Thong Nguyen", "authorId": "2116028119"}, {"name": "Sean MacAvaney", "authorId": "22214396"}, {"name": "Andrew Yates", "authorId": "2136074457"}], "n_citations": 29}, "snippets": ["The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35]."], "score": 0.962890625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268091298, "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Penghao Zhao", "authorId": "2268718776"}, {"name": "Hailin Zhang", "authorId": "2288557803"}, {"name": "Qinhan Yu", "authorId": "2289597580"}, {"name": "Zhengren Wang", "authorId": "2288675277"}, {"name": "Yunteng Geng", "authorId": "2288532368"}, {"name": "Fangcheng Fu", "authorId": "46182701"}, {"name": "Ling Yang", "authorId": "2249513224"}, {"name": "Wentao Zhang", "authorId": "2277807793"}, {"name": "Bin Cui", "authorId": "2277742543"}], "n_citations": 282}, "snippets": ["Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage", "term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance."], "score": 0.923828125}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 247292113, "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Qiong Gao", "authorId": "2117205083"}, {"name": "Kuan-sheng Zou", "authorId": "3271753"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}, {"name": "Rui Guo", "authorId": "2114009505"}, {"name": "Jianfeng Xu", "authorId": "2146054545"}, {"name": "Guanjun Jiang", "authorId": "2084534507"}, {"name": "Luxi Xing", "authorId": "30967877"}, {"name": "Ping Yang", "authorId": "2327836457"}], "n_citations": 23}, "snippets": ["Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]."], "score": 0.92431640625}], "table": null}, {"title": "Matching Mechanism: Lexical vs. Semantic Matching", "tldr": "Sparse retrieval models like BM25 perform lexical matching by comparing exact terms between queries and documents, which excels at finding keyword matches but struggles with synonyms. Dense retrieval models like DPR enable semantic matching through learned representations, capturing meaning relationships even when exact words differ. (12 sources)", "text": "\nThe fundamental difference in how sparse and dense retrieval models evaluate relevance lies in their matching mechanisms. Sparse retrieval models like BM25 rely on *lexical matching*, which measures similarity based on the weighted occurrence of exact terms shared between queries and documents <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>. This approach works by calculating statistical metrics such as term frequency and inverse document frequency to determine relevance <Paper corpusId=\"269448997\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. While effective at identifying documents with exact keyword matches, sparse models \"fail to capture synonyms and paraphrases\" <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper> and are \"vulnerable to the vocabulary mismatch problem\" where retrieval fails when query and document terms differ lexically <Paper corpusId=\"270702658\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models like DPR and SentenceTransformer enable *semantic matching* by representing text in a continuous vector space where similar meanings cluster together, regardless of the exact words used <Paper corpusId=\"259370750\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. These models are trained to encode semantic relationships, allowing them to recognize that terms like \"password\" and \"passwd\" or phrases like \"facebook change password\" and \"fb modify passwd\" are similar in meaning despite having no lexical overlap <Paper corpusId=\"259370750\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. This capability helps dense retrievers overcome the vocabulary mismatch problem that plagues sparse retrieval methods <Paper corpusId=\"259924840\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nThe distinct matching mechanisms lead to different strengths and weaknesses. Sparse retrieval models like BM25 excel at finding documents with precise term matches and are particularly effective at identifying passages with lower relevance values <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. They often provide high recall due to their ability to capture partial matches between queries and documents <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. Dense retrievers, on the other hand, are \"very effective at encoding passages characterized by high relevance labels\" <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> but may struggle with identifying passages of lower relevance value <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nEmpirical studies have shown that the results produced by sparse and dense retrievers have relatively little overlap <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3641284\" paperTitle=\"(McInnes et al., 2018)\" isShortName></Paper>, suggesting they capture complementary aspects of relevance. This complementary nature explains why hybrid approaches that combine both lexical and semantic matching often outperform either method alone <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"277113527\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Sciavolino et al., 2021)", "paper": {"corpus_id": 237562875, "title": "Simple Entity-Centric Questions Challenge Dense Retrievers", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Christopher Sciavolino", "authorId": "2112021127"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Jinhyuk Lee", "authorId": "46664096"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 167}, "snippets": ["Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."], "score": 0.953125}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 269448997, "title": "BMRetriever: Tuning Large Language Models as Better Biomedical Text Retrievers", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ran Xu", "authorId": "2265148831"}, {"name": "Wenqi Shi", "authorId": "2263890944"}, {"name": "Yue Yu", "authorId": "2218865512"}, {"name": "Yuchen Zhuang", "authorId": "8103389"}, {"name": "Yanqiao Zhu", "authorId": "2653121"}, {"name": "M. D. Wang", "authorId": "2237844925"}, {"name": "Joyce C. Ho", "authorId": "2263536473"}, {"name": "Chao Zhang", "authorId": "2256776233"}, {"name": "Carl Yang", "authorId": "2237940940"}], "n_citations": 25}, "snippets": ["Sparse Retrieval Models. Sparse retrieval models rely on lexical matching between query and document terms to calculate similarity scores. \n\n\u2022 BM25 (Robertson et al., 2009) is the most commonly used sparse retrieval model for lexical retrieval, employing a scoring function that calculates the similarity between two highdimensional sparse vectors based on token matching and weighting. \n\nDense Retrieval Models. Dense retrieval models utilize dense vector representations to capture semantic similarity between queries and documents."], "score": 0.8955078125}, {"id": "(Jeong et al., 2024)", "paper": {"corpus_id": 270702658, "title": "Database-Augmented Query Representation for Information Retrieval", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Soyeong Jeong", "authorId": "8599185"}, {"name": "Jinheon Baek", "authorId": "90765684"}, {"name": "Sukmin Cho", "authorId": "2158892171"}, {"name": "Sung Ju Hwang", "authorId": "2260611009"}, {"name": "Jong C. Park", "authorId": "2109285560"}], "n_citations": 2}, "snippets": ["Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data."], "score": 0.9326171875}, {"id": "(Lee et al., 2023)", "paper": {"corpus_id": 259370750, "title": "On Complementarity Objectives for Hybrid Retrieval", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dohyeon Lee", "authorId": "2135607329"}, {"name": "Seung-won Hwang", "authorId": "2153642272"}, {"name": "Kyungjae Lee", "authorId": "79733119"}, {"name": "Seungtaek Choi", "authorId": "5841595"}, {"name": "Sunghyun Park", "authorId": "2108106092"}], "n_citations": 5}, "snippets": ["Classic sparse (or symbolic) retrieval such as BM25 (Robertson and Zaragoza, 2009), quantifies the lexical overlaps (or exact matches) between query q and document d, weighted by term frequency (tf) and inverse document frequency (idf). Such computation can be efficiently localized to a few high-scoring q-d pairs with an inverted index, may fail to match pairs with term mismatches. For example, a text pair with identical intent-\"facebook change password\" and \"fb modify passwd\"-does not share any common word, so the pair cannot be matched by lexical retrieval. \n\nTo overcome such mismatches, dense retrieval models, such as BERT-based DPR (Karpukhin et al., 2020) or coCondenser (Gao and Callan, 2021), aim to support soft \"semantic matching\", by encoding queries and documents into lowdimensional embedding vectors. Dense representation is trained so that \"password\" and \"passwd\" are located close in the space even though they have different lexical representations."], "score": 0.9306640625}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 259924840, "title": "Learning to Retrieve In-Context Examples for Large Language Models", "year": 2023, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Liang Wang", "authorId": "145769448"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Furu Wei", "authorId": "49807919"}], "n_citations": 43}, "snippets": ["Compared to sparse retrieval methods such as BM25, dense retrieval exploits the powerful modeling capacity of pre-trained language models (PLMs) (Devlin et al., 2019) to learn relevance functions and has the potential to overcome the vocabulary mismatch problem."], "score": 0.94873046875}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 248496043, "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hang Li", "authorId": "2118384241"}, {"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "Ahmed Mourad", "authorId": "143832672"}, {"name": "Xueguang Ma", "authorId": "2461713"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 30}, "snippets": ["Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."], "score": 0.9609375}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 237366133, "title": "BERT-based Dense Retrievers Require Interpolation with BM25 for Effective Passage Retrieval", "year": 2021, "venue": "International Conference on the Theory of Information Retrieval", "authors": [{"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 83}, "snippets": ["The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for."], "score": 0.0}, {"id": "(Hoshi et al., 2023)", "paper": {"corpus_id": 257427642, "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yasuto Hoshi", "authorId": "2211101536"}, {"name": "D. Miyashita", "authorId": "2441156"}, {"name": "Yasuhiro Morioka", "authorId": "51194024"}, {"name": "Youyang Ng", "authorId": "20556792"}, {"name": "Osamu Torii", "authorId": "2422593"}, {"name": "J. Deguchi", "authorId": "49192096"}], "n_citations": 0}, "snippets": ["Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models", ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."], "score": 0.90625}, {"id": "(McInnes et al., 2018)", "paper": {"corpus_id": 3641284, "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Leland McInnes", "authorId": "31785573"}, {"name": "John Healy", "authorId": "2062756303"}], "n_citations": 9476}, "snippets": ["UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."], "score": 0.0}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Kim et al., 2025)", "paper": {"corpus_id": 277113527, "title": "Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sejong Kim", "authorId": "2350956983"}, {"name": "Hyunseo Song", "authorId": "2350857179"}, {"name": "Hyunwoo Seo", "authorId": "2351317331"}, {"name": "Hyunjun Kim", "authorId": "2350867590"}], "n_citations": 3}, "snippets": ["Dense retrieval methods, which leverage semantic embeddings generated by models such as BERT (Devlin et al., 2019) or SentenceTransformers (Reimers et al., 2019), excel at capturing deep contextual relationships between queries and documents. However, they may sometimes fail to retrieve documents that contain precise terms, proper nouns, or abbreviations. In contrast, sparse retrieval techniques, employing methods like BM25 (Wang et al., 2021), offer excellent keyword matching capabilities and provide high interpretability, although they often lack the ability to grasp nuanced semantic meaning. (Sengupta et al., 2024)"], "score": 0.9287109375}], "table": null}, {"title": "Training Requirements: Supervised vs. Unsupervised", "tldr": "Sparse retrievers like BM25 are unsupervised methods that don't require training data, while dense retrievers like DPR need substantial supervised training. This fundamental difference affects generalization capabilities, with sparse retrievers showing stronger zero-shot performance across domains despite lower overall effectiveness when training data is available. (13 sources)", "text": "\nA key differentiator between sparse and dense retrieval models is their training requirements. Sparse retrieval models like BM25 are fundamentally unsupervised methods that don't require any training data <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper> <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. They calculate term weights using statistical methods based on term frequency and document frequency, operating effectively without learning from labeled examples. In contrast, dense retrieval models like DPR and SentenceTransformer are supervised approaches that require substantial training data to learn effective representations <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.\n\nLin argues that comparing supervised dense retrievers to unsupervised sparse retrievers like BM25 represents an \"apples-to-oranges juxtaposition,\" and it should not be surprising that supervised techniques show higher effectiveness in domains where training data is available <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>. This perspective is supported by organizing retrieval models along two dimensions: dense vs. sparse representations and supervised vs. unsupervised approaches <Paper corpusId=\"266230831\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238259539\" paperTitle=\"(Lin, 2021)\" isShortName></Paper>.\n\nThe training differences significantly impact generalization capabilities. While dense retrievers consistently outperform BM25 when evaluated on in-domain data, their performance often deteriorates in zero-shot scenarios where the test data differs from the training distribution <Paper corpusId=\"248366550\" paperTitle=\"(Penha et al., 2022)\" isShortName></Paper>. Studies show that BM25 outperforms dense retrievers on most tasks in zero-shot evaluation settings <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233296016\" paperTitle=\"(Thakur et al., 2021)\" isShortName></Paper>. This generalization gap is particularly evident for entity-centric questions and out-of-domain queries <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>.\n\nThe supervised nature of dense retrievers brings additional training complexities. Modern approaches employ sophisticated techniques such as contrastive training with in-batch negatives and hard-negative mining to improve effectiveness <Paper corpusId=\"276107364\" paperTitle=\"(Abdallah et al., 2025)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper> <Paper corpusId=\"220302524\" paperTitle=\"(Xiong et al., 2020)\" isShortName></Paper>. Many dense retrieval methods are initialized with pre-trained language models like BERT and then fine-tuned on domain-specific data <Paper corpusId=\"258840999\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nDespite the superior performance of dense retrievers in tasks with sufficient training data, the training-free nature of BM25 gives it practical advantages in some settings. BM25 remains widely used due to its generalization capabilities and strong lexical matching <Paper corpusId=\"258840999\" paperTitle=\"(Cho et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276107364\" paperTitle=\"(Abdallah et al., 2025)\" isShortName></Paper>. The generalization gap between dense and sparse retrievers has motivated research into models that can better transfer across domains and tasks without requiring domain-specific supervised data <Paper corpusId=\"273549875\" paperTitle=\"(Saberi et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Penha et al., 2022)", "paper": {"corpus_id": 248366550, "title": "Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Gustavo Penha", "authorId": "145579682"}, {"name": "C. Hauff", "authorId": "2731925"}], "n_citations": 0}, "snippets": ["An unsupervised sparse representation model such as BM25 [51] and TF-IDF [23] represents each document and query with a sparse vector with the dimension of the collection's vocabulary, having many zero weights due to non-occurring terms. Since the weights of each term are calculated using term statistics they are considered unsupervised methods.\n\nSupervised dense retrieval models, such as ANCE [65], Rock-etQA [48], PAIR [50] and coCodenser [11], represent query and documents in a smaller fixed-length space, for example of 768 dimensions, which can naturally capture semantics. In this manner they are able to address the vocabulary mismatch problem. While dense retrieval models have shown to consistently outperform BM25 in multiple datasets, this is not so easily the case when dense retrieval models do not have access to training data from the target task, known as the zero-shot scenario."], "score": 0.9521484375}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 248496043, "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hang Li", "authorId": "2118384241"}, {"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "Ahmed Mourad", "authorId": "143832672"}, {"name": "Xueguang Ma", "authorId": "2461713"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 30}, "snippets": ["Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."], "score": 0.9609375}, {"id": "(Lin, 2021)", "paper": {"corpus_id": 238259539, "title": "A proposed conceptual framework for a representational approach to information retrieval", "year": 2021, "venue": "SIGIR Forum", "authors": [{"name": "Jimmy J. Lin", "authorId": "145580839"}], "n_citations": 53}, "snippets": ["Dense retrieval models such as DPR are often compared against sparse retrieval models such as BM25 in experimental evaluations, as (Karpukhin et al., 2020) did in their paper. Not surprisingly, results show that dense retrieval models obtain higher effectiveness. \n\nThis, however, is not a fair comparison. Dense retrieval methods represent an instance of representational learning-the key here is learning. The output of the encoders are learned representations that benefit from (large amounts of) training data under a standard supervised machine learning paradigm. \n\nIn contrast, BM25 is unsupervised. 4 Comparing a supervised method to an unsupervised method is fundamentally an apples-to-oranges juxtaposition; it should not be surprising that a supervised technique is more effective. \n\nAs previously argued in Lin and Ma [2021], the encoders \u03b7 \u2022 should be organized along two distinct dimensions or properties: The first dimension contrasts dense vs. sparse vector representations for queries and documents. The second dimension distinguishes between supervised (learned) and unsupervised representations. Table 1 illustrates this taxonomy. DPR (along with nearly all dense retrieval methods today) are instances of learned dense representations. BM25 is an instance of an unsupervised sparse representation."], "score": 0.935546875}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 266230831, "title": "Simple Yet Effective Neural Ranking and Reranking Baselines for Cross-Lingual Information Retrieval", "year": 2023, "venue": "Text Retrieval Conference", "authors": [{"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "David Alfonso-Hermelo", "authorId": "1419474794"}, {"name": "Vitor Jeronymo", "authorId": "2274328833"}, {"name": "Ehsan Kamalloo", "authorId": "2023642"}, {"name": "Carlos Lassance", "authorId": "2131640257"}, {"name": "Rodrigo Nogueira", "authorId": "2274330487"}, {"name": "Odunayo Ogundepo", "authorId": "2166106776"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Jheng-Hong Yang", "authorId": "2109723027"}, {"name": "Xinyu Crystina Zhang", "authorId": "2118895402"}], "n_citations": 5}, "snippets": ["Recently, Lin (Lin, 2021) made the observation that dense retrieval models, sparse retrieval models, and traditional bag-of-words models (e.g., BM25) are all parametric variations of a bi-encoder architecture, which is shown in Figure 1(a). In all three classes of models, \"encoders\" take queries or documents and generate vector representations. There are two major axes of differences, the first of which lies in the basis of the representation vector: dense retrieval models generate dense (semantic) representations whereas sparse retrieval models and bag-of-words model ground their representation vectors in lexical space. The other major axis of variation is whether these representations are learned: yes in the case of dense and sparse retrieval models, but no in the case of traditional bag-of-words models."], "score": 0.9072265625}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 238744204, "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xilun Chen", "authorId": "1769736"}, {"name": "Kushal Lakhotia", "authorId": "1410624139"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Anchit Gupta", "authorId": "3377939"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Stanislav Peshterliev", "authorId": "3139260"}, {"name": "Yashar Mehdad", "authorId": "2121361882"}, {"name": "Sonal Gupta", "authorId": "2118343423"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 69}, "snippets": ["On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks."], "score": 0.96240234375}, {"id": "(Thakur et al., 2021)", "paper": {"corpus_id": 233296016, "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "year": 2021, "venue": "NeurIPS Datasets and Benchmarks", "authors": [{"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Nils Reimers", "authorId": "2959414"}, {"name": "Andreas Ruckl'e", "authorId": "1404060894"}, {"name": "Abhishek Srivastava", "authorId": "153257123"}, {"name": "Iryna Gurevych", "authorId": "69033154"}], "n_citations": 1055}, "snippets": ["Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."], "score": 0.0}, {"id": "(Sciavolino et al., 2021)", "paper": {"corpus_id": 237562875, "title": "Simple Entity-Centric Questions Challenge Dense Retrievers", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Christopher Sciavolino", "authorId": "2112021127"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Jinhyuk Lee", "authorId": "46664096"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 167}, "snippets": ["Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."], "score": 0.953125}, {"id": "(Abdallah et al., 2025)", "paper": {"corpus_id": 276107364, "title": "Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Abdelrahman Abdallah", "authorId": "2238699512"}, {"name": "Bhawna Piryani", "authorId": "1935823995"}, {"name": "Jamshid Mozafari", "authorId": "1389526186"}, {"name": "Mohammed Ali", "authorId": "2343952059"}, {"name": "Adam Jatowt", "authorId": "2261673463"}], "n_citations": 1}, "snippets": ["Sparse retrievers, such as BM25 (Robertson et al., 1994), remain widely used due to their strong lexical matching capabilities and generalization. However, they struggle with capturing semantic relationships, leading to the rise of dense retrieval methods (Lee et al., 2019), which leverage pre-trained neural encoders to generate embeddings for queries and documents. Notable advancements, such as DPR (Lin et al., 2020) and ANCE (Xiong et al., 2020), improve retrieval effectiveness by employing contrastive training with in-batch negatives (Gillick et al., 2019) and hard-negative mining (Karpukhin et al., 2020)(Xiong et al., 2020)."], "score": 0.9453125}, {"id": "(Xiong et al., 2020)", "paper": {"corpus_id": 220302524, "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Lee Xiong", "authorId": "101655391"}, {"name": "Chenyan Xiong", "authorId": "144628574"}, {"name": "Ye Li", "authorId": "2110766301"}, {"name": "Kwok-Fung Tang", "authorId": "1785396874"}, {"name": "Jialin Liu", "authorId": "2108415378"}, {"name": "Paul N. Bennett", "authorId": "144609235"}, {"name": "Junaid Ahmed", "authorId": "144643947"}, {"name": "Arnold Overwijk", "authorId": "2734525"}], "n_citations": 1234}, "snippets": ["Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up."], "score": 0.0}, {"id": "(Cho et al., 2023)", "paper": {"corpus_id": 258840999, "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Sukmin Cho", "authorId": "2158892171"}, {"name": "Soyeong Jeong", "authorId": "8599185"}, {"name": "Jeong-yeon Seo", "authorId": "2148402840"}, {"name": "Jong C. Park", "authorId": "2109285560"}], "n_citations": 24}, "snippets": ["We use two types of retrievers, sparse and dense retrievers, for retrieving documents re-ranked by LLMs. 1) BM25 (Robertson et al., 2009) is a representative sparse retriever computing the relevance score between a document and a query based on term frequency and inverse document frequency. BM25 has been widely employed because of its fast speed and effective performance. \n\n2) DPR (Karpukhin et al., 2020) interprets training dense retrieval as metric learning problems. The biencoder initialized with BERT (Devlin et al., 2019) is trained with contrastive learning exploiting positive and negative passages for a given query. It shows outperforming results over traditional sparse retrievers."], "score": 0.892578125}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Saberi et al., 2024)", "paper": {"corpus_id": 273549875, "title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Iman Saberi", "authorId": "2085891275"}, {"name": "Fatemeh Fard", "authorId": "2283136625"}], "n_citations": 1}, "snippets": ["To analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data.\n\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods.\n\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data."], "score": 0.8857421875}], "table": null}, {"title": "Performance Characteristics: Strengths and Weaknesses", "tldr": "Sparse retrievers like BM25 excel at precise keyword matching and generalization to new domains, while dense retrievers like DPR better capture semantic relationships but struggle with entity-centric queries without training. Both approaches have complementary strengths that make hybrid solutions often the most effective approach. (19 sources)", "text": "\n## Strengths of Sparse Retrieval (BM25)\n- **High recall capability**: BM25 excels at retrieving documents that partially match the query, offering strong recall even when documents are only somewhat relevant <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Zero-shot generalization**: Consistently outperforms dense retrievers on most tasks in zero-shot evaluation settings where no domain-specific training is available <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233296016\" paperTitle=\"(Thakur et al., 2021)\" isShortName></Paper>\n- **No training required**: Functions effectively without any training data or parameters to tune <Paper corpusId=\"262066417\" paperTitle=\"(Pirozelli et al., 2023)\" isShortName></Paper>\n- **Computational efficiency**: Offers highly efficient search through inverted indices <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Precise term matching**: Particularly effective at capturing salient phrases and exact matches in queries <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>\n\n## Weaknesses of Sparse Retrieval (BM25)\n- **Vocabulary mismatch problem**: Fails when query and document use different terms to express the same concept <Paper corpusId=\"270702658\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>\n- **Limited semantic understanding**: Cannot capture synonyms, paraphrases, or deeper semantic relationships <Paper corpusId=\"268724187\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>\n- **Lower precision for highly relevant content**: Less effective at pushing the most relevant results to the top of rankings <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Bag-of-words limitation**: Treats text as unordered collections of terms, ignoring grammar, word order, and context <Paper corpusId=\"248496840\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>\n\n## Strengths of Dense Retrieval (DPR, SentenceTransformer)\n- **Semantic matching**: Captures meaningful relationships between concepts even when exact terms differ <Paper corpusId=\"262066417\" paperTitle=\"(Pirozelli et al., 2023)\" isShortName></Paper>\n- **Superior performance on highly relevant passages**: Very effective at encoding and retrieving passages with high relevance labels <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n- **Better ranking precision**: Pushes the most relevant documents to the top of search results <Paper corpusId=\"276647280\" paperTitle=\"(Abdallah et al._1, 2025)\" isShortName></Paper>\n- **Cross-lingual capabilities**: Can perform retrieval across different languages and even different scripts, which is impossible with term-matching methods <Paper corpusId=\"258048596\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249097975\" paperTitle=\"(Izacard et al., 2021)\" isShortName></Paper>\n- **Efficient inference**: Once trained, dense retrievers can pre-compute document embeddings offline and perform efficient similarity calculations at query time <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235792476\" paperTitle=\"(Zhuang et al., 2021)\" isShortName></Paper>\n\n## Weaknesses of Dense Retrieval (DPR, SentenceTransformer)\n- **Training dependency**: Requires substantial training data to learn effective representations <Paper corpusId=\"262947262\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>\n- **Poor generalization to new domains**: Performs worse than BM25 on out-of-domain data without specific training <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>\n- **Struggles with entity-centric queries**: Shows poor performance compared to BM25 on simple entity-centric questions <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237562875\" paperTitle=\"(Sciavolino et al., 2021)\" isShortName></Paper>\n- **Misses salient phrases**: Sometimes fails to match important unique phrases in queries that BM25 can easily capture <Paper corpusId=\"238744204\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>\n- **Less effective for passages with lower relevance**: Struggles to identify passages that are only somewhat relevant to the query <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>\n\n## Complementary Nature\n- **Little result overlap**: The results produced by sparse and dense retrievers have relatively little overlap, suggesting they capture different aspects of relevance <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"3641284\" paperTitle=\"(McInnes et al., 2018)\" isShortName></Paper>\n- **Hybrid approaches excel**: Combining sparse and dense methods consistently outperforms either method alone <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"275906690\" paperTitle=\"(Some et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>\n- **Score interpolation benefits**: Interpolating BM25 scores with dense retriever scores significantly improves performance, as dense retrievers alone may miss weaker relevance signals that BM25 captures <Paper corpusId=\"237366133\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>", "citations": [{"id": "(Li et al., 2022)", "paper": {"corpus_id": 248496043, "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hang Li", "authorId": "2118384241"}, {"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "Ahmed Mourad", "authorId": "143832672"}, {"name": "Xueguang Ma", "authorId": "2461713"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 30}, "snippets": ["Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."], "score": 0.9609375}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 238744204, "title": "Salient Phrase Aware Dense Retrieval: Can a Dense Retriever Imitate a Sparse One?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xilun Chen", "authorId": "1769736"}, {"name": "Kushal Lakhotia", "authorId": "1410624139"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Anchit Gupta", "authorId": "3377939"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Stanislav Peshterliev", "authorId": "3139260"}, {"name": "Yashar Mehdad", "authorId": "2121361882"}, {"name": "Sonal Gupta", "authorId": "2118343423"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 69}, "snippets": ["On the other hand, while existing dense retrievers excel at capturing semantics, they sometimes fail to match the salient phrases in the query. For example, (Karpukhin et al., 2020) show that DPR, unlike a sparse BM25 retriever (Robertson et al., 1994), is unable to catch the salient phrase \"Thoros of Myr\" in the query \"Who plays Thoros of Myr in Game of Thrones?\". In addition, dense retrievers struggle to generalize to out-of-domain test data compared to training-free sparse retrievers such as BM25. For instance, (Sciavolino et al., 2021) find that DPR performs poorly compared to BM25 on simple entity-centric questions, and (Thakur et al., 2021) introduce a new BEIR benchmark to evaluate the zero-shot generalization of retrieval models showing that BM25 outperforms dense retrievers on most tasks."], "score": 0.96240234375}, {"id": "(Thakur et al., 2021)", "paper": {"corpus_id": 233296016, "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models", "year": 2021, "venue": "NeurIPS Datasets and Benchmarks", "authors": [{"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Nils Reimers", "authorId": "2959414"}, {"name": "Andreas Ruckl'e", "authorId": "1404060894"}, {"name": "Abhishek Srivastava", "authorId": "153257123"}, {"name": "Iryna Gurevych", "authorId": "69033154"}], "n_citations": 1055}, "snippets": ["Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."], "score": 0.0}, {"id": "(Pirozelli et al., 2023)", "paper": {"corpus_id": 262066417, "title": "Benchmarks for Pir\u00e1 2.0, a Reading Comprehension Dataset about the Ocean, the Brazilian Coast, and Climate Change", "year": 2023, "venue": "Data Intelligence", "authors": [{"name": "Paulo Pirozelli", "authorId": "2243283631"}, {"name": "M. M. Jos'e", "authorId": "2140512811"}, {"name": "I. Silveira", "authorId": "2185349318"}, {"name": "Fl'avio Nakasato", "authorId": "2135817708"}, {"name": "S. M. Peres", "authorId": "2125066"}, {"name": "A. Brand\u00e3o", "authorId": "119984906"}, {"name": "Anna H. R. Costa", "authorId": "2243884199"}, {"name": "F. G. Cozman", "authorId": "70089890"}], "n_citations": 4}, "snippets": ["Two types of IR models were investigated for this benchmark: BM25 (Robertson et al., 2009), a sparse retrieval, and Dense Passage Retrieval (DPR) (Karpukhin et al., 2020), a dense one. BM25 treats sentences as bag-of-words, similarly to Term Frequency-Inverse Document Frequency (TF-IDF), but giving more weight to longer texts. BM25 is a fast algorithm that does not require any training. A disadvantage of sparse methods like this, however, is that they are not able to consider semantic information when retrieving texts. Dense methods such as DPR, instead, rely on converting texts and queries to embeddings through a language model (e.g., BERT), and measuring their similarity. This allows for semantic-based text retrieval, a capability that sparse methods like BM25 lack."], "score": 0.9619140625}, {"id": "(Jeong et al., 2024)", "paper": {"corpus_id": 270702658, "title": "Database-Augmented Query Representation for Information Retrieval", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Soyeong Jeong", "authorId": "8599185"}, {"name": "Jinheon Baek", "authorId": "90765684"}, {"name": "Sukmin Cho", "authorId": "2158892171"}, {"name": "Sung Ju Hwang", "authorId": "2260611009"}, {"name": "Jong C. Park", "authorId": "2109285560"}], "n_citations": 2}, "snippets": ["Retrieval In response to a query from a user, the retrieval task is to search for the most relevant documents from a large corpus (such as Wikipedia) (Zhu et al., 2021). Typically, it can be performed with two types of models: sparse and dense retrievers. Specifically, sparse retrievers such as TF-IDF or BM25 (Robertson et al., 1994) represent the query and document based on their terms and frequencies in a sparse vector space, whereas dense retrievers use a trainable dense vector space to embed the query and document usually with language models (Karpukhin et al., 2020; Izacard et al., 2022).\n\nRecently, due to the limitation of sparse retrievers that are vulnerable to the vocabulary mismatch problem (where the retrieval fails when the lexical terms within the query and document are different), dense retrieval is widely selected as a default choice and many advancements have been made on it. For example, DPR (Karpukhin et al., 2020) is a supervised dense retriever with a dual-encoder architecture that is trained discriminatively on the labeled pair of a query and its relevant documents to achieve higher similarity scores than the pair of the query-irrelevant documents. Also, Contriever (Izacard et al., 2022) utilizes a self-supervised learning strategy, which generates its training samples by creating positive pairs from query-related contexts within and across documents, rather than relying on explicitly annotated data."], "score": 0.9326171875}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 268724187, "title": "Decoy Effect in Search Interaction: Understanding User Behavior and Measuring System Vulnerability", "year": 2024, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Nuo Chen", "authorId": "2257286538"}, {"name": "Jiqun Liu", "authorId": "2265515693"}, {"name": "Hanpei Fang", "authorId": "2293660445"}, {"name": "Yuankai Luo", "authorId": "2293898339"}, {"name": "Tetsuya Sakai", "authorId": "2257233277"}, {"name": "Xiao-Ming Wu", "authorId": "2265517306"}], "n_citations": 5}, "snippets": ["Traditional IR ranking models rely on exact lexical matching, such as Boolean retrieval, BM25 [77,78], and statistical language models [46]. These retrieval models, also known as Bag of Words (BOW) models, are based on sparse vector representation and process queries by organizing documents into inverted indices, wherein each unique term is associated with an inverted list that stores information regarding the documents in which it appears. However, the token-based sparse representation of text cannot fully capture the semantic nuances of each term within the entire textual context. These retrieval models thus suffer from the problem of vocabulary mismatch or semantic mismatch (i.e., relevant documents may not contain terms that appear in the query).\n\nOne approach to deal with the vocabulary mismatch is to use dense vectors, which represent the text in a continuous vector space with predefined dimensions and the dimension is not dependent on the length of the text. The advantage of this approach is that text with similar semantics is typically represented by vectors that are close to each other in the vector space. Ranking models based on dense vectors is referred to as dense retrieval models. Dense retrieval models include Dense Passage Retriever (DPR) [39], Contriever [32], Approximate nearest neighbor Negative Contrastive Learning (ANCE) [105], ColBERT [41], Sentence-BERT (SBERT) [76], and so forth. Many dense retrieval models utilize BERT [25] for encoding queries and passages (e.g., [39,41,76,105]) and utilize techniques such as contrastive learning (e.g., [39,105]) or or Siamese Network (e.g., [76]) during the training process, achieving better semantic matching effectiveness compared to BM25 algorithm on benchmarks such as MS MARCO2."], "score": 0.92431640625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 248496840, "title": "LIDER: An Efficient High-dimensional Learned Index for Large-scale Dense Passage Retrieval", "year": 2022, "venue": "Proceedings of the VLDB Endowment", "authors": [{"name": "Yifan Wang", "authorId": "2115569215"}, {"name": "Haodi Ma", "authorId": "2110816708"}, {"name": "D. Wang", "authorId": "2111220343"}], "n_citations": 13}, "snippets": ["There are two major categories of retrieval methods, sparse and dense retrieval. Sparse retrieval normally uses bag-of-word (BOW) models, where the document representations are sparse vectors, while dense retrieval mostly utilizes neural embeddings from deep neural models which are dense vectors. Typical sparse retrieval methods include BM25, DeepCT [4], Doc2query [31] and docTTTT-Tquery [30], which are commonly used in recent retrieval studies as strong baselines. Due to the power of dense neural embeddings in semantic search, many state-of-the-art retrieval researches focus on dense retrieval."], "score": 0.91455078125}, {"id": "(Abdallah et al._1, 2025)", "paper": {"corpus_id": 276647280, "title": "From Retrieval to Generation: Comparing Different Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Abdelrahman Abdallah", "authorId": "2238699512"}, {"name": "Jamshid Mozafari", "authorId": "1389526186"}, {"name": "Bhawna Piryani", "authorId": "1935823995"}, {"name": "Mohammed Ali", "authorId": "2343952059"}, {"name": "Adam Jatowt", "authorId": "2261673463"}], "n_citations": 0}, "snippets": ["Traditional retrieval models such as BM25 and Dense Passage Retrieval (DPR), efficiently retrieve from large corpora but often lack semantic depth", ".BM25 is a traditional sparse retriever that ranks documents based on term frequency-inverse document frequency (TF-IDF). DPR encodes queries and documents into dense vector representations using pre-trained transformers. The similarity between a query q and a document d is calculated as the dot product of their dense embeddings, i.e., sim(q, d) = E Q (q) \u22a4 E P (d), where E Q and E P are the encoders for the query and document, respectively", ".The results show that DPR achieves the highest Top-1 accuracy at 75.4%, significantly outperforming BM25, which achieves only 54.0%. This indicates that sparse retrieval methods struggle with the complexity of trivia-style questions, while dense retrieval models that leverage learned representations of queries and documents exhibit superior retrieval effectiveness."], "score": 0.912109375}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 258048596, "title": "Learning to Tokenize for Generative Retrieval", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Zheng Chen", "authorId": "2117203270"}, {"name": "Shuaiqiang Wang", "authorId": "2386396"}, {"name": "Haichao Zhu", "authorId": "2387872"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Dawei Yin", "authorId": "2136400100"}, {"name": "M. de Rijke", "authorId": "1696030"}, {"name": "Z. Ren", "authorId": "2780667"}], "n_citations": 75}, "snippets": ["Sparse retrieval. Traditional sparse retrieval calculates the document score using term matching metrics such as TF-IDF [38], query likelihood [22] or BM25 [39]. It is widely used in practice due to its outstanding trade-off between accuracy and efficiency. Some methods adaptively assign the term importance using deep neural network [12,14,51]. With the recent development of pre-trained LMs, DeepCT [10] and HDCT [11] calculate term importance using contextualized text representation from BERT. Doc2Query [32] and DocT5Query [8] predict relevant queries to augment documents before building the BM25 index using a generative model like T5. Sparse retrieval often suffers from the lexical mismatches [24].\n\nDense retrieval. Dense retrieval (DR) presents queries and documents in dense vectors and models their similarities with the inner product or cosine similarity [19]. Compared with sparse retrieval, dense retrieval relieves the lexical mismatch problem. Various techniques have been proposed to improve DR models, such as hard negative mining [34,47], late interaction [20,41], and knowledge distillation [15,26]. Recent studies have shown the effectiveness of pre-training DR models using contrastive learning on large-scale corpora [16,31,37]. Despite their success, DR approaches have several limitations [5,28]: (i) DR models employ an index-retrieval pipeline with a fixed search procedure (MIPS), making it difficult to optimize the model end-to-end [42,46]. (ii) Training DR models relies on contrastive learning [19] to distinguish positives from negatives, which is inconsistent with large LMs training objectives [3] and fails to fully utilize the capabilities of pre-trained LMs [1]."], "score": 0.9384765625}, {"id": "(Izacard et al., 2021)", "paper": {"corpus_id": 249097975, "title": "Unsupervised Dense Information Retrieval with Contrastive Learning", "year": 2021, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Mathilde Caron", "authorId": "2062862676"}, {"name": "Lucas Hosseini", "authorId": "26360550"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Piotr Bojanowski", "authorId": "2329288"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 924}, "snippets": ["Recently, information retrieval has seen the emergence of dense retrievers, using neural networks, as an alternative to classical sparse methods based on term-frequency. These models have obtained state-of-the-art results on datasets and tasks where large training sets are available. However, they do not transfer well to new applications with no training data, and are outperformed by unsupervised term-frequency methods such as BM25. In this work, we explore the limits of contrastive learning as a way to train unsupervised dense retrievers and show that it leads to strong performance in various retrieval settings. On the BEIR benchmark our unsupervised model outperforms BM25 on 11 out of 15 datasets for the Recall@100. When used as pre-training before fine-tuning, either on a few thousands in-domain examples or on the large MS~MARCO dataset, our contrastive model leads to improvements on the BEIR benchmark. Finally, we evaluate our approach for multi-lingual retrieval, where training data is even scarcer than for English, and show that our approach leads to strong unsupervised performance. Our model also exhibits strong cross-lingual transfer when fine-tuned on supervised English data only and evaluated on low resources language such as Swahili. We show that our unsupervised models can perform cross-lingual retrieval between different scripts, such as retrieving English documents from Arabic queries, which would not be possible with term matching methods."], "score": 0.0}, {"id": "(Zhuang et al., 2021)", "paper": {"corpus_id": 235792476, "title": "TILDE: Term Independent Likelihood moDEl for Passage Re-ranking", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 76}, "snippets": ["Deep language models (deep LMs) are increasingly being used for full text retrieval or within cascade retrieval pipelines as later-stage re-rankers. A problem with using deep LMs is that, at query time, a slow inference step needs to be performed -- this hinders the practical adoption of these powerful retrieval models, or limits sensibly how many documents can be considered for re-ranking. We propose the novel, BERT-based, Term Independent Likelihood moDEl (TILDE), which ranks documents by both query and document likelihood. At query time, our model does not require the inference step of deep language models based retrieval approaches, thus providing consistent time-savings, as the prediction of query terms' likelihood can be pre-computed and stored during index creation. This is achieved by relaxing the term dependence assumption made by the deep LMs. In addition, we have devised a novel bi-directional training loss which allows TILDE to maximise both query and document likelihood at the same time during training. At query time, TILDE can rely on its query likelihood component (TILDE-QL) solely, or the combination of TILDE-QL and its document likelihood component (TILDE-DL), thus providing a flexible trade-off between efficiency and effectiveness. Exploiting both components provide the highest effectiveness at a higher computational cost while relying only on TILDE-QL trades off effectiveness for faster response time due to no inference being required. TILDE is evaluated on the MS MARCO and TREC Deep Learning 2019 and 2020 passage ranking datasets. Empirical results show that, compared to other approaches that aim to make deep language models viable operationally, TILDE achieves competitive effectiveness coupled with low query latency."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 262947262, "title": "A Dense Retrieval System and Evaluation Dataset for Scientific Computational Notebooks", "year": 2023, "venue": "IEEE International Conference on e-Science", "authors": [{"name": "N. Li", "authorId": "2085129"}, {"name": "Yangjun Zhang", "authorId": "2247665349"}, {"name": "Zhiming Zhao", "authorId": "2247992622"}], "n_citations": 1}, "snippets": ["Sparse retrieval models (Robertson et al., 1994) usually utilize the statistical characteristics of words to represent queries and computational notebooks. Dense retrieval models [11] aim to map texts and codes into a continuous vector space, and the similarity between queries and computational notebooks can be computed as the dot product between the vector representations. Each type of method has pros and cons. For instance, sparse retrieval models are highly efficient but limited by their lexical essence. Dense retrieval models provide preferable semantic matching between words and sentences but usually require indomain labeled data for training or fine-tuning."], "score": 0.88623046875}, {"id": "(Sciavolino et al., 2021)", "paper": {"corpus_id": 237562875, "title": "Simple Entity-Centric Questions Challenge Dense Retrievers", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Christopher Sciavolino", "authorId": "2112021127"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Jinhyuk Lee", "authorId": "46664096"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 167}, "snippets": ["Sparse retrieval Before the emergence of dense retrievers, traditional sparse retrievers such as TF-IDF or BM25 were the de facto method in opendomain question-answering systems (Chen et al., 2017;Yang et al., 2019). These sparse models measure similarity using weighted term-matching between questions and passages and do not train on a particular data distribution. It is well-known that sparse models are great at lexical matching, but fail to capture synonyms and paraphrases.\n\nDense retrieval On the contrary, dense models (Lee et al., 2019;Karpukhin et al., 2020;Guu et al., 2020) measure similarity using learned representations from supervised QA datasets, leveraging pre-trained language models like BERT."], "score": 0.953125}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 237366133, "title": "BERT-based Dense Retrievers Require Interpolation with BM25 for Effective Passage Retrieval", "year": 2021, "venue": "International Conference on the Theory of Information Retrieval", "authors": [{"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 83}, "snippets": ["The integration of pre-trained deep language models, such as BERT, into retrieval and ranking pipelines has shown to provide large effectiveness gains over traditional bag-of-words models in the passage retrieval task. However, the best setup for integrating such deep language models is still unclear. When BERT is used to re-rank passages (i.e., BERT re-ranker), previous work has empirically shown that, while in practice BERT re-ranker cannot act as initial retriever due to BERT's high query time costs, and thus a bag-of-words model such as BM25 is required. It is not necessary to interpolate BERT re-ranker and bag-of-words scores to generate the final ranking. In fact, the BERT re-ranker scores alone can be used by the re-ranker: the BERT re-ranker score appears to already capture the relevance signal provided by BM25. In this paper, we further investigate the topic of interpolating BM25 and BERT-based rankers. Unlike previous work that considered the BERT re-ranker, however, here we consider BERT-based dense retrievers (RepBERT and ANCE). Dense retrievers encode queries and documents into low dimensional BERT-based embeddings. These methods overcome BERT's high computational costs at query time, and can thus be feasibly used in practice as whole-collection retrievers, rather than just as re-rankers. Our novel empirical findings suggest that, unlike for BERT re-ranker, interpolation with BM25 is necessary for BERT-based dense retrievers to perform effectively; and the gains provided by the interpolation are significant. Further analysis reveals why this is so: dense retrievers are very effective at encoding strong relevance signals, but they fail in identifying weaker relevance signals -- a task that the interpolation with BM25 is able to make up for."], "score": 0.0}, {"id": "(Hoshi et al., 2023)", "paper": {"corpus_id": 257427642, "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yasuto Hoshi", "authorId": "2211101536"}, {"name": "D. Miyashita", "authorId": "2441156"}, {"name": "Yasuhiro Morioka", "authorId": "51194024"}, {"name": "Youyang Ng", "authorId": "20556792"}, {"name": "Osamu Torii", "authorId": "2422593"}, {"name": "J. Deguchi", "authorId": "49192096"}], "n_citations": 0}, "snippets": ["Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models", ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."], "score": 0.90625}, {"id": "(McInnes et al., 2018)", "paper": {"corpus_id": 3641284, "title": "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Leland McInnes", "authorId": "31785573"}, {"name": "John Healy", "authorId": "2062756303"}], "n_citations": 9476}, "snippets": ["UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning."], "score": 0.0}, {"id": "(Hsu et al., 2025)", "paper": {"corpus_id": 277451883, "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hsin-Ling Hsu", "authorId": "2349603255"}, {"name": "Jengnan Tzeng", "authorId": "145168633"}], "n_citations": 1}, "snippets": ["Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap."], "score": 0.90478515625}, {"id": "(Some et al., 2025)", "paper": {"corpus_id": 275906690, "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods", "year": 2025, "venue": "Knowledge-Based Systems", "authors": [{"name": "Lilian Some", "authorId": "2342276561"}, {"name": "Wenli Yang", "authorId": "2341600949"}, {"name": "Michael Bain", "authorId": "2342277330"}, {"name": "Byeong Kang", "authorId": "2341910700"}], "n_citations": 0}, "snippets": ["Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109]."], "score": 0.92578125}], "table": null}, {"title": "Implementation and Efficiency Considerations", "tldr": "Sparse and dense retrieval models use fundamentally different indexing and search architectures that impact their computational efficiency. Sparse retrievers like BM25 rely on inverted indices for fast term-based lookups, while dense retrievers require approximate nearest neighbor search algorithms to efficiently find similar vectors. (7 sources)", "text": "\nThe implementation architectures of sparse and dense retrieval models differ significantly, resulting in distinct efficiency characteristics that impact their practical deployment in RAG systems. Sparse retrieval models like BM25 leverage inverted indices, where each term maps to a list of documents containing that term <Paper corpusId=\"245334864\" paperTitle=\"(Piktus et al., 2021)\" isShortName></Paper>. This enables efficient query processing by retrieving only documents that contain at least one query term <Paper corpusId=\"269950696\" paperTitle=\"(Biswas et al., 2024)\" isShortName></Paper>. The constraint that sparse retrievers only produce non-negative weights comes from their reliance on these inverted indexing software stacks built for traditional lexical search <Paper corpusId=\"257585074\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>.\n\nIn contrast, dense retrieval models encode queries and documents into continuous vector representations and require approximate nearest neighbor (ANN) search algorithms for efficient retrieval <Paper corpusId=\"247292113\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268091298\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. This architectural difference creates a tradeoff: while dense retrievers offer better semantic matching, they introduce computational complexities that sparse retrievers avoid. As Biswas et al. note, \"While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large\" <Paper corpusId=\"269950696\" paperTitle=\"(Biswas et al., 2024)\" isShortName></Paper>.\n\nDense retrievers do offer an important efficiency advantage in their workflow structure. They can pre-compute document embeddings offline during indexing, allowing efficient similarity calculations at query time <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. This approach enables \"retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness\" <Paper corpusId=\"248496043\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. The ColBERT model exemplifies this advantage, as it \"can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing\" <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>.\n\nThe implementation differences between sparse and dense retrievers create distinct scaling characteristics. While sparse retrievers like BM25 maintain consistent performance as corpus size increases, dense retrievers may face challenges with very large collections due to the computational demands of ANN search. However, advancements in vector database technology and ANN algorithms continue to improve the scalability of dense retrieval methods <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThese architectural differences ultimately influence the choice of retrieval model in practical RAG systems. Organizations with limited computational resources might prefer sparse retrievers for their efficiency, while those prioritizing retrieval quality might opt for dense retrievers despite their higher computational requirements. The complementary strengths of both approaches have also led to hybrid systems that combine their indexing and search capabilities to achieve both efficiency and effectiveness <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Piktus et al., 2021)", "paper": {"corpus_id": 245334864, "title": "The Web Is Your Oyster - Knowledge-Intensive NLP against a Very Large Web Corpus", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Aleksandra Piktus", "authorId": "120174856"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Dmytro Okhonko", "authorId": "113568063"}, {"name": "Samuel Broscheit", "authorId": "2966239"}, {"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Barlas Ouguz", "authorId": "1628391446"}, {"name": "Edouard Grave", "authorId": "3024698"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}, {"name": "Sebastian Riedel", "authorId": "48662861"}], "n_citations": 66}, "snippets": ["We consider two retrieval architectures. BM25 (Robertson et al., 2009) is a popular sparse model, where queries and documents are represented as highdimensional, sparse vectors, with dimensions corresponding to vocabulary terms and weights indicating their importance. DPR (Karpukhin et al., 2020) is a dense model which embeds queries and documents into a latent, real-valued vector space of a much lower dimensionality-an idea originating from the Latent Semantic Analysis (Deerwester et al., 1990). DPR is based on a neural bi-encoder architecture with passages and queries embedded with separate text encoders. Although both sparse and dense models use the distance in the vector space as the relevance function, they need different indexing schemes to support efficient retrieval."], "score": 0.93408203125}, {"id": "(Biswas et al., 2024)", "paper": {"corpus_id": 269950696, "title": "Efficient and Interpretable Information Retrieval for Product Question Answering with Heterogeneous Data", "year": 2024, "venue": "ECNLP", "authors": [{"name": "Biplob Biswas", "authorId": "2073867373"}, {"name": "R. Ramnath", "authorId": "2253261"}], "n_citations": 1}, "snippets": ["Existing retrieval approaches can be categorized into two groups -sparse and dense.Sparse retrieval uses a token-based sparse representation of the query and the information, such as bag-ofwords (BoW) obtained via TF-IDF (Sparck Jones, 1988) or BM25 (Robertson et al., 1994), and an inverted index for query processing.Although these BoW models facilitate faster retrieval, they rely on exact matches, and hence cannot identify semantically relevant information having a different set of tokens than the query.Dense retrieval, on the other hand, retrieves by comparing dense representations often computed by neural networks such as BERT (Devlin et al., 2019).While these models can perform semantic-level matching, their computational complexity renders them impractical for online real-time ranking when the corpus becomes large."], "score": 0.9267578125}, {"id": "(Nguyen et al., 2023)", "paper": {"corpus_id": 257585074, "title": "A Unified Framework for Learned Sparse Retrieval", "year": 2023, "venue": "European Conference on Information Retrieval", "authors": [{"name": "Thong Nguyen", "authorId": "2116028119"}, {"name": "Sean MacAvaney", "authorId": "22214396"}, {"name": "Andrew Yates", "authorId": "2136074457"}], "n_citations": 29}, "snippets": ["The third distinction is that encoders in sparse retrieval only produce nonnegative weights, whereas dense encoders have no such constraint. This constraint comes from the fact that sparse retrieval relies on software stacks (inverted indexing, query processing algorithms) built for traditional lexical search (e.g., BM25), where weights are always non-negative term frequencies.\n\nWhether these differences lead to systematically different behavior between LSR and dense retrieval methods is an open question. Researchers have observed that LSR models and token-level dense models like ColBERT tend to generalize better than single-vector dense models on the BEIR benchmark [8]35]."], "score": 0.962890625}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 247292113, "title": "Multi-CPR: A Multi Domain Chinese Dataset for Passage Retrieval", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Qiong Gao", "authorId": "2117205083"}, {"name": "Kuan-sheng Zou", "authorId": "3271753"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}, {"name": "Rui Guo", "authorId": "2114009505"}, {"name": "Jianfeng Xu", "authorId": "2146054545"}, {"name": "Guanjun Jiang", "authorId": "2084534507"}, {"name": "Luxi Xing", "authorId": "30967877"}, {"name": "Ping Yang", "authorId": "2327836457"}], "n_citations": 23}, "snippets": ["Sparse retrieval Models: improving retrieval by obtaining semantic-captured sparse representations and indexing them with the inverted index for efficient retrieval; Dense Retrieval Models: converting query and passage into continuous embedding representations and turning to approximate nearest neighbor (ANN) algorithms for fast retrieval [13]."], "score": 0.92431640625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268091298, "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Penghao Zhao", "authorId": "2268718776"}, {"name": "Hailin Zhang", "authorId": "2288557803"}, {"name": "Qinhan Yu", "authorId": "2289597580"}, {"name": "Zhengren Wang", "authorId": "2288675277"}, {"name": "Yunteng Geng", "authorId": "2288532368"}, {"name": "Fangcheng Fu", "authorId": "46182701"}, {"name": "Ling Yang", "authorId": "2249513224"}, {"name": "Wentao Zhang", "authorId": "2277807793"}, {"name": "Bin Cui", "authorId": "2277742543"}], "n_citations": 282}, "snippets": ["Sparse retrieval methods are commonly used in document retrieval, where the keys/values represent the documents to be searched. These methods leverage", "term matching metrics such as TF-IDF [67], query likelihood [68], and BM25 [19], which analyze word statistics from texts and construct inverted indices for efficient searching. Essentially, BM25 is a strong baseline in large-scale web search, integrating inverse document frequency weights, query token occurrences, and other pertinent metrics.\n\nTo enable efficient search, sparse retrieval typically leverages an inverted index to organize documents. Concretely, each term from the query performs a lookup to obtain a list of candidate documents, which are subsequently ranked based on their statistical scores.\n\nUnlike sparse retrieval, dense retrieval methods represent queries and keys using dense embedding vectors, and build Approximate Nearest Neighbor (ANN) index to speed up the search. This can be applied to all modalities. For text data, recent advancements in pre-trained models (such as BERT [15]) have been employed encode queries and keys individually [20]. This approach is often referred to as Dense Passage Retrieval (DPR). Similar to text, models have been proposed to encode code data [25], audio data [69], image data [24], video data [70], etc. The similarity score between dense representations are usually computed with metrics such as cosine, inner product, L2-distance."], "score": 0.923828125}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 248496043, "title": "To Interpolate or not to Interpolate: PRF, Dense and Sparse Retrievers", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hang Li", "authorId": "2118384241"}, {"name": "Shuai Wang", "authorId": "2146514461"}, {"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "Ahmed Mourad", "authorId": "143832672"}, {"name": "Xueguang Ma", "authorId": "2461713"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 30}, "snippets": ["Traditional unsupervised (bag-of-words -BOWs) sparse retrieval models, such as BM25, use exact term matching to retrieve relevant results from the collection. Recent studies have shown that these models are more likely to retrieve results that partially match the query, i.e., with low relevance labels (Wang et al., 2021). Although unsupervised sparse models often fail to rank the most relevant results at the top, they often offer high recall. Combined with high efficiency, unsupervised bag-of-words sparse retrieval models like BM25 are still widely used within information retrieval pipelines, often as the initial retrieval stage of a more complex setup. To further enhance precision and push highly relevant results to the top, transformer-based dense retrievers (short for learned dense representations) strike a good balance between effectiveness and efficiency compared to traditional unsupervised sparse models and transformer-based deep language model re-rankers (Hofst\u00e4tter et al., 2021)17,(Lin et al., 2021)(Qu et al., 2020)(Ren et al., 2021)(Xiong et al., 2020)[30]. Dense retrievers utilise dual BERT-style encoders to encode queries and passages separately [16]; this allows the preencoding of passages into embeddings at indexing time and their offline storage. During query time, the query embeddings can be efficiently computed \"on-the-fly\" (Zhuang et al., 2021), and relevance estimations measured with a simple similarity calculation. Thus, it becomes feasible to perform retrieval over the entire collection using deep language models with efficiency comparable to traditional unsupervised sparse models, but with much higher effectiveness. While dense retrievers are very effective at encoding passages characterised by high relevance labels (i.e. highly relevant passages), they are less effective at identifying passages of lower relevance value (Wang et al., 2021)."], "score": 0.9609375}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}], "table": null}, {"title": "Complementary Nature and Hybrid Approaches", "tldr": "Sparse retrieval methods like BM25 and dense retrieval methods like DPR capture different aspects of relevance with minimal results overlap, making hybrid approaches that combine both methods consistently outperform either method alone. (5 sources)", "text": "\nDespite their fundamental differences, sparse and dense retrieval models aren't inherently competing approaches but rather complementary techniques that excel in different scenarios. Studies have shown that the overlap of results between dense and sparse retrieval methods is quite small, indicating that they capture different aspects of relevance <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper>. This limited overlap suggests that each retrieval approach identifies different relevant documents that the other might miss.\n\nThis complementary nature has led to the development of hybrid retrieval approaches that combine the strengths of both sparse and dense methods. These hybrid systems have consistently demonstrated superior performance compared to either method used in isolation <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper>. By combining BM25's precise keyword matching capabilities with dense retrieval's semantic understanding, hybrid approaches can address the weaknesses of each individual method while leveraging their respective strengths.\n\nThe effectiveness of hybrid approaches has been well-documented across various retrieval tasks. For example, the combination of Dense Passage Retrieval (DPR) with traditional sparse retrieval methods like BM25 has been shown to enhance retrieval precision, particularly in complex or high-precision tasks <Paper corpusId=\"275906690\" paperTitle=\"(Some et al., 2025)\" isShortName></Paper>. This improvement stems from BM25's ability to capture exact lexical matches that dense retrievers might miss, while dense methods contribute semantic matching capabilities that identify relevant passages without lexical overlap.\n\nEmpirically, researchers have demonstrated that ensemble results combining the relevance scores from both sparse and dense retrievers can exceed the performance of each approach used alone <Paper corpusId=\"257427642\" paperTitle=\"(Hoshi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>. More recently, advanced hybrid approaches like \"Blended RAG\" have leveraged both dense vector indexes and sparse encoder indexes with hybrid query strategies to achieve superior retrieval results across benchmark datasets <Paper corpusId=\"277451883\" paperTitle=\"(Hsu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269043117\" paperTitle=\"(Sawarkar et al., 2024)\" isShortName></Paper>.\n\nIn practical RAG applications, this complementary relationship suggests that rather than choosing between sparse and dense retrieval, the most effective approach often involves integrating both methods. Such integration allows RAG systems to benefit from BM25's robust keyword matching and zero-shot capabilities while also leveraging the semantic understanding provided by dense retrievers, resulting in more comprehensive and accurate information retrieval.", "citations": [{"id": "(Hoshi et al., 2023)", "paper": {"corpus_id": 257427642, "title": "Can a Frozen Pretrained Language Model be used for Zero-shot Neural Retrieval on Entity-centric Questions?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yasuto Hoshi", "authorId": "2211101536"}, {"name": "D. Miyashita", "authorId": "2441156"}, {"name": "Yasuhiro Morioka", "authorId": "51194024"}, {"name": "Youyang Ng", "authorId": "20556792"}, {"name": "Osamu Torii", "authorId": "2422593"}, {"name": "J. Deguchi", "authorId": "49192096"}], "n_citations": 0}, "snippets": ["Passage Retrieval. Unsupervised sparse retrievers have traditionally been used for IR tasks, including answering open-domain questions (Chen et al., 2017). They are based on bag-of-words exact lexical matching, including TF-IDF and a best-match weighting function called BM25 (Robertson et al., 1994)(McInnes et al., 2018). Unlike sparse retrieval, dense retrieval is based on semantic matching in the embedding space. Dense retrievers leverage dense vector representations of sentences embedded by fine-tuned neural language models", ".Difference between Dense Retrieval and Sparse Retrieval. (McInnes et al., 2018) showed that the overlap of results between dense and sparse retrieval was quite small. It has been known empirically that the ensemble results for these relevance scores can exceed the performance of each alone (e.g., Karpukhin et al. 2020)."], "score": 0.90625}, {"id": "(Hsu et al., 2025)", "paper": {"corpus_id": 277451883, "title": "DAT: Dynamic Alpha Tuning for Hybrid Retrieval in Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hsin-Ling Hsu", "authorId": "2349603255"}, {"name": "Jengnan Tzeng", "authorId": "145168633"}], "n_citations": 1}, "snippets": ["Hybrid retrieval (Ma et al., 2020)(Sawarkar et al., 2024)Berntson, 2023) approaches combining sparse (e.g., BM25) and dense methods have demonstrated superior performance compared to either method alone. BM25 (Robertson et al., 1994) excels at precise keyword matching through term frequency calculations, while dense retrieval (Karpukhin et al., 2020) captures semantic relationships that may not involve direct lexical overlap."], "score": 0.90478515625}, {"id": "(Some et al., 2025)", "paper": {"corpus_id": 275906690, "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods", "year": 2025, "venue": "Knowledge-Based Systems", "authors": [{"name": "Lilian Some", "authorId": "2342276561"}, {"name": "Wenli Yang", "authorId": "2341600949"}, {"name": "Michael Bain", "authorId": "2342277330"}, {"name": "Byeong Kang", "authorId": "2341910700"}], "n_citations": 0}, "snippets": ["Dense Passage Retrieval (DPR) technique utilizes dense embeddings to match semantically relevant document chunks to queries and the combination of DPR with traditional sparse retrieval (i.e., BM25) has been shown to further enhance retrieval precision in complex or high-precision tasks [82]109]."], "score": 0.92578125}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Sawarkar et al., 2024)", "paper": {"corpus_id": 269043117, "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers", "year": 2024, "venue": "Conference on Multimedia Information Processing and Retrieval", "authors": [{"name": "Kunal Sawarkar", "authorId": "2003089508"}, {"name": "Abhilasha Mangal", "authorId": "2295990127"}, {"name": "S. R. Solanki", "authorId": "2295990033"}], "n_citations": 57}, "snippets": ["Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a private knowledge base of documents with Large Language Models (LLM) to build Generative Q&A (Question-Answering) systems. However, RAG accuracy becomes increasingly challenging as the corpus of documents scales up, with Retrievers playing an outsized role in the overall RAG accuracy by extracting the most relevant document from the corpus to provide context to the LLM. In this paper, we propose the \u2018Blended RAG\u2019 method of leveraging semantic search techniques, such as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid query strategies. Our study achieves better retrieval results and sets new benchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID datasets. We further extend such a \u2018Blended Retriever\u2019 to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."], "score": 0.0}], "table": null}], "cost": 0.6736709999999999}}
