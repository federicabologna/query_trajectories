{"better_query": "What are the most advanced multimodal language models capable of generating both text and images as output, and what architectures enable this capability?", "better_answer": {"sections": [{"title": "Introduction to Multimodal Language Models", "tldr": "Multimodal language models (MLMs) represent the evolution of AI systems capable of processing and generating content across different modalities including text, images, audio, and video. The most advanced models integrate separate encoders and decoders for different modalities, with sophisticated alignment mechanisms that enable cross-modal generation capabilities. (LLM Memory)", "text": "\nMultimodal language models (MLMs) extend traditional language models by incorporating the ability to understand and generate content across multiple modalities, such as text, images, audio, and video. These models represent a significant advancement in AI capabilities, moving beyond text-only processing to comprehend and create rich, multi-format content that more closely mimics human communication patterns. The most advanced MLMs capable of text-to-image and image-to-text generation typically employ specialized neural architectures that can encode and decode different types of data representations. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nAt their core, these models must solve the fundamental challenge of aligning different representational spaces - the semantic space of language and the visual space of images. This alignment enables the models to translate concepts from one modality to another, allowing for capabilities like generating images from textual descriptions or providing textual explanations of visual content. The architectural designs that facilitate this cross-modal generation typically involve either end-to-end trained unified models or modular approaches that combine specialized components for each modality. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent advances in transformer-based architectures have significantly accelerated the development of these multimodal systems, allowing them to handle increasingly complex tasks that require simultaneous reasoning across different types of information. The progression from unimodal to multimodal models reflects the broader trend in AI research toward creating more versatile systems that can interact with the world in ways that more closely resemble human cognitive capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Notable Advanced Multimodal Models with Text-Image Generation Capabilities", "tldr": "The landscape of multimodal language models has evolved rapidly with several notable systems now capable of both understanding and generating content across text and image modalities. Key models like DreamLLM, Chameleon, LLaVA, and Emu employ different architectural approaches including diffusion models, discrete tokenization, and early fusion techniques to achieve cross-modal generation capabilities. (20 sources)", "text": "\n## DALL-E and CM3\nEarly pioneering multimodal models like DALL-E and CM3 demonstrated remarkable progress in text-to-image and image-to-text generation, but primarily stored all learned knowledge within model parameters, requiring increasingly larger models and training datasets to capture more knowledge <Paper corpusId=\"253802096\" paperTitle=\"(Yasunaga et al., 2023)\" isShortName></Paper>.\n\n## Vision-Language Models\nSeveral notable vision-language models have advanced the field with multimodal capabilities:\n\n- **LLaVA (Language-Image LLaMA)**: Extends the LLaMA architecture to handle both language and vision modalities, enabling tasks like image captioning, visual question answering, and image-based text generation <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. LLaVA was the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n- **BLIP and BLIP-2**: Among the earliest unified vision-language encoder-decoder architectures, introducing a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411402\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n- **Qwen-VL**: A multimodal version of the Qwen large model series that accepts image, text, and bounding box as inputs while outputting text and bounding boxes <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n- **InternLM-XComposer2-VL**: A cutting-edge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs using a Partial LoRA approach <Paper corpusId=\"266191741\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>.\n\n## Text-Image Generation Models\nRecent advances have focused on models with bidirectional generation capabilities:\n\n- **GILL, Emu, and DreamLLM**: These models specialize in interleaved vision-language generation by aligning the output space of multimodal language models with diffusion image decoders <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. DreamLLM specifically focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space, fostering the generation of interleaved documents with both text and image content <Paper corpusId=\"276422421\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n- **Emu2**: A 37B-parameter generative multimodal model demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning <Paper corpusId=\"276422421\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\n- **KOSMOS-G**: Leverages a multimodal large language model to encode interleaved text-visual inputs, using the U-Net of Stable Diffusion v1.5 as the image decoder <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n- **NExT-GPT**: An end-to-end general-purpose any-to-any multimodal system connecting an LLM with multimodal adaptors and different diffusion decoders, enabling perception of inputs and generation of outputs in arbitrary combinations of text, images, videos, and audio <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261696650\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\n## Unified Multimodal Architectures\nSome models use novel architectural approaches for unified multimodal processing:\n\n- **Chameleon**: A family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence <Paper corpusId=\"269791516\" paperTitle=\"(Team et al., 2024)\" isShortName></Paper>. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture, training the model from scratch in an end-to-end manner <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\n- **SEED-X**: A unified and versatile foundation system where continuous image representations from a CLIP ViT encoder are combined with text tokens and fed into a large language model to perform next-word prediction and image representation regression <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n- **Transfusion and Show-O**: These models demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n- **UniFluid**: A unified autoregressive framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks, built upon pre-trained Gemma <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\n- **CoDi (Composable Diffusion)**: A novel generative model capable of generating any combination of output modalities from any combination of input modalities, employing a novel composable generation strategy <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n- **AnyGPT**: A model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music <Paper corpusId=\"270688372\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>.\n\nThe evolution of these models shows a clear trend toward unified architectures that can seamlessly handle multiple modalities while performing both understanding and generation tasks within a single framework.", "citations": [{"id": "(Yasunaga et al., 2023)", "paper": {"corpus_id": 253802096, "title": "Retrieval-Augmented Multimodal Language Modeling", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Armen Aghajanyan", "authorId": "2201435"}, {"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "J. Leskovec", "authorId": "1702139"}, {"name": "Percy Liang", "authorId": "145419642"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 108}, "snippets": ["Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge."], "score": 0.9912109375}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 266191741, "title": "PromptBench: A Unified Library for Evaluation of Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kaijie Zhu", "authorId": "2543684"}, {"name": "Qinlin Zhao", "authorId": "2261935625"}, {"name": "Hao Chen", "authorId": "2261741520"}, {"name": "Jindong Wang", "authorId": "2145270616"}, {"name": "Xing Xie", "authorId": "2249681654"}], "n_citations": 26}, "snippets": ["\u2022 LLaVA (Liu et al., 2024): LlaVA (Language-Image LLaMA) is a multimodal model combining language and image data. It extends the LLaMA architecture to handle both modalities, enabling tasks like image captioning, visual question answering, and imagebased text generation.\n\n\u2022 Qwen-VL series (Bai et al., 2023): Qwen-VL (Qwen Large Vision Language Model) is the multimodal version of the large model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-VL accepts image, text, and bounding box as inputs, outputs text, and bounding box.\n\n\u2022 InternLM-XComposer2-VL (Dong et al., 2024): InternLM-XComposer2 is a cuttingedge vision-language model excelling in free-form text-image composition and comprehension, crafting content from diverse inputs like outlines, detailed specs, and reference images. Using a Partial LoRA (PLoRA) approach, it balances vision understanding and text composition."], "score": 0.96044921875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258179774, "title": "Visual Instruction Tuning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2109737569"}, {"name": "Qingyang Wu", "authorId": "31060482"}, {"name": "Yong Jae Lee", "authorId": "144756076"}], "n_citations": 4921}, "snippets": ["Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."], "score": 0.0}, {"id": "(Khaldi et al., 2024)", "paper": {"corpus_id": 273022981, "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People", "year": 2024, "venue": "", "authors": [{"name": "Rohaifa Khaldi", "authorId": "35729419"}, {"name": "Domingo Alcaraz-Segura", "authorId": "2273276043"}, {"name": "Ignacio S'anchez-Herrera", "authorId": "2323737781"}, {"name": "Javier Mart\u00ednez-L\u00f3pez", "authorId": "2271941340"}, {"name": "Carlos Javier Navarro", "authorId": "2323747675"}, {"name": "S. Tabik", "authorId": "3030006"}], "n_citations": 1}, "snippets": ["LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language", "Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT."], "score": 0.9697265625}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 246411402, "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junnan Li", "authorId": "49299019"}, {"name": "Dongxu Li", "authorId": "2981509"}, {"name": "Caiming Xiong", "authorId": "2054594326"}, {"name": "S. Hoi", "authorId": "1741126"}], "n_citations": 4413}, "snippets": ["Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."], "score": 0.0}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 270391394, "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hao Yang", "authorId": "2261895888"}, {"name": "Yanyan Zhao", "authorId": "49339265"}, {"name": "Yang Wu", "authorId": "143792101"}, {"name": "Shilong Wang", "authorId": "2214828656"}, {"name": "Tian Zheng", "authorId": "2261492452"}, {"name": "Hongbo Zhang", "authorId": "2261686105"}, {"name": "Wanxiang Che", "authorId": "2279751409"}, {"name": "Bing Qin", "authorId": "2203961541"}], "n_citations": 13}, "snippets": ["Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more", "BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer."], "score": 0.9580078125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 267200040, "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "2289660121"}, {"name": "Xue Xu", "authorId": "2118734325"}, {"name": "Jiachen Liu", "authorId": "2288740225"}, {"name": "Xinyan Xiao", "authorId": "2288393805"}], "n_citations": 8}, "snippets": ["Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."], "score": 0.96435546875}, {"id": "(Fang et al., 2025)", "paper": {"corpus_id": 276422421, "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yi Fang", "authorId": "2306162276"}, {"name": "Bowen Jin", "authorId": "2057050247"}, {"name": "Jiacheng Shen", "authorId": "2346106510"}, {"name": "Sirui Ding", "authorId": "2307734628"}, {"name": "Qiaoyu Tan", "authorId": "2282981308"}, {"name": "Jiawei Han", "authorId": "2257136881"}], "n_citations": 2}, "snippets": ["To address this limitation, DreamLLM [6] integrates an LLM backbone with a diffusion model to enable image generation as a multimodal output. Emu2 [31] scales this architecture to 37B parameters, demonstrating strong multimodal in-context learning and the ability to handle complex tasks requiring real-time reasoning, such as visual prompting and object-grounded generation. Chameleon [33] proposes a stable training strategy from the ground up, featuring an alignment process and architectural parameterization tailored to early-fusion, token-based, mixed-modal settings."], "score": 0.97900390625}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 266374640, "title": "Generative Multimodal Models are In-Context Learners", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Quan Sun", "authorId": "2264040642"}, {"name": "Yufeng Cui", "authorId": "2263702215"}, {"name": "Xiaosong Zhang", "authorId": "2264589602"}, {"name": "Fan Zhang", "authorId": "2264274388"}, {"name": "Qiying Yu", "authorId": "23716915"}, {"name": "Zhengxiong Luo", "authorId": "2275808712"}, {"name": "Yueze Wang", "authorId": "2217456303"}, {"name": "Yongming Rao", "authorId": "2275170369"}, {"name": "Jingjing Liu", "authorId": "2222717281"}, {"name": "Tiejun Huang", "authorId": "2257137554"}, {"name": "Xinlong Wang", "authorId": "2263959521"}], "n_citations": 290}, "snippets": ["The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 271709823, "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dongyang Liu", "authorId": "2308683426"}, {"name": "Shitian Zhao", "authorId": "2283419424"}, {"name": "Le Zhuo", "authorId": "2300371391"}, {"name": "Weifeng Lin", "authorId": "2284068796"}, {"name": "Yu Qiao", "authorId": "2312346585"}, {"name": "Hongsheng Li", "authorId": "2266421952"}, {"name": "Peng Gao", "authorId": "2310909023"}], "n_citations": 59}, "snippets": ["Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model."], "score": 0.96044921875}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 261696650, "title": "NExT-GPT: Any-to-Any Multimodal LLM", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Shengqiong Wu", "authorId": "1957924118"}, {"name": "Hao Fei", "authorId": "46959445"}, {"name": "Leigang Qu", "authorId": "1990265392"}, {"name": "Wei Ji", "authorId": "144540018"}, {"name": "Tat-Seng Chua", "authorId": "144078686"}], "n_citations": 506}, "snippets": ["While recently Multimodal Large Language Models (MM-LLMs) have made exciting strides, they mostly fall prey to the limitation of only input-side multimodal understanding, without the ability to produce content in multiple modalities. As we humans always perceive the world and communicate with people through various modalities, developing any-to-any MM-LLMs capable of accepting and delivering content in any modality becomes essential to human-level AI. To fill the gap, we present an end-to-end general-purpose any-to-any MM-LLM system, NExT-GPT. We connect an LLM with multimodal adaptors and different diffusion decoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary combinations of text, images, videos, and audio. By leveraging the existing well-trained highly-performing encoders and decoders, NExT-GPT is tuned with only a small amount of parameter (1%) of certain projection layers, which not only benefits low-cost training and also facilitates convenient expansion to more potential modalities. Moreover, we introduce a modality-switching instruction tuning (MosIT) and manually curate a high-quality dataset for MosIT, based on which NExT-GPT is empowered with complex cross-modal semantic understanding and content generation. Overall, our research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community. Project page: https://next-gpt.github.io/"], "score": 0.0}, {"id": "(Team et al., 2024)", "paper": {"corpus_id": 269791516, "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chameleon Team", "authorId": "2301457346"}, {"name": "Jacob Kahn", "authorId": "39960571"}], "n_citations": 339}, "snippets": ["We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model."], "score": 0.98291015625}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 271924334, "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jinheng Xie", "authorId": "2306603347"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "David Junhao Zhang", "authorId": "2248388634"}, {"name": "Weihao Wang", "authorId": "2316668014"}, {"name": "Kevin Qinghong Lin", "authorId": "2298569956"}, {"name": "Yuchao Gu", "authorId": "2248678348"}, {"name": "Zhijie Chen", "authorId": "2316662510"}, {"name": "Zhenheng Yang", "authorId": "2309246065"}, {"name": "Mike Zheng Shou", "authorId": "2244623976"}], "n_citations": 229}, "snippets": ["In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."], "score": 0.9619140625}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276782523, "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rui Zhao", "authorId": "2248946985"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Mike Zheng Shou", "authorId": "2344762475"}], "n_citations": 1}, "snippets": ["Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."], "score": 0.9658203125}, {"id": "(Niu et al., 2025)", "paper": {"corpus_id": 276929205, "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yuwei Niu", "authorId": "2350309448"}, {"name": "Munan Ning", "authorId": "2253434320"}, {"name": "Mengren Zheng", "authorId": "2349744842"}, {"name": "Bin Lin", "authorId": "2333420566"}, {"name": "Peng Jin", "authorId": "2328406001"}, {"name": "Jiaqi Liao", "authorId": "2315613899"}, {"name": "Kun-Peng Ning", "authorId": "2253467098"}, {"name": "Bin Zhu", "authorId": "2256419313"}, {"name": "Li Yuan", "authorId": "2280992738"}], "n_citations": 26}, "snippets": ["Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."], "score": 0.96630859375}, {"id": "(Fan et al., 2025)", "paper": {"corpus_id": 277103675, "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lijie Fan", "authorId": "2347484699"}, {"name": "Luming Tang", "authorId": "34689393"}, {"name": "Siyang Qin", "authorId": "2333872078"}, {"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Xuan Yang", "authorId": "2350843695"}, {"name": "Siyuan Qiao", "authorId": "2266238535"}, {"name": "Andreas Steiner", "authorId": "2350755056"}, {"name": "Chen Sun", "authorId": "2333512682"}, {"name": "Yuanzhen Li", "authorId": "2281034398"}, {"name": "Tao Zhu", "authorId": "2351408307"}, {"name": "Michael Rubinstein", "authorId": "2325950285"}, {"name": "Michalis Raptis", "authorId": "3063676"}, {"name": "Deqing Sun", "authorId": "2262516617"}, {"name": "Radu Soricut", "authorId": "1737285"}], "n_citations": 8}, "snippets": ["We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image", "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."], "score": 0.9794921875}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258822817, "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zineng Tang", "authorId": "151270642"}, {"name": "Ziyi Yang", "authorId": "2155459391"}, {"name": "Chenguang Zhu", "authorId": "8652308"}, {"name": "Michael Zeng", "authorId": "48262024"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 188}, "snippets": ["We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"], "score": 0.0}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 270688372, "title": "Holistic Evaluation for Interleaved Text-and-Image Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Minqian Liu", "authorId": "2123130842"}, {"name": "Zhiyang Xu", "authorId": "2136442661"}, {"name": "Zihao Lin", "authorId": "2254334449"}, {"name": "Trevor Ashby", "authorId": "2243992517"}, {"name": "Joy Rimchala", "authorId": "2218905547"}, {"name": "Jiaxin Zhang", "authorId": "2307975528"}, {"name": "Lifu Huang", "authorId": "2238885968"}], "n_citations": 11}, "snippets": ["Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023). These models leverage advanced architectures and training techniques to enhance their ability to produce coherent and contextually relevant interleaved content."], "score": 0.97021484375}, {"id": "(Zhan et al., 2024)", "paper": {"corpus_id": 267750101, "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jun Zhan", "authorId": "2217531907"}, {"name": "Junqi Dai", "authorId": "2087363104"}, {"name": "Jiasheng Ye", "authorId": "2153258452"}, {"name": "Yunhua Zhou", "authorId": "2118117212"}, {"name": "Dong Zhang", "authorId": "2109797247"}, {"name": "Zhigeng Liu", "authorId": "2284732560"}, {"name": "Xin Zhang", "authorId": "2278615698"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Linyang Li", "authorId": "2107897400"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Jie Fu", "authorId": "2276508494"}, {"name": "Tao Gui", "authorId": "2067331064"}, {"name": "Tianxiang Sun", "authorId": "153345698"}, {"name": "Yugang Jiang", "authorId": "2284724292"}, {"name": "Xipeng Qiu", "authorId": "1767521"}], "n_citations": 136}, "snippets": ["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"], "score": 0.0}], "table": null}, {"title": "Architectural Approaches for Multimodal Generation", "tldr": "Multimodal generation models employ diverse architectural approaches including unified transformer-based designs, dual-stream architectures, and hybrid systems that combine large language models with specialized visual components. Key design patterns include early fusion methods that process tokens from all modalities uniformly, adapter-based approaches that connect pre-trained components, and end-to-end architectures that handle multiple modalities within a single framework. (24 sources)", "text": "\n## Unified Transformer Architectures\nOne major approach to multimodal generation involves unified transformer architectures that process both text and image tokens within a single framework. These models represent both modalities as discrete tokens that can be handled by the same transformer layers. The Chameleon family of models pioneered this early-fusion token-based approach, capable of understanding and generating images and text in any sequence by representing all modalities as discrete tokens within a unified architecture <Paper corpusId=\"265498814\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Similarly, AnyGPT utilizes discrete representations for unified processing of various modalities, including speech, text, images, and music, without requiring alterations to standard LLM architectures <Paper corpusId=\"274597781\" paperTitle=\"(Serra et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>.\n\nModels like UniFluid demonstrate that a unified autoregressive framework can leverage continuous visual tokens to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"278327487\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. These approaches typically employ either vector-quantized autoregressive models or diffusion-based methods to create consistent token representations across modalities <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>.\n\n## Connector and Adapter Approaches\nAnother prevalent approach involves connecting pre-trained vision and language models through specialized connectors or adapters. For example, Emu proposes an efficient mapping network that translates text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>. Similarly, the Joint Autoregressive Mixture (JAM) framework systematically fuses existing text and image generation models in a modular approach <Paper corpusId=\"262898422\" paperTitle=\"(Aiello et al., 2023)\" isShortName></Paper>.\n\nMany open-source Multimodal Large Language Models (MLLMs) employ architectures with a pre-trained vision encoder, an LLM, and a crossmodal connector. For instance, the BLIP series utilized a Q-Former module to align text and images <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>, while Flamingo employed a gated cross-attention mechanism to integrate images and text <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. LLaVA-1.5 adopted an MLP projector to connect the pre-trained vision encoder with the LLM <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\n## Dual-Stream Architectures\nDual-stream architectures employ separate encoders for different modalities with interaction mechanisms between them. One strategy involves a distinct CNN encoder for visual features and a transformer encoder for text, interacting via co-attentional transformer layers <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>. The CNN extracts visual features providing contextual information to guide language generation, while the transformer handles text semantics. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate alongside text token encoders <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>.\n\n## Diffusion-Based Approaches\nDiffusion models have become central to many multimodal generation architectures. Recent advancements integrate diffusion-based generation with language models in various ways. One approach involves utilizing pre-trained text-to-image diffusion models while training LLMs to generate conditions for these models <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Models like DreamLLM employ diffusion models for the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nTransfusion and Show-O have demonstrated that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>. Similarly, research by Zou et al. explores integrating various visual generation paradigms, such as diffusion models, flow-based generative models, and vector-quantized autoregressive models with LLMs <Paper corpusId=\"276928288\" paperTitle=\"(Zou et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268247980\" paperTitle=\"(Esser et al., 2024)\" isShortName></Paper>.\n\n## End-to-End vs. Cascaded Systems\nArchitectural approaches can also be categorized by their integration style. Some systems, like Visual ChatGPT, connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which can introduce instability and noise <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>. In contrast, end-to-end optimized models align the output space with image diffusion models, enabling both perception and generation of images and text within a unified framework <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>.\n\nThe CoDi (Composable Diffusion) model exemplifies an end-to-end approach as a novel generative model capable of generating any combination of output modalities from any combination of input modalities, employing a composable generation strategy <Paper corpusId=\"274597781\" paperTitle=\"(Serra et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n## Multimodal Output Decoders\nA critical component in multimodal architectures is the output decoder, which reconverts fused, integrated multimodal information back into usable forms tailored to specific tasks. For image captioning, the decoder might generate descriptive text based on visual inputs, while for video understanding tasks, it could produce annotations or summaries combining both visual and auditory data <Paper corpusId=\"271693411\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Efficiency Considerations\nAs multimodal models grow in complexity, efficiency becomes increasingly important. MLLMs face significant computational and memory demands when processing long contexts in multimodal inputs <Paper corpusId=\"273233771\" paperTitle=\"(Liu et al._2, 2024)\" isShortName></Paper>. Recent models like InternLM-XComposer-2.5 address this by supporting long-contextual input and output, enabling the model to handle extensive multimodal contexts efficiently <Paper corpusId=\"275337159\" paperTitle=\"(Franklin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270923961\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Other approaches like mixture-of-experts have been introduced to improve scalability while enhancing relational reasoning capabilities <Paper corpusId=\"275337159\" paperTitle=\"(Franklin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267311517\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\nThese architectural approaches collectively represent the evolving landscape of multimodal generation systems, with each design offering different trade-offs in terms of integration depth, efficiency, and generative capabilities <Paper corpusId=\"278339632\" paperTitle=\"(Jiang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 265498814, "title": "SEED-Bench-2: Benchmarking Multimodal Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Bohao Li", "authorId": "1491798621"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Guangzhi Wang", "authorId": "2243452799"}, {"name": "Rui Wang", "authorId": "2151036422"}, {"name": "Ruimao Zhang", "authorId": "2268726753"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 69}, "snippets": ["Recent work [11,17,18,34,54,60] further empower LLMs with the ability of generating images beyond texts (acting like a combination of GPT-4V [1] and DALL-E 3 [5]), since they contend that the premise for the emergence of multimodal capabilities is that text and image can be represented and processed interchangeably in a unified autoregressive Transformer."], "score": 0.96826171875}, {"id": "(Serra et al., 2024)", "paper": {"corpus_id": 274597781, "title": "The Narrow Gate: Localized Image-Text Communication in Vision-Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Alessandro Serra", "authorId": "2319817038"}, {"name": "Francesco Ortu", "authorId": "2284691756"}, {"name": "Emanuele Panizon", "authorId": "2334479581"}, {"name": "L. Valeriani", "authorId": "108193025"}, {"name": "Lorenzo Basile", "authorId": "2308039508"}, {"name": "A. Ansuini", "authorId": "5366502"}, {"name": "Diego Doimo", "authorId": "104350259"}, {"name": "Alberto Cazzaniga", "authorId": "2269294135"}], "n_citations": 0}, "snippets": ["Recently, the availability of large open datasets [14,15] and improved techniques to align text and image embeddings (Ge et al., 2023) have also enabled the creation of multimodal models that can both understand and generate visual content within a single architecture (Tang et al., 2023)[18](Zhan et al., 2024)[20]."], "score": 0.97509765625}, {"id": "(Zhan et al., 2024)", "paper": {"corpus_id": 267750101, "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jun Zhan", "authorId": "2217531907"}, {"name": "Junqi Dai", "authorId": "2087363104"}, {"name": "Jiasheng Ye", "authorId": "2153258452"}, {"name": "Yunhua Zhou", "authorId": "2118117212"}, {"name": "Dong Zhang", "authorId": "2109797247"}, {"name": "Zhigeng Liu", "authorId": "2284732560"}, {"name": "Xin Zhang", "authorId": "2278615698"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Linyang Li", "authorId": "2107897400"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Jie Fu", "authorId": "2276508494"}, {"name": "Tao Gui", "authorId": "2067331064"}, {"name": "Tianxiang Sun", "authorId": "153345698"}, {"name": "Yugang Jiang", "authorId": "2284724292"}, {"name": "Xipeng Qiu", "authorId": "1767521"}], "n_citations": 136}, "snippets": ["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"], "score": 0.0}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 278327487, "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xinjie Zhang", "authorId": "2359211182"}, {"name": "Jintao Guo", "authorId": "2359201363"}, {"name": "Shanshan Zhao", "authorId": "2337775059"}, {"name": "Minghao Fu", "authorId": "2345947101"}, {"name": "Lunhao Duan", "authorId": "2275153684"}, {"name": "Guo-Hua Wang", "authorId": "2339237238"}, {"name": "Qing-Guo Chen", "authorId": "2304395071"}, {"name": "Zhao Xu", "authorId": "2304361003"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 1}, "snippets": ["In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation."], "score": 0.97998046875}, {"id": "(Niu et al., 2025)", "paper": {"corpus_id": 276929205, "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yuwei Niu", "authorId": "2350309448"}, {"name": "Munan Ning", "authorId": "2253434320"}, {"name": "Mengren Zheng", "authorId": "2349744842"}, {"name": "Bin Lin", "authorId": "2333420566"}, {"name": "Peng Jin", "authorId": "2328406001"}, {"name": "Jiaqi Liao", "authorId": "2315613899"}, {"name": "Kun-Peng Ning", "authorId": "2253467098"}, {"name": "Bin Zhu", "authorId": "2256419313"}, {"name": "Li Yuan", "authorId": "2280992738"}], "n_citations": 26}, "snippets": ["Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."], "score": 0.96630859375}, {"id": "(Koh et al., 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."], "score": 0.98779296875}, {"id": "(Aiello et al., 2023)", "paper": {"corpus_id": 262898422, "title": "Jointly Training Large Autoregressive Multimodal Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Emanuele Aiello", "authorId": "2185505465"}, {"name": "L. Yu", "authorId": "49297123"}, {"name": "Yixin Nie", "authorId": "2247227174"}, {"name": "Armen Aghajanyan", "authorId": "2201435"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}], "n_citations": 31}, "snippets": ["In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models."], "score": 0.986328125}, {"id": "(Cao et al., 2024)", "paper": {"corpus_id": 273350685, "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yue Cao", "authorId": "2312369775"}, {"name": "Yangzhou Liu", "authorId": "2312345209"}, {"name": "Zhe Chen", "authorId": "2305731793"}, {"name": "Guangchen Shi", "authorId": "2146870036"}, {"name": "Wenhai Wang", "authorId": "2257133501"}, {"name": "Danhuai Zhao", "authorId": "2326454048"}, {"name": "Tong Lu", "authorId": "2276323159"}], "n_citations": 8}, "snippets": ["Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector."], "score": 0.9599609375}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junnan Li", "authorId": "49299019"}, {"name": "Dongxu Li", "authorId": "2981509"}, {"name": "S. Savarese", "authorId": "1702137"}, {"name": "Steven C. H. Hoi", "authorId": "2184854289"}], "n_citations": 4653}, "snippets": ["The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."], "score": 0.0}, {"id": "(Alayrac et al., 2022)", "paper": {"corpus_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jean-Baptiste Alayrac", "authorId": "2285263"}, {"name": "Jeff Donahue", "authorId": "7408951"}, {"name": "Pauline Luc", "authorId": "152831141"}, {"name": "Antoine Miech", "authorId": "19200186"}, {"name": "Iain Barr", "authorId": "2159207795"}, {"name": "Yana Hasson", "authorId": "66535271"}, {"name": "Karel Lenc", "authorId": "3257286"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "Malcolm Reynolds", "authorId": "47447264"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Serkan Cabi", "authorId": "12159303"}, {"name": "Tengda Han", "authorId": "22237490"}, {"name": "Zhitao Gong", "authorId": "48398849"}, {"name": "Sina Samangooei", "authorId": "2412073"}, {"name": "Marianne Monteiro", "authorId": "49601928"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Aida Nematzadeh", "authorId": "3208081"}, {"name": "Sahand Sharifzadeh", "authorId": "7782886"}, {"name": "Mikolaj Binkowski", "authorId": "9961753"}, {"name": "Ricardo Barreira", "authorId": "2026369796"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Andrew Zisserman", "authorId": "1688869"}, {"name": "K. Simonyan", "authorId": "34838386"}], "n_citations": 3607}, "snippets": ["Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."], "score": 0.0}, {"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."], "score": 0.0}, {"id": "(Younesi et al., 2024)", "paper": {"corpus_id": 267897956, "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Abolfazl Younesi", "authorId": "2260382962"}, {"name": "Mohsen Ansari", "authorId": "153631156"}, {"name": "Mohammadamin Fazli", "authorId": "2260384993"}, {"name": "A. Ejlali", "authorId": "2490667"}, {"name": "Muhammad Shafique", "authorId": "2238832044"}, {"name": "Joerg Henkel", "authorId": "2247781719"}], "n_citations": 47}, "snippets": ["One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks."], "score": 0.96630859375}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 273969525, "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yiyang Ma", "authorId": "2326805499"}, {"name": "Xingchao Liu", "authorId": "2326998908"}, {"name": "Xi-aokang Chen", "authorId": "2326444644"}, {"name": "Wen Liu", "authorId": "2326440587"}, {"name": "Chengyue Wu", "authorId": "2330475459"}, {"name": "Zhiyu Wu", "authorId": "2326514370"}, {"name": "Zizheng Pan", "authorId": "2327000291"}, {"name": "Zhenda Xie", "authorId": "2279107352"}, {"name": "Haowei Zhang", "authorId": "2315951020"}, {"name": "Xingkai Yu", "authorId": "2279159169"}, {"name": "Liang Zhao", "authorId": "2330538675"}, {"name": "Yisong Wang", "authorId": "2330376936"}, {"name": "Jiaying Liu", "authorId": "2330291409"}, {"name": "C. Ruan", "authorId": "2278217940"}], "n_citations": 39}, "snippets": ["One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]", "Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97]."], "score": 0.9814453125}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Zou et al., 2025)", "paper": {"corpus_id": 276928288, "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jialv Zou", "authorId": "2261949071"}, {"name": "Bencheng Liao", "authorId": "2060439659"}, {"name": "Qian Zhang", "authorId": "2261816376"}, {"name": "Wenyu Liu", "authorId": "2257432695"}, {"name": "Xinggang Wang", "authorId": "2266175736"}], "n_citations": 1}, "snippets": ["Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56]."], "score": 0.96435546875}, {"id": "(Esser et al., 2024)", "paper": {"corpus_id": 268247980, "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Patrick Esser", "authorId": "35175531"}, {"name": "Sumith Kulal", "authorId": "3411322"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Rahim Entezari", "authorId": "2316859494"}, {"name": "Jonas Muller", "authorId": "2188737195"}, {"name": "Harry Saini", "authorId": "2289994508"}, {"name": "Yam Levi", "authorId": "2290013499"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Axel Sauer", "authorId": "40562186"}, {"name": "Frederic Boesel", "authorId": "2290014125"}, {"name": "Dustin Podell", "authorId": "2221125727"}, {"name": "Tim Dockhorn", "authorId": "102541178"}, {"name": "Zion English", "authorId": "2221127565"}, {"name": "Kyle Lacey", "authorId": "2221126982"}, {"name": "Alex Goodwin", "authorId": "2290014122"}, {"name": "Yannik Marek", "authorId": "2290014387"}, {"name": "Robin Rombach", "authorId": "1660819540"}], "n_citations": 1401}, "snippets": ["Diffusion models create data from noise by inverting the forward paths of data towards noise and have emerged as a powerful generative modeling technique for high-dimensional, perceptual data such as images and videos. Rectified flow is a recent generative model formulation that connects data and noise in a straight line. Despite its better theoretical properties and conceptual simplicity, it is not yet decisively established as standard practice. In this work, we improve existing noise sampling techniques for training rectified flow models by biasing them towards perceptually relevant scales. Through a large-scale study, we demonstrate the superior performance of this approach compared to established diffusion formulations for high-resolution text-to-image synthesis. Additionally, we present a novel transformer-based architecture for text-to-image generation that uses separate weights for the two modalities and enables a bidirectional flow of information between image and text tokens, improving text comprehension, typography, and human preference ratings. We demonstrate that this architecture follows predictable scaling trends and correlates lower validation loss to improved text-to-image synthesis as measured by various metrics and human evaluations. Our largest models outperform state-of-the-art models, and we will make our experimental data, code, and model weights publicly available."], "score": 0.0}, {"id": "(Zhu et al._1, 2023)", "paper": {"corpus_id": 266210376, "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jinguo Zhu", "authorId": "2274151094"}, {"name": "Xiaohan Ding", "authorId": "2258784456"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Sijie Zhao", "authorId": "2254048096"}, {"name": "Hengshuang Zhao", "authorId": "2310758544"}, {"name": "Xiaohua Wang", "authorId": "2274490604"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 37}, "snippets": ["To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text."], "score": 0.97607421875}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258822817, "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zineng Tang", "authorId": "151270642"}, {"name": "Ziyi Yang", "authorId": "2155459391"}, {"name": "Chenguang Zhu", "authorId": "8652308"}, {"name": "Michael Zeng", "authorId": "48262024"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 188}, "snippets": ["We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 271693411, "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiaqi Wang", "authorId": "2136025369"}, {"name": "Hanqi Jiang", "authorId": "2273631049"}, {"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Chong-Yi Ma", "authorId": "120688117"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Peiran Gu", "authorId": "2314692435"}, {"name": "Sichen Xia", "authorId": "2314692233"}, {"name": "Wenjun Li", "authorId": "2284031962"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Zihao Wu", "authorId": "2238905102"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Bao Ge", "authorId": "2257302793"}, {"name": "Tuo Zhang", "authorId": "2269508672"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Xi Jiang", "authorId": "2300922460"}, {"name": "Xin Zhang", "authorId": "2290203999"}, {"name": "Wei Zhang", "authorId": "2157449203"}, {"name": "Dinggang Shen", "authorId": "2272672300"}, {"name": "Tianming Liu", "authorId": "2301126629"}, {"name": "Shu Zhang", "authorId": "2277750447"}], "n_citations": 36}, "snippets": ["Multimodal Output Decoder: Lastly, the multimodal output decoder reconverts the fused, integrated multimodal information back into a usable form tailored to specific tasks, such as Image captioning, the decoder might generate descriptive text based on visual inputs. Video understanding tasks, it could produce annotations or summaries combining both visual and auditory data. Each decoder is meticulously designed to optimize accuracy and quality, ensuring that the output precisely reflects the combined insights gained from the integrated modalities."], "score": 0.9619140625}, {"id": "(Liu et al._2, 2024)", "paper": {"corpus_id": 273233771, "title": "PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models", "year": 2024, "venue": "", "authors": [{"name": "Yingen Liu", "authorId": "2325197953"}, {"name": "Fan Wu", "authorId": "2257429787"}, {"name": "Ruihui Li", "authorId": "2262008831"}, {"name": "Zhuo Tang", "authorId": "2242362590"}, {"name": "Kenli Li", "authorId": "2278395518"}], "n_citations": 0}, "snippets": ["Multimodal large language models (MLLMs) demonstrate strong performance across visual tasks, but their efficiency is hindered by significant computational and memory demands from processing long contexts in multimodal inputs."], "score": 0.97607421875}, {"id": "(Franklin et al., 2025)", "paper": {"corpus_id": 275337159, "title": "Vision-Driven Prompt Optimization for Large Language Models in Multimodal Generative Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leo Franklin", "authorId": "2338834916"}, {"name": "Apiradee Boonmee", "authorId": "2324583118"}, {"name": "Kritsada Wongsuwan", "authorId": "2324586634"}], "n_citations": 0}, "snippets": ["Recent works have expanded their applications and enhanced their architectures to achieve better performance, efficiency, and scalability [21]- (Zhou et al., 2022). \n\nA primary focus in the development of LVLMs has been the design of architectures that effectively unify language and vision modalities. Recent models have proposed end-to-end frameworks that leverage shared embeddings for both text and images, enabling them to excel at tasks requiring fine-grained multimodal reasoning (Zhang et al., 2024). Additionally, techniques such as mixture of experts and relational reasoning mechanisms have been introduced to improve scalability and enhance the relational reasoning capabilities of LVLMs (Lin et al., 2024), [26]."], "score": 0.97509765625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 270923961, "title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model Supporting Long-Contextual Input and Output", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pan Zhang", "authorId": "2213750400"}, {"name": "Xiao-wen Dong", "authorId": "2118187561"}, {"name": "Yuhang Zang", "authorId": "12862495"}, {"name": "Yuhang Cao", "authorId": "50206929"}, {"name": "Rui Qian", "authorId": "2274346959"}, {"name": "Lin Chen", "authorId": "2307935043"}, {"name": "Qipeng Guo", "authorId": "2303800191"}, {"name": "Haodong Duan", "authorId": "31463937"}, {"name": "Bin Wang", "authorId": "2256857728"}, {"name": "Linke Ouyang", "authorId": "2161162356"}, {"name": "Songyang Zhang", "authorId": "2266356137"}, {"name": "Wenwei Zhang", "authorId": "2266359401"}, {"name": "Yining Li", "authorId": "47002704"}, {"name": "Yang Gao", "authorId": "2287681221"}, {"name": "Peng Sun", "authorId": "2075416290"}, {"name": "Xinyue Zhang", "authorId": "2268020798"}, {"name": "Wei Li", "authorId": "2268754066"}, {"name": "Jingwen Li", "authorId": "2281785148"}, {"name": "Wenhai Wang", "authorId": "2257133501"}, {"name": "Hang Yan", "authorId": "2282113542"}, {"name": "Conghui He", "authorId": "2267889334"}, {"name": "Xingcheng Zhang", "authorId": "2298585927"}, {"name": "Kai Chen", "authorId": "2250666679"}, {"name": "Jifeng Dai", "authorId": "2292283383"}, {"name": "Yu Qiao", "authorId": "2284394767"}, {"name": "Dahua Lin", "authorId": "2269713956"}, {"name": "Jiaqi Wang", "authorId": "2267494294"}], "n_citations": 117}, "snippets": ["We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision language model that supports long-contextual input and output. IXC-2.5 excels in various text-image comprehension and composition applications, achieving GPT-4V level capabilities with merely 7B LLM backend. Trained with 24K interleaved image-text contexts, it can seamlessly extend to 96K long contexts via RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in tasks requiring extensive input and output contexts. Compared to its previous 2.0 version, InternLM-XComposer-2.5 features three major upgrades in vision-language comprehension: (1) Ultra-High Resolution Understanding, (2) Fine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In addition to comprehension, IXC-2.5 extends to two compelling applications using extra LoRA parameters for text-image composition: (1) Crafting Webpages and (2) Composing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28 benchmarks, outperforming existing open-source state-of-the-art models on 16 benchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on 16 key tasks. The InternLM-XComposer-2.5 is publicly available at https://github.com/InternLM/InternLM-XComposer."], "score": 0.0}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 267311517, "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bin Lin", "authorId": "2254329478"}, {"name": "Zhenyu Tang", "authorId": "2275126715"}, {"name": "Yang Ye", "authorId": "2267264117"}, {"name": "Jiaxi Cui", "authorId": "2220852923"}, {"name": "Bin Zhu", "authorId": "2256419313"}, {"name": "Peng Jin", "authorId": "2185571736"}, {"name": "Jinfa Huang", "authorId": "2000011573"}, {"name": "Junwu Zhang", "authorId": "2253845661"}, {"name": "Munan Ning", "authorId": "2253434320"}, {"name": "Li Yuan", "authorId": "2266507381"}], "n_citations": 178}, "snippets": ["Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs) effectively improves downstream task performances. However, existing scaling methods enable all model parameters to be active for each token in the calculation, which brings massive training and inferring costs. In this work, we propose a simple yet effective training strategy MoE-Tuning for LVLMs. This strategy innovatively addresses the common issue of performance degradation in multi-modal sparsity learning, consequently constructing a sparse model with an outrageous number of parameters but a constant computational cost. Furthermore, we present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely activates only the top-k experts through routers during deployment, keeping the remaining experts inactive. Extensive experiments show the significant performance of MoE-LLaVA in a variety of visual understanding and object hallucination benchmarks. Remarkably, with only approximately 3B sparsely activated parameters, MoE-LLaVA demonstrates performance comparable to the LLaVA-1.5-7B on various visual understanding datasets and even surpasses the LLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to establish a baseline for sparse LVLMs and provide valuable insights for future research in developing more efficient and effective multi-modal learning systems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA."], "score": 0.0}, {"id": "(Jiang et al., 2025)", "paper": {"corpus_id": 278339632, "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Feibo Jiang", "authorId": "40540111"}, {"name": "Cunhua Pan", "authorId": "2290889674"}, {"name": "Li Dong", "authorId": "2152288497"}, {"name": "Kezhi Wang", "authorId": "2244014700"}, {"name": "M. Debbah", "authorId": "145118318"}, {"name": "Dusit Niyato", "authorId": "2340230621"}, {"name": "Zhu Han", "authorId": "2267902891"}], "n_citations": 1}, "snippets": ["LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning", "these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text."], "score": 0.9619140625}], "table": null}, {"title": "Integration Mechanisms Between Language and Vision Modalities", "tldr": "Multimodal language models employ various integration mechanisms to align language and vision modalities, including projection layers, cross-attention mechanisms, and unified token spaces. These approaches range from adapter-based methods that connect pre-trained components to early fusion techniques that process all modalities uniformly within a shared architecture. (16 sources)", "text": "\n## Cross-Modal Alignment Approaches\n\nA fundamental challenge in multimodal systems is effectively aligning information between different modality spaces. Several approaches have emerged to address this challenge. One common method employs projection layers that map between visual and language embedding spaces. For example, Emu uses an efficient mapping network that translates text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>. Similarly, LLaVA adopts a simple MLP projector to connect a pre-trained CLIP visual encoder with a language model <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nAnother prominent approach involves cross-attention mechanisms that enable different modalities to attend to each other. The Flamingo model pioneered the use of gated cross-attention to integrate images and text <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248476411\" paperTitle=\"(Alayrac et al., 2022)\" isShortName></Paper>. This allows the model to selectively focus on relevant aspects of each modality during processing, creating dynamic relationships between visual and textual elements.\n\n## Specialized Connector Modules\n\nBeyond simple projection layers, more sophisticated connector modules have been developed to bridge modalities. The BLIP series introduced the Q-Former module, which employs learnable input queries that interact with image features and the language model <Paper corpusId=\"273350685\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256390509\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>. This approach allows for effective cross-modal learning while maintaining the versatility of the underlying language model.\n\nImageBind represents another important advancement in cross-modal integration, learning a joint embedding space across six different modalities (images, text, audio, depth, thermal, and IMU data) <Paper corpusId=\"270391394\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258564264\" paperTitle=\"(Girdhar et al., 2023)\" isShortName></Paper>. What makes ImageBind particularly notable is that it demonstrates that all combinations of paired data are not necessary to train such a joint embedding, and that image-paired data alone is sufficient to bind multiple modalities together.\n\n## Dual-Stream vs. Early Fusion Architectures\n\nTwo major architectural paradigms have emerged for integrating language and vision: dual-stream and early fusion approaches. Dual-stream architectures employ separate encoders for different modalities with interaction mechanisms between them. One implementation involves a distinct CNN encoder for visual features and a transformer encoder for text, interacting via co-attentional transformer layers <Paper corpusId=\"267897956\" paperTitle=\"(Younesi et al., 2024)\" isShortName></Paper>. The CNN extracts visual features providing contextual information to guide language generation, while the transformer handles text semantics.\n\nIn contrast, early fusion approaches represent all modalities within a unified token space. Chameleon exemplifies this approach as a family of token-based mixed-modal models capable of processing images and text in any arbitrary sequence by representing all modalities as discrete tokens <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. AnyGPT similarly utilizes discrete representations for unified processing of various modalities, including speech, text, images, and music <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nMore recent advancements like UniFluid demonstrate that continuous visual tokens can be effectively used within an autoregressive architecture to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>. This approach embeds both text and continuous visual inputs as tokens in the same space, enabling seamless joint training.\n\n## Low-Rank Adaptation for Efficient Integration\n\nLow-Rank Adaptation (LoRA) has emerged as an efficient technique for enhancing multimodal capabilities. The GPT4Tools framework utilizes LoRA optimization to enhance language models' ability to use multimodal tools <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This approach enables fine-tuning with minimal parameter updates, making it more accessible and computationally efficient to integrate visual capabilities into existing language models.\n\n## Integration for Bidirectional Generation\n\nFor models capable of both text-to-image and image-to-text generation, specialized integration mechanisms have been developed. One approach involves integrating diffusion-based generation with language models by training LLMs to generate conditions for pre-trained text-to-image diffusion models <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. DreamLLM exemplifies this approach through generative modeling of both language and image posteriors via direct sampling in the multimodal space <Paper corpusId=\"273969525\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\nAlternative integration methods for bidirectional generation include combining bidirectional image diffusion with autoregressive text prediction within a unified framework, as demonstrated by Transfusion and Show-O <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. Similarly, the SEED-X model combines continuous image representations from a CLIP ViT encoder with text tokens in a shared architecture <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent work has also explored integrating various visual generation paradigms, such as diffusion models, flow-based generative models, and vector-quantized autoregressive models with LLMs <Paper corpusId=\"276928288\" paperTitle=\"(Zou et al., 2025)\" isShortName></Paper>. This represents a convergence toward more flexible integration mechanisms that can accommodate multiple generative approaches within a unified framework.\n\nThe diversity of integration mechanisms continues to evolve, with research increasingly focused on approaches that allow for more seamless interaction between modalities while enabling both understanding and generation tasks within a unified model architecture <Paper corpusId=\"278327487\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Koh et al., 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."], "score": 0.98779296875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 270391394, "title": "Large Language Models Meet Text-Centric Multimodal Sentiment Analysis: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hao Yang", "authorId": "2261895888"}, {"name": "Yanyan Zhao", "authorId": "49339265"}, {"name": "Yang Wu", "authorId": "143792101"}, {"name": "Shilong Wang", "authorId": "2214828656"}, {"name": "Tian Zheng", "authorId": "2261492452"}, {"name": "Hongbo Zhang", "authorId": "2261686105"}, {"name": "Wanxiang Che", "authorId": "2279751409"}, {"name": "Bing Qin", "authorId": "2203961541"}], "n_citations": 13}, "snippets": ["Notable examples include Gemini [111], GPT-4V, and ImageBind (Girdhar et al., 2023).These models showcase the ability to process text, images, audio, and video, enhancing functionalities such as translation, image recognition, and more", "BLIP-2 [112] introduces a novel approach to integrate a frozen pre-trained visual encoder with a frozen large language model using a Q-former module.This module employs learnable input queries that interact with image features and the LLM, allowing for effective cross-modal learning.This setup helps maintain the versatility of the LLM while incorporating visual information effectively.LLava [113] is a represent large multimodal model integrating a pre-trained CLIP [116] visual encoder (ViT-L/14), the Vicuna [115] language model, and a simple linear projection layer."], "score": 0.9580078125}, {"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."], "score": 0.0}, {"id": "(Cao et al., 2024)", "paper": {"corpus_id": 273350685, "title": "MMFuser: Multimodal Multi-Layer Feature Fuser for Fine-Grained Vision-Language Understanding", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yue Cao", "authorId": "2312369775"}, {"name": "Yangzhou Liu", "authorId": "2312345209"}, {"name": "Zhe Chen", "authorId": "2305731793"}, {"name": "Guangchen Shi", "authorId": "2146870036"}, {"name": "Wenhai Wang", "authorId": "2257133501"}, {"name": "Danhuai Zhao", "authorId": "2326454048"}, {"name": "Tong Lu", "authorId": "2276323159"}], "n_citations": 8}, "snippets": ["Multimodal Large Language Models (MLLMs) integrate visual representations from images with linguistic embeddings from text, thereby enhancing the models' capabilities in comprehending and generating language descriptions of visual content. Most open-source MLLMs employ architectures that include a pre-trained vision encoder, an LLM, and a crossmodal connector. Early models, such as the BLIP series (Dai et al., 2023), (Li et al., 2023), utilized the Q-Former module to align text and images, thus improving multimodal capabilities. Flamingo (Alayrac et al., 2022) employed a gated cross-attention mechanism to integrate images and text. LLaVA-1.5 (Liu et al., 2023) adopted an MLP projector to connect the pre-trained vision encoder with the LLM. InternVL (Chen et al., 2023), [6] employed a dynamic resolution strategy, segmenting images into tiles and encoding both the tiles and the thumbnail view together. It then uses a pixel shuffle operation to reduce the number of visual tokens before integrating these features with the LLM through an MLP projector."], "score": 0.9599609375}, {"id": "(Alayrac et al., 2022)", "paper": {"corpus_id": 248476411, "title": "Flamingo: a Visual Language Model for Few-Shot Learning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jean-Baptiste Alayrac", "authorId": "2285263"}, {"name": "Jeff Donahue", "authorId": "7408951"}, {"name": "Pauline Luc", "authorId": "152831141"}, {"name": "Antoine Miech", "authorId": "19200186"}, {"name": "Iain Barr", "authorId": "2159207795"}, {"name": "Yana Hasson", "authorId": "66535271"}, {"name": "Karel Lenc", "authorId": "3257286"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "Malcolm Reynolds", "authorId": "47447264"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Serkan Cabi", "authorId": "12159303"}, {"name": "Tengda Han", "authorId": "22237490"}, {"name": "Zhitao Gong", "authorId": "48398849"}, {"name": "Sina Samangooei", "authorId": "2412073"}, {"name": "Marianne Monteiro", "authorId": "49601928"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Aida Nematzadeh", "authorId": "3208081"}, {"name": "Sahand Sharifzadeh", "authorId": "7782886"}, {"name": "Mikolaj Binkowski", "authorId": "9961753"}, {"name": "Ricardo Barreira", "authorId": "2026369796"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Andrew Zisserman", "authorId": "1688869"}, {"name": "K. Simonyan", "authorId": "34838386"}], "n_citations": 3607}, "snippets": ["Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data."], "score": 0.0}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 256390509, "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junnan Li", "authorId": "49299019"}, {"name": "Dongxu Li", "authorId": "2981509"}, {"name": "S. Savarese", "authorId": "1702137"}, {"name": "Steven C. H. Hoi", "authorId": "2184854289"}], "n_citations": 4653}, "snippets": ["The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."], "score": 0.0}, {"id": "(Girdhar et al., 2023)", "paper": {"corpus_id": 258564264, "title": "ImageBind One Embedding Space to Bind Them All", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Rohit Girdhar", "authorId": "3102850"}, {"name": "Alaaeldin El-Nouby", "authorId": "1388811741"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Mannat Singh", "authorId": "152964870"}, {"name": "Kalyan Vasudev Alwala", "authorId": "3085301"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Ishan Misra", "authorId": "1806773"}], "n_citations": 941}, "snippets": ["We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications \u2018out-of-the-box\u2019 including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate vision models for visual and non-visual tasks."], "score": 0.0}, {"id": "(Younesi et al., 2024)", "paper": {"corpus_id": 267897956, "title": "A Comprehensive Survey of Convolutions in Deep Learning: Applications, Challenges, and Future Trends", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Abolfazl Younesi", "authorId": "2260382962"}, {"name": "Mohsen Ansari", "authorId": "153631156"}, {"name": "Mohammadamin Fazli", "authorId": "2260384993"}, {"name": "A. Ejlali", "authorId": "2490667"}, {"name": "Muhammad Shafique", "authorId": "2238832044"}, {"name": "Joerg Henkel", "authorId": "2247781719"}], "n_citations": 47}, "snippets": ["One strategy involves employing a dual-stream architecture with distinct CNN and transformer encoders interacting via coattentional transformer layers [97]. The CNN extracts visual features from images, providing contextual information that can guide language generation and comprehension. The transformer architecture models the semantics and syntax of text. Their interaction enables the generation of captions based on image content or the retrieval of pertinent images for textual queries. Alternative methods directly incorporate CNNs within the transformer architecture as visual token encoders that operate with text token encoders [98]. The CNN projections of image patches are appended to text token embeddings as inputs to the transformer layers. This unified architecture allows for end-to-end optimization of parameters for both vision and language tasks."], "score": 0.96630859375}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 271924334, "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jinheng Xie", "authorId": "2306603347"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "David Junhao Zhang", "authorId": "2248388634"}, {"name": "Weihao Wang", "authorId": "2316668014"}, {"name": "Kevin Qinghong Lin", "authorId": "2298569956"}, {"name": "Yuchao Gu", "authorId": "2248678348"}, {"name": "Zhijie Chen", "authorId": "2316662510"}, {"name": "Zhenheng Yang", "authorId": "2309246065"}, {"name": "Mike Zheng Shou", "authorId": "2244623976"}], "n_citations": 229}, "snippets": ["In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."], "score": 0.9619140625}, {"id": "(Fan et al., 2025)", "paper": {"corpus_id": 277103675, "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lijie Fan", "authorId": "2347484699"}, {"name": "Luming Tang", "authorId": "34689393"}, {"name": "Siyang Qin", "authorId": "2333872078"}, {"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Xuan Yang", "authorId": "2350843695"}, {"name": "Siyuan Qiao", "authorId": "2266238535"}, {"name": "Andreas Steiner", "authorId": "2350755056"}, {"name": "Chen Sun", "authorId": "2333512682"}, {"name": "Yuanzhen Li", "authorId": "2281034398"}, {"name": "Tao Zhu", "authorId": "2351408307"}, {"name": "Michael Rubinstein", "authorId": "2325950285"}, {"name": "Michalis Raptis", "authorId": "3063676"}, {"name": "Deqing Sun", "authorId": "2262516617"}, {"name": "Radu Soricut", "authorId": "1737285"}], "n_citations": 8}, "snippets": ["We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image", "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."], "score": 0.9794921875}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 266573642, "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiaxing Huang", "authorId": "2115941903"}, {"name": "Jingyi Zhang", "authorId": "2276743977"}, {"name": "Kai Jiang", "authorId": "2276609830"}, {"name": "Han Qiu", "authorId": "49660254"}, {"name": "Shijian Lu", "authorId": "2237947102"}], "n_citations": 23}, "snippets": ["GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."], "score": 0.9609375}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 273969525, "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yiyang Ma", "authorId": "2326805499"}, {"name": "Xingchao Liu", "authorId": "2326998908"}, {"name": "Xi-aokang Chen", "authorId": "2326444644"}, {"name": "Wen Liu", "authorId": "2326440587"}, {"name": "Chengyue Wu", "authorId": "2330475459"}, {"name": "Zhiyu Wu", "authorId": "2326514370"}, {"name": "Zizheng Pan", "authorId": "2327000291"}, {"name": "Zhenda Xie", "authorId": "2279107352"}, {"name": "Haowei Zhang", "authorId": "2315951020"}, {"name": "Xingkai Yu", "authorId": "2279159169"}, {"name": "Liang Zhao", "authorId": "2330538675"}, {"name": "Yisong Wang", "authorId": "2330376936"}, {"name": "Jiaying Liu", "authorId": "2330291409"}, {"name": "C. Ruan", "authorId": "2278217940"}], "n_citations": 39}, "snippets": ["One prominent direction involves utilizing pre-trained text-to-image models for high-quality generation while training LLMs to generate conditions for these models (Dong et al., 2023)[25][26][27](Sun et al., 2023). However, this approach introduces architectural complexity and potentially constrains the model's capabilities through maintaining separate LLM and generative components. Alternative approaches [88,97,99,100,108] propose training a single LLM for both tasks, typically incorporating either diffusion models (Ho et al., 2020)[83] or vector-quantized autoregressive models (Esser et al., 2020)[86]", "Another approach extends MLLMs with pre-trained diffusion models [19, 25-27, 87, 101]. However, these systems essentially utilize diffusion models as external tools, where the MLLM generates conditions for image generation without possessing direct generative capabilities. This separation often results in suboptimal performance compared to standalone diffusion models [25,87]. Another line of work [88,97,99,100,108] aim to train a single LLM for both tasks. Many of these methods employ vector-quantization [22,86] to convert images into discrete tokens, enabling unified autoregressive processing [88]97]."], "score": 0.9814453125}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276782523, "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rui Zhao", "authorId": "2248946985"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Mike Zheng Shou", "authorId": "2344762475"}], "n_citations": 1}, "snippets": ["Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."], "score": 0.9658203125}, {"id": "(Zou et al., 2025)", "paper": {"corpus_id": 276928288, "title": "OmniMamba: Efficient and Unified Multimodal Understanding and Generation via State Space Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jialv Zou", "authorId": "2261949071"}, {"name": "Bencheng Liao", "authorId": "2060439659"}, {"name": "Qian Zhang", "authorId": "2261816376"}, {"name": "Wenyu Liu", "authorId": "2257432695"}, {"name": "Xinggang Wang", "authorId": "2266175736"}], "n_citations": 1}, "snippets": ["Recent studies have emerged that seek to integrate multimodal understanding with visual generation, aiming to develop unified systems capable of handling both tasks simultaneously. Such designs hold the potential to foster mutual enhancement between generation and understanding, offering a promising pathway toward truly unifying all modalities. Numerous studies have sought to preserve the text generation paradigm of LLMs while exploring the impact [46,64,66,67] of integrating diverse visual generation paradigms, such as diffusion models [24], flow-based generative models (Esser et al., 2024)[40], and vector-quantized autoregressive models [56]."], "score": 0.96435546875}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 278327487, "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xinjie Zhang", "authorId": "2359211182"}, {"name": "Jintao Guo", "authorId": "2359201363"}, {"name": "Shanshan Zhao", "authorId": "2337775059"}, {"name": "Minghao Fu", "authorId": "2345947101"}, {"name": "Lunhao Duan", "authorId": "2275153684"}, {"name": "Guo-Hua Wang", "authorId": "2339237238"}, {"name": "Qing-Guo Chen", "authorId": "2304395071"}, {"name": "Zhao Xu", "authorId": "2304361003"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 1}, "snippets": ["In this section, we primarily focus on unified multimodal models that support vision-language understanding and generation, i.e., models that take both image and text as input and produce either text or image as output. As shown in Fig. 5, existing unified models can be broadly categorized into three main types: diffusion models, autoregressive models, and fused AR + diffusion models. For autoregressive models, we further classify them based on their modality encoding methods into four subcategories: pixel-based encoding, semantic-based encoding, learnable query-based encoding, and hybrid encoding. Each of these encoding strategies represents different ways of handling visual and textual data, leading to varying levels of integration and flexibility in the multimodal representations. Fused AR + diffusion models are divided into two subcategories based on modality encoding: pixel-based encoding and hybrid encoding. These models combine aspects of both autoregressive and diffusion techniques, offering a promising approach to more unified and efficient multimodal generation."], "score": 0.97998046875}], "table": null}, {"title": "Training and Optimization Methods", "tldr": "Training multimodal language models requires specialized approaches to handle diverse data types and architectures, including alignment strategies between modalities, innovative fine-tuning techniques, and efficient parameter optimization methods like Low-Rank Adaptation (LoRA). (12 sources)", "text": "\n## Unified Training Approaches\n\nTraining multimodal language models capable of both understanding and generating content across modalities presents unique challenges that have prompted various specialized approaches. The Chameleon family of models demonstrates an important advancement through its stable training approach \"from inception,\" meaning it trains a unified model from scratch rather than combining pre-trained components. This approach employs an architectural parameterization specifically designed for early-fusion, token-based, mixed-modal settings, allowing the model to learn joint representations of text and images from the beginning <Paper corpusId=\"269791516\" paperTitle=\"(Team et al., 2024)\" isShortName></Paper>.\n\nA growing trend in multimodal model training is the end-to-end approach where all modalities are processed within a unified framework. Rather than treating different modalities as separate components that need to be bridged, models like UniFluid embed both text and continuous visual inputs as tokens in the same representational space. This enables seamless joint training of image generation and understanding tasks within a single architecture. UniFluid specifically builds upon pre-trained Gemma on large-scale text corpora and unlocks visual capabilities through training with paired image-text data <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\n## Parameter-Efficient Fine-Tuning\n\nAs multimodal models grow in complexity, parameter-efficient fine-tuning methods have become increasingly important. Low-Rank Adaptation (LoRA) has emerged as a particularly effective technique for enhancing multimodal capabilities while minimizing computational costs. The GPT4Tools framework exemplifies this approach by utilizing LoRA optimization to enhance language models' ability to use multimodal tools. This method involves constructing a tool-related instructional dataset from advanced language models and then fine-tuning open-source language models using LoRA, enabling them to effectively use tools for visual tasks such as comprehension and image generation <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nLoRA's effectiveness stems from its ability to adapt models with minimal parameter updates, typically modifying only about 1% of parameters in certain projection layers. This not only benefits low-cost training but also facilitates convenient expansion to additional modalities, as demonstrated in frameworks like TextBind, which enhances large language models with multi-turn interleaved multimodal instruction-following capabilities <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Instruction-Based Training\n\nInstruction-based training has become a crucial approach for developing versatile multimodal models. This method involves training models to follow natural language instructions and prompts to perform various tasks, allowing them to adapt to new tasks without requiring gradient updates or large amounts of task-specific training data. Models like GPT-4V, LLaVA, and InstructBLIP exemplify this approach, demonstrating that the instruction format enables rapid adaptation to new tasks and domains <Paper corpusId=\"274305669\" paperTitle=\"(Kang et al., 2024)\" isShortName></Paper>.\n\nTextBind further enhances this approach by reducing the need for high-quality exemplar data, making instruction-based training more accessible and scalable for real-world tasks. The resulting MIM model outperforms recent baselines in open-world multimodal conversations, demonstrating strong performance in textual response generation, image generation, and overall multimodal instruction-following <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Cross-Modal Alignment Strategies\n\nA critical aspect of training multimodal models is developing effective strategies for aligning different modalities. One approach involves creating a shared multimodal space by bridging alignment in the diffusion process, as demonstrated by Composable Diffusion (CoDi). This enables synchronized generation of intertwined modalities, such as temporally aligned video and audio <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\nAnother alignment strategy involves the use of continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X exemplifies this approach by combining continuous image representations from a CLIP ViT encoder with text tokens and feeding them into a large language model to perform next-word prediction and image representation regression <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nDreamLLM adopts a distinctive approach by focusing on generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This method avoids the limitations and information loss inherent to external feature extractors like CLIP, potentially enabling more thorough multimodal understanding <Paper corpusId=\"271924334\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n## Modality-Specific Training Components\n\nTraining effective multimodal generative models often requires specialized components for different modalities. UniFluid, for instance, employs task-specific prediction heads\u2014a diffusion-based head for image generation and a cross-entropy head for text generation\u2014to ensure effective modality-specific training and inference. This approach enables the model to efficiently learn shared representations that mutually enhance its generation and understanding capabilities <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper>.\n\nSimilarly, approaches that augment multimodal language models with image generation capabilities typically introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion for text-to-image generation <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. These approaches, exemplified by models like Emu, use an efficient mapping network to translate text representations from an LLM into the embedding space of visual models, enabling leverage of strong text representations for visual outputs <Paper corpusId=\"271709823\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>.\n\n## Novel Loss Functions and Training Objectives\n\nThe development of specialized loss functions has been crucial for training multimodal models. Instead of relying solely on traditional categorical cross-entropy loss, some approaches like continuous tokenization for image generation define a Diffusion Loss function to model per-token probability distributions. This allows for autoregressive modeling in a continuous-valued space without requiring discrete-valued tokenizers <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270560593\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\nAdditionally, some models employ a pairwise sigmoid loss for image-text pre-training, which operates solely on image-text pairs without requiring a global view of pairwise similarities for normalization. This approach allows for scaling up batch sizes while also performing better at smaller batch sizes, offering flexibility in training configurations <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257767223\" paperTitle=\"(Zhai et al., 2023)\" isShortName></Paper>.\n\nThese diverse training and optimization methods collectively enable the development of increasingly sophisticated multimodal language models capable of both understanding and generating content across different modalities, pushing the boundaries of what's possible in AI-generated multimedia content.", "citations": [{"id": "(Team et al., 2024)", "paper": {"corpus_id": 269791516, "title": "Chameleon: Mixed-Modal Early-Fusion Foundation Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chameleon Team", "authorId": "2301457346"}, {"name": "Jacob Kahn", "authorId": "39960571"}], "n_citations": 339}, "snippets": ["We present Chameleon, a family of early-fusion token-based mixed-modal models capable of understanding and generating images and text in any arbitrary sequence. We outline a stable training approach from inception, an alignment recipe, and an architectural parameterization tailored for the early-fusion, token-based, mixed-modal setting. The models are evaluated on a comprehensive range of tasks, including visual question answering, image captioning, text generation, image generation, and long-form mixed modal generation. Chameleon demonstrates broad and general capabilities, including state-of-the-art performance in image captioning tasks, outperforms Llama-2 in text-only tasks while being competitive with models such as Mixtral 8x7B and Gemini-Pro, and performs non-trivial image generation, all in a single model."], "score": 0.98291015625}, {"id": "(Fan et al., 2025)", "paper": {"corpus_id": 277103675, "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lijie Fan", "authorId": "2347484699"}, {"name": "Luming Tang", "authorId": "34689393"}, {"name": "Siyang Qin", "authorId": "2333872078"}, {"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Xuan Yang", "authorId": "2350843695"}, {"name": "Siyuan Qiao", "authorId": "2266238535"}, {"name": "Andreas Steiner", "authorId": "2350755056"}, {"name": "Chen Sun", "authorId": "2333512682"}, {"name": "Yuanzhen Li", "authorId": "2281034398"}, {"name": "Tao Zhu", "authorId": "2351408307"}, {"name": "Michael Rubinstein", "authorId": "2325950285"}, {"name": "Michalis Raptis", "authorId": "3063676"}, {"name": "Deqing Sun", "authorId": "2262516617"}, {"name": "Radu Soricut", "authorId": "1737285"}], "n_citations": 8}, "snippets": ["We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image", "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."], "score": 0.9794921875}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 266573642, "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiaxing Huang", "authorId": "2115941903"}, {"name": "Jingyi Zhang", "authorId": "2276743977"}, {"name": "Kai Jiang", "authorId": "2276609830"}, {"name": "Han Qiu", "authorId": "49660254"}, {"name": "Shijian Lu", "authorId": "2237947102"}], "n_citations": 23}, "snippets": ["GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."], "score": 0.9609375}, {"id": "(Kang et al., 2024)", "paper": {"corpus_id": 274305669, "title": "VLM-HOI: Vision Language Models for Interpretable Human-Object Interaction Analysis", "year": 2024, "venue": "ECCV Workshops", "authors": [{"name": "Donggoo Kang", "authorId": "1490936425"}, {"name": "Dasol Jeong", "authorId": "1783599864"}, {"name": "Hyunmin Lee", "authorId": "2321673431"}, {"name": "Sangwoo Park", "authorId": "2205412863"}, {"name": "Hasil Park", "authorId": "31237187"}, {"name": "Sunkyu Kwon", "authorId": "2211171908"}, {"name": "Yeongjoon Kim", "authorId": "2268097132"}, {"name": "Joonki Paik", "authorId": "2261728538"}], "n_citations": 0}, "snippets": ["More advanced vision-language tasks require a tighter integration between the visual and linguistic modalities. This has led to a surge of interest in unified multimodal representation models that can process both images and text within a single framework (Baltru\u0161aitis et al., 2017)", "Recently, pretrained language models, such as BERT [11] and GPT (Brown et al., 2020), have significantly influenced the vision-language domain. Researchers have extended these models to handle multimodal inputs, leveraging their contextual understanding of text", "Instruction-based VLMs like GPT-4V [33], LLaVA [28], and InstructBLIP [10] are a new type of large vision-language model that are trained to follow natural language instructions and prompts to perform various tasks. A key advantage these models have over traditional fine-tuning is they can adapt to new tasks without needing gradient updates or lots of specific training data. The instruction format allows rapid adaptation."], "score": 0.96923828125}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 271924334, "title": "Show-o: One Single Transformer to Unify Multimodal Understanding and Generation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jinheng Xie", "authorId": "2306603347"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "David Junhao Zhang", "authorId": "2248388634"}, {"name": "Weihao Wang", "authorId": "2316668014"}, {"name": "Kevin Qinghong Lin", "authorId": "2298569956"}, {"name": "Yuchao Gu", "authorId": "2248678348"}, {"name": "Zhijie Chen", "authorId": "2316662510"}, {"name": "Zhenheng Yang", "authorId": "2309246065"}, {"name": "Mike Zheng Shou", "authorId": "2244623976"}], "n_citations": 229}, "snippets": ["In recent years, an increasing number of studies (Ge et al., 2024;Wu et al., 2023b;(Tang et al., 2023)Ye et al., 2024a;(Dong et al., 2023)Aiello et al., 2024) have focused on unified multimodal language models capable of both comprehension and generation. Some efforts (Zhu et al., 2023b;Sun et al., 2023c;b) use continuous representations interleaved with text tokens for autoregressive modeling to generate images. SEED-X (Ge et al., 2024) proposes a unified and versatile foundation system capable of handling both multimodal understanding and generation tasks. In this approach, continuous image representations from CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model (LLM) to perform next-word prediction and image representation regression. DreamLLM (Dong et al., 2023)) also explores the potential of enabling multimodal comprehension and creation based on LLMs. Chameleon (Team, 2024) introduces a family of token-based mixed-modal models capable of both comprehending and generating images. This approach represents all modalities as discrete tokens and utilizes a unified transformer-based architecture and trains the model from scratch in an end-to-end manner."], "score": 0.9619140625}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258822817, "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zineng Tang", "authorId": "151270642"}, {"name": "Ziyi Yang", "authorId": "2155459391"}, {"name": "Chenguang Zhu", "authorId": "8652308"}, {"name": "Michael Zeng", "authorId": "48262024"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 188}, "snippets": ["We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"], "score": 0.0}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 271709823, "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dongyang Liu", "authorId": "2308683426"}, {"name": "Shitian Zhao", "authorId": "2283419424"}, {"name": "Le Zhuo", "authorId": "2300371391"}, {"name": "Weifeng Lin", "authorId": "2284068796"}, {"name": "Yu Qiao", "authorId": "2312346585"}, {"name": "Hongsheng Li", "authorId": "2266421952"}, {"name": "Peng Gao", "authorId": "2310909023"}], "n_citations": 59}, "snippets": ["Another line of research (Koh et al., 2023)(Dong et al., 2023)Sun et al., 2023;(Wu et al., 2023) has proposed augmenting MLLMs with the ability to generate images, videos, and audio from text instructions. These approaches introduce additional visual tokens for generation and align these generative tokens as conditional information with a pretrained generator, such as Stable Diffusion (Esser et al., 2024;Podell et al., 2023) for text-to-image generation. Consequently, the generation capabilities heavily rely on the external expert generator rather than MLLMs themselves, resulting in inconsistent and inferior generation results. To combine the strength of both approaches, our model aims to learn both understanding and generation of images using an MLLM with native multimodal capabilities, drawing inspiration from Chameleon (Team, 2024a), a mixed-modal early-fusion foundation model."], "score": 0.96044921875}, {"id": "(Koh et al., 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."], "score": 0.98779296875}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 270560593, "title": "Autoregressive Image Generation without Vector Quantization", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Yonglong Tian", "authorId": "2307043887"}, {"name": "He Li", "authorId": "2307146098"}, {"name": "Mingyang Deng", "authorId": "2306970309"}, {"name": "Kaiming He", "authorId": "2270025109"}], "n_citations": 238}, "snippets": ["Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar."], "score": 0.0}, {"id": "(Zhai et al., 2023)", "paper": {"corpus_id": 257767223, "title": "Sigmoid Loss for Language Image Pre-Training", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Xiaohua Zhai", "authorId": "2743563"}, {"name": "Basil Mustafa", "authorId": "40608942"}, {"name": "Alexander Kolesnikov", "authorId": "144629422"}, {"name": "Lucas Beyer", "authorId": "39611591"}], "n_citations": 1204}, "snippets": ["We propose a simple pairwise sigmoid loss for imagetext pre-training. Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. With only four TPUv4 chips, we can train a Base CLIP model at 4k batch size and a Large LiT model at 20k batch size, the latter achieves 84.5% ImageNet zero-shot accuracy in two days. This disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training."], "score": 0.0}], "table": null}, {"title": "Capabilities and Applications of Multimodal Generative Models", "tldr": "Advanced multimodal generative models enable diverse applications ranging from text-image interleaved generation to visual editing via natural language instructions. These capabilities span creative content generation, visual reasoning, multimodal instruction following, and specialized applications like UML-to-code conversion. (17 sources)", "text": "\n## Text-Image Generation and Interleaving\nModern multimodal language models excel at generating coherent content that seamlessly interleaves text and images. Models like Emu, Chameleon, and Unified-IO can process and generate arbitrary sequences of text and images, enabling rich multimedia content creation <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274860024\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249848272\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"266573555\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper>. This capability represents a significant advancement over traditional models that could only generate single modalities or required separate models for different tasks.\n\n## Visual Editing and Manipulation\nSeveral multimodal models offer innovative capabilities for image editing through natural language instructions. Systems like InstructPix2Pix enable users to modify existing images by describing desired changes rather than requiring pixel-level manipulation <Paper corpusId=\"276929408\" paperTitle=\"(Fang et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>. This natural language interface for visual editing makes sophisticated image manipulation accessible to users without technical expertise in image editing software.\n\n## Multimodal Instruction Following\nAdvanced multimodal models demonstrate strong capabilities in following complex instructions that involve multiple modalities. TextBind enhances large language models with multi-turn interleaved multimodal instruction-following capabilities, significantly reducing the need for high-quality exemplar data <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. Similarly, MIM shows remarkable performance in textual response generation, image generation, and overall multimodal instruction-following within open-world multimodal conversations <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Visual Question Answering and Reasoning\nMultimodal models excel at visual question answering tasks, demonstrating sophisticated reasoning about visual content. Notable examples include BLIP, one of the earliest unified vision-language encoder-decoder architectures, and LLaVA, an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding <Paper corpusId=\"273022981\" paperTitle=\"(Khaldi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411402\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. These models can answer complex questions about images, describe scenes, identify objects, and reason about spatial relationships.\n\n## Multimodal In-Context Learning\nSome advanced models like Emu2 exhibit strong multimodal in-context learning abilities, solving tasks that require on-the-fly reasoning with minimal examples <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. This capability allows models to adapt to new tasks through demonstrations rather than requiring fine-tuning, enabling applications like visual prompting and object-grounded generation.\n\n## Cross-Modal Retrieval and Alignment\nMultimodal models can perform sophisticated cross-modal retrieval tasks, finding relevant images based on textual descriptions or generating appropriate textual content for given images. Models like CLIP align visual embeddings with textual embeddings, enabling zero-shot image classification and other cross-modal tasks <Paper corpusId=\"277065986\" paperTitle=\"(Bates et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>. These alignment capabilities form the foundation for many practical applications in content recommendation, search, and organization.\n\n## Specialized Technical Applications\nMultimodal models are increasingly applied to specialized technical domains. For example, some models can interpret UML diagrams and generate corresponding executable code, aligning diagram structures with text-based outputs by combining visual encoders with language models <Paper corpusId=\"277065986\" paperTitle=\"(Bates et al., 2025)\" isShortName></Paper>. This capability enhances productivity in software development workflows.\n\n## End-to-End Multimodal Pipelines\nAdvanced models are moving beyond cascaded pipelines toward end-to-end optimization approaches. While earlier systems like Visual ChatGPT connected LLMs with image generation tools in a cascaded pipeline that could introduce instability and noise, newer approaches optimize models in an end-to-end manner by aligning the output space with image diffusion models <Paper corpusId=\"266210376\" paperTitle=\"(Zhu et al._1, 2023)\" isShortName></Paper>. This end-to-end approach allows models to both perceive and generate images and text with greater coherence.\n\n## Multimodal Reasoning and Planning\nLatest multimodal models demonstrate advanced reasoning capabilities across different modalities. LMMs can perform cross-modal reasoning and prediction tasks, such as generating images or audio from text descriptions <Paper corpusId=\"278339632\" paperTitle=\"(Jiang et al., 2025)\" isShortName></Paper>. This ability enables applications in creative content generation, educational tools, and assistive technologies.\n\n## Unified Autoregressive Frameworks\nModels like UniFluid demonstrate the capabilities of unified autoregressive frameworks that leverage continuous visual tokens to jointly handle vision-language generation and understanding tasks <Paper corpusId=\"277103675\" paperTitle=\"(Fan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270560593\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>. These approaches enable efficient modeling of multimodal sequences through a single architectural framework, where both text and continuous visual inputs are embedded as tokens in the same representational space.\n\n## Tool-Enhanced Multimodal Processing\nSome models incorporate tool-usage capabilities to enhance their multimodal processing abilities. GPT4Tools enables open-source language models to effectively use multimodal tools for visual tasks such as comprehension and image generation through Low-Rank Adaptation optimization <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This approach expands the practical applications of multimodal models by allowing them to leverage specialized tools for different tasks.", "citations": [{"id": "(Koh et al., 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs."], "score": 0.98779296875}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 274860024, "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Xiaochuang Han", "authorId": "2290545053"}, {"name": "Chunting Zhou", "authorId": "2333891614"}, {"name": "Weixin Liang", "authorId": "151253861"}, {"name": "Xi Victoria Lin", "authorId": "2255374957"}, {"name": "Luke S. Zettlemoyer", "authorId": "2256736753"}, {"name": "Lili Yu", "authorId": "2296724476"}], "n_citations": 28}, "snippets": ["Over the past few years, we have seen significant progress in multimodal generative models capable of understanding and generating interleaved text and images in arbitrary sequences (Dong et al., 2023;(Koh et al., 2023)Lin et al., 2024b). Models like Transfusion (Zhou et al., 2024), Chameleon (Team, 2024b), and Unified-IO (Lu et al., 2022)(Lu et al., 2023) demonstrate the potential of unified architectures that seamlessly handle both image and text modalities."], "score": 0.9736328125}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 249848272, "title": "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jiasen Lu", "authorId": "2117718562"}, {"name": "Christopher Clark", "authorId": "143997772"}, {"name": "Rowan Zellers", "authorId": "2545335"}, {"name": "Roozbeh Mottaghi", "authorId": "3012475"}, {"name": "Aniruddha Kembhavi", "authorId": "2684226"}], "n_citations": 413}, "snippets": ["We propose Unified-IO, a model that performs a large variety of AI tasks spanning classical computer vision tasks, including pose estimation, object detection, depth estimation and image generation, vision-and-language tasks such as region captioning and referring expression, to natural language processing tasks such as question answering and paraphrasing. Developing a single unified model for such a large variety of tasks poses unique challenges due to the heterogeneous inputs and outputs pertaining to each task, including RGB images, per-pixel maps, binary masks, bounding boxes, and language. We achieve this unification by homogenizing every supported input and output into a sequence of discrete vocabulary tokens. This common representation across all tasks allows us to train a single transformer-based architecture, jointly on over 90 diverse datasets in the vision and language fields. Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning. Code and demos for Unified-IO are available at: https://unified-io.allenai.org."], "score": 0.0}, {"id": "(Lu et al., 2023)", "paper": {"corpus_id": 266573555, "title": "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Jiasen Lu", "authorId": "2117718562"}, {"name": "Christopher Clark", "authorId": "143997772"}, {"name": "Sangho Lee", "authorId": "2144567767"}, {"name": "Zichen Zhang", "authorId": "5630943"}, {"name": "Savya Khosla", "authorId": "2056070459"}, {"name": "Ryan Marten", "authorId": "2135169537"}, {"name": "Derek Hoiem", "authorId": "2261388484"}, {"name": "Aniruddha Kembhavi", "authorId": "2684226"}], "n_citations": 174}, "snippets": ["We present Unified-IO 2,the. first autoregressive multi-modal model that is capable of understanding and generating image, text, audio, and action. To unify different modalities, we tokenize inputs and outputs - images, text, audio, action, bounding boxes etc., into a shared semantic space and then process them with a single encoder-decoder transformer model. Since training with such diverse modalities is challenging, we propose various architectural improvements to stabilize model training. We train our model from scratch on a large multimodal pre-training corpus from diverse sources with a multimodal mixture of denoisers objective. To learn an expansive set of skills, such as following multimodal instructions, we construct and. finetune on an ensemble of 120 datasets with prompts and augmentations. With a single unified model, Unified-io 2 achieves state-of-the-art performance on the GRIT benchmark and strong results in more than 35 benchmarks, including image generation and understanding, natural language understanding, video and audio understanding, and robotic manipulation. We release all our models to the research community."], "score": 0.0}, {"id": "(Fang et al._1, 2025)", "paper": {"corpus_id": 276929408, "title": "Recognition-Synergistic Scene Text Editing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengyao Fang", "authorId": "2313679002"}, {"name": "Pengyuan Lyu", "authorId": "10344582"}, {"name": "Jingjing Wu", "authorId": "2313920076"}, {"name": "Chengquan Zhang", "authorId": "2248958848"}, {"name": "Jun Yu", "authorId": "2349736964"}, {"name": "Guan-Lun Lu", "authorId": "2261251059"}, {"name": "Wenjie Pei", "authorId": "1678473"}], "n_citations": 1}, "snippets": ["Recent works [1,11,26,(Sun et al., 2023)[40] have developed architectures capable of processing text and image modalities simultaneously, achieving a more nuanced integration of linguistic and visual information. These approaches demonstrate enhanced performance in image generation tasks, where MLLMs generate high-quality visuals that align closely with the semantic content of textual prompts. Furthermore, some MLLMs (Brooks et al., 2022)13,(Mokady et al., 2023) offer innovative capabilities for image editing by enabling users to adjust existing images through descriptive language, such as modifying attributes or inserting new elements, rather than relying on pixel-level manipulation."], "score": 0.9736328125}, {"id": "(Brooks et al., 2022)", "paper": {"corpus_id": 253581213, "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Tim Brooks", "authorId": "2679394"}, {"name": "Aleksander Holynski", "authorId": "2248172435"}, {"name": "Alexei A. Efros", "authorId": "1763086"}], "n_citations": 1833}, "snippets": ["We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 266573642, "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiaxing Huang", "authorId": "2115941903"}, {"name": "Jingyi Zhang", "authorId": "2276743977"}, {"name": "Kai Jiang", "authorId": "2276609830"}, {"name": "Han Qiu", "authorId": "49660254"}, {"name": "Shijian Lu", "authorId": "2237947102"}], "n_citations": 23}, "snippets": ["GPT4Tools [63] enables open-source language models to effectively use multimodal tools. It constructs a tool-related instructional dataset from advanced language models and utilizes Low-Rank Adaptation (LoRA) optimization to enhance the language models' tool-usage capabilities. Additionally, it proposes a benchmark to evaluate the accuracy of language models in using tools, demonstrating significant improvements in tool usage across various visual tasks. As shown in Figure 17, the GPT4Tools framework involves constructing a tool-related instruction dataset by prompting an advanced language model with various multimodal contexts. This dataset is then used to fine-tune open-source language models using Low-Rank Adaptation (LoRA) optimization, enabling them to effectively use tools for visual tasks such as comprehension and image generation.\n\nTextBind [72] enhances large language models with multi-turn interleaved multimodal instruction-following capabilities. It significantly reduces the need for high-quality exemplar data, making it more accessible and scalable for real-world tasks. The proposed model, MIM, trained on TextBind, outperforms recent baselines in open-world multimodal conversations, demonstrating remarkable performance in textual response generation, image generation, and overall multimodal instruction-following. As shown in Figure 18, MIM seamlessly integrates image encoder and decoder models to accommodate interleaved image-text inputs and outputs. It supplements large language models with visual input and output modules, enabling the model to process multi-turn interleaved multimodal instructions and generate coherent responses."], "score": 0.9609375}, {"id": "(Khaldi et al., 2024)", "paper": {"corpus_id": 273022981, "title": "Exploring Social Media Image Categorization Using Large Models with Different Adaptation Methods: A Case Study on Cultural Nature's Contributions to People", "year": 2024, "venue": "", "authors": [{"name": "Rohaifa Khaldi", "authorId": "35729419"}, {"name": "Domingo Alcaraz-Segura", "authorId": "2273276043"}, {"name": "Ignacio S'anchez-Herrera", "authorId": "2323737781"}, {"name": "Javier Mart\u00ednez-L\u00f3pez", "authorId": "2271941340"}, {"name": "Carlos Javier Navarro", "authorId": "2323747675"}, {"name": "S. Tabik", "authorId": "3030006"}], "n_citations": 1}, "snippets": ["LVLMs have emerged as a natural evolution of integrating LLMs with LVMs, enabling seamless multimodal interaction. By utilizing machine-generated instruction-following data during training, these models facilitate more dynamic and interactive communication between humans and machines. LVLMs are capable of processing both visual and textual inputs, generating coherent and contextually relevant responses in natural language", "Notable LVLMs include BLIP (Li et al., 2022), one of the earliest unified vision-language encoder-decoder architectures; LLaVA (Large Language and Vision Assistant) (Liu et al. 2023), an end-to-end multimodal model that connects a vision encoder to a language model for versatile visual-language understanding; and GPT-4 (Achiam et al. 2023), a proprietary foundation model capable of multimodal reasoning, accessible exclusively via APIs such as ChatGPT."], "score": 0.9697265625}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 246411402, "title": "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Junnan Li", "authorId": "49299019"}, {"name": "Dongxu Li", "authorId": "2981509"}, {"name": "Caiming Xiong", "authorId": "2054594326"}, {"name": "S. Hoi", "authorId": "1741126"}], "n_citations": 4413}, "snippets": ["Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code, models, and datasets are released at https://github.com/salesforce/BLIP."], "score": 0.0}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276782523, "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rui Zhao", "authorId": "2248946985"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Mike Zheng Shou", "authorId": "2344762475"}], "n_citations": 1}, "snippets": ["Unified multimodal generative models aim to bridge the gap between understanding and generation tasks, and integrate vision and language into a single framework, enabling the model to learn shared representations across modalities [1,(Aiello et al., 2023)(Dong et al., 2023)19,(Sun et al., 2023)62,71,72,74,76,77,79,85]. SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."], "score": 0.9658203125}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 266374640, "title": "Generative Multimodal Models are In-Context Learners", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Quan Sun", "authorId": "2264040642"}, {"name": "Yufeng Cui", "authorId": "2263702215"}, {"name": "Xiaosong Zhang", "authorId": "2264589602"}, {"name": "Fan Zhang", "authorId": "2264274388"}, {"name": "Qiying Yu", "authorId": "23716915"}, {"name": "Zhengxiong Luo", "authorId": "2275808712"}, {"name": "Yueze Wang", "authorId": "2217456303"}, {"name": "Yongming Rao", "authorId": "2275170369"}, {"name": "Jingjing Liu", "authorId": "2222717281"}, {"name": "Tiejun Huang", "authorId": "2257137554"}, {"name": "Xinlong Wang", "authorId": "2263959521"}], "n_citations": 290}, "snippets": ["The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."], "score": 0.0}, {"id": "(Bates et al., 2025)", "paper": {"corpus_id": 277065986, "title": "Unified Modeling Language Code Generation from Diagram Images Using Multimodal Large Language Models", "year": 2025, "venue": "Machine Learning with Applications", "authors": [{"name": "Averi Bates", "authorId": "2309151434"}, {"name": "Ryan Vavricka", "authorId": "2363651648"}, {"name": "Shane Carleton", "authorId": "2350512740"}, {"name": "Ruosi Shao", "authorId": "2350505892"}, {"name": "Chongle Pan", "authorId": "2084643963"}], "n_citations": 0}, "snippets": ["Researchers developed MM-LLMs that incorporate visual, auditory, and textual data into unified frameworks to address these limitations. Models like CLIP (Radford et al., 2021) align visual embeddings with textual embeddings, enabling cross-modal tasks such as visual question answering (VQA) and zero-shot image classification. For example, CLIP (Contrastive Language-Image Pretraining) learns shared visual and textual semantics from large-scale datasets of image-text pairs, effectively bridging linguistic context and visual perception. MM-LLMs leverage these multimodal capabilities to tackle tasks that traditional LLMs cannot address, such as interpreting UML diagrams and generating corresponding executable code. MM-LLMs align diagram structures with text-based outputs by combining visual encoders with language models, enhancing accuracy and flexibility for applications like UML-to-code automation."], "score": 0.97314453125}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Zhu et al._1, 2023)", "paper": {"corpus_id": 266210376, "title": "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jinguo Zhu", "authorId": "2274151094"}, {"name": "Xiaohan Ding", "authorId": "2258784456"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Sijie Zhao", "authorId": "2254048096"}, {"name": "Hengshuang Zhao", "authorId": "2310758544"}, {"name": "Xiaohua Wang", "authorId": "2274490604"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 37}, "snippets": ["To expand the capabilities of generating image in a multimodal context, certain efforts, e.g., Visual ChatGPT [47], attempt to connect LLMs with image generation tools in a cascaded pipeline by transferring text messages, which inevitably introduce instability and noise. Alternatively, another line of research achieves it by optimizing models in an end-to-end manner [9,18,23,30]48]. By aligning the output space with the image diffusion models, VL models can not only perceive but also generate images and text."], "score": 0.97607421875}, {"id": "(Jiang et al., 2025)", "paper": {"corpus_id": 278339632, "title": "A Comprehensive Survey of Large AI Models for Future Communications: Foundations, Applications and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Feibo Jiang", "authorId": "40540111"}, {"name": "Cunhua Pan", "authorId": "2290889674"}, {"name": "Li Dong", "authorId": "2152288497"}, {"name": "Kezhi Wang", "authorId": "2244014700"}, {"name": "M. Debbah", "authorId": "145118318"}, {"name": "Dusit Niyato", "authorId": "2340230621"}, {"name": "Zhu Han", "authorId": "2267902891"}], "n_citations": 1}, "snippets": ["LMMs are capable of simultaneously processing and understanding data from different modalities, such as vision, language, haptic and auditory. These models achieve comprehensive processing and reasoning of multimodal information by integrating the features of various modalities in a unified high-dimensional space. They utilize advanced neural network architectures, such as transformers and diffusion models, to extract features from each modality and optimize their representations through techniques such as contrastive learning and self-supervised learning", "these LMMs can also perform cross-modal reasoning and prediction, such as generating images or audio from text."], "score": 0.9619140625}, {"id": "(Fan et al., 2025)", "paper": {"corpus_id": 277103675, "title": "Unified Autoregressive Visual Generation and Understanding with Continuous Tokens", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lijie Fan", "authorId": "2347484699"}, {"name": "Luming Tang", "authorId": "34689393"}, {"name": "Siyang Qin", "authorId": "2333872078"}, {"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Xuan Yang", "authorId": "2350843695"}, {"name": "Siyuan Qiao", "authorId": "2266238535"}, {"name": "Andreas Steiner", "authorId": "2350755056"}, {"name": "Chen Sun", "authorId": "2333512682"}, {"name": "Yuanzhen Li", "authorId": "2281034398"}, {"name": "Tao Zhu", "authorId": "2351408307"}, {"name": "Michael Rubinstein", "authorId": "2325950285"}, {"name": "Michalis Raptis", "authorId": "3063676"}, {"name": "Deqing Sun", "authorId": "2262516617"}, {"name": "Radu Soricut", "authorId": "1737285"}], "n_citations": 8}, "snippets": ["We present UniFluid, a unified autoregressive framework for joint visual generation and understanding leveraging continuous visual tokens. Our unified autoregressive architecture processes multimodal image and text inputs, generating discrete tokens for text and continuous tokens for image", "UniFluid, a unified framework that leverages continuous visual tokens within an autoregressive architecture to jointly handle vision-language generation and understanding tasks. Building upon pre-trained Gemma [48] on large-scale text corpus, UniFluid unlocks powerful visual generation and understanding capabilities through training with paired image-text data, and further allows these two tasks to mutually benefit each other within a single architecture. Specifically, UniFluid adopts a unified autoregressive framework where both text and continuous visual inputs are embedded as tokens in the same space, enabling seamless joint training of image generation and understanding tasks. UniFluid integrates a continuous tokenizer [12,(Li et al., 2024) for image generation and a pre-trained SigLIP (Zhai et al., 2023) image encoder for understanding tasks, while textual inputs are processed using a standard SentencePiece tokenizer [20]. The resulting multimodal sequences are modeled autoregressively using Gemma [48] as the underlying transformer backbone. Task-specific prediction heads-a diffusion-based head for image generation and a cross-entropy head for text generation-ensure effective modality-specific training and inference, enabling UniFluid to efficiently learn shared representations that mutually enhance its generation and understanding capabilities."], "score": 0.9794921875}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 270560593, "title": "Autoregressive Image Generation without Vector Quantization", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tianhong Li", "authorId": "2307269819"}, {"name": "Yonglong Tian", "authorId": "2307043887"}, {"name": "He Li", "authorId": "2307146098"}, {"name": "Mingyang Deng", "authorId": "2306970309"}, {"name": "Kaiming He", "authorId": "2270025109"}], "n_citations": 238}, "snippets": ["Conventional wisdom holds that autoregressive models for image generation are typically accompanied by vector-quantized tokens. We observe that while a discrete-valued space can facilitate representing a categorical distribution, it is not a necessity for autoregressive modeling. In this work, we propose to model the per-token probability distribution using a diffusion procedure, which allows us to apply autoregressive models in a continuous-valued space. Rather than using categorical cross-entropy loss, we define a Diffusion Loss function to model the per-token probability. This approach eliminates the need for discrete-valued tokenizers. We evaluate its effectiveness across a wide range of cases, including standard autoregressive models and generalized masked autoregressive (MAR) variants. By removing vector quantization, our image generator achieves strong results while enjoying the speed advantage of sequence modeling. We hope this work will motivate the use of autoregressive generation in other continuous-valued domains and applications. Code is available at: https://github.com/LTH14/mar."], "score": 0.0}], "table": null}], "cost": 0.5981759999999999}}
