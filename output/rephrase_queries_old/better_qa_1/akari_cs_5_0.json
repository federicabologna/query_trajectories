{"better_query": "What recent research compares Retrieval-Augmented Generation (RAG) with fine-tuning for improving generative AI models?", "better_answer": {"sections": [{"title": "Introduction/Background on RAG and Fine-tuning", "tldr": "Retrieval-Augmented Generation (RAG) enhances language models by retrieving relevant external knowledge during generation, while fine-tuning adapts model parameters to specific domains or tasks. Both approaches address different limitations of large language models, with RAG focusing on knowledge access and fine-tuning on specialized capabilities. (9 sources)", "text": "\nRetrieval-Augmented Generation (RAG) emerged as a groundbreaking approach to enhance large language models (LLMs) by combining parametric memory (knowledge stored in model weights) with non-parametric memory (external knowledge bases). First introduced by Facebook AI Research in 2020, RAG addresses fundamental limitations of traditional generative models, particularly their struggle with specialized or rare information that can lead to hallucinations <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. The core mechanism of RAG involves augmenting the input space of language models with retrieved text passages, which has demonstrated significant improvements in knowledge-intensive tasks <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>.\n\nRAG systems improve model performance by generating more accurate and reliable answers through the integration of retrieved context with queries, effectively mitigating hallucinations common in standalone generative models <Paper corpusId=\"273375021\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper>. A key advantage of RAG is its dynamic updating mechanism that refreshes the knowledge base without requiring retraining of the entire model, enhancing both reliability and clarity of responses <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. This capability has made RAG a prominent component in improving the functionality and performance of LLM-based applications <Paper corpusId=\"273654156\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nIn contrast, fine-tuning focuses on adjusting a model's weights according to new data, allowing for customization without retraining the entire model. This approach is particularly effective for adapting pre-trained LLMs to specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. When building applications with domain-specific data, developers commonly use either RAG to augment prompts with external data or fine-tuning to incorporate additional knowledge directly into the model parameters <Paper corpusId=\"267027552\" paperTitle=\"(Balaguer et al., 2024)\" isShortName></Paper>.\n\nRecent research has begun exploring the integration of RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>. Comparative studies show that RAG outperforms unsupervised fine-tuning in scenarios involving new or unseen knowledge, highlighting its superiority in knowledge injection and model adaptation <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Advanced implementations like SELF-RAG have further refined traditional RAG by incorporating selective retrieval and self-reflection mechanisms that ensure only relevant content is retrieved based on the model's self-evaluation <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 273969615, "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zilun Zhang", "authorId": "2270181751"}, {"name": "Haozhan Shen", "authorId": "2174678931"}, {"name": "Tiancheng Zhao", "authorId": "8200875"}, {"name": "Yuhao Wang", "authorId": "2330774884"}, {"name": "Bin Chen", "authorId": "2330612748"}, {"name": "Yuxiang Cai", "authorId": "2149196373"}, {"name": "Yongheng Shang", "authorId": "2093090552"}, {"name": "Jianwei Yin", "authorId": "2111612160"}], "n_citations": 3}, "snippets": ["Retrieval-Augmented Generation (RAG) addresses the limitations of traditional generative models in handling specialized or long-tail knowledge. Early models like GPT, trained on vast corpora, excel at general queries but struggle with domain-specific or rare information, often generating hallucinations [95]. RAG, introduced by Facebook AI Research in 2020 (Lewis et al., 2020), enhances generative models by integrating realtime document retrieval, improving accuracy and contextual grounding. Gao et al. [97] categorize RAG into Naive, Advanced, and Modular paradigms, detailing key components like retrievers, generators, and augmentation methods. A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."], "score": 0.96533203125}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Zeqiu Wu", "authorId": "7806955"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Avirup Sil", "authorId": "2707234"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 780}, "snippets": ["Retrieval-Augmented Generation (RAG) augments the input space of LMs with retrieved text passages (Guu et al., 2020)(Lewis et al., 2020), leading to large improvements in knowledge-intensive tasks after fine-tuning or used with off-the-shelf LMs (Ram et al., 2023). A more recent work (Luo et al., 2023) instruction-tunes an LM with a fixed number of retrieved passages prepended to input, or pre-train a retriever and LM jointly, followed by fewshot fine-tuning on task datasets (Izacard et al., 2022b)."], "score": 0.888671875}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 273375021, "title": "CoFE-RAG: A Comprehensive Full-chain Evaluation Framework for Retrieval-Augmented Generation with Enhanced Data Diversity", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jintao Liu", "authorId": "2326248349"}, {"name": "Ruixue Ding", "authorId": "2058085406"}, {"name": "Linhao Zhang", "authorId": "2326165573"}, {"name": "Pengjun Xie", "authorId": "2326115683"}, {"name": "Fie Huang", "authorId": "2326114691"}], "n_citations": 5}, "snippets": ["In recent years, Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for improving the performance of large language models (LLMs). By integrating the retrieved context with queries, RAG systems can generate more accurate and reliable answers, thereby mitigating the issue of hallucinations that often plagues standalone generative models (Izacard et al., 2022)."], "score": 0.896484375}, {"id": "(Izacard et al., 2022)", "paper": {"corpus_id": 251371732, "title": "Few-shot Learning with Retrieval Augmented Language Models", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "M. Lomeli", "authorId": "3376175"}, {"name": "Lucas Hosseini", "authorId": "26360550"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Jane A. Yu", "authorId": "2129456957"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 783}, "snippets": ["Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."], "score": 0.0}, {"id": "(Anaissi et al., 2024)", "paper": {"corpus_id": 273501949, "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "year": 2024, "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)", "authors": [{"name": "Ali Anaissi", "authorId": "3333168"}, {"name": "Ali Braytee", "authorId": "3069261"}, {"name": "Junaid Akram", "authorId": "1992906806"}], "n_citations": 3}, "snippets": ["Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]", "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches", "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation", "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."], "score": 0.8994140625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273654156, "title": "R3AG: First Workshop on Refined and Reliable Retrieval Augmented Generation", "year": 2024, "venue": "SIGIR-AP", "authors": [{"name": "Zihan Wang", "authorId": "2259065706"}, {"name": "Xuri Ge", "authorId": "1380224383"}, {"name": "Joemon M. Jose", "authorId": "2286309062"}, {"name": "Haitao Yu", "authorId": "2327995614"}, {"name": "Weizhi Ma", "authorId": "2311314421"}, {"name": "Zhaochun Ren", "authorId": "2260895127"}, {"name": "Xin Xin", "authorId": "2294565066"}], "n_citations": 0}, "snippets": ["Retrieval-augmented generation (RAG) has gained wide attention as the key component to improve generative models with external knowledge augmentation from information retrieval. It has shown great prominence in enhancing the functionality and performance of large language model (LLM)-based applications."], "score": 0.89892578125}, {"id": "(Balaguer et al., 2024)", "paper": {"corpus_id": 267027552, "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "M. A. D. L. Balaguer", "authorId": "34938986"}, {"name": "Vinamra Benara", "authorId": "3456623"}, {"name": "Renato Luiz de Freitas Cunha", "authorId": "2279752416"}, {"name": "Roberto de M. Estevao Filho", "authorId": "2279750954"}, {"name": "Todd Hendry", "authorId": "2279749685"}, {"name": "Daniel Holstein", "authorId": "2279750514"}, {"name": "Jennifer Marsman", "authorId": "2279752770"}, {"name": "Nick Mecklenburg", "authorId": "2279750706"}, {"name": "S. Malvar", "authorId": "145707932"}, {"name": "Leonardo Nunes", "authorId": "2256989583"}, {"name": "Rafael Padilha", "authorId": "2279548480"}, {"name": "Morris Sharp", "authorId": "2279750745"}, {"name": "B. Silva", "authorId": "2257019569"}, {"name": "Swati Sharma", "authorId": "2279667352"}, {"name": "Vijay Aski", "authorId": "2257349985"}, {"name": "Ranveer Chandra", "authorId": "2256993742"}], "n_citations": 91}, "snippets": ["There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4", "We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further."], "score": 0.9619140625}], "table": null}, {"title": "Performance Comparison: RAG vs. Fine-tuning", "tldr": "Recent studies consistently show RAG outperforms fine-tuning for low-frequency knowledge and factual accuracy, while fine-tuning offers computational efficiency and better performance on familiar data. Comparative evaluations across multiple metrics demonstrate RAG's superior ability to reduce hallucinations and handle domain-specific information. (15 sources)", "text": "\nExtensive comparative studies have revealed distinct performance patterns between RAG and fine-tuning approaches across various domains and model architectures. Research examining performance on low-frequency entities and domain-specific knowledge has demonstrated that while fine-tuning boosts performance across entities of varying popularity, RAG significantly outperforms fine-tuning particularly for rare or less popular factual knowledge <Paper corpusId=\"268248396\" paperTitle=\"(Soudani et al., 2024)\" isShortName></Paper>. This advantage becomes especially apparent in specialized domains such as medical question answering, where RAG has proven more efficient than traditional model fine-tuning <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>.\n\nQuantitative evaluations across multiple metrics further substantiate RAG's advantages. When comparing RAG with Domain-specific Fine-Tuning (DFT) across multiple language model architectures including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2, RAG consistently outperforms DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in Coverage Score metrics. Interestingly, DFT showed only modest advantages in METEOR scores, suggesting slightly better creative capabilities <Paper corpusId=\"268510325\" paperTitle=\"(Lakatos et al., 2024)\" isShortName></Paper>. This pattern of RAG superiority extends to generative question-answering tasks, where advanced RAG implementations like \"Blended Retriever\" have demonstrated results that surpass fine-tuning performance on datasets such as SQUAD <Paper corpusId=\"269043117\" paperTitle=\"(Sawarkar et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of RAG in improving generation quality compared to purely generative models has been consistently demonstrated across various benchmarks <Paper corpusId=\"269283058\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. Multiple studies confirm that RAG significantly outperforms pure generative models across various domains including question answering, code generation, and content creation <Paper corpusId=\"273233795\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. Particularly notable is RAG's effectiveness in reducing hallucinations by providing factual answers <Paper corpusId=\"271843111\" paperTitle=\"(Kahl et al., 2024)\" isShortName></Paper> <Paper corpusId=\"233240939\" paperTitle=\"(Shuster et al., 2021)\" isShortName></Paper>.\n\nIn knowledge injection capacity evaluation, research has found that while LLMs can address performance issues through unsupervised fine-tuning, RAG consistently outperforms fine-tuning approaches in unsupervised learning scenarios <Paper corpusId=\"269292881\" paperTitle=\"(Efeoglu et al., 2024)\" isShortName></Paper>. This advantage extends to classification tasks, where RAG-based approaches have demonstrated greater flexibility, transparency in decision-making, superior classification performance, and improved robustness against adversarial attacks compared to model fine-tuning <Paper corpusId=\"273502659\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nDespite RAG's numerous advantages, fine-tuning approaches offer important complementary benefits. A recent large-scale empirical evaluation comparing RAG with parameter-efficient fine-tuning methods (LoRA and DoRA) on 20,000 FAQ-based queries revealed that DoRA achieved the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in domain-specific applications <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>. This highlights the trade-offs between these approaches: RAG excels in knowledge grounding, LoRA offers cost-efficient domain adaptation, while DoRA balances fine-tuning efficiency with model precision <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>.\n\nAn important practical consideration is that combining RAG with fine-tuned models can sometimes lead to unexpected performance degradation, suggesting challenges in effectively integrating these approaches <Paper corpusId=\"268510325\" paperTitle=\"(Lakatos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271843111\" paperTitle=\"(Kahl et al., 2024)\" isShortName></Paper>. This indicates that while both approaches have distinct strengths, their integration requires careful implementation to maximize benefits.", "citations": [{"id": "(Soudani et al., 2024)", "paper": {"corpus_id": 268248396, "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge", "year": 2024, "venue": "SIGIR-AP", "authors": [{"name": "Heydar Soudani", "authorId": "2165569122"}, {"name": "E. Kanoulas", "authorId": "1713134"}, {"name": "Faegheh Hasibi", "authorId": "1951737"}], "n_citations": 37}, "snippets": ["Language Models (LMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LMs in handling low-frequency entities on question answering tasks. We conduct extensive experiments on twelve LMs of varying size and type and different FT methods, data augmentation, and retrieval models. Our findings indicate that while FT boosts the performance across entities of varying popularity, RAG surpasses FT by a large margin particularly for least popular factual knowledge."], "score": 0.947265625}, {"id": "(Elgedawy et al., 2024)", "paper": {"corpus_id": 267061013, "title": "Dynamic Q&A of Clinical Documents with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ran Elgedawy", "authorId": "2280063225"}, {"name": "Ioana Danciu", "authorId": "2274464131"}, {"name": "Maria Mahbub", "authorId": "1387927897"}, {"name": "Sudarshan Srinivasan", "authorId": "2149506151"}], "n_citations": 6}, "snippets": ["In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning."], "score": 0.9013671875}, {"id": "(Lakatos et al., 2024)", "paper": {"corpus_id": 268510325, "title": "Investigating the performance of Retrieval-Augmented Generation and fine-tuning for the development of AI-driven knowledge-based systems", "year": 2024, "venue": "Machine Learning and Knowledge Extraction", "authors": [{"name": "R\u00f3bert Lakatos", "authorId": "2284077584"}, {"name": "P. Pollner", "authorId": "2279978930"}, {"name": "Andr\u00e1s Hajdu", "authorId": "2260742529"}, {"name": "Tam\u00e1s Jo\u00f3", "authorId": "2243157601"}], "n_citations": 10}, "snippets": ["In this study, we evaluate the performance of RAG and DFT on several LLM architectures, including GPT-J-6B, OPT-6.7B, LLaMA, and LLaMA-2. We use the ROUGE, BLEU, and METEOR scores to evaluate the performance of the models. We also measure the performance of the models with our own designed cosine similarity-based Coverage Score (CS). Our results, based on experiments across multiple datasets, show that RAG-based systems consistently outperform those fine-tuned with DFT. Specifically, RAG models outperform DFT by an average of 17% in ROUGE, 13% in BLEU, and 36% in CS. At the same time, DFT achieves only a modest advantage in METEOR, suggesting slightly better creative capabilities. We also highlight the challenges of integrating RAG with DFT, as such integration can lead to performance degradation."], "score": 0.9658203125}, {"id": "(Sawarkar et al., 2024)", "paper": {"corpus_id": 269043117, "title": "Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy with Semantic Search and Hybrid Query-Based Retrievers", "year": 2024, "venue": "Conference on Multimedia Information Processing and Retrieval", "authors": [{"name": "Kunal Sawarkar", "authorId": "2003089508"}, {"name": "Abhilasha Mangal", "authorId": "2295990127"}, {"name": "S. R. Solanki", "authorId": "2295990033"}], "n_citations": 57}, "snippets": ["We further extend such a 'Blended Retriever' to the RAG system to demonstrate far superior results on Generative Q&A datasets like SQUAD, even surpassing fine-tuning performance."], "score": 0.90576171875}, {"id": "(Jin et al., 2024)", "paper": {"corpus_id": 269283058, "title": "RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chao Jin", "authorId": "2170749149"}, {"name": "Zili Zhang", "authorId": "2182609505"}, {"name": "Xuanlin Jiang", "authorId": "2297730218"}, {"name": "Fangyue Liu", "authorId": "2297499793"}, {"name": "Xin Liu", "authorId": "2305828489"}, {"name": "Xuanzhe Liu", "authorId": "2237080638"}, {"name": "Xin Jin", "authorId": "2182349318"}], "n_citations": 47}, "snippets": ["Recent work [1](Borgeaud et al., 2021)22,(Lewis et al., 2020)37,42] has demonstrated that RAG can significantly improve the generation quality across various benchmarks compared to solely generative models."], "score": 0.9052734375}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 273233795, "title": "TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed KV Caches for Chunked Text", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Songshuo Lu", "authorId": "2303957265"}, {"name": "Hua Wang", "authorId": "2325461964"}, {"name": "Yutian Rong", "authorId": "2325157287"}, {"name": "Zhi Chen", "authorId": "2325197565"}, {"name": "Yaohua Tang", "authorId": "2304014129"}], "n_citations": 18}, "snippets": ["Recent studies (Borgeaud et al., 2021)Jiang et al., 2024;Trivedi et al., 2022;(Ram et al., 2023) have demonstrated that RAG significantly outperforms pure generative models across various benchmarks, thereby gathering considerable amounts of research interests in various domains such as question answering (Siriwardhana et al., 2022)Han et al., 2024), code generation (Lu et al., 2022), and content creation (Khattab et al., 2022), etc."], "score": 0.89599609375}, {"id": "(Siriwardhana et al., 2022)", "paper": {"corpus_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Rivindu Weerasekera", "authorId": "52001535"}, {"name": "Elliott Wen", "authorId": "2114425044"}, {"name": "Tharindu Kaluarachchi", "authorId": "1992921690"}, {"name": "R. Rana", "authorId": "1814487"}, {"name": "Suranga Nanayakkara", "authorId": "1486464114"}], "n_citations": 179}, "snippets": ["Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."], "score": 0.0}, {"id": "(Ram et al., 2023)", "paper": {"corpus_id": 256459451, "title": "In-Context Retrieval-Augmented Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Ori Ram", "authorId": "73775461"}, {"name": "Yoav Levine", "authorId": "152754428"}, {"name": "Itay Dalmedigos", "authorId": "1491822146"}, {"name": "Dor Muhlgay", "authorId": "51918041"}, {"name": "A. Shashua", "authorId": "3140335"}, {"name": "Kevin Leyton-Brown", "authorId": "2066411743"}, {"name": "Y. Shoham", "authorId": "1701353"}], "n_citations": 605}, "snippets": ["Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"], "score": 0.0}, {"id": "(Kahl et al., 2024)", "paper": {"corpus_id": 271843111, "title": "Evaluating the Impact of Advanced LLM Techniques on AI-Lecture Tutors for a Robotics Course", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Sebastian Kahl", "authorId": "2315811033"}, {"name": "Felix L\u00f6ffler", "authorId": "2320842831"}, {"name": "Martin Maciol", "authorId": "2315810380"}, {"name": "Fabian Ridder", "authorId": "2315810328"}, {"name": "Marius Schmitz", "authorId": "2315811128"}, {"name": "Jennifer Spanagel", "authorId": "2315812881"}, {"name": "Jens Wienkamp", "authorId": "2315811156"}, {"name": "Christopher Burgahn", "authorId": "1397379404"}, {"name": "M. Schilling", "authorId": "1913256"}], "n_citations": 3}, "snippets": ["Our findings underscored the positive impact of Retrieval-Augmented-Generation (RAG) and prompt engineering, which consistently improved model performance across similarity metrics. Particularly, the use of RAG demonstrated a considerable enhancement in providing factual answers and is consistent with the general belief that RAG is reducing hallucinations (Shuster et al., 2021)", "RAG appears as a very valuable technique that should be-together with some form of prompt engineering-considered first", "Fine-tuning has to be considered as a more involved technique. It requires additional effort in setting up a data set for training. As an advantage, in our case we saw that a quite small fine-tuned model (13 billion parameters) consistently performed on the same level-or better-as GPT-3.5 (175 billion parameters) when used without RAG. Fine-tuning produced a much more efficient expert which showed as quite capable. But, on the downside, the process of fine-tuning appeared as more delicate. In our data, we observed a curious drop-off when adding RAG to the fine-tuned model which was unexpected and would contradict our and others' experience with RAG."], "score": 0.896484375}, {"id": "(Shuster et al., 2021)", "paper": {"corpus_id": 233240939, "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kurt Shuster", "authorId": "35752280"}, {"name": "Spencer Poff", "authorId": "1753626755"}, {"name": "Moya Chen", "authorId": "2108267192"}, {"name": "Douwe Kiela", "authorId": "1743722"}, {"name": "J. Weston", "authorId": "145183709"}], "n_citations": 743}, "snippets": ["Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2020). In this work we explore the use of neural-retrieval-in-the-loop architectures - recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) - for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components - retrievers, rankers, and encoder-decoders - with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots."], "score": 0.0}, {"id": "(Efeoglu et al., 2024)", "paper": {"corpus_id": 269292881, "title": "Retrieval-Augmented Generation-based Relation Extraction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Sefika Efeoglu", "authorId": "2189018699"}, {"name": "Adrian Paschke", "authorId": "2259621860"}], "n_citations": 9}, "snippets": ["Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."], "score": 0.9521484375}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 273502659, "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation", "year": 2024, "venue": "", "authors": [{"name": "Jianfa Chen", "authorId": "2327003851"}, {"name": "Emily Shen", "authorId": "2326992786"}, {"name": "Trupti Bavalatti", "authorId": "2297187181"}, {"name": "Xiaowen Lin", "authorId": "2327028660"}, {"name": "Yongkai Wang", "authorId": "2326986310"}, {"name": "Shuming Hu", "authorId": "2327158340"}, {"name": "Harihar Subramanyam", "authorId": "2322094813"}, {"name": "Ksheeraj Sai Vepuri", "authorId": "2149726609"}, {"name": "Ming Jiang", "authorId": "2327303021"}, {"name": "Ji Qi", "authorId": "2327505613"}, {"name": "Li Chen", "authorId": "2287762612"}, {"name": "Nan Jiang", "authorId": "2326964342"}, {"name": "Ankit Jain", "authorId": "2287848816"}], "n_citations": 2}, "snippets": ["We propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."], "score": 0.9482421875}, {"id": "(Baqar et al., 2025)", "paper": {"corpus_id": 276408784, "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mohammad Baqar", "authorId": "2316485338"}, {"name": "Rajat Khanda", "authorId": "69923048"}], "n_citations": 1}, "snippets": ["This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications. Furthermore, this study examines the trade-offs between fine-tuning efficiency, computational cost, and real-time adaptability across different models. Findings highlight RAG's effectiveness in knowledge grounding, LoRA's cost-efficient domain adaptation, and DoRA's ability to balance fine-tuning efficiency with model precision."], "score": 0.986328125}], "table": null}, {"title": "Combined Approaches: Integrating RAG with Fine-tuning", "tldr": "Research shows combining RAG with fine-tuning offers cumulative performance improvements that exceed either approach alone, with studies reporting accuracy increases of over 6 percentage points from fine-tuning plus an additional 5 percentage points when RAG is added. Novel integration frameworks like Retrieval-Augmented Fine-Tuning (RAFT) teach models to dynamically leverage external knowledge while prioritizing relevant content and ignoring distractors. (12 sources)", "text": "\nRecent research has increasingly focused on combining the strengths of RAG and fine-tuning approaches to maximize performance gains. Studies show that these approaches can provide cumulative benefits, with one investigation reporting an accuracy increase of over 6 percentage points when fine-tuning models, followed by an additional 5 percentage point improvement when RAG was subsequently applied <Paper corpusId=\"267027552\" paperTitle=\"(Balaguer et al., 2024)\" isShortName></Paper>. This complementary relationship has been consistently observed across multiple studies, with researchers noting that the combination of fine-tuning with RAG generates responses with improved accuracy <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>.\n\nA notable advancement in this combined approach is Retrieval-Augmented Fine-Tuning (RAFT), which integrates retrieval methods directly with language model supervised fine-tuning. Unlike traditional RAG implementations that simply retrieve documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge while prioritizing relevant content and ignoring distracting information <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268510197\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. This approach has shown consistent performance improvements across domain-specific applications including PubMed, HotpotQA, and Gorilla datasets <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper>.\n\nSome researchers have explored Parameter-Efficient Fine-Tuning (PEFT) methods in conjunction with retrieval-enhanced models. One study applied various PEFT approaches (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) across model sizes ranging from 823 million to 48 billion parameters. Results showed that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process, though GPT models demonstrated higher performance potential with PEFT <Paper corpusId=\"271039066\" paperTitle=\"(Ficek et al., 2024)\" isShortName></Paper>.\n\nFor domain-specific applications, hybrid approaches have proven particularly effective. In medical contexts, studies have combined RAG with fine-tuned models and compared their performance against base models using either RAG or fine-tuning alone, finding the combined approach most effective <Paper corpusId=\"273566432\" paperTitle=\"(Bora et al., 2024)\" isShortName></Paper>. Another novel strategy, named \"Honest AI,\" demonstrated that while RAG alone provided some improvement, the hybrid approach that combined RAG with fine-tuning achieved the highest scores in the CRAG benchmark <Paper corpusId=\"273346023\" paperTitle=\"(Chen et al._1, 2024)\" isShortName></Paper>.\n\nThe integration of these approaches can extend beyond text-only applications. Research has shown that RAG-based frameworks for visual-linguistic models significantly enhance performance in complex tasks requiring background knowledge <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>. Furthermore, studies of pre-training and fine-tuning processes with RAG demonstrate how the approach can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\nCurrent instruction tuning approaches for optimizing LLMs in RAG systems typically use supervised fine-tuning (SFT) to equip models to handle diverse RAG tasks with different instructions. However, researchers have noted limitations in this approach, as it can cause RAG modules to overfit training signals and overlook varying data preferences among agents within the RAG system <Paper corpusId=\"273403480\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This has led to exploration of more sophisticated integration methods that preserve the general capabilities of LLMs while adapting them to RAG scenarios <Paper corpusId=\"270123034\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nAs the field evolves, researchers continue to develop novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments <Paper corpusId=\"270560505\" paperTitle=\"(Balakrishnan et al., 2024)\" isShortName></Paper>. These advances demonstrate the complementary nature of parametric and non-parametric approaches, with integration strategies like SELF-RAG incorporating selective retrieval and self-reflection mechanisms to ensure only relevant content is retrieved based on the model's self-evaluation <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Balaguer et al., 2024)", "paper": {"corpus_id": 267027552, "title": "RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "M. A. D. L. Balaguer", "authorId": "34938986"}, {"name": "Vinamra Benara", "authorId": "3456623"}, {"name": "Renato Luiz de Freitas Cunha", "authorId": "2279752416"}, {"name": "Roberto de M. Estevao Filho", "authorId": "2279750954"}, {"name": "Todd Hendry", "authorId": "2279749685"}, {"name": "Daniel Holstein", "authorId": "2279750514"}, {"name": "Jennifer Marsman", "authorId": "2279752770"}, {"name": "Nick Mecklenburg", "authorId": "2279750706"}, {"name": "S. Malvar", "authorId": "145707932"}, {"name": "Leonardo Nunes", "authorId": "2256989583"}, {"name": "Rafael Padilha", "authorId": "2279548480"}, {"name": "Morris Sharp", "authorId": "2279750745"}, {"name": "B. Silva", "authorId": "2257019569"}, {"name": "Swati Sharma", "authorId": "2279667352"}, {"name": "Vijay Aski", "authorId": "2257349985"}, {"name": "Ranveer Chandra", "authorId": "2256993742"}], "n_citations": 91}, "snippets": ["There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4", "We see an accuracy increase of over 6 p.p. when fine-tuning the model and this is cumulative with RAG, which increases accuracy by 5 p.p. further."], "score": 0.9619140625}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 267412954, "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liang Zhang", "authorId": "2279813822"}, {"name": "Katherine Jijo", "authorId": "2279831793"}, {"name": "Spurthi Setty", "authorId": "2282528163"}, {"name": "Eden Chung", "authorId": "2279830841"}, {"name": "Fatima Javid", "authorId": "2282539958"}, {"name": "Natan Vidra", "authorId": "2279830757"}, {"name": "Thomas Clifford", "authorId": "2279838243"}], "n_citations": 20}, "snippets": ["Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."], "score": 0.91943359375}, {"id": "(Srinivas et al., 2025)", "paper": {"corpus_id": 277501853, "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sakhinana Sagar Srinivas", "authorId": "2203079037"}, {"name": "Venkataramana Runkana", "authorId": "2139833562"}], "n_citations": 1}, "snippets": ["Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering)."], "score": 0.94287109375}, {"id": "(Zhang et al._2, 2024)", "paper": {"corpus_id": 268510197, "title": "RAFT: Adapting Language Model to Domain Specific RAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianjun Zhang", "authorId": "1993655237"}, {"name": "Shishir G. Patil", "authorId": "2257979820"}, {"name": "Naman Jain", "authorId": "1646458461"}, {"name": "Sheng Shen", "authorId": "2191455"}, {"name": "M. Zaharia", "authorId": "143834867"}, {"name": "Ion Stoica", "authorId": "2055174324"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}], "n_citations": 208}, "snippets": ["Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally bake in new knowledge (e.g., time-critical news, or private domain knowledge) into the pretrained model either through RAG-based-prompting, or fine-tuning. However, the optimal methodology for the model to gain such new knowledge remains an open question. In this paper, we present Retrieval Augmented FineTuning (RAFT), a training recipe that improves the model's ability to answer questions in a\"open-book\"in-domain settings. In RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don't help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document that would help answer the question. This coupled with RAFT's chain-of-thought-style response helps improve the model's ability to reason. In domain-specific RAG, RAFT consistently improves the model's performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG. RAFT's code and demo are open-sourced at github.com/ShishirPatil/gorilla."], "score": 0.0}, {"id": "(Ficek et al., 2024)", "paper": {"corpus_id": 271039066, "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Aleksander Ficek", "authorId": "2186740325"}, {"name": "Jiaqi Zeng", "authorId": "2266881428"}, {"name": "Oleksii Kuchaiev", "authorId": "2787022"}], "n_citations": 1}, "snippets": ["Parameter-Efficient Fine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG) have become popular methods for adapting large language models while minimizing compute requirements. In this paper, we apply PEFT methods (P-tuning, Adapters, and LoRA) to a modified Retrieval-Enhanced Transformer (RETRO) and a baseline GPT model across several sizes, ranging from 823 million to 48 billion parameters. We show that RETRO models outperform GPT models in zero-shot settings due to their unique pre-training process but GPT models have higher performance potential with PEFT."], "score": 0.97802734375}, {"id": "(Bora et al., 2024)", "paper": {"corpus_id": 273566432, "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications", "year": 2024, "venue": "Machine Learning and Knowledge Extraction", "authors": [{"name": "Arunabh Bora", "authorId": "2327432431"}, {"name": "H. Cuay\u00e1huitl", "authorId": "1806041"}], "n_citations": 12}, "snippets": ["Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning."], "score": 0.9541015625}, {"id": "(Chen et al._1, 2024)", "paper": {"corpus_id": 273346023, "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xinxi Chen", "authorId": "2325887387"}, {"name": "Li Wang", "authorId": "2284477218"}, {"name": "Wei Wu", "authorId": "2325928099"}, {"name": "Qizhi Tang", "authorId": "2312876232"}, {"name": "Yiyao Liu", "authorId": "2325900000"}], "n_citations": 6}, "snippets": ["In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark."], "score": 0.923828125}, {"id": "(Wang et al._1, 2024)", "paper": {"corpus_id": 274776379, "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yujin Wang", "authorId": "2309429494"}, {"name": "Quanfeng Liu", "authorId": "2310858751"}, {"name": "Jiaqi Fan", "authorId": "1934959729"}, {"name": "Jinlong Hong", "authorId": "9451675"}, {"name": "Hongqing Chu", "authorId": "2309098"}, {"name": "Mengjian Tian", "authorId": "30849412"}, {"name": "Bingzhao Gao", "authorId": "2292395467"}, {"name": "Hong Chen", "authorId": "2238391457"}], "n_citations": 2}, "snippets": ["Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities", "Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance."], "score": 0.94384765625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 273403480, "title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xinze Li", "authorId": "2261354998"}, {"name": "Senkun Mei", "authorId": "2124028252"}, {"name": "Zhenghao Liu", "authorId": "49047064"}, {"name": "Yukun Yan", "authorId": "2277242040"}, {"name": "Shuo Wang", "authorId": "2267033597"}, {"name": "Shi Yu", "authorId": "2314785970"}, {"name": "Zheni Zeng", "authorId": "1633538428"}, {"name": "Hao Chen", "authorId": "2327546188"}, {"name": "Ge Yu", "authorId": "2204644192"}, {"name": "Zhiyuan Liu", "authorId": "2290295914"}, {"name": "Maosong Sun", "authorId": "2273551430"}, {"name": "Chenyan Xiong", "authorId": "2139787803"}], "n_citations": 11}, "snippets": ["To adapt LLMs for the RAG systems, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system."], "score": 0.97314453125}, {"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 270123034, "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yutao Zhu", "authorId": "1900406"}, {"name": "Zhaoheng Huang", "authorId": "2187935160"}, {"name": "Zhicheng Dou", "authorId": "1897235"}, {"name": "Ji-Rong Wen", "authorId": "2186578511"}], "n_citations": 6}, "snippets": ["Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."], "score": 0.958984375}, {"id": "(Balakrishnan et al., 2024)", "paper": {"corpus_id": 270560505, "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability", "year": 2024, "venue": "IEEE India Conference", "authors": [{"name": "Gautam Balakrishnan", "authorId": "2356633197"}, {"name": "A. Purwar", "authorId": "33856997"}], "n_citations": 14}, "snippets": ["More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."], "score": 0.90966796875}, {"id": "(Anaissi et al., 2024)", "paper": {"corpus_id": 273501949, "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "year": 2024, "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)", "authors": [{"name": "Ali Anaissi", "authorId": "3333168"}, {"name": "Ali Braytee", "authorId": "3069261"}, {"name": "Junaid Akram", "authorId": "1992906806"}], "n_citations": 3}, "snippets": ["Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]", "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches", "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation", "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."], "score": 0.8994140625}], "table": null}, {"title": "Domain-Specific Applications", "tldr": "RAG and fine-tuning approaches have been successfully applied across numerous specialized domains, with particularly strong results in medical question answering, dialogue systems, and visual-linguistic tasks. These domain-specific implementations demonstrate how RAG's ability to incorporate external knowledge complements fine-tuning's adaptation capabilities in knowledge-intensive contexts. (10 sources)", "text": "\n- **Medical Question Answering**: RAG has proven particularly effective in medical contexts, where accuracy is critical. Studies show that RAG implementations for medical question answering outperform traditional model fine-tuning approaches in both efficiency and accuracy <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>. Combined approaches that integrate RAG with fine-tuned models have demonstrated superior performance compared to using either base models with RAG or fine-tuning alone in medical applications <Paper corpusId=\"273566432\" paperTitle=\"(Bora et al., 2024)\" isShortName></Paper>.\n\n- **Dialogue Systems**: RAG has enhanced generative dialogue models by integrating knowledge from external databases, improving accuracy and relevance of generated responses. This approach enables continuous updates of domain-specific knowledge and excels particularly in knowledge-intensive conversation tasks <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. However, challenges remain in ensuring accurate retrieval and effectively integrating retrieved information with the model's prior knowledge <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\n- **Question-Answering Systems**: RAG pipelines have become standard in question-answering tasks, leveraging vector stores computed using pretrained embedding models. Despite advancements in embedding technologies, domain adaptation remains challenging, particularly in industry settings where fine-tuning data may be limited <Paper corpusId=\"273403870\" paperTitle=\"(Gupta et al., 2024)\" isShortName></Paper>. Recent LLM-based retrieval approaches have shown substantial improvements in information retrieval due to their enhanced semantic understanding capabilities <Paper corpusId=\"273962778\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>.\n\n- **Visual-Linguistic Models**: RAG-based frameworks have significantly enhanced model performance in complex visual-linguistic tasks, especially those requiring substantial background knowledge. Traditional end-to-end visual-linguistic models often struggle with insufficient knowledge, whereas RAG-enhanced models integrate more contextual information during generation, improving reasoning and generative abilities <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n- **Cross-Modal Reasoning**: Studies of pre-training and fine-tuning processes with RAG demonstrate how the approach can enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. This has proven particularly effective for cross-modal reasoning and retrieval tasks, where RAG significantly improves model performance by leveraging broader background knowledge <Paper corpusId=\"274776379\" paperTitle=\"(Wang et al._1, 2024)\" isShortName></Paper>.\n\n- **Multi-Domain Adaptation**: Research examining fine-tuning effects on LLMs' ability to extract and integrate contextual data for RAG systems across multiple domains has yielded unexpected results. Contrary to the typical improvements observed in standalone LLM applications, some studies found that fine-tuning actually decreased performance compared to baseline models when used in RAG contexts <Paper corpusId=\"270560495\" paperTitle=\"(Barnett et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Elgedawy et al., 2024)", "paper": {"corpus_id": 267061013, "title": "Dynamic Q&A of Clinical Documents with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ran Elgedawy", "authorId": "2280063225"}, {"name": "Ioana Danciu", "authorId": "2274464131"}, {"name": "Maria Mahbub", "authorId": "1387927897"}, {"name": "Sudarshan Srinivasan", "authorId": "2149506151"}], "n_citations": 6}, "snippets": ["In the same vein of the previous works, our research focuses on RAG for medical question answering and exploring how RAG in this context is more efficient than traditional models fine-tuning."], "score": 0.9013671875}, {"id": "(Bora et al., 2024)", "paper": {"corpus_id": 273566432, "title": "Systematic Analysis of Retrieval-Augmented Generation-Based LLMs for Medical Chatbot Applications", "year": 2024, "venue": "Machine Learning and Knowledge Extraction", "authors": [{"name": "Arunabh Bora", "authorId": "2327432431"}, {"name": "H. Cuay\u00e1huitl", "authorId": "1806041"}], "n_citations": 12}, "snippets": ["Existing studies primarily focus on fine-tuning LLMs on medical data, but this paper combines RAG and fine-tuned models and compares them against base models using RAG or only fine-tuning."], "score": 0.9541015625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 271329121, "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought", "year": 2024, "venue": "International Symposium on Chinese Spoken Language Processing", "authors": [{"name": "Yuetong Zhao", "authorId": "2312343839"}, {"name": "Hongyu Cao", "authorId": "2312344958"}, {"name": "Xianyu Zhao", "authorId": "2312340862"}, {"name": "Zhijian Ou", "authorId": "2243267608"}], "n_citations": 4}, "snippets": ["Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge."], "score": 0.90380859375}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Gupta et al., 2024)", "paper": {"corpus_id": 273403870, "title": "REFINE on Scarce Data: Retrieval Enhancement through Fine-Tuning via Model Fusion of Embedding Models", "year": 2024, "venue": "Applied Informatics", "authors": [{"name": "Ambuje Gupta", "authorId": "2326450664"}, {"name": "Mrinal Rawat", "authorId": "2326298544"}, {"name": "A. Stolcke", "authorId": "2306165476"}, {"name": "Roberto Pieraccini", "authorId": "2326296747"}], "n_citations": 1}, "snippets": ["Retrieval augmented generation (RAG) pipelines are commonly used in tasks such as question-answering (QA), relying on retrieving relevant documents from a vector store computed using a pretrained embedding model. However, if the retrieved context is inaccurate, the answers generated using the large language model (LLM) may contain errors or hallucinations. Although pretrained embedding models have advanced, adapting them to new domains remains challenging. Fine-tuning is a potential solution, but industry settings often lack the necessary fine-tuning data."], "score": 0.900390625}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 273962778, "title": "Invar-RAG: Invariant LLM-aligned Retrieval for Better Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ziwei Liu", "authorId": "2330357180"}, {"name": "Liangyin Zhang", "authorId": "2290883726"}, {"name": "Qian Li", "authorId": "2342500515"}, {"name": "Jianghua Wu", "authorId": "2333407955"}, {"name": "Guangxu Zhu", "authorId": "2330382982"}], "n_citations": 2}, "snippets": ["Retrieval-augmented generation (RAG) has shown impressive capability in providing reliable answer predictions and addressing hallucination problems. A typical RAG implementation uses powerful retrieval models to extract external information and large language models (LLMs) to generate answers. In contrast, recent LLM-based retrieval has gained attention for its substantial improvements in information retrieval (IR) due to the LLMs' semantic understanding capability."], "score": 0.89501953125}, {"id": "(Wang et al._1, 2024)", "paper": {"corpus_id": 274776379, "title": "RAC3: Retrieval-Augmented Corner Case Comprehension for Autonomous Driving with Vision-Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yujin Wang", "authorId": "2309429494"}, {"name": "Quanfeng Liu", "authorId": "2310858751"}, {"name": "Jiaqi Fan", "authorId": "1934959729"}, {"name": "Jinlong Hong", "authorId": "9451675"}, {"name": "Hongqing Chu", "authorId": "2309098"}, {"name": "Mengjian Tian", "authorId": "30849412"}, {"name": "Bingzhao Gao", "authorId": "2292395467"}, {"name": "Hong Chen", "authorId": "2238391457"}], "n_citations": 2}, "snippets": ["Jiang et al. [39] propose a RAG-based framework for visual-linguistic models, demonstrating how retrieval-augmented generation significantly enhances model performance in complex tasks, especially those requiring background knowledge. This research indicates that traditional end-to-end VLMs are often limited when faced with insufficient knowledge, whereas RAG, through the incorporation of external knowledge bases, enables the model to integrate more contextual information during the generation process, improving its reasoning and generative abilities", "Ram et al. [41] study the pre-training and fine-tuning processes of RAG, demonstrating how RAG can further enhance model performance in the fine-tuning stage by incorporating large-scale external data sources during pre-training. RAG not only acquires broader background knowledge during the initial training phase but also effectively utilizes this information during fine-tuning, enhancing the model's cross-modal reasoning ability, especially in cross-modal retrieval tasks, where RAG significantly improves model performance."], "score": 0.94384765625}, {"id": "(Barnett et al., 2024)", "paper": {"corpus_id": 270560495, "title": "Fine-Tuning or Fine-Failing? Debunking Performance Myths in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Scott Barnett", "authorId": "2279752649"}, {"name": "Zach Brannelly", "authorId": "2279020735"}, {"name": "Stefanus Kurniawan", "authorId": "2266469333"}, {"name": "Sheng Wong", "authorId": "2307101480"}], "n_citations": 2}, "snippets": ["This study aims to specifically examine the effects of fine-tuning LLMs on their ability to extract and integrate contextual data to enhance the performance of RAG systems across multiple domains. We evaluate the impact of fine-tuning on the LLMs' capacity for data extraction and contextual understanding by comparing the accuracy and completeness of fine-tuned models against baseline performances across datasets from multiple domains. Our findings indicate that fine-tuning resulted in a decline in performance compared to the baseline models, contrary to the improvements observed in standalone LLM applications as suggested by OpenAI."], "score": 0.9296875}], "table": null}, {"title": "Technical Innovations in RAG and Fine-tuning Research", "tldr": "Recent technical innovations in RAG systems include adaptive retrieval approaches like FLARE and Self-RAG that dynamically assess and improve content quality during generation. Novel frameworks such as RbFT, ALoFTRAG, and Finetune-RAG focus on enhancing model resilience against retrieval defects and improving accuracy without manual labeling. (9 sources)", "text": "\n- **Adaptive Retrieval Mechanisms**: Recent advances have shifted from static retrieval to adaptive approaches that dynamically assess and modify retrieval during generation. FLARE uses confidence scores to identify when additional knowledge is needed, retrieving external information for low-confidence sentences and regenerating them. Similarly, Self-RAG incorporates special tokens that enable the model to adaptively retrieve information and reflect on the quality of generated content, ensuring only relevant information is included based on the model's self-evaluation. Another approach, SuRe, generates conditional summarizations of retrievals and evaluates them with carefully designed prompts. <Paper corpusId=\"270688152\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper>\n\n- **Robust Fine-Tuning (RbFT)**: This innovative method addresses a fundamental limitation of RAG systems: their dependence on reliable retrievers and knowledge bases. RbFT enhances LLM resilience against retrieval defects through targeted fine-tuning tasks specifically designed to handle noisy, irrelevant, or misleading counterfactual information that might be retrieved in real-world scenarios. This approach represents a significant advancement in making RAG systems more trustworthy when faced with imperfect retrieval components. <Paper corpusId=\"275993994\" paperTitle=\"(Tu et al., 2025)\" isShortName></Paper>\n\n- **Automatic Local Fine-Tuning of RAG (ALoFTRAG)**: This framework improves RAG accuracy on specific domains without requiring manually labeled data or larger teacher models. ALoFTRAG generates and filters synthetic training data and performs LoRA fine-tuning, demonstrating significant improvements in citation accuracy (8.3%) and answer accuracy (3.0%) across 20 datasets in 26 languages. This approach provides a practical, cost-effective solution particularly valuable for sensitive domains like healthcare and finance. <Paper corpusId=\"275788867\" paperTitle=\"(Devine, 2025)\" isShortName></Paper>\n\n- **Self-Memory Generation (SelfMem)**: This framework moves beyond the limitations of fixed corpus retrieval by iteratively employing a retrieval-augmented generator to create an unbounded memory pool. A memory selector then chooses one output as memory for subsequent generation rounds, enabling the model to leverage its own output as self-memory. This approach has achieved state-of-the-art results in multiple text generation tasks including neural machine translation, abstractive text summarization, and dialogue generation. <Paper corpusId=\"276449952\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258479968\" paperTitle=\"(Cheng et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: This specialized fine-tuning method trains LLMs to distinguish between correct and fictitious context within RAG systems. Unlike approaches that focus on improving retrieval quality, Finetune-RAG targets the generation behavior of models when faced with imperfect or misleading inputs, addressing a critical challenge in RAG implementation. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **Knowledge Selection Optimization**: While substantial research has focused on improving knowledge retrieval for RAG systems, emerging work highlights the importance of knowledge selection\u2014determining which retrieved information to actually use in generation. This represents an underexplored area with significant potential for improving RAG performance by ensuring only the most relevant retrieved information influences generation. <Paper corpusId=\"273403839\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Comprehensive Experimental Frameworks**: Despite growing interest in RAG techniques, researchers have identified a significant gap in the literature: the lack of comprehensive experimental comparisons across the spectrum of advanced RAG techniques. Current literature primarily consists of systematic reviews and direct comparisons between successive state-of-the-art models, indicating a need for more holistic evaluation approaches that can better guide future development. <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Lyu et al., 2024)", "paper": {"corpus_id": 270688152, "title": "Retrieve-Plan-Generation: An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yuanjie Lyu", "authorId": "2187857206"}, {"name": "Zihan Niu", "authorId": "2307915655"}, {"name": "Zheyong Xie", "authorId": "2202470155"}, {"name": "Chao Zhang", "authorId": "2260850374"}, {"name": "Tong Xu", "authorId": "2277237058"}, {"name": "Yang Wang", "authorId": "2308313519"}, {"name": "Enhong Chen", "authorId": "2265580543"}], "n_citations": 11}, "snippets": ["Recent research indicates that adaptive retrieval, tailored to the demands of LLMs, can further enhance generation. FLARE (Jiang et al., 2023b) uses the generated sentence with a low confidence score as the query to retrieve external knowledge adaptively and then regenerates the current sentence, while Self-RAG (Asai et al., 2023) introduces special tokens allowing the model to adaptively retrieve and reflect the quality of generated content. SuRe (Kim et al., 2024) generates conditional summarizations of retrieval and evaluating them with carefully designed prompts."], "score": 0.9052734375}, {"id": "(Anaissi et al., 2024)", "paper": {"corpus_id": 273501949, "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "year": 2024, "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)", "authors": [{"name": "Ali Anaissi", "authorId": "3333168"}, {"name": "Ali Braytee", "authorId": "3069261"}, {"name": "Junaid Akram", "authorId": "1992906806"}], "n_citations": 3}, "snippets": ["Retrieval-Augmented Generation (RAG) leverages both parametric and non-parametric memory, significantly enhancing the performance of Large Language Models (LLMs) in translation and question-answering tasks, as highlighted by Lewis et al. [16]", "The RAG approach improves LLM performance through pretraining, combining various memory types to generate factbased, varied, and accurate language representations. This method employs a dynamic updating mechanism to refresh the knowledge base without retraining the entire model, thereby enhancing reliability and clarity [18]. Nonetheless, RAG faces issues like noise or conflicting information during the retrieval phase, necessitating improvements for response accuracy and reliability [19]. Lin et al. [20] suggest integrating RAG with fine-tuning methods to maximize benefits from both parametric and non-parametric approaches", "SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation", "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]."], "score": 0.8994140625}, {"id": "(Tu et al., 2025)", "paper": {"corpus_id": 275993994, "title": "RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against Retrieval Defects", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yiteng Tu", "authorId": "2275628230"}, {"name": "Weihang Su", "authorId": "2147219374"}, {"name": "Yujia Zhou", "authorId": "2290870875"}, {"name": "Yiqun Liu", "authorId": "2260835922"}, {"name": "Qingyao Ai", "authorId": "2256982003"}], "n_citations": 6}, "snippets": ["Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks."], "score": 0.95849609375}, {"id": "(Devine, 2025)", "paper": {"corpus_id": 275788867, "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Peter Devine", "authorId": "2341534946"}], "n_citations": 0}, "snippets": ["We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models. By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively. Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance."], "score": 0.9384765625}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 276449952, "title": "DH-RAG: A Dynamic Historical Context-Powered Retrieval-Augmented Generation Method for Multi-Turn Dialogue", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Feiyuan Zhang", "authorId": "2343693016"}, {"name": "Dezhi Zhu", "authorId": "2346899749"}, {"name": "James Ming", "authorId": "2346111453"}, {"name": "Yilun Jin", "authorId": "2297644069"}, {"name": "Di Chai", "authorId": "2064208321"}, {"name": "Liu Yang", "authorId": "2145494870"}, {"name": "Han Tian", "authorId": "2308447281"}, {"name": "Zhaoxin Fan", "authorId": "2346645031"}, {"name": "Kai Chen", "authorId": "2343460791"}], "n_citations": 2}, "snippets": ["Retrieval-Augmented Generation (RAG) systems significantly advance the capabilities of dialogue systems and question-answering tasks by amalgamating external knowledge bases with generative models. (Lewis et al., 2020) introduces the RAG models, adeptly merging pre-trained parametric and nonparametric memories for enhanced language generation. Subsequent studies [23] introduce several enhancements to RAG models, focusing on refining retrieval (Cheng et al., 2023)[36] and enhancing generation capabilities (Anderson et al., 2022)[17]. Recent innovations include FLARE [43], which introduces a feedback loop augmented retrieval method to iteratively refine retrieval outcomes and bolster generation quality. Additionally, Sel-fRAG [3] presents a self-supervised retrieval-augmented framework that boosts both retrieval and generation processes through the strategic use of pseudo-labels generated by the model itself."], "score": 0.90234375}, {"id": "(Cheng et al., 2023)", "paper": {"corpus_id": 258479968, "title": "Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xin Cheng", "authorId": "2193630544"}, {"name": "Di Luo", "authorId": "2215612529"}, {"name": "Xiuying Chen", "authorId": "2116950235"}, {"name": "Lemao Liu", "authorId": "2978364"}, {"name": "Dongyan Zhao", "authorId": "144060462"}, {"name": "Rui Yan", "authorId": "144539156"}], "n_citations": 102}, "snippets": ["With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem). The traditional approach for memory retrieval involves selecting memory that exhibits the highest similarity to the input. However, this method is constrained by the quality of the fixed corpus from which memory is retrieved. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a novel framework, selfmem, which addresses this limitation by iteratively employing a retrieval-augmented generator to create an unbounded memory pool and using a memory selector to choose one output as memory for the subsequent generation round. This enables the model to leverage its own output, referred to as self-memory, for improved generation. We evaluate the effectiveness of selfmem on three distinct text generation tasks: neural machine translation, abstractive text summarization, and dialogue generation, under two generation paradigms: fine-tuned small model and few-shot LLM. Our approach achieves state-of-the-art results in four directions in JRC-Acquis, XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1), demonstrating the potential of self-memory in enhancing retrieval-augmented generation models. Furthermore, we conduct thorough analyses of each component in the selfmem framework to identify bottlenecks and provide insights for future research."], "score": 0.0}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs."], "score": 0.95068359375}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 273403839, "title": "How Does Knowledge Selection Help Retrieval Augmented Generation?", "year": 2024, "venue": "", "authors": [{"name": "Xiangci Li", "authorId": "89919188"}, {"name": "Jessica Ouyang", "authorId": "2284862335"}], "n_citations": 0}, "snippets": ["Retrieval-augmented generation (RAG) is a powerful method for enhancing natural language generation by integrating external knowledge into a model's output. While prior work has demonstrated the importance of improving knowledge retrieval for boosting generation quality, the role of knowledge selection remains less clear."], "score": 0.89013671875}, {"id": "(Eibich et al., 2024)", "paper": {"corpus_id": 268819923, "title": "ARAGOG: Advanced RAG Output Grading", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Matouvs Eibich", "authorId": "2294361167"}, {"name": "Shivay Nagpal", "authorId": "2294361283"}, {"name": "Alexander Fred-Ojala", "authorId": "2294362877"}], "n_citations": 4}, "snippets": ["Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing."], "score": 0.92138671875}], "table": null}, {"title": "Limitations and Challenges", "tldr": "Despite their advantages, both RAG and fine-tuning approaches face significant technical limitations including retrieval quality issues, domain adaptation challenges, and computational overhead. The integration of these methods introduces additional complexities like decreased performance in combined systems and difficulties in effectively balancing retrieved knowledge with model parameters. (5 sources)", "text": "\nDespite the impressive results demonstrated by both RAG and fine-tuning approaches, researchers have identified several significant limitations and challenges that affect their practical implementation and performance. A fundamental challenge for RAG systems is their heavy dependence on retrieval quality and accuracy. Poor-quality or irrelevant retrieval results can significantly degrade the performance of RAG models, negatively impacting the generated content <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. This retrieval dependency represents a critical vulnerability in RAG implementations, as their effectiveness is directly tied to the retrieval component's ability to surface relevant information.\n\nAnother significant challenge involves effectively integrating retrieved information with the model's prior knowledge, which remains an open research problem <Paper corpusId=\"271329121\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Finding the optimal balance between leveraging external information and the model's parametric knowledge is not straightforward, particularly when retrieved information contradicts what the model has learned during pre-training. This integration challenge becomes even more pronounced when attempting to combine RAG with fine-tuning approaches, as the interaction between updated model parameters and retrieval mechanisms can produce unexpected results.\n\nDomain adaptation presents a persistent challenge for both approaches. While RAG has shown impressive results in general contexts, adapting it to specialized domains requires careful optimization of both retrieval and generation components <Paper corpusId=\"269149041\" paperTitle=\"(Weng, 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. The original RAG model was primarily trained and explored with Wikipedia-based knowledge, requiring significant modifications for effective deployment in specialized domains such as healthcare or news <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. This has led researchers to develop domain-specific variations like RAG-end2end that can adapt to domain-specific knowledge bases by updating all components during training <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\nThe evaluation of RAG systems presents another substantial challenge due to the complex interaction between retrieval and generation components <Paper corpusId=\"278033562\" paperTitle=\"(Park et al., 2025)\" isShortName></Paper>. The scarcity of benchmarks that facilitate detailed, component-specific assessment has hindered systematic comparison and improvement of RAG implementations <Paper corpusId=\"278033562\" paperTitle=\"(Park et al., 2025)\" isShortName></Paper>. This evaluation challenge extends to comparative studies between RAG and fine-tuning approaches, where the multifaceted nature of performance metrics (factuality, coherence, relevance, etc.) makes comprehensive assessment difficult <Paper corpusId=\"273549218\" paperTitle=\"(Liu et al._2, 2024)\" isShortName></Paper>.\n\nComputational and storage requirements present practical constraints for both approaches. RAG systems typically require maintaining and searching through large document collections, which can introduce latency during inference and significant storage overhead. Fine-tuning approaches, while generally more computationally efficient during inference, require substantial resources during the training phase and result in multiple model versions for different domains or tasks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. These resource constraints become particularly relevant in production environments where efficiency and scalability are critical considerations.\n\nFor combined approaches that integrate RAG with fine-tuning, researchers have identified an additional challenge: the fine-tuned models sometimes underperform baseline models when used in RAG contexts, contrary to the typical improvements observed in standalone LLM applications <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This suggests that the interaction between fine-tuning and retrieval mechanisms may introduce unexpected complications that require careful consideration and specialized optimization strategies.", "citations": [{"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 271329121, "title": "An Empirical Study of Retrieval Augmented Generation with Chain-of-Thought", "year": 2024, "venue": "International Symposium on Chinese Spoken Language Processing", "authors": [{"name": "Yuetong Zhao", "authorId": "2312343839"}, {"name": "Hongyu Cao", "authorId": "2312344958"}, {"name": "Xianyu Zhao", "authorId": "2312340862"}, {"name": "Zhijian Ou", "authorId": "2243267608"}], "n_citations": 4}, "snippets": ["Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is also a promising method to improve the performance of generative dialogue models (Izacard et al., 2022)(Borgeaud et al., 2021)(Guu et al., 2020)(Khandelwal et al., 2019). Retrieval augmented generation method enhances the performance and reliability of generative dialogue models by integrating knowledge from external databases. This method not only increases the accuracy and relevance of the generated text but also enables continuous updates of domain-specific knowledge, especially excelling in knowledge-intensive tasks. However, RAG still faces several challenges. Since the performance of retrieval augmented generation depends on the accuracy and efficiency of the retriever, poor-quality or irrelevant retrieval results may negatively impact the generated content. Additionally, how to effectively integrate the retrieved information with the prior knowledge of the model remains a significant challenge."], "score": 0.90380859375}, {"id": "(Weng, 2024)", "paper": {"corpus_id": 269149041, "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Benjue Weng", "authorId": "2296715370"}], "n_citations": 10}, "snippets": ["\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"], "score": 0.892578125}, {"id": "(Siriwardhana et al., 2022)", "paper": {"corpus_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Rivindu Weerasekera", "authorId": "52001535"}, {"name": "Elliott Wen", "authorId": "2114425044"}, {"name": "Tharindu Kaluarachchi", "authorId": "1992921690"}, {"name": "R. Rana", "authorId": "1814487"}, {"name": "Suranga Nanayakkara", "authorId": "1486464114"}], "n_citations": 179}, "snippets": ["Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."], "score": 0.0}, {"id": "(Park et al., 2025)", "paper": {"corpus_id": 278033562, "title": "MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Chanhee Park", "authorId": "2357698631"}, {"name": "Hyeonseok Moon", "authorId": "2101318920"}, {"name": "Chanjun Park", "authorId": "2115195904"}, {"name": "Heu-Jeoung Lim", "authorId": "83056580"}], "n_citations": 1}, "snippets": ["Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment."], "score": 0.92626953125}, {"id": "(Liu et al._2, 2024)", "paper": {"corpus_id": 273549218, "title": "Aggregated Knowledge Model: Enhancing Domain-Specific QA with Fine-Tuned and Retrieval-Augmented Generation Models", "year": 2024, "venue": "International Conference on AI-ML-Systems", "authors": [{"name": "Fengchen Liu", "authorId": "2327775708"}, {"name": "Jordan Jung", "authorId": "2327510830"}, {"name": "Wei Feinstein", "authorId": "2327335509"}, {"name": "Jeff DAmbrogia", "authorId": "2327336236"}, {"name": "Gary Jung", "authorId": "2327334401"}], "n_citations": 1}, "snippets": ["Our study embarks on a detailed comparison of two fine-tuned large language models and five retrieval-augmented generation (RAG) models."], "score": 0.93505859375}], "table": null}], "cost": 0.437667}}
