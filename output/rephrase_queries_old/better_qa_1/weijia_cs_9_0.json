{"better_query": "How do distinct linguistic capabilities such as syntax, world knowledge, and reasoning emerge at different phases during language model training, and what are the characteristic phase transitions observed?", "better_answer": {"sections": [{"title": "Introduction: Language Model Training and Capability Emergence", "tldr": "Language model pre-training involves autoregressive next-token prediction on vast text corpora, enabling models to acquire linguistic capabilities. Through this self-supervised learning process, models progressively develop syntax understanding, world knowledge, and reasoning abilities. (4 sources)", "text": "\nLarge language models (LLMs) develop their linguistic capabilities through an extensive pre-training process that follows a consistent pattern across different model architectures. This pre-training phase involves autoregressive prediction, where models learn to predict the next token in a sequence based on previous tokens <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. The process is fundamentally self-supervised, with models learning from vast and diverse textual corpora without requiring explicit labeling <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThe mathematical formulation of this learning objective is typically expressed as predicting the probability distribution of the next token given previous tokens, represented as p(wi|w1:i-1) <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. This seemingly simple objective of next-token prediction enables models to capture complex linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nRecent research suggests that the process of predicting subsequent words functions as a form of lossless compression of significant information <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. As models improve their prediction accuracy, they demonstrate a deeper understanding of language, which translates to a more nuanced comprehension of the world <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. This pre-training phase results in what is commonly referred to as a \"base\" model, which serves as a foundation for further specialized training <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThe capabilities that emerge during pre-training\u2014understanding of syntax, acquisition of world knowledge, and development of reasoning abilities\u2014provide a robust foundation for downstream tasks <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. This comprehensive language representation enables models to be applied to a wide range of applications after pre-training <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Huang et al., 2023)", "paper": {"corpus_id": 265067168, "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Weijiang Yu", "authorId": "2248673673"}, {"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Zhangyin Feng", "authorId": "51056532"}, {"name": "Haotian Wang", "authorId": "2256768984"}, {"name": "Qianglong Chen", "authorId": "1500384901"}, {"name": "Weihua Peng", "authorId": "2247980601"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Bing Qin", "authorId": "2257004102"}, {"name": "Ting Liu", "authorId": "2238862997"}], "n_citations": 903}, "snippets": ["Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."], "score": 0.857421875}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 269449935, "title": "Hallucination of Multimodal Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "Pichao Wang", "authorId": "2299164401"}, {"name": "Tianjun Xiao", "authorId": "39102205"}, {"name": "Tong He", "authorId": "2264160722"}, {"name": "Zongbo Han", "authorId": "2008848359"}, {"name": "Zheng Zhang", "authorId": "2298907819"}, {"name": "Mike Zheng Shou", "authorId": "2269732179"}], "n_citations": 197}, "snippets": ["During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."], "score": 0.90673828125}, {"id": "(Cuconasu et al., 2024)", "paper": {"corpus_id": 270688478, "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Florin Cuconasu", "authorId": "2281641042"}, {"name": "Giovanni Trappolini", "authorId": "120709579"}, {"name": "Nicola Tonellotto", "authorId": "2281641625"}, {"name": "Fabrizio Silvestri", "authorId": "2260653973"}], "n_citations": 2}, "snippets": ["Pre-training.Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."], "score": 0.8427734375}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277398866, "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Dingzong Zhang", "authorId": "2326763710"}, {"name": "Devi Listiyani", "authorId": "2345374431"}, {"name": "Priyanka Singh", "authorId": "2302811064"}, {"name": "Manoranjan Mohanty", "authorId": "2309099465"}], "n_citations": 0}, "snippets": ["During the pre-training phase, the model is exposed to a vast corpus of text data, learning to predict the next word in a sentence. This unsupervised learning process enables the model to acquire a deep understanding of syntax, semantics, and world knowledge. By learning from diverse and extensive text data, the model builds a comprehensive language representation that can be applied to various downstream tasks (Radford et al., 2019)."], "score": 0.89404296875}], "table": null}, {"title": "General Training Dynamics and Phase Transitions", "tldr": "Language model training exhibits non-uniform capability development characterized by distinct phase transitions rather than gradual improvement. These transitions manifest as abrupt performance improvements in specific linguistic abilities at different points during training. (9 sources)", "text": "\nThe training trajectory of large language models (LLMs) reveals intriguing dynamics that contradict the notion of uniform capability development. While overall training loss often appears to improve smoothly, research has shown that the acquisition of different capabilities occurs at distinct phases during training <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This non-uniform development is characterized by phase transitions\u2014sudden improvements in specific abilities after periods of apparent stagnation <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\nA particularly striking pattern observed during training is what researchers call \"breakthroughs,\" \"emergence,\" or \"phase transitions,\" where training loss plateaus for significant periods before experiencing precipitous drops <Paper corpusId=\"273661991\" paperTitle=\"(Gopalani et al., 2024)\" isShortName></Paper>. These transitions are not merely artifacts of measurement but reflect fundamental shifts in model capabilities <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper>. Behavioral analysis confirms that different linguistic abilities\u2014such as predicting parts of speech and individual tokens\u2014are acquired during independent phase transitions with varying slopes and inflection points <Paper corpusId=\"264935245\" paperTitle=\"(Quirke et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>.\n\nRecent research has identified specific transition patterns during model training. Chen et al. observed a two-stage acquisition process for grammatical capabilities in masked language models: first, a \"structure onset\" marked by the emergence of syntactic attention structure (SAS), followed by a \"capabilities onset\" where the model demonstrates improved performance on grammatical tasks <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This finding demonstrates how analyzing the evolution of interpretable features throughout training provides deeper insights into emergent model behavior.\n\nWhile some research suggests that certain linguistic processes develop gradually rather than through sharp transitions <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270370801\" paperTitle=\"(Weber et al., 2024)\" isShortName></Paper>, a novel interpretation by Nakagi et al. proposes three common phase transitions across different LLM architectures: (1) an initial surge in brain alignment as models begin following instructions, (2) a period of brain detachment coinciding with downstream task performance stagnation, and (3) a realignment phase as models consolidate their capabilities <Paper corpusId=\"276724796\" paperTitle=\"(Nakagi et al., 2025)\" isShortName></Paper>.\n\nThe existence of these emergent capabilities and phase transitions has significant implications for model scaling. As noted by Wei et al., certain abilities appear only in sufficiently large models and through sharp transitions, making it challenging to predict which capabilities will emerge in scaled-up versions based solely on observations of smaller models <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process", "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."], "score": 0.9501953125}, {"id": "(Nolfi, 2023)", "paper": {"corpus_id": 261049443, "title": "On the Unexpected Abilities of Large Language Models", "year": 2023, "venue": "Adaptive Behavior", "authors": [{"name": "S. Nolfi", "authorId": "3015062"}], "n_citations": 11}, "snippets": ["The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics."], "score": 0.88623046875}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 249674500, "title": "Emergent Abilities of Large Language Models", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Rishi Bommasani", "authorId": "150272855"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Donald Metzler", "authorId": "1680617"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "Tatsunori Hashimoto", "authorId": "2117567142"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "P. Liang", "authorId": "2075292388"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "W. Fedus", "authorId": "26958176"}], "n_citations": 2516}, "snippets": ["Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."], "score": 0.0}, {"id": "(Gopalani et al., 2024)", "paper": {"corpus_id": 273661991, "title": "Abrupt Learning in Transformers: A Case Study on Matrix Completion", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Pulkit Gopalani", "authorId": "2124445996"}, {"name": "Ekdeep Singh Lubana", "authorId": "35573359"}, {"name": "Wei Hu", "authorId": "2293775823"}], "n_citations": 4}, "snippets": ["Recent analysis on the training dynamics of Transformers has unveiled an interesting characteristic: the training loss plateaus for a significant number of training steps, and then suddenly (and sharply) drops to near--optimal values."], "score": 0.85986328125}, {"id": "(Quirke et al., 2023)", "paper": {"corpus_id": 264935245, "title": "Training Dynamics of Contextual N-Grams in Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Lucia Quirke", "authorId": "2243240285"}, {"name": "Lovis Heindrich", "authorId": "2047549746"}, {"name": "Wes Gurnee", "authorId": "2056771333"}, {"name": "Neel Nanda", "authorId": "2051128902"}], "n_citations": 5}, "snippets": ["Behavioral analysis has shown that many model capabilities, such as predicting parts of speech and individual tokens, are learned during independent phase transitions characterized by varying slopes and points of maximum curvature (Chiang et al., 2020)Wei et al., 2022)."], "score": 0.83984375}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273025704, "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jin Hwa Lee", "authorId": "2323997802"}, {"name": "Thomas Jiralerspong", "authorId": "2187058673"}, {"name": "Lei Yu", "authorId": "2324060912"}, {"name": "Y. Bengio", "authorId": "1865800402"}, {"name": "Emily Cheng", "authorId": "2323783941"}], "n_citations": 4}, "snippets": ["Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024)."], "score": 0.88037109375}, {"id": "(Weber et al., 2024)", "paper": {"corpus_id": 270370801, "title": "Interpretability of Language Models via Task Spaces", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Lucas Weber", "authorId": "2261081927"}, {"name": "Jaap Jumelet", "authorId": "51247075"}, {"name": "E. Bruni", "authorId": "2173646563"}, {"name": "Dieuwke Hupkes", "authorId": "3449411"}], "n_citations": 4}, "snippets": ["The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."], "score": 0.0}, {"id": "(Nakagi et al., 2025)", "paper": {"corpus_id": 276724796, "title": "Triple Phase Transitions: Understanding the Learning Dynamics of Large Language Models from a Neuroscience Perspective", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yuko Nakagi", "authorId": "2283312221"}, {"name": "Keigo Tada", "authorId": "2348095768"}, {"name": "Sota Yoshino", "authorId": "2348095652"}, {"name": "Shinji Nishimoto", "authorId": "2346109244"}, {"name": "Yu Takagi", "authorId": "2346108815"}], "n_citations": 0}, "snippets": ["We propose a novel interpretation for the learning dynamics of LLMs that vary in both training data and architecture, revealing that three phase transitions commonly emerge across these models during training: (1) alignment with the entire brain surges as LLMs begin adhering to task instructions Brain Alignment and Instruction Following, (2) unexpectedly, LLMs diverge from the brain during a period in which downstream task accuracy temporarily stagnates Brain Detachment and Stagnation, and (3) alignment with the brain reoccurs as LLMs become capable of solving the downstream tasks Brain Realignment and Consolidation."], "score": 0.97021484375}], "table": null}, {"title": "Emergence of Syntactic Capabilities", "tldr": "Syntactic capabilities emerge early in language model training through distinct phases, beginning with holistic mappings that evolve into increasingly abstract constructions with variable slots. This early acquisition of syntax precedes the development of world knowledge and reasoning abilities. (7 sources)", "text": "\nThe development of syntactic capabilities in language models follows a well-defined progression through multiple phases. Initially, models acquire holistic mappings between utterances and their meanings, which quickly generalize to item-based constructions with variable slots <Paper corpusId=\"258378235\" paperTitle=\"(Doumen et al., 2023)\" isShortName></Paper>. This early phase also sees the emergence of slot-filling constructions and grammatical categories that capture the distribution of construction slots and their fillers. As training progresses, more abstract constructions emerge with an increasing number of variable slots, eventually stabilizing as the model refines its categorical network <Paper corpusId=\"258378235\" paperTitle=\"(Doumen et al., 2023)\" isShortName></Paper>.\n\nResearch consistently shows that syntactic rules are learned early in the training process, well before the acquisition of world knowledge and reasoning capabilities <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This pattern holds across different model architectures, with studies showing that early training stages focus primarily on n-gram learning, gradually refining to accommodate longer contexts and more nuanced linguistic capabilities <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nChen et al. identified a critical \"structure onset\" during training, marked by a dramatic increase in syntactic attention structure (SAS), followed by a \"capabilities onset\" where models demonstrate improved performance on grammatical tasks <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. These transitions coincide with precipitous drops in masked language modeling loss and represent distinct phases in the acquisition of grammatical knowledge.\n\nThe relationship between syntactic capabilities and brain alignment provides further insight into this developmental trajectory. Research demonstrates that alignment with the human language network primarily correlates with formal linguistic competence, which peaks and saturates early in training <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that syntactic and compositional structures are foundational capabilities that stabilize before more complex linguistic functions develop.\n\nIn multilingual contexts, language models first acquire language-specific syntactic knowledge independently before developing cross-linguistic correspondences <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>. This sequential process reflects a progression from mastering token-level knowledge to learning higher-level abstract concepts, indicating an evolution toward more conceptual understanding as training advances <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Doumen et al., 2023)", "paper": {"corpus_id": 258378235, "title": "Modelling Language Acquisition through Syntactico-Semantic Pattern Finding", "year": 2023, "venue": "Findings", "authors": [{"name": "Jonas Doumen", "authorId": "2187454476"}, {"name": "Katrien Beuls", "authorId": "2972424"}, {"name": "Paul Van Eecke", "authorId": "150266649"}], "n_citations": 15}, "snippets": ["The experiments show that a small number of general learning operators, which become active if an utterance cannot be successfully processed by the grammar learnt so far, effectively leads to learning dynamics that are similar to those described in the psycholinguistic literature (Pine et al., 1997)Tomasello, 2003;(Ambridge et al., 2015). In the first phase of the learning process, the learner acquires holistic mappings between utterances and their meaning representation. Soon after that, holophrase constructions are generalised to item-based constructions that integrate a variable slot. At the same time, this generalisation process leads to the emergence of slot-filling constructions, here called lexical constructions. Along with the item-based and lexical constructions, a network of grammatical categories emerges, capturing the distribution of construction slots and their observed fillers. In a third phase, more abstract itembased constructions emerge, with an increasingly large number of variable slots. In the final phase of the learning process, most constructions have already been acquired and most remaining impasses can be solved by adding new links to the categorial network. The learning dynamics are influenced by the degree of entrenchment of constructions."], "score": 0.87255859375}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.91845703125}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."], "score": 0.92138671875}, {"id": "(Chang et al., 2021)", "paper": {"corpus_id": 238408423, "title": "Word Acquisition in Neural Language Models", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 40}, "snippets": ["We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process", "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."], "score": 0.9501953125}, {"id": "(AlKhamissi et al., 2025)", "paper": {"corpus_id": 276766647, "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Badr AlKhamissi", "authorId": "2006905770"}, {"name": "Greta Tuckute", "authorId": "1780803442"}, {"name": "Yingtian Tang", "authorId": "2329164095"}, {"name": "T. Binhuraib", "authorId": "2092548838"}, {"name": "Antoine Bosselut", "authorId": "2691021"}, {"name": "Martin Schrimpf", "authorId": "8551292"}], "n_citations": 3}, "snippets": ["We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."], "score": 0.94287109375}, {"id": "(Inaba et al., 2025)", "paper": {"corpus_id": 276902722, "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tatsuro Inaba", "authorId": "2342412237"}, {"name": "Kentaro Inui", "authorId": "2261744024"}, {"name": "Yusuke Miyao", "authorId": "2302320083"}, {"name": "Yohei Oseki", "authorId": "50856622"}, {"name": "Benjamin Heinzerling", "authorId": "2266692"}, {"name": "Yu Takagi", "authorId": "2310230516"}], "n_citations": 1}, "snippets": ["Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding."], "score": 0.95263671875}], "table": null}, {"title": "Emergence of World Knowledge", "tldr": "World knowledge emerges later in language model training than syntactic capabilities, demonstrating a progressive acquisition pattern from basic linguistic structures to factual information. This knowledge acquisition is more domain-sensitive and less stable than syntactic learning, highlighting a developmental sequence where world understanding builds upon established linguistic foundations. (6 sources)", "text": "\nFollowing the early acquisition of syntactic capabilities, language models develop world knowledge as training progresses, establishing a clear developmental sequence. Multiple studies have confirmed that while syntactic rules are learned early in pre-training, world knowledge emerges later and with less stability <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This developmental pattern reflects a transition from basic n-gram learning to more nuanced linguistic capabilities that incorporate factual information about the world <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of world knowledge appears to be more domain-sensitive than syntactic learning, with factual and commonsense knowledge emerging more slowly and showing greater variation across domains <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This suggests that world knowledge acquisition requires more extensive exposure to varied textual contexts compared to the relatively uniform development of syntactic abilities.\n\nInterestingly, research has shown that world knowledge does not necessarily improve linearly as pre-training proceeds <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This non-linear development indicates that having more pre-training steps does not automatically provide a model with more comprehensive knowledge, highlighting the complex nature of knowledge acquisition in language models.\n\nThe autoregressive prediction mechanism during pre-training plays a crucial role in enabling models to develop world knowledge <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. Through self-supervised training on vast textual corpora, models progressively develop the ability to accurately predict the probability distribution for upcoming words, which indicates a deeper understanding of the world and its factual relationships <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nFrom a neuroscientific perspective, the relationship between world knowledge acquisition and brain alignment provides additional insights. While alignment with the human language network primarily correlates with formal linguistic competence and saturates early in training, functional linguistic competence\u2014which encompasses world knowledge and reasoning\u2014continues to develop beyond this stage <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that world knowledge acquisition may rely on cognitive systems beyond those responsible for syntactic processing, reflecting the multifaceted nature of linguistic development in both humans and language models.", "citations": [{"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.91845703125}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."], "score": 0.92138671875}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 265067168, "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Weijiang Yu", "authorId": "2248673673"}, {"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Zhangyin Feng", "authorId": "51056532"}, {"name": "Haotian Wang", "authorId": "2256768984"}, {"name": "Qianglong Chen", "authorId": "1500384901"}, {"name": "Weihua Peng", "authorId": "2247980601"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Bing Qin", "authorId": "2257004102"}, {"name": "Ting Liu", "authorId": "2238862997"}], "n_citations": 903}, "snippets": ["Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."], "score": 0.857421875}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 269449935, "title": "Hallucination of Multimodal Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "Pichao Wang", "authorId": "2299164401"}, {"name": "Tianjun Xiao", "authorId": "39102205"}, {"name": "Tong He", "authorId": "2264160722"}, {"name": "Zongbo Han", "authorId": "2008848359"}, {"name": "Zheng Zhang", "authorId": "2298907819"}, {"name": "Mike Zheng Shou", "authorId": "2269732179"}], "n_citations": 197}, "snippets": ["During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."], "score": 0.90673828125}, {"id": "(AlKhamissi et al., 2025)", "paper": {"corpus_id": 276766647, "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Badr AlKhamissi", "authorId": "2006905770"}, {"name": "Greta Tuckute", "authorId": "1780803442"}, {"name": "Yingtian Tang", "authorId": "2329164095"}, {"name": "T. Binhuraib", "authorId": "2092548838"}, {"name": "Antoine Bosselut", "authorId": "2691021"}, {"name": "Martin Schrimpf", "authorId": "8551292"}], "n_citations": 3}, "snippets": ["We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."], "score": 0.94287109375}], "table": null}, {"title": "Emergence of Reasoning Abilities", "tldr": "Reasoning capabilities emerge later in language model training, after syntactic understanding and world knowledge acquisition. This hierarchical development suggests reasoning depends on foundational linguistic skills and factual knowledge, with models progressing from simple n-gram prediction to complex inference abilities. (7 sources)", "text": "\nAmong the linguistic capabilities that emerge during language model training, reasoning abilities represent the most advanced phase of development. Research consistently demonstrates that reasoning emerges later in the training process, following the acquisition of syntactic rules and world knowledge <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"222140842\" paperTitle=\"(Chiang et al., 2020)\" isShortName></Paper>. This sequential development suggests a hierarchical dependency where reasoning capabilities build upon the foundation of syntactic understanding and factual knowledge.\n\nUnlike syntactic capabilities that emerge early and stabilize quickly, reasoning abilities demonstrate a more complex developmental trajectory. Studies have found that reasoning abilities are \"not stably acquired\" during training, indicating a more fragile and potentially domain-dependent acquisition process <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This instability contrasts with the more robust development of syntactic capabilities, highlighting the increased complexity involved in mastering reasoning tasks.\n\nThe autoregressive prediction mechanism appears fundamental to the emergence of reasoning capabilities. Through next-token prediction on extensive textual corpora, models progressively develop the ability to make logical inferences and draw conclusions based on contextual information <Paper corpusId=\"265067168\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269449935\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>. This self-supervised learning process enables models to transition from simple n-gram learning early in pre-training to more sophisticated reasoning in later stages <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nFrom a neuroscientific perspective, the development of reasoning abilities corresponds to what researchers term \"functional linguistic competence,\" which continues to improve even after formal linguistic competence (primarily syntax) has plateaued <Paper corpusId=\"276766647\" paperTitle=\"(AlKhamissi et al., 2025)\" isShortName></Paper>. This suggests that reasoning capabilities may engage cognitive systems beyond those responsible for basic language processing, reflecting a more distributed neural architecture for advanced linguistic functions.\n\nThe progressive refinement of language models' predictive capabilities\u2014from unigram frequencies to bigram probabilities and eventually to more nuanced predictions\u2014provides insight into how reasoning abilities might emerge <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. As models accumulate syntactic knowledge and world information, they gradually develop the capacity to make predictions that require multi-step logical inference, demonstrating the emergent nature of reasoning in language model training.", "citations": [{"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.91845703125}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers\"probing\"the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster."], "score": 0.92138671875}, {"id": "(Chiang et al., 2020)", "paper": {"corpus_id": 222140842, "title": "Pretrained Language Model Embryology: The Birth of ALBERT", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng-Han Chiang", "authorId": "1992777064"}, {"name": "Sung-Feng Huang", "authorId": "2210669195"}, {"name": "Hung-yi Lee", "authorId": "1706104"}], "n_citations": 42}, "snippets": ["While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 265067168, "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions", "year": 2023, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Weijiang Yu", "authorId": "2248673673"}, {"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Zhangyin Feng", "authorId": "51056532"}, {"name": "Haotian Wang", "authorId": "2256768984"}, {"name": "Qianglong Chen", "authorId": "1500384901"}, {"name": "Weihua Peng", "authorId": "2247980601"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Bing Qin", "authorId": "2257004102"}, {"name": "Ting Liu", "authorId": "2238862997"}], "n_citations": 903}, "snippets": ["Pre-training is generally considered a crucial stage for LLM to acquire knowledge and skills (Zhou et al., 2023a). Language models, during pre-training, aim to predict the next token in a sequence autoregressively. Through selfsupervised training on extensive textual corpora, the model acquires knowledge of language syntax, world knowledge, and reasoning abilities, providing a robust foundation for subsequent fine-tuning tasks. Besides, recent research (Sutskever, 2023;Del\u00e9tang et al., 2023) suggests that predicting subsequent words is akin to losslessly compressing significant information. The essence of language models lies in predicting the probability distribution for upcoming words. Accurate predictions indicate a profound grasp of knowledge, translating to a nuanced understanding of the world."], "score": 0.857421875}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 269449935, "title": "Hallucination of Multimodal Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zechen Bai", "authorId": "2237427303"}, {"name": "Pichao Wang", "authorId": "2299164401"}, {"name": "Tianjun Xiao", "authorId": "39102205"}, {"name": "Tong He", "authorId": "2264160722"}, {"name": "Zongbo Han", "authorId": "2008848359"}, {"name": "Zheng Zhang", "authorId": "2298907819"}, {"name": "Mike Zheng Shou", "authorId": "2269732179"}], "n_citations": 197}, "snippets": ["During pre-training, language models engage in autoregressive prediction, wherein they predict the subsequent token in a sequence. By undergoing self-supervised training on vast textual datasets, these models develop an understanding of language syntax, gain access to world knowledge, and enhance their reasoning capabilities."], "score": 0.90673828125}, {"id": "(Chang et al., 2021)", "paper": {"corpus_id": 238408423, "title": "Word Acquisition in Neural Language Models", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 40}, "snippets": ["We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words\u2019 ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."], "score": 0.0}, {"id": "(AlKhamissi et al., 2025)", "paper": {"corpus_id": 276766647, "title": "From Language to Cognition: How LLMs Outgrow the Human Language Network", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Badr AlKhamissi", "authorId": "2006905770"}, {"name": "Greta Tuckute", "authorId": "1780803442"}, {"name": "Yingtian Tang", "authorId": "2329164095"}, {"name": "T. Binhuraib", "authorId": "2092548838"}, {"name": "Antoine Bosselut", "authorId": "2691021"}, {"name": "Martin Schrimpf", "authorId": "8551292"}], "n_citations": 3}, "snippets": ["We demonstrated that alignment with the human language network (LN) primarily correlates with formal linguistic competence (Mahowald et al., 2024), peaking and saturating early in training. In contrast, functional linguistic competence, which involves world knowledge and reasoning, continues to grow beyond this stage. These findings suggest that the LN primarily encodes syntactic and compositional structure, in line with the literature of language neuroscience (Fedorenko et al., 2024), while broader linguistic functions may rely on other cognitive systems beyond the LN."], "score": 0.94287109375}], "table": null}, {"title": "Other Emergent Capabilities", "tldr": "Beyond syntax, world knowledge, and reasoning, language models develop several additional capabilities that emerge at different stages of training. These include in-context learning, instruction following, cross-linguistic correspondences, and pattern completion abilities that appear through distinct phase transitions. (7 sources)", "text": "\n- **In-context learning (ICL)**: The ability to learn from examples provided in the context window emerges during a \"phase change\" in the early stages of training. This capability is mechanistically linked to the development of \"induction heads\" in transformer models that enable them to detect and leverage patterns within the prompt <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. ICL has been interpreted as a form of Bayesian inference over latent space or as compositional recombination of linguistic structures <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258999187\" paperTitle=\"(Bietti et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246823378\" paperTitle=\"(Irie et al., 2022)\" isShortName></Paper>.\n\n- **Instruction following**: The ability to understand and follow natural language instructions emerges in larger models through sharp transitions between phases where the system shows no progress and subsequent phases where capabilities rapidly develop <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Step-by-step reasoning**: The capacity to break down complex problems into manageable steps emerges in sufficiently large models and appears through phase transitions that cannot be predicted by extrapolating from smaller models <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Arithmetic skills**: The ability to perform mathematical operations emerges in larger models through distinct phase transitions <Paper corpusId=\"261049443\" paperTitle=\"(Nolfi, 2023)\" isShortName></Paper> <Paper corpusId=\"249674500\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>.\n\n- **Cross-linguistic correspondences**: Language models first acquire language-specific knowledge independently before developing the ability to recognize and leverage similarities across languages. This represents a transition from token-level knowledge to more abstract conceptual understanding <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>.\n\n- **Pattern completion**: The development of functional modules during training enables models to recognize and complete complex patterns, which forms the mechanistic basis for other emergent capabilities <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Wang et al., 2024)", "paper": {"corpus_id": 274776135, "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shuo Wang", "authorId": "2263247139"}, {"name": "Issei Sato", "authorId": "2303397982"}], "n_citations": 0}, "snippets": ["The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]."], "score": 0.869140625}, {"id": "(Berti et al., 2025)", "paper": {"corpus_id": 276903421, "title": "Emergent Abilities in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leonardo Berti", "authorId": "2229196235"}, {"name": "Flavio Giorgi", "authorId": "2325903265"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 6}, "snippets": ["Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84]."], "score": 0.87060546875}, {"id": "(Bietti et al., 2023)", "paper": {"corpus_id": 258999187, "title": "Birth of a Transformer: A Memory Viewpoint", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "A. Bietti", "authorId": "2269602"}, {"name": "Vivien A. Cabannes", "authorId": "1387995815"}, {"name": "Diane Bouchacourt", "authorId": "3365029"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}, {"name": "L. Bottou", "authorId": "52184096"}], "n_citations": 96}, "snippets": ["Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties."], "score": 0.8837890625}, {"id": "(Irie et al., 2022)", "paper": {"corpus_id": 246823378, "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kazuki Irie", "authorId": "2350348"}, {"name": "R'obert Csord'as", "authorId": "2258963332"}, {"name": "J. Schmidhuber", "authorId": "145341374"}], "n_citations": 44}, "snippets": ["Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public."], "score": 0.0}, {"id": "(Nolfi, 2023)", "paper": {"corpus_id": 261049443, "title": "On the Unexpected Abilities of Large Language Models", "year": 2023, "venue": "Adaptive Behavior", "authors": [{"name": "S. Nolfi", "authorId": "3015062"}], "n_citations": 11}, "snippets": ["The interest in this topic was raised by the publication of an influential article by (Wei et al., 2022) entitled \"Emergent abilities of large language models\". The authors observe that several abilities, such as in-context learning, instruction following, step-by-step reasoning, and arithmetic skills, are not present in smaller models. They appear in larger models only. Moreover, they show that the acquisition process of these abilities in sufficiently large models is characterized by a sharp transition between a phase in which the system does not show any progress in the ability and a subsequent phase in which the system progresses. According to the authors, these two observations imply that predicting the abilities that will be acquired by scaled-up models will be impossible. In other words, they claim that the abilities that will be developed by scaled-up models cannot be extrapolated from what we know about smaller models. (Wei et al., 2022) later questioned the presence of sharp transitions during the training process and showed that the occurrence of sharp or more continuous transitions in the acquisition process depends on the metric used to evaluate the performance. The occurrence of sharp transitions might thus be induced by the utilization of specific metrics."], "score": 0.88623046875}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 249674500, "title": "Emergent Abilities of Large Language Models", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Yi Tay", "authorId": "144447820"}, {"name": "Rishi Bommasani", "authorId": "150272855"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "Donald Metzler", "authorId": "1680617"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "Tatsunori Hashimoto", "authorId": "2117567142"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "P. Liang", "authorId": "2075292388"}, {"name": "J. Dean", "authorId": "48448318"}, {"name": "W. Fedus", "authorId": "26958176"}], "n_citations": 2516}, "snippets": ["Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models."], "score": 0.0}, {"id": "(Inaba et al., 2025)", "paper": {"corpus_id": 276902722, "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tatsuro Inaba", "authorId": "2342412237"}, {"name": "Kentaro Inui", "authorId": "2261744024"}, {"name": "Yusuke Miyao", "authorId": "2302320083"}, {"name": "Yohei Oseki", "authorId": "50856622"}, {"name": "Benjamin Heinzerling", "authorId": "2266692"}, {"name": "Yu Takagi", "authorId": "2310230516"}], "n_citations": 1}, "snippets": ["Our findings suggest that LLMs initially acquire language-specific knowledge independently, followed by cross-linguistic correspondences. Moreover, we observe that after mastering token-level knowledge, the model transitions to learning higher-level, abstract concepts, indicating the development of more conceptual understanding."], "score": 0.95263671875}], "table": null}, {"title": "Mechanistic Explanations for Phase Transitions", "tldr": "Phase transitions in language model training are linked to the emergence of specific neural mechanisms that enable new capabilities. These include the formation of syntactic attention structures, development of induction heads for in-context learning, and the gradual emergence of functional modules that support pattern recognition. (7 sources)", "text": "\nThe sudden capability improvements observed during language model training can be mechanistically explained through the development of specific neural structures within the model architecture. Research has identified several key mechanisms underlying these phase transitions, providing insights into how models progress from simpler to more complex linguistic abilities.\n\nOne well-documented mechanism is the emergence of Syntactic Attention Structure (SAS), a property where specific transformer heads begin to focus on particular syntactic relations. Chen et al. observed that models abruptly acquire SAS during a brief window in pretraining, concurrent with a steep drop in masked language modeling loss <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This \"structure onset\" precipitates a subsequent \"capabilities onset\" where models demonstrate improved performance on grammatical tasks, establishing a causal relationship between the emergence of SAS and the development of grammatical capabilities <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nIn-context learning (ICL), another capability that emerges through phase transitions, has been mechanistically linked to the development of \"induction heads\" in transformer models. These specialized attention heads emerge during early training stages and enable models to detect and leverage patterns within the prompt <Paper corpusId=\"274776135\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Multiple studies support this connection, with some framing ICL as a form of Bayesian inference over latent space and others viewing it as compositional recombination of linguistic structures <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.\n\nThe development of functional modules during training represents another mechanistic explanation for emergent capabilities. These modules store training patterns as associative memories within the model's weight matrices, allowing for efficient pattern recognition and completion <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258999187\" paperTitle=\"(Bietti et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246823378\" paperTitle=\"(Irie et al., 2022)\" isShortName></Paper>. This memory-like functionality provides a foundation for capabilities that emerge later in training, as the model learns to store and retrieve increasingly complex linguistic patterns.\n\nWhile these mechanisms explain many phase transitions, research indicates that not all aspects of language learning proceed through distinct phases. Some studies suggest that linguistic circuits involved in specific subtasks remain stable and are gradually reinforced throughout training rather than developing through abrupt transitions <Paper corpusId=\"273025704\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270370801\" paperTitle=\"(Weber et al., 2024)\" isShortName></Paper>. This mixed evidence highlights the complex nature of capability emergence during language model training, with some abilities developing gradually while others manifest through distinct phase transitions.", "citations": [{"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process", "While language model training usually leads to smooth improvements in loss over time (Kaplan et al., 2020), not all knowledge emerges uniformly. Instead, language models acquire different capabilities at different points in training. Some capabilities remain fixed (Press et al., 2023), while others decline (McKenzie et al., 2022), as a function of dataset size or model capacity. Certain capabilities even exhibit abrupt improvements-this paper focuses on such discontinuous dynamics, which are often called breakthroughs (Srivastava et al., 2022), emergence (Wei et al., 2022), breaks (Caballero et al., 2023), or phase transitions (Olsson et al., 2022).\n\n\u2022 Monitoring latent syntactic structure (defined in Section 2.1) throughout training, we identify (Section 4.1) a precipitous loss drop composed of multiple phase transitions (defined in Section 2.3) relating to various linguistic abilities. At the onset of this stage (which we call the structure onset), SAS spikes. After the spike, the model starts handling complex linguistic phenomena correctly, as signaled by a break in BLiMP score (which we call the capabilities onset).\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset."], "score": 0.9501953125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 274776135, "title": "Understanding Knowledge Hijack Mechanism in In-context Learning through Associative Memory", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shuo Wang", "authorId": "2263247139"}, {"name": "Issei Sato", "authorId": "2303397982"}], "n_citations": 0}, "snippets": ["The capability for ICL [5] in language models develops gradually during a \"phase change\" observed in the early stages of training. At the same time, models acquire the ability to complete complex patterns through induction heads, suggesting that induction heads are a key mechanistic basis of ICL [40]."], "score": 0.869140625}, {"id": "(Berti et al., 2025)", "paper": {"corpus_id": 276903421, "title": "Emergent Abilities in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leonardo Berti", "authorId": "2229196235"}, {"name": "Flavio Giorgi", "authorId": "2325903265"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 6}, "snippets": ["Some studies frame ICL as a form of Bayesian inference over the latent space of language [94], while others view it as a compositional recombination of linguistic structures [32]. Mechanistically, ICL has been linked to neural architectures, such as induction heads [24]59] and functional modules that emerge naturally during training (Bietti et al., 2023)(Irie et al., 2022)81,84]."], "score": 0.87060546875}, {"id": "(Bietti et al., 2023)", "paper": {"corpus_id": 258999187, "title": "Birth of a Transformer: A Memory Viewpoint", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "A. Bietti", "authorId": "2269602"}, {"name": "Vivien A. Cabannes", "authorId": "1387995815"}, {"name": "Diane Bouchacourt", "authorId": "3365029"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}, {"name": "L. Bottou", "authorId": "52184096"}], "n_citations": 96}, "snippets": ["Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties."], "score": 0.8837890625}, {"id": "(Irie et al., 2022)", "paper": {"corpus_id": 246823378, "title": "The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kazuki Irie", "authorId": "2350348"}, {"name": "R'obert Csord'as", "authorId": "2258963332"}, {"name": "J. Schmidhuber", "authorId": "145341374"}], "n_citations": 44}, "snippets": ["Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273025704, "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jin Hwa Lee", "authorId": "2323997802"}, {"name": "Thomas Jiralerspong", "authorId": "2187058673"}, {"name": "Lei Yu", "authorId": "2324060912"}, {"name": "Y. Bengio", "authorId": "1865800402"}, {"name": "Emily Cheng", "authorId": "2323783941"}], "n_citations": 4}, "snippets": ["Phase transitions during LM training have been found for some, but not all, aspects of language learning. Negative evidence includes that LM circuits involved in linguistic subtasks are stable (Tigges et al., 2024) and gradually reinforced (Weber et al., 2024) over training. Positive evidence for learning phase transitions includes that the ID of BERT's final [CLS] representation tracks sudden syntax acquisition and drops in training loss (Chen et al., 2023), with similar observations on Transformers trained on formal languages (Lubana et al., 2024)."], "score": 0.88037109375}, {"id": "(Weber et al., 2024)", "paper": {"corpus_id": 270370801, "title": "Interpretability of Language Models via Task Spaces", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Lucas Weber", "authorId": "2261081927"}, {"name": "Jaap Jumelet", "authorId": "51247075"}, {"name": "E. Bruni", "authorId": "2173646563"}, {"name": "Dieuwke Hupkes", "authorId": "3449411"}], "n_citations": 4}, "snippets": ["The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes. In this paper, we present an alternative approach, concentrating on the quality of LM processing, with a focus on their language abilities. To this end, we construct 'linguistic task spaces' -- representations of an LM's language conceptualisation -- that shed light on the connections LMs draw between language phenomena. Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call 'similarity probing'. To disentangle the learning signals of linguistic phenomena, we further introduce a method called 'fine-tuning via gradient differentials' (FTGD). We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs."], "score": 0.0}], "table": null}], "cost": 0.35506200000000004}}
