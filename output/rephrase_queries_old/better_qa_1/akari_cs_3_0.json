{"better_query": "What empirical evidence exists on catastrophic forgetting or performance degradation in LLMs after fine-tuning for alignment or factuality tasks?", "better_answer": {"sections": [{"title": "Introduction to Catastrophic Forgetting in LLMs", "tldr": "Catastrophic forgetting refers to the phenomenon where LLMs lose previously learned capabilities or knowledge when fine-tuned on new tasks. This challenge is particularly concerning when models are fine-tuned for alignment or factuality, as they may trade off general capabilities for improvements in targeted areas. (LLM Memory)", "text": "\nCatastrophic forgetting is a well-documented phenomenon in neural networks where models lose previously acquired knowledge or skills when trained on new data or tasks. In the context of Large Language Models (LLMs), catastrophic forgetting manifests as the degradation of certain capabilities, factual recall, or reasoning skills after fine-tuning for specific objectives such as alignment, safety, or factuality.\n\nThe phenomenon becomes particularly relevant as organizations increasingly fine-tune foundation models to align with human preferences, reduce harmful outputs, or improve factual accuracy. During these fine-tuning processes, models may inadvertently sacrifice capabilities from their pre-training phase. This trade-off creates tension between improving targeted behaviors and maintaining the broad capabilities that make LLMs valuable.\n\nCatastrophic forgetting in LLMs differs somewhat from traditional neural networks due to the massive scale of parameters and the diverse knowledge encoded during pre-training. When fine-tuned on relatively small, specialized datasets, LLMs can experience disproportionate shifts in their output distribution, overemphasizing recently learned patterns at the expense of pre-existing knowledge. This is especially problematic when alignment fine-tuning introduces constraints that conflict with capabilities acquired during pre-training. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Empirical Evidence of Catastrophic Forgetting in LLMs", "tldr": "Numerous studies have demonstrated catastrophic forgetting in LLMs across various model sizes and fine-tuning scenarios, with evidence showing performance degradation in specific capabilities like reasoning, reading comprehension, and domain knowledge after fine-tuning for new tasks. (11 sources)", "text": "\nResearch has consistently documented the phenomenon of catastrophic forgetting in LLMs during fine-tuning processes. A key study by Luo et al. revealed that catastrophic forgetting occurs in LLMs ranging from 1B to 7B parameters, with the surprising finding that larger models actually experience more severe forgetting, possibly due to their initially stronger performance having more room to degrade <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Their experiments showed significant performance drops after continual instruction tuning, with BLOOMZ-7.1b's performance on social science tasks declining from 36.18% to 26.06% <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This finding is corroborated by Huang et al., who also found forgetting becomes more severe as model size increases <Paper corpusId=\"277501673\" paperTitle=\"(Haque, 2025)\" isShortName></Paper>.\n\nFine-tuning impacts different capabilities disproportionately. Reading comprehension appears to suffer the most dramatic forgetting (26.75% in BLOOMZ-7.1b), followed by domain knowledge (18.37%) and reasoning (13.62%) <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Similarly, Zheng et al. demonstrated that when LLaMa-2-7B is sequentially fine-tuned on new concepts, it shows clear evidence of forgetting previously learned knowledge despite achieving perfect memorization of each new task <Paper corpusId=\"267636596\" paperTitle=\"(Zheng et al., 2024)\" isShortName></Paper>.\n\nThe impact of forgetting extends to specialized capabilities as well. Yang et al. observed that vanilla fine-tuning on the OpenFunctions dataset led to a 27% decline in coding ability as measured by the HumanEval benchmark, with performance dropping from 13.4 to 9.8 <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Xu et al. found that supervised fine-tuning of LLaMA3.1-8B models on reasoning tasks can lead to performance degradation compared to zero-shot settings <Paper corpusId=\"276421775\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>. Even more concerning, Li et al. highlight research showing that fine-tuned models can perform worse than their foundation counterparts on several tasks <Paper corpusId=\"270357611\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nSafety alignment is particularly vulnerable to catastrophic forgetting. Studies show that fine-tuning with LoRA on adversarial datasets can effectively remove all safety guardrails previously embedded in pre-trained models like Llama 2 70B <Paper corpusId=\"266933290\" paperTitle=\"(Kalajdzievski, 2024)\" isShortName></Paper>. Similarly, Pletenev et al. demonstrated that increasing the amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance on the TruthfulQA benchmark <Paper corpusId=\"276482659\" paperTitle=\"(Pletenev et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266348792\" paperTitle=\"(Aleixo et al., 2023)\" isShortName></Paper>.\n\nResearchers have begun to identify possible mechanisms behind catastrophic forgetting. Li et al. showed that forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong correlation between sharpness and forgetting <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"260334888\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. Additionally, Wang et al. found that when fine-tuned on tasks requiring specific output formats, LLMs catastrophically forget how to perform other tasks such as question-answering, even with few-shot prompts <Paper corpusId=\"266933290\" paperTitle=\"(Kalajdzievski, 2024)\" isShortName></Paper>.", "citations": [{"id": "(Luo et al., 2023)", "paper": {"corpus_id": 261031244, "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yun Luo", "authorId": "2181146702"}, {"name": "Zhen Yang", "authorId": "2149231975"}, {"name": "Fandong Meng", "authorId": "33427918"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Jie Zhou", "authorId": "48128428"}, {"name": "Yue Zhang", "authorId": "2167740183"}], "n_citations": 318}, "snippets": ["The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM", ".Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning", ".For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting", ".We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively."], "score": 0.93212890625}, {"id": "(Haque, 2025)", "paper": {"corpus_id": 277501673, "title": "Catastrophic Forgetting in LLMs: A Comparative Analysis Across Language Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Naimul Haque", "authorId": "2353260836"}], "n_citations": 0}, "snippets": ["(Huang et al., 2024) conducted an empirical study on catastrophic forgetting in LLMs, finding that forgetting becomes more severe as model size increases, especially in models ranging from 1B to 7B parameters, during continual fine-tuning across domains like reasoning and reading comprehension."], "score": 0.7470703125}, {"id": "(Zheng et al., 2024)", "paper": {"corpus_id": 267636596, "title": "Can LLMs Learn New Concepts Incrementally without Forgetting?", "year": 2024, "venue": "", "authors": [{"name": "Junhao Zheng", "authorId": "2809051"}, {"name": "Shengjie Qiu", "authorId": "2273927657"}, {"name": "Qianli Ma", "authorId": "2274027644"}], "n_citations": 0}, "snippets": ["Main Findings 1: LLMs still suffer from catastrophic forgetting when incrementally learning new concepts. We sequentially fully fine-tuned LLaMa-2-7B on 10 tasks from Concept-1K. Before training, we evaluate the LLM on Concept-1K and find that the accuracy on both the training and test data is nearly zero. This indicates that the LLMs lack the knowledge to answer the questions in Concept-1K, thus avoiding the data leakage issue. \n\nFigure 2 shows a clear tendency for the LLMs to forget old concepts' knowledge when learning new concepts. Specifically, although LLMs achieve 100% memorization accuracy on each new task, the memorized knowledge is gradually forgotten as more tasks are learned. Similarly, the generalized knowledge also diminishes as new knowledge is acquired. Therefore, despite their power, we conclude that LLMs still suffer from catastrophic forgetting when fully fine-tuning on new data."], "score": 0.69873046875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 267769989, "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhaorui Yang", "authorId": "2284865968"}, {"name": "Qian Liu", "authorId": "1409707585"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Han Wang", "authorId": "2285032001"}, {"name": "H. Feng", "authorId": "46854712"}, {"name": "Minfeng Zhu", "authorId": "145314938"}, {"name": "Wei Chen", "authorId": "2256716159"}], "n_citations": 49}, "snippets": ["Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."], "score": 0.75244140625}, {"id": "(Xu et al., 2025)", "paper": {"corpus_id": 276421775, "title": "SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yige Xu", "authorId": "26339093"}, {"name": "Xu Guo", "authorId": "2325014301"}, {"name": "Zhiwei Zeng", "authorId": "49512311"}, {"name": "Chunyan Miao", "authorId": "2238949687"}], "n_citations": 22}, "snippets": ["We empirically observe that supervised finetuning of the LLaMA3.1-8B (Dubey et al., 2024) model with a language modeling objective on reasoning tasks (which is employed by both Coconut and CCoT) can lead to performance degradation compared with the zero-shot settings. We conjecture that this is due to catastrophic forgetting, a phenomenon also observed by Kalajdzievski (2024) and Lobo et al. (2024)."], "score": 0.74072265625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270357611, "title": "Revisiting Catastrophic Forgetting in Large Language Model Tuning", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hongyu Li", "authorId": "2305524549"}, {"name": "Liang Ding", "authorId": "46573238"}, {"name": "Meng Fang", "authorId": "2305485528"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 19}, "snippets": ["Recent works have provided substantial evidence confirming the negative impact of CF on LLMs, e.g., Bi et al. (2024) empirically show that the fine-tuned model is even worse than their foundation counterpart on several tasks, and Zeng et al. (2023) reveal the dropped general performance of LLMs after developing their agent capabilities."], "score": 0.7431640625}, {"id": "(Kalajdzievski, 2024)", "paper": {"corpus_id": 266933290, "title": "Scaling Laws for Forgetting When Fine-Tuning Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Damjan Kalajdzievski", "authorId": "3086897"}], "n_citations": 11}, "snippets": ["In (Luo et al., 2023), the authors instruction fine-tune a variety of pre-trained LLMs and show that the models forget across a variety of tasks, covering domain knowledge, reasoning, and reading comprehension. They observe that domain knowledge tasks suffer the worst forgetting, and performance drops significantly. For example, they show the language understanding on the social science subset of a standard benchmark dataset, MMLU (Hendrycks et al., 2020), drops from 36.18% to 26.06% after continual training. The work (Wang et al., 2023) shows that when fine-tuned on a task requiring a specific output format, LLMs catastrophically forget how to do other tasks such as question-answering, even when provided with few-shot prompts.\n\nThe work (Lermen et al., 2023) lends further evidence to the study of forgetting when fine-tuning LLMs. In particular, they demonstrate that fine-tuning the Llama 2 70B chat model with LoRA on an adversarial dataset can effectively remove all safety guardrails previously embedded in the pre-trained model."], "score": 0.79150390625}, {"id": "(Pletenev et al., 2025)", "paper": {"corpus_id": 276482659, "title": "How Much Knowledge Can You Pack into a LoRA Adapter without Harming LLM?", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Sergey Pletenev", "authorId": "2282523455"}, {"name": "Maria Marina", "authorId": "2346325484"}, {"name": "Daniil Moskovskiy", "authorId": "2268318729"}, {"name": "Vasily Konovalov", "authorId": "2329735486"}, {"name": "Pavel Braslavski", "authorId": "2346329178"}, {"name": "Alexander Panchenko", "authorId": "2282523462"}, {"name": "M. Salnikov", "authorId": "2253458749"}], "n_citations": 1}, "snippets": ["However, these modified LLMs may suffer from drawbacks, such as catastrophic forgetting (Aleixo et al., 2023)Kirkpatrick et al., 2017) or less severe but still notable loss of previously learned associations (Hoelscher-Obermaier et al., 2023). As shown in Figure 1, an increased amount of new data during fine-tuning with LoRA can degrade the model's pre-existing world knowledge, as evidenced by declining performance of the fine-tuned Llama-3.1 model on the TruthfulQA benchmark."], "score": 0.70068359375}, {"id": "(Aleixo et al., 2023)", "paper": {"corpus_id": 266348792, "title": "Catastrophic Forgetting in Deep Learning: A Comprehensive Taxonomy", "year": 2023, "venue": "Journal of the Brazilian Computer Society", "authors": [{"name": "Everton L. Aleixo", "authorId": "2082603039"}, {"name": "J. Colonna", "authorId": "1966519"}, {"name": "Marco Cristo", "authorId": "2275055914"}, {"name": "Everlandio Fernandes", "authorId": "2275056709"}], "n_citations": 26}, "snippets": ["Deep Learning models have achieved remarkable performance in tasks such as image classification or generation, often surpassing human accuracy. However, they can struggle to learn new tasks and update their knowledge without access to previous data, leading to a significant loss of accuracy known as Catastrophic Forgetting (CF). This phenomenon was first observed by McCloskey and Cohen in 1989 and remains an active research topic. Incremental learning without forgetting is widely recognized as a crucial aspect in building better AI systems, as it allows models to adapt to new tasks without losing the ability to perform previously learned ones. This article surveys recent studies that tackle CF in modern Deep Learning models that use gradient descent as their learning algorithm. Although several solutions have been proposed, a definitive solution or consensus on assessing CF is yet to be established. The article provides a comprehensive review of recent solutions, proposes a taxonomy to organize them, and identifies research gaps in this area."], "score": 0.0}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 278327022, "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jinpeng Chen", "authorId": "2217529598"}, {"name": "Runmin Cong", "authorId": "2258230512"}, {"name": "Yuzhi Zhao", "authorId": "2334518513"}, {"name": "Hongzheng Yang", "authorId": "2359170282"}, {"name": "Guangneng Hu", "authorId": "2337369321"}, {"name": "H. Ip", "authorId": "2221824976"}, {"name": "Sam Kwong", "authorId": "2287942988"}], "n_citations": 2}, "snippets": ["(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen."], "score": 0.791015625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 260334888, "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Bohao Li", "authorId": "1491798621"}, {"name": "Rui Wang", "authorId": "2151036422"}, {"name": "Guangzhi Wang", "authorId": "47226701"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Ying Shan", "authorId": "1387190008"}], "n_citations": 571}, "snippets": ["Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability."], "score": 0.0}], "table": null}, {"title": "Impact on Specific Capabilities and Knowledge", "tldr": "Fine-tuning LLMs for specific tasks leads to degradation across multiple capability dimensions, with particularly severe impacts on reasoning abilities, knowledge retention, output formatting, and safety guardrails. (17 sources)", "text": "\nResearch has identified several specific capabilities and knowledge areas that experience significant degradation during fine-tuning:\n\n* **Reasoning and problem-solving abilities**: LLMs show marked decline in logical reasoning capabilities after being fine-tuned on narrow tasks, undermining their ability to tackle complex problems <Paper corpusId=\"268510177\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275820535\" paperTitle=\"(Song et al., 2025)\" isShortName></Paper>.\n\n* **Knowledge retention**: Fine-tuning can distort the pre-training features that encode factual knowledge, resulting in reduced capacity to access previously learned information <Paper corpusId=\"264720747\" paperTitle=\"(Hajipour et al., 2022)\" isShortName></Paper> <Paper corpusId=\"216553067\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper>.\n\n* **Output formatting capabilities**: Models fine-tuned on specific formats can lose their flexibility in generating different types of outputs, leading to rigid response patterns that may not match user expectations <Paper corpusId=\"266844262\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\n* **Domain generalization**: When LLMs are tuned for domain-specific tasks, they often experience severe performance degradation on tasks from other domains <Paper corpusId=\"268247412\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270068369\" paperTitle=\"(Contal et al., 2024)\" isShortName></Paper>.\n\n* **Instruction-following capability**: PPO fine-tuning for specific objectives can impair a model's general ability to follow diverse instructions <Paper corpusId=\"270559716\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper>.\n\n* **Cross-lingual capabilities**: Fine-tuning predominantly on English data can diminish performance on non-English tasks, suggesting a language-specific aspect to catastrophic forgetting <Paper corpusId=\"270562404\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"262054014\" paperTitle=\"(Kotha et al., 2023)\" isShortName></Paper>.\n\n* **Task versatility**: Models fine-tuned on single tasks experience significant deterioration in previously mastered tasks, creating a direct trade-off between specialization and generalization <Paper corpusId=\"271244634\" paperTitle=\"(Roffo, 2024)\" isShortName></Paper>.\n\n* **Safety alignment**: Fine-tuning can erode safety guardrails implemented during alignment, potentially leading to unsafe outputs when prompted adversarially <Paper corpusId=\"259951356\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\n* **Knowledge uncertainty expression**: A critical capability that degrades during fine-tuning is the model's ability to express ignorance when appropriate, which increases the risk of hallucinations <Paper corpusId=\"279410046\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>.\n\n* **Representational collapse**: Fine-tuning can collapse the rich, generalizable representations from pre-training into a narrower set focused only on the immediate task, limiting transfer to new contexts <Paper corpusId=\"221083147\" paperTitle=\"(Aghajanyan et al., 2020)\" isShortName></Paper>.\n\n* **Out-of-distribution performance**: While fine-tuning often improves in-distribution task performance, it can substantially degrade performance on out-of-distribution examples, compromising model robustness <Paper corpusId=\"264720747\" paperTitle=\"(Hajipour et al., 2022)\" isShortName></Paper> <Paper corpusId=\"247011290\" paperTitle=\"(Kumar et al., 2022)\" isShortName></Paper>.\n\n* **In-context learning**: The ability to learn from examples provided in the prompt can diminish after instruction tuning, particularly affecting zero-shot and few-shot learning capabilities <Paper corpusId=\"270562404\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"237416585\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(He et al., 2024)", "paper": {"corpus_id": 268510177, "title": "Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yongquan He", "authorId": "2261355905"}, {"name": "Xuancheng Huang", "authorId": "2292011040"}, {"name": "Minghao Tang", "authorId": "2261366844"}, {"name": "Lingxun Meng", "authorId": "2292412508"}, {"name": "Xiang Li", "authorId": "2292290608"}, {"name": "Wei Lin", "authorId": "2291964148"}, {"name": "Wenyuan Zhang", "authorId": "2108347848"}, {"name": "Yifu Gao", "authorId": "2118545089"}], "n_citations": 5}, "snippets": ["However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded."], "score": 0.62451171875}, {"id": "(Song et al., 2025)", "paper": {"corpus_id": 275820535, "title": "How to Alleviate Catastrophic Forgetting in LLMs Finetuning? Hierarchical Layer-Wise and Element-Wise Regularization", "year": 2025, "venue": "", "authors": [{"name": "Shezheng Song", "authorId": "2152569963"}, {"name": "Hao Xu", "authorId": "2341857958"}, {"name": "Jun Ma", "authorId": "2152610427"}, {"name": "Shasha Li", "authorId": "1484986130"}, {"name": "Long Peng", "authorId": "2154357858"}, {"name": "Qian Wan", "authorId": "2304446070"}, {"name": "Xiaodong Liu", "authorId": "2110779334"}, {"name": "Jie Yu", "authorId": "2116457427"}], "n_citations": 1}, "snippets": ["As shown in Figure 1, catastrophic forgetting refers to the phenomenon where a model, during the process of finetuning, loses or overwrites knowledge learned during pretraining. This issue poses a severe limitation on the broader applicability of LLMs, as it undermines their versatility and reusability across domains. The fixed data composition and format in the fine-tuning data may impair the general knowledge previously learned by the model. This results in a loss of logical reasoning abilities and related general knowledge, which affects the model performance on domain-specific tasks. On the other hand, it may also lead to a decline in the ability to answer general tasks, including questions it was previously capable of answering."], "score": 0.736328125}, {"id": "(Hajipour et al., 2022)", "paper": {"corpus_id": 264720747, "title": "SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models", "year": 2022, "venue": "NAACL-HLT", "authors": [{"name": "Hossein Hajipour", "authorId": "3021793"}, {"name": "Ning Yu", "authorId": "2263370859"}, {"name": "Cristian-Alexandru Staicu", "authorId": "40141524"}, {"name": "Mario Fritz", "authorId": "2260336471"}], "n_citations": 5}, "snippets": ["Despite the effectiveness of the fine-tuning procedure, recent work shows that after fine-tuning, the LLMs can experience catastrophic forgetting in various NLP tasks (Luo et al., 2023;(Chen et al., 2020). Furthermore, (Kumar et al., 2022) validates that fully fine-tuning the models can distort the pretraining feature and adversely impact the OOD generalization performance in image classification tasks."], "score": 0.62890625}, {"id": "(Chen et al., 2020)", "paper": {"corpus_id": 216553067, "title": "Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Sanyuan Chen", "authorId": "2107970655"}, {"name": "Yutai Hou", "authorId": "8471176"}, {"name": "Yiming Cui", "authorId": "3043830"}, {"name": "Wanxiang Che", "authorId": "2256319"}, {"name": "Ting Liu", "authorId": "40282288"}, {"name": "Xiangzhan Yu", "authorId": "2218115449"}], "n_citations": 225}, "snippets": ["Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we propose a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community."], "score": 0.0}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 266844262, "title": "Examining Forgetting in Continual Pre-training of Aligned Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chen-An Li", "authorId": "2186278878"}, {"name": "Hung-yi Lee", "authorId": "2278588523"}], "n_citations": 10}, "snippets": ["Recent advances in Large Language Models (LLMs) have exhibited remarkable proficiency across various tasks. Given the potent applications of LLMs in numerous fields, there has been a surge in LLM development. In developing LLMs, a common practice involves continual pre-training on previously fine-tuned models. However, this can lead to catastrophic forgetting. In our work, we investigate the phenomenon of forgetting that occurs during continual pre-training on an existing fine-tuned LLM. We evaluate the impact of continuous pre-training on the fine-tuned LLM across various dimensions, including output format, knowledge, and reliability. Experiment results highlight the non-trivial challenge of addressing catastrophic forgetting during continual pre-training, especially the repetition issue."], "score": 0.7939453125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268247412, "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models", "year": 2024, "venue": "NAACL-HLT", "authors": [{"name": "Rui Wang", "authorId": "2151036536"}, {"name": "Fei Mi", "authorId": "2248150493"}, {"name": "Yi Chen", "authorId": "2165302640"}, {"name": "Boyang Xue", "authorId": "2248039956"}, {"name": "Hongru Wang", "authorId": "22642319"}, {"name": "Qi Zhu", "authorId": "2269768949"}, {"name": "Kam-Fai Wong", "authorId": "2274098358"}, {"name": "Rui-Lan Xu", "authorId": "8233941"}], "n_citations": 7}, "snippets": ["It has been observed that domain-specific tuning of LLMs can lead to catastrophic forgetting (Lin et al., 2023;Luo et al., 2023b), where an LLM loses its ability to perform previously learned tasks effectively. This suggests a balance must be struck between domain specialization and general proficiency."], "score": 0.68212890625}, {"id": "(Contal et al., 2024)", "paper": {"corpus_id": 270068369, "title": "RAGSys: Item-Cold-Start Recommender as RAG System", "year": 2024, "venue": "IR-RAG@SIGIR", "authors": [{"name": "Emile Contal", "authorId": "2303471987"}, {"name": "Garrin McGoldrick", "authorId": "2283934772"}], "n_citations": 5}, "snippets": ["Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data."], "score": 0.61376953125}, {"id": "(Hu et al., 2024)", "paper": {"corpus_id": 270559716, "title": "SeRTS: Self-Rewarding Tree Search for Biomedical Retrieval-Augmented Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Minda Hu", "authorId": "2249778653"}, {"name": "Licheng Zong", "authorId": "2121435800"}, {"name": "Hongru Wang", "authorId": "22642319"}, {"name": "Jingyan Zhou", "authorId": "30887444"}, {"name": "Jingjing Li", "authorId": "2274072714"}, {"name": "Yichen Gao", "authorId": "2307592921"}, {"name": "Kam-Fai Wong", "authorId": "2237563835"}, {"name": "Yu Li", "authorId": "2307192846"}, {"name": "Irwin King", "authorId": "2258549144"}], "n_citations": 3}, "snippets": ["PPO fine-tuning improves SeRTS's performance but is prone to Catastrophic Forgetting, degrading LLMs' instruction-following capability. Table 7 shows error cases where P query \u03d5 fails to follow the designed instruction. Although manual templates can temporarily address these issues in our current experiments, the noisy trajectories impede further iteration and self-improvement using SeRTS sampling and PPO fine-tuning when fed back into the loop. As a result, we restrict our experiments to a single iteration of sampling and PPO fine-tuning. Future research should prioritize mitigating Catastrophic Forgetting and investigate the potential of multiple PPO fine-tuning iterations for further LLM self-improvement."], "score": 0.67529296875}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 270562404, "title": "Refine Large Language Model Fine-tuning via Instruction Vector", "year": 2024, "venue": "", "authors": [{"name": "Gangwei Jiang", "authorId": "66129075"}, {"name": "Zhaoyi Li", "authorId": "2273744210"}, {"name": "Defu Lian", "authorId": "2266241113"}, {"name": "Ying Wei", "authorId": "2261250492"}], "n_citations": 1}, "snippets": ["Fine-tuning foundational LLMs (Touvron et al., 2023a,b) has become a generic technique for enhancing their capacity of following instructions (Wei et al., 2021)Zhang et al., 2024a,b) and mastering domain-specific content (Yue et al., 2023;Christophe et al., 2024).However, adopting such technique can have a negative effect of hurting the original ability of LLMs, which is widely known as Catastrophic Forgetting (Kirkpatrick et al., 2017;Zhai et al., 2023;Luo et al., 2024;(Kotha et al., 2023)Wu et al., 2024b)."], "score": 0.7373046875}, {"id": "(Kotha et al., 2023)", "paper": {"corpus_id": 262054014, "title": "Understanding Catastrophic Forgetting in Language Models via Implicit Inference", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Suhas Kotha", "authorId": "2150573121"}, {"name": "Jacob Mitchell Springer", "authorId": "153481934"}, {"name": "Aditi Raghunathan", "authorId": "2655157"}], "n_citations": 71}, "snippets": ["We lack a systematic understanding of the effects of fine-tuning (via methods such as instruction-tuning or reinforcement learning from human feedback), particularly on tasks outside the narrow fine-tuning distribution. In a simplified scenario, we demonstrate that improving performance on tasks within the fine-tuning data distribution comes at the expense of capabilities on other tasks. We hypothesize that language models implicitly infer the task of the prompt and that fine-tuning skews this inference towards tasks in the fine-tuning distribution. To test this, we propose Conjugate Prompting, which artificially makes the task look farther from the fine-tuning distribution while requiring the same capability, and we find that this recovers some of the pretraining capabilities in our synthetic setup. Since real-world fine-tuning distributions are predominantly English, we apply conjugate prompting to recover pretrained capabilities in LLMs by simply translating the prompts to different languages. This allows us to recover in-context learning abilities lost via instruction tuning, natural reasoning capability lost during code fine-tuning, and, more concerningly, harmful content generation suppressed by safety fine-tuning in chatbots like ChatGPT."], "score": 0.0}, {"id": "(Roffo, 2024)", "paper": {"corpus_id": 271244634, "title": "Exploring Advanced Large Language Models with LLMsuite", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Giorgio Roffo", "authorId": "2311696660"}], "n_citations": 0}, "snippets": ["Despite its advantages, fine-tuning on a single task may induce catastrophic forgetting, a phenomenon where the model's performance on previously learned tasks deteriorates.This occurs as fine-tuning optimizes the model's weights for the new task, potentially impairing its ability to perform other tasks.For example, while fine-tuning might improve a model's sentiment analysis capabilities, it could diminish its effectiveness in named entity recognition [27]."], "score": 0.61279296875}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 259951356, "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Zhenyi Wang", "authorId": "2920297"}, {"name": "Enneng Yang", "authorId": "151497321"}, {"name": "Li Shen", "authorId": "2172820082"}, {"name": "Heng Huang", "authorId": "2151748239"}], "n_citations": 51}, "snippets": ["Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text", "recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs."], "score": 0.62060546875}, {"id": "(Zhao et al., 2023)", "paper": {"corpus_id": 266374841, "title": "Learning and Forgetting Unsafe Examples in Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jiachen Zhao", "authorId": "2275190738"}, {"name": "Zhun Deng", "authorId": "2275284008"}, {"name": "David Madras", "authorId": "40373515"}, {"name": "James Zou", "authorId": "2276186660"}, {"name": "Mengye Ren", "authorId": "2540599"}], "n_citations": 18}, "snippets": ["Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten."], "score": 0.70703125}, {"id": "(Shen et al., 2025)", "paper": {"corpus_id": 279410046, "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning", "year": 2025, "venue": "", "authors": [{"name": "William F. Shen", "authorId": null}, {"name": "Xinchi Qiu", "authorId": "1702997626"}, {"name": "Nicola Cancedda", "authorId": "2313189467"}, {"name": "N. Lane", "authorId": "2298756346"}], "n_citations": 0}, "snippets": ["Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations."], "score": 0.61474609375}, {"id": "(Aghajanyan et al., 2020)", "paper": {"corpus_id": 221083147, "title": "Better Fine-Tuning by Reducing Representational Collapse", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Armen Aghajanyan", "authorId": "2201435"}, {"name": "Akshat Shrivastava", "authorId": "1519979046"}, {"name": "Anchit Gupta", "authorId": "3377939"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "S. Gupta", "authorId": "2701297"}], "n_citations": 210}, "snippets": ["Catastrophic forgetting, originally proposed as catastrophic interference, is a phenomena that occurs during sequential training where new updates interfere catastrophically with previous updates manifesting in forgetting of certain examples with respect to a fixed task (McCloskey & Cohen, 1989).\n\nInspired by this work, we explore the related problem of representational collapse; the degradation of generalizable representations of pre-trained models during the fine-tuning stage. This definition is independent of a specific fine-tuning task, but is rather over the internal representations generalizabality over a large union of tasks. Another view of this phenomena is that fine-tuning collapses the wide range of information available in the representations into a smaller set needed only for the immediate task and particular training set."], "score": 0.75439453125}, {"id": "(Kumar et al., 2022)", "paper": {"corpus_id": 247011290, "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ananya Kumar", "authorId": "32423266"}, {"name": "Aditi Raghunathan", "authorId": "2655157"}, {"name": "Robbie Jones", "authorId": "48368647"}, {"name": "Tengyu Ma", "authorId": "2114186424"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 685}, "snippets": ["When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer\u2014the \"head\"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \u2192 STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head\u2014this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning)."], "score": 0.0}, {"id": "(Wei et al., 2021)", "paper": {"corpus_id": 237416585, "title": "Finetuned Language Models Are Zero-Shot Learners", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jason Wei", "authorId": "144026731"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Vincent Zhao", "authorId": "2664737"}, {"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Adams Wei Yu", "authorId": "40625240"}, {"name": "Brian Lester", "authorId": "144104130"}, {"name": "Nan Du", "authorId": "2140321952"}, {"name": "Andrew M. Dai", "authorId": "2555924"}, {"name": "Quoc V. Le", "authorId": "2827616"}], "n_citations": 3788}, "snippets": ["This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning."], "score": 0.0}], "table": null}, {"title": "Factors Influencing Catastrophic Forgetting", "tldr": "Multiple factors influence the severity of catastrophic forgetting in LLMs, including model size, fine-tuning dataset characteristics, and optimization dynamics. Research shows that larger models may experience more severe forgetting, while training data composition and loss landscape sharpness also play significant roles. (5 sources)", "text": "\nSeveral key factors have been identified as influencing the severity and nature of catastrophic forgetting in Large Language Models during fine-tuning:\n\n**Model size and architecture**: Contrary to what might be expected, evidence suggests that larger models may experience more severe forgetting. Luo et al. found that as model scale increases from 1B to 7B parameters, the severity of forgetting intensifies, possibly because larger models with stronger initial performance have more room for degradation <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This pattern challenges the assumption that scale inherently provides robustness against forgetting.\n\n**Fine-tuning data characteristics**: The composition, diversity, and distribution of fine-tuning datasets significantly impact forgetting patterns. When there's substantial distribution shift between the pre-training data and fine-tuning data, models are more likely to experience severe capability degradation <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This is particularly evident when fine-tuning on narrowly focused datasets that differ significantly from the broad distribution of pre-training data.\n\n**Training dynamics and optimization**: The loss landscape dynamics during fine-tuning play a crucial role in forgetting. Research shows that catastrophic forgetting becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between loss landscape sharpness and forgetting severity <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"260334888\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This indicates that optimization strategies that maintain flatter loss landscapes might help mitigate forgetting.\n\n**Task sequence and continual learning settings**: The order and manner in which new tasks are introduced affect forgetting patterns. As more instruction tasks are introduced sequentially, general knowledge suffers increasingly significant forgetting <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This suggests that careful task ordering and interleaving might help preserve capabilities.\n\n**Hyperparameter choices**: Fine-tuning hyperparameters, particularly learning rate, significantly impact forgetting. Existing studies have shown that regulating learning rates and minimizing overall loss can mitigate forgetting to some extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>. However, these approaches alone are often insufficient to fully preserve original capabilities.\n\n**Modality interactions**: In multimodal models, catastrophic forgetting can manifest through shifted attention between modalities. During fine-tuning, models may shift from general visual-text alignment to dataset-specific patterns, resulting in performance degradation even when parts of the architecture (like the vision encoder) remain frozen <Paper corpusId=\"278327022\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\n**Balance between old and new knowledge**: The repetition frequency of previously learned examples and the ratio of new to old tasks in fine-tuning datasets can help mitigate forgetting. Appropriate settings for sample repetition and balanced task ratios have shown some success in preserving earlier capabilities <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nUnderstanding these factors is crucial for developing effective strategies to mitigate catastrophic forgetting while still achieving the desired improvements from fine-tuning.", "citations": [{"id": "(Luo et al., 2023)", "paper": {"corpus_id": 261031244, "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yun Luo", "authorId": "2181146702"}, {"name": "Zhen Yang", "authorId": "2149231975"}, {"name": "Fandong Meng", "authorId": "33427918"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Jie Zhou", "authorId": "48128428"}, {"name": "Yue Zhang", "authorId": "2167740183"}], "n_citations": 318}, "snippets": ["The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1b to 7b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model sale range which may result from the much significant initial performance in the larger LLM", ".Our findings reveal that the forgetting problem is generally present in LLMs. Interestingly, as the model scale increases from 1b to 7b parameters, the severity of forgetting intensifies. One potential explanation for this phenomenon is that larger language models exhibit stronger initial performance and, consequently, experience more pronounced performance degradation during continual instruction tuning", ".For instance, the performance of BLOOMZ-7.1b on MMLU-SocialScience in Figure 2 drops from 36.18% to 26.06% after continual training. The declining performance in LLMs indicates the presence of the catastrophic forgetting (CF) problem during the continual instruction tuning process. Moreover, as more instruction tasks are introduced, the general knowledge suffers more significant forgetting", ".We observe that the FG values for domain knowledge, reasoning, and reading comprehension are all above zero, indicating that general knowledge is forgotten during continual instruction tuning. Reading comprehension performance suffers the most drastic forgetting, followed by domain knowledge. For example, the FG values of BLOOMZ-7.1b are 26.75%, 18.37%, and 13.62% in reading comprehension, domain knowledge, and reasoning, respectively."], "score": 0.93212890625}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 267769989, "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhaorui Yang", "authorId": "2284865968"}, {"name": "Qian Liu", "authorId": "1409707585"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Han Wang", "authorId": "2285032001"}, {"name": "H. Feng", "authorId": "46854712"}, {"name": "Minfeng Zhu", "authorId": "145314938"}, {"name": "Wei Chen", "authorId": "2256716159"}], "n_citations": 49}, "snippets": ["Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."], "score": 0.75244140625}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 278327022, "title": "SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jinpeng Chen", "authorId": "2217529598"}, {"name": "Runmin Cong", "authorId": "2258230512"}, {"name": "Yuzhi Zhao", "authorId": "2334518513"}, {"name": "Hongzheng Yang", "authorId": "2359170282"}, {"name": "Guangneng Hu", "authorId": "2337369321"}, {"name": "H. Ip", "authorId": "2221824976"}, {"name": "Sam Kwong", "authorId": "2287942988"}], "n_citations": 2}, "snippets": ["(Li et al., 2023) shows that catastrophic forgetting during LLM fine-tuning becomes more pronounced as the loss landscape sharpens, suggesting a strong positive correlation between sharpness and forgetting. (Zhai et al., 2023) argues that in MLLMs, catastrophic forgetting arises as fine-tuning shifts the model's focus from general visual-text alignment to dataset-specific overfitting, resulting in performance degradation even when the vision encoder is frozen."], "score": 0.791015625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 260334888, "title": "SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Bohao Li", "authorId": "1491798621"}, {"name": "Rui Wang", "authorId": "2151036422"}, {"name": "Guangzhi Wang", "authorId": "47226701"}, {"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Ying Shan", "authorId": "1387190008"}], "n_citations": 571}, "snippets": ["Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, eliminating the need for human or GPT intervention during evaluation. We further evaluate the performance of 18 models across all 12 dimensions, covering both the spatial and temporal understanding. By revealing the limitations of existing MLLMs through evaluation results, we aim for SEED-Bench to provide insights for motivating future research. We will launch and consistently maintain a leaderboard to provide a platform for the community to assess and investigate model capability."], "score": 0.0}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 265158118, "title": "Vision-Language Instruction Tuning: A Review and Analysis", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Chen Li", "authorId": "2256784925"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Dian Li", "authorId": "2243307404"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 11}, "snippets": ["Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter."], "score": 0.62451171875}], "table": null}, {"title": "Safety and Alignment Implications", "tldr": "Catastrophic forgetting poses significant challenges for LLM safety, as fine-tuning can inadvertently compromise carefully implemented safety guardrails and alignment features, leading to models that may produce harmful outputs or fail to express uncertainty appropriately. (6 sources)", "text": "\nThe interplay between catastrophic forgetting and safety alignment represents a critical concern in LLM development. Research has revealed that even when models are initially well-aligned for safety, subsequent fine-tuning\u2014including on seemingly benign datasets\u2014can significantly erode these safety guardrails. Yang et al. found that distribution shifts during fine-tuning can lead to degradation not only in general task capabilities but also in models' safety alignment and helpfulness <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\nThis safety degradation manifests in multiple ways. Adversarial testing or \"red teaming\" has shown that safety mechanisms can be bypassed after fine-tuning, and safety metrics can deteriorate even when models are subsequently fine-tuned on benign downstream tasks <Paper corpusId=\"274982256\" paperTitle=\"(Ung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246634238\" paperTitle=\"(Perez et al., 2022)\" isShortName></Paper>. Zhao et al. demonstrated that while sequential safety fine-tuning can efficiently recover some safety aspects, it leads to catastrophic forgetting where both unsafe and important downstream examples are forgotten <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nA particularly concerning manifestation of catastrophic forgetting in safety contexts is the degradation of a model's ability to express ignorance when appropriate. Shen et al. found that this capability is significantly impaired during conventional fine-tuning, which can lead to increased hallucinations\u2014a serious safety issue when models generate fabricated information with high confidence <Paper corpusId=\"279410046\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>.\n\nThe bidirectional nature of safety and forgetting presents additional challenges. Wang et al. noted that aligned LLMs can readily learn unsafe content during custom fine-tuning, though they tend to forget this unsafe content more significantly than other examples when subsequently fine-tuned on safer content <Paper corpusId=\"259951356\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266374841\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This inconsistent retention pattern complicates safety assurance across the model's development lifecycle.\n\nThese findings highlight the delicate balance required when fine-tuning LLMs: improvements in specific capabilities or task performance must be weighed against potential degradation in safety alignment. The phenomenon suggests that safety is not a static property that can be permanently embedded but rather a dynamic characteristic that requires continuous monitoring and maintenance throughout a model's development and deployment lifecycle.", "citations": [{"id": "(Yang et al., 2024)", "paper": {"corpus_id": 267769989, "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhaorui Yang", "authorId": "2284865968"}, {"name": "Qian Liu", "authorId": "1409707585"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Han Wang", "authorId": "2285032001"}, {"name": "H. Feng", "authorId": "46854712"}, {"name": "Minfeng Zhu", "authorId": "145314938"}, {"name": "Wei Chen", "authorId": "2256716159"}], "n_citations": 49}, "snippets": ["Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."], "score": 0.75244140625}, {"id": "(Ung et al., 2024)", "paper": {"corpus_id": 274982256, "title": "Chained Tuning Leads to Biased Forgetting", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Megan Ung", "authorId": "2268761197"}, {"name": "Alicia Sun", "authorId": "2336870239"}, {"name": "Samuel J. Bell", "authorId": "2330399051"}, {"name": "Bhaktipriya Radharapu", "authorId": "2219919981"}, {"name": "Levent Sagun", "authorId": "2253337895"}, {"name": "Adina Williams", "authorId": "2330433992"}], "n_citations": 0}, "snippets": ["Catastrophic forgetting-the loss of information gained in earlier rounds of training as a consequence of subsequent rounds of training (Mc-Closkey and Cohen, 1989;(Ratcliff, 1990)-can pose a challenge in the context of ML model development (Goodfellow et al., 2013)Kirkpatrick et al., 2016;(Kemker et al., 2017). Recent works have also found evidence of catastrophic forgetting in the context of large language models (LLMs) (Kotha et al., 2023;Luo et al., 2023;(Razdaibiedina et al., 2023)Li and Lee, 2024). While finetuning with methods such as reinforcement learning from human feedback and instruction-tuning have been shown to be helpful for guiding models towards generating more desirable outputs (Bai et al., 2022), LLMs can still be brittle when finetuned on subsequent tasks. For example, previous work has shown that adversarial testing or red teaming can bypass safety mechanisms (Perez et al., 2022), and safety metrics can degrade even when the model is subsequently fine tuned on benign downstream tasks (Qi et al., 2023)."], "score": 0.85400390625}, {"id": "(Perez et al., 2022)", "paper": {"corpus_id": 246634238, "title": "Red Teaming Language Models with Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Francis Song", "authorId": "2059836321"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "John Aslanides", "authorId": "9958912"}, {"name": "Amelia Glaese", "authorId": "2105840001"}, {"name": "Nat McAleese", "authorId": "147687624"}, {"name": "G. Irving", "authorId": "2060655766"}], "n_citations": 668}, "snippets": ["Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\"red teaming\") using another LM. We evaluate the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot\u2019s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users."], "score": 0.0}, {"id": "(Zhao et al., 2023)", "paper": {"corpus_id": 266374841, "title": "Learning and Forgetting Unsafe Examples in Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jiachen Zhao", "authorId": "2275190738"}, {"name": "Zhun Deng", "authorId": "2275284008"}, {"name": "David Madras", "authorId": "40373515"}, {"name": "James Zou", "authorId": "2276186660"}, {"name": "Mengye Ren", "authorId": "2540599"}], "n_citations": 18}, "snippets": ["Sequential safety finetuning can recover the safety of models efficiently, but it leads to catastrophic forgetting, i.e., both unsafe and important downstream examples are forgotten."], "score": 0.70703125}, {"id": "(Shen et al., 2025)", "paper": {"corpus_id": 279410046, "title": "Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning", "year": 2025, "venue": "", "authors": [{"name": "William F. Shen", "authorId": null}, {"name": "Xinchi Qiu", "authorId": "1702997626"}, {"name": "Nicola Cancedda", "authorId": "2313189467"}, {"name": "N. Lane", "authorId": "2298756346"}], "n_citations": 0}, "snippets": ["Existing work on mitigating catastrophic forgetting in large language model (LLM) fine-tuning has primarily focused on preserving specific data or tasks, while critically overlooking the degradation of essential capabilities instilled through safety alignment, particularly the model's ability to faithfully express ignorance. In this work, we show that this capability is significantly degraded during conventional fine-tuning, leading to undesired behaviors such as hallucinations."], "score": 0.61474609375}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 259951356, "title": "A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Zhenyi Wang", "authorId": "2920297"}, {"name": "Enneng Yang", "authorId": "151497321"}, {"name": "Li Shen", "authorId": "2172820082"}, {"name": "Heng Huang", "authorId": "2151748239"}], "n_citations": 51}, "snippets": ["Zhang et al. (Zhang et al., 2024) conduct a detailed analysis of forgetting in LLMs, examining its effects on the topics, styles, and factual knowledge in text", "recent research (Zhao et al., 2023) highlights that safety finetuning of LLMs can lead to the forgetting of downstream task knowledge. To address this issue, ForgetFilter (Zhao et al., 2023) introduces a method that filters out unsafe examples prior to fine-tuning, ensuring that downstream task performance is preserved while maintaining the safety of LLMs."], "score": 0.62060546875}], "table": null}, {"title": "Mitigation Approaches", "tldr": "Researchers have developed several approaches to mitigate catastrophic forgetting in LLMs, including replay-based methods, selective distillation, parameter-efficient fine-tuning, and specialized optimization techniques that maintain model capabilities while adding new knowledge. (10 sources)", "text": "\n* **Replay-based methods**: These approaches incorporate examples from previous tasks during fine-tuning to maintain earlier capabilities. Recent advances in replay-based approaches for language models have shown promise in reducing catastrophic forgetting by rehearsing prior task data during training <Paper corpusId=\"276287820\" paperTitle=\"(Bhushan et al., 2025)\" isShortName></Paper>.\n\n* **Selective Distillation Fine-Tuning (SDFT)**: This technique has demonstrated effectiveness in preserving model capabilities while achieving comparable or superior performance on downstream tasks compared to vanilla fine-tuning. For instance, while vanilla fine-tuning on the OpenFunctions dataset led to a 27% decline in coding ability on the HumanEval benchmark, SDFT not only mitigated this degradation but marginally enhanced accuracy from 13.4 to 15.2 <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Learning rate regulation**: Controlling the learning rate during fine-tuning can help reduce catastrophic forgetting by limiting the extent of parameter updates. Existing studies have shown that regulating learning rates and minimizing overall loss can mitigate forgetting to some extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\n* **Sample repetition and task balancing**: Appropriately setting the repetition frequency of previously learned examples and balancing the ratio of new to old tasks in fine-tuning datasets can help preserve earlier capabilities. The training data in instructional tuning can be equipped with these settings to mitigate forgetting to a certain extent <Paper corpusId=\"265158118\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\n* **Parameter-efficient fine-tuning methods**: These approaches modify only a subset of model parameters during fine-tuning, which can help preserve general capabilities while adding new knowledge. However, even these methods face challenges with catastrophic forgetting, requiring additional techniques for mitigation <Paper corpusId=\"273345395\" paperTitle=\"(Thangarasa et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249921347\" paperTitle=\"(Korbak et al., 2021)\" isShortName></Paper>.\n\n* **Domain-specific adaptation frameworks**: Researchers have developed decision frameworks to guide the selection of appropriate LLM adaptation techniques based on use case constraints around data, compute, and performance needs, which can help balance specialization with versatility <Paper corpusId=\"276574830\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265294420\" paperTitle=\"(Li et al._2, 2023)\" isShortName></Paper>.\n\n* **Conditional Distributional Policy Gradients (CDPG)**: This approach extends Distributional Policy Gradients to conditional tasks, allowing fine-tuning that moves pretrained models closer to meeting control objectives without resulting in catastrophic forgetting, unlike baseline approaches <Paper corpusId=\"249921347\" paperTitle=\"(Korbak et al., 2021)\" isShortName></Paper>.\n\n* **Multimodal alignment preservation**: For multimodal LLMs, careful management of the fine-tuning process is needed to prevent the model from shifting its attention between modalities in ways that cause performance degradation. Early-stage fine-tuning on image datasets can improve performance across other image datasets by enhancing the alignment of text and visual features, but extended fine-tuning must be carefully controlled to avoid hallucination <Paper corpusId=\"267657819\" paperTitle=\"(Wo'zniak et al., 2024)\" isShortName></Paper> <Paper corpusId=\"262055661\" paperTitle=\"(Zhai et al., 2023)\" isShortName></Paper>.\n\n* **Safety-preserving fine-tuning**: Specialized techniques that explicitly maintain safety alignment during fine-tuning are emerging as research demonstrates that even fine-tuning with benign datasets can compromise the safety of seed LLMs <Paper corpusId=\"267769989\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n* **Continual learning approaches**: Methods that enable models to continuously learn new tasks without forgetting previous knowledge are being adapted for LLMs. These approaches are particularly important as LLMs are increasingly deployed in dynamic environments requiring ongoing adaptation <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Bhushan et al., 2025)", "paper": {"corpus_id": 276287820, "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Kushagra Bhushan", "authorId": "2256382094"}, {"name": "Yatin Nandwani", "authorId": "1392630568"}, {"name": "Dinesh Khandelwal", "authorId": "2345003462"}, {"name": "Sonam Gupta", "authorId": "2320314900"}, {"name": "Gaurav Pandey", "authorId": "2345005348"}, {"name": "Dinesh Raghu", "authorId": "1916865"}, {"name": "Sachindra Joshi", "authorId": "2243011716"}], "n_citations": 2}, "snippets": ["Catastrophic forgetting: Catastrophic Forgetting (French, 1999;Zheng et al., 2024) occurs when new domain-specific fine-tuning overwrites previously learned general knowledge, reducing performance on earlier tasks. Replay-based methods (de Masson d'Autume et al., 2019;Rolnick et al., 2019), help mitigate this by rehearsing prior task data during training. Recent advances in replay-based approaches for language models (Scialom et al., 2022;Mok et al., 2023) have shown promise in reducing catastrophic forgetting."], "score": 0.61474609375}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 267769989, "title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhaorui Yang", "authorId": "2284865968"}, {"name": "Qian Liu", "authorId": "1409707585"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Han Wang", "authorId": "2285032001"}, {"name": "H. Feng", "authorId": "46854712"}, {"name": "Minfeng Zhu", "authorId": "145314938"}, {"name": "Wei Chen", "authorId": "2256716159"}], "n_citations": 49}, "snippets": ["Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs.\n\nEchoing our findings, recent studies have highlighted that fine-tuning, even with benign datasets, can compromise the safety of seed LMs (Qi et al., 2024;Yang et al., 2023;Zhan et al., 2023;Pelrine et al., 2023). As evidenced, fine-tuning methods aimed at mitigating catastrophic forgetting are still absent.\n\nFor instance, vanilla fine-tuning on the OpenFunctions dataset (Patil et al., 2023) leads to a significant decrease in pass@1 on the HumanEval benchmark (Chen et al., 2021) from 13.4 to 9.8, constituting a decline of 27%. In contrast, SDFT not only mitigates this degradation, but also marginally enhances the accuracy to 15.2.\n\nOur findings indicate that the distribution shift during fine-tuning can lead to performance degradation in general task capabilities, as well as models' safety alignment and helpfulness."], "score": 0.75244140625}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 265158118, "title": "Vision-Language Instruction Tuning: A Review and Analysis", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Chen Li", "authorId": "2256784925"}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Dian Li", "authorId": "2243307404"}, {"name": "Ying Shan", "authorId": "2265579883"}], "n_citations": 11}, "snippets": ["Selective Forgetting. As pointed out by Zhai et al.., fine-tuning LLMs can easily cause them to forget their original capabilities (e.g., the classification ability of the original CLIP) or instructions, which is catastrophic. In fact, this phenomenon is prevalent in LLM, and abundant research has demonstrated that supervised fine-tuning in downstream tasks frequently causes overfitting of the LLM in the corresponding downstream tasks, leading to the loss of some or all of its original capabilities. Currently, existing studies generally address this issue by regulating the learning rate and minimising the overall loss, achieving certain outcomes.\n\nThe training data in instructional tuning serves as the learning object of the model. It is equipped with appropriate settings, such as the repetition of old samples and the balancing of task ratios, to mitigate the forgetting phenomenon to a certain extent. Nonetheless, further research is necessary to tackle this matter."], "score": 0.62451171875}, {"id": "(Thangarasa et al., 2024)", "paper": {"corpus_id": 273345395, "title": "Self-Data Distillation for Recovering Quality in Pruned Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vithursan Thangarasa", "authorId": "51153332"}, {"name": "Ganesh Venkatesh", "authorId": "2325876819"}, {"name": "Nish Sinnadurai", "authorId": "2325902410"}, {"name": "Sean Lie", "authorId": "2212029838"}], "n_citations": 2}, "snippets": ["One of the major challenges of pruning and distillation techniques in LLMs is catastrophic forgetting, where a model loses its previously learned capabilities during fine-tuning (Kotha et al., 2024;(Korbak et al., 2021)."], "score": 0.640625}, {"id": "(Korbak et al., 2021)", "paper": {"corpus_id": 249921347, "title": "Controlling Conditional Language Models without Catastrophic Forgetting", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tomasz Korbak", "authorId": "30023177"}, {"name": "Hady ElSahar", "authorId": "2218938"}, {"name": "Germ\u00e1n Kruszewski", "authorId": "2067996"}, {"name": "Marc Dymetman", "authorId": "2954698"}], "n_citations": 34}, "snippets": ["Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting."], "score": 0.0}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 276574830, "title": "Balancing Speciality and Versatility: A Coarse to Fine Framework for Mitigating Catastrophic Forgetting in Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Hengyuan Zhang", "authorId": "2290987204"}, {"name": "Yanru Wu", "authorId": "2297192655"}, {"name": "Dawei Li", "authorId": "2346989732"}, {"name": "Sak Yang", "authorId": "2346998771"}, {"name": "Rui Zhao", "authorId": "2287784354"}, {"name": "Yong Jiang", "authorId": "2290854859"}, {"name": "Fei Tan", "authorId": "2186741338"}], "n_citations": 0}, "snippets": ["Despite the versatility, aligned LLMs still fall short in certain tasks or domains, such as mathematics (Gou et al., 2023), finance (Li et al., 2023), and law (Cui et al., 2023a). To bolster performance in these particular tasks or domains, i.e., to gain speciality, a common practice is fine-tuning. However, during the fine-tuning process, the modification of model parameters often leads to catastrophic forgetting (CF), thereby causing a noticeable loss of versatility (Lin et al., 2023b). This loss adversely affects the performance of fine-tuned models across various real-world tasks (Cheng et al., 2023;Dong et al., 2023), propelling several works to investigate and contribute solutions to the CF in LLM versatility (Lin et al., 2023b;Wang et al., 2023c)."], "score": 0.66357421875}, {"id": "(Li et al._2, 2023)", "paper": {"corpus_id": 265294420, "title": "Large Language Models in Finance: A Survey", "year": 2023, "venue": "International Conference on AI in Finance", "authors": [{"name": "Yinheng Li", "authorId": "1527089960"}, {"name": "Shaofei Wang", "authorId": "2267859332"}, {"name": "Han Ding", "authorId": "2267679737"}, {"name": "Hang Chen", "authorId": "2267367945"}], "n_citations": 208}, "snippets": ["Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption. First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks. Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs. Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI."], "score": 0.0}, {"id": "(Wo'zniak et al., 2024)", "paper": {"corpus_id": 267657819, "title": "Personalized Large Language Models", "year": 2024, "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)", "authors": [{"name": "Stanislaw Wo'zniak", "authorId": "2284064954"}, {"name": "Bartlomiej Koptyra", "authorId": "2208962106"}, {"name": "Arkadiusz Janz", "authorId": "32559047"}, {"name": "Przemyslaw Kazienko", "authorId": "1724788"}, {"name": "Jan Koco'n", "authorId": "2284064003"}], "n_citations": 20}, "snippets": ["Fine-tuning can be computationally expensive and time-consuming, especially for large language models. Finetuning a language model on task-specific data can improve its performance on the task, but it may come at the cost of reduced performance on other tasks. This is due to the risk of catastrophic forgetting (French, 1999), where the model may forget some of the knowledge learned during pre-training and alignment processes [36]- (Zhai et al., 2023)."], "score": 0.62548828125}, {"id": "(Zhai et al., 2023)", "paper": {"corpus_id": 262055661, "title": "Investigating the Catastrophic Forgetting in Multimodal Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yuexiang Zhai", "authorId": "119692515"}, {"name": "Shengbang Tong", "authorId": "2143202419"}, {"name": "Xiao Li", "authorId": "2261479326"}, {"name": "Mu Cai", "authorId": "2053144019"}, {"name": "Qing Qu", "authorId": "144403436"}, {"name": "Yong Jae Lee", "authorId": "2244031889"}, {"name": "Y. Ma", "authorId": "144987142"}], "n_citations": 88}, "snippets": ["Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-stage fine-tuning on an image dataset improves performance across other image datasets, by enhancing the alignment of text and visual features. However, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in a significant loss of generalizability, even when the image encoder remains frozen. Our results suggest that MLLMs have yet to demonstrate performance on par with their vision models on standard image classification tasks and the current MLLM fine-tuning procedure still has room for improvement."], "score": 0.0}, {"id": "(Lobo et al., 2024)", "paper": {"corpus_id": 274234789, "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Elita Lobo", "authorId": "2332098876"}, {"name": "Chirag Agarwal", "authorId": "40228633"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 10}, "snippets": ["prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)", "Kalajdzievski (2024) and Liu et al. (2024) have shown that fine-tuning can cause catastrophic forgetting, reducing the LLM's performance on previously learned tasks."], "score": 0.890625}], "table": null}], "cost": 0.374382}}
