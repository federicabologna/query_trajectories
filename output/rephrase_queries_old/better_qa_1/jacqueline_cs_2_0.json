{"better_query": "How has classifier-free guidance (CFG) been adapted and applied in NLP-specific generative tasks such as conditional text generation or language modeling, and what benefits or trade-offs have been observed compared to traditional conditioning methods?", "better_answer": {"sections": [{"title": "Introduction and Background of CFG", "tldr": "Classifier-free guidance (CFG) emerged as an innovative technique to enhance conditional generation quality without requiring separate classifiers. It offers a powerful mechanism for balancing sample fidelity and diversity by combining conditional and unconditional generation during inference. (11 sources)", "text": "\nClassifier-free guidance (CFG), introduced by Ho in 2022, represents a significant advancement in the field of generative modeling, particularly for conditional generation tasks <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper>. CFG was developed as an alternative to classifier guidance, which was earlier proposed by Dhariwal et al. to improve sample quality in diffusion models <Paper corpusId=\"234357997\" paperTitle=\"(Dhariwal et al., 2021)\" isShortName></Paper>. While classifier guidance requires training a separate classifier model alongside the diffusion model, CFG eliminates this requirement by leveraging the generative model itself to provide guidance <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper> <Paper corpusId=\"257505012\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nThe core mechanism of CFG involves jointly training a model that can perform both conditional and unconditional generation <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>. During training, the conditioning information is randomly dropped out with some probability, effectively teaching the model to generate both with and without conditions <Paper corpusId=\"270869763\" paperTitle=\"(Fuest et al., 2024)\" isShortName></Paper>. This is typically implemented by using a null token or placeholder as the conditioning signal for the unconditional case <Paper corpusId=\"257505012\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. During inference, CFG employs a linear combination of the conditional and unconditional predictions to guide the generation process <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This combination can be represented as:\n\n\u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)]\n\nwhere \u03f5(x, t; c) is the conditional noise prediction, \u03f5(x, t) is the unconditional prediction, and w is the guidance weight that controls the balance between sample fidelity and diversity <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>.\n\nThe guidance weight w serves as a critical hyperparameter that allows for explicit control over how strongly the conditional information influences the generation <Paper corpusId=\"273186941\" paperTitle=\"(Kasymov et al., 2024)\" isShortName></Paper>. When w = 1, the process is equivalent to standard conditional generation, while values of w > 1 amplify the conditional prediction, pushing the model to produce outputs that more closely match the conditioning information <Paper corpusId=\"273186941\" paperTitle=\"(Kasymov et al., 2024)\" isShortName></Paper>. Intuitively, this approach decreases the unconditional likelihood while increasing the conditional likelihood, thereby enhancing alignment between the generated output and the provided condition <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>.\n\nCFG gained significant popularity after its successful application in text-to-image generation systems like GLIDE (Guided Language to Image Diffusion for Generation and Editing), where it demonstrated superior performance compared to other guidance techniques <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper> <Paper corpusId=\"233168627\" paperTitle=\"(Franceschelli et al., 2021)\" isShortName></Paper>. Human evaluators preferred images generated with classifier-free guidance for both photorealism and caption similarity <Paper corpusId=\"245335086\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>. Since then, CFG has become a staple technique in various conditional generation tasks, including image generation with Stable Diffusion, video generation, and increasingly in NLP applications <Paper corpusId=\"276421312\" paperTitle=\"(Tang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276961040\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nDespite its widespread adoption, CFG does come with certain limitations. It increases the training computational budget, as the unconditional task can consume up to 20% of the resources <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>. Additionally, while CFG enhances alignment with conditions, this improvement often comes at the cost of reduced sample diversity <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.96435546875}, {"id": "(Dhariwal et al., 2021)", "paper": {"corpus_id": 234357997, "title": "Diffusion Models Beat GANs on Image Synthesis", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Alex Nichol", "authorId": "38967461"}], "n_citations": 7951}, "snippets": ["We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128$\\times$128, 4.59 on ImageNet 256$\\times$256, and 7.72 on ImageNet 512$\\times$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256$\\times$256 and 3.85 on ImageNet 512$\\times$512. We release our code at https://github.com/openai/guided-diffusion"], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 257505012, "title": "Text-to-image Diffusion Models in Generative AI: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Chenshuang Zhang", "authorId": "48934876"}, {"name": "Chaoning Zhang", "authorId": "31044159"}, {"name": "Mengchun Zhang", "authorId": "50495602"}, {"name": "In-So Kweon", "authorId": "145017151"}], "n_citations": 280}, "snippets": ["Classifier-free guidance. Different from classifierguided diffusion model [41] that exploits an additional classfier, it is found in [42] that the guidance can be obtained by the generative model itself without a classifier, termed as classifier-free guidance. Specifically, classifier-free guidance jointly trains a single model with the unconditional score estimator \u03b8 (x) and the conditional \u03b8 (x, c), where c denotes the class label. A null token \u2205 is placed as the class label in the unconditional part, i.e., \u03b8 (x) = \u03b8 (x, \u2205). Experimental results in [42] show that classifier-free guidance achieves a trade-off between quality and diversity similar to that achieved by classifier guidance. Without resorting to a classifier, classifier-free diffusion facilitates more modalities, e.g., text in text-to-image, as guidance."], "score": 0.9716796875}, {"id": "(Franceschelli et al., 2021)", "paper": {"corpus_id": 233168627, "title": "Creativity and Machine Learning: A Survey", "year": 2021, "venue": "ACM Computing Surveys", "authors": [{"name": "Giorgio Franceschelli", "authorId": "2067291198"}, {"name": "Mirco Musolesi", "authorId": "1806767"}], "n_citations": 42}, "snippets": ["A possibility is to use classifier guidance (Dhariwal et al., 2021): the diffusion score (i.e., the added noise) includes the gradient of the log-likelihood of an auxiliary classifier model. An alternative is classifier-free guidance (Ho, 2022): to avoid learning an additional model, a single neural network is used to parameterize two diffusion models, one conditional and one unconditional; the two models are then jointly trained by randomly setting the class for the unconditional model. \n\nFinally, the sampling is performed using a linear combination of conditional and unconditional score estimates. Guided Language to Image Diffusion for Generation and Editing (GLIDE) (Nichol et al., 2021) demonstrates how classifier-free guidance can be effectively used to generate text-conditional images."], "score": 0.9443359375}, {"id": "(Fuest et al., 2024)", "paper": {"corpus_id": 270869763, "title": "Diffusion Models and Representation Learning: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Michael Fuest", "authorId": "2309173490"}, {"name": "Pingchuan Ma", "authorId": "2273647740"}, {"name": "Ming Gui", "authorId": "2309172432"}, {"name": "Johannes S. Fischer", "authorId": "2273655571"}, {"name": "Vincent Tao Hu", "authorId": "2292259521"}, {"name": "Bjorn Ommer", "authorId": "2257038709"}], "n_citations": 24}, "snippets": ["To address this limitation, Classifier-free guidance (CFG) (Ho, 2022) eliminates the need for a pre-trained classifier.CFG works by training an unconditional diffusion model parametrized by \u03f5 \u03b8 (x t , t, \u03d5) together with a conditional model parametrized by \u03f5 \u03b8 (x t , t, c).For the unconditional model, a null input token \u03d5 is used as a conditioning signal c.The network is trained by randomly dropping out the conditioning signal with probability p uncond .Sampling is then performed using a weighted combination of conditional and unconditional score estimates:\n\nThis sampling method does not rely on the gradients of a pre-trained classifier but still requires an annotated dataset to train the conditional denoising network.Fully unconditional approaches have yet to match classifier-free guidance, though recent works using diffusion model representations for self-supervised guidance show promise [73]100].These methods do not need annotated data, allowing the use of larger unlabelled datasets."], "score": 0.94580078125}, {"id": "(Yu et al., 2022)", "paper": {"corpus_id": 249926846, "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jiahui Yu", "authorId": "2338016295"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Thang Luong", "authorId": "1821711"}, {"name": "Gunjan Baid", "authorId": "1396954703"}, {"name": "Zirui Wang", "authorId": "2331539"}, {"name": "Vijay Vasudevan", "authorId": "2053781980"}, {"name": "Alexander Ku", "authorId": "31702389"}, {"name": "Yinfei Yang", "authorId": "2118771180"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Wei Han", "authorId": "143911112"}, {"name": "Zarana Parekh", "authorId": "27456119"}, {"name": "Xin Li", "authorId": "2158973314"}, {"name": "Han Zhang", "authorId": null}, {"name": "Jason Baldridge", "authorId": "1387994164"}, {"name": "Yonghui Wu", "authorId": "48607963"}], "n_citations": 1133}, "snippets": ["Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."], "score": 0.96484375}, {"id": "(Kaiser et al., 2024)", "paper": {"corpus_id": 274117064, "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tim Kaiser", "authorId": "2267395983"}, {"name": "Nikolas Adaloglou", "authorId": "1832165240"}, {"name": "M. Kollmann", "authorId": "2065390431"}], "n_citations": 1}, "snippets": ["The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]."], "score": 0.9482421875}, {"id": "(Kasymov et al., 2024)", "paper": {"corpus_id": 273186941, "title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "A. Kasymov", "authorId": "145172609"}, {"name": "Marcin Sendera", "authorId": "46220915"}, {"name": "Michal Stypulkowski", "authorId": "1379958688"}, {"name": "Maciej Zieba", "authorId": "2310705964"}, {"name": "P. Spurek", "authorId": "1790922"}], "n_citations": 1}, "snippets": ["Classifier-Free Guidance offers a simpler and more robust alternative by eliminating the need for an external classifier. Instead, the diffusion model itself is trained in two modes -conditional and unconditional: \n\n\u2022 conditional mode -the model is trained to predict the denoised data x 0 given noisy data x t and conditioning information y, learning the conditional distribution p \u03b8 (x t\u22121 |x t , y); \u2022 unconditional mode -the same model is also trained without any conditioning, learning the unconditional distribution p \u03b8 (x t\u22121 |x t ). \n\nDuring inference, CFG uses a combination of the conditional and unconditional predictions to guide the generation (see Algorithm 1). Specifically, for a given noisy sample x t , the guidance is achieved by interpolating between the conditional and unconditional predictions as follows: \n\nwhere \u03f5 \u03b8 (x t , \u00f8) is the model's prediction of the noise in x t when no conditioning is provided (unconditional), \u03f5 \u03b8 (x t , y) is the prediction of the noise in x t when conditioned on y, and w is the guidance scale, which controls how strongly the conditional information influences the generation. \n\nBy adjusting w, one can control the balance between sample diversity and adherence to the conditioning y. When w = 1, the process is equivalent to standard conditional generation. When w > 1, the conditional prediction is amplified, guiding the model to produce samples that more closely match the conditioning information, potentially at the cost of diversity."], "score": 0.958984375}, {"id": "(Nichol et al., 2021)", "paper": {"corpus_id": 245335086, "title": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alex Nichol", "authorId": "38967461"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Bob McGrew", "authorId": "39593364"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Mark Chen", "authorId": "2108828435"}], "n_citations": 3629}, "snippets": ["Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im."], "score": 0.0}, {"id": "(Tang et al., 2025)", "paper": {"corpus_id": 276421312, "title": "Diffusion Models without Classifier-free Guidance", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhicong Tang", "authorId": "1387822270"}, {"name": "Jianmin Bao", "authorId": "9324504"}, {"name": "Dongdong Chen", "authorId": "2265723915"}, {"name": "Baining Guo", "authorId": "2238211403"}], "n_citations": 5}, "snippets": ["Classifier-free guidance (CFG) (Ho, 2022)) is a widely adopted technique in conditional diffusion models to enhance generation performance and alignment to conditions. It provides an explicit control of the focus on conditioning variables and avoids to sample within the \"low temperature\" regions with low quality", "CFG has become an widely adopted protocol in most of diffusion models for tasks, such as image generation and video generation."], "score": 0.953125}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276961040, "title": "Studying Classifier(-Free) Guidance From a Classifier-Centric Perspective", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoming Zhao", "authorId": "2144306665"}, {"name": "Alexander G. Schwing", "authorId": "2281750850"}], "n_citations": 1}, "snippets": ["Classifier-free guidance has become a staple for conditional generation with denoising diffusion models. However, a comprehensive understanding of classifier-free guidance is still missing. In this work, we carry out an empirical study to provide a fresh perspective on classifier-free guidance", "For instance, classifier-free guidance, especially at sufficient scale, has been found to be critical for high-quality textto-image (Rombach et al., 2021) and text-to-3D (Poole et al., 2022) generation. Despite its popularity, we think a solid understanding of classifier-free guidance is missing", "It is classifier guidance that decomposes the conditional generation into a combination of an unconditional generation and a classifier prediction. Classifier-free guidance directly mimics this decomposition, replacing the classifier by randomly dropping conditioning information during training (Ho, 2022)."], "score": 0.9306640625}], "table": null}, {"title": "Adaptations of CFG for NLP Tasks", "tldr": "Classifier-free guidance has been successfully adapted from image generation to various NLP tasks by leveraging the autoregressive nature of language models and manipulating token prediction probabilities. These adaptations have enabled more controllable text generation while maintaining the core CFG principle of balancing conditional and unconditional outputs. (14 sources)", "text": "\nThe adaptation of classifier-free guidance (CFG) to natural language processing tasks represents a significant extension of the technique beyond its original image generation domain. Sanchez et al. demonstrated that CFG can be broadly applied as an inference-time technique in pure language modeling, showing improvements across a variety of tasks including question answering, reasoning, code generation, and machine translation <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>. Their work established that CFG could bring performance improvements equivalent to using a model with twice the parameter count, suggesting a cost-effective alternative to scaling model size <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\nFor autoregressive language models, which inherently excel at unconditional generation, CFG represents a natural evolution of existing techniques <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper> <Paper corpusId=\"44104089\" paperTitle=\"(Sajjadi et al., 2018)\" isShortName></Paper>. The adaptation of CFG to language models involves manipulating the generation of subsequent tokens to emphasize conditioning on the prompt, building upon established frameworks of logit guidance and log-probability adjustment <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>.\n\nThe mathematical formulation for applying CFG to language models has been clearly established. Given a sequence of tokens, the likelihood of predicting the entire sequence can be expressed using conditional probabilities, with the CFG sampling denoted as a weighted combination of conditional and unconditional predictions <Paper corpusId=\"266053531\" paperTitle=\"(Zhang et al._1, 2023)\" isShortName></Paper>. The guided logits for token sampling can be computed as:\n\n```\nlogits_guided = logits_uncond + w(logits_cond - logits_uncond)\n```\n\nwhere `w` is the guidance scale that controls the strength of the conditioning <Paper corpusId=\"266162752\" paperTitle=\"(Mizrahi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247628171\" paperTitle=\"(Gafni et al., 2022)\" isShortName></Paper> <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper>. This formulation has been successfully applied not only to text generation but also to multimodal tasks where text conditions other modalities <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nAn interesting extension of CFG for NLP includes negative prompting, which guides generation away from specific aspects by adjusting the guidance parameter to be negative <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>. This technique exploits the latent semantic information in token predictions to provide finer control over the generated content <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper>.\n\nRecent innovations have also introduced unsupervised CFG, extending the technique to scenarios where paired conditional data might not be readily available <Paper corpusId=\"273549320\" paperTitle=\"(Nie et al., 2024)\" isShortName></Paper>. This adaptation involves introducing a mask sequence as a dummy variable that translates the unconditional distribution to a conditional format without adding new information <Paper corpusId=\"273549320\" paperTitle=\"(Nie et al., 2024)\" isShortName></Paper>.\n\nThe application of CFG has expanded beyond pure text generation to include agent training <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>, text-to-video generation <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>, and video-to-audio synthesis <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. In the context of agent training, the guided policy computes a weighted combination of conditional and unconditional action predictions, with experimental evidence showing consistent performance improvements when classifier-free guidance is enabled <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>.\n\nDespite its growing adoption across various NLP tasks, some researchers have noted that certain applications, such as text generation with diffusion models, have primarily used classifiers for guidance rather than the classifier-free approach <Paper corpusId=\"277043967\" paperTitle=\"(Buzzard, 2025)\" isShortName></Paper>. This suggests that there may still be untapped potential for applying CFG more broadly in specific NLP domains.", "citations": [{"id": "(Sanchez et al., 2023)", "paper": {"corpus_id": 259308807, "title": "Stay on topic with Classifier-Free Guidance", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Guillaume Sanchez", "authorId": "2056723344"}, {"name": "Honglu Fan", "authorId": "2072838294"}, {"name": "Alexander Spangher", "authorId": "51444076"}, {"name": "Elad Levi", "authorId": "34490455"}, {"name": "Pawan Sasanka Ammanamanchi", "authorId": "1451644426"}, {"name": "Stella Biderman", "authorId": "103476203"}], "n_citations": 55}, "snippets": ["Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline."], "score": 0.9560546875}, {"id": "(O'Neill et al., 2023)", "paper": {"corpus_id": 260899968, "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "C. O'Neill", "authorId": "2072014290"}, {"name": "Y. Ting", "authorId": "93622633"}, {"name": "I. Ciuc\u0103", "authorId": "50062876"}, {"name": "Jack William Miller", "authorId": "2229023715"}, {"name": "Thang Bui", "authorId": "2229238231"}], "n_citations": 1}, "snippets": ["In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022)."], "score": 0.95068359375}, {"id": "(Sajjadi et al., 2018)", "paper": {"corpus_id": 44104089, "title": "Assessing Generative Models via Precision and Recall", "year": 2018, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mehdi S. M. Sajjadi", "authorId": "2283034"}, {"name": "Olivier Bachem", "authorId": "1936951"}, {"name": "Mario Lucic", "authorId": "34302129"}, {"name": "O. Bousquet", "authorId": "1698617"}, {"name": "S. Gelly", "authorId": "1802148"}], "n_citations": 583}, "snippets": ["Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."], "score": 0.0}, {"id": "(Zhang et al._1, 2023)", "paper": {"corpus_id": 266053531, "title": "Prompt Highlighter: Interactive Control for Multi-Modal LLMs", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Yuechen Zhang", "authorId": "2145915052"}, {"name": "Shengju Qian", "authorId": "152230789"}, {"name": "Bohao Peng", "authorId": "2272493196"}, {"name": "Shu Liu", "authorId": "25059098"}, {"name": "Jiaya Jia", "authorId": "2273012826"}], "n_citations": 24}, "snippets": ["Applying the Bayes rule, P \u0398 (c|x) \u221d P \u0398 (x|c)/P \u0398 (x), the sampling process of the Classifier-Free Guidance (CFG) can be expressed as \n\nin which \u03f5 t is the noise prediction conditioned on the previous output x t+1 and the text condition c. LLM-CFG [21] extended this property to autoregressive language models. Given a sequence of N tokens x = {x 1 , . . . , x N }, the likelihood of predicting the entire sequence can be expressed as \n\nThe model samples each subsequent token from the conditional probability distribution. Based on Eq. ( 1), the CFG sampling on the language model can be denoted as \n\nSimilar to the transaction from Eq. (1) to Eq. ( 2), the likelihood in LLM is represented as the next-token classification probability. Thus next token's logit prediction \n\nThe formulation in Eqs. (3) and (4) offers a paradigm for controllable generation in LLMs [21], with the guidance strength \u03b3 controls the degree of generation focus. Notably, the effectiveness of this guidance depends on the careful design of the conditional prompt c, which should be naturally formed as a complete phrase or sentence to retain its semantic meaning."], "score": 0.9541015625}, {"id": "(Mizrahi et al., 2023)", "paper": {"corpus_id": 266162752, "title": "4M: Massively Multimodal Masked Modeling", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "David Mizrahi", "authorId": "2111623708"}, {"name": "Roman Bachmann", "authorId": "153825349"}, {"name": "Ouguzhan Fatih Kar", "authorId": "2273474116"}, {"name": "Teresa Yeo", "authorId": "143895090"}, {"name": "Mingfei Gao", "authorId": "2273661239"}, {"name": "Afshin Dehghan", "authorId": "2273361790"}, {"name": "Amir Zamir", "authorId": "40029556"}], "n_citations": 74}, "snippets": ["Classifier-free guidance is crucial for improving both image fidelity and how well the generation matches the conditioning. It is most commonly used in diffusion models [50], but can be applied in token-based models as well (Gafni et al., 2022)(Yu et al., 2022)(Chang et al., 2023). We perform classifier-free guidance by computing a weighted combinations of the logits of a forward pass with the conditioning and one without the conditioning: logits guided = logits uncond + w (logits cond \u2212 logits uncond ) . \n\nHere, w is the guidance scale. When performing chained generation, we add each fully generated modality to the set of guided modalities."], "score": 0.93798828125}, {"id": "(Gafni et al., 2022)", "paper": {"corpus_id": 247628171, "title": "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Oran Gafni", "authorId": "90840812"}, {"name": "Adam Polyak", "authorId": "33964593"}, {"name": "Oron Ashual", "authorId": "1388005058"}, {"name": "Shelly Sheynin", "authorId": "2086827528"}, {"name": "Devi Parikh", "authorId": "153432684"}, {"name": "Yaniv Taigman", "authorId": "2188620"}], "n_citations": 524}, "snippets": ["Recent text-to-image generation methods provide a simple yet exciting conversion capability between text and image domains. While these methods have incrementally improved the generated image fidelity and text relevancy, several pivotal gaps remain unanswered, limiting applicability and quality. We propose a novel text-to-image method that addresses these gaps by (i) enabling a simple control mechanism complementary to text in the form of a scene, (ii) introducing elements that substantially improve the tokenization process by employing domain-specific knowledge over key image regions (faces and salient objects), and (iii) adapting classifier-free guidance for the transformer use case. Our model achieves state-of-the-art FID and human evaluation results, unlocking the ability to generate high fidelity images in a resolution of 512x512 pixels, significantly improving visual quality. Through scene controllability, we introduce several new capabilities: (i) Scene editing, (ii) text editing with anchor scenes, (iii) overcoming out-of-distribution text prompts, and (iv) story illustration generation, as demonstrated in the story we wrote."], "score": 0.0}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 255372955, "title": "Muse: Text-To-Image Generation via Masked Generative Transformers", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Huiwen Chang", "authorId": "2914394"}, {"name": "Han Zhang", "authorId": "2146204239"}, {"name": "Jarred Barber", "authorId": "152630175"}, {"name": "AJ Maschinot", "authorId": "2199119286"}, {"name": "Jos\u00e9 Lezama", "authorId": "143923528"}, {"name": "Lu Jiang", "authorId": "39978626"}, {"name": "Ming Yang", "authorId": "152790163"}, {"name": "K. Murphy", "authorId": "1702318"}, {"name": "W. Freeman", "authorId": "1768236"}, {"name": "Michael Rubinstein", "authorId": "144544291"}, {"name": "Yuanzhen Li", "authorId": "2167749913"}, {"name": "Dilip Krishnan", "authorId": "1707347"}], "n_citations": 556}, "snippets": ["We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."], "score": 0.94384765625}, {"id": "(Yu et al., 2022)", "paper": {"corpus_id": 249926846, "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jiahui Yu", "authorId": "2338016295"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Thang Luong", "authorId": "1821711"}, {"name": "Gunjan Baid", "authorId": "1396954703"}, {"name": "Zirui Wang", "authorId": "2331539"}, {"name": "Vijay Vasudevan", "authorId": "2053781980"}, {"name": "Alexander Ku", "authorId": "31702389"}, {"name": "Yinfei Yang", "authorId": "2118771180"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Wei Han", "authorId": "143911112"}, {"name": "Zarana Parekh", "authorId": "27456119"}, {"name": "Xin Li", "authorId": "2158973314"}, {"name": "Han Zhang", "authorId": null}, {"name": "Jason Baldridge", "authorId": "1387994164"}, {"name": "Yonghui Wu", "authorId": "48607963"}], "n_citations": 1133}, "snippets": ["Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."], "score": 0.96484375}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 277043912, "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziqin Zhou", "authorId": "2282670286"}, {"name": "Yifan Yang", "authorId": "2331570564"}, {"name": "Yuqing Yang", "authorId": "2125051198"}, {"name": "Tianyu He", "authorId": "2350871281"}, {"name": "Houwen Peng", "authorId": "2350821834"}, {"name": "Kai Qiu", "authorId": "2268758860"}, {"name": "Qi Dai", "authorId": "2329560121"}, {"name": "Lili Qiu", "authorId": "2160727304"}, {"name": "Chong Luo", "authorId": "2294680622"}, {"name": "Lingqiao Liu", "authorId": "2320820962"}], "n_citations": 1}, "snippets": ["Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."], "score": 0.97607421875}, {"id": "(Nie et al., 2024)", "paper": {"corpus_id": 273549320, "title": "Scaling up Masked Diffusion Models on Text", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Shen Nie", "authorId": "2191077545"}, {"name": "Fengqi Zhu", "authorId": "2305318534"}, {"name": "Chao Du", "authorId": "2325201427"}, {"name": "Tianyu Pang", "authorId": "19201674"}, {"name": "Qian Liu", "authorId": "2284062049"}, {"name": "Guangtao Zeng", "authorId": "2309181383"}, {"name": "Min Lin", "authorId": "2253977831"}, {"name": "Chongxuan Li", "authorId": "2253823025"}], "n_citations": 30}, "snippets": ["CFG (Ho & Salimans, 2022) is an effective and versatile technique widely used in both continuous and discrete diffusion models, with applications spanning image (Ho & Salimans, 2022;Chang et al., 2023) and text generation (Lovelace et al., 2024). Rooted in Bayes' rule, CFG simultaneously trains a conditional and an unconditional diffusion model, introducing a rescaled distribution for inference. Specifically, at a given timestep t \u2208 [0] 1], CFG (Chang et al., 2023) is defined as: \n\nwhere c is the condition, w is a hyperparameter that flexibly controls the strength of c, and p \u03b8 (x 0 |c, x t ) and p \u03b8 (x 0 |x t ) are the conditional and unconditional models respectively. \n\nNotably, it seems that the conditional model must be trained on paired data before applying CFG. Consequently, to the best of our knowledge, all existing work (Ho & Salimans, 2022;Chang et al., 2023;Lovelace et al., 2024) fall into supervised settings, where paired data are readily available. \n\nUnsupervised CFG. We extend CFG to an unsupervised setting by introducing a new formulation: \n\nwhere m is a mask sequence of the same length as c. Compared to Eq. ( 6), the dummy variable m translates the unconditional distribution to a conditional format without adding new information. For simplicity, we continue to refer to p \u03b8 (x 0 |m, x t ) as the unconditional distribution in unsupervised CFG throughout this paper."], "score": 0.9384765625}, {"id": "(Zolna et al., 2024)", "paper": {"corpus_id": 267027965, "title": "GATS: Gather-Attend-Scatter", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "K. Zolna", "authorId": "2065684527"}, {"name": "Serkan Cabi", "authorId": "2066047403"}, {"name": "Yutian Chen", "authorId": "2279763460"}, {"name": "Eric Lau", "authorId": "2279718795"}, {"name": "Claudio Fantacci", "authorId": "2080928701"}, {"name": "J. Pa\u0161ukonis", "authorId": "31143488"}, {"name": "Jost Tobias Springenberg", "authorId": "2060551"}, {"name": "Sergio G\u00f3mez Colmenarejo", "authorId": "2279752493"}], "n_citations": 1}, "snippets": ["Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4)."], "score": 0.93798828125}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 278171703, "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance", "year": 2025, "venue": "", "authors": [{"name": "Yuzhe Liang", "authorId": "2278618341"}, {"name": "Wenzhe Liu", "authorId": "2358111037"}, {"name": "Chunyu Qiang", "authorId": "2358041541"}, {"name": "Zhikang Niu", "authorId": "2229877177"}, {"name": "Yushen Chen", "authorId": "2324996330"}, {"name": "Ziyang Ma", "authorId": "2116609277"}, {"name": "Wenxi Chen", "authorId": "2278584538"}, {"name": "Nan Li", "authorId": "2358116915"}, {"name": "Chen Zhang", "authorId": "2358098456"}, {"name": "Xie Chen", "authorId": "2321881822"}], "n_citations": 0}, "snippets": ["Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."], "score": 0.97607421875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270220558, "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yongqi Wang", "authorId": "2292208450"}, {"name": "Wenxiang Guo", "authorId": "2304739202"}, {"name": "Rongjie Huang", "authorId": "2048021099"}, {"name": "Jia-Bin Huang", "authorId": "3068086"}, {"name": "Zehan Wang", "authorId": "2258561621"}, {"name": "Fuming You", "authorId": "2292197957"}, {"name": "Ruiqi Li", "authorId": "2181010470"}, {"name": "Zhou Zhao", "authorId": "2304453961"}], "n_citations": 13}, "snippets": ["Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."], "score": 0.0}, {"id": "(Buzzard, 2025)", "paper": {"corpus_id": 277043967, "title": "Understanding the Quality-Diversity Trade-off in Diffusion Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zak Buzzard", "authorId": "2332536645"}], "n_citations": 0}, "snippets": ["While classifiers have been used to guide the diffusion process in text generation [14], to the best of my knowledge classifierfree guidance has not yet been explored."], "score": 0.94287109375}], "table": null}, {"title": "Implementation Techniques in NLP", "tldr": "Implementing classifier-free guidance in NLP requires specific training and inference strategies, such as condition dropout during training and specialized token prediction formulations during inference. Different applications have developed variations like multimodal CFG, independent condition guidance, and adaptive guidance scales to address modality-specific requirements. (8 sources)", "text": "\nThe implementation of classifier-free guidance (CFG) in NLP applications follows several established techniques that have been adapted from image generation while accommodating the unique characteristics of language models. A fundamental training technique involves randomly discarding or masking conditioning information for a portion of training samples, typically around 10%, to enable the model to learn both conditional and unconditional generation capabilities <Paper corpusId=\"266149498\" paperTitle=\"(Patel et al., 2023)\" isShortName></Paper> <Paper corpusId=\"278394371\" paperTitle=\"(Pfaff et al., 2025)\" isShortName></Paper>. This approach creates a joint model that can perform both tasks without requiring separate training of an unconditional model.\n\nDuring inference, the core implementation of CFG involves a weighted combination of conditional and unconditional predictions. The mathematical formulation typically follows:\n\n```\n\u03f5_guided = \u03f5_uncond + \u03bb(\u03f5_cond - \u03f5_uncond)\n```\n\nwhere \u03bb (lambda) represents the guidance scale that controls the strength of conditioning <Paper corpusId=\"271957385\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This formulation allows for flexible control over the balance between fidelity to the conditioning information and generation diversity.\n\nSeveral innovative variations of CFG implementation have emerged for NLP tasks. Chang et al. introduced a dynamic guidance approach where the scale increases linearly throughout the sampling process, allowing early tokens to be sampled more freely while later tokens are more strongly influenced by the conditioning prompt <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This dynamic scaling helps mitigate the diversity loss that typically accompanies high guidance values.\n\nFor multimodal applications involving text and other modalities such as speech or video, researchers have developed modality-specific guidance scales. Choi et al. proposed multimodal classifier-free guidance where different guidance scales are applied to different modalities:\n\n```\n\u03f5_guided = \u03f5_uncond + s_t(\u03f5_t_cond - \u03f5_uncond) + s_v(\u03f5_v_cond - \u03f5_uncond)\n```\n\nwhere s_t and s_v are modality-specific guidance scales for text and video, respectively <Paper corpusId=\"278171688\" paperTitle=\"(Choi et al., 2025)\" isShortName></Paper>. This approach enables finer control over the influence of each modality, allowing developers to emphasize text intelligibility or other modal characteristics as needed.\n\nAnother significant implementation advancement is Independent Condition Guidance (ICG), which eliminates the need for separate training of an unconditional model. Sadat et al. demonstrated that by using a conditioning vector independent of the input data, the conditional score function can become equivalent to the unconditional score <Paper corpusId=\"270923987\" paperTitle=\"(Sadat et al., 2024)\" isShortName></Paper>. This insight reduces training complexity while maintaining the benefits of CFG.\n\nIn decision-making and planning applications, CFG has been implemented by training diffusion models using conditions based on discounted returns. During inference, normalized return values can be used as the target condition for CFG sampling <Paper corpusId=\"276741986\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"254044710\" paperTitle=\"(Ajay et al., 2022)\" isShortName></Paper>. However, researchers have noted that using fixed target values may lead to unstable plans, suggesting that adaptive approaches may be necessary for these applications.\n\nFor supporting more sophisticated control over generation, some implementations extend CFG to support negative prompting. This technique replaces the unconditional logit with a logit conditioned on a \"negative prompt,\" encouraging the model to generate content with features associated with the positive prompt while avoiding features associated with the negative prompt <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This capability is particularly valuable in NLP applications where controlling tone, style, or content boundaries is important.", "citations": [{"id": "(Patel et al., 2023)", "paper": {"corpus_id": 266149498, "title": "ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Maitreya Patel", "authorId": "81331041"}, {"name": "C. Kim", "authorId": "2116705496"}, {"name": "Sheng Cheng", "authorId": "2116727866"}, {"name": "Chitta Baral", "authorId": "2064619864"}, {"name": "Yezhou Yang", "authorId": "1784500"}], "n_citations": 19}, "snippets": ["During training, conditions are omitted 10% of the time to foster unconditional generation, subsequently improving test performance as CFG works as implicit classifier guidance [11]."], "score": 0.9248046875}, {"id": "(Pfaff et al., 2025)", "paper": {"corpus_id": 278394371, "title": "Steerable Scene Generation with Post Training and Inference-Time Search", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nicholas Pfaff", "authorId": "2348265706"}, {"name": "Hongkai Dai", "authorId": "1758275"}, {"name": "Sergey Zakharov", "authorId": "2331626375"}, {"name": "Shun Iwase", "authorId": "2359630378"}, {"name": "Russ Tedrake", "authorId": "2263905014"}], "n_citations": 0}, "snippets": ["To enable both conditional and unconditional generation within a single model, we randomly mask the conditioning input with 10% probability during training. This allows classifier-free guidance (CFG) [46] at inference time, where predictions are computed using a weighted combination of conditional and unconditional outputs: \n\nwhere w is the guidance weight, and xcond and xuncond are the model predictions under conditional and unconditional contexts, respectively. A weight of w = \u22121 corresponds to unconditional sampling, w = 0 yields conditional sampling without guidance, and w > 0 applies classifier-free guidance during sampling. We apply CFG to both discrete and continuous components."], "score": 0.966796875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 271957385, "title": "SimpleSpeech 2: Towards Simple and Efficient Text-to-Speech with Flow-based Scalar Latent Transformer Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dongchao Yang", "authorId": "2238138795"}, {"name": "Rongjie Huang", "authorId": "2306974316"}, {"name": "Yuanyuan Wang", "authorId": "2307148701"}, {"name": "Haohan Guo", "authorId": "66855276"}, {"name": "Dading Chong", "authorId": "2316948137"}, {"name": "Songxiang Liu", "authorId": "51263928"}, {"name": "Xixin Wu", "authorId": "2107999711"}, {"name": "Helen M. Meng", "authorId": "2273659859"}], "n_citations": 15}, "snippets": ["Classifier-free guidance (CFG) [24] has been demonstrated as an effective way to enhance the generation quality in both image and audio domains [35], [53]. CFG can be formulated as: \n\nwhere \u03bb denotes the guidance scale. CFC tries to model conditional distribution p(x|y) and unconditional distribution p(x) in the training. In the inference stage, \u03bb = 1 denotes that we do not use classifier-free guidance, when \u03bb > 1 the model decreases the unconditional likelihood of the sample while increasing the conditional likelihood. In other words, classifierfree guidance conducts this by decreasing the unconditional likelihood with a negative score term. During the training stage, previous works try to mask the condition information of some samples (e.g. set the training sample's text as empty with 10% probability), so that these samples can be used to optimize unconditional distribution p(x).\n\nMany prior studies [10], [22] demonstrate that text transcriptions generated by an ASR system can be utilized to train a TTS system. Inevitably, the text transcription from the ASR system is not flawless, referred to as noisy or weak labels. Despite this, existing literature does not thoroughly explain why TTS systems can be effectively trained using such dataset. In this study, we show that including a small part of the noisy label in the training set is equivalent to introducing the CFC training strategy, thus we do not need to deliberately construct masked samples during training stage."], "score": 0.927734375}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 255372955, "title": "Muse: Text-To-Image Generation via Masked Generative Transformers", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Huiwen Chang", "authorId": "2914394"}, {"name": "Han Zhang", "authorId": "2146204239"}, {"name": "Jarred Barber", "authorId": "152630175"}, {"name": "AJ Maschinot", "authorId": "2199119286"}, {"name": "Jos\u00e9 Lezama", "authorId": "143923528"}, {"name": "Lu Jiang", "authorId": "39978626"}, {"name": "Ming Yang", "authorId": "152790163"}, {"name": "K. Murphy", "authorId": "1702318"}, {"name": "W. Freeman", "authorId": "1768236"}, {"name": "Michael Rubinstein", "authorId": "144544291"}, {"name": "Yuanzhen Li", "authorId": "2167749913"}, {"name": "Dilip Krishnan", "authorId": "1707347"}], "n_citations": 556}, "snippets": ["We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."], "score": 0.94384765625}, {"id": "(Choi et al., 2025)", "paper": {"corpus_id": 278171688, "title": "AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jeongsoo Choi", "authorId": "2327997129"}, {"name": "Ji-Hoon Kim", "authorId": "2292215700"}, {"name": "Kim Sung-Bin", "authorId": "1395322099"}, {"name": "Tae-Hyun Oh", "authorId": "2243191148"}, {"name": "Joon Son Chung", "authorId": "2264317306"}], "n_citations": 0}, "snippets": ["In diffusion-based generative models, classifier-free guidance (Ho, 2022) is well-explored to strengthen the influence of conditioning signals during inference. This is achieved by using both conditional and unconditional predictions from the same model to guide the generation process as follows: \n\nwhere s is guidance scale. Since each modality exhibits different characteristics, we hypothesize that using a single guidance scale for all modalities may be sub-optimal. To allow better control over each modality during inference, we propose multimodal classifier-free guidance by assigning modalityspecific guidance scales: \n\nwhere s t is guidance scale for text modality and s v for remained modality, namely video. By adjusting s t and s v , we can adaptively control the focus between modalities. Higher s t encourages the model to follow the text more closely, improving intelligibility, while higher s v leads to better lip synchronizations. \n\nTo support CFG, we apply modality dropout during training by randomly dropping text, video, or both. This not only enables multimodal CFG but also improves robustness in cases where a modality may be missing."], "score": 0.96630859375}, {"id": "(Sadat et al., 2024)", "paper": {"corpus_id": 270923987, "title": "No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Seyedmorteza Sadat", "authorId": "2261742393"}, {"name": "Manuel Kansy", "authorId": "2204861903"}, {"name": "Otmar Hilliges", "authorId": "1466533438"}, {"name": "Romann M. Weber", "authorId": "145848224"}], "n_citations": 14}, "snippets": ["In this paper, we analyze the methodology behind classifier-free guidance and show theoretically that similar behavior can be achieved without additional training of an unconditional model. The main idea is that by using a conditioning vector independent of the input data, the conditional score function becomes equivalent to the unconditional score. This insight leads us to propose independent condition guidance (ICG), a method that replicates the behavior of CFG at inference time without requiring separate training of an unconditional model."], "score": 0.9560546875}, {"id": "(Lu et al., 2025)", "paper": {"corpus_id": 276741986, "title": "What Makes a Good Diffusion Planner for Decision Making?", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Haofei Lu", "authorId": "2315320844"}, {"name": "Dongqi Han", "authorId": "2268676735"}, {"name": "Yifei Shen", "authorId": "2152966656"}, {"name": "Dongsheng Li", "authorId": "2303586558"}], "n_citations": 11}, "snippets": ["Classifier-free guidance: To avoid training classifiers, classifier-free guidance (CFG) is proposed. The main idea of CFG is to train a diffusion model that can be used for both conditional noise predictor \u03f5 \u03b8 (x t , t, c) and unconditional noise predictor \u03f5 \u03b8 (x t , t): \n\nwhere \u03f5 \u03b8 (x t , t) = \u03f5 \u03b8 (x t , t, \u2205). Noise prediction of \u03f5 \u03b8 (x t , t, \u2205) and \u03f5 \u03b8 (x t , t, c) can be jointly learned by randomly discard conditioning with probability of p uncond . For decision making tasks, we can train diffusion models using condition of discounted returns, and using classifier-free guidance for better plan sampling. We can normalize the discounted return in the dataset for training, and use condition of 1 as target return for CFG sampling during inference (Ajay et al., 2022). However, experiments shows that fixing 1 as target may lead to unrealistic or unstable plans."], "score": 0.947265625}, {"id": "(Ajay et al., 2022)", "paper": {"corpus_id": 254044710, "title": "Is Conditional Generative Modeling all you need for Decision-Making?", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Anurag Ajay", "authorId": "150004828"}, {"name": "Yilun Du", "authorId": "15394275"}, {"name": "Abhi Gupta", "authorId": null}, {"name": "J. Tenenbaum", "authorId": "1763295"}, {"name": "T. Jaakkola", "authorId": "35132120"}, {"name": "Pulkit Agrawal", "authorId": "33932184"}], "n_citations": 408}, "snippets": ["Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional diffusion model, we illustrate how we may circumvent the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional diffusion models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making."], "score": 0.0}], "table": null}, {"title": "Benefits of CFG in NLP Applications", "tldr": "Classifier-free guidance offers significant advantages for NLP applications including improved performance across diverse tasks, enhanced prompt adherence, and the ability to balance semantic coherence with diversity. These benefits make CFG a powerful technique for controlling text generation without requiring extensive model modifications. (8 sources)", "text": "\n- **Substantial performance improvements**: CFG has demonstrated significant enhancements across a wide range of NLP tasks including question answering, reasoning, code generation, and machine translation, achieving state-of-the-art results on benchmarks like LAMBADA with smaller models than previously required <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Parameter efficiency**: Applying CFG can bring improvements equivalent to using a model with twice the parameter count, offering a cost-effective alternative to scaling model size for better performance <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Compatibility with other techniques**: CFG can be stacked alongside other inference-time methods such as Chain-of-Thought and Self-Consistency, yielding additional improvements in difficult tasks <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.\n\n- **Enhanced semantic alignment**: By amplifying the directional shift between conditional and unconditional predictions, CFG improves the adherence of generated text to provided prompts or conditions without requiring external classifiers <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>.\n\n- **Controllable fidelity-diversity trade-off**: The guidance scale parameter offers precise control over the balance between semantic coherence with the prompt and creative diversity in the generated output <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\n- **Support for negative prompting**: CFG enables the avoidance of specific aspects in generation through negative prompting, allowing models to be guided away from undesired content or styles <Paper corpusId=\"260899968\" paperTitle=\"(O'Neill et al., 2023)\" isShortName></Paper> <Paper corpusId=\"44104089\" paperTitle=\"(Sajjadi et al., 2018)\" isShortName></Paper>.\n\n- **Cross-modal generation capabilities**: The technique has been successfully adapted for multimodal applications including text-to-video generation and video-to-audio synthesis, demonstrating its versatility across different media types <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper> <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n- **Improved agent performance**: When applied to agent training, CFG consistently improves performance by enhancing the alignment between language conditioning and agent actions <Paper corpusId=\"267027965\" paperTitle=\"(Zolna et al., 2024)\" isShortName></Paper>.\n\n- **Flexibility in implementation**: The technique can be implemented with minimal modifications to existing models and training pipelines, making it an accessible approach for improving conditional generation <Paper corpusId=\"276774646\" paperTitle=\"(Jacobi et al., 2025)\" isShortName></Paper>.\n\n- **Enhanced user experience**: Human evaluations have shown a 75% preference for models using CFG over baseline approaches for assistant-style interactions, indicating significant improvements in faithfulness and coherence for challenging prompts <Paper corpusId=\"259308807\" paperTitle=\"(Sanchez et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Sanchez et al., 2023)", "paper": {"corpus_id": 259308807, "title": "Stay on topic with Classifier-Free Guidance", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Guillaume Sanchez", "authorId": "2056723344"}, {"name": "Honglu Fan", "authorId": "2072838294"}, {"name": "Alexander Spangher", "authorId": "51444076"}, {"name": "Elad Levi", "authorId": "34490455"}, {"name": "Pawan Sasanka Ammanamanchi", "authorId": "1451644426"}, {"name": "Stella Biderman", "authorId": "103476203"}], "n_citations": 55}, "snippets": ["Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q\\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75\\% preference for GPT4All using CFG over baseline."], "score": 0.9560546875}, {"id": "(Jacobi et al., 2025)", "paper": {"corpus_id": 276774646, "title": "Superscopes: Amplifying Internal Feature Representations for Language Model Interpretation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jonathan Jacobi", "authorId": "2348485713"}, {"name": "Gal Niv", "authorId": "2333352"}], "n_citations": 0}, "snippets": ["In diffusion models, Classifier-Free Guidance (CFG) (Ho et al. (2022)) enhances the alignment of generated outputs with a given condition (such as a text prompt). It is widely used in diffusion-based models (Chen et al. (2024)).\n\nCFG operates by amplifying the direction that represents the condition, where the difference between conditional and unconditional predictions defines this direction in the model's latent space. By amplifying this directional shift, the model is effectively steered toward outputs that better align with the given condition. Reapplying this shift further reinforces alignment, emphasizing the semantic meaning embedded in the conditional guidance.\n\nThis technique is particularly useful in text-to-image generation models like Stable Diffusion and GLIDE, as it improves adherence to prompts without requiring an external classifier."], "score": 0.96826171875}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 277043912, "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziqin Zhou", "authorId": "2282670286"}, {"name": "Yifan Yang", "authorId": "2331570564"}, {"name": "Yuqing Yang", "authorId": "2125051198"}, {"name": "Tianyu He", "authorId": "2350871281"}, {"name": "Houwen Peng", "authorId": "2350821834"}, {"name": "Kai Qiu", "authorId": "2268758860"}, {"name": "Qi Dai", "authorId": "2329560121"}, {"name": "Lili Qiu", "authorId": "2160727304"}, {"name": "Chong Luo", "authorId": "2294680622"}, {"name": "Lingqiao Liu", "authorId": "2320820962"}], "n_citations": 1}, "snippets": ["Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."], "score": 0.97607421875}, {"id": "(O'Neill et al., 2023)", "paper": {"corpus_id": 260899968, "title": "Steering Language Generation: Harnessing Contrastive Expert Guidance and Negative Prompting for Coherent and Diverse Synthetic Data Generation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "C. O'Neill", "authorId": "2072014290"}, {"name": "Y. Ting", "authorId": "93622633"}, {"name": "I. Ciuc\u0103", "authorId": "50062876"}, {"name": "Jack William Miller", "authorId": "2229023715"}, {"name": "Thang Bui", "authorId": "2229238231"}], "n_citations": 1}, "snippets": ["In the context of autoregressive language models, which inherently excel in unconditional generation, CFG represents a natural evolution (Sajjadi et al., 2018). By manipulating the generation of subsequent tokens to accentuate the conditioning on the prompt, it builds upon the existing framework of logit guidance and log-probability adjustment. This manipulation can be formally expressed as follows: \n\n(2) \n\nCFG also allows for the avoidance of specific aspects of generation through the use of negative prompting. By adjusting \u03b3 to be negative in the equation above, control is exerted to guide generation away from a given prompt. This methodology has found exceptional efficacy in diffusion models (Andrew 2023; Crowson et al. 2022;Du, Li, and Mordatch 2020;(Sahu et al., 2022)Miyake et al. 2023), further enriching the spectrum of control over generative processes. Both CFG and negative prompting exploit the latent semantic information in token predictions (Winterhalder, Bellagente, and Nachman 2021;Jiang 2023;Lee et al. 2019;Durrani et al. 2022)."], "score": 0.95068359375}, {"id": "(Sajjadi et al., 2018)", "paper": {"corpus_id": 44104089, "title": "Assessing Generative Models via Precision and Recall", "year": 2018, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mehdi S. M. Sajjadi", "authorId": "2283034"}, {"name": "Olivier Bachem", "authorId": "1936951"}, {"name": "Mario Lucic", "authorId": "34302129"}, {"name": "O. Bousquet", "authorId": "1698617"}, {"name": "S. Gelly", "authorId": "1802148"}], "n_citations": 583}, "snippets": ["Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution."], "score": 0.0}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 278171703, "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance", "year": 2025, "venue": "", "authors": [{"name": "Yuzhe Liang", "authorId": "2278618341"}, {"name": "Wenzhe Liu", "authorId": "2358111037"}, {"name": "Chunyu Qiang", "authorId": "2358041541"}, {"name": "Zhikang Niu", "authorId": "2229877177"}, {"name": "Yushen Chen", "authorId": "2324996330"}, {"name": "Ziyang Ma", "authorId": "2116609277"}, {"name": "Wenxi Chen", "authorId": "2278584538"}, {"name": "Nan Li", "authorId": "2358116915"}, {"name": "Chen Zhang", "authorId": "2358098456"}, {"name": "Xie Chen", "authorId": "2321881822"}], "n_citations": 0}, "snippets": ["Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."], "score": 0.97607421875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270220558, "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yongqi Wang", "authorId": "2292208450"}, {"name": "Wenxiang Guo", "authorId": "2304739202"}, {"name": "Rongjie Huang", "authorId": "2048021099"}, {"name": "Jia-Bin Huang", "authorId": "3068086"}, {"name": "Zehan Wang", "authorId": "2258561621"}, {"name": "Fuming You", "authorId": "2292197957"}, {"name": "Ruiqi Li", "authorId": "2181010470"}, {"name": "Zhou Zhao", "authorId": "2304453961"}], "n_citations": 13}, "snippets": ["Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."], "score": 0.0}, {"id": "(Zolna et al., 2024)", "paper": {"corpus_id": 267027965, "title": "GATS: Gather-Attend-Scatter", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "K. Zolna", "authorId": "2065684527"}, {"name": "Serkan Cabi", "authorId": "2066047403"}, {"name": "Yutian Chen", "authorId": "2279763460"}, {"name": "Eric Lau", "authorId": "2279718795"}, {"name": "Claudio Fantacci", "authorId": "2080928701"}, {"name": "J. Pa\u0161ukonis", "authorId": "31143488"}, {"name": "Jost Tobias Springenberg", "authorId": "2060551"}, {"name": "Sergio G\u00f3mez Colmenarejo", "authorId": "2279752493"}], "n_citations": 1}, "snippets": ["Classifier-free guidance has been widely adopted in diffusion-based generative models to improve the output alignment with the conditioning input. The same idea has also been applied to agent training (Lifshitz et al., 2023). \n\nLet (, ) and () be the logit vector of the predicted agent action before softmax given observation , with or without language conditioning  respectively. The guided policy is computed as \n\nwhere scalar  \u2265 0 controls the guidance strength. When  = 0, it is the standard conditional generative model. When  > 0, it puts higher probability on tokens where they are more likely with the given conditioning input compared to the marginal distribution. In our experiments we set  = 0.5. To obtain an agent network we compute both conditional and unconditional policies. We randomly mask the text input with a probability of 0.02 during training. Enabling the classifierfree guidance consistently improves the performance of our agents (see ablation experiments in Section 4.4)."], "score": 0.93798828125}], "table": null}, {"title": "Trade-offs and Limitations", "tldr": "While classifier-free guidance offers significant benefits for NLP applications, it introduces several important trade-offs, primarily between sample quality and diversity. The technique also comes with computational overhead, implementation challenges, and potential for quality degradation in specific scenarios. (12 sources)", "text": "\n- **Reduced sample diversity**: One of the most significant limitations of CFG is that improvements in conditional alignment and sample quality come at the expense of reduced output diversity. Higher guidance scales lead to stronger conditioning but cause the model to focus on fewer modes of the distribution <Paper corpusId=\"249926846\" paperTitle=\"(Yu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"249145348\" paperTitle=\"(Ho, 2022)\" isShortName></Paper> <Paper corpusId=\"266693789\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\n- **Computational overhead**: CFG increases the computational budget for both training and inference. During training, the unconditional task can consume up to 20% of the resources, while inference requires computing both conditional and unconditional predictions at each step <Paper corpusId=\"274117064\" paperTitle=\"(Kaiser et al., 2024)\" isShortName></Paper> <Paper corpusId=\"254854389\" paperTitle=\"(Peebles et al., 2022)\" isShortName></Paper>.\n\n- **Training complexity**: Joint learning of unconditional and conditional generation can result in poor priors for the unconditional case, which may in turn degrade the quality of the conditional generation <Paper corpusId=\"277321603\" paperTitle=\"(Phunyaphibarn et al., 2025)\" isShortName></Paper>.\n\n- **Sensitivity to guidance scale**: Finding the optimal guidance weight presents a challenge, as incorrect values can lead to either poor conditional alignment (when too low) or degraded sample quality (when too high) <Paper corpusId=\"274514993\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n- **Risk of mode collapse**: At high guidance scales, CFG tends to trigger mode collapse, severely limiting the diversity of generated outputs and potentially producing stereotypical or repetitive content <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270220558\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n- **Task-specific tuning requirements**: Different NLP tasks may require different guidance scales and implementation strategies, making it difficult to establish universal best practices across applications <Paper corpusId=\"259212148\" paperTitle=\"(Meekeren et al., 2023)\" isShortName></Paper>.\n\n- **Structural degradation at high guidance scales**: Particularly in multimodal applications, excessive guidance can lead to chaotic object structures, over-stylization, and reduced overall quality, requiring careful balancing of the guidance parameter <Paper corpusId=\"274514993\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n- **Implementation complexity**: Implementing dynamic guidance approaches, such as linearly increasing the scale throughout the sampling process, adds complexity to the system but may be necessary to mitigate diversity issues <Paper corpusId=\"255372955\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>.\n\n- **Incomplete theoretical understanding**: Despite its practical success, the working principles of CFG have not been fully explained theoretically, which can limit systematic improvements and adaptations to new domains <Paper corpusId=\"278171703\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n- **Integration challenges with other techniques**: While CFG can be combined with other methods, such integration requires careful design to avoid conflicting effects between different approaches <Paper corpusId=\"270199289\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Yu et al., 2022)", "paper": {"corpus_id": 249926846, "title": "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Jiahui Yu", "authorId": "2338016295"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Thang Luong", "authorId": "1821711"}, {"name": "Gunjan Baid", "authorId": "1396954703"}, {"name": "Zirui Wang", "authorId": "2331539"}, {"name": "Vijay Vasudevan", "authorId": "2053781980"}, {"name": "Alexander Ku", "authorId": "31702389"}, {"name": "Yinfei Yang", "authorId": "2118771180"}, {"name": "Burcu Karagol Ayan", "authorId": "143990191"}, {"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Wei Han", "authorId": "143911112"}, {"name": "Zarana Parekh", "authorId": "27456119"}, {"name": "Xin Li", "authorId": "2158973314"}, {"name": "Han Zhang", "authorId": null}, {"name": "Jason Baldridge", "authorId": "1387994164"}, {"name": "Yonghui Wu", "authorId": "48607963"}], "n_citations": 1133}, "snippets": ["Classifier-free guidance (Ho, 2022) (CF-guidance in short) is critical in the context of improving the sample quality of diffusion models [11,12,13] without pretrained classifiers. In this setup, a generative model G is trained to be able to perform unconditional generation G(z) (where z represents random noise) and conditional generation G(z, c) (where c represents some condition, such as language descriptions). It is implemented as randomly dropping out the conditional vector (masking out or switching to a learned embedding) with some probability. During the inference process, sampling of an output I is done by using a linear combination of the unconditional and conditional predictions: \n\nwhere \u03bb is a hyperparameter representing the weight of classifier-free guidance. Intuitively, it decreases the unconditional likelihood of the sample while increasing the conditional likelihood, which can be viewed as encouraging alignment between the generated sample and the text condition. \n\nClassifier-free guidance has been similarly applied in the context of autoregressive models for textto-image generation [10,38] to great effect. Make-A-Scene [10] finetunes the model by randomly replacing the text prompts with padded tokens. During inference, tokens are sampled from a linear combination of logits sampled from an unconditional model and a conditional model on a text prompt. We also apply CF-guidance in Parti, and find it has a significant improvement on the output image-text alignment, especially on challenging text prompts."], "score": 0.96484375}, {"id": "(Ho, 2022)", "paper": {"corpus_id": 249145348, "title": "Classifier-Free Diffusion Guidance", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}], "n_citations": 3970}, "snippets": ["Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance."], "score": 0.96435546875}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 266693789, "title": "Diffusion Model with Perceptual Loss", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Shanchuan Lin", "authorId": "32370203"}, {"name": "Xiao Yang", "authorId": "2277412143"}], "n_citations": 17}, "snippets": ["Classifier-Free Guidance (CFG) (Ho, 2022) uses Bayes' rule and finds that the diffusion model itself can be inverted as an implicit classifier. Specifically, the model is queried both conditionally and unconditionally at every inference step and the difference is amplified toward the conditional direction. Both methods only work for conditional generation and entangle sample quality with conditional alignment [24]. Self-Supervised Guidance [20] uses self-supervised networks to generate synthetic clustering labels for unconditional data. This allows unconditional data to use CFG for improving quality. Guidance-Free Training [4] shows that CFG can be applied at training. This bakes the guided flow into the model and saves computation during inference", "Although these methods are effective in improving quality, they also present issues such as increased complexity and reduced diversity (Ho, 2022)[24], etc."], "score": 0.9404296875}, {"id": "(Kaiser et al., 2024)", "paper": {"corpus_id": 274117064, "title": "The Unreasonable Effectiveness of Guidance for Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tim Kaiser", "authorId": "2267395983"}, {"name": "Nikolas Adaloglou", "authorId": "1832165240"}, {"name": "M. Kollmann", "authorId": "2065390431"}], "n_citations": 1}, "snippets": ["The current most popular method, classifier-free guidance (CFG), improves image quality by increasing the probability that an image belongs to a certain class label [25]. Unlike its predecessor, classifier guidance [12], which relies on training an external classifier on labeled noisy images, CFG combines conditional and unconditional denoisers, which can be trained jointly [16].\n\nIn the following, we denote by x a noisy image and by \u03f5(x, t; c) and \u03f5(x, t) the class conditional and unconditional noise predictors at timestep t of the denoising process [12]. CFG linearly combines noise predictions during sampling using the extrapolation scheme \u03b5(x, t; c) = \u03f5(x, t; c) + w[\u03f5(x, t; c) \u2212 \u03f5(x, t)], (1) with guidance weight w > 0. CFG can be viewed as an error-correcting method [7,45]. Equivalent extrapolation schemes can be found for all diffusion model formulations, such as target prediction [24] or flow matching [42].\n\nDespite the widespread use of CFG in conditional synthesis [37], it comes with notable limitations. First, it increases the training budget: when trained jointly, the unconditional task can consume up to 20% of the computational cost [16]. Additionally, while CFG reduces class mismatch between samples and condition c of the noise predictor [41], this benefit comes at the expense of sample diversity, as this sampling method focuses on regions with high class probability [25]."], "score": 0.9482421875}, {"id": "(Peebles et al., 2022)", "paper": {"corpus_id": 254854389, "title": "Scalable Diffusion Models with Transformers", "year": 2022, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "William S. Peebles", "authorId": "35235273"}, {"name": "Saining Xie", "authorId": "1817030"}], "n_citations": 2433}, "snippets": ["We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops\u2014through increased transformer depth/width or increased number of input tokens\u2014consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512\u00d7512 and 256\u00d7256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter."], "score": 0.0}, {"id": "(Phunyaphibarn et al., 2025)", "paper": {"corpus_id": 277321603, "title": "Unconditional Priors Matter! Improving Conditional Generation of Fine-Tuned Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Prin Phunyaphibarn", "authorId": "2268399939"}, {"name": "Phillip Y. Lee", "authorId": "2328609307"}, {"name": "Jaihoon Kim", "authorId": "2292419803"}, {"name": "Minhyuk Sung", "authorId": "2292259803"}], "n_citations": 1}, "snippets": ["Classifier-Free Guidance (CFG) [28] is a fundamental technique in training conditional diffusion models. The common practice for CFG-based training is to use a single network to learn both conditional and unconditional noise prediction, with a small dropout rate for conditioning. However, we observe that the joint learning of unconditional noise with limited bandwidth in training results in poor priors for the unconditional case. More importantly, these poor unconditional noise predictions become a serious reason for degrading the quality of conditional generation."], "score": 0.9638671875}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 274514993, "title": "Liquid: Language Models are Scalable and Unified Multi-modal Generators", "year": 2024, "venue": "", "authors": [{"name": "Junfeng Wu", "authorId": "2146667089"}, {"name": "Yi Jiang", "authorId": "2294789302"}, {"name": "Chuofan Ma", "authorId": "2186151254"}, {"name": "Yuliang Liu", "authorId": "2266428398"}, {"name": "Hengshuang Zhao", "authorId": "2310758544"}, {"name": "Zehuan Yuan", "authorId": "2244754235"}, {"name": "Song Bai", "authorId": "2257280698"}, {"name": "Xiang Bai", "authorId": "2273663142"}], "n_citations": 9}, "snippets": ["Impact of Classifier-free Guidance. Classifier-Free Guidance (CFG) scale is hyperparameter that control the trade-off between sample quality and diversity in conditional generative models. The visual variations of generated images with different CFG scales t are illustrated in Fig. 10 As observed, higher CFG scales lead to better alignment between the generated images and the text prompts, but cause more chaotic object structures, stronger stylization, and worse photorealism. For example, when CFG=15, the structure of the book in the image becomes disordered. Conversely, lower CFG scales result in poorer consistency between the image content and the prompt, but improve the photorealism and fine-grained texture details."], "score": 0.92529296875}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 278171703, "title": "Towards Flow-Matching-based TTS without Classifier-Free Guidance", "year": 2025, "venue": "", "authors": [{"name": "Yuzhe Liang", "authorId": "2278618341"}, {"name": "Wenzhe Liu", "authorId": "2358111037"}, {"name": "Chunyu Qiang", "authorId": "2358041541"}, {"name": "Zhikang Niu", "authorId": "2229877177"}, {"name": "Yushen Chen", "authorId": "2324996330"}, {"name": "Ziyang Ma", "authorId": "2116609277"}, {"name": "Wenxi Chen", "authorId": "2278584538"}, {"name": "Nan Li", "authorId": "2358116915"}, {"name": "Chen Zhang", "authorId": "2358098456"}, {"name": "Xie Chen", "authorId": "2321881822"}], "n_citations": 0}, "snippets": ["Classifier-Free Guidance (CFG) [21] is a pivotal advancement in conditional diffusion modeling, designed to enhance the alignment between generated samples and conditioning signals without relying on auxiliary classifiers. This technique has been widely adopted in various domains, including text-to-speech synthesis and crossmodal generation (Wang et al., 2024). The core mechanism involves interpolating predictions from both conditional and unconditional diffusion processes during sampling. Specifically, the guided noise prediction is formulated as: \n\nwhere \u03c9 \u2265 0 denotes the guidance scale. When \u03c9 = 0, the generation degenerates to an unconditional process governed by p \u03b8 (x); increasing \u03c9 amplifies the conditioning effect, thereby improving semantic consistency with the input c at the potential cost of sample diversity. Empirical studies suggest that an optimal \u03c9 value balances artifact suppression and conditional fidelity. Despite the remarkable success of CFG in diffusion modeling, it still faces many challenges in practical applications. For example, a high guidance scale tends to trigger mode collapse (Mokady et al., 2023)- [32]. More critically, although CFG performs well in practice, its working principle has not been fully explained theoretically [33]."], "score": 0.97607421875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270220558, "title": "Frieren: Efficient Video-to-Audio Generation Network with Rectified Flow Matching", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yongqi Wang", "authorId": "2292208450"}, {"name": "Wenxiang Guo", "authorId": "2304739202"}, {"name": "Rongjie Huang", "authorId": "2048021099"}, {"name": "Jia-Bin Huang", "authorId": "3068086"}, {"name": "Zehan Wang", "authorId": "2258561621"}, {"name": "Fuming You", "authorId": "2292197957"}, {"name": "Ruiqi Li", "authorId": "2181010470"}, {"name": "Zhou Zhao", "authorId": "2304453961"}], "n_citations": 13}, "snippets": ["Video-to-audio (V2A) generation aims to synthesize content-matching audio from silent video, and it remains challenging to build V2A models with high generation quality, efficiency, and visual-audio temporal synchrony. We propose Frieren, a V2A model based on rectified flow matching. Frieren regresses the conditional transport vector field from noise to spectrogram latent with straight paths and conducts sampling by solving ODE, outperforming autoregressive and score-based models in terms of audio quality. By employing a non-autoregressive vector field estimator based on a feed-forward transformer and channel-level cross-modal feature fusion with strong temporal alignment, our model generates audio that is highly synchronized with the input video. Furthermore, through reflow and one-step distillation with guided vector field, our model can generate decent audio in a few, or even only one sampling step. Experiments indicate that Frieren achieves state-of-the-art performance in both generation quality and temporal alignment on VGGSound, with alignment accuracy reaching 97.22%, and 6.2% improvement in inception score over the strong diffusion-based baseline. Audio samples are available at http://frieren-v2a.github.io."], "score": 0.0}, {"id": "(Meekeren et al., 2023)", "paper": {"corpus_id": 259212148, "title": "Exploring the Effectiveness of Dataset Synthesis: An application of Apple Detection in Orchards", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "A. V. Meekeren", "authorId": "104149480"}, {"name": "Maya Aghaei", "authorId": "51304755"}, {"name": "K. Dijkstra", "authorId": "40109500"}], "n_citations": 1}, "snippets": ["While prompt-based image generation can produce realistic images, using Classifier-free Guidance (CFG) [42] can provide even greater control over the generation process. CFG, in Stable Diffusion, amplifies the effect of the text prompt on the generated image. By default, Stable Diffusion applies a classifier to the text prompt to guide the generation of the image [12]. However, CFG allows for fine-tuning the influence of this classifier, resulting in more creative control over the generated image. Typically, CFG is defined to be in a range between 1 and 30, with lower values generating more creative images. \n\nMore specifically, CFG determines the trade-off between the coverage of modes and image fidelity [42]. When set to 1, the model generates samples based solely on the prior distribution, without any guidance. As the guidance scale increases, the model is instructed to produce samples that better match some given condition. The classifier-free aspect of this technique refers to the fact that it does not require training a classifier to incorporate guidance during generation [42]. Instead, the model is guided by adding a penalty term to the generation process, forcing the generated images to match the desired condition."], "score": 0.9453125}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 255372955, "title": "Muse: Text-To-Image Generation via Masked Generative Transformers", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Huiwen Chang", "authorId": "2914394"}, {"name": "Han Zhang", "authorId": "2146204239"}, {"name": "Jarred Barber", "authorId": "152630175"}, {"name": "AJ Maschinot", "authorId": "2199119286"}, {"name": "Jos\u00e9 Lezama", "authorId": "143923528"}, {"name": "Lu Jiang", "authorId": "39978626"}, {"name": "Ming Yang", "authorId": "152790163"}, {"name": "K. Murphy", "authorId": "1702318"}, {"name": "W. Freeman", "authorId": "1768236"}, {"name": "Michael Rubinstein", "authorId": "144544291"}, {"name": "Yuanzhen Li", "authorId": "2167749913"}, {"name": "Dilip Krishnan", "authorId": "1707347"}], "n_citations": 556}, "snippets": ["We employ classifier-free guidance (CFG) (Ho & Salimans, 2022) to improve our generation quality and our text-image alignment. At training time, we remove text conditioning on 10% of samples chosen randomly (thus attention reduces to image token self-attention). At inference time, we compute a conditional logit c and an unconditional logit u for each masked token. We then form the final logits g by moving away from the unconditional logits by an amount t, the guidance scale: \n\nIntuitively, CFG trades off diversity for fidelity. Different from previous approaches, we reduce the hit to diversity by linearly increasing the guidance scale t through the sampling procedure. This allows the early tokens to be sampled more freely, with low or no guidance, but increases the influence of the conditioning prompt for the later tokens. \n\nWe also exploit this mechanism to enable negative prompting (NegPrompt, 2022) by replacing the unconditional logit u with a logit conditioned on a \"negative prompt\". This encourages the resulting image to have features associated with the positive prompt c and remove features associated with the negative prompt u."], "score": 0.94384765625}, {"id": "(Gu et al., 2024)", "paper": {"corpus_id": 270199289, "title": "Kaleido Diffusion: Improving Conditional Diffusion Models with Autoregressive Latent Modeling", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jiatao Gu", "authorId": "2287733778"}, {"name": "Ying Shen", "authorId": "2304371756"}, {"name": "Shuangfei Zhai", "authorId": "2443456"}, {"name": "Yizhe Zhang", "authorId": "2254045488"}, {"name": "N. Jaitly", "authorId": "3111912"}, {"name": "J. Susskind", "authorId": "49158771"}], "n_citations": 10}, "snippets": ["Classifier-free Guidance (CFG), which utilizes the diffusion model itself to perform guidance at test time. More specifically, we perform sampling using the following linear combination:\n\nwhere \u03b3 is the guidance weight, and x \u03b8 (x t ) = x \u03b8 (x t , c = \u2205) is the unconditional denoising output.\n\nDuring training, we drop the condition c with certain probability p uncond to facilitate unconditional prediction. When \u03b3 > 1, CFG takes effect and amplifies the difference between conditional and unconditional generation, leading to a global control of high-quality generation", ".However, it's notable that CFG can significantly impact the diversity of the diffusion output, which motivates us to revisit the basics and combine the strengths of both."], "score": 0.94580078125}], "table": null}, {"title": "Applications and Case Studies", "tldr": "Classifier-free guidance has been successfully applied across diverse NLP and multimodal applications, demonstrating particular value in controllable text generation, image editing, and text-to-video generation. Real-world implementations show how CFG enables precise control over generated content while balancing fidelity and diversity requirements. (6 sources)", "text": "\nClassifier-free guidance has been implemented in a variety of practical applications, demonstrating its versatility across both pure NLP and multimodal systems. In text-to-image generation, CFG has become a standard technique for improving alignment between textual prompts and generated visual content. When applied to slot-based image generation systems like LSSD (Latent Slot-based Scene Decomposition), CFG can be applied directly to input slots without requiring model modifications, significantly improving image quality while maintaining the semantic structure dictated by the text input <Paper corpusId=\"257632090\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>.\n\nFor music generation and editing, CFG has been combined with diffusion models to enable controllable manipulation of audio characteristics. The InstructME system employed both classifier guidance and classifier-free guidance approaches to achieve a balance between quality and controllability, particularly for specialized tasks like instrument and genre manipulation in music remixing <Paper corpusId=\"261242901\" paperTitle=\"(Han et al., 2023)\" isShortName></Paper>. This hybrid approach demonstrates how CFG can be adapted to domain-specific requirements where training data might have unique characteristics that make pure CFG challenging to implement.\n\nIn image editing applications, CFG has been leveraged to create sophisticated inpainting systems. The ControlFill framework uses learned embeddings to guide diffusion networks, allowing users to control the intensity of object removal or creation by adjusting the relative significance of different prompts through classifier-free guidance <Paper corpusId=\"276813045\" paperTitle=\"(Jeon, 2025)\" isShortName></Paper>. This application demonstrates how CFG can enable user-controlled content generation without requiring text encoders during inference, making the approach more computationally efficient.\n\nText-to-video generation represents another important case study where CFG has been successfully implemented. By extending the CFG mechanism from diffusion models to autoregressive language model-based video generation frameworks, developers have achieved a balance between semantic coherence and visual diversity <Paper corpusId=\"277043912\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. Empirical investigations across various CFG configurations have revealed that scales within the range of 5.0 to 7.5 typically produce high-quality videos with optimal adherence to input prompts, highlighting the importance of parameter tuning in practical applications.\n\nThe joint training approach of unconditional and conditional models within a single architecture has proven particularly valuable for resource-constrained applications. By setting the condition to a null token during training, models can learn both conditional and unconditional generation capabilities simultaneously <Paper corpusId=\"274422874\" paperTitle=\"(Hyung et al., 2024)\" isShortName></Paper>. This approach has enabled high-fidelity outputs in various generative tasks while maintaining computational efficiency.\n\nIn controlled scene generation tasks, CFG provides an effective mechanism for balancing accuracy and diversity. By adjusting the guidance weight parameter, developers can achieve the appropriate balance for specific use cases, making the technique highly adaptable to different application requirements <Paper corpusId=\"276741036\" paperTitle=\"(Bian et al., 2024)\" isShortName></Paper>. This adaptability has contributed to CFG's widespread adoption across diverse generative applications.", "citations": [{"id": "(Jiang et al., 2023)", "paper": {"corpus_id": 257632090, "title": "Object-Centric Slot Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jindong Jiang", "authorId": "2211907956"}, {"name": "Fei Deng", "authorId": "2212038042"}, {"name": "Gautam Singh", "authorId": "2212051383"}, {"name": "S. Ahn", "authorId": "2112164991"}], "n_citations": 61}, "snippets": ["Classifier-free guidance (Ho, 2022) uses a mixing weight (abbreviated as cfg) to control the weighted sum of noise predictions between a conditional and an unconditional diffusion model. With cf g = 1, it operates similarly to standard conditional generation. When cf g > 1, the model generates images that are more align to the text input, but comes with a cost of reduced image diversity. Practically speaking, an optimal cf g > 1 can yield images that not only better match the conditions but also exhibit enhanced quality. Since LSSD provides object slots as text tokens to the pre-trained DMs, classifier-free guidance can be applied directly to the input slot, akin to how it's applied to input text in standard DMs, without any model modifications. And our experiment shows that a cf g > 1 for slots can also significantly improve the image generation quality."], "score": 0.931640625}, {"id": "(Han et al., 2023)", "paper": {"corpus_id": 261242901, "title": "InstructME: An Instruction Guided Music Edit And Remix Framework with Latent Diffusion Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Bing Han", "authorId": "2118914313"}, {"name": "Junyu Dai", "authorId": "2147243559"}, {"name": "Xuchen Song", "authorId": "84556222"}, {"name": "Weituo Hao", "authorId": "3314779"}, {"name": "Xinyan He", "authorId": "2234532553"}, {"name": "Dong Guo", "authorId": "2234359627"}, {"name": "Jitong Chen", "authorId": "2855690"}, {"name": "Yuxuan Wang", "authorId": "2234520458"}, {"name": "Y. Qian", "authorId": "2480051"}], "n_citations": 16}, "snippets": ["For diffusion models, there exist two primary strategies for achieving controllable generation. One of these is classifier guidance (CG) (Dhariwal and Nichol 2021; Liu et al. 2023b), which utilizes a classifier during the sampling process and mixes its input gradient of the log probability with the score estimate of diffusion model. It is flexible and controllable, but tends to suffer a performance degradation (Ho and Salimans 2022). Another approach, named classifierfree guidance (CFG) (Ho and Salimans 2022;Nichol et al. 2021;Ramesh et al. 2022;Saharia et al. 2022), achieves the same effect through training a conditional diffusion model directly without a guidance classifier. This method performs better but requires a large amount of data with diverse text descriptions, which is difficult for our InstructME trained with source-target paired data. In this work, to attain a tradeoff between quality and controllability, we adopt both classifier and classifier-free guidance to achieve the controllable editing of Remix operations.\n\nWe specify instrument and genre tags with CFG by incorporating these tags into text commands to train the conditional diffusion models. During the training, we discard our text condition y randomly with a certain probability p CFG following (Liu et al. 2023a;Wang et al. 2023). Then, in the sampling, we can estimate the noise \u03b5\u03b8 (t, T (y), p s , z s , z t ) with a linear combination of the conditional and unconditional score estimates:"], "score": 0.953125}, {"id": "(Jeon, 2025)", "paper": {"corpus_id": 276813045, "title": "ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Boseong Jeon", "authorId": "2348738665"}], "n_citations": 0}, "snippets": ["In this report, I present an inpainting framework named \\textit{ControlFill}, which involves training two distinct prompts: one for generating plausible objects within a designated mask (\\textit{creation}) and another for filling the region by extending the background (\\textit{removal}). During the inference stage, these learned embeddings guide a diffusion network that operates without requiring heavy text encoders. By adjusting the relative significance of the two prompts and employing classifier-free guidance, users can control the intensity of removal or creation. Furthermore, I introduce a method to spatially vary the intensity of guidance by assigning different scales to individual pixels."], "score": 0.9609375}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 277043912, "title": "HiTVideo: Hierarchical Tokenizers for Enhancing Text-to-Video Generation with Autoregressive Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziqin Zhou", "authorId": "2282670286"}, {"name": "Yifan Yang", "authorId": "2331570564"}, {"name": "Yuqing Yang", "authorId": "2125051198"}, {"name": "Tianyu He", "authorId": "2350871281"}, {"name": "Houwen Peng", "authorId": "2350821834"}, {"name": "Kai Qiu", "authorId": "2268758860"}, {"name": "Qi Dai", "authorId": "2329560121"}, {"name": "Lili Qiu", "authorId": "2160727304"}, {"name": "Chong Luo", "authorId": "2294680622"}, {"name": "Lingqiao Liu", "authorId": "2320820962"}], "n_citations": 1}, "snippets": ["Inspired by the CFG mechanism used in diffusion models, we extend this concept to an autoregressive languagemodel-based text-to-video generation framework. During training, we introduce an unconditional embedding, enabling the model to effectively utilize conditional and unconditional contexts simultaneously. This strategy equips the model with greater flexibility, allowing it to generate outputs that remain semantically coherent while exploring creatively diverse variations. To empirically investigate the effect of the CFG scale, we conducted experiments across various CFG configurations, examining the inherent tradeoffs between semantic alignment and visual diversity.\n\nThe balance determined by the CFG scale is pivotal, directly affecting both the semantic coherence and the visual appeal of the generated videos. Therefore, careful tuning of the CFG parameter is essential for optimizing video generation quality. The computation of logits used for sampling video tokens, incorporating the CFG factor, can be represented by the following pseudo-code: logits = uncond logits+ (cond logits \u2212 uncond logits) \u2022 cfg scale\n\nIn our experiments, we observed that CFG scales within the range of 5.0 to 7.5 typically result in high-quality videos with optimal adherence to input prompts, as illustrated in Fig. 8 -Fig. 10."], "score": 0.97607421875}, {"id": "(Hyung et al., 2024)", "paper": {"corpus_id": 274422874, "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "J. Hyung", "authorId": "2071816370"}, {"name": "Kinam Kim", "authorId": "2333250087"}, {"name": "Susung Hong", "authorId": "2186865215"}, {"name": "Minjeong Kim", "authorId": "2117955517"}, {"name": "J. Choo", "authorId": "1795455"}], "n_citations": 4}, "snippets": ["Classifier-Free Guidance (CFG) [10] uses Bayes' rule to replace a classifier-guided score with a linear combination of conditional and unconditional score estimates: \n\nCFG jointly trains the unconditional model \u03f5 \u03b8 (x t |\u03d5) and the conditional model \u03f5 \u03b8 (x t |c) (= \u03f5 \u03b8 (x t )) within a single model by setting the condition c to a null token \u03d5. Using this guided denoising process, the model better captures the conditions and often generates high-fidelity outputs."], "score": 0.9228515625}, {"id": "(Bian et al., 2024)", "paper": {"corpus_id": 276741036, "title": "DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hengwei Bian", "authorId": "2327218388"}, {"name": "Lingdong Kong", "authorId": "2152007435"}, {"name": "Haozhe Xie", "authorId": "2237483104"}, {"name": "Liang Pan", "authorId": "2272233402"}, {"name": "Yu Qiao", "authorId": "2327935778"}, {"name": "Ziwei Liu", "authorId": "2277717448"}], "n_citations": 5}, "snippets": ["Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) could improve the performance of conditional generative models without relying on an external classifier. Specifically, during training, the model simultaneously learns both conditional generation p(x|c) and unconditional generation p(x), and guidance during sampling is provided by the following equation: \n\nwhere xt (c) is the result conditioned on c, xt (\u2205) is the unconditioned result, and w is a weight parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate balance between the accuracy and diversity of the generated scenes can be achieved."], "score": 0.9580078125}], "table": null}], "cost": 0.5172390000000001}}
