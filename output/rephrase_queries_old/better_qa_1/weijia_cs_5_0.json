{"better_query": "What are the most effective methods for detecting pre-training data contamination in large language models, and how do current benchmarks like WIKIMIA evaluate these methods?", "better_answer": {"sections": [{"title": "Introduction and Background", "tldr": "Data contamination in large language models (LLMs) occurs when evaluation benchmarks overlap with training data, artificially inflating performance metrics. Various detection methods have emerged to identify contamination, primarily focusing on n-gram matching between training and evaluation datasets. (4 sources)", "text": "\nData contamination has become a growing concern in the evaluation of large language models (LLMs), hampering the interpretation of benchmark scores and raising questions about the reliability of reported model performance <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. The issue arises when evaluation data has been inadvertently included in the model's training corpus, potentially enabling the model to simply recall memorized information rather than demonstrate true generalization capabilities.\n\nNumerous studies have investigated this phenomenon, with most existing definitions of contamination being based on n-gram duplication between pre-training and evaluation datasets <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. For example, some research teams have established specific thresholds: PaLM researchers classified evaluation data as \"contaminated\" if at least 70% of all possible 8-grams in an evaluation sample appeared at least once in the pre-training corpus, while Llama 2 developers provided a more fine-grained definition based on the percentage of tokens that appear in both training and evaluation sets <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>.\n\nHowever, these traditional n-gram-based approaches have significant limitations. They primarily target direct duplications, which can lead to both high false positive rates (as semantically different texts may have overlaps) and false negative rates (since simple paraphrasing can evade detection) <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. Additionally, existing contamination detection methods have shown limited success and cannot effectively quantify the contamination's impact on model performance <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>. These methods also make strict assumptions about the contamination process, making them relatively easy to circumvent <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nTwo primary solutions have been proposed to address data contamination: data contamination detection and dynamic data evaluation <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. While detection methods aim to identify overlaps between model outputs and training data, they face considerable challenges with proprietary models like the GPT series, which often incorporate special filtering mechanisms during generation <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. Furthermore, current detection approaches primarily focus on extreme memorization (direct reproduction of training data), making it difficult to capture more subtle forms of contamination that might still affect evaluation results <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>.\n\nThe complexity of defining and detecting contamination has led researchers to propose that contamination metrics should be assessed based on whether models benefit from the examples they mark as contaminated, suggesting a more outcome-oriented approach to understanding this phenomenon <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Singh et al., 2024)", "paper": {"corpus_id": 273850342, "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Aaditya K. Singh", "authorId": "2306863572"}, {"name": "Muhammed Yusuf Kocyigit", "authorId": "1665851460"}, {"name": "Andrew Poulton", "authorId": "2282542314"}, {"name": "David Esiobu", "authorId": "71039937"}, {"name": "Maria Lomeli", "authorId": "2253400960"}, {"name": "Gergely Szilvasy", "authorId": "2253402270"}, {"name": "Dieuwke Hupkes", "authorId": "3449411"}], "n_citations": 13}, "snippets": ["Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects."], "score": 0.92529296875}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 266933004, "title": "Investigating Data Contamination for Pre-training Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Minhao Jiang", "authorId": "2800541"}, {"name": "Ken Ziyu Liu", "authorId": "2298016051"}, {"name": "Ming Zhong", "authorId": "1606040932"}, {"name": "Rylan Schaeffer", "authorId": "1749176844"}, {"name": "Siru Ouyang", "authorId": "2260339714"}, {"name": "Jiawei Han", "authorId": "2259869648"}, {"name": "Sanmi Koyejo", "authorId": "123593472"}], "n_citations": 72}, "snippets": ["Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36])."], "score": 0.9453125}, {"id": "(Dekoninck et al., 2024)", "paper": {"corpus_id": 270063170, "title": "ConStat: Performance-Based Contamination Detection in Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jasper Dekoninck", "authorId": "2268310707"}, {"name": "Mark Niklas M\u00fcller", "authorId": "2116235329"}, {"name": "Martin T. Vechev", "authorId": "1736447"}], "n_citations": 8}, "snippets": ["Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17]."], "score": 0.97705078125}, {"id": "(Fan, 2025)", "paper": {"corpus_id": 275906726, "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yang Fan", "authorId": "2342797440"}], "n_citations": 0}, "snippets": ["To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination."], "score": 0.94580078125}], "table": null}, {"title": "Matching-Based Detection Methods", "tldr": "Matching-based detection methods identify contamination by directly comparing evaluation data with pre-training corpora or by assessing model behavior when prompted with segments of evaluation data, with approaches ranging from n-gram overlap analysis to more sophisticated retrieval-based systems. (6 sources)", "text": "\nMatching-based detection methods form a significant category of techniques for identifying benchmark data contamination (BDC) in large language models. These approaches typically involve direct comparisons between evaluation datasets and pre-training corpora, or analyzing model behavior when presented with portions of evaluation data.\n\nThe most traditional approach involves calculating n-gram overlap between pre-training and evaluation datasets, which has been widely employed in the development of models like GPT-3, LLaMA, and Claude <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This method directly checks whether test examples appear verbatim in pre-training corpora, with Common Crawl often serving as a reference point because it comprises a majority of pre-training data for many LLMs <Paper corpusId=\"266359062\" paperTitle=\"(Li, 2023)\" isShortName></Paper>. Beyond simple n-gram analysis, some researchers have employed BM25 for indexing and matching across datasets <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nHowever, these direct matching approaches face significant limitations. As pre-training datasets grow exponentially, even basic n-gram statistics become extremely resource-intensive to compute. Furthermore, these methods often prove unreliable due to unintentional contamination risks <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. They also typically assume access to pre-training data and can only detect contamination when multiple test examples co-occur, as they rely on data sketching tools that are only effective for sequences above certain lengths <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>.\n\nTo address scenarios where pre-training data is inaccessible (especially with proprietary models), researchers have developed more sophisticated detection techniques. Deng et al. proposed a retrieval-based system and a \"Testset Slot Guessing\" protocol that involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. Similarly, Golchin and Surdeanu introduced the \"Data Contamination Quiz\" (DCQ), which presents models with multiple-choice questions containing three perturbed versions of each dataset instance. The model's ability to identify the original instance among the perturbed ones indicates potential exposure during pre-training <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>.\n\nAnother notable approach is the \"guided instruction\" method, which identifies potential contamination by prompting an LLM with the dataset name, partition type, and a random-length initial segment of a reference instance, then asking it to complete the text. An instance is flagged as contaminated if the model's output closely matches the latter segment of the reference <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>. Some researchers have also leveraged dataset metadata for detection, such as dataset ordering or the assignment of examples to specific data splits <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>.\n\nOren et al. proposed a method that provides provable guarantees of test set contamination without requiring access to pre-training data or model weights. Their approach is based on the principle that in uncontaminated models, all orderings of an exchangeable benchmark should be equally likely. By comparing the likelihood of a canonically ordered benchmark dataset with its shuffled version, they can detect potential contamination <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 270619707, "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Qin Zhu", "authorId": "2307992515"}, {"name": "Qingyuan Cheng", "authorId": "2290056525"}, {"name": "Runyu Peng", "authorId": "2307468261"}, {"name": "Xiaonan Li", "authorId": "50080067"}, {"name": "Tengxiao Liu", "authorId": "2136108329"}, {"name": "Runyu Peng", "authorId": "2307468261"}, {"name": "Xipeng Qiu", "authorId": "2282972251"}, {"name": "Xuanjing Huang", "authorId": "2284750473"}], "n_citations": 7}, "snippets": ["Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b)."], "score": 0.92431640625}, {"id": "(Li, 2023)", "paper": {"corpus_id": 266359062, "title": "An Open-Source Data Contamination Report for Large Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yucheng Li", "authorId": "1527099159"}], "n_citations": 19}, "snippets": ["The central goal of data contamination analysis is to categorise test samples as either clean or contaminated and then evaluate models separately on the clean and contaminated samples to assess the impact of contamination on the performance metrics. In this section, we describe our methodology to identify contaminated test samples. The basic idea of detecting contaminated examples in our method is to check whether test examples appear verbatim in Common Crawl. We base on Common Crawl because it is completely open-sourced and often comprises the majority of pre-training data for large language models, e.g., Common Crawl weights over 80% in GPT-3 and LLaMA training data (Brown et al., 2020;Touvron et al., 2023a)."], "score": 0.92138671875}, {"id": "(Palavalli et al., 2024)", "paper": {"corpus_id": 271097946, "title": "A Taxonomy for Data Contamination in Large Language Models", "year": 2024, "venue": "CONDA", "authors": [{"name": "Medha Palavalli", "authorId": "2220962946"}, {"name": "Amanda Bertsch", "authorId": "2138301112"}, {"name": "Matthew R. Gormley", "authorId": "1762110"}], "n_citations": 4}, "snippets": ["Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives."], "score": 0.95166015625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 270285708, "title": "Benchmark Data Contamination of Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cheng Xu", "authorId": "2153079650"}, {"name": "Shuhao Guan", "authorId": "2304954016"}, {"name": "Derek Greene", "authorId": "2304952028"}, {"name": "Mohand-Tahar Kechadi", "authorId": "2266906186"}], "n_citations": 56}, "snippets": ["Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively", "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps", "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training", "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection", "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models", "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC", "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods", "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination", "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."], "score": 0.97265625}, {"id": "(Golchin et al., 2023)", "paper": {"corpus_id": 260925501, "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Shahriar Golchin", "authorId": "65754049"}, {"name": "M. Surdeanu", "authorId": "1760868"}], "n_citations": 108}, "snippets": ["To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts."], "score": 0.95458984375}, {"id": "(Oren et al., 2023)", "paper": {"corpus_id": 264490730, "title": "Proving Test Set Contamination in Black Box Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yonatan Oren", "authorId": "153163779"}, {"name": "Nicole Meister", "authorId": "2261737865"}, {"name": "Niladri S. Chatterji", "authorId": "22193324"}, {"name": "Faisal Ladhak", "authorId": "8759332"}, {"name": "Tatsunori Hashimoto", "authorId": "2244446164"}], "n_citations": 146}, "snippets": ["In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."], "score": 0.95849609375}], "table": null}, {"title": "Statistical and Probability-Based Detection Methods", "tldr": "Statistical and probability-based detection methods identify contamination by analyzing model behaviors like output probabilities and confidence patterns, without requiring direct access to pre-training data, making them applicable to proprietary models. (7 sources)", "text": "\nStatistical and probability-based detection methods offer an alternative approach to identifying data contamination by focusing on model behaviors rather than direct corpus matching. These techniques are particularly valuable when pre-training data is inaccessible, as is often the case with proprietary models.\n\nOne of the most prominent statistical approaches is Min-K% Prob, which assesses whether a text was present in an LLM's pre-training data by calculating the average log-likelihood of the lowest-probability tokens in the sequence. A high result suggests the text's presence in the training data <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. However, this method faces limitations in real-world applications, including the lack of established thresholds for determining contamination and challenges in implementation due to unavailable code <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.\n\nAnother significant statistical method is the Canonical Order Statistical Testing approach proposed by Oren et al., which provides provable guarantees of test set contamination without requiring access to training data or model weights. This method exploits the exchangeability property of datasets, where the order of examples should not affect the joint distribution. By comparing the language model's log probability on a \"canonical\" ordering versus a shuffled version, the test can detect if a model shows a statistically significant preference for a particular ordering, which would indicate contamination <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>. Despite its statistical rigor, this approach can be computationally intensive, especially when analyzing longer data examples <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.\n\nResearchers have also explored direct observation of model memorization behaviors as an indicator of contamination. Li proposed quantifying potential contamination by examining whether models exhibit memorization behavior on test instances, rather than identifying n-gram overlaps between training and test sets <Paper corpusId=\"262055119\" paperTitle=\"(Li_1, 2023)\" isShortName></Paper>. Similarly, Zhang et al. introduced PaCoST (Paired Confidence Significance Testing), which constructs counterparts for each data instance and analyzes corresponding confidence scores to determine if a model demonstrates significantly higher confidence when presented with original benchmarks, operating under the assumption that models show greater confidence when responding to questions they've been trained on <Paper corpusId=\"270737802\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nWhile these methods have shown promise, their effectiveness varies depending on the context. Yu et al. found that loss-value comparison methods like Min-K% Prob are effective for detecting pre-training contamination but less reliable for identifying contamination during fine-tuning <Paper corpusId=\"267897557\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>. Additionally, the diversity and complexity of training data can amplify the difficulty of distinguishing between members and non-members, leading to suboptimal performance in detecting pre-training data contamination <Paper corpusId=\"273350935\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>.\n\nBeyond these primary methods, researchers have developed approaches that leverage dataset metadata to detect contamination without direct access to pre-training data. These include methods that exploit dataset ordering or the assignment of examples to specific data splits <Paper corpusId=\"271097946\" paperTitle=\"(Palavalli et al., 2024)\" isShortName></Paper>. The Token Completion Overlap Score is one such technique that prompts an LLM with dataset name, partition type, and a random initial segment of a reference instance, flagging it as contaminated if the model's output closely matches the latter part of the reference <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Samuel et al., 2024)", "paper": {"corpus_id": 272689919, "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Vinay Samuel", "authorId": "2298040858"}, {"name": "Yue Zhou", "authorId": "2261321156"}, {"name": "Henry Peng Zou", "authorId": "2261285492"}], "n_citations": 8}, "snippets": ["To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."], "score": 0.9736328125}, {"id": "(Oren et al., 2023)", "paper": {"corpus_id": 264490730, "title": "Proving Test Set Contamination in Black Box Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yonatan Oren", "authorId": "153163779"}, {"name": "Nicole Meister", "authorId": "2261737865"}, {"name": "Niladri S. Chatterji", "authorId": "22193324"}, {"name": "Faisal Ladhak", "authorId": "8759332"}, {"name": "Tatsunori Hashimoto", "authorId": "2244446164"}], "n_citations": 146}, "snippets": ["In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."], "score": 0.95849609375}, {"id": "(Li_1, 2023)", "paper": {"corpus_id": 262055119, "title": "Estimating Contamination via Perplexity: Quantifying Memorisation in Language Model Evaluation", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yucheng Li", "authorId": "1527099159"}], "n_citations": 35}, "snippets": ["In this paper, we propose a novel approach to quantify potential contamination in language model evaluation benchmarks without accessing the entire training data. Instead of identifying n-gram overlaps between training and test set, we directly observe whether models exhibit memorisation behaviour on test instances."], "score": 0.93212890625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 270737802, "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Huixuan Zhang", "authorId": "2288559437"}, {"name": "Yun Lin", "authorId": "2297335276"}, {"name": "Xiaojun Wan", "authorId": "2288537941"}], "n_citations": 0}, "snippets": ["In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments."], "score": 0.9736328125}, {"id": "(Yu et al., 2024)", "paper": {"corpus_id": 267897557, "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhuohao Yu", "authorId": "2164113313"}, {"name": "Chang Gao", "authorId": "2287659901"}, {"name": "Wenjin Yao", "authorId": "2286328804"}, {"name": "Yidong Wang", "authorId": "2108024279"}, {"name": "Wei Ye", "authorId": "145235149"}, {"name": "Jindong Wang", "authorId": "2273553706"}, {"name": "Xing Xie", "authorId": "2249681654"}, {"name": "Yue Zhang", "authorId": "2250437942"}, {"name": "Shikun Zhang", "authorId": "1705434"}], "n_citations": 28}, "snippets": ["Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning."], "score": 0.962890625}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 273350935, "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hengxiang Zhang", "authorId": "2316591285"}, {"name": "Songxin Zhang", "authorId": "2266803682"}, {"name": "Bingyi Jing", "authorId": "2283306681"}, {"name": "Hongxin Wei", "authorId": "2325203633"}], "n_citations": 1}, "snippets": ["Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data."], "score": 0.95556640625}, {"id": "(Palavalli et al., 2024)", "paper": {"corpus_id": 271097946, "title": "A Taxonomy for Data Contamination in Large Language Models", "year": 2024, "venue": "CONDA", "authors": [{"name": "Medha Palavalli", "authorId": "2220962946"}, {"name": "Amanda Bertsch", "authorId": "2138301112"}, {"name": "Matthew R. Gormley", "authorId": "1762110"}], "n_citations": 4}, "snippets": ["Methods with access to pretraining data Early research on LLM data contamination primarily employed methods akin to high-order n-gram overlap detection between pretraining and evaluation data (Radford et al., 2019a;Brown et al., 2020;Wei et al., 2021;Touvron et al., 2023). Tools for qualitative analysis on large-scale corpora (such as Data Portraits (Marone and Durme, 2023) and the ROOTS Search Tool (Piktus et al., 2023)) have further increased the practicality of this type of contamination detection. However, these approaches have several limitations: they remain fairly computationally expensive, assume access to pretraining data, and generally can only detect contamination when a cluster of several test set examples co-occur (as most methods leverage data sketching (Broder, 1997) tools that are only effective for sequences above a certain length). Yang et al. (2023) proposes an LLM-based decontamination method, which leverages embedding similarity search followed by evaluation with a strong language model (e.g. GPT-4), to identify and mitigate contamination. This is computationally costly but can identify noisy contamination Methods without access to pretraining data Some approaches are capable of detecting contamination without direct access to pretraining data, but assume that the test data has not been modified or distributed across the pretraining corpus. These methods leverage metadata from the dataset to detect contamination, e.g. by leveraging dataset ordering (Sainz et al., 2023b) or the assignment of examples to specific data splits (Golchin and Surdeanu, 2023). Golchin and Surdeanu (2024) introduce the Data Contamination Quiz, a streamlined method that efficiently detects and estimates verbatim contamination in LLMs by crafting multiple choice questions that prompt a model to correctly dataset-specific content among similar but noisy alternatives."], "score": 0.95166015625}], "table": null}, {"title": "Min-K% Prob and its Variants", "tldr": "Min-K% Prob detects contamination by identifying low-probability tokens in text sequences, with the intuition that models assign higher probabilities to previously seen content. Several variants have emerged that improve upon this method, including Min-K%++ and approaches that focus on \"surprising tokens\" or contextual comparisons. (6 sources)", "text": "\nMin-K% Prob has emerged as a significant approach for detecting data contamination in large language models. Introduced by Shi et al., this method operates on a simple yet effective hypothesis: \"an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities\" <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. What makes Min-K% Prob particularly valuable is that it requires neither knowledge about the pre-training corpus nor additional training, distinguishing it from previous detection methods that demanded training reference models on data similar to pre-training data <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of Min-K% Prob has been demonstrated on the WIKIMIA benchmark, where it achieved a 7.4% improvement over previous methods <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This benchmark was specifically designed to support gold truth detection by using data created before and after model training <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. However, while Min-K% Prob has shown effectiveness for detecting pre-training contamination, research by Yu et al. indicates it may be less reliable for identifying contamination that occurs during fine-tuning <Paper corpusId=\"267897557\" paperTitle=\"(Yu et al., 2024)\" isShortName></Paper>.\n\nSeveral variants have been developed to address limitations in the original Min-K% Prob approach. Min-K%++ represents a significant enhancement that normalizes the initial log-probability by subtracting the expected log-probability and dividing by the variance <Paper corpusId=\"268819579\" paperTitle=\"(Ravaut et al., 2024)\" isShortName></Paper>. This adaptation has yielded substantial improvements, with Min-K%++ outperforming the original Min-K% Prob by up to 10 points on the WIKIMIA benchmark <Paper corpusId=\"268819579\" paperTitle=\"(Ravaut et al., 2024)\" isShortName></Paper>.\n\nZhang et al. proposed another adaptive pre-training data detection method that focuses on \"surprising tokens\" <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. This approach defines a token as surprising when the model's prediction is \"certain but wrong\" \u2013 characterized by low Shannon entropy in the probability distribution and a low probability assigned to the ground truth token <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>. The method builds on the hypothesis that seen data is less surprising to the model than unseen data, thereby reducing dependence on verbatim memorization for detection <Paper corpusId=\"271570943\" paperTitle=\"(Zhang et al._2, 2024)\" isShortName></Paper>.\n\nA contextual contrast approach developed by Wang et al. offers yet another variation, arguing that both member and non-member contexts should be considered simultaneously <Paper corpusId=\"272423548\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Contrary to previous assumptions that member contexts provide minimal information due to small distributional shifts, their analysis revealed that these subtle shifts can be effectively leveraged when contrasted with non-member contexts <Paper corpusId=\"272423548\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nDespite these advances, Min-K% Prob and its variants face several practical limitations. Samuel et al. identified three key challenges with the original approach: (1) the reliance on assumptions about ground truth in the WIKIMIA dataset that may require further justification, (2) the absence of established thresholds for determining contamination in real-world applications where oracle AUC measurements are unavailable, and (3) a lack of publicly available code for implementation <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. These limitations highlight ongoing challenges in applying these detection methods to real-world contamination scenarios.", "citations": [{"id": "(Shi et al., 2023)", "paper": {"corpus_id": 264451585, "title": "Detecting Pretraining Data from Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Anirudh Ajith", "authorId": "2218438150"}, {"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Yangsibo Huang", "authorId": "108053318"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}], "n_citations": 201}, "snippets": ["To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods."], "score": 0.9736328125}, {"id": "(Yu et al., 2024)", "paper": {"corpus_id": 267897557, "title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhuohao Yu", "authorId": "2164113313"}, {"name": "Chang Gao", "authorId": "2287659901"}, {"name": "Wenjin Yao", "authorId": "2286328804"}, {"name": "Yidong Wang", "authorId": "2108024279"}, {"name": "Wei Ye", "authorId": "145235149"}, {"name": "Jindong Wang", "authorId": "2273553706"}, {"name": "Xing Xie", "authorId": "2249681654"}, {"name": "Yue Zhang", "authorId": "2250437942"}, {"name": "Shikun Zhang", "authorId": "1705434"}], "n_citations": 28}, "snippets": ["Detecting data contamination, a form of Membership Inference Attack (MIA), poses challenges for large language models (LLMs) due to their training on vast corpora and the difficulty of conducting ablation studies (Shi et al., 2023). To detect such contamination of LLMs, Wei et al. (2023) suggested comparing average loss values between training and test datasets, while Shi et al. (2023) introduced Min-K% Prob based on loss values to identify texts used in training. Our experiments show these methods are effective for pre-training but not for detecting contamination during fine-tuning."], "score": 0.962890625}, {"id": "(Ravaut et al., 2024)", "paper": {"corpus_id": 268819579, "title": "A Comprehensive Survey of Contamination Detection Methods in Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Mathieu Ravaut", "authorId": "14038850"}, {"name": "Bosheng Ding", "authorId": "2064493724"}, {"name": "Fangkai Jiao", "authorId": "1689176705"}, {"name": "Hailin Chen", "authorId": "2258571998"}, {"name": "Xingxuan Li", "authorId": "2155447436"}, {"name": "Ruochen Zhao", "authorId": "2091437375"}, {"name": "Chengwei Qin", "authorId": "2084609980"}, {"name": "Caiming Xiong", "authorId": "2267728986"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}], "n_citations": 7}, "snippets": ["Min-K%++ (Zhang et al., 2024a) proposes another extension of the idea of Min-K% Prob: rather than simply getting next-token predicted probabilities, the authors propose a score which subtracts the expected log-probability and divides the score by the variance; in other words, normalizing the initial log-probability. The sequence scoring mechanism is then identical to the one of Min-K% Prob", "Min-K%++ reaches state-of-the-art on WikiMIA, outperforming Min-K% Prob by up to 10 points."], "score": 0.955078125}, {"id": "(Zhang et al._2, 2024)", "paper": {"corpus_id": 271570943, "title": "Adaptive Pre-training Data Detection for Large Language Models via Surprising Tokens", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Anqi Zhang", "authorId": "2313922585"}, {"name": "Chaofeng Wu", "authorId": "2314513929"}], "n_citations": 6}, "snippets": ["Current solutions to this problem leverage techniques explored in machine learning privacy such as Membership Inference Attacks (MIAs), which heavily depend on LLMs' capability of verbatim memorization. However, this reliance presents challenges, especially given the vast amount of training data and the restricted number of effective training epochs. In this paper, we propose an adaptive pre-training data detection method which alleviates this reliance and effectively amplify the identification. Our method adaptively locates \\textit{surprising tokens} of the input. A token is surprising to a LLM if the prediction on the token is\"certain but wrong\", which refers to low Shannon entropy of the probability distribution and low probability of the ground truth token at the same time. By using the prediction probability of surprising tokens to measure \\textit{surprising}, the detection method is achieved based on the simple hypothesis that seeing seen data is less surprising for the model compared with seeing unseen data."], "score": 0.93603515625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 272423548, "title": "Con-ReCall: Detecting Pre-training Data in LLMs via Contrastive Decoding", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Cheng Wang", "authorId": "2319955259"}, {"name": "Yiwei Wang", "authorId": "2280103482"}, {"name": "Bryan Hooi", "authorId": "2305483565"}, {"name": "Yujun Cai", "authorId": "1928716951"}, {"name": "Nanyun Peng", "authorId": "2256996328"}, {"name": "Kai-Wei Chang", "authorId": "2257127887"}], "n_citations": 6}, "snippets": ["Existing methods typically analyze target text in isolation or solely with non-member contexts, overlooking potential insights from simultaneously considering both member and non-member contexts. While previous work suggested that member contexts provide little information due to the minor distributional shift they induce, our analysis reveals that these subtle shifts can be effectively leveraged when contrasted with non-member contexts."], "score": 0.9306640625}, {"id": "(Samuel et al., 2024)", "paper": {"corpus_id": 272689919, "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Vinay Samuel", "authorId": "2298040858"}, {"name": "Yue Zhou", "authorId": "2261321156"}, {"name": "Henry Peng Zou", "authorId": "2261285492"}], "n_citations": 8}, "snippets": ["To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."], "score": 0.9736328125}], "table": null}, {"title": "Benchmark Datasets for Contamination Detection", "tldr": "Several benchmark datasets have been developed specifically for evaluating contamination detection methods, including WIKIMIA, ArxivMIA, DetCon, and ComiEval, each providing different advantages for contamination analysis in various domains. (6 sources)", "text": "\nA number of specialized benchmark datasets have been created to evaluate the effectiveness of contamination detection methods in large language models:\n\n- **WIKIMIA**: Introduced by Shi et al., WIKIMIA is a dynamic benchmark that uses data created both before and after model training to support \"gold truth\" detection. This benchmark has become particularly valuable because it provides known ground truth about which data was available during pre-training, allowing researchers to definitively evaluate detection methods. Min-K% Prob demonstrated a 7.4% improvement over previous methods when tested on this benchmark. <Paper corpusId=\"264451585\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n- **ArxivMIA**: Liu et al. proposed this challenging benchmark comprising arXiv abstracts from Computer Science and Mathematics categories. Rather than focusing on superficial features like perplexities in generated texts, ArxivMIA was designed to support more sophisticated detection methods that examine a model's internal activations through probing techniques. <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n- **DetCon and ComiEval**: Introduced by Dong et al., these benchmarks were specifically designed for assessing both benchmark data contamination (BDC) and the effectiveness of contamination mitigation methods. DetCon focuses on evaluating detection techniques, while ComiEval provides a framework for testing mitigation strategies. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **Canonically Ordered Benchmarks**: Oren et al. developed a methodology that leverages existing benchmarks in a novel way by comparing the likelihood of canonically ordered datasets versus shuffled versions. While not a new dataset per se, this approach transforms standard benchmarks into contamination detection tools. This method has been successfully applied to detect contamination in models as small as 1.4 billion parameters and on small test sets of only 1,000 examples. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264490730\" paperTitle=\"(Oren et al., 2023)\" isShortName></Paper>\n\n- **ConTAM**: Singh et al. proposed this novel analysis method that evaluates contamination metrics based on whether models actually benefit from examples marked as contaminated. Through a large-scale survey across 13 benchmarks and 7 models from 2 different families, ConTAM provides a framework for better understanding evaluation data contamination and its effects. <Paper corpusId=\"273850342\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>\n\n- **Watermarked Benchmarks**: Sander et al. introduced an innovative approach that deliberately watermarks benchmark datasets before their release. This method reformulates original questions with a watermarked LLM without altering benchmark utility, then detects \"radioactivity\" traces during evaluation. Testing on benchmarks like ARC-Easy, ARC-Challenge, and MMLU demonstrated successful contamination detection when models were contaminated enough to enhance performance. <Paper corpusId=\"276575227\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>\n\nThese benchmark datasets have enabled researchers to systematically evaluate contamination detection methods across diverse contexts and model architectures, providing essential tools for addressing the critical challenge of benchmark data contamination.", "citations": [{"id": "(Shi et al., 2023)", "paper": {"corpus_id": 264451585, "title": "Detecting Pretraining Data from Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Anirudh Ajith", "authorId": "2218438150"}, {"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Yangsibo Huang", "authorId": "108053318"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}], "n_citations": 201}, "snippets": ["To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods."], "score": 0.9736328125}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270217411, "title": "Probing Language Models for Pre-training Data Detection", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhenhua Liu", "authorId": "2294376388"}, {"name": "Tong Zhu", "authorId": "1914586128"}, {"name": "Chuanyuan Tan", "authorId": "2186374155"}, {"name": "Haonan Lu", "authorId": "2304460083"}, {"name": "Bing Liu", "authorId": "2330946427"}, {"name": "Wenliang Chen", "authorId": "2265943980"}], "n_citations": 13}, "snippets": ["Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories."], "score": 0.9775390625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 270285708, "title": "Benchmark Data Contamination of Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cheng Xu", "authorId": "2153079650"}, {"name": "Shuhao Guan", "authorId": "2304954016"}, {"name": "Derek Greene", "authorId": "2304952028"}, {"name": "Mohand-Tahar Kechadi", "authorId": "2266906186"}], "n_citations": 56}, "snippets": ["Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively", "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps", "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training", "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection", "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models", "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC", "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods", "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination", "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."], "score": 0.97265625}, {"id": "(Oren et al., 2023)", "paper": {"corpus_id": 264490730, "title": "Proving Test Set Contamination in Black Box Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yonatan Oren", "authorId": "153163779"}, {"name": "Nicole Meister", "authorId": "2261737865"}, {"name": "Niladri S. Chatterji", "authorId": "22193324"}, {"name": "Faisal Ladhak", "authorId": "8759332"}, {"name": "Tatsunori Hashimoto", "authorId": "2244446164"}], "n_citations": 146}, "snippets": ["In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model's training data or weights.\n\nTo achieve these guarantees, we exploit the fact that many datasets have a property known as exchangeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular ordering of the dataset -such as a canonical ordering that appears in publicly available repositories- this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model's log probability on the 'canonical' ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.\n\nUsing these ideas, we propose a computationally efficient and statistically powerful test for contamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false positive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values."], "score": 0.95849609375}, {"id": "(Singh et al., 2024)", "paper": {"corpus_id": 273850342, "title": "Evaluation data contamination in LLMs: how do we measure it and (when) does it matter?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Aaditya K. Singh", "authorId": "2306863572"}, {"name": "Muhammed Yusuf Kocyigit", "authorId": "1665851460"}, {"name": "Andrew Poulton", "authorId": "2282542314"}, {"name": "David Esiobu", "authorId": "71039937"}, {"name": "Maria Lomeli", "authorId": "2253400960"}, {"name": "Gergely Szilvasy", "authorId": "2253402270"}, {"name": "Dieuwke Hupkes", "authorId": "3449411"}], "n_citations": 13}, "snippets": ["Hampering the interpretation of benchmark scores, evaluation data contamination has become a growing concern in the evaluation of LLMs, and an active area of research studies its effects. While evaluation data contamination is easily understood intuitively, it is surprisingly difficult to define precisely which samples should be considered contaminated and, consequently, how it impacts benchmark scores. We propose that these questions should be addressed together and that contamination metrics can be assessed based on whether models benefit from the examples they mark contaminated. We propose a novel analysis method called ConTAM, and show with a large scale survey of existing and novel n-gram based contamination metrics across 13 benchmarks and 7 models from 2 different families that ConTAM can be used to better understand evaluation data contamination and its effects."], "score": 0.92529296875}, {"id": "(Sander et al., 2025)", "paper": {"corpus_id": 276575227, "title": "Detecting Benchmark Contamination Through Watermarking", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tom Sander", "authorId": "2283934407"}, {"name": "Pierre Fernandez", "authorId": "2322442744"}, {"name": "Saeed Mahloujifar", "authorId": "2290845753"}, {"name": "A. Durmus", "authorId": "2283933043"}, {"name": "Chuan Guo", "authorId": "2290241478"}], "n_citations": 1}, "snippets": ["Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy."], "score": 0.955078125}], "table": null}, {"title": "Limitations of Current Detection Methods", "tldr": "Current contamination detection methods face significant challenges including resource intensity, dependence on inaccessible pre-training data, inability to quantify performance impacts, and vulnerability to evasion through paraphrasing or subtle contamination. (7 sources)", "text": "\nDespite substantial progress in developing methods to detect data contamination in large language models, existing approaches face several critical limitations that hinder their effectiveness. Traditional contamination detection methods, which primarily rely on calculating textual overlap between pre-training and evaluation data, have become increasingly impractical as pre-training datasets grow exponentially in size. Even computing basic n-gram statistics has become extremely resource-intensive <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This computational burden significantly constrains researchers' ability to comprehensively assess contamination in modern LLMs.\n\nA fundamental challenge is that most detection methods assume access to pre-training data, which is rarely available for proprietary models like the GPT series <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. This inaccessibility has led researchers to develop black-box approaches, but these often focus on extreme cases of memorization, making it difficult to identify more subtle forms of contamination that might still significantly impact evaluation results <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>. Current detection methods also typically cannot quantify the contamination's effect on model performance, limiting their practical utility <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nThe standard n-gram-based detection approaches suffer from both high false positive and false negative rates, as semantically different texts may share n-gram overlaps while simple paraphrasing can easily evade detection <Paper corpusId=\"266933004\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. This superficiality fails to capture deeper forms of contamination that may affect model performance <Paper corpusId=\"270620798\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Additionally, existing methods must make strict assumptions about the contamination process, making them relatively easy to circumvent <Paper corpusId=\"270063170\" paperTitle=\"(Dekoninck et al., 2024)\" isShortName></Paper>.\n\nEven state-of-the-art detection methods have significant limitations. Min-K% Prob, while innovative, lacks established thresholds for determining contamination in real-world applications where oracle AUC measurements are unavailable <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. The Canonical Order Statistical Testing method becomes prohibitively expensive when analyzing longer data examples due to the combinatorial explosion of sample/shard/permutation combinations <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>. Token Completion Overlap Score and other prompt-based approaches are limited by the LLM's comprehension abilities and response to instructions <Paper corpusId=\"270619707\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>.\n\nA particularly challenging issue is that the diversity and complexity of training data magnify the difficulty of distinguishing between members and non-members of the training set, leading to suboptimal performance in detecting pre-training data contamination <Paper corpusId=\"273350935\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This challenge is exacerbated by proprietary models that incorporate special filtering mechanisms during generation, further complicating detection efforts <Paper corpusId=\"275906726\" paperTitle=\"(Fan, 2025)\" isShortName></Paper>.\n\nThese limitations collectively underscore the need for more robust, efficient, and nuanced approaches to contamination detection that can address the full spectrum of contamination scenarios while being practical to implement at scale.", "citations": [{"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 270619707, "title": "Inference-Time Decontamination: Reusing Leaked Benchmarks for Large Language Model Evaluation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Qin Zhu", "authorId": "2307992515"}, {"name": "Qingyuan Cheng", "authorId": "2290056525"}, {"name": "Runyu Peng", "authorId": "2307468261"}, {"name": "Xiaonan Li", "authorId": "50080067"}, {"name": "Tengxiao Liu", "authorId": "2136108329"}, {"name": "Runyu Peng", "authorId": "2307468261"}, {"name": "Xipeng Qiu", "authorId": "2282972251"}, {"name": "Xuanjing Huang", "authorId": "2284750473"}], "n_citations": 7}, "snippets": ["Traditional contamination detection methods directly calculate the overlap between pre-training data and evaluation datasets, including n-gram analysis (Touvron et al., 2023b;OpenAI, 2023;Team et al., 2023;Bai et al., 2023) and BM25 (Jiang et al., 2024) for indexing and matching.However, as pre-training data grows exponentially, even simple n-gram statistics become extremely resource-intensive.Yang et al. (2023c); Gunasekar et al. (2023) find n-gram detection unreliable due to unintentional contamination risks.More importantly, training corpora for mainstream LLMs are mostly inaccessible, so recent research has turned to focus on: i)-exploiting the distributional differences between the benchmark training set and the test set to evaluated (Xu et al., 2024).ii)-Evaluate sample-level contamination by providing text segments and black-box access to the LLM (Shi et al., 2023).Other work evaluates contamination through LLM-generated content, limited by the LLM's comprehension abilities to instrurction (Deng et al., 2023;Golchin and Surdeanu, 2023a).Some studies test if models can coherently continue a given sample part (Golchin and Surdeanu, 2023b)."], "score": 0.92431640625}, {"id": "(Fan, 2025)", "paper": {"corpus_id": 275906726, "title": "AdEval: Alignment-based Dynamic Evaluation to Mitigate Data Contamination in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yang Fan", "authorId": "2342797440"}], "n_citations": 0}, "snippets": ["To address data contamination, two primary solutions have been proposed: data contamination detection and dynamic data evaluation [6]. Data contamination detection aims to identify overlaps between model outputs and training data [7]. However, these methods face limitations, particularly for proprietary models such as the GPT series, which often incorporate special filtering mechanisms during generation. Additionally, detection methods primarily focus on extreme memorization (i.e., direct reproduction of training data), making it difficult to capture more subtle forms of contamination."], "score": 0.94580078125}, {"id": "(Dekoninck et al., 2024)", "paper": {"corpus_id": 270063170, "title": "ConStat: Performance-Based Contamination Detection in Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jasper Dekoninck", "authorId": "2268310707"}, {"name": "Mark Niklas M\u00fcller", "authorId": "2116235329"}, {"name": "Martin T. Vechev", "authorId": "1736447"}], "n_citations": 8}, "snippets": ["Traditional Contamination Detection Existing contamination detection methods [18,23,24,30,(Mattern et al., 2023)36,39,40,48] aim to detect the inclusion of benchmark samples in the training data as a measure of contamination.However, these approaches show limited success, cannot quantify the contamination's effect on model performance, and have to make strict assumptions about the contamination process, making them easy to evade [17]."], "score": 0.97705078125}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 266933004, "title": "Investigating Data Contamination for Pre-training Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Minhao Jiang", "authorId": "2800541"}, {"name": "Ken Ziyu Liu", "authorId": "2298016051"}, {"name": "Ming Zhong", "authorId": "1606040932"}, {"name": "Rylan Schaeffer", "authorId": "1749176844"}, {"name": "Siru Ouyang", "authorId": "2260339714"}, {"name": "Jiawei Han", "authorId": "2259869648"}, {"name": "Sanmi Koyejo", "authorId": "123593472"}], "n_citations": 72}, "snippets": ["Numerous studies on large language models (LLMs) have explored and investigated the concept of data contamination and demonstrated the robustness of these models against potential contamination in their evaluation datasets (Radford et al., 2019)(Brown et al., 2020)6,27,33,34,11]. Most definitions proposed in the existing studies are based on n-gram duplication between pre-training data and evaluation data. For instance, PaLM [6] divides the evaluation data into two categories-\"clean\" and \"contaminated\"-based on whether at least 70% of all possible 8-grams in the evaluation sample were seen at least once in the pre-training corpus. Llama 2 [34] provides a more fine-grained definition: a token is considered contaminated if it appears in any token n-gram longer than 10 tokens in both the evaluation sample and the training set, and the contamination percentage of an evaluation sample is defined to be the percentage of tokens contaminated; the evaluation data are then divided into 4 buckets-\"Clean\", \"Not Clean\", \"Not Dirty\", and \"Dirty\"-based on the contamination percentage of each evaluation sample. While intuitive, these contamination definitions primarily revolve around n-gram or token overlaps, which only target direct duplications present in both training and evaluation datasets and might provide both high false positive rate (since many semantically different texts have overlaps) and false negative rate (since simple paraphrasing can evade detection [36])."], "score": 0.9453125}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 270620798, "title": "Data Contamination Can Cross Language Barriers", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Feng Yao", "authorId": "2297810571"}, {"name": "Yufan Zhuang", "authorId": "1505801820"}, {"name": "Zihao Sun", "authorId": "2307478708"}, {"name": "Sunan Xu", "authorId": "2307559061"}, {"name": "Animesh Kumar", "authorId": "2297831856"}, {"name": "Jingbo Shang", "authorId": "2297773933"}], "n_citations": 12}, "snippets": ["Existing contamination detection methods are typically based on the text overlap between training and evaluation data, which can be too superficial to reflect deeper forms of contamination."], "score": 0.92822265625}, {"id": "(Samuel et al., 2024)", "paper": {"corpus_id": 272689919, "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Vinay Samuel", "authorId": "2298040858"}, {"name": "Yue Zhou", "authorId": "2261321156"}, {"name": "Henry Peng Zou", "authorId": "2261285492"}], "n_citations": 8}, "snippets": ["To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."], "score": 0.9736328125}, {"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 273350935, "title": "Fine-tuning can Help Detect Pretraining Data from Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hengxiang Zhang", "authorId": "2316591285"}, {"name": "Songxin Zhang", "authorId": "2266803682"}, {"name": "Bingyi Jing", "authorId": "2283306681"}, {"name": "Hongxin Wei", "authorId": "2325203633"}], "n_citations": 1}, "snippets": ["Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal performance in detecting pretraining data.\n\nOn the one hand, model developers can remove evaluation benchmark data from training data by retrieval-based methods with access to pertaining data (Ravaut et al., 2024;Chowdhery et al., 2023). Specifically, those methods employ n-gram tokenization and string-matching for detecting data contamination (Brown et al., 2020;Touvron et al., 2023b;Team et al., 2023;Radford et al., 2019). On the other hand, researchers utilize prompting techniques (Golchin & Surdeanu, 2024), performance analysis (Ye et al., 2024;Debenedetti et al., 2024), model likelihood (Oren et al., 2024;Shi et al., 2024;Xu et al., 2024b) to detect potential contamination without access to the training data."], "score": 0.95556640625}], "table": null}, {"title": "Novel and Alternative Detection Approaches", "tldr": "Researchers have developed innovative contamination detection methods that overcome limitations of traditional approaches, including probing techniques that examine model internals, statistical tests comparing output distributions, and even deliberate watermarking of benchmark datasets. (6 sources)", "text": "\nSeveral novel contamination detection approaches have emerged to address the limitations of traditional methods:\n\n- **Guided Instruction and Data Contamination Quiz (DCQ)**: Golchin and Surdeanu developed a method that prompts an LLM with dataset name, partition type, and a random-length initial segment of a reference instance, flagging contamination if the model's output closely matches the latter segment. They also introduced DCQ, which presents models with multiple-choice questions containing perturbed versions of dataset instances to detect contamination. Their approach achieved 92-100% accuracy across seven datasets when compared with expert evaluations. <Paper corpusId=\"260925501\" paperTitle=\"(Golchin et al., 2023)\" isShortName></Paper>\n\n- **Probing Techniques**: Liu et al. proposed using probing techniques to examine a model's internal activations rather than focusing on superficial features like perplexities in generated texts. They demonstrated this approach on their ArxivMIA benchmark, which contains challenging arXiv abstracts from Computer Science and Mathematics categories. <Paper corpusId=\"270217411\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n- **Contamination Detection via Output Distribution (CDD)**: Dong et al. focused on analyzing the distribution of generated content to detect contamination, introducing DetCon and ComiEval benchmarks specifically for assessing contamination detection and mitigation methods. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **Paired Confidence Significance Testing (PaCoST)**: Zhang et al. introduced this approach which constructs counterparts for each data instance with similar distribution, then analyzes confidence scores to determine if a model shows significantly higher confidence when presented with original benchmarks. The method operates on the assumption that models demonstrate greater confidence when responding to questions they've been trained on. <Paper corpusId=\"270737802\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>\n\n- **Benchmark Watermarking**: Sander et al. proposed an innovative preemptive solution by watermarking benchmarks before their release. This approach reformulates original questions with a watermarked LLM without altering benchmark utility, then detects \"radioactivity\" traces during evaluation. Testing on benchmarks like ARC-Easy, ARC-Challenge, and MMLU demonstrated successful contamination detection when models were contaminated enough to enhance performance. <Paper corpusId=\"276575227\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>\n\n- **Testset Slot Guessing (TS-Guessing)**: Deng et al. developed this protocol which masks incorrect answers in multiple-choice questions and prompts the model to fill in the gaps, using the model's response patterns to identify potential contamination. <Paper corpusId=\"270285708\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\nWhile these approaches represent significant advances, they still face limitations. Token Completion Overlap Score is constrained by the LLM's comprehension abilities and response to instructions, while Canonical Order Statistical Testing becomes prohibitively expensive when analyzing longer data examples due to combinatorial explosion of sample/shard/permutation combinations. <Paper corpusId=\"272689919\" paperTitle=\"(Samuel et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Golchin et al., 2023)", "paper": {"corpus_id": 260925501, "title": "Time Travel in LLMs: Tracing Data Contamination in Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Shahriar Golchin", "authorId": "65754049"}, {"name": "M. Surdeanu", "authorId": "1760868"}], "n_citations": 108}, "snippets": ["To address this issue, we propose an inexpensive and robust approach to detect data contamination for a given dataset partition automatically. Importantly, our approach functions under two realistic assumptions: (a) we lack direct access to the pre-training data of the LLMs, and (b) we have limited computational resources. Intuitively, our method starts by identifying potential contamination in individual instances that are drawn from a small random sample of the corresponding dataset partition (we use samples of 10 instances in this work).\n\nAt its core, our approach starts by identifying potential contamination at the instance level; using this information, our approach then assesses wider contamination at the partition level. To estimate contamination of individual instances, we employ\"guided instruction:\"a prompt consisting of the dataset name, partition type, and the random-length initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or nearly matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset partition as contaminated if the average overlap score with the reference instances (as measured by ROUGE-L or BLEURT) is statistically significantly better with the completions from guided instruction compared to a\"general instruction\"that does not include the dataset and partition name. The second idea marks a dataset partition as contaminated if a classifier based on GPT-4 with few-shot in-context learning prompt marks multiple generated completions as exact/near-exact matches of the corresponding reference instances. Our best method achieves an accuracy between 92% and 100% in detecting if an LLM is contaminated with seven datasets, containing train and test/validation partitions, when contrasted with manual evaluation by human experts."], "score": 0.95458984375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270217411, "title": "Probing Language Models for Pre-training Data Detection", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhenhua Liu", "authorId": "2294376388"}, {"name": "Tong Zhu", "authorId": "1914586128"}, {"name": "Chuanyuan Tan", "authorId": "2186374155"}, {"name": "Haonan Lu", "authorId": "2304460083"}, {"name": "Bing Liu", "authorId": "2330946427"}, {"name": "Wenliang Chen", "authorId": "2265943980"}], "n_citations": 13}, "snippets": ["Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model's internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories."], "score": 0.9775390625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 270285708, "title": "Benchmark Data Contamination of Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cheng Xu", "authorId": "2153079650"}, {"name": "Shuhao Guan", "authorId": "2304954016"}, {"name": "Derek Greene", "authorId": "2304952028"}, {"name": "Mohand-Tahar Kechadi", "authorId": "2266906186"}], "n_citations": 56}, "snippets": ["Efficient identification of BDC in evaluation benchmarks represents a fundamental aspect in ensuring the reliability and integrity of LLMs, while also providing the basis for developing effective mitigation strategies.This section provides a comprehensive review of literature related to the detection of BDC.We categorize the methodologies into two distinct strategies: matching-based and comparison-based, discussed in Sections 3.1 and 3.2, respectively", "Deng et al. [30] proposed two novel methods to detect potential overlaps between evaluation benchmarks and pre-training corpora, tailored for both open-source and proprietary LLMs.They introduced a retrieval-based system and a Testset Slot Guessing (TS-Guessing) protocol, which involves masking incorrect answers in multiple-choice questions and prompting the model to fill in the gaps", "Golchin and Surdeanu [46] presented another novel approach, the Data Contamination Quiz (DCQ), to detect and quantify BDC in LLMs.The DCQ is a series of multiple-choice questions with three perturbed versions of each dataset instance, including only word-level changes.The LLM's ability to identify the original instance among the perturbed ones indicates potential exposure to the data during pre-training", "Golchin and Surdeanu (Golchin et al., 2023) presented a technique that combines instance-level identification using guided instruction prompts with partition-level assessment through overlap score comparison and classifier-based detection", "Li et al. [91] presented a comprehensive report on BDC across over 15 LLMs and six multiplechoice QA benchmarks.They introduced an open-source pipeline to conduct contamination analysis on customized data and models", "Dong et al. [34] focused on the distribution of generated content, they proposed a novel method, CDD (Contamination Detection via output Distribution), which uses the output distribution of LLMs to detect BDC", "The paper also presents two new benchmarks, DetCon and ComiEval, for assessing BDC and mitigation methods", "Li [89] proposed a novel method to detect BDC in language model evaluation without requiring access to the full training set.The technique uses perplexity to measure the extent of contamination", "Oren et al. (Oren et al., 2023) presented a method to detect test set contamination in language models without needing access to the model's pre-training data or weights.The authors used a statistical test to identify contamination by comparing the likelihood of a benchmark dataset's canonical ordering against a shuffled version."], "score": 0.97265625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 270737802, "title": "PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Huixuan Zhang", "authorId": "2288559437"}, {"name": "Yun Lin", "authorId": "2297335276"}, {"name": "Xiaojun Wan", "authorId": "2288537941"}], "n_citations": 0}, "snippets": ["In this study, we introduce a novel approach named PaCoST (Paired Confidence Significance Testing) designed for the detection of benchmark contamination in open-source LLMs. Our method entails a three-step statistical analysis, capable of identifying benchmarks within the model's training data. Specifically, our approach involves constructing counterparts for each data instance with similar distribution, followed by statistical analysis of corresponding confidence scores to ascertain whether the model exhibits significantly higher confidence when presented with original benchmarks. We operate under the assumption that the model tends to demonstrate greater confidence when responding to questions it has been trained on. To validate our method rigorously, we conduct a series of controlled experiments."], "score": 0.9736328125}, {"id": "(Sander et al., 2025)", "paper": {"corpus_id": 276575227, "title": "Detecting Benchmark Contamination Through Watermarking", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tom Sander", "authorId": "2283934407"}, {"name": "Pierre Fernandez", "authorId": "2322442744"}, {"name": "Saeed Mahloujifar", "authorId": "2290845753"}, {"name": "A. Durmus", "authorId": "2283933043"}, {"name": "Chuan Guo", "authorId": "2290241478"}], "n_citations": 1}, "snippets": ["Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect \"radioactivity\", i.e., traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, e.g., p-val = 10 \u22123 for +5% on ARC-Easy."], "score": 0.955078125}, {"id": "(Samuel et al., 2024)", "paper": {"corpus_id": 272689919, "title": "Towards Data Contamination Detection for Modern Large Language Models: Limitations, Inconsistencies, and Oracle Challenges", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Vinay Samuel", "authorId": "2298040858"}, {"name": "Yue Zhou", "authorId": "2261321156"}, {"name": "Henry Peng Zou", "authorId": "2261285492"}], "n_citations": 8}, "snippets": ["To address this gap and provide a dual investigation of SOTA LLM contamination status and detection method robustness, we evaluate five contamination detection approaches with four state-of-the-art LLMs across eight challenging datasets often used in modern LLM evaluation.\n\nWe examine five distinct approaches to detecting data contamination in LLMs, including three state-of-the-art techniques from ICLR (2023-2024) and two exploratory prompt-based approaches. Two approaches are based on sequence probabilities and require access to model parameters. Figure 1 illustrates the visual overview of each approach. For each method, we also note the limitations we identified during our examination. The overview of these methods are illustrated in Figure 1.\n\n\u2022 Min-K% Prob (Shi et al., 2023) assesses whether a text was in an LLM's pre-training data by calculating the average log-likelihood of the k% lowestprobability tokens, with a high result suggesting the text's presence in the training data. Limitations: (1) The authors report AUC based on the proposed WiKiMIA dataset, in which they regarded data events before the model release as contaminated data. Such a strong assumption on the ground truth may require more justification. (2) They did not provide the threshold to determine the value of min-K%-prob in the paper since they claim they can use AUC; however, in real-world settings, we do not always have the oracle to determine AUC -instead, we need a metric for determining whether arbitrary datasets are contaminated. (3) The code is not available.\n\n\u2022 Canonical Order Statistical Testing (Oren et al., 2024) identifies contamination in a pre-training dataset by checking if the model shows a preference for the canonical order of examples over random shuffling. This preference is tested by comparing their log probabilities, with results aggregated across datasets to ensure a low false positive rate. Limitations: When an individual data example's length is long, the combination of sample/shards/permutations in the setup can be costly.\n\n\u2022 Token Completion Overlap Score (Golchin and Surdeanu, 2024b) detects contamination by prompting the LLM with a dataset name, partition type, and a random initial segment of a reference instance. If the LLM's output closely matches the latter part of the reference, the instance is flagged as contaminated."], "score": 0.9736328125}], "table": null}], "cost": 0.41722500000000007}}
