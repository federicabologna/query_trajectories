{"better_query": "What are recent advances in graph-based neural approaches for abstractive multi-document summarization, and which papers have demonstrated state-of-the-art results?", "better_answer": {"sections": [{"title": "Introduction and Background", "tldr": "Graph-based neural approaches have evolved significantly in multi-document summarization, progressing from basic graph ranking methods to sophisticated neural architectures that can process multiple documents while capturing their complex relationships. Recent advances have focused on leveraging graph neural networks to model cross-document relationships and improve information extraction from lengthy source documents. (7 sources)", "text": "\nMulti-document summarization (MDS) has evolved through several distinct phases over the years, with graph-based methods playing a crucial role throughout this development. The earliest approaches focused on graph ranking-based extractive methods such as TextRank and LexRank, which used graph structures to identify important sentences in document collections <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. These early graph-based methods were followed by syntax and structure-based compression techniques that attempted to address information redundancy and paraphrasing across multiple documents <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper>.\n\nThe field underwent a significant transformation with the introduction of neural sequence-to-sequence abstractive methods around 2017, which built upon advances in single-document summarization and incorporated techniques like pointer generator networks with maximal marginal relevance <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. This neural evolution coincided with the development of important datasets like WikiSum and MultiNews, which have become standard benchmarks for evaluating MDS systems.\n\nGraph Neural Networks (GNNs) have demonstrated particular effectiveness in various NLP applications, including multi-document summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. Early applications of GNNs to MDS, such as the work by Christensen et al., created multi-document graphs that captured discourse relationships between sentences <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. The introduction of Graph Convolutional Networks (GCN) by Kipf and Welling further advanced the field, enabling the generation of sentence relation graphs for improved summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>.\n\nRecent developments have increasingly focused on leveraging graph representations to pre-synthesize dependencies between articles prior to summarization <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. These graph-based approaches help capture cross-document relations, which is crucial when summarizing lengthy documents <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. Additionally, innovative architectures like Hierarchical Transformers have been proposed to better represent cross-token and cross-document information <Paper corpusId=\"251594480\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nDespite these advances, significant challenges remain in capturing and aggregating salient information distributed across multiple lengthy source documents. Current language models, while proficient at summarization, still struggle with processing extensive inputs. Existing solutions often rely on sequence truncation rather than fully exploiting graph-based modeling of semantic units to identify essential information <Paper corpusId=\"273784877\" paperTitle=\"(Ragazzi et al., 2025)\" isShortName></Paper>. This highlights the continuing need for more sophisticated graph-based neural approaches that can effectively handle the complexity of multi-document summarization.", "citations": [{"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.96435546875}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Li et al., 2014)", "paper": {"corpus_id": 10112929, "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees", "year": 2014, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Li", "authorId": "40475614"}, {"name": "Yang Liu", "authorId": "2152797181"}, {"name": "Fei Liu", "authorId": "144544919"}, {"name": "Lin Zhao", "authorId": "46586837"}, {"name": "F. Weng", "authorId": "1807350"}], "n_citations": 46}, "snippets": ["In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."], "score": 0.0}, {"id": "(Golestani et al., 2022)", "paper": {"corpus_id": 247446769, "title": "Pruned Graph Neural Network for Short Story Ordering", "year": 2022, "venue": "International Conference of the Italian Association for Artificial Intelligence", "authors": [{"name": "Melika Golestani", "authorId": "2124373879"}, {"name": "Zeinab Borhanifard", "authorId": "19218725"}, {"name": "Farnaz Tahmasebian", "authorId": "51133923"}, {"name": "Heshaam Faili", "authorId": "3054779"}], "n_citations": 0}, "snippets": ["Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization."], "score": 0.97265625}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.0}, {"id": "(Ma et al., 2022)", "paper": {"corpus_id": 251594480, "title": "Parallel Hierarchical Transformer with Attention Alignment for Abstractive Multi-Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Ye Ma", "authorId": "51439912"}, {"name": "Lu Zong", "authorId": "48373979"}], "n_citations": 0}, "snippets": ["In comparison to single-document summarization, MDS brings challenges on the representation and coverage of its lengthy and linked sources", "On the other hand, Liu and Lapata (2019) propose a Hierarchical Transformer (HT) with local and global encoder layers to represent cross-token and cross-document information."], "score": 0.9609375}, {"id": "(Ragazzi et al., 2025)", "paper": {"corpus_id": 273784877, "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge", "year": 2025, "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "authors": [{"name": "Luca Ragazzi", "authorId": "134327204"}, {"name": "Gianluca Moro", "authorId": "2237646991"}, {"name": "Lorenzo Valgimigli", "authorId": "2132084411"}, {"name": "Riccardo Fiorani", "authorId": "2328891652"}], "n_citations": 0}, "snippets": ["Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."], "score": 0.986328125}], "table": null}, {"title": "Evolution of Graph-based Neural Approaches", "tldr": "Graph-based neural approaches for multi-document summarization have progressed from basic ranking methods to sophisticated architectures that model complex document relationships. Recent innovations include heterogeneous graphs, hypergraphs, and hierarchical structures that better capture cross-document dependencies and improve information selection. (20 sources)", "text": "\nThe evolution of graph-based neural approaches for multi-document summarization can be traced through several distinct developmental phases. Early graph-based methods like TextRank and LexRank represented fundamental approaches that used graph ranking to identify important sentences in document collections <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. These methods were followed by syntax and structure-based compression techniques that addressed information redundancy across multiple documents <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"10112929\" paperTitle=\"(Li et al., 2014)\" isShortName></Paper>.\n\nA significant advancement came with the integration of Graph Neural Networks (GNNs) into multi-document summarization systems. Christensen et al. created multi-document graphs that captured discourse relationships between sentences, while Kipf and Welling's Graph Convolutional Networks (GCN) further enabled the generation of sentence relation graphs for improved summarization <Paper corpusId=\"247446769\" paperTitle=\"(Golestani et al., 2022)\" isShortName></Paper>. These early applications demonstrated GNNs' effectiveness in modeling document structures and relationships that are crucial for summarization tasks.\n\nThe introduction of pointer-generator networks by See et al. represented another milestone in neural abstractive summarization <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>. This approach was subsequently enhanced by incorporating various types of information, including topic information <Paper corpusId=\"203641742\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>, document structural information <Paper corpusId=\"46936631\" paperTitle=\"(Song et al., 2018)\" isShortName></Paper>, and semantic information <Paper corpusId=\"52111191\" paperTitle=\"(Hardy et al., 2018)\" isShortName></Paper>. The model was further improved by replacing LSTM modules with more advanced Transformer architectures to better handle long sequence inputs <Paper corpusId=\"202541012\" paperTitle=\"(Pilault et al., 2020)\" isShortName></Paper> <Paper corpusId=\"229171633\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"249062562\" paperTitle=\"(Fonseca et al., 2022)\" isShortName></Paper>.\n\nRecent developments have shifted toward more sophisticated graph-based neural architectures. These include heterogeneous graph neural networks that integrate nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"269791107\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Other innovations include hypergraph transformers like HEGEL, which capture high-order cross-sentence relations <Paper corpusId=\"252780923\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>, and hierarchical structures such as HierGNN, which learns document structure through latent structure trees <Paper corpusId=\"253155963\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>.\n\nGraph-based models for abstractive multi-document summarization have also evolved to leverage different types of graphs for encoding documents. GraphSum constructs similarity graphs over paragraphs <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>, while MGSum creates a three-level hierarchical graph spanning document, sentence, and word levels <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>. More recent approaches have incorporated heterogeneous graphs that consider different edge types and semantic nodes of various granularities <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper> <Paper corpusId=\"257496469\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe integration of topic information into graph-based models has emerged as another important trend. Neural topic models have been jointly used with graph neural networks to discover latent topics that bridge different documents and guide summary generation <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>. Similarly, approaches like BASS have unified semantic graphs to aggregate co-referent phrases distributed across a wide range of context <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.\n\nThese advances reflect the ongoing effort to develop more effective methods for capturing cross-document relationships and improving information selection in multi-document summarization. By better modeling the complex dependencies between documents, sentences, and words, these graph-based neural approaches aim to generate more coherent and informative summaries.", "citations": [{"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. Our system, based on LexRank ranked in first place in more than one task in the recent DUC 2004 evaluation. In this paper we present a detailed analysis of our approach and apply it to a larger data set including data from earlier DUC evaluations. We discuss several methods to compute centrality using the similarity graph. The results show that degree-based methods (including LexRank) outperform both centroid-based methods and other systems participating in DUC in most of the cases. Furthermore, the LexRank with threshold method outperforms the other degree-based techniques including continuous LexRank. We also show that our approach is quite insensitive to the noise in the data that may result from an imperfect topical clustering of documents."], "score": 0.0}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.96435546875}, {"id": "(Li et al., 2014)", "paper": {"corpus_id": 10112929, "title": "Improving Multi-documents Summarization by Sentence Compression based on Expanded Constituent Parse Trees", "year": 2014, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Li", "authorId": "40475614"}, {"name": "Yang Liu", "authorId": "2152797181"}, {"name": "Fei Liu", "authorId": "144544919"}, {"name": "Lin Zhao", "authorId": "46586837"}, {"name": "F. Weng", "authorId": "1807350"}], "n_citations": 46}, "snippets": ["In this paper, we focus on the problem of using sentence compression techniques to improve multi-document summarization. We propose an innovative sentence compression method by considering every node in the constituent parse tree and deciding its status \u2013 remove or retain. Integer liner programming with discriminative training is used to solve the problem. Under this model, we incorporate various constraints to improve the linguistic quality of the compressed sentences. Then we utilize a pipeline summarization framework where sentences are first compressed by our proposed compression model to obtain top-n candidates and then a sentence selection module is used to generate the final summary. Compared with state-ofthe-art algorithms, our model has similar ROUGE-2 scores but better linguistic quality on TAC data."], "score": 0.0}, {"id": "(Golestani et al., 2022)", "paper": {"corpus_id": 247446769, "title": "Pruned Graph Neural Network for Short Story Ordering", "year": 2022, "venue": "International Conference of the Italian Association for Artificial Intelligence", "authors": [{"name": "Melika Golestani", "authorId": "2124373879"}, {"name": "Zeinab Borhanifard", "authorId": "19218725"}, {"name": "Farnaz Tahmasebian", "authorId": "51133923"}, {"name": "Heshaam Faili", "authorId": "3054779"}], "n_citations": 0}, "snippets": ["Graph neural networks (GNN) have shown to be effective in NLP applications, including syntactic dependency trees (Marcheggiani and Titov, 2017), neural machine translation (Beck et al., 2018), knowledge graphs (Wang et al., 2018), semantic graphs (Song et al., 2018), sequence-to-graph learning (Johnson, 2017), graph-to-sequence learning (Beck et al., 2018), sentence ordering (Yin et al., 2019), and multi-document summarization (Christensen et al., 2013;Yasunaga et al., 2017). \n\nChristensen et al. (2013) used a GNN in multi-document summarization. They create multi-document graphs which determine pairwise ordering constraints of sentences based on the discourse relationship between them. Kipf and Welling (2017) proposed Graph Convolutional Networks (GCN), which is used in Yasunaga et al. (2017) to generate sentence relation graphs. The final sentence embeddings indicate the graph representation and are utilized as inputs to achieve satisfactory results on multi-document summarization."], "score": 0.97265625}, {"id": "(See et al., 2017)", "paper": {"corpus_id": 8314118, "title": "Get To The Point: Summarization with Pointer-Generator Networks", "year": 2017, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "A. See", "authorId": "13070498"}, {"name": "Peter J. Liu", "authorId": "35025299"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 4028}, "snippets": ["Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."], "score": 0.0}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 203641742, "title": "Topic-Aware Pointer-Generator Networks for Summarizing Spoken Conversations", "year": 2019, "venue": "Automatic Speech Recognition & Understanding", "authors": [{"name": "Zhengyuan Liu", "authorId": "49293155"}, {"name": "A. Ng", "authorId": "114743098"}, {"name": "Sheldon Lee Shao Guang", "authorId": "51129684"}, {"name": "AiTi Aw", "authorId": "1829583"}, {"name": "Nancy F. Chen", "authorId": "2185019"}], "n_citations": 101}, "snippets": ["Due to the lack of publicly available resources, conversation summarization has received far less attention than text summarization. As the purpose of conversations is to exchange information between at least two interlocutors, key information about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers. This phenomenon is more pronounced during spoken conversations, where speech characteristics such as backchanneling and false-starts might interrupt the topical flow. Moreover, topic diffusion and (intra-utterance) topic drift are also more common in human-to-human conversations. Such linguistic characteristics of dialogue topics make sentence-level extractive summarization approaches used in spoken documents ill-suited for summarizing conversations. Pointer-generator networks have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural modeling in text summarization. To the best of our knowledge, to date no one has adopted it for summarizing conversations. In this work, we propose a topic-aware architecture to exploit the inherent hierarchical structure in conversations to further adapt the pointer-generator model. Our approach significantly outperforms competitive baselines, achieves more efficient learning outcomes, and attains more robust performance."], "score": 0.0}, {"id": "(Song et al., 2018)", "paper": {"corpus_id": 46936631, "title": "Structure-Infused Copy Mechanisms for Abstractive Summarization", "year": 2018, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Kaiqiang Song", "authorId": "50982080"}, {"name": "Lin Zhao", "authorId": "46586837"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 76}, "snippets": ["Seq2seq learning has produced promising results on summarization. However, in many cases, system summaries still struggle to keep the meaning of the original intact. They may miss out important words or relations that play critical roles in the syntactic structure of source sentences. In this paper, we present structure-infused copy mechanisms to facilitate copying important words and relations from the source sentence to summary sentence. The approach naturally combines source dependency structure with the copy mechanism of an abstractive sentence summarizer. Experimental results demonstrate the effectiveness of incorporating source-side syntactic information in the system, and our proposed approach compares favorably to state-of-the-art methods."], "score": 0.0}, {"id": "(Hardy et al., 2018)", "paper": {"corpus_id": 52111191, "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hardy Hardy", "authorId": "46441201"}, {"name": "Andreas Vlachos", "authorId": "2064056928"}], "n_citations": 69}, "snippets": ["Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset."], "score": 0.0}, {"id": "(Pilault et al., 2020)", "paper": {"corpus_id": 202541012, "title": "On Extractive and Abstractive Neural Document Summarization with Transformer Language Models", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jonathan Pilault", "authorId": "104354626"}, {"name": "Raymond Li", "authorId": "144235909"}, {"name": "Sandeep Subramanian", "authorId": "50324141"}, {"name": "C. Pal", "authorId": "1972076"}], "n_citations": 221}, "snippets": ["We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores. We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two. We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets. We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts. We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance. We hope that these architectures and experiments may serve as strong points of comparison for future work. Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper."], "score": 0.0}, {"id": "(Wang et al., 2020)", "paper": {"corpus_id": 229171633, "title": "Exploring Explainable Selection to Control Abstractive Summarization", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Haonan Wang", "authorId": null}, {"name": "Yang Gao", "authorId": "145644809"}, {"name": "Yu Bai", "authorId": "144843219"}, {"name": "Mirella Lapata", "authorId": "1747893"}, {"name": "Heyan Huang", "authorId": "4590286"}], "n_citations": 17}, "snippets": ["Like humans, document summarization models can interpret a document\u2019s contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and BERT-based configurations. In a series of experiments assessed with ROUGE metrics and two human evaluations, ESCA outperformed eight state-of-the-art models on the CNN/DailyMail and NYT50 benchmark datasets."], "score": 0.0}, {"id": "(Fonseca et al., 2022)", "paper": {"corpus_id": 249062562, "title": "Factorizing Content and Budget Decisions in Abstractive Summarization of Long Documents", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Marcio Fonseca", "authorId": "2142910610"}, {"name": "Yftah Ziser", "authorId": "7264689"}, {"name": "Shay B. Cohen", "authorId": "40146204"}], "n_citations": 14}, "snippets": ["We argue that disentangling content selection from the budget used to cover salient content improves the performance and applicability of abstractive summarizers. Our method, FactorSum, does this disentanglement by factorizing summarization into two steps through an energy function: (1) generation of abstractive summary views covering salient information in subsets of the input document (document views); (2) combination of these views into a final summary, following a budget and content guidance. This guidance may come from different sources, including from an advisor model such as BART or BigBird, or in oracle mode \u2013 from the reference. This factorization achieves significantly higher ROUGE scores on multiple benchmarks for long document summarization, namely PubMed, arXiv, and GovReport. Most notably, our model is effective for domain adaptation. When trained only on PubMed samples, it achieves a 46.29 ROUGE-1 score on arXiv, outperforming PEGASUS trained in domain by a large margin. Our experimental results indicate that the performance gains are due to more flexible budget adaptation and processing of shorter contexts provided by partial document views."], "score": 0.0}, {"id": "(Wang et al._1, 2020)", "paper": {"corpus_id": 216552978, "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Danqing Wang", "authorId": "49371126"}, {"name": "Pengfei Liu", "authorId": "144118452"}, {"name": "Y. Zheng", "authorId": "3337238"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Xuanjing Huang", "authorId": "1790227"}], "n_citations": 282}, "snippets": ["As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."], "score": 0.0}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 269791107, "title": "Hierarchical Attention Graph for Scientific Document Summarization in Global and Local Level", "year": 2024, "venue": "NAACL-HLT", "authors": [{"name": "Chenlong Zhao", "authorId": "2294460391"}, {"name": "Xiwen Zhou", "authorId": "2294514721"}, {"name": "Xiaopeng Xie", "authorId": "2295151257"}, {"name": "Yong Zhang", "authorId": "2294558459"}], "n_citations": 5}, "snippets": ["Graph neural networks have been widely used for extractive summarization due to their flexibility and scalability.Dong et al. (2020) proposed an unsupervised graph-based model that combines both sentence similarity and hierarchical discourse structure to rank sentences.Cui et al. (2020) injected latent topic information into graph neural networks to further improve performance.(Wang et al., 2020) constructed a word-document heterogeneous graph using word nodes as intermediate to connect sentences.Zhang et al. (2022) proposed a hypergraph transformer to model long-distance dependency while emphasizing the importance of high-order inter-sentence relations in extraction summarization."], "score": 0.97802734375}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 252780923, "title": "HEGEL: Hypergraph Transformer for Long Document Summarization", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Haopeng Zhang", "authorId": "2135688409"}, {"name": "Xiao Liu", "authorId": null}, {"name": "Jiawei Zhang", "authorId": "2168548350"}], "n_citations": 45}, "snippets": ["The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure", "To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices", "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations."], "score": 0.97119140625}, {"id": "(Qiu et al., 2022)", "paper": {"corpus_id": 253155963, "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yifu Qiu", "authorId": "2159539050"}, {"name": "Shay B. Cohen", "authorId": "40146204"}], "n_citations": 13}, "snippets": ["Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)", "we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information."], "score": 0.97607421875}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.0}, {"id": "(Jin et al., 2020)", "paper": {"corpus_id": 220045815, "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Tian-ming Wang", "authorId": "1751960"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 99}, "snippets": ["In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."], "score": 0.0}, {"id": "(Cui et al., 2021)", "paper": {"corpus_id": 239050558, "title": "Topic-Guided Abstractive Multi-Document Summarization", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Peng Cui", "authorId": "143738684"}, {"name": "Le Hu", "authorId": "2109312896"}], "n_citations": 41}, "snippets": ["A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 257496469, "title": "Compressed Heterogeneous Graph for Abstractive Multi-Document Summarization", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Miao Li", "authorId": "2143544766"}, {"name": "Jianzhong Qi", "authorId": "2149459181"}, {"name": "Jey Han Lau", "authorId": "1800564"}], "n_citations": 11}, "snippets": ["Graph-Based Models Although graphs are commonly used to boost text summarization (Wu et al., 2021)(You et al., 2022)(Song et al., 2022), there are only a handful of models which have been proposed to use graphs to encode the documents in abstractive MDS (Li et al., 2020)(Jin et al., 2020)(Li et al., 2021)(Cui et al., 2021). Most of these models only leverage homogeneous graphs as they do not consider different edge types of graphs. For example, MGSum (Jin et al., 2020) constructs a threelevel (i.e., document, sentence, and word levels) hierarchical graph and learns semantics with a multi-level interaction network. GraphSum (Li et al., 2020)) constructs a similarity graph over the paragraphs. It learns a graph representation for the paragraphs and uses a hierarchical graph attention mechanism to guide the summary generation process."], "score": 0.97021484375}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 235187330, "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wenhao Wu", "authorId": "2139644141"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Ziqiang Cao", "authorId": "2314396"}, {"name": "Sujian Li", "authorId": "48831399"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 45}, "snippets": ["Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."], "score": 0.0}], "table": null}, {"title": "Recent Graph Neural Network Architectures", "tldr": "Recent graph-based neural approaches for abstractive multi-document summarization feature diverse architectures including heterogeneous graphs, hypergraphs, and attention mechanisms to better model cross-document relationships. These models leverage various edge types, node granularities, and specialized encoder-decoder frameworks to improve summary generation quality. (10 sources)", "text": "\n- **GraphSum** incorporates document similarity graphs built over paragraphs to capture cross-document relations, crucial for summarizing long documents. It leverages graphs during both encoding and decoding phases, and can be combined with pre-trained language models for improved performance. <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>\n\n- **MGSum** constructs a three-level hierarchical graph spanning document, sentence, and word levels, learning semantics through a multi-level interaction network. This approach allows the model to capture relationships at different textual granularities. <Paper corpusId=\"220045815\" paperTitle=\"(Jin et al., 2020)\" isShortName></Paper>\n\n- **HeterGNN** represents multiple documents as a heterogeneous graph, considering semantic nodes of different granularities. This approach better captures the complex relationships between document elements and improves information selection. <Paper corpusId=\"239050558\" paperTitle=\"(Cui et al., 2021)\" isShortName></Paper>\n\n- **HEGEL** (HypErGraph transformer for Extractive Long document summarization) uses hypergraph neural networks to capture high-order cross-sentence relations in long documents. The model updates and learns effective sentence representations through hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keyword coreference, and section structure. <Paper corpusId=\"252780923\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>\n\n- **HierGNN** presents a hierarchy-aware graph neural network that learns document structure through a latent structure tree. The model captures dependencies through three main steps: learning hierarchical document structure, propagating sentence information over this structure to identify salient content, and using graph-level attention to guide the decoder. <Paper corpusId=\"253155963\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>\n\n- **GTASum** (Graph-Based Topic-Aware abstract text summarization) constructs a heterogeneous document graph containing sentence representation nodes and potential topic nodes. The model first encodes documents with BERT while using a Neural Topic Model to learn potential topics, then revises these representations using a Graph Attention Network before feeding them into a Transformer-based decoder. <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>\n\n- **KGSum** leverages knowledge graphs for multi-document scientific summarization, incorporating them in both encoding and decoding processes. Two graph-based modules incorporate knowledge graph information into paper encoding, while a two-stage decoder first generates knowledge graph information as descriptive sentences before producing the final summary. <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>\n\n- **SACA** enhances graph structure integration during decoding by re-encoding the input graph. Unlike GraphSum, which only introduces one graph attention layer in each decoder layer, SACA re-computes the input graph representation by conditioning it on newly generated text at each decoding step. <Paper corpusId=\"252280335\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n\n- **BASS** (Boosting Abstractive Summarization based on a unified Semantic graph) aggregates co-referent phrases distributed across a wide range of context and conveys rich relations between phrases. This approach helps analyze long-distance relations in text for both long-document and multi-document summarization. <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>\n\n- **HeterTls** presents a joint learning-based heterogeneous graph attention network for timeline summarization, combining date selection and event detection into a unified framework. The heterogeneous graph involves multiple types of nodes with representations iteratively learned across heterogeneous graph attention layers. <Paper corpusId=\"250390945\" paperTitle=\"(You et al., 2022)\" isShortName></Paper>", "citations": [{"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.0}, {"id": "(Jin et al., 2020)", "paper": {"corpus_id": 220045815, "title": "Multi-Granularity Interaction Network for Extractive and Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hanqi Jin", "authorId": "1491212422"}, {"name": "Tian-ming Wang", "authorId": "1751960"}, {"name": "Xiaojun Wan", "authorId": "145078589"}], "n_citations": 99}, "snippets": ["In this paper, we propose a multi-granularity interaction network for extractive and abstractive multi-document summarization, which jointly learn semantic representations for words, sentences, and documents. The word representations are used to generate an abstractive summary while the sentence representations are used to produce an extractive summary. We employ attention mechanisms to interact between different granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset."], "score": 0.0}, {"id": "(Cui et al., 2021)", "paper": {"corpus_id": 239050558, "title": "Topic-Guided Abstractive Multi-Document Summarization", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Peng Cui", "authorId": "143738684"}, {"name": "Le Hu", "authorId": "2109312896"}], "n_citations": 41}, "snippets": ["A critical point of multi-document summarization (MDS) is to learn the relations among various documents. In this paper, we propose a novel abstractive MDS model, in which we represent multiple documents as a heterogeneous graph, taking semantic nodes of different granularities into account, and then apply a graph-to-sequence framework to generate summaries. Moreover, we employ a neural topic model to jointly discover latent topics that can act as cross-document semantic units to bridge different documents and provide global information to guide the summary generation. Since topic extraction can be viewed as a special type of summarization that\"summarizes\"texts into a more abstract format, i.e., a topic distribution, we adopt a multi-task learning strategy to jointly train the topic and summarization module, allowing the promotion of each other. Experimental results on the Multi-News dataset demonstrate that our model outperforms previous state-of-the-art MDS models on both Rouge metrics and human evaluation, meanwhile learns high-quality topics."], "score": 0.0}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 252780923, "title": "HEGEL: Hypergraph Transformer for Long Document Summarization", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Haopeng Zhang", "authorId": "2135688409"}, {"name": "Xiao Liu", "authorId": null}, {"name": "Jiawei Zhang", "authorId": "2168548350"}], "n_citations": 45}, "snippets": ["The paper proposes HEGEL, a hypergraph neural network for long document summarization by capturing high-order cross-sentence relations. HEGEL updates and learns effective sentence representations with hypergraph transformer layers and fuses different types of sentence dependencies, including latent topics, keywords coreference, and section structure", "To address the above issues, we propose HEGEL (HypErGraph transformer for Extractive Long document summarization), a graph-based model designed for summarizing long documents with rich discourse information. To better model high-order cross-sentence relations, we represent a document as a hypergraph, a generalization of graph structure, in which an edge can join any number of vertices", "Graph-based models have been exploited for extractive summarization to capture cross-sentence dependencies. Unsupervised graph summarization methods rely on graph connectivity to score and rank sentences Zheng and Lapata, 2019;Dong et al., 2020). Researchers also explore supervised graph neural networks for summarization. Yasunaga et al. (2017) applied Graph Convolutional Network (GCN) on the approximate discourse graph. Xu et al. (2019) proposed to apply GCN on structural discourse graphs based on RST trees and coreference mentions. Cui et al. (2020) leveraged topical information by building topicsentence graphs. Recently, Wang et al. (2020) proposed to construct word-document heterogeneous graphs and use word nodes as the intermediary between sentences. Jing et al. (2021) proposed to use multiplex graph to consider different sentence relations."], "score": 0.97119140625}, {"id": "(Qiu et al., 2022)", "paper": {"corpus_id": 253155963, "title": "Abstractive Summarization Guided by Latent Hierarchical Document Structure", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yifu Qiu", "authorId": "2159539050"}, {"name": "Shay B. Cohen", "authorId": "40146204"}], "n_citations": 13}, "snippets": ["Neural Abstractive Summarization (Rush et al., 2015) first proposed to use a sequence-to-sequence model with an attention mechanism to perform sentence compression. (Mendes et al., 2019) demonstrated the advantages and limitations of neural methods based on sentence compression. The pointer-generator networks (PGN; (See et al., 2017)) enhances the attention model with a copying functionality. PGN has also been further extended to create summarization systems by incorporating the topic information (Liu et al., 2019), document structural information (Song et al., 2018), semantic information (Hardy et al., 2018), and was improved by replacing the plain LSTM module with the more advanced Transformer model to overcome the difficulty in modeling long sequence input (Pilault et al., 2020)(Wang et al., 2020)(Fonseca et al., 2022)", "we propose a hierarchy-aware graph neural network (HierGNN) which captures such dependencies through three main steps: 1) learning a hierarchical document structure through a latent structure tree learned by a sparse matrix-tree computation; 2) propagating sentence information over this structure using a novel message-passing node propagation mechanism to identify salient information; 3) using graph-level attention to concentrate the decoder on salient information."], "score": 0.97607421875}, {"id": "(Jiang et al., 2022)", "paper": {"corpus_id": 250012667, "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization", "year": 2022, "venue": "Information Technology and Control", "authors": [{"name": "Ming Jiang", "authorId": "2040863712"}, {"name": "Yifan Zou", "authorId": "2034370007"}, {"name": "Jian Xu", "authorId": "2110980173"}, {"name": "Min Zhang", "authorId": "2156053270"}], "n_citations": 11}, "snippets": ["In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries."], "score": 0.9765625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 252185277, "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Kunyuan Pang", "authorId": "2116489"}, {"name": "Liangliang He", "authorId": "50670961"}, {"name": "Dong Li", "authorId": "2108821455"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "38972135"}], "n_citations": 15}, "snippets": ["Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset", ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."], "score": 0.98095703125}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 252280335, "title": "Graph-to-Text Generation with Dynamic Structure Pruning", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Liang Li", "authorId": "2154884699"}, {"name": "Ruiying Geng", "authorId": "9706609"}, {"name": "Bowen Li", "authorId": "2132475886"}, {"name": "Can Ma", "authorId": "2112563315"}, {"name": "Yinliang Yue", "authorId": "35755264"}, {"name": "Binhua Li", "authorId": "66200440"}, {"name": "Yongbin Li", "authorId": "2323761746"}], "n_citations": 2}, "snippets": ["A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step."], "score": 0.9609375}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 235187330, "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wenhao Wu", "authorId": "2139644141"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Ziqiang Cao", "authorId": "2314396"}, {"name": "Sujian Li", "authorId": "48831399"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 45}, "snippets": ["Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."], "score": 0.0}, {"id": "(You et al., 2022)", "paper": {"corpus_id": 250390945, "title": "Joint Learning-based Heterogeneous Graph Attention Network for Timeline Summarization", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jingyi You", "authorId": "2072795135"}, {"name": "Dongyuan Li", "authorId": "2115198544"}, {"name": "Hidetaka Kamigaito", "authorId": "2300756"}, {"name": "Kotaro Funakoshi", "authorId": "1747395"}, {"name": "M. Okumura", "authorId": "144859189"}], "n_citations": 12}, "snippets": ["Previous studies on the timeline summarization (TLS) task ignored the information interaction between sentences and dates, and adopted pre-defined unlearnable representations for them. They also considered date selection and event detection as two independent tasks, which makes it impossible to integrate their advantages and obtain a globally optimal summary. In this paper, we present a joint learning-based heterogeneous graph attention network for TLS (HeterTls), in which date selection and event detection are combined into a unified framework to improve the extraction accuracy and remove redundant sentences simultaneously. Our heterogeneous graph involves multiple types of nodes, the representations of which are iteratively learned across the heterogeneous graph attention layer. We evaluated our model on four datasets, and found that it significantly outperformed the current state-of-the-art baselines with regard to ROUGE scores and date selection metrics."], "score": 0.0}], "table": null}, {"title": "Heterogeneous Graph Networks", "tldr": "Heterogeneous graph networks for multi-document summarization incorporate nodes of different types and granularity levels to better capture cross-document relationships. These models represent documents as complex graphs with various node types (sentences, words, entities, topics) and edge relationships that enable richer information exchange and improve summary quality. (11 sources)", "text": "\nHeterogeneous graph neural networks (HGNNs) represent a significant advancement in graph-based approaches for multi-document summarization by introducing diverse node types that go beyond basic sentence relationships. Unlike homogeneous graphs that only model sentence-to-sentence connections, heterogeneous graphs incorporate nodes of varying granularity levels, creating richer representations of document content and relationships <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nHETERSUMGRAPH pioneered this approach by introducing semantic nodes of different granularity levels alongside sentence nodes. These additional nodes serve as intermediaries between sentences, enriching cross-sentence relationships and enabling more effective information exchange <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"265607988\" paperTitle=\"(Roy et al., 2023)\" isShortName></Paper>. A key advantage of this architecture is its flexibility in extending from single-document to multi-document summarization through the addition of document-level nodes <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>.\n\nThe heterogeneous graph structure allows different sentences to interact through overlapping word information, which helps capture semantic connections that might otherwise be missed in sentence-only graphs <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This is particularly valuable in multi-document summarization, where information is often distributed sparsely across multiple documents, challenging the effectiveness of sentence-level models <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nMore advanced heterogeneous graph models have further extended this approach. EMSum augments the Transformer architecture with a knowledge graph consisting of text units and entities as nodes, utilizing Graph Attention Networks to capture cross-document information and identify relevant content across multiple documents <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. Similarly, BART-Long-Graph integrates graphical information with the pre-trained BART model using a separate graph encoder, demonstrating significant improvements on benchmark datasets like Multi-News and DUC-2004 <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\nRecent heterogeneous graph approaches have also incorporated topic modeling to enhance document representation. Models like GTASum construct heterogeneous document graphs containing both sentence representation nodes and topic nodes <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. These models initially encode documents with BERT while simultaneously employing Neural Topic Models to identify potential topics, then refine these representations using Graph Attention Networks before generating summaries <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper> <Paper corpusId=\"233169121\" paperTitle=\"(An et al., 2021)\" isShortName></Paper>.\n\nThe Multi-GraS model further demonstrates the versatility of heterogeneous graphs by considering various types of intersentential relations (like semantic similarity and natural connections) alongside intrasentential relations between words <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>. This multi-layered approach captures both document-level structure and fine-grained semantic relationships, leading to more comprehensive document representations.\n\nAnother innovative approach involves integrating passage nodes into heterogeneous graph structures alongside word and sentence nodes <Paper corpusId=\"277284940\" paperTitle=\"(Umair et al., 2025)\" isShortName></Paper>. This multi-level architecture helps models better understand document structure and identify important information across different granularity levels, improving summarization performance particularly for long or complex documents.\n\nThe evolution from homogeneous graphs with static nodes to heterogeneous networks with diverse node types represents a significant trend in modern text summarization research <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. By capturing a wider range of textual elements and enabling dynamic updates during the summarization process, heterogeneous graph networks provide a powerful framework for addressing the complex challenges of multi-document summarization.", "citations": [{"id": "(Wang et al._1, 2020)", "paper": {"corpus_id": 216552978, "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Danqing Wang", "authorId": "49371126"}, {"name": "Pengfei Liu", "authorId": "144118452"}, {"name": "Y. Zheng", "authorId": "3337238"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Xuanjing Huang", "authorId": "1790227"}], "n_citations": 282}, "snippets": ["As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."], "score": 0.0}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264146402, "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review", "year": 2023, "venue": "Discret. Math. Algorithms Appl.", "authors": [{"name": "Guanghua Wang", "authorId": "2258949345"}, {"name": "Weili Wu", "authorId": "2258731583"}], "n_citations": 4}, "snippets": ["A heterogeneous GNN, HETERSUMGRAPH (Wang et al., 2020) is introduced for extractive document summarization. This network includes nodes of different granularity levels apart from sentences, which act as intermediaries and enrich cross-sentence relations. This approach allows different sentences to interact considering overlapping word information. Moreover, the graph network can accommodate additional node types, such as document nodes for multi-document summarization."], "score": 0.974609375}, {"id": "(Roy et al., 2023)", "paper": {"corpus_id": 265607988, "title": "Generating Extractive and Abstractive Summaries in Parallel from Scientific Articles Incorporating Citing Statements", "year": 2023, "venue": "NEWSUM", "authors": [{"name": "Sudipta Singha Roy", "authorId": "2164696118"}, {"name": "Robert E. Mercer", "authorId": "2269458234"}], "n_citations": 0}, "snippets": ["(Wang et al., 2020) introduced an heterogeneous graph neural network for extractive summarization which used additional semantic units (words) as intermediate nodes to construct relationships between sentences", "Recently, state-ofthe-art solutions on abstractive summarization are built upon the transformer (Vaswani et al., 2017) and BERT (Devlin et al., 2019) models."], "score": 0.962890625}, {"id": "(Zesheng et al., 2023)", "paper": {"corpus_id": 257219819, "title": "Topic-Selective Graph Network for Topic-Focused Summarization", "year": 2023, "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "authors": [{"name": "Shi Zesheng", "authorId": "1764406"}, {"name": "Yucheng Zhou", "authorId": "2110348767"}], "n_citations": 4}, "snippets": ["Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."], "score": 0.97119140625}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 266244733, "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks", "year": 2023, "venue": "PeerJ Computer Science", "authors": [{"name": "Wu Su", "authorId": "2274342252"}, {"name": "Jin Jiang", "authorId": "2274904693"}, {"name": "Kaihui Huang", "authorId": "2274639188"}], "n_citations": 1}, "snippets": ["The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes."], "score": 0.97314453125}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.96484375}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": [","], "score": 0.0}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents."], "score": 0.0}, {"id": "(Khaliq et al., 2024)", "paper": {"corpus_id": 271938110, "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "A. Khaliq", "authorId": "2058964352"}, {"name": "Atif Khan", "authorId": "2149073250"}, {"name": "Salman Afsar Awan", "authorId": "2316766711"}, {"name": "Salman Jan", "authorId": "2316764100"}, {"name": "Muhammad Umair", "authorId": "2257111507"}, {"name": "M. Zuhairi", "authorId": "38509860"}], "n_citations": 1}, "snippets": ["In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."], "score": 0.9892578125}, {"id": "(An et al., 2021)", "paper": {"corpus_id": 233169121, "title": "Enhancing Scientific Papers Summarization with Citation Graph", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Chen An", "authorId": "2064164220"}, {"name": "Ming Zhong", "authorId": "1606040932"}, {"name": "Yiran Chen", "authorId": "2135836407"}, {"name": "Danqing Wang", "authorId": "49371126"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Xuanjing Huang", "authorId": "1790227"}], "n_citations": 44}, "snippets": ["Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network.\nHowever, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community.\nIn this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references.\nIn addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph.\nExtensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture.\nThe results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries."], "score": 0.0}, {"id": "(Umair et al., 2025)", "paper": {"corpus_id": 277284940, "title": "Global and Local Context Fusion in Heterogeneous Graph Neural Network for Summarizing Lengthy Scientific Documents", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Muhammad Umair", "authorId": "2257111507"}, {"name": "Atif Khan", "authorId": "2149073250"}, {"name": "Fasee Ullah", "authorId": "47122876"}, {"name": "Atef Masmoudi", "authorId": "2282843278"}, {"name": "Muhammed Faheem", "authorId": "2299236773"}], "n_citations": 1}, "snippets": ["In the domain of graph-based approaches, [3] implement a modified version of the GAT designed to identify inter-sentence relationships. To enhance the summarization of lengthy documents, they implemented Neural Topic Modeling (NTM) to identify latent topics. This approach introduced NTM as a new node type, which was then integrated with existing sentence nodes to improve the overall structure and accuracy of the summarization process.\n\nThe researchers of [7] integrated passage nodes into the heterogeneous graph structure alongside the existing word and sentence nodes. In the study [34], the authors attempted to redefine the task of review formalization, focusing on scientific paper summarization using citation graphs. The CGSUM model included information from both the source paper and its references, and enhanced the contextual relevance of summarization. The authors also built the Semantic Scholar Network (SSN) from 141K research papers and 661K citation relations with a broad view of scientific domains."], "score": 0.9765625}], "table": null}, {"title": "Topic-aware Graph Models", "tldr": "Topic-aware graph models enhance multi-document summarization by incorporating latent topic information into graph structures, allowing better representation of document content relationships. These models typically combine neural topic modeling with graph neural networks to discover semantic themes across documents and use this information to guide the summarization process. (6 sources)", "text": "\nTopic-aware graph models represent a significant advancement in graph-based neural approaches for multi-document summarization by integrating topic modeling with graph neural networks. These models address a fundamental challenge in multi-document summarization: understanding the thematic relationships across documents and identifying the most relevant information based on these themes.\n\nThe GTASum (Graph-Based Topic-Aware abstract text summarization) model exemplifies this approach by constructing a heterogeneous document graph containing both sentence representation nodes and potential topic nodes <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>. This model employs a dual-process approach, simultaneously encoding documents with BERT to obtain contextual sentence representations while using Neural Topic Models (NTM) to learn potential topics within the documents <Paper corpusId=\"52182945\" paperTitle=\"(Ding et al., 2018)\" isShortName></Paper>. The sentence representations are then revised using a Graph Attention Network before being fed into a Transformer-based decoder to generate summaries <Paper corpusId=\"250012667\" paperTitle=\"(Jiang et al., 2022)\" isShortName></Paper>.\n\nSimilar approaches have been developed by other researchers, such as Huang et al., who built a sentence-level graph-based model that combines BERT for sentence encoding with joint neural topic modeling for discovering latent topic information <Paper corpusId=\"233189611\" paperTitle=\"(Huang et al., 2021)\" isShortName></Paper>. This integration helps capture global information and thematic relationships that might be missed in purely structural approaches <Paper corpusId=\"253763913\" paperTitle=\"(Umair et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of topic-aware graph models stems from their ability to address the challenge of sparsely distributed information in documents. By incorporating topic modeling, these approaches can better handle relationships between distantly-placed sentences and capture significant information that might otherwise be omitted in sentence-level models <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>. Topic models are particularly valuable for extracting topic-specific features and establishing probability distributions that link topics with sentence nodes, providing a more comprehensive understanding of document content <Paper corpusId=\"266244733\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nRecent research has explored various approaches to topic integration in graph structures. Some models use K-Means and Gaussian Mixture Models for topic extraction, while others employ Neural Topic Models <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. Models like Topic-GraphSum, SSN-DM, HeterGraphLongSum, and GTASum all leverage graph structures to represent relationships between sentences and topics, facilitating more effective summarization <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>.\n\nThe evolution of topic-aware graph models reflects the broader trend in text summarization research: moving from homogeneous graphs with static nodes to heterogeneous networks that can represent a wider range of textual elements and allow for dynamic updates during the summarization process <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>. By combining the structural modeling capabilities of graph neural networks with the semantic understanding provided by topic models, these approaches offer a more comprehensive framework for addressing the complex challenges of multi-document summarization.", "citations": [{"id": "(Jiang et al., 2022)", "paper": {"corpus_id": 250012667, "title": "GATSum: Graph-Based Topic-Aware Abstract Text Summarization", "year": 2022, "venue": "Information Technology and Control", "authors": [{"name": "Ming Jiang", "authorId": "2040863712"}, {"name": "Yifan Zou", "authorId": "2034370007"}, {"name": "Jian Xu", "authorId": "2110980173"}, {"name": "Min Zhang", "authorId": "2156053270"}], "n_citations": 11}, "snippets": ["In this work, we suggest a novel GTASum model (Graph-Based Topic-Aware abstract text summarization). First, the document is encoded with BERT (\u67f4\u7530, 2020) (Bidirectional Encoder Representation from Transformers) to obtain contextual sentence representations; meanwhile, NTM (Ding et al., 2018) is used to learn the potential topic of the document. Second, construct a heterogeneous document graph that contains sentence representation nodes and potential topic nodes, then revise them using a adapted GAT [30]. Finally, the sentence representations containing topic information are fed into a Transformer-based decoder to generate summaries."], "score": 0.9765625}, {"id": "(Ding et al., 2018)", "paper": {"corpus_id": 52182945, "title": "Coherence-Aware Neural Topic Modeling", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ran Ding", "authorId": "2058085578"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 79}, "snippets": ["Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence."], "score": 0.0}, {"id": "(Huang et al., 2021)", "paper": {"corpus_id": 233189611, "title": "Extractive Summarization Considering Discourse and Coreference Relations based on Heterogeneous Graph", "year": 2021, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yin Jou Huang", "authorId": "1796312335"}, {"name": "S. Kurohashi", "authorId": "1795664"}], "n_citations": 28}, "snippets": ["Modeling the relations between text spans in a document is a crucial yet challenging problem for extractive summarization. Various kinds of relations exist among text spans of different granularity, such as discourse relations between elementary discourse units and coreference relations between phrase mentions. In this paper, we propose a heterogeneous graph based model for extractive summarization that incorporates both discourse and coreference relations. The heterogeneous graph contains three types of nodes, each corresponds to text spans of different granularity. Experimental results on a benchmark summarization dataset verify the effectiveness of our proposed method."], "score": 0.0}, {"id": "(Umair et al., 2022)", "paper": {"corpus_id": 253763913, "title": "N-GPETS: Neural Attention Graph-Based Pretrained Statistical Model for Extractive Text Summarization", "year": 2022, "venue": "Computational Intelligence and Neuroscience", "authors": [{"name": "Muhammad Umair", "authorId": "2257111507"}, {"name": "I. Alam", "authorId": "46487765"}, {"name": "Atif Khan", "authorId": "2149073250"}, {"name": "Inayat Khan", "authorId": "143835958"}, {"name": "Niamat Ullah", "authorId": "2175498951"}, {"name": "Mohammad Yusuf Momand", "authorId": "2191729128"}], "n_citations": 3}, "snippets": ["The authors in [48] presented an approach that modeled redundancy-aware heterogeneous graphs and refned sentence representation using neural networks for extractive summarization. The studies [9]56]", "between sentences are learned. Te work done by (Huang et al., 2021) built a sentence-level graph-based model, using BERT for sentence encoding and joint neural network model (NTM) for discovering latent topic information. Te authors in (Bangotra et al., 2020) proposed a heterogeneous graph structure for modelling crosssentence relationship between sentences. To represent the relationships between the EDUs, they used three diferent types of nodes, including sentence nodes, EDU nodes, and entity nodes, and RST discourse parsing and leverage external discourse expertise to enhance the model's performance."], "score": 0.95947265625}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 266244733, "title": "Multi-granularity adaptive extractive document summarization with heterogeneous graph neural networks", "year": 2023, "venue": "PeerJ Computer Science", "authors": [{"name": "Wu Su", "authorId": "2274342252"}, {"name": "Jin Jiang", "authorId": "2274904693"}, {"name": "Kaihui Huang", "authorId": "2274639188"}], "n_citations": 1}, "snippets": ["The crucial aspect of extractive document summarization lies in understanding the interrelations between sentences. Documents inherently comprise a multitude of sentences, and sentence-level models frequently fail to consider the relationships between distantly-placed sentences, resulting in the omission of significant information in the summary. Moreover, information within documents tends to be distributed sparsely, challenging the efficacy of sentence-level models. In the realm of heterogeneous graph neural networks, it has been observed that semantic nodes with varying levels of granularity encapsulate distinct semantic connections. Initially, the incorporation of edge features into the computation of dynamic graph attention networks is performed to account for node relationships. Subsequently, given the multiplicity of topics in a document or a set of documents, a topic model is employed to extract topic-specific features and the probability distribution linking these topics with sentence nodes."], "score": 0.97314453125}, {"id": "(Khaliq et al., 2024)", "paper": {"corpus_id": 271938110, "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "A. Khaliq", "authorId": "2058964352"}, {"name": "Atif Khan", "authorId": "2149073250"}, {"name": "Salman Afsar Awan", "authorId": "2316766711"}, {"name": "Salman Jan", "authorId": "2316764100"}, {"name": "Muhammad Umair", "authorId": "2257111507"}, {"name": "M. Zuhairi", "authorId": "38509860"}], "n_citations": 1}, "snippets": ["In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."], "score": 0.9892578125}], "table": null}, {"title": "State-of-the-art Models and Results", "tldr": "Recent graph-based neural models for multi-document summarization have achieved impressive performance on benchmark datasets through various architectural innovations. Key advances include integrating knowledge graphs, combining pre-trained language models with graph structures, and developing specialized encoding-decoding mechanisms. (15 sources)", "text": "\n- **GraphSum** has emerged as a significant baseline in graph-based neural approaches for multi-document summarization. The model leverages graph representations of documents such as similarity graphs and discourse graphs to more effectively process multiple input documents. GraphSum utilizes these graphs both during encoding to capture cross-document relations and during the generation process to guide summary production. When combined with pre-trained language models, GraphSum demonstrates substantial improvements over previous baselines on the WikiSum and MultiNews datasets <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>.\n\n- **SACA** (Structure-Aware Cross-Attention) improves upon GraphSum by enhancing how graph structures are integrated during decoding. While GraphSum only introduces one graph attention layer in each decoder layer, SACA re-encodes the input graph by re-computing the graph representation conditioned on newly generated text at each decoding step. This allows for more dynamic incorporation of graph information throughout the generation process <Paper corpusId=\"252280335\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n- **KGSum** centers on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization. In the encoding process, two graph-based modules incorporate knowledge graph information into paper encoding. The decoding process uses a two-stage approach, first generating knowledge graph information as descriptive sentences before producing the final summary. Empirical results show that KGSum brings substantial improvements over baselines including GraphSum, PEGASUS <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>, Pointer-Generator <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>, and BertABS <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper> on the Multi-Xscience dataset <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n- **EMSum** is an entity-aware model that augments the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks. This design enables EMSum to capture cross-document information and identify relevant information among documents. The knowledge graph is constructed using extracted semantic entities through co-reference resolution tools from AllenNLP <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.\n\n- **BART-Long-Graph** combines the pre-trained BART sequence-to-sequence Transformer model <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper> with Longformer's local and global attention mechanisms for encoding long texts. It integrates graphical information through a separate graph encoder. The semantic knowledge graph is constructed using AllenNLP at the document level and OpenIE at the sentence level to capture multi-level semantic information. BART-Long-Graph achieved remarkable results on benchmark datasets including Multi-News <Paper corpusId=\"174799390\" paperTitle=\"(Fabbri et al., 2019)\" isShortName></Paper> and DUC-2004, showing an average 1.8 ROUGE score improvement over previous work <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper> <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.\n\n- **Topic-GraphSum**, **SSN-DM**, **HeterGraphLongSum**, and **GTASum** all utilize graph structures to represent document information and relationships between sentences or topics. These models construct graphs that integrate topic information, with some using Neural Topic Models to identify potential topics and others employing techniques like K-Means and Gaussian Mixture Models for topic extraction. Graph Attention Networks are commonly used to refine sentence and topic node representations before generating summaries <Paper corpusId=\"271938110\" paperTitle=\"(Khaliq et al., 2024)\" isShortName></Paper>.\n\n- **AMR-based models** take a different approach by first generating Abstract Meaning Representation (AMR) graphs for input documents, then extracting summary graphs from these AMR graphs, and finally creating summaries from the summary graphs. This semantic representation approach provides a structured way to capture document meaning for summarization <Paper corpusId=\"248496597\" paperTitle=\"(Wang et al._1, 2022)\" isShortName></Paper>.\n\nPerformance evaluations of these models extend beyond standard ROUGE scores to examine how well each model assimilates and processes linguistic and thematic information across different document types. Comprehensive assessments also analyze performance in relation to various node configurations and graph structures used in competing models <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>. While graph-based models have shown significant advantages, hybrid approaches that combine graph-based extractive modules with neural abstractive sequence-to-sequence architectures have proven particularly effective for challenging multi-document summarization tasks <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.0}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 252280335, "title": "Graph-to-Text Generation with Dynamic Structure Pruning", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Liang Li", "authorId": "2154884699"}, {"name": "Ruiying Geng", "authorId": "9706609"}, {"name": "Bowen Li", "authorId": "2132475886"}, {"name": "Can Ma", "authorId": "2112563315"}, {"name": "Yinliang Yue", "authorId": "35755264"}, {"name": "Binhua Li", "authorId": "66200440"}, {"name": "Yongbin Li", "authorId": "2323761746"}], "n_citations": 2}, "snippets": ["A recently proposed neural abstractive Multi-Document Summarization (MDS) model, Graph-Summ (Li et al., 2020), also considers the input graph structure during decoding. The biggest difference between Graphsum and our proposed SACA is that the former only introduces one graph attention layer in each decoder layer. SACA, on the other hand, injects graph structure into decoding by re-encoding the input graph. Specifically, it re-computes the input graph representation by conditioning it on the newly generated text at each decoding step."], "score": 0.9609375}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 209405420, "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jingqing Zhang", "authorId": "47540100"}, {"name": "Yao Zhao", "authorId": "2143397386"}, {"name": "Mohammad Saleh", "authorId": "144413479"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 2054}, "snippets": ["Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."], "score": 0.0}, {"id": "(See et al., 2017)", "paper": {"corpus_id": 8314118, "title": "Get To The Point: Summarization with Pointer-Generator Networks", "year": 2017, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "A. See", "authorId": "13070498"}, {"name": "Peter J. Liu", "authorId": "35025299"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 4028}, "snippets": ["Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."], "score": 0.0}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 252185277, "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Kunyuan Pang", "authorId": "2116489"}, {"name": "Liangliang He", "authorId": "50670961"}, {"name": "Dong Li", "authorId": "2108821455"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "38972135"}], "n_citations": 15}, "snippets": ["Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset", ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."], "score": 0.98095703125}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP.Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.96484375}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": [","], "score": 0.0}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "M. Lewis", "authorId": "35084211"}, {"name": "Yinhan Liu", "authorId": "11323179"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Marjan Ghazvininejad", "authorId": "2320509"}, {"name": "Abdel-rahman Mohamed", "authorId": "113947684"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 10856}, "snippets": ["We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."], "score": 0.0}, {"id": "(Fabbri et al., 2019)", "paper": {"corpus_id": 174799390, "title": "Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexander R. Fabbri", "authorId": "46255971"}, {"name": "Irene Li", "authorId": "46331602"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Suyi Li", "authorId": "50341789"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 590}, "snippets": ["Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting."], "score": 0.0}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents."], "score": 0.0}, {"id": "(Khaliq et al., 2024)", "paper": {"corpus_id": 271938110, "title": "Integrating Topic-Aware Heterogeneous Graph Neural Network With Transformer Model for Medical Scientific Document Abstractive Summarization", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "A. Khaliq", "authorId": "2058964352"}, {"name": "Atif Khan", "authorId": "2149073250"}, {"name": "Salman Afsar Awan", "authorId": "2316766711"}, {"name": "Salman Jan", "authorId": "2316764100"}, {"name": "Muhammad Umair", "authorId": "2257111507"}, {"name": "M. Zuhairi", "authorId": "38509860"}], "n_citations": 1}, "snippets": ["In recent years, there have been notable achievements in applying Graph Neural Networks (GNNs) or Graph Attention Networks (GAT) [11] to summarize documents [4], [12], [13], [14], [15], [16], [17]. This success arises from their ability to capture complex inter-sentence relationships within documents. Specifically, GNN models demonstrate proficiency in representing complex structural data by encapsulating semantic units (nodes) and their interconnections (edges). Due to these capabilities, there is growing interest in leveraging the GAT framework alongside topic modeling for abstractive summarization tasks. The researchers in [12] proposed a heterogeneous neural graph structure using BERT to obtain contextual sentence representations. They simultaneously trained with latent topics using the Neural Topic Model (NTM) to model global information. The study of [18] proposed an expanded model that employed NTM for abstractive text summarization. \n\nThe authors of [15] introduced a graph structure enriched with latent topical information. They employed K-Means and Gaussian Mixture Models (GMM) for topic extraction. The study of [3] proposed an innovative Graph-Based Topic-Aware abstractive text summarization model. Initially, documents were encoded with BERT to generate sentence representations. Concurrently, NTM identified potential topics, while GAT refined these sentences and topic nodes. Finally, the sentence embeddings, enriched with topic information, were fed into a Transformer-driven decoder to generate summaries.\n\nModern text summarization research has shifted from homogeneous graphs with static nodes to heterogeneous networks. These networks allow for the inclusion of diverse node types, which represent wide range of textual elements. In addition, they allow for dynamic updates during the summarization process. The approach of [4] constructed a graph neural network based on word co-occurrences within the document to capture word-level relationships. The study in [42] utilized syntactic graph convolutional networks (GCNs) to model the non-Euclidean structure of documents.\n\nThis category includes models like Topic-GraphSum [12], SSN-DM [57] HeterGraphLongSum [14] and GTASum [3] that utilize graph structures to represent document information and relationships between sentences or topics, facilitating summarization."], "score": 0.9892578125}, {"id": "(Wang et al._1, 2022)", "paper": {"corpus_id": 248496597, "title": "Large-Scale Multi-Document Summarization with Information Extraction and Compression", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Ning Wang", "authorId": "2152171283"}, {"name": "Han Liu", "authorId": "2140162523"}, {"name": "D. Klabjan", "authorId": "1753376"}], "n_citations": 1}, "snippets": ["Dohare et al. (2018) propose to generate abstractive summaries by first generating AMR graphs (abstract meaning representation) for corresponding input stories, extract summary graphs from the AMR graphs, and lastly create summaries from the summary graphs."], "score": 0.96240234375}, {"id": "(Khaliq et al._1, 2024)", "paper": {"corpus_id": 271791043, "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks", "year": 2024, "venue": "Computers, Materials &amp; Continua", "authors": [{"name": "A. Khaliq", "authorId": "2058964352"}, {"name": "Salman Afsar Awan", "authorId": "2269634624"}, {"name": "Fahad Ahmad", "authorId": "2315394721"}, {"name": "Muhammad Azam Zia", "authorId": "2315423888"}, {"name": "Muhammad Zafar Iqbal", "authorId": "2255632961"}], "n_citations": 2}, "snippets": ["Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities."], "score": 0.97802734375}, {"id": "(Chen et al., 2022)", "paper": {"corpus_id": 250374734, "title": "Two-phase Multi-document Event Summarization on Core Event Graphs", "year": 2022, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Zengjian Chen", "authorId": "48354529"}, {"name": "Jin Xu", "authorId": "2116315442"}, {"name": "M. Liao", "authorId": "145865588"}, {"name": "Tong Xue", "authorId": "2138967543"}, {"name": "Kun He", "authorId": "2190820519"}], "n_citations": 2}, "snippets": ["Abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020). Our model, which also combines a graph-based extractive module and neural abstractive sequence-to-sequence architecture, is a higher level summarization and focuses on the core event summarization from multiple documents, which is more challenging."], "score": 0.9775390625}], "table": null}, {"title": "Datasets and Evaluation", "tldr": "Evaluation of graph-based neural approaches for multi-document summarization relies on several benchmark datasets including Multi-News, WikiSum, DUC, and Multi-Xscience. Performance is typically measured using ROUGE metrics, though more comprehensive assessments increasingly examine models' abilities to process linguistic and thematic information across different document types. (6 sources)", "text": "\nMulti-document summarization models are evaluated across several established benchmark datasets that vary in domain, document length, and summary characteristics. The Multi-Xscience dataset has emerged as an important benchmark for Multi-Document Scientific Summarization (MDSS), specifically designed to evaluate models' ability to process clusters of topic-relevant scientific papers and generate coherent summaries <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This dataset presents particular challenges as it requires precise understanding of paper content and accurate modeling of cross-paper relationships.\n\nWikiSum and MultiNews represent other commonly used benchmarks for evaluating graph-based neural approaches. These datasets have been instrumental in demonstrating the performance improvements achieved by models like GraphSum, which leverages graph representations to capture cross-document relations <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper>. The DUC-2004 dataset, though older, continues to serve as a standard evaluation benchmark, particularly for comparing newer approaches against established baselines.\n\nThe predominant evaluation metrics for multi-document summarization remain the ROUGE scores (Recall-Oriented Understudy for Gisting Evaluation), which measure the overlap of n-grams between system-generated summaries and reference summaries. However, as the field advances, researchers are increasingly employing more comprehensive assessment approaches that go beyond simple ROUGE scores. These evaluations examine how effectively models assimilate and process linguistic and thematic information across different document types <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>.\n\nComparative evaluations typically measure graph-based approaches against strong baselines including PEGASUS <Paper corpusId=\"209405420\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>, a sequence-to-sequence model with gap-sentences generation as a pre-training objective; Pointer-Generator <Paper corpusId=\"8314118\" paperTitle=\"(See et al., 2017)\" isShortName></Paper>, which uses an attention mechanism with the ability to copy words from source documents; and BertABS, which utilizes pre-trained BERT encoders <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. Domain-specific variants, such as SciBertABS with encoders pre-trained on scientific articles, are also used as comparisons for specialized summarization tasks <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nRecent evaluation approaches have become more nuanced, examining performance in relation to various node configurations and graph structures employed by competing models. These comprehensive assessments provide deeper insights into each model's abilities beyond simple performance metrics <Paper corpusId=\"271791043\" paperTitle=\"(Khaliq et al._1, 2024)\" isShortName></Paper>. Such thorough evaluations are particularly valuable as the field continues to advance, with researchers exploring increasingly complex graph representations and neural architectures for multi-document summarization.", "citations": [{"id": "(Wang et al., 2022)", "paper": {"corpus_id": 252185277, "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Kunyuan Pang", "authorId": "2116489"}, {"name": "Liangliang He", "authorId": "50670961"}, {"name": "Dong Li", "authorId": "2108821455"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "38972135"}], "n_citations": 15}, "snippets": ["Multi-Document Scientific Summarization (MDSS) aims to produce coherent and concise summaries for clusters of topic-relevant scientific papers. This task requires precise understanding of paper content and accurate modeling of cross-paper relationships. Knowledge graphs convey compact and interpretable structured information for documents, which makes them ideal for content modeling and relationship modeling. In this paper, we present KGSum, an MDSS model centred on knowledge graphs during both the encoding and decoding process. Specifically, in the encoding process, two graph-based modules are proposed to incorporate knowledge graph information into paper encoding, while in the decoding process, we propose a two-stage decoder by first generating knowledge graph information of summary in the form of descriptive sentences, followed by generating the final summary. Empirical results show that the proposed architecture brings substantial improvements over baselines on the Multi-Xscience dataset", ".GraphSum (Li et al., 2020)) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2019) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."], "score": 0.98095703125}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits for detecting salient information from multiple documents and generating overall coherent summaries. In this paper, we develop a neural abstractive multi-document summarization (MDS) model which can leverage well-known graph representations of documents such as similarity graph and discourse graph, to more effectively process multiple input documents and produce abstractive summaries. Our model utilizes graphs to encode documents in order to capture cross-document relations, which is crucial to summarizing long documents. Our model can also take advantage of graphs to guide the summary generation process, which is beneficial for generating coherent and concise summaries. Furthermore, pre-trained language models can be easily combined with our model, which further improve the summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines."], "score": 0.0}, {"id": "(Khaliq et al._1, 2024)", "paper": {"corpus_id": 271791043, "title": "Enhanced Topic-Aware Summarization Using Statistical Graph Neural Networks", "year": 2024, "venue": "Computers, Materials &amp; Continua", "authors": [{"name": "A. Khaliq", "authorId": "2058964352"}, {"name": "Salman Afsar Awan", "authorId": "2269634624"}, {"name": "Fahad Ahmad", "authorId": "2315394721"}, {"name": "Muhammad Azam Zia", "authorId": "2315423888"}, {"name": "Muhammad Zafar Iqbal", "authorId": "2255632961"}], "n_citations": 2}, "snippets": ["Through systematic comparisons, proposed model is tested against well-known benchmarks and recent advances in neural graph-based models. These assessments are not just limited to performance metrics like ROUGE scores but also extend to an examination of how well each model assimilates and processes linguistic and thematic information across different document types. This research examines the performance in relation to various node configurations and graph structures used in competing models, providing a comprehensive view of the proposed model's abilities."], "score": 0.97802734375}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 209405420, "title": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Jingqing Zhang", "authorId": "47540100"}, {"name": "Yao Zhao", "authorId": "2143397386"}, {"name": "Mohammad Saleh", "authorId": "144413479"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 2054}, "snippets": ["Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets."], "score": 0.0}, {"id": "(See et al., 2017)", "paper": {"corpus_id": 8314118, "title": "Get To The Point: Summarization with Pointer-Generator Networks", "year": 2017, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "A. See", "authorId": "13070498"}, {"name": "Peter J. Liu", "authorId": "35025299"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 4028}, "snippets": ["Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points."], "score": 0.0}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}], "table": null}], "cost": 0.47950800000000005}}
