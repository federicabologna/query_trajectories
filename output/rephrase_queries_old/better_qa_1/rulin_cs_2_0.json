{"better_query": "How do sparse attention mechanisms and memory-efficient implementations, such as FlashAttention and BigBird, compare in their ability to extend transformer context length while minimizing computational costs?", "better_answer": {"sections": [{"title": "Introduction and Background of Transformer Attention Limitations", "tldr": "Transformers revolutionized NLP with their self-attention mechanism, but standard attention has quadratic complexity that limits context length. This fundamental bottleneck creates memory and computational challenges when processing long sequences. (LLM Memory)", "text": "\nTransformer models have fundamentally changed the landscape of natural language processing since their introduction in 2017. At their core, these models rely on a self-attention mechanism that allows them to process input sequences in parallel while capturing relationships between all tokens in the sequence. This mechanism enables transformers to model long-range dependencies more effectively than previous architectures like RNNs. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nHowever, the standard self-attention mechanism in transformers has a critical limitation: it scales quadratically with sequence length in both computational complexity and memory usage. This is because for each token in a sequence of length n, the model must compute attention scores with every other token, resulting in an n\u00d7n attention matrix. As sequence length grows, this quadratic scaling becomes prohibitively expensive. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFor example, doubling the context length from 2,048 to 4,096 tokens increases memory requirements by 4x and computational costs by 4x. This quadratic bottleneck severely restricts the maximum context length that transformer models can effectively process, typically limiting earlier models to sequences of only a few thousand tokens. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThis limitation has practical implications for many applications requiring long-context understanding, such as document analysis, conversation modeling, and code generation. As a result, researchers have been actively developing alternative attention mechanisms and optimizations to overcome these constraints while preserving the powerful capabilities of transformer architectures. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Types and Approaches of Sparse Attention Mechanisms", "tldr": "Sparse attention mechanisms reduce transformer complexity by selectively computing attention for subsets of tokens rather than all pairs. Key approaches include pattern-based methods like BigBird and Longformer that combine local, global, and random attention patterns to achieve linear complexity while maintaining model effectiveness. (12 sources)", "text": "\nSparse attention mechanisms fundamentally work by computing attention scores for only a subset of token pairs rather than the full n\u00d7n attention matrix used in standard transformers. This selective approach significantly reduces the computational and memory requirements from quadratic O(n\u00b2) to linear O(n) or sub-quadratic O(n log n) complexity, enabling the processing of much longer sequences.\n\nSeveral distinct categories of sparse attention have emerged in the research literature:\n\n## Local Window Attention\nOne of the most intuitive approaches is to restrict attention to a fixed window of neighboring tokens. This pattern assumes that nearby tokens are most relevant for understanding context. Local attention appears in models like Longformer, which uses sliding window patterns where each token attends only to tokens within a fixed distance <Paper corpusId=\"215737171\" paperTitle=\"(Beltagy et al., 2020)\" isShortName></Paper>. Some implementations extend this approach with dilated sliding windows to increase the effective receptive field without adding computational cost <Paper corpusId=\"258840932\" paperTitle=\"(Zhuang et al., 2022)\" isShortName></Paper>.\n\n## Global Attention\nTo maintain the ability to capture long-range dependencies, many sparse attention models incorporate global attention mechanisms. These allow certain tokens (like classification tokens) to attend to all other tokens in the sequence. BigBird notably includes a set of global tokens that attend to the entire sequence, which theoretical analysis showed to be particularly beneficial <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\n## Random Attention\nSome models incorporate randomized attention patterns to capture unexpected long-range dependencies. BigBird combines its global and local attention with random attention, where each token attends to a fixed number of randomly selected tokens throughout the sequence <Paper corpusId=\"270063477\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>. This random component helps ensure that important connections aren't systematically overlooked.\n\n## Combined Pattern Approaches\nMost successful sparse attention implementations combine multiple patterns. BigBird integrates local window attention, global attention, and random attention <Paper corpusId=\"258762176\" paperTitle=\"(KABENAMUALU et al., 2023)\" isShortName></Paper>. Longformer combines local windowed attention with task-specific global attention <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. These hybrid approaches aim to balance computational efficiency with the ability to capture diverse types of dependencies.\n\n## Locality-Sensitive Hashing\nRather than using fixed patterns, some models dynamically determine which tokens should attend to each other. Reformer uses locality-sensitive hashing (LSH) to identify similar tokens that are likely to have high attention scores, reducing complexity to O(n log n) <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"273821735\" paperTitle=\"(Datta, 2024)\" isShortName></Paper>.\n\n## Adaptive and Learnable Sparsity\nMore sophisticated approaches learn the sparsity pattern during training. The Adaptively Sparse Transformer replaces the standard softmax with \u03b1-entmax, allowing the model to learn which tokens should receive zero attention weight <Paper corpusId=\"202538495\" paperTitle=\"(Correia et al., 2019)\" isShortName></Paper>. This creates flexible, context-dependent sparsity patterns that can adapt to different types of inputs and tasks.\n\nDespite their efficiency advantages, sparse attention mechanisms face certain limitations. Research has shown that sparse attention cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Some problems require dense attention to be solved efficiently, with theoretical analysis demonstrating that certain tasks solvable by dense attention in O(1) layers would require \u03a9(n) layers with sparse attention <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nAnother practical challenge is implementation complexity\u2014many sparse attention patterns require custom CUDA kernels or specialized programming, making them difficult to maintain and use <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper> <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nThe trade-offs between sparse and dense attention are context-dependent. Studies show that sparse attention models like BigBird, Longformer, and Sparse Transformer outperform standard transformers on long document tasks but may underperform on shorter sequences where full attention is more beneficial <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Beltagy et al., 2020)", "paper": {"corpus_id": 215737171, "title": "Longformer: The Long-Document Transformer", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Iz Beltagy", "authorId": "46181066"}, {"name": "Matthew E. Peters", "authorId": "39139825"}, {"name": "Arman Cohan", "authorId": "2527954"}], "n_citations": 4100}, "snippets": ["Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset."], "score": 0.0}, {"id": "(Zhuang et al., 2022)", "paper": {"corpus_id": 258840932, "title": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence Learning Ability", "year": 2022, "venue": "UniReps", "authors": [{"name": "Yufan Zhuang", "authorId": "1505801820"}, {"name": "Zihan Wang", "authorId": "2240689"}, {"name": "Fangbo Tao", "authorId": "3180064"}, {"name": "Jingbo Shang", "authorId": "2163679367"}], "n_citations": 3}, "snippets": ["Sparse Attention. Perhaps the most intuitive solution to alleviate the quadratic cost, Sparse Attention only calculates a portion of the full n 2 attention matrix. Early stage methods include Local Attention (Parmar et al., 2018) and Multi-passage BERT (Wang et al., 2019) use sliding windows or chunked blocks to speed up computation. Longformer [1] and BigBird (Zaheer et al., 2020) further combine global attention, sliding window attention, dilated sliding window attention, and random attention together to form strong sparse attention mechanisms, and BigBird showed that their method is a universal approximator of sequence functions."], "score": 0.79541015625}, {"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."], "score": 0.82763671875}, {"id": "(Yan et al., 2024)", "paper": {"corpus_id": 270063477, "title": "Scorch: A Library for Sparse Deep Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bobby Yan", "authorId": "2303621454"}, {"name": "Alexander J Root", "authorId": "2303402046"}, {"name": "Trevor Gale", "authorId": "2303401237"}, {"name": "David Broman", "authorId": "2303401552"}, {"name": "Fredrik Kjolstad", "authorId": "2303400919"}], "n_citations": 1}, "snippets": ["BigBird is designed to handle long sequences while maintaining a manageable computational complexity. It achieves this by using a sparse attention mechanism that attends to a subset of tokens in the sequence, rather than attending to all tokens as in the transformer. The sparse attention pattern in BigBird consists of three components:\n\n1. Global attention: Attend to all tokens in fixed-size blocks at regular intervals.\n2. Sliding window attention: Attend to neighboring tokens within a fixed-size window that slides over the sequence.\n3. Random attention: Attend to a fixed number of randomly selected tokens in the sequence.\n\nBy combining these attention patterns, sparse transformers can capture both local and global dependencies while significantly reducing the computational cost compared to the standard Transformer."], "score": 0.681640625}, {"id": "(KABENAMUALU et al., 2023)", "paper": {"corpus_id": 258762176, "title": "ORKG-Leaderboards: a systematic workflow for mining leaderboards as a knowledge graph", "year": 2023, "venue": "International Journal on Digital Libraries", "authors": [{"name": "Salomon Kabongo KABENAMUALU", "authorId": "1591123106"}, {"name": "J. D\u2019Souza", "authorId": "1789682566"}, {"name": "S. Auer", "authorId": "145044578"}], "n_citations": 20}, "snippets": ["BigBird is a sparse-attention-based transformer that extends Transformer based models, such as BERT, to much longer sequences. Moreover, BigBird comes along with a theoretical understanding of the capabilities of a complete transformer that the sparse model can handle (Zaheer et al., 2020). BigBird takes inspiration from graph sparsification methods by relaxing the need for the attention to fully attend to all the input tokens. Formally the model first builds a set of g global tokens attending on all parts of the sequence, then all tokens attend to a set of w local neighboring tokens, and finally, all tokens attend to a set of r random tokens. The empirical configuration explained in the last paragraph leads to a high-performing attention mechanism scaling to much longer sequence lengths (8x) (Zaheer et al., 2020)."], "score": 0.6875}, {"id": "(Pham et al., 2024)", "paper": {"corpus_id": 273856640, "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Linh Manh Pham", "authorId": "2292320440"}, {"name": "Hoang Cao the", "authorId": "2330416501"}], "n_citations": 3}, "snippets": ["The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)", "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."], "score": 0.615234375}, {"id": "(Kitaev et al., 2020)", "paper": {"corpus_id": 209315300, "title": "Reformer: The Efficient Transformer", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Nikita Kitaev", "authorId": "143808231"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "Anselm Levskaya", "authorId": "6639036"}], "n_citations": 2333}, "snippets": ["Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."], "score": 0.0}, {"id": "(Datta, 2024)", "paper": {"corpus_id": 273821735, "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Akul Datta", "authorId": "2345187428"}], "n_citations": 1}, "snippets": ["Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices."], "score": 0.7080078125}, {"id": "(Correia et al., 2019)", "paper": {"corpus_id": 202538495, "title": "Adaptively Sparse Transformers", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Gon\u00e7alo M. Correia", "authorId": "146783606"}, {"name": "Vlad Niculae", "authorId": "2114966"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "145644643"}], "n_citations": 256}, "snippets": ["Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."], "score": 0.0}, {"id": "(Ding et al., 2021)", "paper": {"corpus_id": 229923177, "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Siyu Ding", "authorId": "2092641069"}, {"name": "Junyuan Shang", "authorId": "40861754"}, {"name": "Shuohuan Wang", "authorId": "104463827"}, {"name": "Yu Sun", "authorId": "2117103617"}, {"name": "Hao Tian", "authorId": "50007795"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 55}, "snippets": ["Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."], "score": 0.6767578125}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 259858862, "title": "Adaptive Attention for Sparse-based Long-sequence Transformer", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xuanyu Zhang", "authorId": "4874730"}, {"name": "Zhepeng Lv", "authorId": "2068974"}, {"name": "Qing Yang", "authorId": "2149535351"}], "n_citations": 7}, "snippets": ["Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered."], "score": 0.60986328125}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 237260051, "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Chuhan Wu", "authorId": "15161448"}, {"name": "Fangzhao Wu", "authorId": "2397264"}, {"name": "Tao Qi", "authorId": "50329599"}, {"name": "Yongfeng Huang", "authorId": "1731776"}], "n_citations": 3}, "snippets": ["We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms", "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."], "score": 0.88623046875}], "table": null}, {"title": "FlashAttention and Memory-Efficient Implementations", "tldr": "FlashAttention introduces an I/O-aware approach that reduces memory usage from O(n\u00b2) to O(n) through tiling and block-wise computation without sacrificing accuracy. This innovation has enabled longer context lengths and spawned extensions like sparse FlashAttention variants that combine efficiency optimizations with sparsity patterns for even greater computational gains. (12 sources)", "text": "\nWhile sparse attention mechanisms like BigBird focus on reducing complexity by limiting which tokens attend to each other, FlashAttention takes a fundamentally different approach. It addresses the memory bottleneck in transformer attention by optimizing how data moves between different levels of GPU memory. FlashAttention uses tiling to process smaller chunks of the attention matrix sequentially, minimizing the number of memory transfers between high-bandwidth memory (HBM) and on-chip SRAM <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"276725385\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper>.\n\nThe key insight of FlashAttention is that it maintains mathematical equivalence to standard attention while achieving O(n) memory complexity through block-wise computation and the use of \"online softmax\" techniques <Paper corpusId=\"276775748\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. This I/O-aware approach resulted in significant practical speed improvements: 15% faster training for BERT-large, 3\u00d7 speedup for GPT-2, and 2.4\u00d7 speedup on long-range arena benchmarks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nMore importantly, FlashAttention enabled transformers to handle much longer sequences than previously possible. Models built with FlashAttention achieved better-than-chance performance on extremely long sequences of 16K tokens (Path-X challenge) and 64K tokens (Path-256), opening new capabilities for transformer models <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\n## Extensions and Hybrid Approaches\n\nFlashAttention has evolved through several iterations. FlashAttention-2 improved parallelism and work distribution for better GPU utilization, while FlashAttention-3 introduced asynchronous computation and FP8 precision support to reach 75% GPU utilization on H100 hardware <Paper corpusId=\"276575796\" paperTitle=\"(Navardi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276725385\" paperTitle=\"(Xia et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271098045\" paperTitle=\"(Shah et al., 2024)\" isShortName></Paper>.\n\nResearchers have also created hybrid approaches that combine FlashAttention's memory efficiency with sparse attention patterns:\n\n1. **Block-Sparse FlashAttention** extends the original algorithm with a two-dimensional block mask matrix to skip computations for masked blocks, maintaining efficiency while incorporating sparsity <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273026224\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n2. **Sparse Causal Flash Attention (SCFA)** optimizes for causal attention in autoregressive models, adding support for key/query dropping and hashing-based attention patterns. This approach achieved training speedups of 2.0\u00d7 and 3.3\u00d7 for sequences of 8K and 16K tokens, respectively, without sacrificing model quality <Paper corpusId=\"259063695\" paperTitle=\"(Pagliardini et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273026224\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Alternative Memory-Efficient Approaches\n\nBeyond FlashAttention, other memory-efficient implementations have emerged:\n\n1. **Multi-query attention (MQA)** and **Grouped-query attention (GQA)** reduce memory bandwidth by using fewer \"key\" and \"value\" heads, addressing the I/O bottleneck from a different angle. GQA is notably used in LLaMA 2 <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\n2. **Retrieval-augmented models** avoid storing the full attention context by retrieving information from external memory, offering another approach to process lengthy inputs <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n3. **Clustering-based approaches** allow queries to attend to different sets of key-value (KV) pairs, though these often face challenges with memory redundancy during long-range inference <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212718077\" paperTitle=\"(Roy et al., 2020)\" isShortName></Paper>.\n\nThe trend in memory-efficient implementations reflects a shift in focus from reducing raw computational complexity (FLOPs) to optimizing memory access patterns, as the bottleneck in modern hardware has increasingly become I/O overhead rather than computation itself <Paper corpusId=\"266999285\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This change in perspective has been crucial for enabling transformers to handle the increasingly long contexts required by modern applications.", "citations": [{"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.7119140625}, {"id": "(Xia et al., 2025)", "paper": {"corpus_id": 276725385, "title": "Training-free and Adaptive Sparse Attention for Efficient Long Video Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yifei Xia", "authorId": "2344790336"}, {"name": "Suhan Ling", "authorId": "2348282342"}, {"name": "Fangcheng Fu", "authorId": "46182701"}, {"name": "Yujie Wang", "authorId": "2167500394"}, {"name": "Huixia Li", "authorId": "2108525422"}, {"name": "Xuefeng Xiao", "authorId": "2319391688"}, {"name": "Bin Cui", "authorId": "2313408987"}], "n_citations": 11}, "snippets": ["FlashAttention [12,(Dao et al., 2022)(Shah et al., 2024) addresses this issue by performing attention in a blockwise manner. Instead of storing the full attention matrix, FlashAttention processes smaller chunks sequentially. In FlashAttention, attention is computed for smaller blocks of tokens, and the key idea is to perform attention on these blocks without constructing the entire attention matrix at once. Specifically, for each block, the attention is computed as: \n\nwhere Q b and K b represent the query and key matrices for block b, where L \u226b b, and online_softmax [41] is a blockwise equivalent version of the safe softmax. The result is then multiplied by the value matrix for the block, V b , to obtain the final attention output: \n\nThis block-wise computation significantly reduces the memory footprint to O(Lb), as only a subset of the full attention matrix is processed at any given time. FlashAttention is particularly effective for large-scale transformers and longsequence tasks, such as 3D Full Attention."], "score": 0.603515625}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276775748, "title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yujiao Yang", "authorId": "2348389336"}, {"name": "Jing Lian", "authorId": "2282559167"}, {"name": "Linhui Li", "authorId": "2244250357"}], "n_citations": 0}, "snippets": ["Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within a sequence", "Flash Attention (Dao et al., 2022) reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion."], "score": 0.6806640625}, {"id": "(Navardi et al., 2025)", "paper": {"corpus_id": 276575796, "title": "GenAI at the Edge: Comprehensive Survey on Empowering Edge Devices", "year": 2025, "venue": "Proceedings of the AAAI Symposium Series", "authors": [{"name": "Mozhgan Navardi", "authorId": "2180067662"}, {"name": "Romina Aalishah", "authorId": "2346327200"}, {"name": "Yuzhe Fu", "authorId": "2335995540"}, {"name": "Yueqian Lin", "authorId": "2223973348"}, {"name": "Hai Li", "authorId": "2346061748"}, {"name": "Yiran Chen", "authorId": "2247106559"}, {"name": "T. Mohsenin", "authorId": "2393902"}], "n_citations": 2}, "snippets": ["FlashAttention (Dao et al. 2022) reorders attention operations to reduce the number of reads and writes between GPU high bandwidth memory (HBM) and on-chip static RAM (SRAM) by splitting queries, keys, and values into smaller blocks, recomputing attention onchip during the backward pass, and fusing multiple GPU kernels into one. Built on this, FlashAttention-2 (Dao 2023) takes the foundation of memory efficiency and adds better parallelism and work distribution to further increase speed and GPU utilization, especially for longer sequences. Then, FlashAttention-3 (Shah et al. 2024) introduces asynchrony and low-precision computation to further optimize the attention mechanism for modern GPU architectures, which allows for even higher performance and efficiency, along with reduced error for low-precision (FP8) computing. Besides these, xFormers (Lefaudeux et al. 2022), a PyTorchbased library, provides a collection of optimized attention and Transformer blocks, including custom GPU kernels and memory-efficient attention implementations", "Subsequent techniques like Longformer (Beltagy et al. 2020) by using a combination of sliding window local attention and taskmotivated global attention, Big Bird (Zaheer et al. 2020) by combining random, windowed, and global attention to create a sparse attention mechanism, and Linformer (Wang et al. 2020a) by decomposing attention with linear projections to achieve linear complexity introduced various structured sparsity patterns."], "score": 0.82470703125}, {"id": "(Shah et al., 2024)", "paper": {"corpus_id": 271098045, "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jay Shah", "authorId": "2275225725"}, {"name": "Ganesh Bikshandi", "authorId": "3206527"}, {"name": "Ying Zhang", "authorId": "2310719512"}, {"name": "Vijay Thakkar", "authorId": "2310700174"}, {"name": "Pradeep Ramani", "authorId": "2310700428"}, {"name": "Tri Dao", "authorId": "2310701039"}], "n_citations": 157}, "snippets": ["Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273026224, "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guoxia Wang", "authorId": "2315166927"}, {"name": "Jinle Zeng", "authorId": "2072984835"}, {"name": "Xiyuan Xiao", "authorId": "2325510350"}, {"name": "Siming Wu", "authorId": "2323811334"}, {"name": "Jiabin Yang", "authorId": "2323896653"}, {"name": "Lujing Zheng", "authorId": "2324060184"}, {"name": "Zeyu Chen", "authorId": "2323781886"}, {"name": "Jiang Bian", "authorId": "2324064223"}, {"name": "Dianhai Yu", "authorId": "2315250077"}, {"name": "Haifeng Wang", "authorId": "2323851061"}], "n_citations": 3}, "snippets": ["FlashAttention achieves a memory overhead of O(), proving particularly effective in tasks without custom masking requirements. Furthermore, FlashAttention extends to Block-Sparse FlashAttention, introducing a two-dimensional block mask matrix representation to indicate masked tiling blocks. This innovation allows for the skipping of computations for masked blocks, thereby accelerating the process", "Sparse Causal Flash Attention (SCFA) (Pagliardini et al., 2023) extends FlashAttention to optimize QK-Sparse and Hash-Sparse scenarios in causal attention structures. SCFA employs indices of queries and keys in the original uncompressed tensors to describe masks, enabling the omission of computations for masked blocks and enhancing computational efficiency. FlexAttention He et al. (2024) leverages compiler techniques to simplify mask attention implementations, exploiting sparsity in the attention mask to skip certain masked blocks and achieve improved speed."], "score": 0.701171875}, {"id": "(Pagliardini et al., 2023)", "paper": {"corpus_id": 259063695, "title": "Faster Causal Attention Over Large Sequences Through Sparse Flash Attention", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Matteo Pagliardini", "authorId": "2435537"}, {"name": "Daniele Paliotta", "authorId": "50552613"}, {"name": "Martin Jaggi", "authorId": "2456863"}, {"name": "Franccois Fleuret", "authorId": "116272138"}], "n_citations": 25}, "snippets": ["Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens."], "score": 0.732421875}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 266999285, "title": "Extending LLMs' Context Window with 100 Samples", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yikai Zhang", "authorId": "2279654115"}, {"name": "Junlong Li", "authorId": "2278801242"}, {"name": "Pengfei Liu", "authorId": "2256991660"}], "n_citations": 12}, "snippets": ["Sparse transformers (Child et al., 2019;(Zaheer et al., 2020)Kitaev et al., 2020;Beltagy et al., 2020;Ainslie et al., 2020;(Zaheer et al., 2020)Ding et al., 2023) replace the original full attention mechanism with a sparsified version to make the computation more efficient. Linear transformers (Wang et al., 2020;Katharopoulos et al., 2020;Choromanski et al., 2020), rather than forcing the attention mechanism to attend to fewer tokens, propose an alternative approach by leveraging low-rank matrix multiplication or linear dot-product of kernel feature maps to approximate the original attention mechanism, achieving linear time complexity. Meanwhile, retrieval-augmented models (Guu et al., 2020)(Lewis et al., 2020)Wu et al., 2022;Bulatov et al., 2023;Tworkowski et al., 2023) integrate retrieval with attention. During inference time, these models avoid directly modeling lengthy inputs by retrieving information from an external memory that stores previous key-value pairs. While prior research primarily focuses on reducing FLOPs, the bottleneck of transformer inference on modern computing hardware has shifted to the overhead from memory access (IO). Multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023), for instance, address the memory-bandwidth cost associated with loading the large \"keys\" and \"values\" tensors in the multi-head attention mechanism by proposing the use of fewer \"key\" and \"value\" heads. Notably, GQA is employed in LLaMA2 (Touvron et al., 2023b). Additionally, FlashAttention (Dao et al., 2022)Dao, 2023) introduces an IO-aware exact attention approach that utilizes tiling to reduce memory IOs."], "score": 0.67529296875}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Lou et al., 2024)", "paper": {"corpus_id": 270703226, "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chao Lou", "authorId": "2061806821"}, {"name": "Zixia Jia", "authorId": "1453587987"}, {"name": "Zilong Zheng", "authorId": "2266032392"}, {"name": "Kewei Tu", "authorId": "40341553"}], "n_citations": 26}, "snippets": ["Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters."], "score": 0.74609375}, {"id": "(Roy et al., 2020)", "paper": {"corpus_id": 212718077, "title": "Efficient Content-Based Sparse Attention with Routing Transformers", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Aurko Roy", "authorId": "39788470"}, {"name": "M. Saffar", "authorId": "2814161"}, {"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "David Grangier", "authorId": "2529182"}], "n_citations": 603}, "snippets": ["Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1"], "score": 0.0}], "table": null}, {"title": "Computational Complexity Comparison", "tldr": "Different sparse attention mechanisms reduce transformer complexity from O(n\u00b2) to O(n) or O(n log n), with methods like BigBird and Longformer achieving linear scaling through complementary attention patterns. FlashAttention takes a different approach by optimizing memory usage while maintaining full attention computation. (12 sources)", "text": "\nTransformers with the standard attention mechanism have a computational and memory complexity of O(n\u00b2) with respect to sequence length, creating a fundamental bottleneck for processing long documents. Sparse attention mechanisms address this challenge by reducing this quadratic complexity to more manageable levels:\n\n## Linear Complexity O(n) Approaches\n\nSeveral sparse attention methods achieve linear complexity by limiting attention to a subset of token pairs. BigBird combines local window attention, global tokens, and random attention patterns to reduce complexity to O(n) while maintaining theoretical properties like Turing completeness and universal approximation <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. Similarly, Longformer uses a combination of sliding window patterns and global attention to achieve O(n) complexity <Paper corpusId=\"266110855\" paperTitle=\"(Zhou, 2023)\" isShortName></Paper> <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\nThese pattern-based approaches provide a significant advantage: for a sequence of length n, they require only O(n) memory and computation rather than O(n\u00b2), enabling them to process sequences up to 8 times longer than was previously possible on the same hardware <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\n## Sub-quadratic O(n log n) Approaches\n\nSome methods achieve a complexity of O(n log n), striking a middle ground between full quadratic attention and strictly linear approaches:\n\n- Reformer uses locality-sensitive hashing (LSH) to identify similar tokens likely to have high attention scores, reducing complexity to O(n log n) <Paper corpusId=\"209315300\" paperTitle=\"(Kitaev et al., 2020)\" isShortName></Paper> <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.\n- Sparse Transformer uses factorized self-attention patterns that reduce complexity to O(n\u221an) <Paper corpusId=\"248427085\" paperTitle=\"(Soydaner, 2022)\" isShortName></Paper>.\n\n## Theoretical Limitations\n\nDespite their efficiency gains, sparse attention mechanisms face theoretical limitations. Research has shown that sparse attention cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Some problems that can be solved by dense attention in O(1) layers would require \u03a9(n) layers with sparse attention <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. This theoretical limitation has practical implications, as noted by researchers who caution that models like BigBird and Longformer may not be universal replacements for dense attention but rather task-specific solutions for long sequences <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\n## Implementation Challenges\n\nImplementation of sparse attention presents technical challenges. Most current implementations are not fully \"hardware-aware\" and may perform redundant computations, such as dense-dense matrix multiplication followed by masking based on the attention pattern <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. Many sparse attention patterns require custom CUDA kernels or specialized programming, making them difficult to maintain and use in practice <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.\n\n## Alternative Approaches\n\nWhile sparse attention reduces computational complexity by computing fewer attention scores, other approaches tackle the efficiency problem differently:\n\n- Linear attention methods like Performers and Linear Transformers reformulate the attention function to achieve linear complexity without explicit computation of the full attention matrix <Paper corpusId=\"273821735\" paperTitle=\"(Datta, 2024)\" isShortName></Paper> <Paper corpusId=\"220250819\" paperTitle=\"(Katharopoulos et al., 2020)\" isShortName></Paper>.\n- Low-rank approximation methods like Linformer reduce complexity by approximating the attention matrix <Paper corpusId=\"266210450\" paperTitle=\"(Song et al., 2023)\" isShortName></Paper>.\n- Memory-efficient implementations like FlashAttention maintain the standard attention computation but optimize memory access patterns, addressing I/O bottlenecks rather than reducing computational complexity <Paper corpusId=\"273850602\" paperTitle=\"(Haris, 2024)\" isShortName></Paper>.\n\nThe diversity of approaches reflects the complex trade-offs between computational efficiency, memory usage, and model effectiveness, with the optimal choice depending on the specific application requirements and hardware constraints <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."], "score": 0.82763671875}, {"id": "(Zhou, 2023)", "paper": {"corpus_id": 266110855, "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Le Zhou", "authorId": "2189279577"}], "n_citations": 0}, "snippets": ["As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy."], "score": 0.669921875}, {"id": "(Bae et al., 2022)", "paper": {"corpus_id": 254725259, "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion", "year": 2022, "venue": "IEEE Access", "authors": [{"name": "Jongseong Bae", "authorId": "2195944251"}, {"name": "Byung Do Cheon", "authorId": "2196700504"}, {"name": "Ha Young Kim", "authorId": "2129420267"}], "n_citations": 0}, "snippets": ["In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."], "score": 0.81591796875}, {"id": "(Kitaev et al., 2020)", "paper": {"corpus_id": 209315300, "title": "Reformer: The Efficient Transformer", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Nikita Kitaev", "authorId": "143808231"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "Anselm Levskaya", "authorId": "6639036"}], "n_citations": 2333}, "snippets": ["Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."], "score": 0.0}, {"id": "(Ding et al., 2021)", "paper": {"corpus_id": 229923177, "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Siyu Ding", "authorId": "2092641069"}, {"name": "Junyuan Shang", "authorId": "40861754"}, {"name": "Shuohuan Wang", "authorId": "104463827"}, {"name": "Yu Sun", "authorId": "2117103617"}, {"name": "Hao Tian", "authorId": "50007795"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 55}, "snippets": ["Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."], "score": 0.6767578125}, {"id": "(Soydaner, 2022)", "paper": {"corpus_id": 248427085, "title": "Attention mechanism in neural networks: where it comes and where it goes", "year": 2022, "venue": "Neural computing & applications (Print)", "authors": [{"name": "Derya Soydaner", "authorId": "1409173278"}], "n_citations": 166}, "snippets": ["Sparse Transformer introduces sparse factorizations of the attention matrix by using factorized self-attention, and avoids the quadratic growth of computational burden [176]. It also shows the possibility of modeling sequences of length one million or more by using self-attention in theory. In the Transformer, all the attention heads with the softmax attention assign a nonzero weight to all context words. Adaptively Sparse Transformer replaces softmax with a-entmax which is a differentiable generalization of softmax allowing low-scoring words to receive precisely zero weight (Correia et al., 2019). By means of context-dependent sparsity patterns, the attention heads become flexible in the Adaptively Sparse Transformer. Random feature attention approximates softmax attention with random feature methods (Peng et al., 2021). Skyformer replaces softmax with a Gaussian kernel and adapts Nystr\u00f6m method (Chen et al., 2021). A sparse attention mechanism named BIGBIRD aims to reduce the quadratic dependency of Transformer-based models to linear (Zaheer et al., 2020). Different from the similar studies, BIGBIRD performs well for genomics data alongside NLP tasks such as question answering."], "score": 0.6787109375}, {"id": "(Pham et al., 2024)", "paper": {"corpus_id": 273856640, "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Linh Manh Pham", "authorId": "2292320440"}, {"name": "Hoang Cao the", "authorId": "2330416501"}], "n_citations": 3}, "snippets": ["The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)", "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."], "score": 0.615234375}, {"id": "(Tomczak et al., 2025)", "paper": {"corpus_id": 276106883, "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nathaniel Tomczak", "authorId": "2276535724"}, {"name": "S. Kuppannagari", "authorId": "2873546"}], "n_citations": 0}, "snippets": ["A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach", "However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements."], "score": 0.82958984375}, {"id": "(Datta, 2024)", "paper": {"corpus_id": 273821735, "title": "The Evolution of RWKV: Advancements in Efficient Language Modeling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Akul Datta", "authorId": "2345187428"}], "n_citations": 1}, "snippets": ["Sparse Attention: Methods like Sparse Transformer [Child et al., 2019] and Longformer [Beltagy et al., 2020] restrict attention to a subset of tokens, reducing the number of attention scores computed. This can be achieved by attending only to local neighborhoods, using fixed patterns, or learning the sparse attention structure. \u2022 Linear Attention: Techniques like Performers [Choromanski et al., 2020] and Linear Transformers (Katharopoulos et al., 2020) reformulate attention to achieve linear complexity. They typically use kernel methods or other approximations to avoid explicit computation of the full attention matrix. These methods often involve factorizing the attention matrix or using feature maps to represent token interactions. \u2022 Local Attention: Models like Transformer-XL [Dai et al., 2019] and Big Bird (Zaheer et al., 2020) combine local attention patterns with a small number of global tokens that attend to all other tokens. This allows for efficient processing of long sequences while still maintaining some global context. \u2022 Efficient Transformers: Architectures like Reformer [Kitaev et al., 2020] and Linformer [Wang et al., 2020] employ various techniques such as locality-sensitive hashing (LSH) or low-rank approximations to reduce the complexity of attention. LSH allows for efficient approximate nearest neighbor search, while low-rank approximations reduce the dimensionality of the attention matrices."], "score": 0.7080078125}, {"id": "(Katharopoulos et al., 2020)", "paper": {"corpus_id": 220250819, "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Angelos Katharopoulos", "authorId": "3493855"}, {"name": "Apoorv Vyas", "authorId": "2992087"}, {"name": "Nikolaos Pappas", "authorId": "143958923"}, {"name": "Franccois Fleuret", "authorId": "116272138"}], "n_citations": 1790}, "snippets": ["Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences."], "score": 0.0}, {"id": "(Song et al., 2023)", "paper": {"corpus_id": 266210450, "title": "Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kaiqiang Song", "authorId": "50982080"}, {"name": "Xiaoyang Wang", "authorId": "2250363276"}, {"name": "Sangwoo Cho", "authorId": "2173531"}, {"name": "Xiaoman Pan", "authorId": "2243367575"}, {"name": "Dong Yu", "authorId": "2256336899"}], "n_citations": 7}, "snippets": ["Attention. The Transformer architecture has a self-attention component with O(N 2 ) computation complexity. Numerous studies have been proposed to enhance the time and memory efficiency of Transformer models. One approach involves leveraging sparse attention patterns, enabling the transformation of full quadratic attention computations to O(N log N ) or linear complexity. Our work falls within this method by grouping sparse and full attention patterns. Methods such as Sinkhorn (Tay et al., 2020), Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020) have been introduced to incorporate both sparse and full attention mechanisms. Another set of approaches involves utilizing the low-rank approximation of the attention matrix. This includes methods such as Linformer (Wang et al., 2020), Performer (Choromanski et al., 2022), and Random Feature Attention (Peng et al., 2021)."], "score": 0.59375}, {"id": "(Haris, 2024)", "paper": {"corpus_id": 273850602, "title": "$k$NN Attention Demystified: A Theoretical Exploration for Scalable Transformers", "year": 2024, "venue": "", "authors": [{"name": "Themistoklis Haris", "authorId": "2329374652"}], "n_citations": 0}, "snippets": ["Despite their power, Transformers face challenges with long sequences due to the quadratic complexity of self-attention", "Efficient computation of self-attention has been a focal point of research in recent years (Fournier et al., 2021). Flash Attention [DFE + 22] and related work [SY[24] optimize the exact calculation of attention by minimizing wasted computation during GPU I/O operations. However, most approaches focus on approximating the attention function. Sparse Transformers improve efficiency by allowing each token to attend to only a small subset of tokens [MLAC[21]. These subsets are identified through deterministic methods [CGRS19, GQL + 19, SM20, LJX + 19, QML + 19, BPC[20], randomized algorithms [KKL20, HJK + 23, ZHDK23, PPJF[24], or adaptive techniques [CNM[19]."], "score": 0.61474609375}], "table": null}, {"title": "Performance and Context Length Capabilities", "tldr": "Sparse attention mechanisms like BigBird and Longformer enable processing sequences up to 8 times longer than standard transformers while maintaining theoretical properties like Turing completeness. FlashAttention achieves even greater capabilities, handling sequences of 16K to 64K tokens while improving model quality. (9 sources)", "text": "\nSparse attention mechanisms have significantly extended the maximum context length that transformer models can effectively process. BigBird, with its combination of global, local, and random attention patterns, can handle sequences up to 8 times longer than was previously possible using similar hardware <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper> <Paper corpusId=\"264426102\" paperTitle=\"(Madani et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper>. This dramatic increase in context length capability has practical implications for long-document tasks such as document classification, question answering, and summarization.\n\nModels like Longformer and BigBird have demonstrated particularly strong performance on tasks requiring long-context understanding. Research comparing these sparse attention models with standard transformers found that they outperform vanilla transformers on long document modeling tasks such as classification on Amazon, IMDB, and MIND datasets <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. The performance advantage stems from their ability to process more of the input text, as standard transformers must truncate long documents due to computational constraints.\n\nHowever, sparse attention models show a performance trade-off based on sequence length. For shorter sequences, standard transformers with full attention can outperform sparse attention models. Studies have shown that sparse attention approaches are inferior to vanilla transformers in short sequence modeling tasks like AG classification and recommendation on MIND <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. This suggests that sparse attention models cannot fully capture important interactions between tokens when working with shorter inputs, highlighting that they are not universal replacements for dense attention but rather specialized solutions for long-sequence tasks <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\nThe architectural choices in sparse attention models involve significant trade-offs. While models with global attention elements can capture long-range dependencies, they still retain some quadratic computational elements that limit maximum context length. Conversely, models using only local attention can process much longer documents but may struggle to effectively model long-range dependencies, resulting in reduced accuracy <Paper corpusId=\"266110855\" paperTitle=\"(Zhou, 2023)\" isShortName></Paper>.\n\nMemory-efficient implementations like FlashAttention take a different approach to extending context length. Rather than sparsifying attention, FlashAttention optimizes memory usage while maintaining full attention computation. This approach has yielded impressive results, enabling models to achieve better-than-chance performance on extremely long sequences: 61.4% accuracy on the Path-X challenge with 16K tokens and 63.1% accuracy on Path-256 with 64K tokens <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. Beyond just handling longer sequences, FlashAttention also improves model quality, delivering 0.7 better perplexity on GPT-2 and 6.4 points of improvement on long-document classification tasks <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nThe evolution of context length capabilities has continued to advance rapidly. Recent commercial models have pushed boundaries even further, with OpenAI's GPT-4o and Google's Gemini 2.0 Flash reportedly supporting context windows of 1 million tokens for both text-only and multimodal inputs by combining blockwise attention with disk-paged key-value caches <Paper corpusId=\"278000642\" paperTitle=\"(Ahn, 2025)\" isShortName></Paper>. This represents a dramatic expansion beyond the 16-32K token limits that earlier sparse attention families like Longformer, BigBird, and Reformer helped establish.\n\nDespite these advances, theoretical analysis reveals that sparse attention mechanisms still face fundamental limitations. Research has shown that certain problems solvable by dense attention in O(1) layers would require \u03a9(n) layers with any sparse attention mechanism <Paper corpusId=\"273856640\" paperTitle=\"(Pham et al., 2024)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. This theoretical constraint helps explain why sparse attention models, despite their efficiency advantages, cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."], "score": 0.82763671875}, {"id": "(Madani et al., 2023)", "paper": {"corpus_id": 264426102, "title": "REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization", "year": 2023, "venue": "Conference on Computational Natural Language Learning", "authors": [{"name": "Mohammad Reza Ghasemi Madani", "authorId": "2261279246"}, {"name": "Pasquale Minervini", "authorId": "2294362638"}], "n_citations": 4}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, (Zaheer et al., 2020) proposed BIGBIRD, a sparse attention mechanism that reduces this quadratic dependency to linear. They show that BIGBIRD is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, their theoretical analysis reveals some of the benefits of having O(1) global tokens (such as CLS) that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to eight times what was previously possible using similar hardware."], "score": 0.857421875}, {"id": "(Pham et al., 2024)", "paper": {"corpus_id": 273856640, "title": "LNLF-BERT: Transformer for Long Document Classification With Multiple Attention Levels", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Linh Manh Pham", "authorId": "2292320440"}, {"name": "Hoang Cao the", "authorId": "2330416501"}], "n_citations": 3}, "snippets": ["The BIGBIRD self-attention architecture has three main parts: (a) a global set of g tokens that attends across all parts of the sequence, (b) all tokens participate in a set of local neighbor tokens, (c) all tokens participate in a random set of r tokens. This results in a high-performance attention mechanism that scales with much longer sequence lengths (8 times) (Zaheer et al., 2020)", "While a complete Transformer based on a quadratic attention mechanism is Turing complete (P\u00e9rez et al., 2019), the authors in the BIGBIRD paper have shown that it is possible to use a sparse encoder and a sparse decoder to simulate any Turing machine (Zaheer et al., 2020). And the authors have also shown that there is a natural task that can be solved by a full attention mechanism in O(1) layers. However, under standard complexity theory assumptions, this problem requires \u02dc (n) layers for any sparse attention layer with \u00d5(n) edges (not just BIGBIRD) (here \u00d5 hides the logarithmic factors) (Zaheer et al., 2020)."], "score": 0.615234375}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 237260051, "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Chuhan Wu", "authorId": "15161448"}, {"name": "Fangzhao Wu", "authorId": "2397264"}, {"name": "Tao Qi", "authorId": "50329599"}, {"name": "Yongfeng Huang", "authorId": "1731776"}], "n_citations": 3}, "snippets": ["We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms", "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."], "score": 0.88623046875}, {"id": "(Bae et al., 2022)", "paper": {"corpus_id": 254725259, "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion", "year": 2022, "venue": "IEEE Access", "authors": [{"name": "Jongseong Bae", "authorId": "2195944251"}, {"name": "Byung Do Cheon", "authorId": "2196700504"}, {"name": "Ha Young Kim", "authorId": "2129420267"}], "n_citations": 0}, "snippets": ["In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."], "score": 0.81591796875}, {"id": "(Zhou, 2023)", "paper": {"corpus_id": 266110855, "title": "LongT5-Mulla: LongT5 With Multi-Level Local Attention for a Longer Sequence", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Le Zhou", "authorId": "2189279577"}], "n_citations": 0}, "snippets": ["As the main category of efficient Transformers, Transformers with sparse attention have a key feature that their attention matrices are sparse, which means that only a small number of selected tokens, instead of all tokens in the input sequence, are attended to during the attention computation. According to the different ways of sparsification, it is called local attention [7] if we select neighbor tokens of the query token as the attended tokens, and it is called global attention (Ainslie et al., 2020) if we select shared special tokens as them. With the help of local and global attention alone or jointly, efficient Transformer models such as LED [7], Bigbird (Zaheer et al., 2020), LongT5 (Guo et al., 2021) have achieved state-of-the-art results in long text understanding and generative NLP tasks with relatively low resource consumption, and have extended the length limit from 4k to 16k. But unfortunately, on one hand, models with global attention fail to process longer documents in mainstream hardware conditions due to their essentially quadratic computational complexity, which results in a lack of efficiency. And on the other hand, other models with only local attention can process much longer documents but cannot effectively capture long-range dependencies, which results in reduced accuracy."], "score": 0.669921875}, {"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.7119140625}, {"id": "(Ahn, 2025)", "paper": {"corpus_id": 278000642, "title": "HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kwangseob Ahn", "authorId": "2356854731"}], "n_citations": 0}, "snippets": ["Sparse-attention families (Longformer, BigBird, Reformer) reduce the quadratic cost of self-attention, pushing context to 16-32 K tokens. Recurrent variants (Transformer-XL) extend effective history into the hundreds of thousands, while compression schemes (Compressive Transformer, LongRoPE) selectively down-sample distant states. 2025 models shifted the ceiling dramatically: OpenAI's GPT-4o v and Google's Gemini 2.0 Flash vi advertise 1 M-token windows for text-only and multimodal inputs, respectively, by combining blockwise attention with disk-paged key-value caches. Open-source explorations such as Long-VITA show similar scaling in vision-language settings without heavy token compression."], "score": 0.693359375}, {"id": "(Ding et al., 2021)", "paper": {"corpus_id": 229923177, "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Siyu Ding", "authorId": "2092641069"}, {"name": "Junyuan Shang", "authorId": "40861754"}, {"name": "Shuohuan Wang", "authorId": "104463827"}, {"name": "Yu Sun", "authorId": "2117103617"}, {"name": "Hao Tian", "authorId": "50007795"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 55}, "snippets": ["Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."], "score": 0.6767578125}], "table": null}, {"title": "Limitations and Trade-offs", "tldr": "While sparse attention mechanisms reduce computational complexity from O(n\u00b2) to O(n) or O(n log n), they face fundamental theoretical limitations, implementation challenges, and context-dependent performance trade-offs that prevent them from universally replacing dense attention for all tasks. (14 sources)", "text": "\nDespite their impressive efficiency gains, sparse attention mechanisms and memory-efficient implementations face several critical limitations and trade-offs that impact their practical utility across different applications.\n\n## Theoretical Limitations\n\nResearch has demonstrated that sparse attention mechanisms cannot universally replace dense attention for all tasks <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. This theoretical constraint has practical implications: certain problems that can be efficiently solved by dense attention in O(1) layers would require \u03a9(n) layers with any sparse attention mechanism <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>. This fundamental limitation helps explain why sparse attention models, while efficient, cannot serve as universal replacements for standard transformers across all applications <Paper corpusId=\"254725259\" paperTitle=\"(Bae et al., 2022)\" isShortName></Paper>.\n\n## Implementation Challenges\n\nMost sparse attention mechanisms require specialized programming and custom CUDA kernels, making them difficult to implement, maintain, and use in practice <Paper corpusId=\"229923177\" paperTitle=\"(Ding et al., 2021)\" isShortName></Paper>. Additionally, current implementations of sparse attention are often not fully \"hardware-aware\" \u2013 they typically perform dense-dense matrix multiplication followed by masking based on the attention pattern, potentially resulting in redundant computations <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>.\n\nMany implementations face efficiency challenges at both training and inference times. For example, clustering-based methods that allow queries to attend to different sets of key-value pairs generally cost O(n log n) to maintain clusters <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper>. During long-range inference, these methods can lead to significant memory redundancy as key-value embeddings must be fully stored to avoid repetitive computation <Paper corpusId=\"270703226\" paperTitle=\"(Lou et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264439578\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n## Context-Dependent Performance\n\nThe performance of sparse attention models shows a strong dependency on sequence length and task requirements. Empirical studies demonstrate that while sparse attention models like BigBird and Longformer outperform standard transformers on long document tasks (e.g., classification on Amazon, IMDB, and MIND datasets), they underperform compared to vanilla transformers on short sequence modeling tasks (e.g., AG classification) <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>. This performance differential suggests that sparse attention models cannot fully capture important interactions between tokens in shorter contexts <Paper corpusId=\"237260051\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper>.\n\n## Pattern Design Limitations\n\nMost early sparse attention approaches rely on hand-crafted patterns that are selected empirically or randomly. Models like BigBird combine random attention, global attention, and local sliding windows, while Longformer employs task-specific global attention and local windowed attention <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. These predefined patterns, while effective, inherently limit flexibility <Paper corpusId=\"276618265\" paperTitle=\"(Fu et al., 2025)\" isShortName></Paper>. The challenge of adaptively selecting useful tokens for sparse attention according to context remains an important research problem <Paper corpusId=\"259858862\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\nMore recent approaches have attempted to address this limitation through learnable sparsity patterns. The Adaptively Sparse Transformer replaces the standard softmax with \u03b1-entmax, allowing attention heads to choose between focused or spread-out behavior <Paper corpusId=\"252216464\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"202538495\" paperTitle=\"(Correia et al., 2019)\" isShortName></Paper>. However, these adaptive approaches often introduce additional computational overhead that can offset some of the efficiency gains from sparsity.\n\n## Information Loss\n\nBy design, sparse attention mechanisms sacrifice full access to the long sequence during attention, resulting in an inevitable loss of some contextual information <Paper corpusId=\"264451707\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This trade-off is particularly challenging for block-sparse attention methods, which struggle to balance accuracy and efficiency due to the computational cost of measuring block importance <Paper corpusId=\"277151262\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\n## Computational vs. Memory Optimization\n\nAn important distinction exists between approaches that optimize computational complexity (FLOPs) versus those that target memory efficiency. Many sparse attention mechanisms focus primarily on reducing computational complexity but may still face memory bottlenecks. Conversely, memory-efficient implementations like FlashAttention optimize memory access patterns while maintaining the standard attention computation, addressing I/O bottlenecks rather than reducing computational complexity directly <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nThe diversity of approaches\u2014from pattern-based sparse attention to memory-efficient implementations\u2014reflects the complex, multi-dimensional trade-offs between computational efficiency, memory usage, and model effectiveness. The optimal choice among these approaches depends heavily on specific application requirements, hardware constraints, and whether the priority is processing extremely long sequences or maintaining full attention capabilities <Paper corpusId=\"273228328\" paperTitle=\"(Eisner, 2024)\" isShortName></Paper>.", "citations": [{"id": "(Ding et al., 2021)", "paper": {"corpus_id": 229923177, "title": "ERNIE-Doc: A Retrospective Long-Document Modeling Transformer", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Siyu Ding", "authorId": "2092641069"}, {"name": "Junyuan Shang", "authorId": "40861754"}, {"name": "Shuohuan Wang", "authorId": "104463827"}, {"name": "Yu Sun", "authorId": "2117103617"}, {"name": "Hao Tian", "authorId": "50007795"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 55}, "snippets": ["Sparse Attention Transformers have been extensively explored Tay et al., 2020;Beltagy et al., 2020;Zaheer et al., 2020). The key idea is to sparsify the self-attention operation, which scales quadratically with the sequence length. For instance, the Sparse Transformer  uses a dilated sliding window that reduces the complexity to O(L \u221a L), where L is the sequence length. Reformer (Kitaev et al., 2020) further reduces the complexity to O(L log L) using locality-sensitive hashing attention to compute the nearest neighbors. BP-Transformers (Ye et al., 2019) employs a binary partition for the input sequence. Recently, Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) have been proposed, and both achieved state-of-the-art performance on a variety of long-document tasks. They reduce the complexity of self-attention to O(L) by combining random attention, window attention, and global attention. However, it has been proven in Zaheer et al. (2020) that sparse attention mech-anisms cannot universally replace dense attention mechanisms; moreover, solving the simple problem of finding the furthest vector requires \u03a9(n)-layers of a sparse attention mechanism but only O(1)layers of a dense attention mechanism. In addition, the aforementioned methods require customized CUDA kernels or TVM programming to implement sparse attention, which are not maintainable and are difficult to use."], "score": 0.6767578125}, {"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware."], "score": 0.82763671875}, {"id": "(Bae et al., 2022)", "paper": {"corpus_id": 254725259, "title": "Pro-Attention: Efficient Probability Distribution Matching-Based Attention Through Feature Space Conversion", "year": 2022, "venue": "IEEE Access", "authors": [{"name": "Jongseong Bae", "authorId": "2195944251"}, {"name": "Byung Do Cheon", "authorId": "2196700504"}, {"name": "Ha Young Kim", "authorId": "2129420267"}], "n_citations": 0}, "snippets": ["In Longformer [16], for instance, the full self-attention matrix becomes sparse due to the use of attention patterns such as sliding window, dilated sliding window, and global attention. BigBird (Zaheer et al., 2020) is another model with sparse attention. BigBird introduces random windows and global attention. Longformer and BigBird reduce the quadratic dependency of the original attention mechanism to linear. However, the total computation per layer in those models is bigger than in models with the original attention when the input dimension is large. Hence, BigBird and Longformer are not completely upgraded models of the original transformer which can universally replace dense attention mechanisms but task-specific models for a long sequence length."], "score": 0.81591796875}, {"id": "(Tomczak et al., 2025)", "paper": {"corpus_id": 276106883, "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nathaniel Tomczak", "authorId": "2276535724"}, {"name": "S. Kuppannagari", "authorId": "2873546"}], "n_citations": 0}, "snippets": ["A popular approach for reducing the computational complexity is to introduce sparsity in the attention mechanism (Zaheer et al., 2020)- [8]. In this approach, an L \u00d7 L 0-1 attention mask is used to capture the interactions between pairs. The mask is made sparse by reducing the number of interacting pairs (making the corresponding entries 0) in a structured manner. Local, dilated windowed, block, and random are popular patterns that are used to introduce sparsity while ensuring that critical interactions are preserved to maintain the accuracy of the models. [7] has demonstrated that, even with an exponentially decreasing number of connections with respect to the distance between pairs, the test perplexity scores remain comparable to dense models. This demonstrates that utilizing sparsity to achieve ultra-long sequence modeling is a feasible approach", "However, existing implementations of sparse attention mechanisms are not \"hardware-aware\". They perform densedense matrix multiplication followed by invalidation of entries based on the attention mask [9], [10]. Optimizations have focused on partitioning the matrices into blocks, applying matrix reordering techniques to obtain denser blocks, and performing block matrix multiplication only on the blocks that have nonzero entries in the mask (see Section III for details). However, these techniques end up performing excess computations as the block may contain several 0 mask elements."], "score": 0.82958984375}, {"id": "(Lou et al., 2024)", "paper": {"corpus_id": 270703226, "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chao Lou", "authorId": "2061806821"}, {"name": "Zixia Jia", "authorId": "1453587987"}, {"name": "Zilong Zheng", "authorId": "2266032392"}, {"name": "Kewei Tu", "authorId": "40341553"}], "n_citations": 26}, "snippets": ["Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths.However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs.Clustering-based methods [39,(Roy et al., 2020) allow queries to attend to different sets of KV pairs.In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81,42](Yu et al., 2023).(ii) Previous learnable sparse attention often has super-linear complexity, especially during training.For example, clustering-based methods usually cost O(n log n) to maintain clusters."], "score": 0.74609375}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 264439578, "title": "TRAMS: Training-free Memory Selection for Long-range Language Modeling", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Haofei Yu", "authorId": "2261476365"}, {"name": "Cunxiang Wang", "authorId": "35504092"}, {"name": "Yue Zhang", "authorId": "2261496744"}, {"name": "Wei Bi", "authorId": "2237804371"}], "n_citations": 6}, "snippets": ["The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters."], "score": 0.0}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 237260051, "title": "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Chuhan Wu", "authorId": "15161448"}, {"name": "Fangzhao Wu", "authorId": "2397264"}, {"name": "Tao Qi", "authorId": "50329599"}, {"name": "Yongfeng Huang", "authorId": "1731776"}], "n_citations": 3}, "snippets": ["We compare Smart Bird with the vanilla Transformer model and its several efficient variants based on sparse attention mechanism, including: \n\n(1) Sparse Transformer (Child et al., 2019), a sparse attention based Transformer model that uses a combination of local attention and stride attention; (2) Longformer (Beltagy et al., 2020), a Transformer variant based on sliding window attention and global attention at a few positions; (3) Big Bird (Zaheer et al., 2020), a Transformer variant that integrates local attention, global attention and random attention mechanisms", "First, the performance of efficient Transformer baselines including Sparse Transformer, Longformer and Big Bird outperform the vanilla Transformer in long document modeling (e.g., classification tasks on Amazon, IMDB and MIND). This is because the input text length of the vanilla Transformer is limited by the computing resources, and many useful contexts cannot be exploited when using a relatively short text truncation length. Second, the baseline methods based on sparse attention are inferior to the vanilla Transformer in short sequence modeling (i.e., classification on AG and recommendation on MIND). This is because these methods cannot fully capture the important interactions between different tokens."], "score": 0.88623046875}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 259858862, "title": "Adaptive Attention for Sparse-based Long-sequence Transformer", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xuanyu Zhang", "authorId": "4874730"}, {"name": "Zhepeng Lv", "authorId": "2068974"}, {"name": "Qing Yang", "authorId": "2149535351"}], "n_citations": 7}, "snippets": ["Different from traditional sparse Transformer (Martins et al., 2016)(Correia et al., 2019)(Peters et al., 2019) with different softmax and pattern-related quadratic computation, recent works mainly adopt sliding windows to achieve linear complexity. For example, Longformer (Beltagy et al., 2020) employs an attention pattern that combines local windowed attention with task-motivated global attention while also scaling linearly with the sequence length. BigBird (Zaheer et al., 2020) incorporates random attention (queries attend to random keys) besides global tokens and local sliding windows. However, these hand-crafted attention patterns mentioned above are usually selected empirically or randomly. It is not an ideal solution for modeling long sequences. How to adaptively select useful tokens for sparse attention according to the context is still an important problem to be considered."], "score": 0.60986328125}, {"id": "(Fu et al., 2025)", "paper": {"corpus_id": 276618265, "title": "Sliding Window Attention Training for Efficient Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zichuan Fu", "authorId": "2275537250"}, {"name": "Wentao Song", "authorId": "2347893589"}, {"name": "Yejing Wang", "authorId": "2162455919"}, {"name": "Xian Wu", "authorId": "2277462592"}, {"name": "Yefeng Zheng", "authorId": "2237585282"}, {"name": "Yingying Zhang", "authorId": "2344958511"}, {"name": "Derong Xu", "authorId": "2262514619"}, {"name": "Xuetao Wei", "authorId": "2298206411"}, {"name": "Tong Xu", "authorId": "2151647484"}, {"name": "Xiangyu Zhao", "authorId": "2281902096"}], "n_citations": 2}, "snippets": ["While architectural innovations offer one path to efficiency, research also focuses on optimizing the Transformer itself, particularly through sparse attention patterns to reduce computational cost. \n\nEarly work in this direction focused on structured sparsity patterns. Sparse Transformer (Child et al., 2019) demonstrated that using fixed sparse attention patterns could maintain model performance while significantly reducing computation. This idea was further developed by Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2021), which introduced more sophisticated attention patterns combining local windows with global tokens to capture dependencies effectively. These models, however, still rely on predefined attention patterns, which can limit flexibility."], "score": 0.63427734375}, {"id": "(Chen et al., 2022)", "paper": {"corpus_id": 252216464, "title": "Denoising Self-Attentive Sequential Recommendation", "year": 2022, "venue": "ACM Conference on Recommender Systems", "authors": [{"name": "Huiyuan Chen", "authorId": "1504511015"}, {"name": "Yusan Lin", "authorId": "2410838"}, {"name": "Menghai Pan", "authorId": "29913565"}, {"name": "Lan Wang", "authorId": "2127181454"}, {"name": "Chin-Chia Michael Yeh", "authorId": "3056465"}, {"name": "Xiaoting Li", "authorId": "2185014510"}, {"name": "Yan Zheng", "authorId": "2185013996"}, {"name": "Fei Wang", "authorId": "2148956204"}, {"name": "Hao Yang", "authorId": "2145058012"}], "n_citations": 50}, "snippets": ["Recently, many lightweight Transformers seek to achieve sparse attention maps since not all attentions carry important information in the self-attention layers [2,10,(Guo et al., 2019)29,58]. For instance, Reformer [29] computes attentions based on locality-sensitive hashing, leading to lower memory consumption. Star Transformer (Guo et al., 2019) replaces the fully-connected structure of self-attention with a star-shape topology. Sparse Transformer [10] and Longformer [2] achieve sparsity by using various sparse patterns, such as diagonal sliding windows, dilated sliding windows, local and global sliding windows. BigBird [58] uses random and several fixed patterns to build sparse blocks. It has been shown that these sparse attentions can obtain the state-of-the-art performance and greatly reduce computational complexity. However, many of them rely on fixed attention schemas that lack flexibility and require tremendous engineering efforts."], "score": 0.71435546875}, {"id": "(Correia et al., 2019)", "paper": {"corpus_id": 202538495, "title": "Adaptively Sparse Transformers", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Gon\u00e7alo M. Correia", "authorId": "146783606"}, {"name": "Vlad Niculae", "authorId": "2114966"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "145644643"}], "n_citations": 256}, "snippets": ["Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with alpha-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the alpha parameter \u2013 which controls the shape and sparsity of alpha-entmax \u2013 allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264451707, "title": "CLEX: Continuous Length Extrapolation for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guanzheng Chen", "authorId": "2155315840"}, {"name": "Xin Li", "authorId": "40613621"}, {"name": "Zaiqiao Meng", "authorId": "3451645"}, {"name": "Shangsong Liang", "authorId": "3279808"}, {"name": "Li Bing", "authorId": "2211459675"}], "n_citations": 32}, "snippets": ["Hierarchical Architecture / Sparse Attention. To overcome the quadratic complexity of attention, (Dai et al., 2019) proposes the Transformer-XL that handles the long sequence at segment level by Transformer, with these segments interacting through a recurrence mechanism. The Recurrent Memory Transformer (Bulatov et al., 2022) refines this mechanism by incorporating special memory tokens into the recurrence, which is capable of scaling the context length to the millions (Bulatov et al., 2023). On the other hand, Child et al. (2019); Beltagy et al. (2020) proposed using the sparse attention to circumvent the full access to the long sequences, hence reducing the complexity. The sparse attention has been adopted by Ding et al. (2023a) to scale the context length of transformers into the billions. However, these methods sacrifice the utilisation of the entire sequence during attention, resulting in an inevitable loss of some contextual information."], "score": 0.72216796875}, {"id": "(Xu et al., 2025)", "paper": {"corpus_id": 277151262, "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ruyi Xu", "authorId": "2351045310"}, {"name": "Guangxuan Xiao", "authorId": "2046958974"}, {"name": "Haofeng Huang", "authorId": "2351511467"}, {"name": "Junxian Guo", "authorId": "2325916768"}, {"name": "Song Han", "authorId": "2249530374"}], "n_citations": 15}, "snippets": ["Long-Context Transformer Models (LCTMs) are vital for real-world applications but suffer high computational costs due to attention's quadratic complexity. Block-sparse attention mitigates this by focusing computation on critical regions, yet existing methods struggle with balancing accuracy and efficiency due to costly block importance measurements."], "score": 0.65234375}, {"id": "(Eisner, 2024)", "paper": {"corpus_id": 273228328, "title": "InAttention: Linear Context Scaling for Transformers", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Joseph Eisner", "authorId": "2325097804"}], "n_citations": 0}, "snippets": ["These Foundation Models seem to benefit from extended context length but the selfattention mechanism at the heart of the transformer scales quadratically with context length -making training and inference on extremely long queries cost prohibitive. One way to make long queries cheaper is to make the attention mechanism sparse, not allowing tokens to attend every token before them, but merely a subset. \n\nMohtashami and Jaggi [11] give an excellent summary 1 of sparse attention techniques:", "For example, Child et al. [6] limit the attention to a local window around each token, while BigBird additionally suggests attending to a random subset of previous tokens as well as several globally accessible tokens [18]. Longformer [2] further introduces dilated sliding window patterns to increase attention's receptive field and manually picks the window sizes for each layer. Linformer [17] uses a low-rank approximation of the attention matrix while Performer [7] uses a non-softmax kernel to obtain a more efficient implementation. Reformer [9] uses locality-sensitive-hashing (LSH) to retrieve the closest key vectors which should account for the highest scores in the attention matrix. Combiner [13] utilizes a hierarchical attention mechanism and heuristic reduction techniques, such as max-pooling, to derive key and query vectors for input blocks", "while their own approach involves inserting special landmark tokens to stand in for consecutive blocks -allowing models to restrict their attention to blocks which contain at least one high scoring token."], "score": 0.6142578125}], "table": null}], "cost": 0.6239459999999999}}
