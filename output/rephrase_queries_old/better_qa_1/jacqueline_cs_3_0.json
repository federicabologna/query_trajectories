{"better_query": "What are the advantages and limitations of using size-matched or identically-architected teacher and student models in knowledge distillation, and how does this setup impact student model performance?", "better_answer": {"sections": [{"title": "Introduction to Knowledge Distillation", "tldr": "Knowledge distillation is a technique where a smaller student model learns from a larger teacher model by mimicking its outputs. This approach enables the transfer of knowledge from complex models to simpler ones, which can then perform with similar accuracy while requiring fewer computational resources. (LLM Memory)", "text": "\nKnowledge distillation is a model compression technique first formalized by Hinton et al. in 2015, which involves transferring knowledge from a large, complex model (the teacher) to a smaller, more efficient model (the student). The core idea is that the student model learns not just from hard labels in the training data, but also from the \"soft targets\" or probability distributions produced by the teacher model. These soft targets contain richer information about the relationships between classes than one-hot encoded ground truth labels, often revealing how the teacher model generalizes and what it has learned about the similarities between different categories. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe traditional knowledge distillation process involves training a large teacher model to high accuracy, then using this teacher to guide the training of a smaller student model. The student model is typically trained with a combined loss function that includes both a standard cross-entropy loss against the ground truth labels and a distillation loss that measures the divergence between the student's predictions and the teacher's soft outputs. This approach allows the student to mimic the teacher's behavior while maintaining a smaller footprint in terms of parameters and computational requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nWhile the conventional setup involves a significant size difference between teacher and student, there has been growing interest in scenarios where the teacher and student have identical or similar architectures, which is often referred to as self-distillation or same-architecture knowledge distillation. This approach challenges the traditional notion that knowledge transfer requires a disparity in model capacity. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Same-Architecture Knowledge Distillation (Self-Distillation)", "tldr": "Self-distillation is a specialized form of knowledge distillation where the teacher and student models share identical architectures. This approach challenges conventional wisdom by showing that knowledge transfer can occur effectively between models of the same capacity, often resulting in student models that outperform their teachers. (8 sources)", "text": "\nSelf-distillation represents a distinct variation of knowledge distillation where both the teacher and student networks share the same architecture <Paper corpusId=\"235694419\" paperTitle=\"(Mazumder et al., 2021)\" isShortName></Paper> <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. While Hinton's original knowledge distillation framework emphasized transferring knowledge from larger teacher models to smaller student models, self-distillation demonstrates that knowledge transfer can occur effectively between models of identical capacity <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper>.\n\nA particularly intriguing finding is that students in self-distillation scenarios can actually outperform their teachers, especially in cases where networks are overparameterized and the teacher model is trained with early stopping <Paper corpusId=\"277954831\" paperTitle=\"(Aslam et al., 2025)\" isShortName></Paper> <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. Furlanello et al. introduced the concept of \"Born-Again Networks\" (BANs), demonstrating that students with identical parameterization to their teachers could achieve superior performance across both computer vision and language modeling tasks <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nThe self-distillation process enhances model generalization without requiring additional labeled training data <Paper corpusId=\"235694419\" paperTitle=\"(Mazumder et al., 2021)\" isShortName></Paper>. This improvement occurs because self-distillation effectively regularizes the \"dark knowledge\" (knowledge about incorrect predictions) within a single network by encouraging more consistent and meaningful predictions within each class <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. The process can be conducted in multiple rounds, with each successive round potentially improving performance further <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper>.\n\nSelf-distillation has become an important technique in model compression and acceleration research, offering a way to improve model performance without changing the architecture <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>. This approach has proven particularly valuable in scenarios where computational resources are limited but model performance cannot be compromised.", "citations": [{"id": "(Mazumder et al., 2021)", "paper": {"corpus_id": 235694419, "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation", "year": 2021, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Pratik Mazumder", "authorId": "31222412"}, {"name": "Pravendra Singh", "authorId": "144377059"}, {"name": "Vinay P. Namboodiri", "authorId": "145460361"}], "n_citations": 2}, "snippets": ["When the teacher and student architectures are the same, the knowledge distillation process is referred to as self-distillation. The authors in [20] demonstrate that self-distillation improves the test set performance of the network. The distillation process increases the generalization ability of the network without requiring additional labeled data for training."], "score": 0.80712890625}, {"id": "(Capogrosso et al., 2023)", "paper": {"corpus_id": 262084420, "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Luigi Capogrosso", "authorId": "2135267479"}, {"name": "Federico Cunico", "authorId": "1396330675"}, {"name": "D. Cheng", "authorId": "1780197"}, {"name": "Franco Fummi", "authorId": "2243336023"}, {"name": "Marco Cristani", "authorId": "2238815087"}], "n_citations": 43}, "snippets": ["self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."], "score": 0.69580078125}, {"id": "(Yun et al., 2020)", "paper": {"corpus_id": 214727822, "title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation", "year": 2020, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Sukmin Yun", "authorId": "66863443"}, {"name": "Jongjin Park", "authorId": "2109073979"}, {"name": "Kimin Lee", "authorId": "3436470"}, {"name": "Jinwoo Shin", "authorId": "143720148"}], "n_citations": 281}, "snippets": ["Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."], "score": 0.0}, {"id": "(Boschini et al., 2022)", "paper": {"corpus_id": 245650327, "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Matteo Boschini", "authorId": "51096265"}, {"name": "Lorenzo Bonicelli", "authorId": "2123319338"}, {"name": "Pietro Buzzega", "authorId": "1429191945"}, {"name": "Angelo Porrello", "authorId": "51119730"}, {"name": "S. Calderara", "authorId": "2175529"}], "n_citations": 141}, "snippets": ["While Hinton et al. originally proposed to distillate large teachers -possibly ensembles -into smaller students, further studies revealed additional interesting properties about this technique. In particular, Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."], "score": 0.69140625}, {"id": "(Aslam et al., 2025)", "paper": {"corpus_id": 277954831, "title": "Learning from Stochastic Teacher Representations Using Student-Guided Knowledge Distillation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Muhammad Haseeb Aslam", "authorId": "2057319769"}, {"name": "Clara Martinez", "authorId": "2356582101"}, {"name": "Marco Pedersoli", "authorId": "3048367"}, {"name": "A. Koerich", "authorId": "2263129961"}, {"name": "Ali Etemad", "authorId": "2269469250"}, {"name": "Eric Granger", "authorId": "2256991724"}], "n_citations": 0}, "snippets": ["Advances in self-distillation have shown that when knowledge is distilled from a teacher to a student using the same deep learning (DL) architecture, the student performance can surpass the teacher particularly when the network is overparameterized and the teacher is trained with early stopping."], "score": 0.6591796875}, {"id": "(Furlanello et al., 2018)", "paper": {"corpus_id": 4110009, "title": "Born Again Neural Networks", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tommaso Furlanello", "authorId": "2067208583"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "Michael Tschannen", "authorId": "143902495"}, {"name": "L. Itti", "authorId": "7326223"}, {"name": "Anima Anandkumar", "authorId": "2047844"}], "n_citations": 1034}, "snippets": ["Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."], "score": 0.0}, {"id": "(Fuente et al., 2024)", "paper": {"corpus_id": 273963558, "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ra'ul de la Fuente", "authorId": "2330192141"}, {"name": "Luciano Radrig\u00e1n", "authorId": "2047941908"}, {"name": "Anibal S Morales", "authorId": "2330183718"}], "n_citations": 0}, "snippets": ["Self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020)."], "score": 0.66796875}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}], "table": null}, {"title": "Advantages of Size-Matched or Same-Architecture Models", "tldr": "Size-matched or identical-architecture knowledge distillation setups address the capacity gap problem that can hinder effective knowledge transfer. These approaches can lead to better student performance, sometimes even surpassing the teacher model. (7 sources)", "text": "\n- **Better knowledge transfer efficiency**: When there is a large capacity gap between teacher and student models, knowledge distillation can lose its effectiveness in transferring knowledge, resulting in weaker student performance <Paper corpusId=\"235262724\" paperTitle=\"(Asadian et al., 2021)\" isShortName></Paper>. Using size-matched or similar-capacity models helps overcome this limitation.\n\n- **Capacity matching for optimal distillation**: The effectiveness of distillation may be bounded by the capability of the student model. A simple student with fewer parameters may not be able to approximate a very complex teacher model, making it essential to choose teacher models that match the capacities of student models <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\n- **Potential to outperform teachers**: In self-distillation scenarios, student models can match or even outperform their teachers, which is unusual in traditional knowledge distillation where students typically remain weaker than their teachers <Paper corpusId=\"247446679\" paperTitle=\"(Gong et al., 2022)\" isShortName></Paper>.\n\n- **Avoiding overfitting to complex patterns**: Very complex teacher models may capture finer-grained patterns in data, causing the student model to overfit in some areas while underfitting in others. Size-matched models can mitigate this problem <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.\n\n- **Better performance with synthetic data**: When working with synthetic datasets, relatively weak teacher models can achieve better distillation performance than stronger ones. As teacher model capacity increases beyond what's needed, a significant drop in performance can be observed <Paper corpusId=\"258832674\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n- **Improved optimization dynamics**: The limited capacity in student networks can become a bottleneck for knowledge distillation, implying that increasing the capacity of student models would be beneficial to reduce the performance gap between teacher and student <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>.\n\n- **Building powerful teachers without changing capacity**: An effective approach is to build powerful teacher models without changing the capacity (width and depth) of the student model, allowing for optimal performance transfer between teacher and student <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Asadian et al., 2021)", "paper": {"corpus_id": 235262724, "title": "Distilling Knowledge via Intermediate Classifiers", "year": 2021, "venue": "", "authors": [{"name": "Aryan Asadian", "authorId": "2051713799"}, {"name": "Amirali Salehi-Abari", "authorId": "1403711294"}], "n_citations": 1}, "snippets": ["However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student."], "score": 0.65771484375}, {"id": "(Yuan et al., 2020)", "paper": {"corpus_id": 228376532, "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Fei Yuan", "authorId": "40247395"}, {"name": "Linjun Shou", "authorId": "24962156"}, {"name": "J. Pei", "authorId": "145525190"}, {"name": "Wutao Lin", "authorId": "5617558"}, {"name": "Ming Gong", "authorId": "50175330"}, {"name": "Yan Fu", "authorId": "1832664242"}, {"name": "Daxin Jiang", "authorId": "71790825"}], "n_citations": 122}, "snippets": ["Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models."], "score": 0.708984375}, {"id": "(Gong et al., 2022)", "paper": {"corpus_id": 247446679, "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Yuan Gong", "authorId": "145802952"}, {"name": "Sameer Khurana", "authorId": "40570741"}, {"name": "Andrew Rouditchenko", "authorId": "2110769738"}, {"name": "James R. Glass", "authorId": "145898106"}], "n_citations": 29}, "snippets": ["For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."], "score": 0.76025390625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 258832674, "title": "Is Synthetic Data From Diffusion Models Ready for Knowledge Distillation?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Zheng Li", "authorId": "2146248526"}, {"name": "Yuxuan Li", "authorId": "2116210564"}, {"name": "Penghai Zhao", "authorId": "2284827556"}, {"name": "Renjie Song", "authorId": "2067622132"}, {"name": "Xiang Li", "authorId": "2144439048"}, {"name": "Jian Yang", "authorId": "2146236917"}], "n_citations": 20}, "snippets": ["When training on real datasets, it is common to use a relatively large teacher model to train the student model, such as distilling ResNet34 to ResNet18. In general, smaller teacher models often fail to achieve satisfactory distillation performance compared to larger teacher models. However, when working with synthetic datasets, we observe the opposite phenomenon: relatively weak teacher models can actually achieve better distillation performance than strong ones, as shown in Fig. 3 and Fig. 4. Interestingly, we found that as the capacity of the teacher model increases, a significant drop in performance is observed. Specifically, when training ResNet34 on the synthetic dataset, using ResNet18 as the teacher model leads to a 3% improvement in performance compared to using ResNet50 as the teacher model."], "score": 0.67724609375}, {"id": "(Kang et al., 2019)", "paper": {"corpus_id": 208513309, "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Minsoo Kang", "authorId": "2111623202"}, {"name": "Jonghwan Mun", "authorId": "8511875"}, {"name": "Bohyung Han", "authorId": "40030651"}], "n_citations": 44}, "snippets": ["This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student."], "score": 0.70263671875}, {"id": "(Yu, 2024)", "paper": {"corpus_id": 271956980, "title": "Bring the Power of Diffusion Model to Defect Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuyi Yu", "authorId": "2317142609"}], "n_citations": 1}, "snippets": ["In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."], "score": 0.7265625}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}], "table": null}, {"title": "Limitations of Size-Matched or Same-Architecture Models", "tldr": "While size-matched knowledge distillation offers advantages, it also introduces significant constraints by limiting the diversity of potential teacher models and reducing flexibility in practical applications. These limitations can prevent students from accessing optimal knowledge transfer and restrict deployment in domain-specific or evolving model landscapes. (6 sources)", "text": "\n- **Limited knowledge diversity**: Restricting teachers to similar architectures as students (SAKD - similar-architecture knowledge distillation) can fail to include the optimal knowledge needed to enhance student performance. Heterogeneous teachers sometimes provide better knowledge transfer than homogeneous ones, as demonstrated when distilling from a ViT-Base to ResNet50 yields superior results compared to using a ResNet152 teacher. <Paper corpusId=\"273375639\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **Reduced flexibility in teacher selection**: The emergence of new model architectures or scarcity of well-tuned homogeneous teachers for domain-specific tasks creates significant challenges in finding suitable same-architecture teachers, limiting the practical applicability of similar-architecture knowledge distillation. <Paper corpusId=\"273375639\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3719281\" paperTitle=\"(Ronneberger et al., 2015)\" isShortName></Paper> <Paper corpusId=\"268554202\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Potential weakening of representational differences**: In anomaly detection contexts, using identical structures for student-teacher networks may weaken the representational discrepancy for anomalies, reducing the effectiveness of the approach for detecting unusual patterns. <Paper corpusId=\"267364776\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>\n\n- **Architectural constraints**: Size-matched distillation limits exploration of architectural innovations that could benefit the student model, such as deeper but thinner networks that might generalize better while running faster. <Paper corpusId=\"2723173\" paperTitle=\"(Romero et al., 2014)\" isShortName></Paper>\n\n- **Suboptimal for representation learning**: Same-architecture distillation may not fully capture the structural knowledge of the teacher network, potentially ignoring important representational information that could be better transferred through alternative approaches like contrastive learning. <Paper corpusId=\"204838340\" paperTitle=\"(Tian et al., 2019)\" isShortName></Paper>\n\n- **Trade-off with normal data performance**: While different structures between teacher and student can improve anomaly detection, they may increase the likelihood of divergent performance on normal data, creating a challenging trade-off. <Paper corpusId=\"267364776\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 273375639, "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Guopeng Li", "authorId": "2108494287"}, {"name": "Qiang Wang", "authorId": "2280102525"}, {"name": "Ke Yan", "authorId": "2266750866"}, {"name": "Shouhong Ding", "authorId": "2266754413"}, {"name": "Yuan Gao", "authorId": "2287700762"}, {"name": "Gui-Song Xia", "authorId": "2286883466"}], "n_citations": 0}, "snippets": ["Most existing KD approaches focus on similar-architecture distillation (Romero et al., 2014)(Tian et al., 2019)Liu et al., 2023) (called SAKD), i.e., optional teachers are restricted to a limited scope with structures similar to the student model. However, this homogeneous distillation presents two principal limitations: (1) Limited Potential: Compared to the broader range of arbitrary teachers (including homogeneous and heterogeneous ones), the restricted scope of teachers in SAKD may fail to include the optimal knowledge necessary to enhance the performance of certain students. For instance, as OFA (Hao et al., 2023) demonstrated, distilling knowledge from a heterogeneous ViT-Base to ResNet50 yields superior student performance compared to using a ResNet152 as the homogeneous teacher. (2) Limited Flexibility: The emergence of new models (Liu et al., 2022; Preprint  Tolstikhin et al., 2021) or the scarcity of perfectly tuned homogeneous teachers in domain-specific tasks (Ronneberger et al., 2015)(Li et al., 2024) poses significant challenges in obtaining suitable homogeneous teachers, thereby impeding the applicability of SAKD."], "score": 0.76806640625}, {"id": "(Ronneberger et al., 2015)", "paper": {"corpus_id": 3719281, "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation", "year": 2015, "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "authors": [{"name": "O. Ronneberger", "authorId": "1737326"}, {"name": "P. Fischer", "authorId": "152702479"}, {"name": "T. Brox", "authorId": "1710872"}], "n_citations": 77405}, "snippets": ["There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net ."], "score": 0.0}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 268554202, "title": "Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Guopeng Li", "authorId": "2108494287"}, {"name": "Ming Qian", "authorId": "2292410067"}, {"name": "Gui-Song Xia", "authorId": "2259003019"}], "n_citations": 22}, "snippets": ["This paper investigates the effective utilization of unlabeled data for large-area cross-view gee-localization (CVGL), encompassing both unsupervised and semi-supervised settings. Common approaches to CVGL rely on ground-satellite image pairs and employ label-driven supervised training. However, the cost of collecting precise cross-view image pairs hinders the deployment of CVGL in real-life scenarios. Without the pairs, CVGL will be more challenging to handle the significant imaging and spatial gaps between ground and satellite images. To this end, we propose an unsupervised framework including a cross-view projection to guide the model for retrieving initial pseudo-labels and a fast re-ranking mechanism to refine the pseudo-labels by leveraging the fact that \"the perfectly paired ground-satellite image is located in a unique and identical scene\". The framework exhibits competitive performance compared with supervised works on three open-source benchmarks. Our code and models will be released on https://github.com/liguopeng0923/UCVGL."], "score": 0.0}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 267364776, "title": "Dual-Student Knowledge Distillation Networks for Unsupervised Anomaly Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liyi Yao", "authorId": "2151789151"}, {"name": "Shaobing Gao", "authorId": "2282156422"}], "n_citations": 1}, "snippets": ["Employing identical structures to construct the S-T network may weaken the representative discrepancy on anomalies. But using different structures can increase the likelihood of divergent performance on normal data."], "score": 0.734375}, {"id": "(Romero et al., 2014)", "paper": {"corpus_id": 2723173, "title": "FitNets: Hints for Thin Deep Nets", "year": 2014, "venue": "International Conference on Learning Representations", "authors": [{"name": "Adriana Romero", "authorId": "2069136633"}, {"name": "Nicolas Ballas", "authorId": "2482072"}, {"name": "Samira Ebrahimi Kahou", "authorId": "3127597"}, {"name": "Antoine Chassang", "authorId": "3186079"}, {"name": "C. Gatta", "authorId": "143706039"}, {"name": "Yoshua Bengio", "authorId": "1751762"}], "n_citations": 3899}, "snippets": ["While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network."], "score": 0.0}, {"id": "(Tian et al., 2019)", "paper": {"corpus_id": 204838340, "title": "Contrastive Representation Distillation", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yonglong Tian", "authorId": "2476765"}, {"name": "Dilip Krishnan", "authorId": "1707347"}, {"name": "Phillip Isola", "authorId": "2094770"}], "n_citations": 1054}, "snippets": ["Often we wish to transfer representational knowledge from one neural network to another. Examples include distilling a large network into a smaller one, transferring knowledge from one sensory modality to a second, or ensembling a collection of models into a single estimator. Knowledge distillation, the standard approach to these problems, minimizes the KL divergence between the probabilistic outputs of a teacher and student network. We demonstrate that this objective ignores important structural knowledge of the teacher network. This motivates an alternative objective by which we train a student to capture significantly more information in the teacher's representation of the data. We formulate this objective as contrastive learning. Experiments demonstrate that our resulting new objective outperforms knowledge distillation and other cutting-edge distillers on a variety of knowledge transfer tasks, including single model compression, ensemble distillation, and cross-modal transfer. Our method sets a new state-of-the-art in many transfer tasks, and sometimes even outperforms the teacher network when combined with knowledge distillation. Code: this http URL"], "score": 0.0}], "table": null}, {"title": "Impact of Capacity Gap on Knowledge Distillation", "tldr": "The capacity gap between teacher and student models significantly affects knowledge distillation effectiveness, with overly large disparities leading to performance degradation. Research shows that as this gap widens, students struggle to absorb complex teacher knowledge, creating a fundamental limitation that can be addressed through intermediate models or capacity-matched architectures. (15 sources)", "text": "\nKnowledge distillation faces a critical challenge when there exists a substantial difference in model complexity between teacher and student networks. This phenomenon, known as the capacity gap problem, can significantly diminish the effectiveness of knowledge transfer <Paper corpusId=\"235262724\" paperTitle=\"(Asadian et al., 2021)\" isShortName></Paper> <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>. When the gap becomes too large, the student model simply cannot effectively approximate the intricate patterns captured by the more complex teacher, resulting in weaker overall performance <Paper corpusId=\"278602421\" paperTitle=\"(Alif et al., 2025)\" isShortName></Paper>.\n\nThis limitation manifests in several ways. First, a student with fewer parameters may be fundamentally incapable of representing the finer-grained patterns captured by more complex teacher models <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>. This constraint becomes particularly evident with modern large language models, where the capacity disparity between teacher LLMs and smaller student models has grown dramatically, exacerbating the student's ability to match the teacher's output distribution <Paper corpusId=\"270257777\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nCounterintuitively, research has demonstrated that stronger teacher models do not necessarily produce better student models. In some cases, students distilled from weaker teachers actually outperform those trained with stronger teachers <Paper corpusId=\"237605152\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. This paradoxical finding can be attributed to two factors: first, larger teachers tend to produce less soft logits that are harder to mimic; second, the increased capacity gap weakens the knowledge distillation process <Paper corpusId=\"237605152\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nThe capacity gap also creates architectural challenges. In networks with significantly different depths, intermediate layer matching becomes problematic as multiple teacher layers must be skipped or combined, potentially losing valuable information <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper>. This issue is further complicated when dealing with heterogeneous architectures, where features from different model families reside in different latent spaces, making direct matching ineffective or even detrimental <Paper corpusId=\"276533262\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265871679\" paperTitle=\"(Hao et al., 2023)\" isShortName></Paper>.\n\nTo address these limitations, researchers have proposed several approaches. One promising strategy involves introducing intermediate-sized \"teacher assistant\" models to bridge the gap between large teachers and small students <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper> <Paper corpusId=\"221802641\" paperTitle=\"(Son et al., 2020)\" isShortName></Paper>. Experimental evidence shows that this approach helps students learn more relevant features by providing a smoother knowledge transfer pathway <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper>.\n\nThe fundamental tension in knowledge distillation lies in balancing model interpretability with performance. Simpler student models increase interpretability but widen the capacity gap, making knowledge transfer less effective <Paper corpusId=\"270737867\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215745611\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper>. This trade-off highlights the inherent challenge in model downsizing \u2013 gains in efficiency and deployability often come at the cost of reduced accuracy and capability <Paper corpusId=\"272753230\" paperTitle=\"(Ballout et al., 2024)\" isShortName></Paper>.\n\nWhen designing knowledge distillation approaches, researchers must carefully consider the capacity relationship between teacher and student models. The limited capacity in student networks can become a bottleneck for knowledge distillation, suggesting that increasing student capacity would be beneficial to reduce the performance gap <Paper corpusId=\"208513309\" paperTitle=\"(Kang et al., 2019)\" isShortName></Paper>. Alternatively, building powerful teacher models without increasing their width and depth beyond what the student can absorb offers another effective strategy <Paper corpusId=\"228376532\" paperTitle=\"(Yuan et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Asadian et al., 2021)", "paper": {"corpus_id": 235262724, "title": "Distilling Knowledge via Intermediate Classifiers", "year": 2021, "venue": "", "authors": [{"name": "Aryan Asadian", "authorId": "2051713799"}, {"name": "Amirali Salehi-Abari", "authorId": "1403711294"}], "n_citations": 1}, "snippets": ["However, when there is a large difference between the model complexities of teacher and student (i.e., capacity gap), knowledge distillation loses its strength in transferring knowledge from the teacher to the student, thus training a weaker student."], "score": 0.65771484375}, {"id": "(Kang et al., 2019)", "paper": {"corpus_id": 208513309, "title": "Towards Oracle Knowledge Distillation with Neural Architecture Search", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Minsoo Kang", "authorId": "2111623202"}, {"name": "Jonghwan Mun", "authorId": "8511875"}, {"name": "Bohyung Han", "authorId": "40030651"}], "n_citations": 44}, "snippets": ["This is partly because a large gap in model capacity between student and teacher hinders learning process of KD as discussed in (Mirzadeh et al. 2019), and the simple objective function to fit the representations of the teacher given by model averaging is not effective to take full advantage of teacher models. In other words, the limited capacity in the student network becomes a bottleneck of KD, which implies that increasing capacity of student models would be beneficial to reduce the performance gap between teacher and student."], "score": 0.70263671875}, {"id": "(Alif et al., 2025)", "paper": {"corpus_id": 278602421, "title": "DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images", "year": 2025, "venue": "", "authors": [{"name": "Sadman Sakib Alif", "authorId": "2316207082"}, {"name": "Nasim Anzum Promise", "authorId": "2316207342"}, {"name": "Fiaz Al Abid", "authorId": "2361268094"}, {"name": "Aniqua Nusrat Zereen", "authorId": "9748590"}], "n_citations": 0}, "snippets": ["These methods transfer knowledge from a large teacher to a smaller student model. However, if the student is significantly less complex it may fail to capture and replicate the teacher's intricate outputs."], "score": 0.69189453125}, {"id": "(Yuan et al., 2020)", "paper": {"corpus_id": 228376532, "title": "Reinforced Multi-Teacher Selection for Knowledge Distillation", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Fei Yuan", "authorId": "40247395"}, {"name": "Linjun Shou", "authorId": "24962156"}, {"name": "J. Pei", "authorId": "145525190"}, {"name": "Wutao Lin", "authorId": "5617558"}, {"name": "Ming Gong", "authorId": "50175330"}, {"name": "Yan Fu", "authorId": "1832664242"}, {"name": "Daxin Jiang", "authorId": "71790825"}], "n_citations": 122}, "snippets": ["Surprisingly, a stronger teacher model may not necessarily lead to a better student model. As shown in Table 1 (Sun et al. (2019)), the RoBERTa-Base model performs better than the BERT-Base model on the MRPC and MNLI-mm tasks. However, the student model using three-layer transformer BERT distilled from the weaker teacher model performs better on the same tasks than the same student model distilled from the stronger teacher model. One possible reason is that the effectiveness of distillation may be bounded by the capability of the student model. A simple student model with fewer parameters may not be able to approximate a very complex teacher model, since the complex teacher model may capture finer-grained patterns in data and cause the student model to overfit in some parts of the data and under some other parts. To achieve good distillation, we have to choose teacher models matching capacities of student models."], "score": 0.708984375}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 270257777, "title": "PLaD: Preference-based Large Language Model Distillation with Pseudo-Preference Pairs", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Rongzhi Zhang", "authorId": "2288852433"}, {"name": "Jiaming Shen", "authorId": "2266463492"}, {"name": "Tianqi Liu", "authorId": "2239381730"}, {"name": "Haorui Wang", "authorId": "2266420540"}, {"name": "Zhen Qin", "authorId": "2266819166"}, {"name": "Feng Han", "authorId": "2304748093"}, {"name": "Jialu Liu", "authorId": "2239559694"}, {"name": "Simon Baumgartner", "authorId": "2282531735"}, {"name": "Michael Bendersky", "authorId": "1815447"}, {"name": "Chao Zhang", "authorId": "2305503379"}], "n_citations": 8}, "snippets": ["Second, the capacity gap between the student model and LLM teachers becomes significantly larger compared to the previous instances when a relatively smaller teacher model was employed.This disparity exacerbates the student model's limited ability to fully match the teacher LLM's output distribution."], "score": 0.708984375}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 237605152, "title": "Dynamic Knowledge Distillation for Pre-trained Language Models", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Lei Li", "authorId": "49192881"}, {"name": "Yankai Lin", "authorId": "2149202150"}, {"name": "Shuhuai Ren", "authorId": "1906099"}, {"name": "Peng Li", "authorId": "50492525"}, {"name": "Jie Zhou", "authorId": "2108485135"}, {"name": "Xu Sun", "authorId": "11774802"}], "n_citations": 49}, "snippets": ["We surprisingly find that while the BERT LARGE teacher clearly outperforms the small BERT BASE teacher model, the student model distilled by the BERT BASE teacher achieves better performance on all three datasets. This phenomenon is counter-intuitive as a larger teacher is supposed to provide better supervision signal for the student model. We think that there are two possible factors regarding the size of teacher model that leading to the deteriorated performance:\n\n(1) The predicted logits of the teacher model become less soft as the teacher model becomes larger and more confident about its prediction (Guo et al.,  The teacher model is BERT BASE with 12 layers. 2017; Desai and Durrett, 2020), which decreases the effect of knowledge transfer via the soft targets. We find that a smaller \u03c4 also leads to a decreased performance of the student model, indicating the the less-softened teacher prediction will decrease the student performance. 2 (2) The capacity gap between the teacher and student model increases as the teacher becomes larger. The competency of the student model can not match that of the large teacher model, which weakens the performance of KD.\n\nTo explore the combined influence of these factors, we distill student models with different layers and plot the performance gain compared to directly training the student model without distillation in Figure 2. It can be found that by decreasing the student size, the better supervision from teacher model boosts the performance, while the two counteractive factors dominate as the gap becomes much larger, decreasing the performance gain. We notice that this phenomenon is also observed by Mirzadeh et al. (2020) in computer vision tasks using convolutional networks, showing that it is a widespread issue and"], "score": 0.65234375}, {"id": "(Passban et al., 2020)", "paper": {"corpus_id": 229679667, "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Peyman Passban", "authorId": "5062230"}, {"name": "Yimeng Wu", "authorId": "2000863903"}, {"name": "Mehdi Rezagholizadeh", "authorId": "1924511"}, {"name": "Qun Liu", "authorId": "2115900360"}], "n_citations": 122}, "snippets": ["Experimental results show that intermediate layer matching could be quite effective, but in our study we realized that it may suffer from two shortcomings:\n\n\u2022 If n m, multiple layers in T have to be ignored for distillation but we know that those layers consist of precious information for which we spend expensive resources to learn. This issue is referred to as the skip problem in this paper.\n\n\u2022 Moreover, it seems the way teacher layers are kept/skipped is somewhat arbitrary as there is no particular strategy behind it. Before training, we lack enough knowledge to judge which subset of teacher layers contributes more to the distillation process, so there is a good chance of skipping significant layers if we pick them in an arbitrary fashion. Finding the best subset of layers to distill from requires an exhaustive search or an expert in the field to signify connections. We refer to this issue as the search problem."], "score": 0.7919921875}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276533262, "title": "Decoupled Classifier Knowledge Distillation", "year": 2025, "venue": "PLoS ONE", "authors": [{"name": "Hairui Wang", "authorId": "2195337296"}, {"name": "Mengjie Dong", "authorId": "2346974967"}, {"name": "Guifu Zhu", "authorId": "2181805784"}, {"name": "Ya Li", "authorId": "50024555"}], "n_citations": 0}, "snippets": ["In the case of heterogeneous architectures between teacher and student models (Hao et al., 2023), the features of these models reside in different latent feature spaces, making it challenging to ensure effective alignment of the learned features. Consequently, directly matching these unrelated features is not only unproductive but may also hinder the student model's performance. Furthermore, feature-based distillation methods focus more on local regions, and this localized attention may be insufficient for effectively transferring knowledge from the teacher model to the student model in knowledge distillation [15]. The knowledge embedded in the teacher model is often too complex for the student model to fully absorb and process [16]. Although teacher models are generally more complex and capable of capturing more knowledge, the main challenge lies in distilling this knowledge into a form that is accessible and beneficial for the student model [16]."], "score": 0.72998046875}, {"id": "(Hao et al., 2023)", "paper": {"corpus_id": 265871679, "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhiwei Hao", "authorId": "2147215540"}, {"name": "Jianyuan Guo", "authorId": "2148899357"}, {"name": "Kai Han", "authorId": "3826388"}, {"name": "Yehui Tang", "authorId": "103603255"}, {"name": "Han Hu", "authorId": "2270807097"}, {"name": "Yunhe Wang", "authorId": "2108702980"}, {"name": "Chang Xu", "authorId": "2271675319"}], "n_citations": 68}, "snippets": ["Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."], "score": 0.0}, {"id": "(Adhane et al., 2024)", "paper": {"corpus_id": 277652299, "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Gereziher W. Adhane", "authorId": "7827164"}, {"name": "Mohammad Mahdi Dehshibi", "authorId": "2178547"}, {"name": "Dennis Vetter", "authorId": "2313639114"}, {"name": "David Masip", "authorId": "2284862027"}, {"name": "Gemma Roig", "authorId": "2313639123"}], "n_citations": 0}, "snippets": ["The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18", "Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction."], "score": 0.78271484375}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}, {"id": "(Son et al., 2020)", "paper": {"corpus_id": 221802641, "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants", "year": 2020, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Wonchul Son", "authorId": "1557388434"}, {"name": "Jaemin Na", "authorId": "107598106"}, {"name": "Wonjun Hwang", "authorId": "34600044"}], "n_citations": 119}, "snippets": ["With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1"], "score": 0.0}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 270737867, "title": "InFiConD: Interactive No-code Fine-tuning with Concept-based Knowledge Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinbin Huang", "authorId": "8669194"}, {"name": "Wenbin He", "authorId": "2003575022"}, {"name": "Liangke Gou", "authorId": "52158581"}, {"name": "Liu Ren", "authorId": "2265644812"}, {"name": "C. Bryan", "authorId": "2256515852"}], "n_citations": 0}, "snippets": ["Existing methods (e.g., (Kim et al., 2018)(Passalis et al., 2020)64,82]) often prioritize student model performance at the cost of introducing complex architectures.This increased complexity hinders the adoption of KD (Gou et al., 2020) and negatively impacts subsequent fine-tuning processes, such as identifying the causes of underperformance, devising improvement strategies, and executing them effectively (Wang et al., 2020)(Zhang et al., 2022).Conversely, \"simpler\" student models can increase interpretability but also widen the capacity gap between the student and teacher models, making it challenging to transfer the desired knowledge effectively [56].Therefore, striking an appropriate balance between high student effectiveness and suitable interpretability is crucial for successful knowledge distillation and fine-tuning processes."], "score": 0.734375}, {"id": "(Wang et al., 2020)", "paper": {"corpus_id": 215745611, "title": "Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks", "year": 2020, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Lin Wang", "authorId": "2144734901"}, {"name": "Kuk-Jin Yoon", "authorId": "51182421"}], "n_citations": 700}, "snippets": ["Deep neural models, in recent years, have been successful in almost every field, even solving the most complex problem statements. However, these models are huge in size with millions (and even billions) of parameters, demanding heavy computation power and failing to be deployed on edge devices. Besides, the performance boost is highly dependent on redundant labeled data. To achieve faster speeds and to handle the problems caused by the lack of labeled data, knowledge distillation (KD) has been proposed to transfer information learned from one model to another. KD is often characterized by the so-called \u2018Student-Teacher\u2019 (S-T) learning framework and has been broadly applied in model compression and knowledge transfer. This paper is about KD and S-T learning, which are being actively studied in recent years. First, we aim to provide explanations of what KD is and how/why it works. Then, we provide a comprehensive survey on the recent progress of KD methods together with S-T frameworks typically used for vision tasks. In general, we investigate some fundamental questions that have been driving this research area and thoroughly generalize the research progress and technical details. Additionally, we systematically analyze the research status of KD in vision applications. Finally, we discuss the potentials and open challenges of existing methods and prospect the future directions of KD and S-T learning."], "score": 0.0}, {"id": "(Ballout et al., 2024)", "paper": {"corpus_id": 272753230, "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights", "year": 2024, "venue": "International Conference on Applications of Natural Language to Data Bases", "authors": [{"name": "Mohamad Ballout", "authorId": "1491169373"}, {"name": "U. Krumnack", "authorId": "1751765"}, {"name": "Gunther Heidemann", "authorId": "2238205582"}, {"name": "Kai-Uwe K\u00fchnberger", "authorId": "1743582"}], "n_citations": 3}, "snippets": ["This limitation underscores the inherent challenge in knowledge distillation and model scaling. While our method effectively transfers knowledge from a large to a small model, the reduced capacity of the smaller model limits its ability to fully replicate the performance of its larger counterpart. This performance discrepancy highlights the trade-offs involved in model downsizing, where gains in efficiency and deployability often come at the cost of reduced accuracy and overall capability."], "score": 0.92626953125}], "table": null}, {"title": "Alternative Approaches and Solutions", "tldr": "Researchers have developed various strategies to overcome the challenges of capacity gaps in knowledge distillation, including cross-architecture approaches, intermediate teacher assistants, and feature alignment techniques that create common representation spaces between heterogeneous models. (8 sources)", "text": "\nWhen faced with the limitations of size-matched or similar-architecture knowledge distillation, researchers have developed several alternative approaches to enhance knowledge transfer effectiveness. One key strategy is cross-architecture distillation, which involves teacher and student models with different architectures <Paper corpusId=\"251135436\" paperTitle=\"(Galke et al., 2021)\" isShortName></Paper>. While this approach introduces new challenges, it also provides greater flexibility in teacher selection and potentially better knowledge transfer.\n\nA promising solution to address the capacity gap problem is the use of intermediate \"teacher assistant\" models that bridge the knowledge transfer between large teachers and small students. This multi-step knowledge distillation approach helps students learn more relevant features by creating a smoother knowledge transfer pathway <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. Experiments comparing direct distillation from ResNet-101 to ResNet-18 versus using a ResNet-50 teacher assistant showed that the latter approach helped students learn more salient features while avoiding irrelevant ones <Paper corpusId=\"277652299\" paperTitle=\"(Adhane et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221802641\" paperTitle=\"(Son et al., 2020)\" isShortName></Paper>.\n\nAnother important distinction in knowledge distillation approaches is between black-box and white-box methodologies. Black-box knowledge distillation involves the student learning solely from the teacher's outputs without access to internal mechanisms, which is particularly useful when teacher models are proprietary or architecturally distinct. In contrast, white-box distillation allows students to access the teacher's internal states, enabling deeper and more precise learning, though requiring careful architectural alignment <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\nFor heterogeneous architectures, traditional feature-based methods often perform poorly due to differences in feature representation. These substantial differences in meta-architecture, input format, and spatial representation prevent the use of simple similarity metrics for aligning stage-wise feature representations <Paper corpusId=\"270094801\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. To address this challenge, researchers have developed approaches like OFA-KD, which projects intermediate features into an aligned latent space where architecture-specific information is discarded, significantly improving distillation performance between heterogeneous architectures <Paper corpusId=\"270094801\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265871679\" paperTitle=\"(Hao et al., 2023)\" isShortName></Paper>.\n\nSimple Knowledge Distillation (SimKD) represents another innovative approach that allows compression of powerful teacher models into lightweight students without sacrificing performance. This technique reuses the teacher's discriminative classifier for student inference while training the student encoder through feature alignment with a single loss. By incorporating a projector to match the student encoder with the teacher classifier, SimKD achieves state-of-the-art results across various teacher-student architecture combinations, though at the cost of a reduced compression ratio <Paper corpusId=\"259378480\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>.\n\nThese alternative approaches demonstrate that effective knowledge transfer is possible even between models with significant capacity differences, providing practical solutions to overcome the limitations of size-matched or identical-architecture knowledge distillation.", "citations": [{"id": "(Galke et al., 2021)", "paper": {"corpus_id": 251135436, "title": "General Cross-Architecture Distillation of Pretrained Language Models into Matrix Embeddings", "year": 2021, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Lukas Galke", "authorId": "11220282"}, {"name": "Isabelle Cuber", "authorId": "2127384933"}, {"name": "Christophe Meyer", "authorId": "2115007899"}, {"name": "Henrik Ferdinand Nolscher", "authorId": "2127389543"}, {"name": "Angelina Sonderecker", "authorId": "2127393042"}, {"name": "A. Scherp", "authorId": "1753135"}], "n_citations": 2}, "snippets": ["When student and teacher models have a different architecture, we call that cross-architecture distillation."], "score": 0.74755859375}, {"id": "(Adhane et al., 2024)", "paper": {"corpus_id": 277652299, "title": "On Explaining Knowledge Distillation: Measuring and Visualising the Knowledge Transfer Process", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Gereziher W. Adhane", "authorId": "7827164"}, {"name": "Mohammad Mahdi Dehshibi", "authorId": "2178547"}, {"name": "Dennis Vetter", "authorId": "2313639114"}, {"name": "David Masip", "authorId": "2284862027"}, {"name": "Gemma Roig", "authorId": "2313639123"}], "n_citations": 0}, "snippets": ["The Student's performance often declines when there is a large architecture (capacity) gap between the Teacher and the Student (Mirzadeh et al., 2019)(Stanton et al., 2021). The drop in the Student's performance may stem from either its own challenges in learning relevant features or the overwhelming knowledge of the Teacher. To investigate this issue, we employ two distillation strategies in our experiments using ResNet-101 as the Teacher and ResNet-18 as the Student, which have a significant capacity disparity. In the first approach, we conduct direct distillation from ResNet-101 to ResNet-18. The second approach introduces an intermediate \"Teacher assistant\" (Son et al., 2020) to help bridge the capacity gap between ResNet-101 and ResNet-18", "Figure 7 compares the saliency maps of the distilled features learned by two Students: ResNet-18 directly distilled from ResNet-101 (R18-R101) and ResNet-18 distilled from ResNet-101 through Teacher assistant ResNet-50 (R18-R50-R101). The saliency maps, visualised using UniCAM, reveal that the Teacher assistant helps learn more relevant features that highlight the object parts. In contrast, R18-R101 learns some irrelevant features and misses the salient features for the gt prediction."], "score": 0.78271484375}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}, {"id": "(Son et al., 2020)", "paper": {"corpus_id": 221802641, "title": "Densely Guided Knowledge Distillation using Multiple Teacher Assistants", "year": 2020, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Wonchul Son", "authorId": "1557388434"}, {"name": "Jaemin Na", "authorId": "107598106"}, {"name": "Wonjun Hwang", "authorId": "34600044"}], "n_citations": 119}, "snippets": ["With the success of deep neural networks, knowledge distillation which guides the learning of a small student network from a large teacher network is being actively studied for model compression and transfer learning. However, few studies have been performed to resolve the poor learning issue of the student network when the student and teacher model sizes significantly differ. In this paper, we propose a densely guided knowledge distillation using multiple teacher assistants that gradually decreases the model size to efficiently bridge the large gap between the teacher and student networks. To stimulate more efficient learning of the student network, we guide each teacher assistant to every other smaller teacher assistants iteratively. Specifically, when teaching a smaller teacher assistant at the next step, the existing larger teacher assistants from the previous step are used as well as the teacher network. Moreover, we design stochastic teaching where, for each mini-batch, a teacher or teacher assistants are randomly dropped. This acts as a regularizer to improve the efficiency of teaching of the student network. Thus, the student can always learn salient distilled knowledge from the multiple sources. We verified the effectiveness of the proposed method for a classification task using CIFAR-10, CIFAR-100, and ImageNet. We also achieved significant performance improvements with various backbone architectures such as ResNet, WideResNet, and VGG.1"], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 272368391, "title": "On-Device Language Models: A Comprehensive Review", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiajun Xu", "authorId": "2316519813"}, {"name": "Zhiyuan Li", "authorId": "2294674012"}, {"name": "Wei Chen", "authorId": "2294845809"}, {"name": "Qun Wang", "authorId": "2316514278"}, {"name": "Xin Gao", "authorId": "2319809164"}, {"name": "Qi Cai", "authorId": "2364055424"}, {"name": "Ziyuan Ling", "authorId": "2319410023"}], "n_citations": 35}, "snippets": ["This approach involves the student model learning solely from the outputs of the teacher model, without access to its internal mechanics or parameters. It is particularly advantageous when the teacher model's details are proprietary or when the architectures of the teacher and student models differ markedly. For instance, Gu et al. (2023) demonstrated that black-box KD could effectively train models using only the output data from LLM APIs like ChatGPT. The student model trains to emulate the teacher's output distribution based on input-output pairs, a process that, while effective, limits learning to external behaviors without tapping into the teacher's deeper internal states", "In contrast, White-box Knowledge Distillation allows the student model to access the internal states and workings of the teacher, facilitating a deeper and more precise learning process. This method enables the student to mimic not just the outputs but also the internal state distributions of the teacher, enhancing learning efficacy and depth. The increased access to the teacher's detailed workings helps guide the student's learning, resulting in more accurate and robust models. However, this technique requires a careful alignment of model architectures to ensure effective knowledge transfer and is generally more complex to implement."], "score": 0.76025390625}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 270094801, "title": "Aligning in a Compact Space: Contrastive Knowledge Distillation between Heterogeneous Architectures", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hongjun Wu", "authorId": "2243295129"}, {"name": "Li Xiao", "authorId": "2303803900"}, {"name": "Xingkuo Zhang", "authorId": "2303860883"}, {"name": "Yining Miao", "authorId": "2303653169"}], "n_citations": 1}, "snippets": ["Knowledge distillation is commonly employed to compress neural networks, reducing the inference costs and memory footprint. In the scenario of homogenous architecture, feature-based methods have been widely validated for their effectiveness. However, in scenarios where the teacher and student models are of heterogeneous architectures, the inherent differences in feature representation significantly degrade the performance of these methods.\n\nExisting knowledge distillation methods primarily employ logit [1,2,3,4] or intermediate feature maps [5,6,7]8] as the medium for knowledge transfer", "While these methods often excel in knowledge distillation with homogeneous architectures due to the intrinsic similarity of feature representations, they face serious setback in heterogeneous settings (Hao et al., 2023).The substantial differences in meta-architecture, input format, and spatial representation between heterogeneous models preclude the use of simple similarity metrics for aligning stage-wise feature representations.Moreover, mimicing the local details in feature representations may lead to detrimental guidance."], "score": 0.80126953125}, {"id": "(Hao et al., 2023)", "paper": {"corpus_id": 265871679, "title": "One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhiwei Hao", "authorId": "2147215540"}, {"name": "Jianyuan Guo", "authorId": "2148899357"}, {"name": "Kai Han", "authorId": "3826388"}, {"name": "Yehui Tang", "authorId": "103603255"}, {"name": "Han Hu", "authorId": "2270807097"}, {"name": "Yunhe Wang", "authorId": "2108702980"}, {"name": "Chang Xu", "authorId": "2271675319"}], "n_citations": 68}, "snippets": ["Knowledge distillation~(KD) has proven to be a highly effective approach for enhancing model performance through a teacher-student training scheme. However, most existing distillation methods are designed under the assumption that the teacher and student models belong to the same model family, particularly the hint-based approaches. By using centered kernel alignment (CKA) to compare the learned features between heterogeneous teacher and student models, we observe significant feature divergence. This divergence illustrates the ineffectiveness of previous hint-based methods in cross-architecture distillation. To tackle the challenge in distilling heterogeneous models, we propose a simple yet effective one-for-all KD framework called OFA-KD, which significantly improves the distillation performance between heterogeneous architectures. Specifically, we project intermediate features into an aligned latent space such as the logits space, where architecture-specific information is discarded. Additionally, we introduce an adaptive target enhancement scheme to prevent the student from being disturbed by irrelevant information. Extensive experiments with various architectures, including CNN, Transformer, and MLP, demonstrate the superiority of our OFA-KD framework in enabling distillation between heterogeneous architectures. Specifically, when equipped with our OFA-KD, the student models achieve notable performance improvements, with a maximum gain of 8.0% on the CIFAR-100 dataset and 0.7% on the ImageNet-1K dataset. PyTorch code and checkpoints can be found at https://github.com/Hao840/OFAKD."], "score": 0.0}, {"id": "(Gao, 2023)", "paper": {"corpus_id": 259378480, "title": "Review of Recent Distillation Studies", "year": 2023, "venue": "MATEC Web of Conferences", "authors": [{"name": "Min Gao", "authorId": "2147417425"}], "n_citations": 0}, "snippets": ["The use of Simple Knowledge Distillation (SimKD) allows the compression of powerful teacher models into lightweight student models without sacrificing performance. This is achieved by reusing the discriminative classifier of the teacher model for student inference and training the student encoder by feature alignment with a single loss. A projector was also developed to help match the student encoder with the teacher classifier, making the technique applicable to a variety of teacher and student architectures. Experiments show that with the addition of the projector, the technique achieves state-of-the-art results at the cost of a reduced compression ratio."], "score": 0.68310546875}], "table": null}], "cost": 0.37642200000000003}}
