{"better_query": "What are recent advances in synthetic data generation techniques for solving hard reasoning tasks?", "better_answer": {"sections": [{"title": "Introduction to Synthetic Data Generation", "tldr": "Synthetic data generation has evolved from basic autoencoder architectures to sophisticated models like GANs, VAEs, and diffusion models, enabling high-quality data creation across various domains. These techniques address challenges of data scarcity, privacy concerns, and annotation costs by generating diverse, customizable datasets for training AI systems. (11 sources)", "text": "\nSynthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. This approach has emerged as a promising solution to address fundamental challenges in AI development, including data scarcity and the high costs associated with data collection and annotation <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper>.\n\nThe field has witnessed significant evolution in generative techniques. Early approaches centered around basic autoencoder architectures <Paper corpusId=\"276903896\" paperTitle=\"(Kuo et al., 2025)\" isShortName></Paper>, but recent advances have led to more sophisticated methods for modeling complex high-dimensional datasets while addressing privacy concerns <Paper corpusId=\"269702974\" paperTitle=\"(Yadav et al., 2024)\" isShortName></Paper>. Among the most prominent approaches are:\n\n1. **Generative Adversarial Networks (GANs)**: These models work by pitting a generator against a discriminator, resulting in two highly skilled networks after training <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. GANs have demonstrated remarkable success in generating realistic images from text descriptions <Paper corpusId=\"1563370\" paperTitle=\"(Reed et al., 2016)\" isShortName></Paper> and are frequently the best-performing synthetic data generation method due to their adaptability to various tasks <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\n2. **Variational Autoencoders (VAEs)**: These encode input data into a lower-dimensional representation and then decode it back, making them especially useful for learning from data with disentangled features <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>. VAEs excel at generating valid data from this compressed representation.\n\n3. **Diffusion Models**: Denoising diffusion probabilistic models (DDPMs) have achieved state-of-the-art results in image generation tasks <Paper corpusId=\"219955663\" paperTitle=\"(Ho et al., 2020)\" isShortName></Paper>, with impressive quality metrics on standard datasets.\n\n4. **Recurrent Neural Networks**: These are particularly suitable for generating sequential data of arbitrary length, making them ideal for tasks like speech synthesis, music, and time series generation <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\n5. **Virtual Environments**: Computer simulations where algorithms interact based on predefined rules to generate synthetic data <Paper corpusId=\"275916340\" paperTitle=\"(Karst et al., 2025)\" isShortName></Paper>.\n\nIn the context of language models, synthetic data generation for post-training has taken several approaches. Traditional methods rely on a small set of high-quality human-annotated seed samples, using large language models (LLMs) to create diverse augmentations <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper>. These include sampling seed instructions as few-shot examples <Paper corpusId=\"273025760\" paperTitle=\"(Toshniwal et al., 2024)\" isShortName></Paper>, rephrasing seed samples <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>, or leveraging pre-training data as an underutilized resource for scalable generation <Paper corpusId=\"269605607\" paperTitle=\"(Yue et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269981934\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nMore recent approaches, such as instruction backtranslation, automatically label human-written text with corresponding instructions, enabling the construction of high-quality instruction-following language models <Paper corpusId=\"260866107\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This method has proven particularly effective for mathematical reasoning tasks, where model performance has seen significant improvements through synthetic data generation <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Karst et al., 2025)", "paper": {"corpus_id": 275916340, "title": "SynDEc: A Synthetic Data Ecosystem", "year": 2025, "venue": "Electronic Markets", "authors": [{"name": "F. Karst", "authorId": "2277750192"}, {"name": "M. Li", "authorId": "2263394895"}, {"name": "J. Leimeister", "authorId": "1737216"}], "n_citations": 2}, "snippets": ["Synthetic data can be defined as \"data that has been generated using a purpose-built mathematical model or algorithm, with the aim of solving a (set of) data science task(s)\" (Jordon et al., 2022, p. 5). This generation process can take many forms as comprehensively categorized by Bauer et al. (2024) into 20 distinct method types. Among these, generative adversarial networks (GANs) are the most popular. GANs learn by pitting a generator (synthesizes data from random noise) and a discriminator (classifies samples as real or fake) against each other, resulting in two highly skilled networks (Goodfellow et al., 2014). This architecture is highly adaptable, as discriminator and generator can be easily adjusted to new tasks (e.g., time series or graph generation) while being frequently the best-performing synthetic data generation method (Bauer et al., 2024). Another commonly employed synthetic data generation method is autoencoder-based architectures, especially variational autoencoder (VAE) (Kingma & Welling, 2013). VAEs are trained by mapping an input sample to a hidden representation, which is then mapped back to the original vector, thus creating a model that synthesizes valid data from a lower dimensional representation. This decoder model is then used to generate data from random noise which makes it especially useful for learning from data with disentangled features (Bauer et al., 2024). Third, recurrent neural networks, feedforward neural networks which include recurrent edges, are able to generate sequential data of arbitrary length. This makes them ideal for sequence generation tasks such as speech synthesis, music, and time series generation (Lipton et al., 2015). Finally, virtual environments are computer simulations in which algorithms interact with each other based on predefined rules, generating synthetic data in the process (Bonabeau, 2002)."], "score": 0.91748046875}, {"id": "(Qin et al., 2025)", "paper": {"corpus_id": 277313659, "title": "Scaling Laws of Synthetic Data for Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zeyu Qin", "authorId": "2118242824"}, {"name": "Qingxiu Dong", "authorId": "2287927238"}, {"name": "Xingxing Zhang", "authorId": "2284863493"}, {"name": "Li Dong", "authorId": "2294850817"}, {"name": "Xiaolong Huang", "authorId": "2116768132"}, {"name": "Ziyi Yang", "authorId": "2291073936"}, {"name": "Mahmoud Khademi", "authorId": "2268760479"}, {"name": "Dongdong Zhang", "authorId": "2273919921"}, {"name": "H. Awadalla", "authorId": "3032929"}, {"name": "Yi R. Fung", "authorId": "2352012716"}, {"name": "Weizhu Chen", "authorId": "2347682196"}, {"name": "Minhao Cheng", "authorId": "2258337019"}, {"name": "Furu Wei", "authorId": "2323870436"}], "n_citations": 7}, "snippets": ["Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods."], "score": 0.96337890625}, {"id": "(Kuo et al., 2025)", "paper": {"corpus_id": 276903896, "title": "Attention-Based Synthetic Data Generation for Calibration-Enhanced Survival Analysis: A Case Study for Chronic Kidney Disease Using Electronic Health Records", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nicholas I-Hsien Kuo", "authorId": "2265851129"}, {"name": "Blanca Gallego", "authorId": "2292255090"}, {"name": "Louisa R Jorm", "authorId": "2292472962"}], "n_citations": 0}, "snippets": ["Synthetic data generation has long been a core component of ML development, progressing from autoencoders (Hinton et al., 2011) to sophisticated models like variational autoencoders (VAEs) (Lopez et al., 2020), generative adversarial networks (GANs) [18], and denoising diffusion probabilistic models (DDPMs) (Ho et al., 2020). These methods have achieved exceptional success in domains such as image generation (195944196), natural language processing in text (Reed et al., 2016), and text-to-video generation [47]."], "score": 0.91552734375}, {"id": "(Yadav et al., 2024)", "paper": {"corpus_id": 269702974, "title": "Rigorous Experimental Analysis of Tabular Data Generated using TVAE and CTGAN", "year": 2024, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Parul Yadav", "authorId": "2300552611"}, {"name": "Manish Gaur", "authorId": "2300755856"}, {"name": "Rahul Kumar Madhukar", "authorId": "94670511"}, {"name": "Gaurav Verma", "authorId": "2300646169"}, {"name": "Pankaj Kumar", "authorId": "2300984779"}, {"name": "Nishat Fatima", "authorId": "2206080534"}, {"name": "Saqib Sarwar", "authorId": "2277471527"}, {"name": "Yash Raj Dwivedi", "authorId": "2300596310"}], "n_citations": 1}, "snippets": ["Recent advances in generative models have led to more efficient modeling of complex high-dimensional datasets. Also, privacy concerns have led to the development of robust models with lesser risk of privacy breaches."], "score": 0.92333984375}, {"id": "(Reed et al., 2016)", "paper": {"corpus_id": 1563370, "title": "Generative Adversarial Text to Image Synthesis", "year": 2016, "venue": "International Conference on Machine Learning", "authors": [{"name": "Scott E. Reed", "authorId": "144828948"}, {"name": "Zeynep Akata", "authorId": "2893664"}, {"name": "Xinchen Yan", "authorId": "3084614"}, {"name": "Lajanugen Logeswaran", "authorId": "2876316"}, {"name": "B. Schiele", "authorId": "48920094"}, {"name": "Honglak Lee", "authorId": "1697141"}], "n_citations": 3149}, "snippets": ["Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions."], "score": 0.0}, {"id": "(Ho et al., 2020)", "paper": {"corpus_id": 219955663, "title": "Denoising Diffusion Probabilistic Models", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jonathan Ho", "authorId": "2126278"}, {"name": "Ajay Jain", "authorId": "1623995772"}, {"name": "P. Abbeel", "authorId": "1689992"}], "n_citations": 18352}, "snippets": ["We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at this https URL"], "score": 0.0}, {"id": "(Toshniwal et al., 2024)", "paper": {"corpus_id": 273025760, "title": "OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Shubham Toshniwal", "authorId": "2634203"}, {"name": "Wei Du", "authorId": "2323910998"}, {"name": "Ivan Moshkov", "authorId": "2284217750"}, {"name": "B. Kisa\u010danin", "authorId": "9107622"}, {"name": "Alexan Ayrapetyan", "authorId": "2323782209"}, {"name": "Igor Gitman", "authorId": "25683112"}], "n_citations": 71}, "snippets": ["Mathematical reasoning continues to be a critical challenge in large language model (LLM) development with significant interest. However, most of the cutting-edge progress in mathematical reasoning with LLMs has become \\emph{closed-source} due to lack of access to training data. This lack of data access limits researchers from understanding the impact of different choices for synthesizing and utilizing the data. With the goal of creating a high-quality finetuning (SFT) dataset for math reasoning, we conduct careful ablation experiments on data synthesis using the recently released \\texttt{Llama3.1} family of models. Our experiments show that: (a) solution format matters, with excessively verbose solutions proving detrimental to SFT performance, (b) data generated by a strong teacher outperforms equally-sized data generated by a weak student model, (c) SFT is robust to low-quality solutions, allowing for imprecise data filtering, and (d) question diversity is crucial for achieving data scaling gains. Based on these insights, we create the OpenMathInstruct-2 dataset, which consists of 14M question-solution pairs ($\\approx$ 600K unique questions), making it nearly eight times larger than the previous largest open-source math reasoning dataset. Finetuning the \\texttt{Llama-3.1-8B-Base} using OpenMathInstruct-2 outperforms \\texttt{Llama3.1-8B-Instruct} on MATH by an absolute 15.9\\% (51.9\\% $\\rightarrow$ 67.8\\%). Finally, to accelerate the open-source efforts, we release the code, the finetuned models, and the OpenMathInstruct-2 dataset under a commercially permissive license."], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 262084051, "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "L. Yu", "authorId": "2112584251"}, {"name": "Weisen Jiang", "authorId": "2152123946"}, {"name": "Han Shi", "authorId": "152751416"}, {"name": "Jincheng Yu", "authorId": "2193887687"}, {"name": "Zhengying Liu", "authorId": "2239065052"}, {"name": "Yu Zhang", "authorId": "2153638098"}, {"name": "James T. Kwok", "authorId": "2243335442"}, {"name": "Zheng Li", "authorId": "121544682"}, {"name": "Adrian Weller", "authorId": "145689461"}, {"name": "Weiyang Liu", "authorId": "2243412679"}], "n_citations": 395}, "snippets": ["Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use."], "score": 0.0}, {"id": "(Yue et al., 2024)", "paper": {"corpus_id": 269605607, "title": "MAmmoTH2: Scaling Instructions from the Web", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xiang Yue", "authorId": "2284988933"}, {"name": "Tuney Zheng", "authorId": "2300091474"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Wenhu Chen", "authorId": "2249847177"}], "n_citations": 101}, "snippets": ["Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."], "score": 0.0}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 269981934, "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kun Zhou", "authorId": "2265383494"}, {"name": "Beichen Zhang", "authorId": "2107926615"}, {"name": "Jiapeng Wang", "authorId": "2302813110"}, {"name": "Zhipeng Chen", "authorId": "2111335050"}, {"name": "Wayne Xin Zhao", "authorId": "2257376413"}, {"name": "Jing Sha", "authorId": "2165225571"}, {"name": "Zhichao Sheng", "authorId": "2125340023"}, {"name": "Shijin Wang", "authorId": "2302793582"}, {"name": "Ji-Rong Wen", "authorId": "2274218622"}], "n_citations": 34}, "snippets": ["Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 260866107, "title": "Self-Alignment with Instruction Backtranslation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xian Li", "authorId": "2116235416"}, {"name": "Ping Yu", "authorId": "2114104308"}, {"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "J. Weston", "authorId": "145183709"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 134}, "snippets": ["We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment."], "score": 0.0}], "table": null}, {"title": "Large Language Models (LLMs) for Synthetic Data", "tldr": "Large language models have emerged as powerful tools for synthetic data generation, enabling the creation of high-quality training datasets for complex reasoning tasks through techniques like hierarchical prompting, reverse task generation, and iterative bootstrapping. These approaches have been successfully applied to diverse domains including mathematical reasoning, code generation, and commonsense reasoning, often outperforming traditional data collection methods. (15 sources)", "text": "\nThe use of large language models (LLMs) as synthetic data generators has revolutionized the way researchers approach data scarcity challenges. One key insight driving this innovation is that LLMs can generate useful data even for tasks they cannot directly solve. Josifoski et al. demonstrated this by prompting LLMs to perform tasks in the reverse direction\u2014generating plausible input text for target output structures\u2014enabling the production of large-scale, high-quality data for complex tasks such as closed information extraction <Paper corpusId=\"257378179\" paperTitle=\"(Josifoski et al., 2023)\" isShortName></Paper>.\n\nSynthetic data generation with LLMs has been applied across numerous domains. For commonsense reasoning, researchers have shown that general language models can author commonsense knowledge graphs, effectively distilling knowledge symbolically as text <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238857304\" paperTitle=\"(West et al., 2021)\" isShortName></Paper>. In the realm of mathematical reasoning, researchers have curated diverse corpora of mathematics problems with chain-of-thought or program-of-thought annotations <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper>. For code generation, approaches include synthesizing programming textbooks and exercises, and generating programming puzzles with verified solutions <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"251197051\" paperTitle=\"(Haluptzok et al., 2022)\" isShortName></Paper>.\n\nSeveral innovative techniques have emerged to enhance synthetic data quality:\n\n1. **Hierarchical prompting** methods promote dataset diversity by conditioning generation on topics, target audiences, and specific attributes. This approach has been used to create children's stories, textbooks, and code exercises <Paper corpusId=\"272593422\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n2. **Textbook-style generation** combines conceptual explanations with practical examples. Intellecta, for instance, creates dual-composed content that simulates complex thought processes alongside textbook-style elucidations <Paper corpusId=\"269294098\" paperTitle=\"(Ajmal et al., 2024)\" isShortName></Paper>.\n\n3. **Rephrasing approaches** enhance data quality by transforming existing texts. While simple rephrasing primarily affects grammatical styles <Paper corpusId=\"273403575\" paperTitle=\"(Akter et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267312030\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper>, more advanced techniques like MIND add semantic variations and structured complexity through conversational synthetic data generation <Paper corpusId=\"273403575\" paperTitle=\"(Akter et al., 2024)\" isShortName></Paper>.\n\n4. **Self-training methods** utilize synthetic data for iterative fine-tuning, significantly improving reasoning capabilities <Paper corpusId=\"272770433\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. The Self-Taught Reasoner (STaR) technique bootstraps complex reasoning abilities by generating rationales, correcting errors, and fine-tuning on successful examples <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247762790\" paperTitle=\"(Zelikman et al., 2022)\" isShortName></Paper>.\n\n5. **Symbolic Chain-of-Thought Distillation (SCoTD)** enables smaller models (125M-1.3B parameters) to benefit from chain-of-thought prompting by training them on rationalizations from larger teacher models <Paper corpusId=\"265456227\" paperTitle=\"(Jain et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259251773\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper>.\n\nAn emerging technique called \"bootstrapping\" involves an iterative process where a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This process raises important questions about optimal budget allocation across iterations.\n\nThe ability to verify the correctness of generated content provides a significant advantage in certain domains. In code generation, for instance, execution-based verification allows for the creation of large-scale, correct-by-construction synthetic datasets that have helped open-source code models approach the competency of proprietary alternatives <Paper corpusId=\"277104955\" paperTitle=\"(Nadas et al., 2025)\" isShortName></Paper>.\n\nWhile significant progress has been made in text and code domains, similar advances have been slower in other areas such as audio <Paper corpusId=\"273185896\" paperTitle=\"(Raghavan et al., 2024)\" isShortName></Paper>. However, the success of text-to-image generation models suggests promising directions for multimodal synthetic data generation <Paper corpusId=\"273185896\" paperTitle=\"(Raghavan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"232035663\" paperTitle=\"(Ramesh et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Josifoski et al., 2023)", "paper": {"corpus_id": 257378179, "title": "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Martin Josifoski", "authorId": "65826567"}, {"name": "Marija Sakota", "authorId": "2122910580"}, {"name": "Maxime Peyrard", "authorId": "35512303"}, {"name": "Robert West", "authorId": "145387102"}], "n_citations": 85}, "snippets": ["Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date."], "score": 0.92431640625}, {"id": "(Jain et al., 2023)", "paper": {"corpus_id": 265456227, "title": "LLM-Assisted Code Cleaning For Training Accurate Code Generators", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Naman Jain", "authorId": "1646458461"}, {"name": "Tianjun Zhang", "authorId": "1993655237"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "Koushik Sen", "authorId": "2268398121"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 32}, "snippets": ["Synthetic data for LLMS. Recent works have explored using synthetic datasets for generalpurpose or task-specific finetuning of LLMS. These approaches work by generating synthetic datasets from a strong LLM (like GPT-3.5-TURBO or GPT-4) using a set of existing tasks (Taori et al., 2023;Chiang et al., 2023) or generating new tasks using self-instruct (Wang et al., 2022) or evol-instruct (Xu et al., 2023) approaches. This has been also applied for task-specific finetuningin common-sense reasoning (West et al., 2021)), text-summarization (Sclar et al., 2022), mathematical reasoning (Luo et al., 2023a;Yue et al., 2023), tool use (Patil et al., 2023), coding (Luo et al., 2023b), and general-purpose reasoning (Li et al., 2023); (Zelikman et al., 2022). \n\nMore specifically, Yue et al. (2023) curates diverse corpus of mathematics problems with chain-ofthought or program-of-thought (Chen et al., 2022b) annotations for mathematical reasoning analogous to our plans. Gunasekar et al. (2023) proposed pre-training models on programming \"textbooks\" generated synthetically from GPT-3.5-TURBO. (Haluptzok et al., 2022) similarly generates programming puzzles and corresponding solutions from language models."], "score": 0.95654296875}, {"id": "(West et al., 2021)", "paper": {"corpus_id": 238857304, "title": "Symbolic Knowledge Distillation: from General Language Models to Commonsense Models", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Peter West", "authorId": "119659229"}, {"name": "Chandrasekhar Bhagavatula", "authorId": "51177106"}, {"name": "Jack Hessel", "authorId": "2689239"}, {"name": "Jena D. Hwang", "authorId": "2012510"}, {"name": "Liwei Jiang", "authorId": "2112504145"}, {"name": "Ronan Le Bras", "authorId": "39227408"}, {"name": "Ximing Lu", "authorId": "50085131"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 333}, "snippets": ["The common practice for training commonsense models has gone from\u2013human\u2013to\u2013corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models."], "score": 0.0}, {"id": "(Haluptzok et al., 2022)", "paper": {"corpus_id": 251197051, "title": "Language Models Can Teach Themselves to Program Better", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Patrick M. Haluptzok", "authorId": "2214247"}, {"name": "Matthew Bowers", "authorId": "2058735196"}, {"name": "A. Kalai", "authorId": "2186481"}], "n_citations": 82}, "snippets": ["Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model 'improves itself' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance."], "score": 0.0}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 272593422, "title": "Synthetic continued pretraining", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Zitong Yang", "authorId": "2283514777"}, {"name": "Neil Band", "authorId": "2294359410"}, {"name": "Shuangping Li", "authorId": "2320941550"}, {"name": "Emmanuel J. Candes", "authorId": "2283307289"}, {"name": "Tatsunori Hashimoto", "authorId": "2294362683"}], "n_citations": 16}, "snippets": ["Recent approaches synthesize pretraining data using hierarchical prompting methods to promote dataset diversity. Eldan & Li (2023) prompt API-based LLMs to generate children's stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text. Gunasekar et al. (2023) synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work (Li et al., 2023b;Abdin et al., 2023;2024). However, their datasets and prompts are not publicly available. (Maini et al., 2024) prompt an LM to rephrase documents for pretraining, improving training efficiency. Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus. Mecklenburg et al. (2024) consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA. We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task. Ovadia et al. (2024) continually pretrain Llama 2-based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements."], "score": 0.912109375}, {"id": "(Ajmal et al., 2024)", "paper": {"corpus_id": 269294098, "title": "Intellecta Cognitiva: A Comprehensive Dataset for Advancing Academic Knowledge and Machine Reasoning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "PS Ajmal", "authorId": "2297772548"}, {"name": "PS Ditto", "authorId": "2297772495"}, {"name": "VG Jithin", "authorId": "2297773046"}], "n_citations": 0}, "snippets": ["Integral to Intellecta is the incorporation of advanced synthetic generation techniques, which fabricate a dual-composed content: one that simulates complex thought processes and another that yields textbookstyle elucidations laden with core concepts and pragmatic examples.We took available open source instruction data as the seed data to generate the synthetic data.From the instructions, we first create a textbook style Gunasekar et al. [2023] text explaining the concept required for answering the instruction.The second step is to enrich the response from the seed data with thought process how the model arrive at that result.Combining these 2 will provide a textbook style concept explanation followed by exercises and the thought process to resolve the same.This correlate to the same a student learn a chapter starting with textbook followed by exercises and thought process behind it."], "score": 0.92626953125}, {"id": "(Akter et al., 2024)", "paper": {"corpus_id": 273403575, "title": "MIND: Math Informed syNthetic Dialogues for Pretraining LLMs", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Syeda Nahida Akter", "authorId": "1900302322"}, {"name": "Shrimai Prabhumoye", "authorId": "9358910"}, {"name": "John Kamalu", "authorId": "51028721"}, {"name": "S. Satheesh", "authorId": "145031342"}, {"name": "Eric Nyberg", "authorId": "2279547669"}, {"name": "M. Patwary", "authorId": "66870756"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2264406909"}], "n_citations": 2}, "snippets": ["To generate high-quality data at scale, current synthetic data generation approach explores rephrasing texts using LLMs in varied syntax while preserving the core content (Maini et al., 2024). However, their proposed approach limits up-sampling high-quality data in a way that does not go beyond grammatical styles or surface form transformations-leading little to no improvement when it comes to performance across complex and logical reasoning tasks. We hypothesize that simple rephrasing does not leverage the full potential of the synthetic data to improve the mathematical and complex multi-hop reasoning ability of LLM. Therefore, we propose, MIND, a conversational synthetic data generation approach that adds semantic variations and structured complexity to the raw text which is required to improve complex reasoning ability of the LLMs. In addition, multi-turn conversations can break down the original context step-by-step while each step addresses a sub-context at a time by often injecting complimentary reasoning or explanations. This resonates with how human solves a complex problem using consecutive chain-of-thought reasoning."], "score": 0.9326171875}, {"id": "(Maini et al., 2024)", "paper": {"corpus_id": 267312030, "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pratyush Maini", "authorId": "153742303"}, {"name": "Skyler Seto", "authorId": "31855650"}, {"name": "Richard He Bai", "authorId": "37374479"}, {"name": "David Grangier", "authorId": "2529182"}, {"name": "Yizhe Zhang", "authorId": "2254045488"}, {"name": "N. Jaitly", "authorId": "3111912"}], "n_citations": 66}, "snippets": ["Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as\"like Wikipedia\"or in\"question-answer format\"to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 272770433, "title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingjie Liu", "authorId": "2264029479"}, {"name": "Yun-Da Tsai", "authorId": "3328096"}, {"name": "Wenfei Zhou", "authorId": "2322144595"}, {"name": "Haoxing Ren", "authorId": "2268825069"}], "n_citations": 17}, "snippets": ["Synthetic Data Generation for Model Fine-tuning. The performance of large language models (LLMs) hinge on the quality and diversity of their training data. To address the limitations of manual datasets, synthetic data generation methods (Wang et al., 2022;Xu et al., 2023) have been developed to automatically create instruction-following examples from LLMs, reducing reliance on human annotations. Various techniques enhance data quality: Wang et al. (2022) generates multiple reasoning traces and selects the most frequent output to improve robustness, while other approaches (Lightman et al., 2023;Zhang et al., 2024b) assess response quality based on these traces. Self-training methods utilize synthetic data for iterative fine-tuning, boosting reasoning capabilities (Singh et al., 2023;Feng et al., 2023). These advancements show how synthetic data can effectively scale and optimize models through iterative feedback."], "score": 0.95361328125}, {"id": "(Zelikman et al., 2022)", "paper": {"corpus_id": 247762790, "title": "STaR: Bootstrapping Reasoning With Reasoning", "year": 2022, "venue": "", "authors": [{"name": "E. Zelikman", "authorId": "49456763"}, {"name": "Yuhuai Wu", "authorId": "3374063"}, {"name": "Noah D. Goodman", "authorId": "144002017"}], "n_citations": 510}, "snippets": ["Generating step-by-step\"chain-of-thought\"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the\"Self-Taught Reasoner\"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning."], "score": 0.0}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 259251773, "title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Liunian Harold Li", "authorId": "32562635"}, {"name": "Jack Hessel", "authorId": "2689239"}, {"name": "Youngjae Yu", "authorId": "2111510510"}, {"name": "Xiang Ren", "authorId": "2115257544"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 143}, "snippets": ["Chain-of-thought prompting (e.g., \"Let\u2019s think step-by-ste\") primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code."], "score": 0.0}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276079713, "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Pu Yang", "authorId": "2284644270"}, {"name": "Yunzhen Feng", "authorId": "2343647731"}, {"name": "Ziyuan Chen", "authorId": "2343512547"}, {"name": "Yuhang Wu", "authorId": "2343592601"}, {"name": "Zhuoyuan Li", "authorId": "2344832360"}], "n_citations": 0}, "snippets": ["Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?"], "score": 0.93017578125}, {"id": "(Nadas et al., 2025)", "paper": {"corpus_id": 277104955, "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mihai Nadas", "authorId": "2350754914"}, {"name": "Laura Dio\u015fan", "authorId": "2306585020"}, {"name": "Andreea Tomescu", "authorId": "2350756350"}], "n_citations": 3}, "snippets": ["In the text domain, LLM-generated data has proven especially useful for low-resource scenarios, delivering substantial improvements in tasks like classification and QA when human-labeled data is limited [33]. Techniques such as promptbased augmentation (zero-shot, few-shot, etc.), retrieval augmentation for grounding facts [3], and iterative refinement have pushed synthetic text data closer in effectiveness to real data.\n\nIn the code domain, LLMs have unlocked new possibilities by generating code snippets, programming instructions, and even whole problem solutions, facilitating better training of code models. The ability to verify code correctness via execution [37] provides a strong advantage, allowing the curation of large-scale, correct-by-construction synthetic code datasets (e.g., Code Alpaca, WizardCoder) that have propelled open-source code models to approach the competency of their proprietary counterparts."], "score": 0.97607421875}, {"id": "(Raghavan et al., 2024)", "paper": {"corpus_id": 273185896, "title": "Did You Hear That? Introducing AADG: A Framework for Generating Benchmark Data in Audio Anomaly Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ksheeraja Raghavan", "authorId": "2093263031"}, {"name": "Samiran Gode", "authorId": "2199184565"}, {"name": "Ankit Shah", "authorId": "2257395734"}, {"name": "Surabhi Raghavan", "authorId": "2324783136"}, {"name": "Wolfram Burgard", "authorId": "2322439428"}, {"name": "Bhiksha Raj", "authorId": "2288787089"}, {"name": "Rita Singh", "authorId": "2289375291"}], "n_citations": 0}, "snippets": ["Recent works have shown significant progress in synthetic data generation in the image space in (Liang et al., 2023) (Ramesh et al., 2021)) (Sun et al. 2024) (Bae et al., 2022) etc. However, similar progress has not been seen in the audio space. Zeroshot text-to-image generation approaches have expanded the scope of synthetic data applications by enabling the generation of novel image data from unseen textual prompts, highlighting the model's ability to generalize from limited examples (Ramesh et al., 2021). Digiface-1m (Bae et al., 2022)) dataset exemplifies the practical applications of these technologies, providing a robust framework for testing and improving face recognition algorithms through access to one million digital face images. (Ye et al. 2022) outlines a method to leverage LLMs to create synthetic datasets produced entirely using pre-trained language models (PLMs) without human interference while emphasizing the efficiency and flexibility of using synthetic datasets to train task-specific models. (Yu et al., 2023)) explores generation of training data that not only focuses on diversity, but also addresses inherent biases within the data generated by LLMs. It highlights the critical role of using diversely attributed prompts that enhance quality and utility of synthetic datasets improving model performance across NLP tasks. (Patel, Raffel, and Callison-Burch 2024) presents a tool designed to streamline synthetic data generation using LLMs providing a platform to generate, train, and share data sets and models."], "score": 0.931640625}, {"id": "(Ramesh et al., 2021)", "paper": {"corpus_id": 232035663, "title": "Zero-Shot Text-to-Image Generation", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Mikhail Pavlov", "authorId": "2068123790"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Chelsea Voss", "authorId": "153387869"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 5000}, "snippets": ["Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion."], "score": 0.0}], "table": null}, {"title": "Graph-based Approaches for Reasoning Tasks", "tldr": "Graph-based approaches leverage structured knowledge representations to generate high-quality synthetic data for complex reasoning tasks. These methods use graph structures to encode relationships between concepts, enabling scalable data generation with controlled reasoning paths and improved semantic coherence. (4 sources)", "text": "\nGraph structures provide an effective framework for generating synthetic reasoning data by explicitly representing relationships between concepts. This approach has gained traction for addressing challenges in commonsense reasoning, where manual annotation of datasets is expensive and not scalable. Maharana et al. introduced GraDA, a graph-generative data augmentation framework that synthesizes factual samples from knowledge graphs for commonsense reasoning datasets. Their system trains a graph-to-text model for generating questions from graph entities and relations, then employs a GAN-based approach to create plausible distractors for these synthetic questions <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\nRecent research has explored how graph-based synthetic data can enhance large language models' capabilities on complex reasoning tasks that involve long reasoning chains. Zhou et al. investigated this approach by designing a random walk sampling algorithm on graphs combined with a novel prompting strategy. Their method first extracts a reasoning chain from the graph structure and then derives the answer, providing a more controlled way to generate reasoning data <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al._1, 2024)\" isShortName></Paper>.\n\nBuilding on these ideas, Wang et al. proposed the Graph-based Synthetic Data Pipeline (GSDP), which addresses the dual challenges of quality and scalability. Their framework extracts knowledge points from seed data and constructs a knowledge point relationships graph to map interconnections between concepts. By exploring these implicit relationships, GSDP achieved remarkable data expansion (255 times the original dataset size) while maintaining high quality. Notably, their approach produced synthetic data comparable to that generated by GPT-4-0613 but at approximately 100 times lower cost <Paper corpusId=\"274656618\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe integration of neuro-symbolic (NeSy) architectures with graph-based approaches represents another promising direction. Savazzi et al. explored using scene graphs\u2014structured representations where nodes represent objects and edges represent relations\u2014as a symbolic framework for conditioning image generation. Their work suggests that scene graphs encode valuable background knowledge that can guide the synthetic data generation process, ensuring adherence to structural and semantic constraints particularly for tasks like Scene Graph Generation from images <Paper corpusId=\"277244110\" paperTitle=\"(Savazzi et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Maharana et al., 2022)", "paper": {"corpus_id": 250390686, "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Adyasha Maharana", "authorId": "8785371"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 96}, "snippets": ["Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."], "score": 0.91064453125}, {"id": "(Zhou et al._1, 2024)", "paper": {"corpus_id": 272753147, "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiaming Zhou", "authorId": "2297828274"}, {"name": "Abbas Ghaddar", "authorId": "2321867953"}, {"name": "Ge Zhang", "authorId": "2321875142"}, {"name": "Liheng Ma", "authorId": "1892081076"}, {"name": "Yaochen Hu", "authorId": "2288403553"}, {"name": "Soumyasundar Pal", "authorId": "38939190"}, {"name": "Mark Coates", "authorId": "2287938359"}, {"name": "Bin Wang", "authorId": "2321911323"}, {"name": "Yingxue Zhang", "authorId": "2275529643"}, {"name": "Jianye Hao", "authorId": "2307072418"}], "n_citations": 4}, "snippets": ["Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nRecently, several works (Xu et al., 2024;Abdin et al., 2024;Anil et al., 2023) have demonstrated the efficacy of boosting the LLMs' reasoning capacity via fine-tuning on synthetic data generated by stronger LLMs. However, how to make such synthetic data generation effective and controllable for specific applications remains an open question.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer."], "score": 0.951171875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 274656618, "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning Instructions", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiankang Wang", "authorId": "2335083125"}, {"name": "Jianjun Xu", "authorId": "2187441024"}, {"name": "Xiaorui Wang", "authorId": "2334884780"}, {"name": "Yuxin Wang", "authorId": "2143529316"}, {"name": "Mengting Xing", "authorId": "2067080682"}, {"name": "Shancheng Fang", "authorId": "13808397"}, {"name": "Zhineng Chen", "authorId": "2334916976"}, {"name": "Hongtao Xie", "authorId": "2323899630"}, {"name": "Yongdong Zhang", "authorId": "2290965957"}], "n_citations": 1}, "snippets": ["Synthesizing high-quality reasoning data for continual training has been proven to be effective in enhancing the performance of Large Language Models (LLMs). However, previous synthetic approaches struggle to easily scale up data and incur high costs in the pursuit of high quality. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality reasoning data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge point relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs."], "score": 0.95361328125}, {"id": "(Savazzi et al., 2025)", "paper": {"corpus_id": 277244110, "title": "Neuro-Symbolic Scene Graph Conditioning for Synthetic Image Dataset Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Giacomo Savazzi", "authorId": "2351599231"}, {"name": "Eugenio Lomurno", "authorId": "2221012084"}, {"name": "Cristian Sbrolli", "authorId": "2172115265"}, {"name": "Agnese Chiatti", "authorId": "2607532"}, {"name": "Matteo Matteucci", "authorId": "2304551810"}], "n_citations": 0}, "snippets": ["While NeSy architectures have demonstrated enhanced generalization in reasoning tasks, their potential for improving synthetic data generation remains largely unexplored. Scene graphs-structured representations where nodes represent objects and edges represent relations-offer a promising symbolic framework for conditioning image generation. This work explores the integration of such NeSy approaches into dataset generation to improve performance on complex tasks, particularly Scene Graph Generation (SGG) from images. The hypothesis posits that scene graphs encode useful background knowledge that can guide the generation process, ensuring synthetic data adhere to structural and semantic constraints."], "score": 0.984375}], "table": null}, {"title": "Mathematical Reasoning Data Synthesis", "tldr": "Mathematical reasoning data synthesis leverages techniques like key-point extraction, concept graphs, and instruction evolution to create high-quality synthetic datasets that enhance LLMs' problem-solving capabilities. Recent advances focus on scalability, diversity, and verification methods to generate millions of math problems across difficulty levels, resulting in significant performance improvements for fine-tuned models. (12 sources)", "text": "\nMathematical reasoning has emerged as a particularly fruitful domain for synthetic data generation due to its inherent structure and verifiability. Researchers have developed specialized approaches to address the scarcity of high-quality, reasoning-focused mathematical training data. A notable advancement in this area is the Key-Point-Driven Data Synthesis (KPDDS) framework, which leverages key points and exemplar practices from authentic data sources to synthesize novel question-answer pairs. This approach has led to the creation of KPMath, an extensive dataset comprising over 800,000 mathematical reasoning problems <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\nBuilding on similar principles, the MathScale method creates high-quality mathematical reasoning data by first extracting topics and knowledge points from seed math questions and then constructing a concept graph, which is subsequently used to generate new math problems. This approach has demonstrated impressive scalability, resulting in MathScaleQA, a dataset containing two million math question-answer pairs <Paper corpusId=\"268247902\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is ScaleQuest, which introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO). This method enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models, producing one million problem-solution pairs without relying on powerful proprietary models or seed data <Paper corpusId=\"273549775\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>.\n\nData augmentation based on existing annotated training sets has been widely explored, with methods ranging from self-evolving instructions <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> and question paraphrasing <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper> to solution augmentation. However, these approaches are often limited by the available training data, constraining synthesis diversity <Paper corpusId=\"278171321\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nMathematical reasoning synthetic data has been particularly effective for fine-tuning smaller language models. The WizardMath approach, which applies Reinforcement Learning from Evol-Instruct Feedback (RLEIF) to the domain of mathematics, has demonstrated remarkable capabilities. The resulting WizardMath-Mistral 7B model surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency <Paper corpusId=\"261030818\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Similarly, MetaMath, which bootstraps mathematical questions by rewriting them from multiple perspectives, has achieved significant improvements on benchmarks like GSM8K and MATH <Paper corpusId=\"262084051\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\nRecent approaches have also explored creating targeted, instance-specific data rather than broad-coverage datasets. This direction enables data generation agents to produce specific data based on a particular model's weaknesses <Paper corpusId=\"273228651\" paperTitle=\"(Khan et al., 2024)\" isShortName></Paper>. Unlike methods that modify problems in surface form, these approaches first infer a latent structure and then create problems by sampling from this structure <Paper corpusId=\"277780529\" paperTitle=\"(Khan et al., 2025)\" isShortName></Paper>.\n\nIn the realm of advanced mathematical reasoning, AlphaGeometry represents a milestone in theorem proving for Euclidean plane geometry. This neuro-symbolic system uses a neural language model trained from scratch on large-scale synthetic data to guide a symbolic deduction engine through challenging problems. On a test set of 30 olympiad-level problems, AlphaGeometry solved 25, approaching the performance of an average International Mathematical Olympiad gold medalist <Paper corpusId=\"267032902\" paperTitle=\"(Trinh et al., 2024)\" isShortName></Paper>.\n\nAnother noteworthy approach is Additional Logic Training (ALT), which enhances LLMs' reasoning capabilities through program-generated logical reasoning samples. Based on principles from symbolic logic theory, researchers constructed a synthetic corpus called Formal Logic Deduction Diverse (FLD\u00d72), comprising numerous samples of multi-step deduction with diverse reasoning rules. This training has shown substantial improvements in reasoning capabilities for models like LLaMA-3.1-70B, with gains of up to 30 points on logical reasoning benchmarks and up to 10 points on math benchmarks <Paper corpusId=\"274141027\" paperTitle=\"(Morishita et al., 2024)\" isShortName></Paper>.\n\nThe field has also benefited from approaches like EntailmentBank, which generates explanations in the form of entailment trees\u2014showing a line of reasoning from known facts, through intermediate conclusions, to the answer. This structured approach to synthetic data generation has shown promising results, particularly when relevant sentences are included in the input <Paper corpusId=\"233297051\" paperTitle=\"(Dalvi et al., 2021)\" isShortName></Paper>.\n\nThese diverse approaches to mathematical reasoning data synthesis highlight the field's rapid evolution, with increasingly sophisticated methods enabling the creation of high-quality, scalable datasets that significantly improve model performance on complex reasoning tasks.", "citations": [{"id": "(Huang et al., 2024)", "paper": {"corpus_id": 268247488, "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yiming Huang", "authorId": "2261394934"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yeyun Gong", "authorId": "2254121650"}, {"name": "Zhibin Gou", "authorId": "1797090"}, {"name": "Yelong Shen", "authorId": "2237948786"}, {"name": "Nan Duan", "authorId": "2269471632"}, {"name": "Weizhu Chen", "authorId": "2249538838"}], "n_citations": 43}, "snippets": ["Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs."], "score": 0.90380859375}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 268247902, "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Zhengyang Tang", "authorId": "2284834012"}, {"name": "Xingxing Zhang", "authorId": "2284863493"}, {"name": "Benyou Wang", "authorId": "2284827140"}, {"name": "Furu Wei", "authorId": "2290016262"}], "n_citations": 82}, "snippets": ["Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively."], "score": 0.0}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 273549775, "title": "Unleashing LLM Reasoning Capability via Scalable Question Synthesis from Scratch", "year": 2024, "venue": "", "authors": [{"name": "Yuyang Ding", "authorId": "2243263723"}, {"name": "Xinyu Shi", "authorId": "2327658780"}, {"name": "Xiaobo Liang", "authorId": "48083523"}, {"name": "Juntao Li", "authorId": "2257093356"}, {"name": "Zhaopeng Tu", "authorId": "2363558591"}, {"name": "Qiaoming Zhu", "authorId": "2243490219"}, {"name": "Min Zhang", "authorId": "2258690233"}], "n_citations": 8}, "snippets": ["In this paper, we propose ScaleQuest, a novel, scalable, and cost-effective data synthesis method that enables the generation of large-scale mathematical reasoning datasets using lightweight 7B-scale models. ScaleQuest introduces a two-stage question-tuning process comprising Question Fine-Tuning (QFT) and Question Preference Optimization (QPO) to unlock the question generation capabilities of problem-solving models. By generating diverse questions from scratch -- without relying on powerful proprietary models or seed data -- we produce a dataset of 1 million problem-solution pairs."], "score": 0.966796875}, {"id": "(Zeng et al., 2024)", "paper": {"corpus_id": 270213007, "title": "Automatic Instruction Evolving for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Weihao Zeng", "authorId": "2069640466"}, {"name": "Can Xu", "authorId": "2304522809"}, {"name": "Yingxiu Zhao", "authorId": "2290476555"}, {"name": "Jian-Guang Lou", "authorId": "2304469600"}, {"name": "Weizhu Chen", "authorId": "2264439430"}], "n_citations": 10}, "snippets": ["Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 262084051, "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "L. Yu", "authorId": "2112584251"}, {"name": "Weisen Jiang", "authorId": "2152123946"}, {"name": "Han Shi", "authorId": "152751416"}, {"name": "Jincheng Yu", "authorId": "2193887687"}, {"name": "Zhengying Liu", "authorId": "2239065052"}, {"name": "Yu Zhang", "authorId": "2153638098"}, {"name": "James T. Kwok", "authorId": "2243335442"}, {"name": "Zheng Li", "authorId": "121544682"}, {"name": "Adrian Weller", "authorId": "145689461"}, {"name": "Weiyang Liu", "authorId": "2243412679"}], "n_citations": 395}, "snippets": ["Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278171321, "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiapeng Wang", "authorId": "2302813110"}, {"name": "Jinhao Jiang", "authorId": "2118240359"}, {"name": "Zhiqiang Zhang", "authorId": "2358106383"}, {"name": "Jun Zhou", "authorId": "2279870653"}, {"name": "Wayne Xin Zhao", "authorId": "2294811281"}], "n_citations": 0}, "snippets": ["To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."], "score": 0.93798828125}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 261030818, "title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Haipeng Luo", "authorId": "2131127"}, {"name": "Qingfeng Sun", "authorId": "2112549330"}, {"name": "Can Xu", "authorId": "46747953"}, {"name": "Pu Zhao", "authorId": "2007757792"}, {"name": "Jian-Guang Lou", "authorId": "4648762"}, {"name": "Chongyang Tao", "authorId": "8801869"}, {"name": "Xiubo Geng", "authorId": "2442662"}, {"name": "Qingwei Lin", "authorId": "2793487"}, {"name": "Shifeng Chen", "authorId": "2232739272"}, {"name": "Dongmei Zhang", "authorId": "2109581369"}], "n_citations": 467}, "snippets": ["Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM"], "score": 0.0}, {"id": "(Khan et al., 2024)", "paper": {"corpus_id": 273228651, "title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Zaid Khan", "authorId": "2324992429"}, {"name": "Elias Stengel-Eskin", "authorId": "2281825070"}, {"name": "Jaemin Cho", "authorId": "2706729"}, {"name": "Mohit Bansal", "authorId": "2281826842"}], "n_citations": 3}, "snippets": ["The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Approaches using LLMs as annotators reduce human effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents - or teachers - is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid, scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides student feedback. The agent's goal is to improve student performance. Students are iteratively trained and evaluated on generated data, and their feedback (in the form of errors or weak skills) is reported to the agent after each iteration. DataEnvGym includes multiple teacher environment instantiations across 3 levels of structure in the state representation and action space. More structured environments are based on inferred skills and offer more interpretability and curriculum control. We support 4 domains (math, code, VQA, and tool-use) and test multiple students and teachers. Example agents in our teaching environments can iteratively improve students across tasks and settings. Moreover, we show that environments teach different skill levels and test variants of key modules, pointing to future work in improving data generation agents, engines, and feedback mechanisms."], "score": 0.0}, {"id": "(Khan et al., 2025)", "paper": {"corpus_id": 277780529, "title": "Executable Functional Abstractions: Inferring Generative Programs for Advanced Math Problems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zaid Khan", "authorId": "2324992429"}, {"name": "Elias Stengel-Eskin", "authorId": "2281825070"}, {"name": "Archiki Prasad", "authorId": "1677896557"}, {"name": "Jaemin Cho", "authorId": "2706729"}, {"name": "Mohit Bansal", "authorId": "2281826842"}], "n_citations": 0}, "snippets": ["Past work has generally approached improving models on reasoning tasks like math by generating large amounts of broad-coverage training data. This trend builds on work in generating instruction-tuning data (Wang et al., 2022), where model-generated instructions have been used to teach models to follow prompts. Luo et al. (2023) introduced generation method based on Evol-Instruct (Xu et al., 2023), which augmented a seed dataset of math problems by generating easier and harder problems. Related lines of work have sought to expand datasets by augmenting existing math datasets (Yu et al., 2023), adding multiple reasoning strategies (Yang et al., 2023), covering challenging competition problems (Li et al., 2024), or curating responses (Liu et al., 2024). The data generated in these settings differs from our data in a number of respects: first, it is generally broad-coverage, focusing on large-scale diverse data, as opposed to targeted, instancespecific data. This direction was also explored by (Khan et al., 2024), who define data generation agents that can generate specific data based on a particular model's weaknesses, covering math and several other domains. Finally, past work that has augmented a seed dataset (e.g., (Yu et al., 2023); (Yang et al., 2023)) has done so by modifying problems in the surface form, whereas our method first infers a latent structure and then creates problems by sampling from the structure."], "score": 0.9248046875}, {"id": "(Trinh et al., 2024)", "paper": {"corpus_id": 267032902, "title": "Solving olympiad geometry without human demonstrations", "year": 2024, "venue": "The Naturalist", "authors": [{"name": "Trieu H. Trinh", "authorId": "40895509"}, {"name": "Yuhuai Wu", "authorId": "2279769334"}, {"name": "Quoc V. Le", "authorId": "2279797756"}, {"name": "He He", "authorId": "2279870483"}, {"name": "Thang Luong", "authorId": "2279807154"}], "n_citations": 390}, "snippets": ["Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning1\u20134, owing to their reputed difficulty among the world\u2019s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges1,5, resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004."], "score": 0.0}, {"id": "(Morishita et al., 2024)", "paper": {"corpus_id": 274141027, "title": "Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Terufumi Morishita", "authorId": "1379579811"}, {"name": "Gaku Morio", "authorId": "29347584"}, {"name": "Atsuki Yamaguchi", "authorId": "145412147"}, {"name": "Yasuhiro Sogawa", "authorId": "2106369"}], "n_citations": 15}, "snippets": ["Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning. To address this, we propose $\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples. We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights. Then, based on these principles, we construct a synthetic corpus named $\\textbf{Formal Logic Deduction Diverse}$ ($\\textbf{FLD}$$_{\\times 2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors. Finally, we empirically show that ALT on FLD$_{\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B. Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH."], "score": 0.0}, {"id": "(Dalvi et al., 2021)", "paper": {"corpus_id": 233297051, "title": "Explaining Answers with Entailment Trees", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Bhavana Dalvi", "authorId": "40135250"}, {"name": "Peter Alexander Jansen", "authorId": "144949918"}, {"name": "Oyvind Tafjord", "authorId": "3385516"}, {"name": "Zhengnan Xie", "authorId": "9746804"}, {"name": "Hannah Smith", "authorId": "2110645880"}, {"name": "Leighanna Pipatanangkura", "authorId": "2078502192"}, {"name": "Peter Clark", "authorId": "48323507"}], "n_citations": 185}, "snippets": ["Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \"rationale\"). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."], "score": 0.0}], "table": null}, {"title": "Code Reasoning Data Synthesis", "tldr": "Synthetic data generation for code reasoning has evolved from basic instruction-following examples to sophisticated approaches that leverage open-source code repositories and execution-based verification. These techniques have significantly improved smaller language models' code generation capabilities, in some cases enabling them to match or exceed proprietary models on standard benchmarks. (7 sources)", "text": "\nCode reasoning represents another fertile domain for synthetic data generation, with approaches that leverage the unique advantage of code's executability for verification. Early efforts like Code Alpaca used self-instruct methodology to generate 20,000 instruction-tuning examples based on seed tasks <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. This approach has since evolved significantly with frameworks like WizardCoder, which adapted the Evol-Instruct method to the code domain, creating increasingly complex instruction-tuning data <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259164815\" paperTitle=\"(Luo et al._1, 2023)\" isShortName></Paper>.\n\nA particularly influential innovation in code data synthesis is OSS-Instruct, implemented in the Magicoder framework. This approach addresses the inherent bias in LLM-generated synthetic data by leveraging open-source code snippets as references. By generating 75,000 diverse instruction-solution pairs, Magicoder produced models that substantially outperformed state-of-the-art code models of similar or even larger sizes. Notably, the MagicoderS-CL-7B model, built on CodeLlama, achieved results surpassing ChatGPT on the HumanEval+ benchmark <Paper corpusId=\"272988100\" paperTitle=\"(Chan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270358041\" paperTitle=\"(Wei et al., 2023)\" isShortName></Paper>.\n\nThe ability to verify the correctness of generated code provides a crucial advantage for synthetic data generation in this domain. By running test cases or directly executing the code, researchers can filter out incorrect solutions and create high-quality datasets <Paper corpusId=\"272988100\" paperTitle=\"(Chan et al., 2024)\" isShortName></Paper>. This verification capability has helped narrow the performance gap between smaller open-source models and larger proprietary alternatives.\n\nRecent frameworks have expanded beyond traditional code generation to address multiple code-related tasks. WaveCoder, for example, focuses on generating diverse, high-quality instruction data for multi-task scenarios, resulting in the CodeSeaXDataset with nearly 20,000 instruction instances spanning four code-related tasks <Paper corpusId=\"277502040\" paperTitle=\"(Ahmad et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270258158\" paperTitle=\"(Yu et al._1, 2023)\" isShortName></Paper>.\n\nA novel approach called SoftSRV takes synthetic data generation for code reasoning a step further by using a data-driven loss minimization strategy. Instead of relying on human-engineered prompt templates, which can be labor-intensive and domain-specific, SoftSRV steers a frozen large language model to generate synthetic sequences similar to those from a target distribution. This provides a more systematic and generalizable framework for generating task-specific synthetic data <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nThe rapid advancement in code reasoning data synthesis techniques has led to impressive results, with recent works generating diverse instruction-solution pairs that, when used for fine-tuning base models, achieve top performance on standard benchmarks like HumanEval, MBPP, and BigCodeBench <Paper corpusId=\"277502040\" paperTitle=\"(Ahmad et al., 2025)\" isShortName></Paper>. These approaches have consistently demonstrated that synthetic data can effectively enhance the code reasoning capabilities of language models, enabling smaller, more accessible models to achieve performance comparable to much larger, proprietary alternatives.", "citations": [{"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 272753174, "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jin Jiang", "authorId": "2319385003"}, {"name": "Yuchen Yan", "authorId": "2284984220"}, {"name": "Yang Liu", "authorId": "2316670312"}, {"name": "Yonggang Jin", "authorId": "2321879179"}, {"name": "Shuai Peng", "authorId": "2072715089"}, {"name": "Mengdi Zhang", "authorId": "2284132141"}, {"name": "Xunliang Cai", "authorId": "2317059694"}, {"name": "Yixin Cao", "authorId": "2319459835"}, {"name": "Liangcai Gao", "authorId": "2165982472"}, {"name": "Zhi Tang", "authorId": "2268046262"}], "n_citations": 7}, "snippets": ["Synthetic data has played an important role in enhancing LLMs' reasoning capabilities (Dubey et al., 2024), especially in mathematics and coding domains. \n\nFor mathematical reasoning, synthetic data generation includes problem-driven methods such as evol-instruct (Luo et al., 2023a;Zeng et al., 2024), problem restatement (Yu et al., 2023), backtranslation (Lu et al., 2024), and few-shot examples (Dalvi et al., 2021), as well as knowledge-driven methods that rely on knowledge bases (Dalvi et al., 2021) or concept graphs (Tang et al., 2024) to generate new problems by sampling key reasoning points (Huang et al., 2024). \n\nIn terms of code reasoning, from Code Alpaca's (Wang et al., 2023) use of self-instruct to generate 20K instruction data based on seed tasks, to Wiz-ardCoder's (Luo et al., 2023b) use of Code evolinstruct to generate more complex tasks, to Magicoder's use of oss-instruct (Wei et al., 2024) to extract 75K instructions from open source code. Synthesizing data continuously improves the model's code reasoning capabilities."], "score": 0.93896484375}, {"id": "(Luo et al._1, 2023)", "paper": {"corpus_id": 259164815, "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ziyang Luo", "authorId": "23523733"}, {"name": "Can Xu", "authorId": "46747953"}, {"name": "Pu Zhao", "authorId": "2007757792"}, {"name": "Qingfeng Sun", "authorId": "2112549330"}, {"name": "Xiubo Geng", "authorId": "2442662"}, {"name": "Wenxiang Hu", "authorId": "50105419"}, {"name": "Chongyang Tao", "authorId": "8801869"}, {"name": "Jing Ma", "authorId": "2157405974"}, {"name": "Qingwei Lin", "authorId": "2793487"}, {"name": "Daxin Jiang", "authorId": "2086994543"}], "n_citations": 689}, "snippets": ["Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM"], "score": 0.0}, {"id": "(Chan et al., 2024)", "paper": {"corpus_id": 272988100, "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yung-Chieh Chan", "authorId": "2324074843"}, {"name": "George Pu", "authorId": "2323513733"}, {"name": "Apaar Shanker", "authorId": "1390689296"}, {"name": "Parth Suresh", "authorId": "2268781105"}, {"name": "Penn Jenks", "authorId": "2323512524"}, {"name": "John Heyer", "authorId": "2323512547"}, {"name": "Sam Denton", "authorId": "2323515242"}], "n_citations": 10}, "snippets": ["In the domain of mathematical reasoning, highquality instructions are scarce, so many works leverage LLM-generated synthetic data to significantly improved the math reasoning ability of small LLMs (Yu et al., 2023;(Qin et al., 2024)Setlur et al., 2024;(Luo et al., 2023). In code generation, synthetic data from LLMs can be further verified by running test cases or the code directly, which helps close the gap between closed-source LLMs and smaller LLMs (Wei et al., 2023)Yang et al., 2024)."], "score": 0.90283203125}, {"id": "(Wei et al., 2023)", "paper": {"corpus_id": 270358041, "title": "Magicoder: Empowering Code Generation with OSS-Instruct", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Yuxiang Wei", "authorId": "2237736409"}, {"name": "Zhe Wang", "authorId": "2269532027"}, {"name": "Jiawei Liu", "authorId": "2296736695"}, {"name": "Yifeng Ding", "authorId": "2111235780"}, {"name": "Lingming Zhang", "authorId": "2289125201"}], "n_citations": 118}, "snippets": ["We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references."], "score": 0.0}, {"id": "(Ahmad et al., 2025)", "paper": {"corpus_id": 277502040, "title": "OpenCodeReasoning: Advancing Data Distillation for Competitive Coding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wasi Uddin Ahmad", "authorId": "38123220"}, {"name": "Sean Narenthiran", "authorId": "2135333375"}, {"name": "Somshubra Majumdar", "authorId": "9099952"}, {"name": "Aleksander Ficek", "authorId": "2186740325"}, {"name": "Siddhartha Jain", "authorId": "2353320260"}, {"name": "Jocelyn Huang", "authorId": "1390669078"}, {"name": "V. Noroozi", "authorId": "2353998"}, {"name": "Boris Ginsburg", "authorId": "2353271013"}], "n_citations": 13}, "snippets": ["To overcome this bottleneck, many have successfully leveraged LLMs to generate high-quality synthetic code data (Luo et al., 2023)(Yu et al., 2023). Notably, works like Wei et al. (2024a) and Huang et al. (2025) have generated diverse instruction-solution pairs, subsequently fine-tuning base models to achieve top results in HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BigCodeBench (Zhuo et al., 2025) benchmarks."], "score": 0.900390625}, {"id": "(Yu et al._1, 2023)", "paper": {"corpus_id": 270258158, "title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhaojian Yu", "authorId": "2276315776"}, {"name": "Xin Zhang", "authorId": "2276235606"}, {"name": "Ning Shang", "authorId": "2276207911"}, {"name": "Yangyu Huang", "authorId": "2276731918"}, {"name": "Can Xu", "authorId": "2276263216"}, {"name": "Yishujie Zhao", "authorId": "2276509780"}, {"name": "Wenxiang Hu", "authorId": "2276366691"}, {"name": "Qiufeng Yin", "authorId": "2276205940"}], "n_citations": 28}, "snippets": ["Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeSeaXDataset, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks."], "score": 0.0}, {"id": "(DeSalvo et al., 2024)", "paper": {"corpus_id": 273507197, "title": "SoftSRV: Learn to Generate Targeted Synthetic Data", "year": 2024, "venue": "", "authors": [{"name": "Giulia DeSalvo", "authorId": "2280911353"}, {"name": "Jean-Fracois Kagy", "authorId": "2327046826"}, {"name": "Lazaros Karydas", "authorId": "2283008"}, {"name": "Afshin Rostamizadeh", "authorId": "2435268"}, {"name": "Sanjiv Kumar", "authorId": "2275226495"}], "n_citations": 0}, "snippets": ["We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning)."], "score": 0.94921875}], "table": null}, {"title": "Key Techniques and Frameworks", "tldr": "Synthetic data generation leverages diverse technical approaches including differential privacy mechanisms, generative models, and specialized frameworks for domain-specific tasks. These techniques range from privacy-preserving methods like DP-MERF to task-specific frameworks like CLIPPER for narrative verification and comprehensive systems like MagiCoder for code generation. (22 sources)", "text": "\n## Differential Privacy Approaches\n\nDifferential privacy (DP) has become fundamental in synthetic data generation, offering rigorous privacy guarantees while maintaining utility. Recent approaches fall into several categories:\n\n1. **Query-based methods** release differentially private answers to counting queries and use them as the basis for synthetic data generation. Notable examples include AIM <Paper corpusId=\"246430835\" paperTitle=\"(McKenna et al., 2022)\" isShortName></Paper>, Private-PGM <Paper corpusId=\"236976348\" paperTitle=\"(McKenna et al., 2021)\" isShortName></Paper>, and HDMM <Paper corpusId=\"51869463\" paperTitle=\"(McKenna et al., 2018)\" isShortName></Paper>.\n\n2. **Generative model approaches** train models like GANs on real datasets under differential privacy constraints. Examples include GS-WGAN <Paper corpusId=\"219687936\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper> and G-PATE <Paper corpusId=\"245634703\" paperTitle=\"(Long et al., 2019)\" isShortName></Paper>, which leverages private aggregation mechanisms to ensure strong privacy guarantees.\n\n3. **Hybrid frameworks** combine aspects of both approaches, as seen in DP-MERF <Paper corpusId=\"225077562\" paperTitle=\"(Harder et al., 2020)\" isShortName></Paper>, which uses random feature representations of kernel mean embeddings when comparing distributions, providing minimal privacy cost for training deep generative models.\n\n## Domain-Specific Frameworks\n\nSeveral frameworks have been developed to address specific domains and reasoning tasks:\n\n1. **Code Generation Frameworks**\n - **MagiCoder** implements OSS-Instruct to generate diverse instruction-solution pairs for code tasks by leveraging open-source code snippets as references <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>.\n - **WaveCoder** focuses on multi-task scenarios, generating diverse instruction data across multiple code-related tasks <Paper corpusId=\"268230871\" paperTitle=\"(Sudalairaj et al., 2024)\" isShortName></Paper>.\n\n2. **Mathematical Reasoning Frameworks**\n - **Evol-Instruct** synthesizes iteratively more complex instructions to overcome limitations of previous methods <Paper corpusId=\"268230871\" paperTitle=\"(Sudalairaj et al., 2024)\" isShortName></Paper>.\n - **UCTR-ST** addresses complex tabular reasoning by incorporating program management, transformation, and table-text manipulation components <Paper corpusId=\"254877131\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n3. **Multimodal Frameworks**\n - **Visual Chain-of-Thought (VCOT)** combines the efficiency of chain-of-thought reasoning with multimodal capabilities, generating synthetic data by recursively creating multimodal infillings <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>.\n - **Hybrid-Synth** introduces background complexity to synthetic images along with programmatic variation of rotation, lighting, and scale <Paper corpusId=\"257607791\" paperTitle=\"(Natarajan et al., 2023)\" isShortName></Paper>.\n\n## Generation Techniques\n\n1. **GAN-based techniques** remain popular for tabular data generation, with implementations like TableGAN <Paper corpusId=\"47017667\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper> and medBGAN <Paper corpusId=\"54479855\" paperTitle=\"(Baowaly et al., 2018)\" isShortName></Paper> demonstrating effectiveness in generating realistic synthetic records while preserving privacy.\n\n2. **Diffusion models** are increasingly applied to tabular data generation, showing superior performance over GANs and VAEs in certain contexts <Paper corpusId=\"249847841\" paperTitle=\"(Kim et al., 2022)\" isShortName></Paper> <Paper corpusId=\"264439324\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\n3. **Instruction backtranslation** automatically labels human-written text with corresponding instructions, enabling construction of high-quality instruction-following models <Paper corpusId=\"260866107\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n4. **Bootstrapping techniques** involve iterative processes where models generate synthetic data, which is filtered and used for further fine-tuning <Paper corpusId=\"268553593\" paperTitle=\"(Peyrard et al., 2024)\" isShortName></Paper>.\n\n5. **Graph-based generation** techniques like GraDA synthesize factual samples from knowledge graphs for commonsense reasoning datasets <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\n6. **Compression-based approaches** like CLIPPER generate synthetic data for narrative claim verification by first compressing books into outlines and summaries before generating claims <Paper corpusId=\"276482747\" paperTitle=\"(Pham et al., 2025)\" isShortName></Paper>.\n\n7. **Repeated sampling strategies** enable smaller language models to achieve comparable performance to larger models for certain tasks when operating within fixed computational budgets <Paper corpusId=\"272146630\" paperTitle=\"(Bansal et al., 2024)\" isShortName></Paper>.\n\n8. **Pre-training data utilization** approaches like SYNTHLLM leverage existing web documents as an underutilized resource for scalable post-training data generation <Paper corpusId=\"277313659\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605607\" paperTitle=\"(Yue et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269981934\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThese diverse frameworks and techniques illustrate the rapid evolution of synthetic data generation approaches across different domains, with increasing focus on task-specific optimizations, privacy preservation, and computational efficiency.", "citations": [{"id": "(McKenna et al., 2022)", "paper": {"corpus_id": 246430835, "title": "AIM: An Adaptive and Iterative Mechanism for Differentially Private Synthetic Data", "year": 2022, "venue": "Proceedings of the VLDB Endowment", "authors": [{"name": "Ryan McKenna", "authorId": "35836504"}, {"name": "Brett Mullins", "authorId": "122380107"}, {"name": "D. Sheldon", "authorId": "144799908"}, {"name": "G. Miklau", "authorId": "1729605"}], "n_citations": 86}, "snippets": ["We propose AIM, a new algorithm for differentially private synthetic data generation. AIM is a workload-adaptive algorithm within the paradigm of algorithms that first selects a set of queries, then privately measures those queries, and finally generates synthetic data from the noisy measurements. It uses a set of innovative features to iteratively select the most useful measurements, reflecting both their relevance to the workload and their value in approximating the input data. We also provide analytic expressions to bound per-query error with high probability which can be used to construct confidence intervals and inform users about the accuracy of generated data. We show empirically that AIM consistently outperforms a wide variety of existing mechanisms across a variety of experimental settings."], "score": 0.0}, {"id": "(McKenna et al., 2021)", "paper": {"corpus_id": 236976348, "title": "Winning the NIST Contest: A scalable and general approach to differentially private synthetic data", "year": 2021, "venue": "Journal of Privacy and Confidentiality", "authors": [{"name": "Ryan McKenna", "authorId": "35836504"}, {"name": "G. Miklau", "authorId": "1729605"}, {"name": "D. Sheldon", "authorId": "144799908"}], "n_citations": 126}, "snippets": ["We propose a general approach for differentially private synthetic data generation, that consists of three steps: (1) select a collection of low-dimensional marginals, (2) measure those marginals with a noise addition mechanism, and (3)\u00a0generate synthetic data that preserves the measured marginals well. Central to this approach is Private-PGM, a post-processing method that is used to estimate a high-dimensional data distribution from noisy measurements of its marginals. We present two mechanisms, NIST-MST and MST, that are instances of this general approach. NIST-MST was the winning mechanism in the 2018 NIST differential privacy synthetic data competition, and MST is a new mechanism that can work in more general settings, while still performing comparably to NIST-MST. We believe our general approach should be of broad interest, and can be adopted in future mechanisms for synthetic data generation."], "score": 0.0}, {"id": "(McKenna et al., 2018)", "paper": {"corpus_id": 51869463, "title": "Optimizing error of high-dimensional statistical queries under differential privacy", "year": 2018, "venue": "Proceedings of the VLDB Endowment", "authors": [{"name": "Ryan McKenna", "authorId": "35836504"}, {"name": "G. Miklau", "authorId": "1729605"}, {"name": "Michael Hay", "authorId": "145256244"}, {"name": "Ashwin Machanavajjhala", "authorId": "2357165"}], "n_citations": 114}, "snippets": ["Differentially private algorithms for answering sets of predicate counting queries on a sensitive database have many applications. Organizations that collect individual-level data, such as statistical agencies and medical institutions, use them to safely release summary tabulations. However, existing techniques are accurate only on a narrow class of query workloads, or are extremely slow, especially when analyzing more than one or two dimensions of the data.\n In this work we propose HDMM, a new differentially private algorithm for answering a workload of predicate counting queries, that is especially effective for higher-dimensional datasets. HDMM represents query workloads using an implicit matrix representation and exploits this compact representation to efficiently search (a subset of) the space of differentially private algorithms for one that answers the input query workload with high accuracy. We empirically show that HDMM can efficiently answer queries with lower error than state-of-the-art techniques on a variety of low and high dimensional datasets."], "score": 0.0}, {"id": "(Chen et al., 2020)", "paper": {"corpus_id": 219687936, "title": "GS-WGAN: A Gradient-Sanitized Approach for Learning Differentially Private Generators", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Dingfan Chen", "authorId": "153642281"}, {"name": "Tribhuvanesh Orekondy", "authorId": "9517443"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 185}, "snippets": ["The wide-spread availability of rich data has fueled the growth of machine learning applications in numerous domains. However, growth in domains with highly-sensitive data (e.g., medical) is largely hindered as the private nature of data prohibits it from being shared. To this end, we propose Gradient-sanitized Wasserstein Generative Adversarial Networks (GS-WGAN), which allows releasing a sanitized form of the sensitive data with rigorous privacy guarantees. In contrast to prior work, our approach is able to distort gradient information more precisely, and thereby enabling training deeper models which generate more informative samples. Moreover, our formulation naturally allows for training GANs in both centralized and federated (i.e., decentralized) data scenarios. Through extensive experiments, we find our approach consistently outperforms state-of-the-art approaches across multiple metrics (e.g., sample quality) and datasets."], "score": 0.0}, {"id": "(Long et al., 2019)", "paper": {"corpus_id": 245634703, "title": "G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yunhui Long", "authorId": "3147214"}, {"name": "Boxin Wang", "authorId": "2153207717"}, {"name": "Zhuolin Yang", "authorId": "2119399524"}, {"name": "B. Kailkhura", "authorId": "1749353"}, {"name": "Aston Zhang", "authorId": "2085709"}, {"name": "C.A. Gunter", "authorId": "119755266"}, {"name": "Bo Li", "authorId": "2155883218"}], "n_citations": 74}, "snippets": ["Recent advances in machine learning have largely benefited from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differentially private data generator that preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guarantees. Compared to existing approaches, G-PATE significantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that flows from teacher discriminators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the first work being able to generate high-dimensional image data with high data utility under limited privacy budgets ($\\epsilon \\le 1$). Our code is available at https://github.com/AI-secure/G-PATE."], "score": 0.0}, {"id": "(Harder et al., 2020)", "paper": {"corpus_id": 225077562, "title": "DP-MERF: Differentially Private Mean Embeddings with RandomFeatures for Practical Privacy-preserving Data Generation", "year": 2020, "venue": "International Conference on Artificial Intelligence and Statistics", "authors": [{"name": "Frederik Harder", "authorId": "153653986"}, {"name": "Kamil Adamczewski", "authorId": "2220045"}, {"name": "Mijung Park", "authorId": "50811522"}], "n_citations": 101}, "snippets": ["We propose a differentially private data generation paradigm using random feature representations of kernel mean embeddings when comparing the distribution of true data with that of synthetic data. We exploit the random feature representations for two important benefits. First, we require a minimal privacy cost for training deep generative models. This is because unlike kernel-based distance metrics that require computing the kernel matrix on all pairs of true and synthetic data points, we can detach the data-dependent term from the term solely dependent on synthetic data. Hence, we need to perturb the data-dependent term once and for all and then use it repeatedly during the generator training. Second, we can obtain an analytic sensitivity of the kernel mean embedding as the random features are norm bounded by construction. This removes the necessity of hyper-parameter search for a clipping norm to handle the unknown sensitivity of a generator network. We provide several variants of our algorithm, differentially private mean embeddings with random features (DP-MERF) to jointly generate labels and input features for datasets such as heterogeneous tabular data and image data. Our algorithm achieves drastically better privacy-utility trade-offs than existing methods when tested on several datasets."], "score": 0.0}, {"id": "(Rose et al., 2023)", "paper": {"corpus_id": 258461502, "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Daniel Philip Rose", "authorId": "2215927057"}, {"name": "Vaishnavi Himakunthala", "authorId": "2215926792"}, {"name": "Andy Ouyang", "authorId": "2215914015"}, {"name": "Ryan He", "authorId": "2215915076"}, {"name": "Alex Mei", "authorId": "2185480449"}, {"name": "Yujie Lu", "authorId": "47006228"}, {"name": "Michael Stephen Saxon", "authorId": "48227633"}, {"name": "Chinmay Sonar", "authorId": "26961225"}, {"name": "Diba Mirza", "authorId": "1705929"}, {"name": "William Yang Wang", "authorId": "1682479"}], "n_citations": 47}, "snippets": ["We propose Visual Chain-of-Thought (VCOT), which combines the efficiency, robustness, and multi-step reasoning of COT with the multimodal capabilities of vision-language models. VCOT synthetically augments sequential datasets and bridges logical gaps by recursively generating multimodal infillings and using the synthetic data to improve downstream task performance. These synthetic generations also serve as human-interpretable insights into AI systems' ability of multi-step reasoning. We demonstrate that VCOT creates consistent and novel synthetic data that enhances downstream performance on the VIST (Huang et al., 2016) and WIKIHOW (Koupaee & Wang, 2018) datasets."], "score": 0.9150390625}, {"id": "(Sudalairaj et al., 2024)", "paper": {"corpus_id": 268230871, "title": "LAB: Large-Scale Alignment for ChatBots", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shivchander Sudalairaj", "authorId": "2114819100"}, {"name": "Abhishek Bhandwaldar", "authorId": "150894502"}, {"name": "Aldo Pareja", "authorId": "2288530668"}, {"name": "Kai Xu", "authorId": "2261102120"}, {"name": "David D. Cox", "authorId": "2289845472"}, {"name": "Akash Srivastava", "authorId": "2243025154"}], "n_citations": 34}, "snippets": ["Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task diversity and scalability, alongside a progressive training framework aimed at improving the model's reasoning ability and response style to match teacher models. This is achieved by generating rich reasoning signals in the generated answer and progressively training on datasets of varying difficulty in incremental phases."], "score": 0.951171875}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 254877131, "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework", "year": 2022, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Zhenyu Li", "authorId": "2155354022"}, {"name": "Xiuxing Li", "authorId": "2116521868"}, {"name": "Sunqi Fan", "authorId": "2272781120"}, {"name": "Jianyong Wang", "authorId": "2115642141"}], "n_citations": 6}, "snippets": ["To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic. Furthermore, we optimize the procedure using \"Table-Text Manipulator\" to handle joint table-text reasoning scenarios."], "score": 0.919921875}, {"id": "(Natarajan et al., 2023)", "paper": {"corpus_id": 257607791, "title": "Hybrid synthetic data generation pipeline that outperforms real data", "year": 2023, "venue": "J. Electronic Imaging", "authors": [{"name": "S. Natarajan", "authorId": "143927102"}, {"name": "M. G. Madden", "authorId": "32418087"}], "n_citations": 2}, "snippets": ["We therefore exploit advances in open-source plugins and game engines, combined with unique techniques in synthetic data generation to perform state-of-the-art synthetic to real transfer learning, and make the following contributions. \n\n1. We present a synthetic data generation framework with an approach of introducing background complexity to synthetic images, in addition to the ability to programmatically vary rotation, lighting, backgrounds, and scale, making the resulting classifier very robust. We have made our framework publicly available (https://github.com/saiabinesh/hybrid-synth), which can be used to generate a dataset with any number of arbitrary classes."], "score": 0.90283203125}, {"id": "(Park et al., 2018)", "paper": {"corpus_id": 47017667, "title": "Data Synthesis based on Generative Adversarial Networks", "year": 2018, "venue": "Proceedings of the VLDB Endowment", "authors": [{"name": "Noseong Park", "authorId": "5166698"}, {"name": "Mahmoud Mohammadi", "authorId": "48794484"}, {"name": "Kshitij Gorde", "authorId": "50978924"}, {"name": "S. Jajodia", "authorId": "1699357"}, {"name": "Hongkyu Park", "authorId": "2110627459"}, {"name": "Youngmin Kim", "authorId": null}], "n_citations": 484}, "snippets": ["Privacy is an important concern for our society where sharing data with partners or releasing data to the public is a frequent occurrence. Some of the techniques that are being used to achieve privacy are to remove identifiers, alter quasi-identifiers, and perturb values. Unfortunately, these approaches suffer from two limitations. First, it has been shown that private information can still be leaked if attackers possess some background knowledge or other information sources. Second, they do not take into account the adverse impact these methods will have on the utility of the released data. In this paper, we propose a method that meets both requirements. Our method, called\n table-GAN\n , uses generative adversarial networks (GANs) to synthesize fake tables that are statistically similar to the original table yet do not incur information leakage. We show that the machine learning models trained using our synthetic tables exhibit performance that is similar to that of models trained using the original table for unknown testing cases. We call this property\n model compatibility\n . We believe that anonymization/perturbation/synthesis methods without model compatibility are of little value. We used four real-world datasets from four different domains for our experiments and conducted indepth comparisons with state-of-the-art anonymization, perturbation, and generation techniques. Throughout our experiments, only our method consistently shows balance between privacy level and model compatibility."], "score": 0.0}, {"id": "(Baowaly et al., 2018)", "paper": {"corpus_id": 54479855, "title": "Synthesizing electronic health records using improved generative adversarial networks", "year": 2018, "venue": "J. Am. Medical Informatics Assoc.", "authors": [{"name": "M. K. Baowaly", "authorId": "29805373"}, {"name": "Chia-Ching Lin", "authorId": "2116503720"}, {"name": "Chao-Lin Liu", "authorId": "39986827"}, {"name": "Kuan-Ta Chen", "authorId": "6270307"}], "n_citations": 197}, "snippets": ["Objective\nThe aim of this study was to generate synthetic electronic health records (EHRs). The generated EHR data will be more realistic than those generated using the existing medical Generative Adversarial Network (medGAN) method.\n\n\nMaterials and Methods\nWe modified medGAN to obtain two synthetic data generation models-designated as medical Wasserstein GAN with gradient penalty (medWGAN) and medical boundary-seeking GAN (medBGAN)-and compared the results obtained using the three models. We used 2 databases: MIMIC-III and National Health Insurance Research Database (NHIRD), Taiwan. First, we trained the models and generated synthetic EHRs by using these three 3 models. We then analyzed and compared the models' performance by using a few statistical methods (Kolmogorov-Smirnov test, dimension-wise probability for binary data, and dimension-wise average count for count data) and 2 machine learning tasks (association rule mining and prediction).\n\n\nResults\nWe conducted a comprehensive analysis and found our models were adequately efficient for generating synthetic EHR data. The proposed models outperformed medGAN in all cases, and among the 3 models, boundary-seeking GAN (medBGAN) performed the best.\n\n\nDiscussion\nTo generate realistic synthetic EHR data, the proposed models will be effective in the medical industry and related research from the viewpoint of providing better services. Moreover, they will eliminate barriers including limited access to EHR data and thus accelerate research on medical informatics.\n\n\nConclusion\nThe proposed models can adequately learn the data distribution of real EHRs and efficiently generate realistic synthetic EHRs. The results show the superiority of our models over the existing model."], "score": 0.0}, {"id": "(Kim et al., 2022)", "paper": {"corpus_id": 249847841, "title": "SOS: Score-based Oversampling for Tabular Data", "year": 2022, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Jayoung Kim", "authorId": "2109187265"}, {"name": "Chae-Eun Lee", "authorId": "2109513298"}, {"name": "Yehjin Shin", "authorId": "2171121134"}, {"name": "Sewon Park", "authorId": "2107883259"}, {"name": "Minjung Kim", "authorId": "2117955473"}, {"name": "Noseong Park", "authorId": "5166698"}, {"name": "Jihoon Cho", "authorId": "2115699011"}], "n_citations": 38}, "snippets": ["Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases."], "score": 0.0}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 264439324, "title": "DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Weijie Xu", "authorId": "2110546424"}, {"name": "Wenxiang Hu", "authorId": "2261645232"}, {"name": "Fanyou Wu", "authorId": "2261413304"}, {"name": "Srinivasan H. Sengamedu", "authorId": "1757518"}], "n_citations": 18}, "snippets": ["In the burgeoning field of natural language processing (NLP), Neural Topic Models (NTMs) , Large Language Models (LLMs) and Diffusion model have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic based text generation. NTMs have never been combined with diffusion model for text generation. Our study addresses these gaps by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages Encoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion model, our framework also provides the capability to do topic based text generation. This dual functionality allows users to efficiently produce highly clustered topics and topic based text generation simultaneously. DeTiME's potential extends to generating clustered embeddings as well. Notably, our proposed framework(both encoder-decoder based LLM and diffusion model) proves to be efficient to train and exhibits high adaptability to other LLMs and diffusion model, demonstrating its potential for a wide array of applications."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 260866107, "title": "Self-Alignment with Instruction Backtranslation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xian Li", "authorId": "2116235416"}, {"name": "Ping Yu", "authorId": "2114104308"}, {"name": "Chunting Zhou", "authorId": "2384711"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "J. Weston", "authorId": "145183709"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 134}, "snippets": ["We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then selecting high quality examples from among these candidates (self-curation). This data is then used to finetune a stronger model. Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment."], "score": 0.0}, {"id": "(Peyrard et al., 2024)", "paper": {"corpus_id": 268553593, "title": "Agentic AI: The Era of Semantic Decoding", "year": 2024, "venue": "", "authors": [{"name": "Maxime Peyrard", "authorId": "35512303"}, {"name": "Martin Josifoski", "authorId": "65826567"}, {"name": "Robert West", "authorId": "2269473532"}], "n_citations": 0}, "snippets": ["The synthetic data generation Flows can leverage domain knowledge (Tang et al., 2023), task properties (Lu et al., 2024;Veselovsky et al., 2023;(Josifoski et al., 2023), or collaboration (Abdullin et al., 2024), and synthesize data of notably higher quality than what a single model or simple heuristics can achieve. This sets the stage for effective self-improvement loops where a language model participates in a semantic decoding algorithm producing high-quality synthetic data. Then, the language model improves itself through fine-tuning, thereby improving the Flow's capacity to generate even better synthetic data in a virtuous cycle (Silver et al., 2017;Burns et al., 2023;Singh et al., 2023;Chen et al., 2024b). An example of such a Flow is MAGDi (Chen et al., 2024a), a framework designed to distill reasoning interactions among multiple LLMs into smaller ones. This approach surpasses single-teacher distillation (Li et al., 2023)(Magister et al., 2022) and finetuning based on reasoning trajectories sampled from GPT-4 (Chen et al., 2023a)."], "score": 0.95166015625}, {"id": "(Maharana et al., 2022)", "paper": {"corpus_id": 250390686, "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Adyasha Maharana", "authorId": "8785371"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 96}, "snippets": ["Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."], "score": 0.91064453125}, {"id": "(Pham et al., 2025)", "paper": {"corpus_id": 276482747, "title": "CLIPPER: Compression enables long-context synthetic data generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chau Minh Pham", "authorId": "2264486147"}, {"name": "Yapei Chang", "authorId": "144455052"}, {"name": "Mohit Iyyer", "authorId": "2136562"}], "n_citations": 1}, "snippets": ["We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex."], "score": 0.958984375}, {"id": "(Bansal et al., 2024)", "paper": {"corpus_id": 272146630, "title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hritik Bansal", "authorId": "2317010356"}, {"name": "Arian Hosseini", "authorId": "2090537547"}, {"name": "Rishabh Agarwal", "authorId": "2317013277"}, {"name": "Vinh Q. Tran", "authorId": "2317984275"}, {"name": "Mehran Kazemi", "authorId": "2317010095"}], "n_citations": 49}, "snippets": ["Several recent works aim to understand the utility of the weak but cheaper LMs in comparison to the strong but expensive LMs for reasoning. Specifically, Brown et al. (2024); Snell et al. (2024); Song et al. (2024) show that the solve rate of the small LMs can increase significantly with repeated sampling. In addition, Hassid et al. (2024) demonstrate that repeated generations from smaller LMs can outperform the data generated by larger LMs at a fixed sampling computational budget during inference for coding tasks."], "score": 0.90234375}, {"id": "(Qin et al., 2025)", "paper": {"corpus_id": 277313659, "title": "Scaling Laws of Synthetic Data for Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zeyu Qin", "authorId": "2118242824"}, {"name": "Qingxiu Dong", "authorId": "2287927238"}, {"name": "Xingxing Zhang", "authorId": "2284863493"}, {"name": "Li Dong", "authorId": "2294850817"}, {"name": "Xiaolong Huang", "authorId": "2116768132"}, {"name": "Ziyi Yang", "authorId": "2291073936"}, {"name": "Mahmoud Khademi", "authorId": "2268760479"}, {"name": "Dongdong Zhang", "authorId": "2273919921"}, {"name": "H. Awadalla", "authorId": "3032929"}, {"name": "Yi R. Fung", "authorId": "2352012716"}, {"name": "Weizhu Chen", "authorId": "2347682196"}, {"name": "Minhao Cheng", "authorId": "2258337019"}, {"name": "Furu Wei", "authorId": "2323870436"}], "n_citations": 7}, "snippets": ["Synthetic data has emerged as a promising approach to augment real data, addressing challenges related to data scarcity and the high cost of data collection and annotation. It has been widely applied in pre-training, instruction following, and reasoning tasks, demonstrating notable improvements in model performance. In this work, we focus on synthetic data generation for the post-training phase. Existing synthetic data generation methods typically rely on a small set of high-quality human-annotated seed samples, leveraging LLMs to generate diverse augmentations through various techniques: 1) sampling seed instructions as few-shot examples to elicit LLMs to generate new instructions [19,23,(Toshniwal et al., 2024)[50]; 2) leveraging LLMs to rephrase seed samples [12,21,34,35,(Yu et al., 2023). However, the scarcity of high-quality seed examples limits the scalability and diversity of generated data. To address this, Xu et al. [51] and Li et al. [24] propose generating synthetic data from scratch, leveraging either the inherent uncertainty of LLMs or a predefined knowledge taxonomy to guide the generation process. In contrast, pre-training data-being both vast and highly diverse-still remains an underutilized resource for scalable post-training data generation. Recent methods have explored extracting high-quality samples directly from web documents (similar to our Level-1 method) [54](Yue et al., 2024)(Zhou et al., 2024) or employing document backtranslation to generate questions (Li et al., 2023). Our SYNTHLLM aligns with this line of research and offers a more efficient approach to leveraging web documents for scaling diverse sample generation compared to aforementioned methods."], "score": 0.96337890625}, {"id": "(Yue et al., 2024)", "paper": {"corpus_id": 269605607, "title": "MAmmoTH2: Scaling Instructions from the Web", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xiang Yue", "authorId": "2284988933"}, {"name": "Tuney Zheng", "authorId": "2300091474"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Wenhu Chen", "authorId": "2249847177"}], "n_citations": 101}, "snippets": ["Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B's (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data."], "score": 0.0}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 269981934, "title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kun Zhou", "authorId": "2265383494"}, {"name": "Beichen Zhang", "authorId": "2107926615"}, {"name": "Jiapeng Wang", "authorId": "2302813110"}, {"name": "Zhipeng Chen", "authorId": "2111335050"}, {"name": "Wayne Xin Zhao", "authorId": "2257376413"}, {"name": "Jing Sha", "authorId": "2165225571"}, {"name": "Zhichao Sheng", "authorId": "2125340023"}, {"name": "Shijin Wang", "authorId": "2302793582"}, {"name": "Ji-Rong Wen", "authorId": "2274218622"}], "n_citations": 34}, "snippets": ["Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \\url{https://github.com/RUCAIBox/JiuZhang3.0}."], "score": 0.0}], "table": null}, {"title": "Evaluation and Optimization Methods", "tldr": "Evaluating and optimizing synthetic data generation involves both quality assessment through external validators and strategic resource allocation across bootstrapping iterations. Recent frameworks have introduced data-driven approaches that replace traditional human-engineered prompts with systematic optimization techniques to generate more effective task-specific training data. (2 sources)", "text": "\nEvaluating the quality and effectiveness of synthetically generated data presents unique challenges that have prompted the development of specialized methodologies. A critical component in many recent synthetic data pipelines is the use of external verification mechanisms to filter out low-quality samples. This verification step has proven particularly valuable in bootstrapping approaches, where models generate synthetic data iteratively for fine-tuning <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nThe bootstrapping process raises important questions about resource optimization. Yang et al. explored how total budgets for generation and training should be allocated across multiple iterations to maximize final performance. Their research highlights the tradeoff between generating more data in each iteration versus conducting more iterations with smaller datasets <Paper corpusId=\"276079713\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This optimization challenge becomes increasingly relevant as synthetic data generation scales up, particularly for resource-intensive tasks like mathematical reasoning and code generation.\n\nBeyond filtering approaches, recent work has introduced more sophisticated frameworks for optimizing synthetic data quality. The SoftSRV framework represents a significant advancement by using a data-driven loss minimization approach to steer large language models toward generating high-quality synthetic sequences for specific tasks. This approach provides a practical improvement over traditional prompt engineering methods that rely on human-crafted templates, which tend to be labor-intensive and domain-specific <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nSoftSRV has demonstrated effectiveness across multiple domains including coding, mathematical reasoning, and general reasoning tasks. By systematically optimizing the generation process based on target distributions, this framework creates more effective fine-tuning data that can significantly improve the performance of smaller language models <Paper corpusId=\"273507197\" paperTitle=\"(DeSalvo et al., 2024)\" isShortName></Paper>.\n\nThe move toward data-driven optimization represents an important shift in the field. Rather than relying on human intuition to craft prompts that might elicit useful synthetic data, these newer approaches employ statistical measures and learning-based techniques to systematically guide the generation process. This trend aligns with the broader movement in AI toward more automated and principled methods for model improvement <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAs synthetic data generation techniques continue to evolve, we can expect further refinements in evaluation metrics and optimization strategies. Developing robust, domain-agnostic methods for assessing synthetic data quality remains an active area of research, with particular emphasis on measuring how well synthetic datasets transfer to downstream tasks and generalize beyond their training distribution <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276079713, "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Boostrapping", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Pu Yang", "authorId": "2284644270"}, {"name": "Yunzhen Feng", "authorId": "2343647731"}, {"name": "Ziyuan Chen", "authorId": "2343512547"}, {"name": "Yuhang Wu", "authorId": "2343592601"}, {"name": "Zhuoyuan Li", "authorId": "2344832360"}], "n_citations": 0}, "snippets": ["Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model's performance improves--raising a crucial question: how should the total budget on generation and training be allocated across iterations to maximize final performance?"], "score": 0.93017578125}, {"id": "(DeSalvo et al., 2024)", "paper": {"corpus_id": 273507197, "title": "SoftSRV: Learn to Generate Targeted Synthetic Data", "year": 2024, "venue": "", "authors": [{"name": "Giulia DeSalvo", "authorId": "2280911353"}, {"name": "Jean-Fracois Kagy", "authorId": "2327046826"}, {"name": "Lazaros Karydas", "authorId": "2283008"}, {"name": "Afshin Rostamizadeh", "authorId": "2435268"}, {"name": "Sanjiv Kumar", "authorId": "2275226495"}], "n_citations": 0}, "snippets": ["We present a novel framework, SoftSRV, that is used to generate targeted synthetic fine-tuning data for improving task-specific model performance. Given a sample from a target distribution, our proposed framework uses a data-driven loss minimization approach to steer a frozen large language model (LLM) to generate synthetic sequences that are similar to those from the target distribution. SoftSRV provides a practical improvement over common prompt engineering approaches that rely on human-engineered prompt-templates, which can be idiosyncratic, labor-intensive to craft, and may need to be specialized per domain. We empirically evaluate our method against standard baselines guiding a large LLM to generate synthetic data to fine-tune a smaller language model on three different domains (coding, math, reasoning)."], "score": 0.94921875}], "table": null}, {"title": "Future Directions", "tldr": "Future research in synthetic data generation will likely focus on creating larger, more diverse datasets across complex data types while developing innovative architectures and privacy-preserving mechanisms. These advancements aim to democratize AI development and expand the application of synthetic data to new domains without requiring extensive computational resources. (1 source)", "text": "\nDespite significant progress in synthetic data generation for reasoning tasks, several promising directions remain for future exploration. Expanding the scale and diversity of synthetic datasets represents a primary goal for researchers. The ability to generate extensive datasets with high variability would address persistent data scarcity challenges in specialized domains and enhance the applicability of synthetic data in complex machine learning tasks <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nThe development of novel architectures beyond current generative models presents another important avenue for advancement. While GANs, VAEs, and diffusion models have demonstrated impressive capabilities, exploring innovative architectures could further improve synthetic data quality and diversity. Importantly, future research should focus on making these advancements accessible with limited computational resources, democratizing access to cutting-edge synthetic data generation techniques without requiring extensive infrastructure <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nPrivacy preservation remains a critical area for future development. As synthetic data generation expands into more sensitive domains like healthcare and finance, integrating robust privacy-preserving techniques becomes essential. Future work will likely focus on combining differential privacy mechanisms with generative models to provide stronger guarantees while maintaining data utility <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nFinally, expanding synthetic data generation to more complex data types represents a significant opportunity. While current approaches have shown promise for text, code, and certain types of images, applying these techniques to time-series data, graph structures, and multimodal datasets would substantially broaden their impact. Tailoring generative models to effectively handle these complex data structures could unlock new applications across various fields and further enhance reasoning capabilities in AI systems <Paper corpusId=\"273811804\" paperTitle=\"(Zbeeb et al., 2024)\" isShortName></Paper>.\n\nAs the field continues to mature, these future directions will likely converge with ongoing research in verification methods, domain-specific optimization techniques, and automated data generation workflows to create increasingly powerful tools for addressing complex reasoning tasks <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Zbeeb et al., 2024)", "paper": {"corpus_id": 273811804, "title": "Exploring the Landscape for Generative Sequence Models for Specialized Data Synthesis", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mohammad Zbeeb", "authorId": "2329103285"}, {"name": "Mohammad Ghorayeb", "authorId": "2329102879"}, {"name": "Mariam Salman", "authorId": "2329103319"}], "n_citations": 0}, "snippets": ["To further advance the field of synthetic data generation, several key areas warrant additional exploration and development. One significant avenue is the capability to generate larger and more diverse datasets. Expanding the capacity to synthesize extensive datasets with high variability would greatly enhance the applicability of synthetic data in machine learning tasks, especially in domains where data scarcity remains a challenge. \n\nMoreover, exploring innovative architectures beyond the current models can lead to substantial advancements. Investigating new generative models or enhancing existing ones could improve the quality and diversity of synthetic data. Importantly, demonstrating that these advancements can be achieved using accessible computational resources, such as a personal computer with a well-coded pipeline, would underscore the feasibility of cutting-edge AI developments without the need for extensive infrastructure. This democratization of technology could encourage broader participation in the field and accelerate innovation. \n\nAdditionally, integrating more robust privacy-preserving techniques into the data generation process remains a critical area for future work. As privacy concerns continue to grow, developing methods that ensure data utility while rigorously protecting sensitive information is essential. Combining differential privacy mechanisms with generative models could provide stronger guarantees and expand the adoption of synthetic data in sensitive domains. \n\nFinally, applying synthetic data generation techniques to a wider range of applications, including those with complex data types such as time-series, graphs, and multimodal data, would significantly broaden the impact of this research. Tailoring generative models to handle these complex data structures effectively could open new opportunities in various fields, from healthcare to finance, where such data types are prevalent."], "score": 0.943359375}], "table": null}], "cost": 0.6010229999999999}}
