{"better_query": "What specific cultural biases and language artifacts are introduced when translating English evaluation benchmarks into non-English languages, and how do they impact model performance and reliability assessments?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Natural language processing (NLP) evaluation has historically centered on English, with multilingual benchmarks typically created through translation from English sources. This approach introduces cultural biases and representational issues that can skew evaluation results and impact our understanding of model capabilities across different languages. (2 sources)", "text": "\nMultilingual evaluation in natural language processing has been predominantly English-centric, with most benchmarks originating in English and then being translated to other languages. Popular multilingual datasets like XNLI, XQUAD, MLQA, and Belebele follow this translation-based approach, where English content serves as the source material. <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> While this methodology facilitates valuable comparisons across languages through parallel content, it introduces significant challenges to fair and accurate evaluation.\n\nThe translation process itself is not culturally neutral. When English evaluation benchmarks are translated into non-English languages, they carry forward Western cultural assumptions, contexts, and references that may not resonate or function equivalently in target languages and cultures. <Paper corpusId=\"274610534\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> These embedded biases can impact how models perform on translated benchmarks, potentially providing an inaccurate picture of a model's capabilities in non-English languages.\n\nBeyond cultural representation issues, translation introduces structural artifacts that reflect the source language rather than natural expressions in the target language. These translation artifacts affect experimental conclusions by reflecting the origin culture and linguistic patterns rather than authentic target language usage. <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> This can lead to evaluation results that don't accurately represent how models would perform on content originally created in those languages.", "citations": [{"id": "(Etxaniz et al., 2024)", "paper": {"corpus_id": 270380088, "title": "BertaQA: How Much Do Language Models Know About Local Culture?", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Julen Etxaniz", "authorId": "2226458991"}, {"name": "Gorka Azkune", "authorId": "2481918"}, {"name": "A. Soroa", "authorId": "2260104163"}, {"name": "Oier L\u00f3pez de Lacalle", "authorId": "2251043402"}, {"name": "Mikel Artetxe", "authorId": "2347956"}], "n_citations": 11}, "snippets": ["Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture."], "score": 0.525390625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 274610534, "title": "KULTURE Bench: A Benchmark for Assessing Language Model in Korean Cultural Context", "year": 2024, "venue": "Pacific Asia Conference on Language, Information and Computation", "authors": [{"name": "Xiaonan Wang", "authorId": "2334689696"}, {"name": "Jinyoung Yeo", "authorId": "2334573099"}, {"name": "Joon-Ho Lim", "authorId": "2334827754"}, {"name": "Hansaem Kim", "authorId": "2334743749"}], "n_citations": 2}, "snippets": ["Current multilingual benchmarks often use translated English versions, which may incorporate Western cultural biases that do not accurately assess other languages and cultures."], "score": 0.5400390625}], "table": null}, {"title": "Types of Cultural Biases in Translated Benchmarks", "tldr": "Translated benchmarks contain multiple forms of cultural bias including Western-centric content representation, cultural nuance loss during translation, and representation biases toward dominant cultures, making them unreliable indicators of model performance across different languages and cultural contexts. (10 sources)", "text": "\nTranslating English benchmarks into other languages introduces several distinct types of cultural biases that significantly impact evaluation validity. One of the most pervasive issues is content bias, where benchmarks remain rooted in Anglo-American cultural contexts despite being translated. The MMLU benchmark, for example, contains numerous questions about specific Anglo-American cultural facts that models trained primarily on non-English data would struggle to answer correctly <Paper corpusId=\"276106987\" paperTitle=\"(Magnini et al., 2025)\" isShortName></Paper> <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. This creates an inherent disadvantage for evaluating models in non-English languages and cultures <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nRepresentation bias constitutes another significant concern. Datasets like mLAMA demonstrate a clear bias toward Western countries and facts, creating fairness issues when probing multilingual models <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>. This occurs because many benchmarks are built from English Wikipedia or Western-centric knowledge sources, resulting in evaluation sets that disproportionately feature Western cultural references <Paper corpusId=\"277066532\" paperTitle=\"(Etori et al., 2025)\" isShortName></Paper>.\n\nCultural nuance loss represents a third critical bias type. Culturally specific idioms, proverbs, and colloquial expressions often lose their meaning or are mistranslated when models lack cultural context <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. For instance, English idioms like \"raining cats and dogs\" might be translated literally into other languages, creating confusing or meaningless phrases in the target culture rather than finding culturally equivalent expressions <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nTranslation processes frequently demonstrate bias toward dominant cultures. Machine translation systems may favor translations that align with Western cultural norms even when source text belongs to non-Western cultures <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. This creates situations where culturally significant concepts are diluted or misrepresented in translated benchmarks <Paper corpusId=\"253885863\" paperTitle=\"(Milios et al., 2022)\" isShortName></Paper>.\n\nSocial and behavioral differences across cultures create additional bias challenges. When translating datasets into other languages, decisions must be made about modifying items to make them intelligible in the target culture <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. While some approaches attempt to automatically flag and remove culturally specific examples, these methods cannot capture all instances of cultural over-specificity, such as references to sports teams or region-specific foods <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>.\n\nThe mechanisms for addressing bias may themselves perpetuate inequality. Research shows that reasoning approaches like Chain-of-Thought may be more aligned with Western fairness norms while reinforcing cultural specificity in non-English languages <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This indicates an imbalance in multilingual fairness mechanisms, where bias mitigation efforts developed primarily for English-speaking cultures leave non-Western biases more embedded <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThese cultural biases collectively undermine the effectiveness of translated benchmarks as global evaluation tools <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>. They stem not only from language differences but also from the cultural knowledge required to interpret questions correctly, reducing the practical utility of translated datasets like MMLU for comprehensive multilingual evaluation <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Magnini et al., 2025)", "paper": {"corpus_id": 276106987, "title": "Evalita-LLM: Benchmarking Large Language Models on Italian", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bernardo Magnini", "authorId": "2338997440"}, {"name": "Roberto Zanoli", "authorId": "36024018"}, {"name": "Michele Resta", "authorId": "2343747257"}, {"name": "Martin Cimmino", "authorId": "2343745825"}, {"name": "Paolo Albano", "authorId": "2343739878"}, {"name": "Marco Madeddu", "authorId": "2277456934"}, {"name": "Viviana Patti", "authorId": "2337691090"}], "n_citations": 1}, "snippets": ["\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results."], "score": 0.5693359375}, {"id": "(Hendrycks et al., 2020)", "paper": {"corpus_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dan Hendrycks", "authorId": "3422872"}, {"name": "Collin Burns", "authorId": "90909974"}, {"name": "Steven Basart", "authorId": "104444594"}, {"name": "Andy Zou", "authorId": "1380103052"}, {"name": "Mantas Mazeika", "authorId": "16787428"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "J. Steinhardt", "authorId": "5164568"}], "n_citations": 4568}, "snippets": ["We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."], "score": 0.0}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures", ".Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."], "score": 0.63134765625}, {"id": "(Keleg et al., 2023)", "paper": {"corpus_id": 259108559, "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Amr Keleg", "authorId": "1725417862"}, {"name": "Walid Magdy", "authorId": "1745226"}], "n_citations": 11}, "snippets": ["While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks", ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models", ".The quality of the template might degrade after automatically translating it from English", "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias", ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."], "score": 0.53173828125}, {"id": "(Etori et al., 2025)", "paper": {"corpus_id": 277066532, "title": "LAG-MMLU: Benchmarking Frontier LLM Understanding in Latvian and Giriama", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Naome A. Etori", "authorId": "1742219452"}, {"name": "Kevin Lu", "authorId": "2350516308"}, {"name": "Randu Karisa", "authorId": "2350516959"}, {"name": "Arturs Kanepajs", "authorId": "2322446945"}], "n_citations": 0}, "snippets": ["However, translating English benchmarks introduces biases and noise and fails to account for region-specific and cultural understanding. This may also add another layer of cultural misalignment of the target language, such as questions about U.S. laws or customs (Liu et al., 2023)."], "score": 0.62890625}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 274130807, "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yufei Guo", "authorId": "2331613833"}, {"name": "Muzhe Guo", "authorId": "2331371661"}, {"name": "Juntao Su", "authorId": "2273060735"}, {"name": "Zhou Yang", "authorId": "2331371753"}, {"name": "Mengqiu Zhu", "authorId": "2331357933"}, {"name": "Hongfei Li", "authorId": "2331365682"}, {"name": "Mengyang Qiu", "authorId": "1455277182"}, {"name": "Shuo Shuo Liu", "authorId": "2331377869"}], "n_citations": 22}, "snippets": ["\u2022 Cultural or Regional Bias -Cultural Nuance Loss: Cultural idioms, proverbs, or colloquial phrases often lose their meaning or are mistranslated when a model does not consider the cultural context. For example, the English idiom \"It's raining cats and dogs\" could be literally translated into another language, resulting in a confusing or meaningless phrase in that target culture. A culturally aware model would translate it into a local equivalent, such as \"Il pleut des cordes\" (It's raining ropes) in French (Vanmassenhove et al., 2019). \n\n-Bias Toward Dominant Cultures: A machine translation system might favor translations that align with Western cultural norms over those of less dominant cultures. For instance, translating phrases related to food, clothing, or customs might reflect Western standards, even when the source text belongs to a non-Western culture. An example could be translating a traditional Chinese clothing item, \"Qipao,\" simply as \"dress,\" which dilutes the cultural significance (Sennrich, 2016)."], "score": 0.57275390625}, {"id": "(Milios et al., 2022)", "paper": {"corpus_id": 253885863, "title": "An Analysis of Social Biases Present in BERT Variants Across Multiple Languages", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aristides Milios", "authorId": "1516303035"}, {"name": "Parishad BehnamGhader", "authorId": "2101317786"}], "n_citations": 8}, "snippets": ["In this paper, we observed that cultural differences, the dominant religion, and the history of countries (e.g. wars) strongly affect the bias the models learn from a given language corpus. This explains why directly translating benchmarks (mostly originally written in English, with an Anglophone cultural context in mind) across languages is not necessarily a useful measure of the bias present in a given language model. It was observed that bias probes tend to be relatively \"fragile\", in terms of their sensitivity to exact wording, in ways that may be contrary to human expectations that a model be invariant to equivalent wordings. To most effectively investigate bias in a given language model, cultural experts seem to be necessary to provide input on how to best craft probing tasks, as oftentimes bias is expressed through coded language and synecdoche, where straight translation of bias benchmarks from other languages will not detect it."], "score": 0.59521484375}, {"id": "(Hershcovich et al., 2022)", "paper": {"corpus_id": 247594499, "title": "Challenges and Strategies in Cross-Cultural NLP", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Daniel Hershcovich", "authorId": "2064295987"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Heather Lent", "authorId": "49568895"}, {"name": "Miryam de Lhoneux", "authorId": "3295381"}, {"name": "Mostafa Abdou", "authorId": "30671790"}, {"name": "Stephanie Brandl", "authorId": "6547490"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Laura Cabello Piqueras", "authorId": "2093582149"}, {"name": "Ilias Chalkidis", "authorId": "2125376289"}, {"name": "Ruixiang Cui", "authorId": "1717462692"}, {"name": "Constanza Fierro", "authorId": "50110151"}, {"name": "Katerina Margatina", "authorId": "82259306"}, {"name": "Phillip Rust", "authorId": "1660797358"}, {"name": "Anders S\u00f8gaard", "authorId": "1700187"}], "n_citations": 182}, "snippets": ["When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process."], "score": 0.5400390625}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276580130, "title": "Assessing Large Language Models in Agentic Multilingual National Bias", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qianying Liu", "authorId": "4289746"}, {"name": "Katrina Qiyao Wang", "authorId": "2347161903"}, {"name": "Fei Cheng", "authorId": "2342687540"}, {"name": "S. Kurohashi", "authorId": "1795664"}], "n_citations": 0}, "snippets": ["English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems."], "score": 0.50634765625}, {"id": "(Singh et al., 2024)", "paper": {"corpus_id": 274464561, "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shivalika Singh", "authorId": "2283844788"}, {"name": "Angelika Romanou", "authorId": "1910588458"}, {"name": "Cl\u00e9mentine Fourrier", "authorId": "2080941785"}, {"name": "David Ifeoluwa Adelani", "authorId": "2518906"}, {"name": "Jian Gang Ngui", "authorId": "119076558"}, {"name": "Daniel Vila-Suero", "authorId": "1403849795"}, {"name": "Peerat Limkonchotiwat", "authorId": "1596821065"}, {"name": "Kelly Marchisio", "authorId": "1396188646"}, {"name": "Wei Qi Leong", "authorId": "2140097897"}, {"name": "Yosephine Susanto", "authorId": "2239100505"}, {"name": "Raymond Ng", "authorId": "2352918897"}, {"name": "Shayne Longpre", "authorId": "2283848744"}, {"name": "Wei-Yin Ko", "authorId": "2309005865"}, {"name": "Madeline Smith", "authorId": "2303318993"}, {"name": "Antoine Bosselut", "authorId": "2284866282"}, {"name": "Alice Oh", "authorId": "2320522443"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "2334310309"}, {"name": "Leshem Choshen", "authorId": "2283849613"}, {"name": "Daphne Ippolito", "authorId": "2290769562"}, {"name": "Enzo Ferrante", "authorId": "2268314228"}, {"name": "Marzieh Fadaee", "authorId": "2818759"}, {"name": "B. Ermi\u015f", "authorId": "2445273"}, {"name": "Sara Hooker", "authorId": "2257040307"}], "n_citations": 34}, "snippets": ["Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges."], "score": 0.921875}], "table": null}, {"title": "Translation Artifacts and Inconsistencies", "tldr": "Translation processes introduce systematic artifacts that compromise benchmark integrity, including inconsistencies between source and target languages that disproportionately affect low-resource languages. These artifacts not only reflect translation quality issues but also create evaluation challenges where benchmarks measure both model performance and translation tool effectiveness. (5 sources)", "text": "\nBeyond cultural biases, the translation process itself creates technical artifacts that significantly impact benchmark validity. These translation artifacts manifest as structural inconsistencies between source and target languages that can fundamentally alter evaluation outcomes. Researchers note that these artifacts don't necessarily indicate flaws exclusively in translated content\u2014original datasets may contain their own artifacts that translation either changes or potentially mitigates <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nTranslation inconsistencies disproportionately affect low-resource languages, creating an uneven evaluation landscape. In the XNLI benchmark, researchers identified significant performance gaps between human-translated and machine-translated target text versions, with these gaps serving as indicators of translation errors. This finding was confirmed through manual verification for Hindi and Urdu, where reannotated human-translated test instances showed poor agreement with the original English labels they were meant to preserve <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nThe evaluation methodology compounds these issues, particularly for multilingual benchmarks. Most multilingual assessments rely on automatic translation tools to convert English benchmarks into target languages due to the volume of questions. Projects like Okapi use ChatGPT for translating benchmarks, while GPT-4 evaluations used Azure Translator <Paper corpusId=\"270737791\" paperTitle=\"(Plaza et al., 2024)\" isShortName></Paper>. This practice means that evaluations in non-English languages effectively measure both the language model's performance and the translation tool's quality simultaneously, making it difficult to isolate genuine model capabilities.\n\nThese translation artifacts create methodological challenges for effective cross-lingual evaluation. Researchers recommend several approaches to address these issues, including using original annotations in all languages, starting with non-English annotations and translating to other languages, or conducting document-level translation to minimize inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Additionally, when designing multilingual bias benchmarks, considering various cultural contexts becomes crucial for developing culturally diverse datasets <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."], "score": 0.6298828125}, {"id": "(Agrawal et al., 2024)", "paper": {"corpus_id": 267413041, "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ashish Agrawal", "authorId": "2282542641"}, {"name": "Barah Fazili", "authorId": "2187454108"}, {"name": "P. Jyothi", "authorId": "144859542"}], "n_citations": 3}, "snippets": ["We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."], "score": 0.5244140625}, {"id": "(Plaza et al., 2024)", "paper": {"corpus_id": 270737791, "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Irene Plaza", "authorId": "2308274656"}, {"name": "Nina Melero", "authorId": "2293315903"}, {"name": "C. Pozo", "authorId": "2308274509"}, {"name": "Javier Conde", "authorId": "2230852635"}, {"name": "Pedro Reviriego", "authorId": "2243081043"}, {"name": "Marina Mayor-Rocher", "authorId": "2308274000"}, {"name": "Mar\u00eda Grandury", "authorId": "2176184513"}], "n_citations": 8}, "snippets": ["This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results."], "score": 0.54248046875}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268819377, "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias", "year": 2024, "venue": "Frontiers Comput. Sci.", "authors": [{"name": "Yuemei Xu", "authorId": "2257136845"}, {"name": "Ling Hu", "authorId": "2258334185"}, {"name": "Jiayi Zhao", "authorId": "2294513520"}, {"name": "Zihan Qiu", "authorId": "2294361104"}, {"name": "Yuqi Ye", "authorId": "2294363807"}, {"name": "Hanwen Gu", "authorId": "2294933103"}], "n_citations": 43}, "snippets": ["Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022)."], "score": 0.63427734375}, {"id": "(Talat1 et al., 2022)", "paper": {"corpus_id": 247626152, "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings", "year": 2022, "venue": "BIGSCIENCE", "authors": [{"name": "Zeerak Talat1", "authorId": "2165041549"}, {"name": "Aur\u00e9lie N\u00e9v\u00e9ol", "authorId": "1692256"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Manan Dey", "authorId": "1879591269"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "A. Luccioni", "authorId": "2993731"}, {"name": "Maraim Masoud", "authorId": "153528116"}, {"name": "Margaret Mitchell", "authorId": "49501003"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "S. Sharma", "authorId": "1409842673"}, {"name": "Arjun Subramonian", "authorId": "1677386832"}, {"name": "Jaesung Tae", "authorId": "2112211652"}, {"name": "Samson Tan", "authorId": "145814654"}, {"name": "D. Tunuguntla", "authorId": "70209311"}, {"name": "Oskar van der Wal", "authorId": "1986356851"}], "n_citations": 102}, "snippets": ["Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."], "score": 0.475341796875}], "table": null}, {"title": "Impact on Model Performance and Evaluation Reliability", "tldr": "Translation artifacts and cultural biases significantly skew performance measurements in multilingual evaluations, creating an illusion of language capability gaps that may actually reflect translation quality issues. These distortions particularly disadvantage low-resource languages and models trained primarily on non-English data, leading to unreliable assessments of true multilingual capabilities. (7 sources)", "text": "\nThe translation artifacts and cultural biases in multilingual benchmarks substantially impact model performance measurements and evaluation reliability in several interconnected ways. Most fundamentally, these issues create artificial performance disparities that may not reflect true language capabilities. Research shows that using English prompts to recall facts from multilingual models typically yields significantly better and more consistent performance than using non-English prompts, suggesting that evaluation results are skewed by translation quality rather than model competence <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>.\n\nThis distortion is particularly pronounced for low-resource languages. Analysis of the XNLI benchmark demonstrates that translation inconsistencies disproportionately affect languages with fewer resources, creating an uneven evaluation landscape that disadvantages certain languages systematically <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. This finding is supported by manual reannotation efforts in Hindi and Urdu, which revealed poor agreement between human-translated test instances and their original English labels, indicating fundamental translation problems rather than model capability issues <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nThe evaluation methodology itself compounds these reliability problems. When benchmarks in languages other than English are created through automatic translation tools, the evaluation results inadvertently measure both the language model's performance and the translation tool's quality simultaneously <Paper corpusId=\"270737791\" paperTitle=\"(Plaza et al., 2024)\" isShortName></Paper>. This dual measurement makes it nearly impossible to isolate genuine model capabilities from translation artifacts, creating significant uncertainty about actual multilingual performance.\n\nCultural knowledge requirements further undermine evaluation reliability. Many benchmarks contain questions that require specific cultural knowledge, such as those in MMLU that reference Anglo-American facts <Paper corpusId=\"276106987\" paperTitle=\"(Magnini et al., 2025)\" isShortName></Paper> <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. Models trained primarily on non-English data would naturally struggle with these culturally specific questions, leading to artificially lowered performance scores that reflect cultural knowledge gaps rather than language understanding deficiencies <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.\n\nEven bias mitigation strategies demonstrate uneven effectiveness across languages. Research indicates that reasoning approaches like Chain-of-Thought may align well with Western fairness norms while potentially reinforcing cultural specificity in non-English languages <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This creates an imbalance where bias mitigation efforts developed primarily for English-speaking cultures leave non-Western biases more deeply embedded, further complicating reliable cross-cultural performance assessment <Paper corpusId=\"276580130\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThese combined factors\u2014translation artifacts, cultural knowledge requirements, and inconsistent bias mitigation\u2014significantly reduce the reliability of multilingual benchmarks as accurate performance indicators. The result is an evaluation landscape where performance gaps between languages may reflect translation quality and cultural alignment more than actual model capabilities <Paper corpusId=\"274464561\" paperTitle=\"(Singh et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Keleg et al., 2023)", "paper": {"corpus_id": 259108559, "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Amr Keleg", "authorId": "1725417862"}, {"name": "Walid Magdy", "authorId": "1745226"}], "n_citations": 11}, "snippets": ["While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks", ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models", ".The quality of the template might degrade after automatically translating it from English", "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias", ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."], "score": 0.53173828125}, {"id": "(Agrawal et al., 2024)", "paper": {"corpus_id": 267413041, "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ashish Agrawal", "authorId": "2282542641"}, {"name": "Barah Fazili", "authorId": "2187454108"}, {"name": "P. Jyothi", "authorId": "144859542"}], "n_citations": 3}, "snippets": ["We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."], "score": 0.5244140625}, {"id": "(Plaza et al., 2024)", "paper": {"corpus_id": 270737791, "title": "Spanish and LLM Benchmarks: is MMLU Lost in Translation?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Irene Plaza", "authorId": "2308274656"}, {"name": "Nina Melero", "authorId": "2293315903"}, {"name": "C. Pozo", "authorId": "2308274509"}, {"name": "Javier Conde", "authorId": "2230852635"}, {"name": "Pedro Reviriego", "authorId": "2243081043"}, {"name": "Marina Mayor-Rocher", "authorId": "2308274000"}, {"name": "Mar\u00eda Grandury", "authorId": "2176184513"}], "n_citations": 8}, "snippets": ["This clearly introduces a cultural bias, especially when questions are related to subjects such as history, geography, art or general culture.Ideally, specific tests should be developed or at least adapted for each language.However, this is not the only problem.To be able to evaluate LLMs in many languages, and given the large number of questions of the benchmarks, the standard procedure is to translate the English test to the target language using automatic translation tools, for example, in the Okapi project Lai et al. (2023) three benchmarks from the Open LLM Leaderboard Beeching et al. (2023) are translated using ChatGPT while in the evaluation of GPT4, the tests were translated using Azure Translator Achiam et al. (2023).This implies that the benchmarks in languages other than English are not only measuring the performance of the LLM but also of the translation tool as the quality of the translation can clearly impact the results."], "score": 0.54248046875}, {"id": "(Magnini et al., 2025)", "paper": {"corpus_id": 276106987, "title": "Evalita-LLM: Benchmarking Large Language Models on Italian", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bernardo Magnini", "authorId": "2338997440"}, {"name": "Roberto Zanoli", "authorId": "36024018"}, {"name": "Michele Resta", "authorId": "2343747257"}, {"name": "Martin Cimmino", "authorId": "2343745825"}, {"name": "Paolo Albano", "authorId": "2343739878"}, {"name": "Marco Madeddu", "authorId": "2277456934"}, {"name": "Viviana Patti", "authorId": "2337691090"}], "n_citations": 1}, "snippets": ["\u2022 Quality of translations. Translation from English is carried out using available automatic translators, whose quality is rarely checked by professionals. In (Moroni et al., 2024), several issues related to the translation of English benchmarks are highlighted. Furthermore, the code and models used to translate these benchmarks are not directly available, making it hard -if not impossible -to reproduce the translations. This, in turn, makes it difficult to analyze whether there are errors or if there is a margin for improvement in the translation process originally used to translate the benchmarks. \u2022 Content. Although translated into Italian, benchmarks are still based on angloamerican texts, which do not consider the crucial diversity of Italian culture. As an example, in the MMLU benchmark (Hendrycks et al., 2020), several questions refer to very specific facts about the anglo-american culture, which are hardly known by LLMs trained only on Italian data. The presence of such cultural biases can potentially alter the evaluation process. \u2022 Style. Translated benchmarks are very sensitive to the style of automatic translation and do not fully reflect the stylistic varieties of native Italian. Particularly for generative tasks (e.g., producing a summary), stylistic biases may affect the evaluation results."], "score": 0.5693359375}, {"id": "(Hendrycks et al., 2020)", "paper": {"corpus_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dan Hendrycks", "authorId": "3422872"}, {"name": "Collin Burns", "authorId": "90909974"}, {"name": "Steven Basart", "authorId": "104444594"}, {"name": "Andy Zou", "authorId": "1380103052"}, {"name": "Mantas Mazeika", "authorId": "16787428"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "J. Steinhardt", "authorId": "5164568"}], "n_citations": 4568}, "snippets": ["We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."], "score": 0.0}, {"id": "(Singh et al., 2024)", "paper": {"corpus_id": 274464561, "title": "Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shivalika Singh", "authorId": "2283844788"}, {"name": "Angelika Romanou", "authorId": "1910588458"}, {"name": "Cl\u00e9mentine Fourrier", "authorId": "2080941785"}, {"name": "David Ifeoluwa Adelani", "authorId": "2518906"}, {"name": "Jian Gang Ngui", "authorId": "119076558"}, {"name": "Daniel Vila-Suero", "authorId": "1403849795"}, {"name": "Peerat Limkonchotiwat", "authorId": "1596821065"}, {"name": "Kelly Marchisio", "authorId": "1396188646"}, {"name": "Wei Qi Leong", "authorId": "2140097897"}, {"name": "Yosephine Susanto", "authorId": "2239100505"}, {"name": "Raymond Ng", "authorId": "2352918897"}, {"name": "Shayne Longpre", "authorId": "2283848744"}, {"name": "Wei-Yin Ko", "authorId": "2309005865"}, {"name": "Madeline Smith", "authorId": "2303318993"}, {"name": "Antoine Bosselut", "authorId": "2284866282"}, {"name": "Alice Oh", "authorId": "2320522443"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "2334310309"}, {"name": "Leshem Choshen", "authorId": "2283849613"}, {"name": "Daphne Ippolito", "authorId": "2290769562"}, {"name": "Enzo Ferrante", "authorId": "2268314228"}, {"name": "Marzieh Fadaee", "authorId": "2818759"}, {"name": "B. Ermi\u015f", "authorId": "2445273"}, {"name": "Sara Hooker", "authorId": "2257040307"}], "n_citations": 34}, "snippets": ["Cultural biases in multilingual datasets pose significant challenges for their effectiveness as global benchmarks. These biases stem not only from differences in language but also from the cultural knowledge required to interpret questions, reducing the practical utility of translated datasets like MMLU. Furthermore, translation often introduces artefacts that can distort the meaning or clarity of questions in the target language. A common practice in multilingual evaluation is to rely on machine-translated evaluation sets, but simply translating a dataset is insufficient to address these challenges."], "score": 0.921875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276580130, "title": "Assessing Large Language Models in Agentic Multilingual National Bias", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qianying Liu", "authorId": "4289746"}, {"name": "Katrina Qiyao Wang", "authorId": "2347161903"}, {"name": "Fei Cheng", "authorId": "2342687540"}, {"name": "S. Kurohashi", "authorId": "1795664"}], "n_citations": 0}, "snippets": ["English-speaking countries do not show the same bias amplification. CoT may be more aligned with Western fairness norms, while it reinforces cultural specificity in non-English languages. This shows an imbalance in multilingual fairness mechanisms, where bias mitigation efforts may be disproportionately developed for English-speaking cultures, leaving non-Western biases more embedded. Establishing a bias baseline without CoT can allow us to evaluate whether structured reasoning frameworks introduce additional bias artifacts, raising concerns about fairness in multilingual AI systems."], "score": 0.50634765625}], "table": null}, {"title": "Proposed Solutions and Recommendations", "tldr": "Researchers propose several approaches to address translation biases in multilingual benchmarks, including using original non-English content, implementing careful documentation, and employing local or regional resources rather than translations. (7 sources)", "text": "\n## Improving Benchmark Design\n- **Develop consistent multilingual test sets** through one of three approaches: (1) using original annotations in all languages, (2) starting with non-English annotations and translating to other languages, or (3) when translating from English, doing so at the document level to minimize inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n \n- **Create benchmarks from regional resources** rather than translating English content. The Include benchmark demonstrates this approach by using local exam sources that naturally contain questions about local history, culture, and regional knowledge instead of translating culturally biased benchmarks <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\n- **Engage professional and community annotators** to evaluate cultural biases in benchmarks. Global-MMLU improved quality by having annotators label samples as culturally-sensitive or culturally-agnostic, helping verify translations and evaluate cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\n## Addressing Cultural Representation\n- **Consider various cultural contexts** when designing multilingual bias benchmarks to develop culturally diverse datasets <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Expand targets of bias beyond gender** and address cultural differences that exist between languages when developing multilingual bias evaluation frameworks <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Implement cultural prompting** as a control strategy to increase cultural alignment for different countries and territories, which has been shown to improve cultural alignment in model outputs for 71-81% of countries in later GPT models <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.\n\n## Mitigating Translation Artifacts\n- **Automatically flag and remove culturally specific examples** from datasets, though this approach has limitations in capturing all instances of cultural over-specificity (like references to sports teams or region-specific foods) <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>.\n\n- **Address representational bias** in fact-based benchmarks by ensuring balanced sampling rather than randomly selecting facts that might favor Western cultures <Paper corpusId=\"259108559\" paperTitle=\"(Keleg et al., 2023)\" isShortName></Paper>.\n\n- **Increase transparency through documentation** of the translation process and potential biases to better understand limitations of multilingual benchmarks <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247626152\" paperTitle=\"(Talat1 et al., 2022)\" isShortName></Paper>.\n\n- **Conduct ongoing evaluation** of cultural bias in generative AI outputs to continuously improve multilingual performance assessment <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265445838\" paperTitle=\"(Tao et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["Future evaluation should better account for translation artifacts. The evaluation issues raised by our analysis do not have a simple solution. In fact, while we use the term translation artifacts to highlight that they are an unintended effect of translation that impacts final evaluation, one could also argue that it is the original datasets that contain the artifacts, which translation simply alters or even mitigates. 12 In any case, this is a more general issue that falls beyond the scope of cross-lingual transfer learning, so we argue that it should be carefully controlled when evaluating cross-lingual models. In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."], "score": 0.6298828125}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["The second problem relates to cultural biases inherently embedded in the English benchmarks that cannot be addressed using automated translations only (Singh et al., 2024). For example, the MMLU benchmark contains graduate questions from the US that rely heavily on national knowledge (regarding history, religion etc.) (Hendrycks et al., 2021). These questions are not meaningful when evaluating the reasoning and QA capabilities of LLMs regarding European languages and cultures", ".Cultural biases in multilingual datasets present substantial obstacles with regard to their scope, validity and reliability as global benchmarks (Singh et al., 2024). Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark.\n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."], "score": 0.63134765625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268819377, "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias", "year": 2024, "venue": "Frontiers Comput. Sci.", "authors": [{"name": "Yuemei Xu", "authorId": "2257136845"}, {"name": "Ling Hu", "authorId": "2258334185"}, {"name": "Jiayi Zhao", "authorId": "2294513520"}, {"name": "Zihan Qiu", "authorId": "2294361104"}, {"name": "Yuqi Ye", "authorId": "2294363807"}, {"name": "Hanwen Gu", "authorId": "2294933103"}], "n_citations": 43}, "snippets": ["Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences. Thus, when designing a multilingual bias benchmark, it's crucial to consider various cultural contexts and develop cultural-diverse datasets (Talat1 et al., 2022)."], "score": 0.63427734375}, {"id": "(Talat1 et al., 2022)", "paper": {"corpus_id": 247626152, "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings", "year": 2022, "venue": "BIGSCIENCE", "authors": [{"name": "Zeerak Talat1", "authorId": "2165041549"}, {"name": "Aur\u00e9lie N\u00e9v\u00e9ol", "authorId": "1692256"}, {"name": "Stella Biderman", "authorId": "103476203"}, {"name": "Miruna Clinciu", "authorId": "2029314697"}, {"name": "Manan Dey", "authorId": "1879591269"}, {"name": "S. Longpre", "authorId": "29909347"}, {"name": "A. Luccioni", "authorId": "2993731"}, {"name": "Maraim Masoud", "authorId": "153528116"}, {"name": "Margaret Mitchell", "authorId": "49501003"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}, {"name": "S. Sharma", "authorId": "1409842673"}, {"name": "Arjun Subramonian", "authorId": "1677386832"}, {"name": "Jaesung Tae", "authorId": "2112211652"}, {"name": "Samson Tan", "authorId": "145814654"}, {"name": "D. Tunuguntla", "authorId": "70209311"}, {"name": "Oskar van der Wal", "authorId": "1986356851"}], "n_citations": 102}, "snippets": ["Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies."], "score": 0.475341796875}, {"id": "(Tao et al., 2023)", "paper": {"corpus_id": 265445838, "title": "Cultural bias and cultural alignment of large language models", "year": 2023, "venue": "PNAS Nexus", "authors": [{"name": "Yan Tao", "authorId": "2105711243"}, {"name": "Olga Viberg", "authorId": "2269470454"}, {"name": "Ryan S. Baker", "authorId": "2255317189"}, {"name": "Ren\u00e9 F. Kizilcec", "authorId": "2246853606"}], "n_citations": 87}, "snippets": ["Abstract Culture fundamentally shapes people\u2019s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people\u2019s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI\u2019s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models\u2019 responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models\u2019 output for 71\u201381% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI."], "score": 0.0}, {"id": "(Hershcovich et al., 2022)", "paper": {"corpus_id": 247594499, "title": "Challenges and Strategies in Cross-Cultural NLP", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Daniel Hershcovich", "authorId": "2064295987"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Heather Lent", "authorId": "49568895"}, {"name": "Miryam de Lhoneux", "authorId": "3295381"}, {"name": "Mostafa Abdou", "authorId": "30671790"}, {"name": "Stephanie Brandl", "authorId": "6547490"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Laura Cabello Piqueras", "authorId": "2093582149"}, {"name": "Ilias Chalkidis", "authorId": "2125376289"}, {"name": "Ruixiang Cui", "authorId": "1717462692"}, {"name": "Constanza Fierro", "authorId": "50110151"}, {"name": "Katerina Margatina", "authorId": "82259306"}, {"name": "Phillip Rust", "authorId": "1660797358"}, {"name": "Anders S\u00f8gaard", "authorId": "1700187"}], "n_citations": 182}, "snippets": ["When translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture. Lin et al. (2021a) use machine translation to translate two common-sense reasoning datasets from English into 14 other languages. They attempt to deal with difficult cases by automatically flagging and removing examples which contain 'social keywords' from the dataset, or that are (again, automatically) labeled as containing non-neutral sentiment. However, these methods are unlikely to capture all examples of social behaviour and cannot identify examples of cultural over-specificity (sports teams, jalape\u00f1os). Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process."], "score": 0.5400390625}, {"id": "(Keleg et al., 2023)", "paper": {"corpus_id": 259108559, "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Amr Keleg", "authorId": "1725417862"}, {"name": "Walid Magdy", "authorId": "1745226"}], "n_citations": 11}, "snippets": ["While translating English benchmarks saves the time and money needed to build new language-specific benchmarks, it might introduce unintended biases or artifacts into the benchmarks", ".results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models", ".The quality of the template might degrade after automatically translating it from English", "Translating the underlying facts of a benchmark, initially designed to probe English PLMs, might cause a representational bias", ".Randomly sampling the triples from T-REx might introduce a representation bias toward Western cultures, since only facts aligned to English Wikipedia abstracts are considered."], "score": 0.53173828125}], "table": null}], "cost": 0.228195}}
