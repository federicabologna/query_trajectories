{"better_query": "What are the specific inference-time scaling laws identified in recent research, and how do they differ from traditional training scaling laws?", "better_answer": {"sections": [{"title": "Introduction to Scaling Laws", "tldr": "Scaling laws are mathematical relationships that predict how AI model performance changes with factors like model size, dataset size, and computational resources. These relationships typically follow power-law patterns across various domains and have expanded to include inference-time considerations beyond traditional training-focused scaling. (5 sources)", "text": "\nScaling laws represent mathematical relationships that predict how a model's performance\u2014typically measured by test loss\u2014changes with respect to key factors such as model size (parameter count), dataset size, and training compute resources <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. These laws have become fundamental to understanding the development trajectory of deep learning models, particularly as researchers pursue increasingly larger models to achieve better performance <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nEmpirical and theoretical research has consistently demonstrated that neural network performance follows power-law relationships with respect to parameter count and dataset size for well-trained models <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>. This pattern holds across various domains, including language modeling, computer vision, and reinforcement learning, though with different coefficients and exponents for each domain <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. The seminal work by Kaplan et al. (2020) established that language model performance follows these power-law relationships as computational resources, parameters, and data increase <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>.\n\nBeyond simple power-laws, research has shown that generalizations of power-law or non-power-law distributions can sometimes model neural scaling laws more accurately <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>. For instance, some studies demonstrate that exponential scaling works better than power-law scaling under certain data pruning conditions, and countably infinite parameter models may follow non-power-law distributions in unbounded data complexity regimes <Paper corpusId=\"272524356\" paperTitle=\"(Boopathy et al., 2024)\" isShortName></Paper>.\n\nScaling laws have practical applications in optimizing resource allocation. They help determine the optimal distribution of training data and model parameters within computational constraints <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This optimization is especially crucial for large language models (LLMs) that require significant computational resources for training. Even when prioritizing data usage over computational efficiency during training (as in approaches like LLAMA), scaling laws remain necessary to evaluate whether performance gains from larger models justify the increased inference costs <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe scope of scaling laws has expanded beyond traditional training-focused metrics. Recent research has extended scaling laws to predict downstream task performance <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> and\u2014critically for our discussion\u2014inference time considerations <Paper corpusId=\"273375506\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper>. This emerging focus on \"test-time compute scaling\" explores how models can leverage additional computational resources during inference to improve performance, sometimes more effectively than simply increasing model parameters <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Bullock et al., 2025)", "paper": {"corpus_id": 276903172, "title": "AGI, Governments, and Free Societies", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Justin B. Bullock", "authorId": "2281052678"}, {"name": "Samuel Hammond", "authorId": "2349384137"}, {"name": "Seb Krier", "authorId": "2349382249"}], "n_citations": 0}, "snippets": ["Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters."], "score": 0.73583984375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271600832, "title": "Are Bigger Encoders Always Better in Vision Large Models?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bozhou Li", "authorId": "2303441380"}, {"name": "Hao Liang", "authorId": "2303856806"}, {"name": "Zimo Meng", "authorId": "2314372736"}, {"name": "Wentao Zhang", "authorId": "2309265357"}], "n_citations": 3}, "snippets": ["In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary."], "score": 0.61865234375}, {"id": "(Boopathy et al., 2024)", "paper": {"corpus_id": 272524356, "title": "Breaking Neural Network Scaling Laws with Modularity", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Akhilan Boopathy", "authorId": "52089350"}, {"name": "Sunshine Jiang", "authorId": "2320292273"}, {"name": "William Yue", "authorId": "2297765362"}, {"name": "Jaedong Hwang", "authorId": "2238396678"}, {"name": "Abhiram Iyer", "authorId": "2297772570"}, {"name": "I. Fiete", "authorId": "8657128"}], "n_citations": 2}, "snippets": ["Many works present frameworks to quantify scaling laws that map a NN's parameter count or training dataset size to an estimated testing loss. Empirically and theoretically, these works find that testing loss scales as a power-law with respect to the dataset size and parameter count on welltrained NNs (Bahri et al., 2021;Rosenfeld et al., 2020), including transformer-based language models (Sharma, 2022)(Clark et al., 2022)(Tay et al., 2021). \n\nMany previous works also conclude that generalizations of power-law or nonpower-law-based distributions can also model neural scaling laws well, in many cases better than vanilla power-law frameworks (Mahmood et al., 2022)Alabdulmohsin et al., 2022). For instance, Hutter (2021) shows that countably infinite parameter models closely follow non-power-law-based distributions under unbounded data complexity regimes. In another case, Sorscher et al. (2022) show that exponential scaling works better than power-law scaling if the testing loss is associated with a pruned dataset size, given a pruning metric that discards easy or hard examples under abundant or scarce data guarantees, respectively."], "score": 0.73046875}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 273375506, "title": "Towards Neural Scaling Laws for Time Series Foundation Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Qingren Yao", "authorId": "2326120888"}, {"name": "Chao-Han Huck Yang", "authorId": "2326237480"}, {"name": "Renhe Jiang", "authorId": "2268522268"}, {"name": "Yuxuan Liang", "authorId": "2253824408"}, {"name": "Ming Jin", "authorId": "2298723734"}, {"name": "Shirui Pan", "authorId": "2254047333"}], "n_citations": 9}, "snippets": ["In language domains, Kaplan et al. (2020) demonstrated that performance follows a power-law relationship, improving as more computational resources, parameters, and data are utilized. Subsequent research has expanded this to predict other factors, such as downstream task performance (Isik et al., 2024) and inference time (Sardana et al., 2023)."], "score": 0.63818359375}, {"id": "(Sardana et al., 2023)", "paper": {"corpus_id": 266693796, "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Sardana", "authorId": "2277217297"}, {"name": "Sasha Doubov", "authorId": "2040790531"}, {"name": "Jonathan Frankle", "authorId": "2277215716"}], "n_citations": 88}, "snippets": ["Large language model (LLM) scaling laws are empirical formulas that estimate changes in model quality as a result of increasing parameter count and training data. However, these formulas, including the popular Deepmind Chinchilla scaling laws, neglect to include the cost of inference. We modify the Chinchilla scaling laws to calculate the optimal LLM parameter count and pre-training data size to train and deploy a model of a given quality and inference demand. We conduct our analysis both in terms of a compute budget and real-world costs and find that LLM researchers expecting reasonably large inference demand (~1B requests) should train models smaller and longer than Chinchilla-optimal. Furthermore, we train 47 models of varying sizes and parameter counts to validate our formula and find that model quality continues to improve as we scale tokens per parameter to extreme ranges (up to 10,000). Finally, we ablate the procedure used to fit the Chinchilla scaling law coefficients and find that developing scaling laws only from data collected at typical token/parameter ratios overestimates the impact of additional tokens at these extreme ranges."], "score": 0.0}], "table": null}, {"title": "Inference-Time Scaling Laws Definition", "tldr": "Inference-time scaling laws describe mathematical relationships between model performance and computational resources allocated during inference, as opposed to training. These laws often follow log-linear or exponentiated power-law patterns and focus on optimizing how models utilize additional computation when generating outputs. (4 sources)", "text": "\nInference-time scaling laws represent a significant extension of traditional scaling laws, focusing specifically on how model performance changes with computational resources allocated during the inference (testing) phase rather than during training. These laws examine the relationship between performance metrics and the amount of computation used when a model is generating outputs or solving problems <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws are particularly relevant for decoder-only language models where generation quality depends on the tokens produced during inference <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>.\n\nA fundamental concept in inference-time scaling is \"coverage,\" defined as the fraction of problems that can be solved by any generated sample. Research has demonstrated that coverage scales with the number of samples over multiple orders of magnitude, often following a log-linear relationship that can be modeled as an exponentiated power law <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>. This consistent mathematical pattern suggests the existence of inference-time scaling laws that are distinct from but complementary to traditional training scaling laws.\n\nThe emergence of these inference-time scaling laws has practical implications for model deployment strategies. While traditional scaling laws have guided the development of increasingly large models, inference scaling laws help optimize how these models are used in practice, balancing performance against operational costs <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. Recent research has shown that optimizing test-time compute allocation can significantly enhance problem-solving performance, and that dynamic adjustments in sample allocation can maximize efficiency under compute constraints <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nThe concept of \"test-time compute scaling\" encompasses techniques that allow models to utilize additional computational resources when processing specific prompts, potentially leading to improved performance that can sometimes be more effective than simply scaling model parameters <Paper corpusId=\"276903172\" paperTitle=\"(Bullock et al., 2025)\" isShortName></Paper>. This strategic allocation of compute during inference represents a promising direction for enhancing AI capabilities beyond the traditional approach of increasing model size or training data.", "citations": [{"id": "(Zeng et al., 2025)", "paper": {"corpus_id": 278367792, "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xinyue Zeng", "authorId": "2359897638"}, {"name": "Haohui Wang", "authorId": "2155587513"}, {"name": "Junhong Lin", "authorId": "2311427666"}, {"name": "Jun Wu", "authorId": "2359758871"}, {"name": "Tyler Cody", "authorId": "2359450667"}, {"name": "Dawei Zhou", "authorId": "2313576252"}], "n_citations": 0}, "snippets": ["Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs."], "score": 0.75732421875}, {"id": "(Brown et al., 2024)", "paper": {"corpus_id": 271571035, "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bradley Brown", "authorId": "2283198901"}, {"name": "Jordan Juravsky", "authorId": "50875781"}, {"name": "Ryan Ehrlich", "authorId": "2283134957"}, {"name": "Ronald Clark", "authorId": "2313919316"}, {"name": "Quoc V. Le", "authorId": "2151097303"}, {"name": "Christopher R'e", "authorId": "2313917068"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}], "n_citations": 330}, "snippets": ["Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance."], "score": 0.7138671875}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276580569, "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bingxuan Li", "authorId": "2319420762"}, {"name": "Yiwei Wang", "authorId": "2280103482"}, {"name": "Jiuxiang Gu", "authorId": "2343700013"}, {"name": "Kai-Wei Chang", "authorId": "2257127887"}, {"name": "Nanyun Peng", "authorId": "2256996328"}], "n_citations": 5}, "snippets": ["Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."], "score": 0.78466796875}, {"id": "(Bullock et al., 2025)", "paper": {"corpus_id": 276903172, "title": "AGI, Governments, and Free Societies", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Justin B. Bullock", "authorId": "2281052678"}, {"name": "Samuel Hammond", "authorId": "2349384137"}, {"name": "Seb Krier", "authorId": "2349382249"}], "n_citations": 0}, "snippets": ["Scaling laws, in their formal sense, are mathematical relationships that predict how a model's performance (often measured by test loss) changes with factors like model size, dataset size, and training compute. Notably, these laws have exhibited a consistent pattern across various domains, including language modeling, computer vision, and reinforcement learning--demonstrating that power laws (albeit with different coefficients and exponents) exist in these domains. More generally however, 'scaling' refers to the approach of improving AI capabilities by increasing the scale of inputs within existing model paradigms. This includes increasing model size, data, and compute, and investing in post-training techniques like prompting, tool use, and scaffolding (i.e., 'unhobbling' the model). Crucially, recent research has expanded the understanding of scaling to include the strategic allocation of compute during inference. This 'test-time compute scaling' involves techniques that allow models to utilize additional computational resources when processing a specific prompt, leading to improved performance. Snell et al. (2024) demonstrate that, in certain cases, optimizing inference-time compute can be more effective than simply scaling model parameters."], "score": 0.73583984375}], "table": null}, {"title": "Specific Inference-Time Scaling Laws and Patterns", "tldr": "Recent research has identified distinct mathematical patterns in how model performance improves with increased inference computation, including log-linear relationships in coverage scaling and power-law patterns across multiple domains. These inference-time scaling laws reveal that performance gains can be achieved through various strategies such as multiple sampling, recursive techniques, and optimal allocation of inference resources. (7 sources)", "text": "\nResearch has revealed several specific mathematical patterns that characterize inference-time scaling laws across different AI domains. A foundational pattern observed by Brown et al. is that coverage\u2014the fraction of problems solved by any generated sample\u2014scales with the number of samples over four orders of magnitude, following a log-linear relationship that can be modeled as an exponentiated power law <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>. This pattern is particularly evident in domains like coding and formal proofs, where solution verification is automated and increased coverage directly translates to improved performance <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper>.\n\nSchaeffer et al. identified an apparent puzzle in these scaling patterns: while aggregate success rates across multiple tasks follow power-law scaling with respect to the number of attempts, individual problems show exponential scaling in their failure rates <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>. They reconciled this apparent contradiction by demonstrating that if the distribution of single-attempt success probabilities is heavy-tailed\u2014with a small fraction of extremely difficult problems\u2014this warps the aggregate success trend into a power law even as each individual problem scales exponentially <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>.\n\nBeyond language models, inference-time scaling has been explored in other domains such as diffusion models, which inherently allow adjustment of inference computation through the number of denoising steps <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This research demonstrates that inference-time scaling behaviors extend across different model architectures, suggesting these laws may represent fundamental properties of computational systems.\n\nRecent advances in recursive inference strategies (RINS) have revealed another dimension of inference-time scaling. Alabdulmohsin et al. found that RINS improves both the scaling exponent and asymptotic limit of performance <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>. Their analysis showed that when modeling RINS performance as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, the exponent cr increases with recursion rounds r while the asymptotic limit \u03b5r decreases <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>. This implies that increasing recursion rounds may initially show higher loss due to larger \u03b2r values, but eventually leads to superior performance through faster convergence and lower asymptotic limits <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper>.\n\nMultiple studies have confirmed these inference-time scaling patterns in diverse applications. Chain-of-thought prompting demonstrates how longer inference paths through additional token generation can improve reasoning capabilities in LLMs <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>. Similarly, systems like AlphaCode generate millions of diverse programs and then filter them down, achieving competitive performance in programming competitions <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>. These approaches show consistent patterns: increasing inference-time compute yields reliable performance improvements that follow predictable scaling laws <Paper corpusId=\"277467322\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Brown et al., 2024)", "paper": {"corpus_id": 271571035, "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bradley Brown", "authorId": "2283198901"}, {"name": "Jordan Juravsky", "authorId": "50875781"}, {"name": "Ryan Ehrlich", "authorId": "2283134957"}, {"name": "Ronald Clark", "authorId": "2313919316"}, {"name": "Quoc V. Le", "authorId": "2151097303"}, {"name": "Christopher R'e", "authorId": "2313917068"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}], "n_citations": 330}, "snippets": ["Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance."], "score": 0.7138671875}, {"id": "(Schaeffer et al., 2025)", "paper": {"corpus_id": 276580891, "title": "How Do Large Language Monkeys Get Their Power (Laws)?", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rylan Schaeffer", "authorId": "1749176844"}, {"name": "Joshua Kazdan", "authorId": "2327048379"}, {"name": "John Hughes", "authorId": "2294572631"}, {"name": "Jordan Juravsky", "authorId": "50875781"}, {"name": "Sara Price", "authorId": "2333593482"}, {"name": "Aengus Lynch", "authorId": "2287830769"}, {"name": "Erik Jones", "authorId": "2334069920"}, {"name": "Robert Kirk", "authorId": "2311693657"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}, {"name": "Oluwasanmi Koyejo", "authorId": "143812875"}], "n_citations": 4}, "snippets": ["Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own."], "score": 0.75341796875}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 275570556, "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nanye Ma", "authorId": "2279750963"}, {"name": "Shangyuan Tong", "authorId": "2058178039"}, {"name": "Haolin Jia", "authorId": "2340654494"}, {"name": "Hexiang Hu", "authorId": "2307548497"}, {"name": "Yu-Chuan Su", "authorId": "2269866136"}, {"name": "Mingda Zhang", "authorId": "2326256475"}, {"name": "Xuan Yang", "authorId": "2340726220"}, {"name": "Yandong Li", "authorId": "2324838111"}, {"name": "T. Jaakkola", "authorId": "35132120"}, {"name": "Xuhui Jia", "authorId": "2325917399"}, {"name": "Saining Xie", "authorId": "2324769373"}], "n_citations": 69}, "snippets": ["Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation."], "score": 0.919921875}, {"id": "(Alabdulmohsin et al., 2025)", "paper": {"corpus_id": 276259426, "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ibrahim M. Alabdulmohsin", "authorId": "2922782"}, {"name": "Xiao-Qi Zhai", "authorId": "2045380893"}], "n_citations": 0}, "snippets": ["Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such", "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance."], "score": 0.8095703125}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Dale Schuurmans", "authorId": "1714772"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "F. Xia", "authorId": "144956443"}, {"name": "Quoc Le", "authorId": "1998340269"}, {"name": "Denny Zhou", "authorId": "65855107"}], "n_citations": 9683}, "snippets": ["We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."], "score": 0.0}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 246527904, "title": "Competition-level code generation with AlphaCode", "year": 2022, "venue": "Science", "authors": [{"name": "Yujia Li", "authorId": "47002813"}, {"name": "David Choi", "authorId": "2114950020"}, {"name": "Junyoung Chung", "authorId": "8270717"}, {"name": "Nate Kushman", "authorId": "1684887"}, {"name": "Julian Schrittwieser", "authorId": "4337102"}, {"name": "R\u00e9mi Leblond", "authorId": "37212795"}, {"name": "Tom", "authorId": "2152472076"}, {"name": "Eccles", "authorId": "2152469120"}, {"name": "James Keeling", "authorId": "2058168486"}, {"name": "Felix Gimeno", "authorId": "49423009"}, {"name": "A. D. Lago", "authorId": "2152469362"}, {"name": "T. Hubert", "authorId": "2067208983"}, {"name": "Peter Choy", "authorId": "2070068655"}, {"name": "Cyprien de", "authorId": "2154435638"}, {"name": "Masson d\u2019Autume", "authorId": "2152471553"}, {"name": "Igor Babuschkin", "authorId": "2256699276"}, {"name": "Xinyun Chen", "authorId": "1425082935"}, {"name": "Po-Sen Huang", "authorId": "2421691"}, {"name": "Johannes Welbl", "authorId": "1851564"}, {"name": "Sven Gowal", "authorId": "2071666"}, {"name": "Alexey", "authorId": "2079024030"}, {"name": "Cherepanov", "authorId": "152394142"}, {"name": "James Molloy", "authorId": "2065370007"}, {"name": "D. Mankowitz", "authorId": "3187297"}, {"name": "Esme Sutherland Robson", "authorId": "2152471960"}, {"name": "Pushmeet Kohli", "authorId": "143967473"}, {"name": "Nando de", "authorId": "2152472162"}, {"name": "Freitas", "authorId": "2152469135"}, {"name": "K. Kavukcuoglu", "authorId": "2645384"}, {"name": "O. Vinyals", "authorId": "1689108"}], "n_citations": 1419}, "snippets": ["Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests."], "score": 0.0}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277467322, "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?", "year": 2025, "venue": "", "authors": [{"name": "Qiyuan Zhang", "authorId": "2124959429"}, {"name": "Fuyuan Lyu", "authorId": "1704274486"}, {"name": "Zexu Sun", "authorId": "2353075323"}, {"name": "Lei Wang", "authorId": "2353322623"}, {"name": "Weixu Zhang", "authorId": "2353196827"}, {"name": "Zhihan Guo", "authorId": "2301269240"}, {"name": "Yufei Wang", "authorId": "2268629268"}, {"name": "Irwin King", "authorId": "2258549144"}, {"name": "Xue Liu", "authorId": "2188246843"}, {"name": "Chen Ma", "authorId": "2324831308"}], "n_citations": 14}, "snippets": ["Notably, some studies (Brown et al., 2024;Wu et al., 2024d) observe patterns akin to scaling laws: increasing inference-time compute yields consistent performance improvements."], "score": 0.6806640625}], "table": null}, {"title": "Applications and Implementations of Inference-Time Scaling", "tldr": "Inference-time scaling has been implemented in various applications including code generation systems, reasoning frameworks, image generation models, and commercial LLMs like OpenAI's o1 and DeepSeek R1. These implementations leverage strategies such as multiple sampling, chain-of-thought prompting, and recursive techniques to achieve significant performance improvements across different tasks. (12 sources)", "text": "\nInference-time scaling approaches have been successfully implemented across diverse applications in the AI landscape. In programming domains, systems like AlphaCode and Codex generate multiple program samples during inference to enhance code generation capabilities. <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper> This approach has proven particularly effective for competitive programming problems, with AlphaCode achieving rankings in the top 54.3% in programming competitions, representing a breakthrough in AI performance on such complex tasks. <Paper corpusId=\"246527904\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>\n\nFor reasoning tasks, chain-of-thought (CoT) prompting has emerged as a powerful inference-time scaling technique. By generating longer inference paths through additional token generation, CoT significantly improves large language models' reasoning capabilities across arithmetic, commonsense, and symbolic reasoning tasks. <Paper corpusId=\"276259426\" paperTitle=\"(Alabdulmohsin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper> The empirical gains can be striking\u2014for instance, prompting a 540B-parameter language model with just eight chain-of-thought exemplars has achieved state-of-the-art accuracy on mathematical word problems. <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>\n\nBeyond language models, diffusion models represent another domain where inference-time scaling has been applied. These models inherently possess the flexibility to adjust inference-time computation through the number of denoising steps, though performance gains typically plateau after a few dozen steps. Recent research explores extending inference-time scaling behavior in diffusion models beyond simply increasing denoising steps to further improve generation performance with increased computation. <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\nCommercial implementations of inference-time scaling have gained prominence with systems like OpenAI's o1 model and DeepSeek R1, which demonstrate consistent output improvements with increased inference computation. <Paper corpusId=\"277313756\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper> Research shows that significantly increasing computational resources during inference can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. <Paper corpusId=\"275471424\" paperTitle=\"(Wan, 2025)\" isShortName></Paper> This finding validates the importance of computational scaling during inference and provides a new technical pathway for optimizing LLM performance, with o1's outstanding performance confirming the significant positive correlation between inference process resource investment and model performance. <Paper corpusId=\"275471424\" paperTitle=\"(Wan, 2025)\" isShortName></Paper>\n\nRecent research has also explored optimization strategies for inference-time scaling. The concept of \"test-time budget forcing\" aims to achieve high efficiency with limited token sampling during inference. <Paper corpusId=\"277313756\" paperTitle=\"(Kim et al., 2025)\" isShortName></Paper> Additionally, quantization research has investigated how reduced bit precision affects inference scaling, with studies examining precisions from 3 to 16-bit to disentangle factors that improve scaling. <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper> This work is particularly relevant for deployment scenarios with small batch sizes common for consumers and small organizations. <Paper corpusId=\"254853733\" paperTitle=\"(Dettmers et al., 2022)\" isShortName></Paper>\n\nFor specific task domains, inference-time scaling implementations show varying effectiveness. In reading comprehension tasks such as CoQA and SQuAD, denser models perform better, potentially due to their higher inference-time compute compared to perplexity-matched sparse models. <Paper corpusId=\"275789885\" paperTitle=\"(Abnar et al., 2025)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper> <Paper corpusId=\"52055325\" paperTitle=\"(Reddy et al., 2018)\" isShortName></Paper> This performance gap can be addressed through strategies that increase inference-time compute dynamically, such as chain-of-thought prompting or the \"pause-training\" technique, which allows models to process extra computation before committing to an answer. <Paper corpusId=\"275789885\" paperTitle=\"(Abnar et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper> <Paper corpusId=\"263608983\" paperTitle=\"(Goyal et al., 2023)\" isShortName></Paper>\n\nWhile many inference-time scaling implementations have demonstrated success, most current approaches rely on specific scenarios and datasets. <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> This limitation highlights the need for more generalizable inference-time scaling strategies that can be applied across diverse tasks and domains.", "citations": [{"id": "(Alabdulmohsin et al., 2025)", "paper": {"corpus_id": 276259426, "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ibrahim M. Alabdulmohsin", "authorId": "2922782"}, {"name": "Xiao-Qi Zhai", "authorId": "2045380893"}], "n_citations": 0}, "snippets": ["Besides this conventional approach of scaling training compute, the impact of increased inference compute on model performance has emerged as another key scaling dimension. For example, chainof-thought (CoT) prompting show that eliciting longer inference paths through additional token generation could improve the reasoning capabilities of LLMs (Wei et al., 2022), similar to the success of critiquing before evaluating (Ankner et al., 2024). Also, AlphaCode (Li et al., 2022) and Codex (Chen et al., 2021) generate multiple samples during inference to enhance code generation. Remarkably, Brown et al. (2024) shows that the benefit of sampling multiple solutions in tasks such", "As shown in Figure 5a, RINS improves both the scaling exponent and the asymptotic limit. The improvement in the asymptotic limit provides further evidence that the performance gap in favor of RINS is not closed by overtraining non-recursive models.\n\nOptimal Number of Recursion Rounds. We observe from the scaling parameters in the previous section that if the performance of RINS is modeled as a power law fr(x) = \u03b2r x^(-cr) + \u03b5r, then cr increases with r while \u03b5r decreases. Furthermore, the coefficient \u03b2r also increases with r. This implies that while scaling inference by increasing r might initially exhibit a higher loss due to the larger \u03b2r, its faster convergence (cr) and lower asymptotic limit (\u03b5r) will eventually lead to superior performance."], "score": 0.8095703125}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 246527904, "title": "Competition-level code generation with AlphaCode", "year": 2022, "venue": "Science", "authors": [{"name": "Yujia Li", "authorId": "47002813"}, {"name": "David Choi", "authorId": "2114950020"}, {"name": "Junyoung Chung", "authorId": "8270717"}, {"name": "Nate Kushman", "authorId": "1684887"}, {"name": "Julian Schrittwieser", "authorId": "4337102"}, {"name": "R\u00e9mi Leblond", "authorId": "37212795"}, {"name": "Tom", "authorId": "2152472076"}, {"name": "Eccles", "authorId": "2152469120"}, {"name": "James Keeling", "authorId": "2058168486"}, {"name": "Felix Gimeno", "authorId": "49423009"}, {"name": "A. D. Lago", "authorId": "2152469362"}, {"name": "T. Hubert", "authorId": "2067208983"}, {"name": "Peter Choy", "authorId": "2070068655"}, {"name": "Cyprien de", "authorId": "2154435638"}, {"name": "Masson d\u2019Autume", "authorId": "2152471553"}, {"name": "Igor Babuschkin", "authorId": "2256699276"}, {"name": "Xinyun Chen", "authorId": "1425082935"}, {"name": "Po-Sen Huang", "authorId": "2421691"}, {"name": "Johannes Welbl", "authorId": "1851564"}, {"name": "Sven Gowal", "authorId": "2071666"}, {"name": "Alexey", "authorId": "2079024030"}, {"name": "Cherepanov", "authorId": "152394142"}, {"name": "James Molloy", "authorId": "2065370007"}, {"name": "D. Mankowitz", "authorId": "3187297"}, {"name": "Esme Sutherland Robson", "authorId": "2152471960"}, {"name": "Pushmeet Kohli", "authorId": "143967473"}, {"name": "Nando de", "authorId": "2152472162"}, {"name": "Freitas", "authorId": "2152469135"}, {"name": "K. Kavukcuoglu", "authorId": "2645384"}, {"name": "O. Vinyals", "authorId": "1689108"}], "n_citations": 1419}, "snippets": ["Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers\u2019 productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. \u2014YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests."], "score": 0.0}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Dale Schuurmans", "authorId": "1714772"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "F. Xia", "authorId": "144956443"}, {"name": "Quoc Le", "authorId": "1998340269"}, {"name": "Denny Zhou", "authorId": "65855107"}], "n_citations": 9683}, "snippets": ["We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."], "score": 0.0}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 275570556, "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nanye Ma", "authorId": "2279750963"}, {"name": "Shangyuan Tong", "authorId": "2058178039"}, {"name": "Haolin Jia", "authorId": "2340654494"}, {"name": "Hexiang Hu", "authorId": "2307548497"}, {"name": "Yu-Chuan Su", "authorId": "2269866136"}, {"name": "Mingda Zhang", "authorId": "2326256475"}, {"name": "Xuan Yang", "authorId": "2340726220"}, {"name": "Yandong Li", "authorId": "2324838111"}, {"name": "T. Jaakkola", "authorId": "35132120"}, {"name": "Xuhui Jia", "authorId": "2325917399"}, {"name": "Saining Xie", "authorId": "2324769373"}], "n_citations": 69}, "snippets": ["Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation."], "score": 0.919921875}, {"id": "(Kim et al., 2025)", "paper": {"corpus_id": 277313756, "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jaihoon Kim", "authorId": "2292419803"}, {"name": "Taehoon Yoon", "authorId": "2328014454"}, {"name": "Jisung Hwang", "authorId": "2322353657"}, {"name": "Minhyuk Sung", "authorId": "2322097538"}], "n_citations": 3}, "snippets": ["Over the past years, scaling laws of AI models have mainly focused on increasing model size and training data. However, recent advancements have shifted attention toward inference-time scaling (Stiennon et al., 2020)[51], leveraging computational resources during inference to enhance model performance. \n\nOpenAI o1 [38] and DeepSeek R1 [11] exemplify this approach, demonstrating consistent output improvements with increased inference computation. Recent research in LLMs [37] attempting to replicate such improvements has introduced test-time budget forcing, achieving high efficiency with limited token sampling during inference."], "score": 0.92333984375}, {"id": "(Wan, 2025)", "paper": {"corpus_id": 275471424, "title": "Unifying Two Types of Scaling Laws from the Perspective of Conditional Kolmogorov Complexity", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jun Wan", "authorId": "2340178423"}], "n_citations": 1}, "snippets": ["Snell et al. (2024) conducted an in-depth study on the feasibility of improving LLMs performance by increasing computational resources during the inference process. This research direction was empirically supported by OpenAI's o1 model released in September 2024. Research shows that significantly increasing computational resources and time investment during the inference process can effectively improve model performance in complex tasks such as mathematics, programming, and logical reasoning. This finding not only validates the importance of computational scaling during inference but also provides a new technical pathway for optimizing LLMs performance. The outstanding performance of OpenAI's o1 model (Jaech et al., 2024) further confirms that there is a significant positive correlation between inference process resource investment and model performance, particularly demonstrating important practical value when handling high-complexity tasks."], "score": 0.89013671875}, {"id": "(Dettmers et al., 2022)", "paper": {"corpus_id": 254853733, "title": "The case for 4-bit precision: k-bit Inference Scaling Laws", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 234}, "snippets": ["For inference, there has been work that studies scaling trends of zero-shot performance for 4-bit vs. 16-bit models (Zeng et al., 2022). \n\nWe study precisions from 3 to 16-bit and disentangle the factors that improve scaling. Work by Pope et al. (2022) looks at scaling inference in a production setting where large batch sizes are common. While they only study quantization rudimentary, they disentangle factors that lead to better model FLOPS utilization (MFU). Since reducing the bit-precision of bits loaded leads to higher MFU, it is similar to our approach to studying bit-level scaling. The main difference is that we vary the bit-width of models and study small batch sizes that are common for consumers and small organizations."], "score": 0.6298828125}, {"id": "(Abnar et al., 2025)", "paper": {"corpus_id": 275789885, "title": "Parameters vs FLOPs: Scaling Laws for Optimal Sparsity for Mixture-of-Experts Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Samira Abnar", "authorId": "2328415570"}, {"name": "Harshay Shah", "authorId": "2341539581"}, {"name": "Dan Busbridge", "authorId": "46254693"}, {"name": "Alaaeldin Mohamed Elnouby Ali", "authorId": "2341667138"}, {"name": "Josh Susskind", "authorId": "2243336902"}, {"name": "Vimal Thilak", "authorId": "3042871"}], "n_citations": 10}, "snippets": ["For most downstream tasks, models with similar pretraining perplexity have similar downstream task performance regardless of sparsity. For reading comprehension tasks (e.g., CoQA (Reddy et al., 2018), SQuAD (Rajpurkar et al., 2018)), denser models perform better, potentially due to their higher inference-time compute than a perplexity-matched sparse model. Strategies to increase inference time compute dynamically (Wei et al., 2022)(Goyal et al., 2023) may address this gap."], "score": 0.65966796875}, {"id": "(Rajpurkar et al., 2018)", "paper": {"corpus_id": 47018994, "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pranav Rajpurkar", "authorId": "2706258"}, {"name": "Robin Jia", "authorId": "3422908"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 2854}, "snippets": ["Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."], "score": 0.0}, {"id": "(Reddy et al., 2018)", "paper": {"corpus_id": 52055325, "title": "CoQA: A Conversational Question Answering Challenge", "year": 2018, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Siva Reddy", "authorId": "145732771"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 1212}, "snippets": ["Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4%, which is 23.4 points behind human performance (88.8%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa."], "score": 0.0}, {"id": "(Goyal et al., 2023)", "paper": {"corpus_id": 263608983, "title": "Think before you speak: Training Language Models With Pause Tokens", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Sachin Goyal", "authorId": "2253470854"}, {"name": "Ziwei Ji", "authorId": "2253445320"}, {"name": "A. Rawat", "authorId": "2241094"}, {"name": "A. Menon", "authorId": "2844480"}, {"name": "Sanjiv Kumar", "authorId": "2254172047"}, {"name": "Vaishnavh Nagarajan", "authorId": "34602162"}], "n_citations": 122}, "snippets": ["Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that inference-time delays show gains when the model is both pre-trained and finetuned with delays. For the 1B model, we witness gains on 8 of 9 tasks, most prominently, a gain of $18\\%$ EM score on the QA task of SQuAD, $8\\%$ on CommonSenseQA and $1\\%$ accuracy on the reasoning task of GSM8k. Our work raises a range of conceptual and practical future research questions on making delayed next-token prediction a widely applicable new paradigm."], "score": 0.0}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 278636433, "title": "Parallel Scaling Law for Language Models", "year": 2025, "venue": "", "authors": [{"name": "Mouxiang Chen", "authorId": "2125101083"}, {"name": "Binyuan Hui", "authorId": "2321578848"}, {"name": "Zeyu Cui", "authorId": "2248072386"}, {"name": "Jiaxin Yang", "authorId": "2328943044"}, {"name": "Dayiheng Liu", "authorId": "2248487202"}, {"name": "Jianling Sun", "authorId": "2362357192"}, {"name": "Junyang Lin", "authorId": "2326803484"}, {"name": "Zhongxin Liu", "authorId": "2361700209"}], "n_citations": 2}, "snippets": ["Inference-Time Scaling: Enhancing the reasoning ability through large-scale reinforcement learning (RL) to scale reasoning tokens during inference. Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2025;Snell et al., 2025), but most rely on specific scenarios and datasets."], "score": 0.8515625}], "table": null}, {"title": "Comparison: Inference-Time vs. Traditional Training Scaling Laws", "tldr": "Traditional training scaling laws focus on how performance improves with increased model size, data, and training compute, typically following power-law relationships. In contrast, inference-time scaling laws examine how performance scales with computation during deployment, often favoring smaller models with optimized inference strategies over simply increasing parameter counts. (6 sources)", "text": "\nTraditional scaling laws, as established by seminal works like Kaplan et al. (2020), primarily focus on how model performance improves with increased model size, dataset size, and training compute. These laws typically follow power-law relationships when assuming infinite training resources. However, this assumption \"cannot be fulfilled in real-world, thus the power-law may not be accurate to portrait the temporal behaviors of LLM performance during pre-training\" <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. Traditional scaling laws have driven the trend toward larger models with more parameters to achieve better performance, often without explicitly considering inference costs.\n\nInference-time scaling laws represent a fundamental shift in how we conceptualize model optimization. Unlike traditional training scaling laws that focus on model size, training tokens, and training compute, \"inference scaling laws examine the relationship between model performance and the amount of computation used during inference\" <Paper corpusId=\"278367792\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>. This distinction is particularly important for decoder-only language models where generation quality depends heavily on the computational resources allocated during token generation.\n\nA key difference between these paradigms is their impact on model size optimization. Traditional scaling laws typically push toward larger models, while inference-aware scaling laws may favor smaller models with optimized inference strategies. Research has shown that when \"accounting for the expected inference cost of the model,\" optimal configurations naturally \"skew toward smaller models\" <Paper corpusId=\"270764838\" paperTitle=\"(Porian et al., 2024)\" isShortName></Paper>. This suggests that the performance advantages of larger models may not always justify their increased computational costs during deployment.\n\nThe shift toward inference-aware scaling recognizes that \"using larger models inherently translates to increased computational costs during inference,\" raising a critical question: \"is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference?\" <Paper corpusId=\"271600832\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This consideration is relevant even when following approaches like LLAMA that prioritize data usage over computational efficiency during training.\n\nRecent research has begun exploring how smaller models\u2014when paired with compute-efficient inference strategies\u2014can sometimes outperform larger ones. Sardana et al. proposed methods where \"smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases\" <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. Similarly, Snell et al. investigated \"strategies for optimizing compute specifically at test time\" <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. These approaches recognize that the traditional focus on training scaling alone provides an incomplete picture of model efficiency.\n\nLooking forward, research suggests that scaling laws should comprehensively \"incorporate test-time computation\" since \"compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones\" <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. The future of AI scaling appears to be moving toward balancing training versus inference compute costs to develop models that perform efficiently in real-world applications rather than simply pursuing ever-larger parameter counts.", "citations": [{"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 269449894, "title": "Temporal Scaling Law for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizhe Xiong", "authorId": "2249971338"}, {"name": "Xiansheng Chen", "authorId": "2298904872"}, {"name": "Xin Ye", "authorId": "2299108794"}, {"name": "Hui Chen", "authorId": "2298921971"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Haoran Lian", "authorId": "2298903058"}, {"name": "Jianwei Niu", "authorId": "2293626051"}, {"name": "Guiguang Ding", "authorId": "2242661989"}], "n_citations": 10}, "snippets": ["Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."], "score": 0.68359375}, {"id": "(Zeng et al., 2025)", "paper": {"corpus_id": 278367792, "title": "LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xinyue Zeng", "authorId": "2359897638"}, {"name": "Haohui Wang", "authorId": "2155587513"}, {"name": "Junhong Lin", "authorId": "2311427666"}, {"name": "Jun Wu", "authorId": "2359758871"}, {"name": "Tyler Cody", "authorId": "2359450667"}, {"name": "Dawei Zhou", "authorId": "2313576252"}], "n_citations": 0}, "snippets": ["Recent research in scaling laws, exemplified by the work of Hoffmann et al. (2022), has introduced \"inference-time\" scaling laws that focus on how model performance scales with inference computation. Unlike traditional training scaling laws that consider model size, training tokens, and training compute, inference scaling laws examine the relationship between model performance and the amount of computation used during inference. This distinction is important for several reasons. First, inference-time scaling laws are particularly relevant for decoder-only language models where generation quality depends on the number of tokens generated. Second, while training scaling laws have guided the development of increasingly large models, inference scaling laws help optimize deployment strategies for these models, balancing performance against operational costs."], "score": 0.75732421875}, {"id": "(Porian et al., 2024)", "paper": {"corpus_id": 270764838, "title": "Resolving Discrepancies in Compute-Optimal Scaling of Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tomer Porian", "authorId": "2308470091"}, {"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "J. Jitsev", "authorId": "2191688"}, {"name": "Ludwig Schmidt", "authorId": "2253541812"}, {"name": "Y. Carmon", "authorId": "2444742"}], "n_citations": 26}, "snippets": ["Sardana and Frankle [46] account for the expected inference cost of the model, showing that it naturally skews optimal settings toward smaller models."], "score": 0.71484375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271600832, "title": "Are Bigger Encoders Always Better in Vision Large Models?", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bozhou Li", "authorId": "2303441380"}, {"name": "Hao Liang", "authorId": "2303856806"}, {"name": "Zimo Meng", "authorId": "2314372736"}, {"name": "Wentao Zhang", "authorId": "2309265357"}], "n_citations": 3}, "snippets": ["In the process of deep learning development, there has been a tendency to train larger models to achieve better By combining this estimation with scaling laws, we can determine the optimal allocation of training data and model parameters within limited computational resources Hoffmann et al. [2022]. This approach is particularly useful for LLMs since training them requires a significant amount of computational resources. Even if we embrace the philosophy underlying LLAMA Touvron et al. [2023], which entails setting aside computational expenses during the training phase and maximizing data usage to augment model performance at inference time, we consequently face an additional consideration: using larger models inherently translates to increased computational costs during inference. Therefore, even under conditions of ample data availability, a critical question arises: is the performance boost from expanding model parameter counts justified by the resultant higher computational overhead at inference? Under this assumption, scaling laws are still necessary."], "score": 0.61865234375}, {"id": "(Lu, 2025)", "paper": {"corpus_id": 275336968, "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chien-Ping Lu", "authorId": "2338865687"}], "n_citations": 1}, "snippets": ["Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."], "score": 0.865234375}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 276421468, "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications."], "score": 0.8544921875}], "table": null}, {"title": "Future Directions and Research Gaps", "tldr": "While current research has established fundamental inference-time scaling patterns, significant gaps remain in understanding how these laws apply across different model types, tasks, and inference strategies. Future research should focus on balancing training and inference compute costs, extending inference-time scaling to diverse applications, and developing comprehensive frameworks that incorporate both training and inference-time considerations. (3 sources)", "text": "\nCurrent research on inference-time scaling laws reveals promising directions but also highlights several critical gaps that require further investigation. One significant research gap is the limited understanding of inference-time scaling behaviors across diverse model architectures and applications. While studies have demonstrated the effectiveness of inference-time scaling in language models, its existence and patterns in other contexts, such as different model types and cross-modal generation, remain under-explored <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Extending research to these areas would provide a more comprehensive understanding of inference-time scaling laws.\n\nAnother important direction involves developing more sophisticated inference-time computation strategies. Current research mainly focuses on basic prompting techniques like chain-of-thought, self-consistency, or simple agent scaffolding. Expanding these approaches to include more intensive inference-time computation scenarios and post-training setups, including fine-tuning, represents a valuable research direction <Paper corpusId=\"269899695\" paperTitle=\"(Ruan et al., 2024)\" isShortName></Paper>. Advanced inference strategies such as iterative refinement, tree search, and retrieval-based augmentation hold significant potential for allowing smaller models to outperform larger ones in specific tasks <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>.\n\nThe optimization of resource allocation between training and inference phases represents a crucial area for future research. Current scaling laws often consider training and inference separately, but developing integrated frameworks that balance these costs would enable more efficient model deployment in real-world applications <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This approach recognizes that the most effective AI systems may not simply be those with the largest parameter counts, but rather those that optimize compute efficiency across their entire lifecycle.\n\nAdditionally, research should focus on dynamic inference-time compute allocation strategies. While some studies have demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance and that dynamic adjustments in sample allocation can maximize efficiency under compute constraints, these approaches need further development and validation across a wider range of applications <Paper corpusId=\"276580569\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. The ability to dynamically adjust inference resources based on task difficulty or importance could significantly improve the efficiency of deployed AI systems.\n\nFinally, future research should work toward developing unified scaling frameworks that incorporate both training and inference considerations. These comprehensive frameworks would enable researchers and practitioners to make more informed decisions about model architecture, size, and deployment strategies based on their specific application requirements and resource constraints <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. Such integrated approaches would help the field move beyond the current focus on parameter count as the primary metric for model capability and toward more nuanced measures of AI system effectiveness.", "citations": [{"id": "(Li et al., 2025)", "paper": {"corpus_id": 276580569, "title": "METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Bingxuan Li", "authorId": "2319420762"}, {"name": "Yiwei Wang", "authorId": "2280103482"}, {"name": "Jiuxiang Gu", "authorId": "2343700013"}, {"name": "Kai-Wei Chang", "authorId": "2257127887"}, {"name": "Nanyun Peng", "authorId": "2256996328"}], "n_citations": 5}, "snippets": ["Recent research has explored test-time scaling law for language model inference. For example, Wu et al. (2024b) empirically demonstrated that optimizing test-time compute allocation can significantly enhance problem-solving performance, while Zhang et al. (2024) and Snell et al. (2024) highlighted that dynamic adjustments in sample allocation can maximize efficiency under compute constraints. Although these studies collectively underscore the promise of test-time scaling for enhancing reasoning performance of LLMs, its existence in other contexts, such as different model types and application to cross-modal generation, remains under-explored."], "score": 0.78466796875}, {"id": "(Ruan et al., 2024)", "paper": {"corpus_id": 269899695, "title": "Observational Scaling Laws and the Predictability of Language Model Performance", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yangjun Ruan", "authorId": "82939895"}, {"name": "Chris J. Maddison", "authorId": "2772217"}, {"name": "Tatsunori B. Hashimoto", "authorId": "2302156937"}], "n_citations": 62}, "snippets": ["Secondly, our study mostly focuses on the scaling behavior of model capabilities measured through few-shot prompting or basic prompting techniques (such as CoT, self-consistency, or simple agent scaffolding). Extending our approach to other post-training setups, including scenarios involving fine-tuning or more intensive inference-time computation [12]81], would be valuable."], "score": 0.68212890625}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 276421468, "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["Inference-aware scaling: Scaling laws should incorporate test-time computation, as compute-efficient inference strategies (e.g., iterative refinement, tree search, retrieval-based augmentation) can allow smaller models to outperform larger ones. Future research should focus on balancing training vs. inference compute costs to develop models that scale efficiently in real-world applications."], "score": 0.8544921875}], "table": null}], "cost": 0.359451}}
