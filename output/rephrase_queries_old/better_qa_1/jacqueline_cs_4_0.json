{"better_query": "What are the most effective techniques for measuring the difficulty of datapoints when constructing a curriculum for training large language models, and how do these techniques influence training outcomes?", "better_answer": {"sections": [{"title": "Background and Introduction to Curriculum Learning", "tldr": "Curriculum learning is an approach that organizes training examples from easy to difficult, mimicking human learning processes. This strategy aims to improve model convergence, generalization, and training efficiency by gradually exposing models to increasingly complex data. (LLM Memory)", "text": "\nCurriculum learning in machine learning, particularly for training large language models (LLMs), is inspired by the way humans learn complex tasks - starting with simple concepts before progressing to more difficult ones. This approach was formally introduced to machine learning by Bengio et al. in 2009, who demonstrated that organizing training examples in increasing order of difficulty could lead to better generalization and faster convergence. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn the context of LLMs, curriculum learning involves strategically ordering training data based on difficulty measures rather than the standard random sampling approach. The core principle is to initially expose the model to \"easy\" examples that establish foundational knowledge, then gradually introduce more complex examples that build upon this foundation. This progressive training regime helps the model develop more robust representations and avoid getting stuck in poor local optima during optimization. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nCurriculum learning offers several potential benefits for LLM training: improved final performance, faster convergence rates, enhanced generalization to unseen data, and better handling of noisy or adversarial examples. The effectiveness of a curriculum critically depends on how accurately we can measure the difficulty of training examples, which is precisely what makes this an active and challenging research area. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFor implementation, curriculum learning can follow either a discrete schedule with distinct phases of increasing difficulty, or a continuous pacing function that gradually introduces harder examples. Some approaches also incorporate competence-based progression, where the model only advances to harder examples after demonstrating mastery of simpler concepts. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Types of Difficulty Measurement Techniques", "tldr": "Difficulty measurement techniques for curriculum learning can be broadly categorized into predefined/static approaches and dynamic/adaptive approaches. These techniques form the foundation of curriculum design, determining how training examples are ordered from easy to difficult. (5 sources)", "text": "\nCurriculum learning requires two fundamental components: a method to measure the difficulty of training examples and a strategy to schedule these examples during training <Paper corpusId=\"265506572\" paperTitle=\"(Chobey et al., 2023)\" isShortName></Paper> <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. The approaches for measuring difficulty can be classified into several categories, reflecting different methodologies for determining which examples are \"easy\" or \"hard\" for a model to learn.\n\nPredefined or static difficulty measures establish the curriculum before training begins and maintain this fixed ordering throughout the training process. These include metadata-based approaches, which leverage inherent characteristics of the data (such as utterance duration or signal-to-noise ratio) to estimate difficulty <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>. Such predefined curricula were dominant in early curriculum learning research but lack flexibility for broad applications <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nAnother static approach is transfer learning-based difficulty measurement, which relies on an external, pre-trained \"teacher\" model to evaluate training examples. The assumption is that examples which cause more errors in the teacher model are likely more difficult <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>. These teacher models provide curricula to guide the learning process of student models <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nIn contrast, dynamic or adaptive approaches adjust the curriculum during training based on feedback from the model itself. One notable example is self-paced learning (SPL), introduced by Kumar et al. in 2010, which automatically orders data according to training loss <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This approach recognizes that the perceived difficulty of examples changes as training progresses and dynamically adjusts the curriculum to match the model's current capabilities <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>.\n\nMore recent innovations in adaptive curriculum learning include reinforcement learning methods, where teacher models adapt the curriculum based on student model feedback, as well as approaches based on Bayesian optimization, meta-learning, and adversarial learning <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These automatic curriculum learning techniques aim to overcome the limitations of predefined approaches by creating more flexible and generalizable difficulty measurements.\n\nThe choice of difficulty measurement technique significantly influences how a curriculum is constructed and ultimately affects the training dynamics and performance of the resulting model. The increasing sophistication of these techniques reflects the growing understanding of how to effectively structure the learning process for large language models.", "citations": [{"id": "(Chobey et al., 2023)", "paper": {"corpus_id": 265506572, "title": "Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?", "year": 2023, "venue": "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning", "authors": [{"name": "Aryaman Chobey", "authorId": "2268760204"}, {"name": "Oliver Smith", "authorId": "2268760018"}, {"name": "Anzi Wang", "authorId": "2268796061"}, {"name": "Grusha Prasad", "authorId": "2268760229"}], "n_citations": 5}, "snippets": ["There are two steps involved in designing a curriculum: assigning a difficulty score to every training example (\"difficulty measurer\") and using these difficulty scores to determine the order in which training examples are presented to the model (\"training scheduler\") (Wang et al., 2021)."], "score": 0.91162109375}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 232362223, "title": "A Survey on Curriculum Learning", "year": 2021, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Xin Wang", "authorId": "2153687490"}, {"name": "Yudong Chen", "authorId": "51310474"}, {"name": "Wenwu Zhu", "authorId": "145583986"}], "n_citations": 611}, "snippets": ["Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."], "score": 0.0}, {"id": "(Karakasidis et al., 2022)", "paper": {"corpus_id": 251493126, "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR", "year": 2022, "venue": "Interspeech", "authors": [{"name": "Georgios Karakasidis", "authorId": "2181120338"}, {"name": "Tam'as Gr'osz", "authorId": "2066645527"}, {"name": "M. Kurimo", "authorId": "1719346"}], "n_citations": 2}, "snippets": ["To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."], "score": 0.93603515625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269762685, "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Xin Wang", "authorId": "2256599610"}, {"name": "Yuwei Zhou", "authorId": "2261888564"}, {"name": "Hong Chen", "authorId": "2191043236"}, {"name": "Wenwu Zhu", "authorId": "2156154955"}], "n_citations": 4}, "snippets": ["In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning."], "score": 0.85546875}, {"id": "(Hacohen et al., 2019)", "paper": {"corpus_id": 102350936, "title": "On The Power of Curriculum Learning in Training Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}], "n_citations": 449}, "snippets": ["Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."], "score": 0.0}], "table": null}, {"title": "Predefined/Static Difficulty Metrics", "tldr": "Predefined difficulty metrics establish fixed criteria for measuring training example complexity before model training begins. These static metrics include text length, word rarity, linguistic features, domain-specific properties, and signal quality measures that enable systematic curriculum organization. (17 sources)", "text": "\nPredefined or static difficulty metrics establish a fixed curriculum ordering before training begins, providing a straightforward approach to implementing curriculum learning for large language models. These metrics can be categorized based on their underlying methodology:\n\n## Text-Based Features\n- **Length-based metrics**: Sentence or utterance length is one of the most widely used simple difficulty metrics, where shorter sentences are typically considered easier <Paper corpusId=\"26468344\" paperTitle=\"(Kocmi et al., 2017)\" isShortName></Paper> <Paper corpusId=\"166228313\" paperTitle=\"(Tay et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233433844\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. This approach is particularly common in machine translation tasks.\n- **Word frequency and rarity**: Examples containing more common words are considered easier than those with rare words <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This metric assumes that learning from common vocabulary first provides a better foundation.\n- **N-gram size**: Larger n-grams typically represent more complex linguistic patterns <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>.\n\n## Linguistic Complexity\n- **Readability scores**: Standard readability indices measure text complexity based on sentence structure and vocabulary <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>.\n- **Grammatical error count**: The number of grammatical errors in a text can serve as a difficulty metric, with more errors indicating higher complexity <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>.\n- **Linguistic complexity indices**: Weighted combinations of various linguistic features can provide customized difficulty measurements tailored to specific models and tasks <Paper corpusId=\"264819795\" paperTitle=\"(Elgaar et al., 2023)\" isShortName></Paper>.\n\n## Domain-Specific Features\n- **Visual complexity**: In multimodal learning, examples with fewer and larger objects are considered easier <Paper corpusId=\"6954583\" paperTitle=\"(Shi et al., 2016)\" isShortName></Paper>.\n- **Signal quality**: For speech processing, signal-to-noise ratio (SNR) serves as a difficulty measure, where cleaner signals are easier to learn from <Paper corpusId=\"14928979\" paperTitle=\"(Braun et al., 2016)\" isShortName></Paper> <Paper corpusId=\"231979234\" paperTitle=\"(Higuchi et al., 2021)\" isShortName></Paper>.\n- **Content comprehensibility**: Metrics based on how easy it is to understand the content, considering factors beyond pure linguistic features <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n\n## Composite Approaches\n- **Multi-factor scoring**: Combining length, rarity, and comprehensibility into a single difficulty score can provide more nuanced curriculum organization <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n- **Cross-review methods**: Using multiple evaluation criteria to create a more robust difficulty assessment <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>.\n\nThe primary limitation of predefined metrics is their static nature and potential disconnect from how models actually perceive difficulty <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. These metrics are often highly task-specific and may not generalize well across different domains <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper>. Additionally, traditional linguistics-inspired metrics may not effectively reflect the actual challenges in specific learning objectives like masked language modeling <Paper corpusId=\"254685579\" paperTitle=\"(Lee et al., 2022)\" isShortName></Paper>.\n\nDespite these limitations, predefined metrics remain popular due to their simplicity and interpretability, requiring no model feedback during curriculum construction <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. They provide a straightforward starting point for implementing curriculum learning in large language model training.", "citations": [{"id": "(Kocmi et al., 2017)", "paper": {"corpus_id": 26468344, "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation", "year": 2017, "venue": "Recent Advances in Natural Language Processing", "authors": [{"name": "Tom Kocmi", "authorId": "3452584"}, {"name": "Ondrej Bojar", "authorId": "143832874"}], "n_citations": 141}, "snippets": ["We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \"curriculum learning\"). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \"curricula\" achieve a small improvement over the baseline."], "score": 0.0}, {"id": "(Tay et al., 2019)", "paper": {"corpus_id": 166228313, "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Shuohang Wang", "authorId": "2992833"}, {"name": "Anh Tuan Luu", "authorId": "26336902"}, {"name": "Jie Fu", "authorId": "49252800"}, {"name": "Minh C. Phan", "authorId": "22209930"}, {"name": "Xingdi Yuan", "authorId": "2854297"}, {"name": "J. Rao", "authorId": "30586030"}, {"name": "S. Hui", "authorId": "144194328"}, {"name": "Aston Zhang", "authorId": "2085709"}], "n_citations": 110}, "snippets": ["This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components."], "score": 0.0}, {"id": "(Zhang et al., 2021)", "paper": {"corpus_id": 233433844, "title": "Reducing BERT Computation by Padding Removal and Curriculum Learning", "year": 2021, "venue": "IEEE International Symposium on Performance Analysis of Systems and Software", "authors": [{"name": "Wei Zhang", "authorId": "144142217"}, {"name": "Wei Wei", "authorId": "2149192554"}, {"name": "Wen Wang", "authorId": "2108908267"}, {"name": "Lingling Jin", "authorId": "2152164167"}, {"name": "Zheng Cao", "authorId": "144332880"}], "n_citations": 19}, "snippets": ["BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time."], "score": 0.0}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 51606954, "title": "Curriculum Learning for Natural Answer Generation", "year": 2018, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Cao Liu", "authorId": "49046540"}, {"name": "Shizhu He", "authorId": "1954845"}, {"name": "Kang Liu", "authorId": "2200096"}, {"name": "Jun Zhao", "authorId": "1390572170"}], "n_citations": 85}, "snippets": ["By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 269741199, "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Joonho Lee", "authorId": "2116659852"}, {"name": "Jae Oh Woo", "authorId": "2301016207"}, {"name": "Juree Seok", "authorId": "2301014929"}, {"name": "Parisa Hassanzadeh", "authorId": "2301015562"}, {"name": "Wooseok Jang", "authorId": "2301015224"}, {"name": "JuYoun Son", "authorId": "2301016428"}, {"name": "Sima Didari", "authorId": "91748824"}, {"name": "Baruch Gutow", "authorId": "2301014892"}, {"name": "Heng Hao", "authorId": "2065513368"}, {"name": "Hankyu Moon", "authorId": "2301015932"}, {"name": "Wenjun Hu", "authorId": "2301166595"}, {"name": "Yeong-Dae Kwon", "authorId": "2301413463"}, {"name": "Taehee Lee", "authorId": "2301133161"}, {"name": "Seungjai Min", "authorId": "2301015935"}], "n_citations": 2}, "snippets": ["Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)", "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks", "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."], "score": 0.83544921875}, {"id": "(Graves et al., 2017)", "paper": {"corpus_id": 11137059, "title": "Automated Curriculum Learning for Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alex Graves", "authorId": "1753223"}, {"name": "Marc G. Bellemare", "authorId": "1792298"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "R. Munos", "authorId": "1708654"}, {"name": "K. Kavukcuoglu", "authorId": "2645384"}], "n_citations": 530}, "snippets": ["We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level."], "score": 0.0}, {"id": "(Zeng et al., 2023)", "paper": {"corpus_id": 259593617, "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zijie Zeng", "authorId": "8647899"}, {"name": "D. Ga\u0161evi\u0107", "authorId": "65953975"}, {"name": "Guangliang Chen", "authorId": "49901492"}], "n_citations": 6}, "snippets": ["Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms."], "score": 0.826171875}, {"id": "(Elgaar et al., 2023)", "paper": {"corpus_id": 264819795, "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 2}, "snippets": ["We assume there exists a subset of linguistic complexity indices that are most influential to learning an NLP task by a particular model. To identify these indices for each model and NLP task, we derive a weight factor \u03c1 i \u2208 [\u22121, 1] for each linguistic index that quantifies how well the index estimates the true difficulty of data samples to the model, determined by model instantaneous loss against validation data. By learning these weight factors, we obtain precise estimations that shed light on the core linguistic complexity indices that each model needs at different stages of its training to learn an NLP task. In addition, these estimates can be readily used for linguistic curriculum development, e.g., by training models with linguistically easy samples (with respect to the model) and gradually introducing linguistically challenging samples."], "score": 0.93115234375}, {"id": "(Shi et al., 2016)", "paper": {"corpus_id": 6954583, "title": "Weakly Supervised Object Localization Using Size Estimates", "year": 2016, "venue": "European Conference on Computer Vision", "authors": [{"name": "Miaojing Shi", "authorId": "8003001"}, {"name": "V. Ferrari", "authorId": "143865718"}], "n_citations": 81}, "snippets": ["We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques."], "score": 0.0}, {"id": "(Braun et al., 2016)", "paper": {"corpus_id": 14928979, "title": "A curriculum learning method for improved noise robustness in automatic speech recognition", "year": 2016, "venue": "European Signal Processing Conference", "authors": [{"name": "Stefan Braun", "authorId": "143651538"}, {"name": "Daniel Neil", "authorId": "145243593"}, {"name": "Shih-Chii Liu", "authorId": "1704961"}], "n_citations": 84}, "snippets": ["The performance of automatic speech recognition systems under noisy environments still leaves room for improvement. Speech enhancement or feature enhancement techniques for increasing noise robustness of these systems usually add components to the recognition system that need careful optimization. In this work, we propose the use of a relatively simple curriculum training strategy called accordion annealing (ACCAN). It uses a multi-stage training schedule where samples at signal-to-noise ratio (SNR) values as low as 0dB are first added and samples at increasing higher SNR values are gradually added up to an SNR value of 50dB. We also use a method called per-epoch noise mixing (PEM) that generates noisy training samples online during training and thus enables dynamically changing the SNR of our training data. Both the ACCAN and the PEM methods are evaluated on a end-to-end speech recognition pipeline on the Wall Street Journal corpus. ACCAN decreases the average word error rate (WER) on the 20dB to \u221210dB SNR range by up to 31.4% when compared to a conventional multi-condition training method."], "score": 0.0}, {"id": "(Higuchi et al., 2021)", "paper": {"corpus_id": 231979234, "title": "Dynamic Curriculum Learning via Data Parameters for Noise Robust Keyword Spotting", "year": 2021, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "T. Higuchi", "authorId": "47159622"}, {"name": "S. Saxena", "authorId": "46708564"}, {"name": "M. Souden", "authorId": "1786479"}, {"name": "Tien Dung Tran", "authorId": "2428613"}, {"name": "Masood Delfarah", "authorId": "8749296"}, {"name": "C. Dhir", "authorId": "1839173"}], "n_citations": 8}, "snippets": ["We propose dynamic curriculum learning via data parameters for noise robust keyword spotting. Data parameter learning has recently been introduced for image processing, where weight parameters, so-called data parameters, for target classes and instances are introduced and optimized along with model parameters. The data parameters scale logits and control importance over classes and instances during training, which enables automatic curriculum learning without additional annotations for training data. Similarly, in this paper, we propose using this curriculum learning approach for acoustic modeling, and train an acoustic model on clean and noisy utterances with the data parameters. The proposed approach automatically learns the difficulty of the classes and instances, e.g. due to low speech to noise ratio (SNR), in the gradient descent optimization and performs curriculum learning. This curriculum learning leads to overall improvement of the accuracy of the acoustic model. We evaluate the effectiveness of the proposed approach on a keyword spotting task. Experimental results show 7.7% relative reduction in false reject ratio with the data parameters compared to a base-line model which is simply trained on the multiconditioned dataset."], "score": 0.0}, {"id": "(Ranaldi et al., 2023)", "paper": {"corpus_id": 265068175, "title": "Modeling Easiness for Training Transformers with Curriculum Learning", "year": 2023, "venue": "Recent Advances in Natural Language Processing", "authors": [{"name": "Leonardo Ranaldi", "authorId": "2008183566"}, {"name": "Giulia Pucci", "authorId": "2199247500"}, {"name": "F. M. Zanzotto", "authorId": "103839825"}], "n_citations": 13}, "snippets": ["Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."], "score": 0.0}, {"id": "(Zhang et al._1, 2021)", "paper": {"corpus_id": 243766208, "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding", "year": 2021, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "authors": [{"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 14}, "snippets": ["With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."], "score": 0.0}, {"id": "(Xu et al., 2020)", "paper": {"corpus_id": 220045816, "title": "Curriculum Learning for Natural Language Understanding", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Hongtao Xie", "authorId": "143994657"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 206}, "snippets": ["The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model."], "score": 0.9140625}, {"id": "(Jia et al., 2022)", "paper": {"corpus_id": 253734400, "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Qi Jia", "authorId": "2056108122"}, {"name": "Yizhu Liu", "authorId": "5826956"}, {"name": "Haifeng Tang", "authorId": "2112389755"}, {"name": "Kenny Q. Zhu", "authorId": "1796651"}], "n_citations": 4}, "snippets": ["Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks."], "score": 0.892578125}, {"id": "(Lee et al., 2022)", "paper": {"corpus_id": 254685579, "title": "Efficient Pre-training of Masked Language Model via Concept-based Curriculum Masking", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mingyu Lee", "authorId": "2165377996"}, {"name": "Jun-Hyung Park", "authorId": "8289290"}, {"name": "Junho Kim", "authorId": "2145466424"}, {"name": "Kang-Min Kim", "authorId": "14694269"}, {"name": "SangKeun Lee", "authorId": "66593705"}], "n_citations": 12}, "snippets": ["However, we argue that existing linguisticsinspired criteria, such as length, rarity, and masking ratio of a sequence, do not effectively reflect the nature of MLM, as verified empirically in Table 1.\n\nThe difficulty associated with MLM is significantly affected by the choice of tokens to be masked in the given sequence, rather than by the given sequence itself. For example, given \"The man is a Stanford <mask> student\", we can easily predict that the masked token would be University, whereas given \"The man is a <mask> University student\", it would be relatively difficult to predict the original token due to the insufficient clues in the context. Then, how can we measure the MLM difficulty? MLM can be viewed as predicting masked tokens based on other contextual tokens related to masked tokens. Therefore, if a word is related to many other words and phrases, it is likely that it has several clues in the context that make MLM easier."], "score": 0.83056640625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling."], "score": 0.962890625}], "table": null}, {"title": "Model-Based/Dynamic Difficulty Metrics", "tldr": "Dynamic difficulty metrics evolve during training by measuring how the model itself interacts with training examples. These approaches include loss-based metrics, gradient-based measurements, and adaptive techniques that continuously adjust difficulty assessments based on model performance. (18 sources)", "text": "\nModel-based or dynamic difficulty metrics represent a more sophisticated approach to curriculum learning, adapting the difficulty assessment as training progresses. Unlike static metrics, these techniques evolve based on the model's changing capabilities and performance. The main categories include:\n\n## Loss-Based Metrics\n- **Training loss**: Self-paced learning (SPL) uses the current training loss to determine example difficulty, with lower loss examples considered easier <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52902973\" paperTitle=\"(Han et al., 2018)\" isShortName></Paper>.\n- **Perplexity scores**: The perplexity of samples under a reference language model can effectively identify difficult examples for pruning or curriculum ordering <Paper corpusId=\"270199394\" paperTitle=\"(Ankner et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n- **Instruction-Following Difficulty (IFD)**: This compares model loss with and without instructional context to measure how much an instruction helps in generating responses <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\n## Gradient-Based Metrics\n- **Gradient magnitude**: The magnitude of loss gradients provides a more informative measure of example difficulty than loss alone, especially in plateau regions of the loss landscape <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n- **Gradient variance**: High variance in gradients across training iterations can indicate challenging examples <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n\n## Model Behavior Metrics\n- **Misclassification frequency**: The number of times a model incorrectly classifies an example during training can indicate difficulty <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n- **Forgettability**: Examples that are frequently forgotten over the course of training are considered more difficult <Paper corpusId=\"270215134\" paperTitle=\"(Kessler et al., 2024)\" isShortName></Paper>.\n- **Learning percentage/order**: Examples that show more learning in earlier epochs are considered easier, while those requiring more training time are deemed harder <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\n## Adaptive Curriculum Approaches\n- **Teacher-student feedback**: External \"teacher\" models evaluate training examples and create curricula for \"student\" models <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper> <Paper corpusId=\"102350936\" paperTitle=\"(Hacohen et al., 2019)\" isShortName></Paper>.\n- **Data Selection Curriculum (DSC) score**: This considers both the difficulty of enhancing model performance through a specific instance and the expected performance on that instance <Paper corpusId=\"266177072\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n- **Pass rate filtering**: Using multiple model evaluations to measure the pass rate on examples and filtering based on predefined thresholds <Paper corpusId=\"277596006\" paperTitle=\"(Bae et al., 2025)\" isShortName></Paper>.\n- **Policy-driven measurements**: These assess difficulty based on the model's real-time performance rather than static metrics <Paper corpusId=\"271855275\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n## Specialized Model-Based Metrics\n- **Representation distance**: Measuring the distance between example representations in the model's embedding space can indicate difficulty <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n- **Prediction depth**: The layer at which a k-NN classifier can successfully classify an example serves as a measure of computational difficulty <Paper corpusId=\"265456857\" paperTitle=\"(Hajimolahoseini et al., 2023)\" isShortName></Paper>.\n- **Neural scoring models**: Specialized models trained to ascertain the difficulty level of data instances based on pairwise easy-hard datasets <Paper corpusId=\"274886046\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>.\n\nDynamic difficulty metrics offer several advantages over static approaches, particularly their ability to adapt to the model's evolving capabilities during training <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>. As models improve, what constitutes a \"difficult\" example changes, and these adaptive approaches can account for such shifts <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>. However, implementing these metrics typically requires additional computational resources for monitoring model performance throughout training <Paper corpusId=\"247292373\" paperTitle=\"(Varshney et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 248810776, "title": "Exploring the Learning Difficulty of Data: Theory and Measure", "year": 2022, "venue": "ACM Transactions on Knowledge Discovery from Data", "authors": [{"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ou Wu", "authorId": "2061463125"}, {"name": "Fengguang Su", "authorId": "2165378509"}, {"name": "Yingjun Deng", "authorId": "2165450358"}], "n_citations": 6}, "snippets": ["Numerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) (Han et al., 2018)(Xu et al., 2021), the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i (Lin et al., 2017) . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. (Huang et al., 2019) designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty."], "score": 0.8095703125}, {"id": "(Han et al., 2018)", "paper": {"corpus_id": 52902973, "title": "Privacy-Preserving Stochastic Gradual Learning", "year": 2018, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Bo Han", "authorId": "2087238859"}, {"name": "I. Tsang", "authorId": "1807998"}, {"name": "Xiaokui Xiao", "authorId": "33285410"}, {"name": "Ling Chen", "authorId": "2119322767"}, {"name": "S. Fung", "authorId": "35148540"}, {"name": "C. Yu", "authorId": "23157864"}], "n_citations": 9}, "snippets": ["It is challenging for stochastic optimization to handle large-scale sensitive data safely. Duchi et al. recently proposed a private sampling strategy to solve privacy leakage in stochastic optimization. However, this strategy leads to a degeneration in robustness, since this strategy is equal to noise injection on each gradient, which adversely affects updates of the primal variable. To address this challenge, we introduce a robust stochastic optimization under the framework of local privacy, which is called Privacy-pREserving StochasTIc Gradual lEarning (PRESTIGE). PRESTIGE bridges private updates of the primal variable (by private sampling) with gradual curriculum learning (CL). The noise injection leads to similar issue from label noise, but the robust learning process of CL can combat with label noise. Thus, PRESTIGE yields \"private but robust\" updates of the primal variable on the curriculum, that is, a reordered label sequence provided by CL. In theory, we reveal the convergence rate and maximum complexity of PRESTIGE. Empirical results on six datasets show that PRESTIGE achieves a good tradeoff between privacy preservation and robustness over baselines."], "score": 0.0}, {"id": "(Ankner et al., 2024)", "paper": {"corpus_id": 270199394, "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Zachary Ankner", "authorId": "2172356226"}, {"name": "Cody Blakeney", "authorId": "73066558"}, {"name": "Kartik K. Sreenivasan", "authorId": "34824875"}, {"name": "Max Marion", "authorId": "2304322161"}, {"name": "Matthew L. Leavitt", "authorId": "2028252288"}, {"name": "Mansheej Paul", "authorId": "1690452"}], "n_citations": 34}, "snippets": ["Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."], "score": 0.90283203125}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 266166227, "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jianing Zhou", "authorId": "102489044"}, {"name": "Ziheng Zeng", "authorId": "41048608"}, {"name": "Hongyu Gong", "authorId": "2008458"}, {"name": "Suma Bhat", "authorId": "2263637139"}], "n_citations": 2}, "snippets": ["Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."], "score": 0.923828125}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 261076515, "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ming Li", "authorId": "2150655891"}, {"name": "Yong Zhang", "authorId": "2144289768"}, {"name": "Zhitao Li", "authorId": "2111336489"}, {"name": "Jiuhai Chen", "authorId": "1391200710"}, {"name": "Lichang Chen", "authorId": "2108451006"}, {"name": "Ning Cheng", "authorId": "145292435"}, {"name": "Jianzong Wang", "authorId": "66063851"}, {"name": "Tianyi Zhou", "authorId": "2213956781"}, {"name": "Jing Xiao", "authorId": "91353860"}], "n_citations": 211}, "snippets": ["Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\"."], "score": 0.8603515625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268696658, "title": "Symmetric Self-Paced Learning for Domain Generalization", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Di Zhao", "authorId": "2110600983"}, {"name": "Yun Sing Koh", "authorId": "34930533"}, {"name": "Gillian Dobbie", "authorId": "2276344048"}, {"name": "Hongsheng Hu", "authorId": "2293565669"}, {"name": "Philippe Fournier-Viger", "authorId": "2293453267"}], "n_citations": 5}, "snippets": ["To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."], "score": 0.89013671875}, {"id": "(Hajimolahoseini et al., 2023)", "paper": {"corpus_id": 265456857, "title": "SwiftLearn: A Data-Efficient Training Method of Deep Learning Models using Importance Sampling", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Habib Hajimolahoseini", "authorId": "3401413"}, {"name": "Omar Mohamed Awad", "authorId": "152432835"}, {"name": "Walid Ahmed", "authorId": "2238208401"}, {"name": "Austin Wen", "authorId": "2244623536"}, {"name": "Saina Asani", "authorId": "2265581307"}, {"name": "Mohammad Hassanpour", "authorId": "2265582207"}, {"name": "Farnoosh Javadi", "authorId": "2265581967"}, {"name": "Mehdi Ahmadi", "authorId": "2268405981"}, {"name": "Foozhan Ataiefard", "authorId": "71106411"}, {"name": "Kangling Liu", "authorId": "2265599969"}, {"name": "Yang Liu", "authorId": "2238404323"}], "n_citations": 2}, "snippets": ["Other metrics include the number of times a model misclassifies a sample and the high variance of gradients, indicating example difficulty (Dadalto et al., 2023). Prediction depth, the layer at which a k-NN classifier can successfully classify an example, can also serve as a measure of computational difficulty (Ma and Karaman, 2018).\n\nCurriculum learning methods revolve around identifying the optimal order of data samples to accelerate model convergence. Unlike dataset pruning, these methods do not remove data samples but emphasize the importance of the order in which data samples are presented during training. Curriculum learning methods define metrics to quantify the importance of each data sample, and the order of learning is determined based on these scores. These metrics can be crafted manually or generated automatically (Zhang and Pfister, 2021;de Mathelin et al., 2022). While handcrafted metrics may be task-specific, they often outperform automated metrics."], "score": 0.7900390625}, {"id": "(Kessler et al., 2024)", "paper": {"corpus_id": 270215134, "title": "SAVA: Scalable Learning-Agnostic Data Valuation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Samuel Kessler", "authorId": "2304474212"}, {"name": "Tam Le", "authorId": "2304718669"}, {"name": "Vu Nguyen", "authorId": "2304715378"}], "n_citations": 0}, "snippets": ["It has also been shown that pruning data according to how easy they are to be forgotten over the course of training -as a measure of difficulty -results in training on less data while maintaining performance (Toneva et al., 2018)."], "score": 0.80615234375}, {"id": "(Mekala et al., 2024)", "paper": {"corpus_id": 267740312, "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Alex Nguyen", "authorId": "2284673632"}, {"name": "Jingbo Shang", "authorId": "2284595153"}], "n_citations": 21}, "snippets": ["Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."], "score": 0.9521484375}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 249060677, "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Chengyu Dong", "authorId": "2113540861"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 20}, "snippets": ["Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."], "score": 0.0}, {"id": "(Karakasidis et al., 2022)", "paper": {"corpus_id": 251493126, "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR", "year": 2022, "venue": "Interspeech", "authors": [{"name": "Georgios Karakasidis", "authorId": "2181120338"}, {"name": "Tam'as Gr'osz", "authorId": "2066645527"}, {"name": "M. Kurimo", "authorId": "1719346"}], "n_citations": 2}, "snippets": ["To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."], "score": 0.93603515625}, {"id": "(Hacohen et al., 2019)", "paper": {"corpus_id": 102350936, "title": "On The Power of Curriculum Learning in Training Deep Networks", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}], "n_citations": 449}, "snippets": ["Training neural networks is traditionally done by providing a sequence of random mini-batches sampled uniformly from the entire training data. In this work, we analyze the effect of curriculum learning, which involves the non-uniform sampling of mini-batches, on the training of deep networks, and specifically CNNs trained for image recognition. To employ curriculum learning, the training algorithm must resolve 2 problems: (i) sort the training examples by difficulty; (ii) compute a series of mini-batches that exhibit an increasing level of difficulty. We address challenge (i) using two methods: transfer learning from some competitive ``teacher\" network, and bootstrapping. In our empirical evaluation, both methods show similar benefits in terms of increased learning speed and improved final performance on test data. We address challenge (ii) by investigating different pacing functions to guide the sampling. The empirical investigation includes a variety of network architectures, using images from CIFAR-10, CIFAR-100 and subsets of ImageNet. We conclude with a novel theoretical analysis of curriculum learning, where we show how it effectively modifies the optimization landscape. We then define the concept of an ideal curriculum, and show that under mild conditions it does not change the corresponding global minimum of the optimization function."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 266177072, "title": "Data Selection Curriculum for Abstractive Text Summarization", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shichao Sun", "authorId": "48904791"}, {"name": "Ruifeng Yuan", "authorId": "2273661632"}, {"name": "Jianfei He", "authorId": "2237429230"}, {"name": "Ziqiang Cao", "authorId": "2314396"}, {"name": "Wenjie Li", "authorId": "2237591981"}, {"name": "Xiaohua Jia", "authorId": "2237752880"}], "n_citations": 2}, "snippets": ["In this study, we introduce a novel scoring system called the Data Selection Curriculum (DSC) score, which serves as a measure of learning difficulty. The DSC score considers two essential factors: the difficulty of enhancing the ATS model through a specific instance and the expected performance of the model on that particular instance. Instances on which the current model already exhibits promising performance and stands to benefit significantly from slight adjustments are assigned as a lower learning difficulty. This is because these instances have the potential to contribute more to the final performance."], "score": 0.7890625}, {"id": "(Bae et al., 2025)", "paper": {"corpus_id": 277596006, "title": "Online Difficulty Filtering for Reasoning Oriented Reinforcement Learning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sanghwan Bae", "authorId": "152846184"}, {"name": "Jiwoo Hong", "authorId": "2354048562"}, {"name": "M. Lee", "authorId": "2294848361"}, {"name": "Hanbyul Kim", "authorId": "2354326988"}, {"name": "JeongYeon Nam", "authorId": "2353956752"}, {"name": "Donghyun Kwak", "authorId": "10469987"}], "n_citations": 12}, "snippets": ["Online filtering methods further explore this idea by dynamically adjusting the training data to match the current ability of the model (Cui et al., 2025). However, while previous work demonstrate the empirical effectiveness of such techniques, they often lack a detailed analysis of why or when certain difficulty distributions yield better learning outcomes. With G rollouts for each prompt x, we measure the pass rate p(x) as the average accuracy and filter them by predefined thresholds: e.g., 0.25 \u2264 p(x) \u2264 0.75 in this case."], "score": 0.921875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 271855275, "title": "P3: A Policy-Driven, Pace-Adaptive, and Diversity-Promoted Framework for Optimizing LLM Training", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yingxuan Yang", "authorId": "2224854311"}, {"name": "Huayi Wang", "authorId": "2315948077"}, {"name": "Muning Wen", "authorId": "2111875607"}, {"name": "Weinan Zhang", "authorId": "2315940339"}], "n_citations": 0}, "snippets": ["Policy-driven Difficulty Measurement: we begin by measuring the difficulty of data based on the model's real-time performance, transitioning from static, predefined metrics to more dynamic and adaptable ones."], "score": 0.94580078125}, {"id": "(Jung et al., 2024)", "paper": {"corpus_id": 274886046, "title": "Assessing and Scoring Difficulty of Hard-to-Solve Data in Summarization Tasks", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Jeesu Jung", "authorId": "2159741574"}, {"name": "H. Seo", "authorId": "2065447556"}, {"name": "Hyuk Namgoong", "authorId": "2210707252"}, {"name": "Sangkeun Jung", "authorId": "2313048573"}], "n_citations": 0}, "snippets": ["In recent years, data-driven and machine learning-based natural language processing (NLP) technologies have effectively addressed various challenges. To further enhance the performance of NLP models, it is crucial to understand the types of data that a model can handle well and those it struggles with. This study introduces a method to discern which types of data can be effectively processed by given neural network-based models and which pose difficulties. We define the criteria for hard-to-solve data, construct a pairwise easy-hard dataset, and propose a neural scoring model. This model ascertains the difficulty level of each data instance. To utilize the proposed difficulty level as an application, we employed curriculum learning. The experimental results show that our methodology can effectively distinguish between easy and hard data, and performance improves when applying the curriculum learning approach."], "score": 0.923828125}, {"id": "(Jarca et al., 2024)", "paper": {"corpus_id": 271051051, "title": "CBM: Curriculum by Masking", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Andrei Jarca", "authorId": "2310341007"}, {"name": "Florinel-Alin Croitoru", "authorId": "2154573729"}, {"name": "R. Ionescu", "authorId": "2249763264"}], "n_citations": 0}, "snippets": ["The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model."], "score": 0.83203125}, {"id": "(Varshney et al., 2022)", "paper": {"corpus_id": 247292373, "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Neeraj Varshney", "authorId": "2067056655"}, {"name": "Swaroop Mishra", "authorId": "1817207"}, {"name": "Chitta Baral", "authorId": "2064619864"}], "n_citations": 19}, "snippets": ["We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1)."], "score": 0.8203125}], "table": null}, {"title": "Implementation Strategies for Curriculum Learning", "tldr": "Implementing curriculum learning requires choosing appropriate difficulty metrics and designing effective scheduling mechanisms to gradually expose models to more complex data. Various approaches range from static predefined curricula to dynamic adaptive systems that adjust training patterns based on model feedback. (6 sources)", "text": "\nCurriculum learning implementation can be structured in several ways, depending on how training examples are scheduled and presented to the model. The fundamental components of any curriculum learning implementation include a difficulty measurement technique and a strategy for scheduling examples during training.\n\n## Curriculum Scheduling Approaches\n\n### Static Scheduling\nStatic or predefined curricula establish a fixed ordering of training examples before training begins. These include:\n\n- **Staged training**: Training proceeds through distinct phases of increasing difficulty, with each phase containing examples within a specific difficulty range <Paper corpusId=\"251493126\" paperTitle=\"(Karakasidis et al., 2022)\" isShortName></Paper>.\n- **Augmentation-based scheduling**: Starting with original data before gradually increasing the proportion of augmented (and presumably more difficult) examples <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n- **Class-based scheduling**: Organizing training by class difficulty, allowing the model to learn easier classes before progressing to more difficult ones <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n\n### Dynamic Scheduling\nDynamic curricula adjust as training progresses, based on model feedback:\n\n- **Self-paced learning (SPL)**: Automatically orders data according to training loss, enabling examples to be introduced based on the model's current capabilities <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n- **Teacher-student feedback**: Uses teacher models to provide curricula guidance to student models, with adjustments based on student performance <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n- **Progressive overload curriculum learning (POCL)**: Incrementally introduces subsets of training samples from easy to hard at fixed intervals, applying loss functions with progressively rising temperatures <Paper corpusId=\"279243528\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\n## Pacing Functions\n\nThe rate at which more difficult examples are introduced is controlled by pacing functions:\n\n- **Linear pacing**: Gradually increases the difficulty threshold in a linear fashion.\n- **Exponential pacing**: Accelerates the introduction of difficult examples as training progresses.\n- **Step-wise pacing**: Increases difficulty in discrete jumps at predetermined intervals.\n\nRecent research has challenged conventional wisdom about curriculum pacing, suggesting that the optimal curricula may not always follow monotonic patterns. Non-monotonic curricula, which intermix examples of varying difficulty rather than strictly progressing from easy to hard, have shown superior performance in some cases <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>.\n\n## Practical Considerations\n\nFor effective curriculum learning implementation, several factors should be considered:\n\n- **Distribution of difficulty**: A wider range of difficulty in training data tends to yield greater improvements from curriculum learning <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.\n- **Computational overhead**: Dynamic curricula typically require additional computational resources to monitor model performance and adjust the curriculum accordingly.\n- **Generalization**: Curricula discovered for smaller datasets and models have shown promising performance when transferred to larger datasets and models <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>.\n- **Temperature parameters**: Parameters that control the distribution of example difficulty can significantly impact curriculum effectiveness <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>.\n\nThe evolution of curriculum learning implementations has moved from rigid predefined approaches toward more flexible and general automatic methods. These include reinforcement learning-based techniques, Bayesian optimization approaches, meta-learning strategies, and adversarial learning methods <Paper corpusId=\"269762685\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. This shift reflects the growing understanding that effective curricula should adapt to the specific learning dynamics of the model being trained.", "citations": [{"id": "(Karakasidis et al., 2022)", "paper": {"corpus_id": 251493126, "title": "Comparison and Analysis of New Curriculum Criteria for End-to-End ASR", "year": 2022, "venue": "Interspeech", "authors": [{"name": "Georgios Karakasidis", "authorId": "2181120338"}, {"name": "Tam'as Gr'osz", "authorId": "2066645527"}, {"name": "M. Kurimo", "authorId": "1719346"}], "n_citations": 2}, "snippets": ["To create an efficient CL algorithm, we need a suitable scoring function, capable of estimating the difficulty of each training sample. The most common approaches for creating such functions can be grouped into the following three categories: \n\nThe metadata-based approach estimates the difficulty scores based on some meta-information (e.g. utterance duration or SNR) at the beginning of the training process. This is a static solution since the curriculum is established before training, and the order is kept fixed. \n\nThe transfer-learning approach relies on an external, already trained teacher model that is used to infer the training data before the first epoch (Hacohen et al., 2019). The assumption is that the external model should recognize the easy samples with fewer errors than the difficult ones. This method utilizes the outputs of the teacher to sort the utterances at the start of the training, so this is a static approach, as well. \n\nThe adaptive approach proposes to sort the examples adaptively using feedback from the student neural network we are training. This approach addresses the fact that the difficulty of the examples (as perceived by the model) could change as the training progresses. One can view this approach as dynamically adjusting the curriculum to the actual state of the model."], "score": 0.93603515625}, {"id": "(Wang et al._1, 2021)", "paper": {"corpus_id": 244908620, "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding", "year": 2021, "venue": "International Conference on Software Engineering", "authors": [{"name": "Deze Wang", "authorId": "1455165108"}, {"name": "Zhouyang Jia", "authorId": "10414205"}, {"name": "Shanshan Li", "authorId": "50342128"}, {"name": "Yue Yu", "authorId": null}, {"name": "Yun Xiong", "authorId": "2153657836"}, {"name": "Wei Dong", "authorId": "2114050396"}, {"name": "Xiangke Liao", "authorId": "144078016"}], "n_citations": 80}, "snippets": ["The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks."], "score": 0.8408203125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269762685, "title": "Curriculum Learning: Theories, Approaches, Applications, Tools, and Future Directions in the Era of Large Language Models", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Xin Wang", "authorId": "2256599610"}, {"name": "Yuwei Zhou", "authorId": "2261888564"}, {"name": "Hong Chen", "authorId": "2191043236"}, {"name": "Wenwu Zhu", "authorId": "2156154955"}], "n_citations": 4}, "snippets": ["In the early stages, predefined CL takes the mainstream.However, this type of predefined approach is not flexible and general enough for widespread applications.In 2010, Kumar et al. propose self-paced learning (SPL), enabling automatic curriculum scheduling by ordering data according to their training loss.Subsequently, a variety of automatic curriculum learning methods have continued to emerge.For example, transfer learning methods employ teacher models to offer student models curricula.Reinforcement learning methods allow teacher models to adapt curriculum based on the feedback from student models.In addition, there are other ones based on Bayesian optimization, meta-learning, and adversarial learning for implementing automatic curriculum learning."], "score": 0.85546875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 279243528, "title": "Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework", "year": 2025, "venue": "", "authors": [{"name": "Lingyuan Liu", "authorId": "2366142077"}, {"name": "Mengxiang Zhang", "authorId": "2366090070"}], "n_citations": 0}, "snippets": ["To address these challenges, we propose a novel, plug-in curriculum learning framework inspired by the strength training principle of\"progressive overload\"(POCL), which can be seamlessly integrated into existing white-box KD approaches with minimal computational overhead. The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures."], "score": 0.95458984375}, {"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."], "score": 0.8955078125}, {"id": "(Wei et al., 2021)", "paper": {"corpus_id": 232233485, "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jason Wei", "authorId": "144026731"}, {"name": "Chengyu Huang", "authorId": "2150608874"}, {"name": "Soroush Vosoughi", "authorId": "1918441"}, {"name": "Yu Cheng", "authorId": "2153510147"}, {"name": "Shiqi Xu", "authorId": "1845277073"}], "n_citations": 64}, "snippets": ["Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning."], "score": 0.8037109375}], "table": null}, {"title": "Impact on Training Outcomes", "tldr": "Curriculum learning strategies can significantly improve convergence speed, final model performance, and training efficiency across various NLP tasks. The specific benefits depend on the chosen difficulty metrics, the distribution of difficulty in training data, and the implementation approach. (13 sources)", "text": "\nCurriculum learning (CL) has demonstrated substantial positive impacts on training outcomes across a range of natural language processing tasks. One of the most consistently reported benefits is improved convergence speed, with studies showing that well-designed curricula can reduce training time by up to 70% <Paper corpusId=\"85498775\" paperTitle=\"(Platanios et al., 2019)\" isShortName></Paper>. This acceleration occurs because curriculum approaches help models avoid getting stuck in poor local optima early in training, leading to more efficient learning trajectories.\n\nBeyond faster convergence, curriculum learning has been shown to improve model performance on various benchmarks. For machine translation tasks, multiple studies have demonstrated that curriculum strategies contribute to both convergence speed and final model performance <Paper corpusId=\"234338422\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"155089817\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper> <Paper corpusId=\"219260306\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>. Some implementations have achieved performance improvements of up to 2.2 BLEU points <Paper corpusId=\"85498775\" paperTitle=\"(Platanios et al., 2019)\" isShortName></Paper>, while others have reported improvements of 1.17 to 1.56 BLEU points along with training speedups of 2.22x to 3.33x <Paper corpusId=\"219260306\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper>.\n\nThe magnitude of performance improvement from curriculum learning appears to be directly related to the distribution of difficulty within the training data. When training data exhibits a wider range of difficulty, the potential benefits of curriculum learning increase substantially <Paper corpusId=\"232233485\" paperTitle=\"(Wei et al., 2021)\" isShortName></Paper>. This is especially relevant for augmentation-based approaches, where temperature parameters control the perturbation level of examples and thereby affect the distribution of difficulty.\n\nNotably, recent research has challenged conventional wisdom about the optimal structure of curricula. While traditional approaches have favored monotonic progressions (either easy-to-hard or hard-to-easy), studies have found that non-monotonic curricula\u2014which intermix examples of varying difficulty\u2014often outperform strictly monotonic approaches <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This suggests that the relationship between curriculum structure and training outcomes is more complex than initially theorized.\n\nThe specific difficulty measurement technique used can significantly influence training outcomes. In automated text scoring systems, curriculum learning has shown improvements of up to 4.5%, with difficulty measures based on grammatical error counts often outperforming other metrics <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>. Additionally, curricula discovered for smaller datasets and models have demonstrated good transferability to larger datasets and models <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>, suggesting that effective curriculum strategies may be somewhat generalizable across scale.\n\nBeyond direct performance improvements, curriculum learning approaches have shown beneficial effects as regularization methods. For example, implementing a curriculum based on sequence length has been shown to reduce gradient variance, enabling training of auto-regressive models with significantly larger batch sizes and learning rates without introducing training instability <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\nMore sophisticated adaptive approaches, such as those used in knowledge distillation, can further enhance training outcomes by dynamically adjusting the emphasis on different samples based on the model's current performance <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. These methods ensure that the training process remains responsive to the evolving capabilities of the model, leading to more efficient and effective outcomes.\n\nThe benefits of curriculum learning extend beyond standard training scenarios. Research has shown that model-based difficulty metrics used in curriculum learning can also improve selective prediction ability (helping models determine when not to answer), enhance understanding of training dynamics, and enable more efficient evaluations <Paper corpusId=\"248965007\" paperTitle=\"(Varshney et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"247292373\" paperTitle=\"(Varshney et al., 2022)\" isShortName></Paper> <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>.\n\nDespite these benefits, traditional curriculum approaches based on coarse-grained categories (such as question length) or simplistic LLM-based scoring methods may not capture the nuanced reasoning demands of complex tasks <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. This highlights the need for more sophisticated, fine-grained adaptive frameworks that can effectively bridge the gap between data difficulty and model capability as training progresses.", "citations": [{"id": "(Platanios et al., 2019)", "paper": {"corpus_id": 85498775, "title": "Competence-based Curriculum Learning for Neural Machine Translation", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Emmanouil Antonios Platanios", "authorId": "144888672"}, {"name": "Otilia Stretcu", "authorId": "3397269"}, {"name": "Graham Neubig", "authorId": "1700325"}, {"name": "B. P\u00f3czos", "authorId": "1719347"}, {"name": "Tom Michael Mitchell", "authorId": "40975594"}], "n_citations": 344}, "snippets": ["Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU."], "score": 0.0}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 234338422, "title": "Self-Guided Curriculum Learning for Neural Machine Translation", "year": 2021, "venue": "International Workshop on Spoken Language Translation", "authors": [{"name": "Lei Zhou", "authorId": "2144764522"}, {"name": "Liang Ding", "authorId": "46573238"}, {"name": "Kevin Duh", "authorId": "1800354"}, {"name": "Ryohei Sasano", "authorId": "2293543"}, {"name": "Koichi Takeda", "authorId": "2874038"}], "n_citations": 17}, "snippets": ["In the scenario of neural machine translation (NMT), empirical studies have shown that CL strategy contributes to convergence speed and model performance (Zhang et al., 2018;(Platanios et al., 2019)(Zhang et al., 2019)(Liu et al., 2020)(Zhan et al., 2021)(Ruiter et al., 2020). In these approaches, the learning difficulty of each training example is measured by different difficulty criteria. Early approaches depend on prior knowledge from various sources, including manually crafted features like sentence length and word rarity (Kocmi et al., 2017). The drawback lies in the fact that human understand learning difficulty differently from NMT models. Recent works choose to derive difficulty criteria based on the probability distribution of training examples, to approximate the perspective of an NMT model. For example, (Platanios et al., 2019) turns discrete numerical difficulty scores into relative probabilities and then construct the criterion, while others derive criteria from independently pre-trained models like language model (Zhang et al., 2019;Dou et al., 2020;Liu et al., 2020) and word embedding model (Zhou et al., 2020b)."], "score": 0.83203125}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 155089817, "title": "Curriculum Learning for Domain Adaptation in Neural Machine Translation", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xuan Zhang", "authorId": "49469742"}, {"name": "Pamela Shapiro", "authorId": "38581144"}, {"name": "Manish Kumar", "authorId": "48387892"}, {"name": "Paul McNamee", "authorId": "145324163"}, {"name": "Marine Carpuat", "authorId": "2954727"}, {"name": "Kevin Duh", "authorId": "1800354"}], "n_citations": 124}, "snippets": ["We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs."], "score": 0.0}, {"id": "(Liu et al., 2020)", "paper": {"corpus_id": 219260306, "title": "Norm-Based Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xuebo Liu", "authorId": "1390611971"}, {"name": "Houtim Lai", "authorId": "1736553736"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 120}, "snippets": ["A neural machine translation (NMT) system is expensive to train, especially with high-resource settings. As the NMT architectures become deeper and wider, this issue gets worse and worse. In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method. We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence. The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties. It is easy to determine and contains learning-dependent features. The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT\u201914 English-German and WMT\u201917 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x)."], "score": 0.0}, {"id": "(Wei et al., 2021)", "paper": {"corpus_id": 232233485, "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jason Wei", "authorId": "144026731"}, {"name": "Chengyu Huang", "authorId": "2150608874"}, {"name": "Soroush Vosoughi", "authorId": "1918441"}, {"name": "Yu Cheng", "authorId": "2153510147"}, {"name": "Shiqi Xu", "authorId": "1845277073"}], "n_citations": 64}, "snippets": ["Effective curriculum learning necessitates a range of difficulty in training data. In our case, this range is controlled by augmentation temperature, a parameter that dictates how perturbed augmented examples are and therefore affects the distribution of difficulty in training examples. When the distribution of difficulty in data is larger, we should expect a greater improvement from curriculum learning."], "score": 0.8037109375}, {"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."], "score": 0.8955078125}, {"id": "(Zeng et al., 2023)", "paper": {"corpus_id": 259593617, "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zijie Zeng", "authorId": "8647899"}, {"name": "D. Ga\u0161evi\u0107", "authorId": "65953975"}, {"name": "Guangliang Chen", "authorId": "49901492"}], "n_citations": 6}, "snippets": ["Specifically, we designed two types of difficulty measurers: (i) pre-defined, calculated by measuring a sample's readability, length, the number of grammatical errors or unique words it contains; and (ii) automatic, calculated based on whether a model in a training epoch can accurately score the samples. These measurers were tested in both the easy-to-hard to hard-to-easy training paradigms. Through extensive evaluations on two widely-used datasets (one for short answer scoring and the other for long essay scoring), we demonstrated that (a) CL indeed could boost the performance of state-of-the-art ATS models, and the maximum improvement could be up to 4.5%, but most improvements were achieved when assessing short and easy answers; (b) the pre-defined measurer calculated based on the number of grammatical errors contained in a text sample tended to outperform the other difficulty measurers across different training paradigms."], "score": 0.826171875}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 269741199, "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Joonho Lee", "authorId": "2116659852"}, {"name": "Jae Oh Woo", "authorId": "2301016207"}, {"name": "Juree Seok", "authorId": "2301014929"}, {"name": "Parisa Hassanzadeh", "authorId": "2301015562"}, {"name": "Wooseok Jang", "authorId": "2301015224"}, {"name": "JuYoun Son", "authorId": "2301016428"}, {"name": "Sima Didari", "authorId": "91748824"}, {"name": "Baruch Gutow", "authorId": "2301014892"}, {"name": "Heng Hao", "authorId": "2065513368"}, {"name": "Hankyu Moon", "authorId": "2301015932"}, {"name": "Wenjun Hu", "authorId": "2301166595"}, {"name": "Yeong-Dae Kwon", "authorId": "2301413463"}, {"name": "Taehee Lee", "authorId": "2301133161"}, {"name": "Seungjai Min", "authorId": "2301015935"}], "n_citations": 2}, "snippets": ["Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)", "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence. They demonstrated that CL behaves as a regularization method and reduces the gradient variance, therefore enabling training auto-regressive models with much larger batch sizes and learning rates without training instability (for example, training GPT-2 models with 8x larger batch size and 4x larger learning rate). (Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks", "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."], "score": 0.83544921875}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277398866, "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Dingzong Zhang", "authorId": "2326763710"}, {"name": "Devi Listiyani", "authorId": "2345374431"}, {"name": "Priyanka Singh", "authorId": "2302811064"}, {"name": "Manoranjan Mohanty", "authorId": "2309099465"}], "n_citations": 0}, "snippets": ["For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. These methods ensure that the distillation process is more responsive to the learning needs of the student model, leading to more efficient and effective training outcomes."], "score": 0.9208984375}, {"id": "(Varshney et al._1, 2022)", "paper": {"corpus_id": 248965007, "title": "Let the Model Decide its Curriculum for Multitask Learning", "year": 2022, "venue": "Workshop on Deep Learning Approaches for Low-Resource Natural Language Processing", "authors": [{"name": "Neeraj Varshney", "authorId": "2067056655"}, {"name": "Swaroop Mishra", "authorId": "1817207"}, {"name": "Chitta Baral", "authorId": "2064619864"}], "n_citations": 8}, "snippets": ["In addition to arranging the training instances into a learning curriculum, computing difficulty scores using model-based techniques has shown its benefits in several other areas, such as improving selective prediction ability (Varshney et al., 2020), under-standing training dynamics (Swayamdipta et al., 2020), and efficient evaluations (Varshney et al., 2022)."], "score": 0.79638671875}, {"id": "(Varshney et al., 2022)", "paper": {"corpus_id": 247292373, "title": "ILDAE: Instance-Level Difficulty Analysis of Evaluation Data", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Neeraj Varshney", "authorId": "2067056655"}, {"name": "Swaroop Mishra", "authorId": "1817207"}, {"name": "Chitta Baral", "authorId": "2064619864"}], "n_citations": 19}, "snippets": ["We use RoBERTa-large model (Liu et al., 2019) for this procedure and train each model for E = 10 epochs, resulting in N = 120 predictions for each evaluation instance. Our difficulty computation method is general and can be used with any other model or configurations; we use RoBERTa-large as it has been shown to achieve high performance across diverse NLP tasks (Liu et al., 2019). In addition, we show that difficulty scores computed using our procedure also generalize for other models (3.5.1)."], "score": 0.8203125}, {"id": "(Swayamdipta et al., 2020)", "paper": {"corpus_id": 221856637, "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Swabha Swayamdipta", "authorId": "2705113"}, {"name": "Roy Schwartz", "authorId": "2279023325"}, {"name": "Nicholas Lourie", "authorId": "35219984"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 452}, "snippets": ["Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."], "score": 0.0}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 277824032, "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qianjin Yu", "authorId": "2356617563"}, {"name": "Keyu Wu", "authorId": "2355802308"}, {"name": "Zihan Chen", "authorId": "2355823766"}, {"name": "Chushu Zhang", "authorId": "2355786652"}, {"name": "Manlin Mei", "authorId": "2355643664"}, {"name": "Lingjun Huang", "authorId": "2356799678"}, {"name": "Fang Tan", "authorId": "2355645282"}, {"name": "Yongsheng Du", "authorId": "2355785017"}, {"name": "Kunlin Liu", "authorId": "2356331597"}, {"name": "Yurui Zhu", "authorId": "2355782385"}], "n_citations": 3}, "snippets": ["Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."], "score": 0.8232421875}], "table": null}, {"title": "Challenges and Limitations", "tldr": "Despite its effectiveness, curriculum learning faces significant challenges including difficulty defining accurate metrics, reliance on human expertise, and implementing dynamic approaches that adapt to evolving model capabilities. (8 sources)", "text": "\nImplementing effective curriculum learning for large language models presents several notable challenges and limitations. One of the fundamental difficulties lies in defining appropriate metrics for measuring example difficulty. Traditional approaches often rely on predefined heuristics such as text length or word rarity, which may not accurately reflect the true complexity of training examples from the model's perspective <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. These simplistic metrics frequently fail to capture the nuanced reasoning demands of complex tasks, limiting their effectiveness across different domains <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nMany curriculum learning approaches suffer from poor generalizability across different tasks and domains. Strategies that utilize human-crafted rules based on linguistic features or task-specific observations are highly dependent on the target dataset and often fail to transfer effectively to different applications <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. This task-specificity creates significant barriers to developing universal curriculum learning approaches.\n\nAnother critical limitation is the dependence on human expert input for designing effective curricula. Traditional curriculum strategies, while proven effective, often lack practicality due to their reliance on domain expertise that may not always be available <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper> <Paper corpusId=\"204539326\" paperTitle=\"(Jimenez-Sanchez et al., 2019)\" isShortName></Paper>. This dependency on human judgment introduces scalability challenges, especially when applying curriculum learning to novel domains or extremely large datasets.\n\nStatic curricula present another significant limitation, as they remain fixed throughout the training process without adapting to the model's evolving capabilities. Predefined curricula cannot account for the continuously changing perception of difficulty as training progresses <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>. This inflexibility may result in suboptimal training schedules that fail to align with the model's actual learning trajectory.\n\nCoarse-grained curriculum designs often oversimplify difficulty levels, such as categorizing questions merely by length, while LLM-based scoring methods struggle to capture the nuanced demands of complex reasoning tasks <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. These approaches lack the granularity needed to effectively bridge the gap between data difficulty and model capability as training progresses.\n\nEven model-based curriculum approaches face challenges in accurately measuring difficulty. Models trained on previous epochs or external data may not perfectly reflect the current model's perception of difficulty <Paper corpusId=\"253734400\" paperTitle=\"(Jia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. The dynamic nature of perceived difficulty as the model learns makes it challenging to design curricula that remain optimal throughout the training process.\n\nFor real-world applications, the computational overhead of dynamic curriculum approaches can be prohibitive. Self-paced learning and other adaptive methods require continuous assessment of model performance, which adds significant computational cost, especially for large-scale language models <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper> .\n\nCross-domain applicability represents another persistent challenge. While some researchers have proposed more generalizable approaches such as the Cross Review method for evaluating difficulty, many curriculum learning strategies remain highly specialized to particular tasks or domains <Paper corpusId=\"220045816\" paperTitle=\"(Xu et al., 2020)\" isShortName></Paper>. This specialization makes it difficult to transfer successful curriculum strategies between different applications without significant modifications.\n\nThese limitations highlight the need for more sophisticated, fine-grained adaptive frameworks that can effectively measure example difficulty from the model's perspective while maintaining practical implementation requirements. Future research directions may focus on developing more generalizable difficulty metrics and curriculum strategies that can adapt dynamically to the model's evolving capabilities across diverse tasks and domains.", "citations": [{"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling."], "score": 0.962890625}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 277824032, "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qianjin Yu", "authorId": "2356617563"}, {"name": "Keyu Wu", "authorId": "2355802308"}, {"name": "Zihan Chen", "authorId": "2355823766"}, {"name": "Chushu Zhang", "authorId": "2355786652"}, {"name": "Manlin Mei", "authorId": "2355643664"}, {"name": "Lingjun Huang", "authorId": "2356799678"}, {"name": "Fang Tan", "authorId": "2355645282"}, {"name": "Yongsheng Du", "authorId": "2355785017"}, {"name": "Kunlin Liu", "authorId": "2356331597"}, {"name": "Yurui Zhu", "authorId": "2355782385"}], "n_citations": 3}, "snippets": ["Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."], "score": 0.8232421875}, {"id": "(Jia et al., 2022)", "paper": {"corpus_id": 253734400, "title": "In-sample Curriculum Learning by Sequence Completion for Natural Language Generation", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Qi Jia", "authorId": "2056108122"}, {"name": "Yizhu Liu", "authorId": "5826956"}, {"name": "Haifeng Tang", "authorId": "2112389755"}, {"name": "Kenny Q. Zhu", "authorId": "1796651"}], "n_citations": 4}, "snippets": ["Previous work on CL for NLG focuses on measuring the difficulty of training samples in two ways. One is to resort to human-crafted rules based on various linguistic features and human observations (Liu et al., 2018)(Kocmi et al., 2017). The other uses models either trained from outside data or the same data but in previous epochs/steps (Zhou et al., 2020)Kumar et al., 2019;(Shen et al., 2020). Either way seeks to produce a numeric score for each training sample relying on domain expertise so that it can be ranked, making it difficult to generalize to different tasks."], "score": 0.892578125}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 51606954, "title": "Curriculum Learning for Natural Answer Generation", "year": 2018, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Cao Liu", "authorId": "49046540"}, {"name": "Shizhu He", "authorId": "1954845"}, {"name": "Kang Liu", "authorId": "2200096"}, {"name": "Jun Zhao", "authorId": "1390572170"}], "n_citations": 85}, "snippets": ["By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."], "score": 0.0}, {"id": "(Jarca et al., 2024)", "paper": {"corpus_id": 271051051, "title": "CBM: Curriculum by Masking", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Andrei Jarca", "authorId": "2310341007"}, {"name": "Florinel-Alin Croitoru", "authorId": "2154573729"}, {"name": "R. Ionescu", "authorId": "2249763264"}], "n_citations": 0}, "snippets": ["The aforementioned curriculum strategies have proven to be effective.However, they have been found to lack practicality due to their reliance on human expert input (Jim\u00e9nez-S\u00e1nchez et al., 2019), which may not always be available.Moreover, these methods remain fixed during training and may not adapt the curriculum to the changing needs of the models.As a result, the research community developed new curriculum-based approaches to overcome these limitations.For in-stance, Kumar et al. [32] introduced self-paced learning, a method that measures the difficulty of the training samples based on the performance of the trained model."], "score": 0.83203125}, {"id": "(Jimenez-Sanchez et al., 2019)", "paper": {"corpus_id": 204539326, "title": "Medical-based Deep Curriculum Learning for Improved Fracture Classification", "year": 2019, "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "authors": [{"name": "Amelia Jim\u00e9nez-S\u00e1nchez", "authorId": "1404288037"}, {"name": "D. Mateus", "authorId": "1704770"}, {"name": "S. Kirchhoff", "authorId": "79884102"}, {"name": "C. Kirchhoff", "authorId": "4858301"}, {"name": "P. Biberthaler", "authorId": "5140452"}, {"name": "N. Navab", "authorId": "145587209"}, {"name": "M. Ballester", "authorId": "49463953"}, {"name": "G. Piella", "authorId": "50186507"}], "n_citations": 48}, "snippets": ["Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons."], "score": 0.0}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Xu et al., 2020)", "paper": {"corpus_id": 220045816, "title": "Curriculum Learning for Natural Language Understanding", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Hongtao Xie", "authorId": "143994657"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 206}, "snippets": ["The key challenge in designing a successful CL strategy lies in how to define easy/difficult examples. One straightforward way is to simply predefine the difficulty in revised rules by observing the particular target task formation or training data structure accordingly (Guo et al., 2018;Platanios et al., 2019;Tay et al., 2019). For example, (Bengio et al., 2009) utilized an easier version of shape recognition trainset which comprised of less varied shapes, before the training of complex one started. More recently, (Tay et al., 2019) considered the paragraph length of a question answering example as its reflection of difficulty. However, such strategies are highly dependent on the target dataset itself and often fails to generalize to different tasks.\n\nTo address this challenge, we propose our Cross Review method for evaluating difficulty. Specifically, we define easy examples as those well solved by the exact model that we are to employ in the task. For different tasks, we adopt their corresponding golden metrics to calculate a difficulty score for each example in the trainset. Then based on these difficulty scores, we further design a re-arranging algorithm to construct the learning curriculum in an annealing style, which provides a soft transition from easy to difficult for the model."], "score": 0.9140625}], "table": null}, {"title": "Advanced and Recent Approaches", "tldr": "Recent innovations in curriculum learning focus on non-monotonic training sequences, gradient-based difficulty assessments, and adaptive frameworks that respond to model evolution. These approaches overcome limitations of traditional methods by providing more fine-grained, dynamic difficulty measurements that better align with actual learning trajectories. (7 sources)", "text": "\nRecent research in curriculum learning has challenged conventional wisdom and introduced several innovative approaches that address key limitations of traditional methods. One significant advancement is the discovery that optimal curricula may not always follow the strictly monotonic easy-to-hard or hard-to-easy progression that dominated earlier work. Studies have shown that non-monotonic curricula\u2014which strategically intermix examples of varying difficulty\u2014often outperform traditional monotonic approaches <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This finding suggests that the relationship between curriculum structure and training effectiveness is more complex than previously understood.\n\nThe transferability of curriculum strategies has also seen important breakthroughs. Research has demonstrated that curricula discovered for smaller datasets and models can perform well when applied to larger datasets and models, suggesting that effective curriculum strategies may be somewhat generalizable across scale <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This scalability has significant implications for efficiently implementing curriculum learning in increasingly large language models.\n\nNovel difficulty measurement techniques have emerged to address the limitations of static metrics. The Gradient-based Difficulty Measure (GDM) evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Unlike training loss, which can be misleading in plateau regions of the loss landscape, gradient magnitude provides more fine-grained insights into how examples influence model parameter updates, resulting in more accurate difficulty assessments <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is the Instruction-Following Difficulty (IFD) score, which evaluates how much an instruction helps in generating corresponding responses by comparing model loss with and without instructional context <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This method forms part of a self-guided process that begins by familiarizing the model with a small dataset subset before evaluating and selecting more challenging examples for focused training <Paper corpusId=\"261076515\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nLearning-based difficulty metrics represent another advancement, with methods that utilize learning percentage as a difficulty metric for self-ranking training data <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper>. This approach considers examples that show more learning in earlier epochs as easier, while those requiring more training time are deemed harder, allowing models to prioritize more challenging examples <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>.\n\nClass-based and augmentation-based curriculum strategies have also shown promise. Class-based approaches calculate average difficulty scores for each class and schedule training to progress from easier classes to more difficult ones <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>. Augmentation-based strategies initially train on original data before gradually increasing the proportion of augmented data, which is typically more challenging for models to learn <Paper corpusId=\"244908620\" paperTitle=\"(Wang et al._1, 2021)\" isShortName></Paper>.\n\nDespite these advances, existing approaches still face limitations in accurately accounting for continuously evolving model capabilities. Traditional methods often rely on static difficulty labels or heuristic rules that inadequately track model development <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. Recent work has highlighted the shortcomings of coarse-grained curriculum designs that oversimplify difficulty levels (e.g., categorizing questions merely by length) and LLM-based scoring methods that struggle to capture nuanced reasoning demands <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nThe latest research emphasizes the need for more sophisticated, fine-grained adaptive frameworks that can bridge the gap between data difficulty and model capability as training progresses <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>. These approaches draw inspiration from adaptive assessment techniques in educational settings to automatically calibrate training data to align with the model's current competence, optimizing learning efficiency <Paper corpusId=\"277824032\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as apposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively."], "score": 0.8955078125}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268696658, "title": "Symmetric Self-Paced Learning for Domain Generalization", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Di Zhao", "authorId": "2110600983"}, {"name": "Yun Sing Koh", "authorId": "34930533"}, {"name": "Gillian Dobbie", "authorId": "2276344048"}, {"name": "Hongsheng Hu", "authorId": "2293565669"}, {"name": "Philippe Fournier-Viger", "authorId": "2293453267"}], "n_citations": 5}, "snippets": ["To address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."], "score": 0.89013671875}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 261076515, "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ming Li", "authorId": "2150655891"}, {"name": "Yong Zhang", "authorId": "2144289768"}, {"name": "Zhitao Li", "authorId": "2111336489"}, {"name": "Jiuhai Chen", "authorId": "1391200710"}, {"name": "Lichang Chen", "authorId": "2108451006"}, {"name": "Ning Cheng", "authorId": "145292435"}, {"name": "Jianzong Wang", "authorId": "66063851"}, {"name": "Tianyi Zhou", "authorId": "2213956781"}, {"name": "Jing Xiao", "authorId": "91353860"}], "n_citations": 211}, "snippets": ["Our method involves a self-guided process that begins with familiarizing the model with a small subset of the dataset during the \"Learning from Brief Experience\" phase. This phase lays the groundwork for the subsequent \"Evaluating Based on Experience\" phase, where we introduce the Instruction-Following Difficulty (IFD) score. This metric evaluates how much help the instruction provides to the generation of the corresponding response, by comparing the loss in model responses with and without instructional context. The higher IFD score, indicating less instructional help, suggests a greater difficulty with instructions. On the contrary, the lower IFD score represents that the given instruction can directly benefit the language model largely even without further training, representing the easiness and necessity of the instruction. Thus in the final \"Retraining from Self-Guided Experience\" phase, we use data with relatively large IFD scores as the cherry data to train our model, resulting in what we term \"cherry models\"."], "score": 0.8603515625}, {"id": "(Mekala et al., 2024)", "paper": {"corpus_id": 267740312, "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Alex Nguyen", "authorId": "2284673632"}, {"name": "Jingbo Shang", "authorId": "2284595153"}], "n_citations": 21}, "snippets": ["Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered. We then select the most difficult sample subsets based on this ranking and instruction-tune a language model."], "score": 0.9521484375}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 249060677, "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Chengyu Dong", "authorId": "2113540861"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 20}, "snippets": ["Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."], "score": 0.0}, {"id": "(Wang et al._1, 2021)", "paper": {"corpus_id": 244908620, "title": "Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding", "year": 2021, "venue": "International Conference on Software Engineering", "authors": [{"name": "Deze Wang", "authorId": "1455165108"}, {"name": "Zhouyang Jia", "authorId": "10414205"}, {"name": "Shanshan Li", "authorId": "50342128"}, {"name": "Yue Yu", "authorId": null}, {"name": "Yun Xiong", "authorId": "2153657836"}, {"name": "Wei Dong", "authorId": "2114050396"}, {"name": "Xiangke Liao", "authorId": "144078016"}], "n_citations": 80}, "snippets": ["The key challenge of curriculum learning is how to define easy/difficult examples. In this paper, we propose two difficulty scoring functions based on the hypotheses presented in Section 2.3.\n\nAugmentation-based Curriculum Strategy. The previous section has introduced data augmentation techniques for code data, and it is cheap to generate diverse data through transformations. However, compared with original data, the augmented data can be regarded as perturbations or adversarial examples of original data [37]53], and they should be more difficult to learn as verified in Section 2.3.\n\nTherefore, we design an augmentation-based curriculum strategy. We first train on only the original data, and then gradually increase the proportion of the augmented data, ensuring that the model is exposed to more data and the difficulty gradually increases during the training process.\n\nClass-based Curriculum Strategy. Especially for multiclass classification tasks, based on the hypothesis verified in Section 2.3, we propose a class-based curriculum strategy.\n\nSpecifically, the leave-one-out strategy is employed to obtain the difficulty scores on the entire training set, and then the average difficulty score on each class is calculated. The samples in the same class take the average class difficulty score as their difficulty scores. In the training process, this setting allows the model to learn easier classes first, and then to more difficult classes. Obviously, the model needs to extract and learn more features to deal with increasingly difficult tasks."], "score": 0.8408203125}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 277824032, "title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qianjin Yu", "authorId": "2356617563"}, {"name": "Keyu Wu", "authorId": "2355802308"}, {"name": "Zihan Chen", "authorId": "2355823766"}, {"name": "Chushu Zhang", "authorId": "2355786652"}, {"name": "Manlin Mei", "authorId": "2355643664"}, {"name": "Lingjun Huang", "authorId": "2356799678"}, {"name": "Fang Tan", "authorId": "2355645282"}, {"name": "Yongsheng Du", "authorId": "2355785017"}, {"name": "Kunlin Liu", "authorId": "2356331597"}, {"name": "Yurui Zhu", "authorId": "2355782385"}], "n_citations": 3}, "snippets": ["Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the model's current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a;Xie et al., 2024;(Lee et al., 2024) or adopting curriculum learning frameworks (Wen et al., 2025a;Min et al., 2024;(Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. \n\nFor instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."], "score": 0.8232421875}], "table": null}], "cost": 0.537042}}
