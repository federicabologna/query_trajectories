{"better_query": "What are the risks and mitigation strategies for egocentric bias when using GPT-4 to evaluate its own generated data?", "better_answer": {"sections": [{"title": "Introduction to Egocentric Bias in LLMs", "tldr": "Egocentric bias in large language models refers to their tendency to prefer their own responses over others when evaluating outputs. This self-preference manifests when models like GPT-4 rate their own generations higher than comparable or even superior responses from other sources. (4 sources)", "text": "\nEgocentric bias, originating from cognitive psychology, refers to the tendency to have a higher opinion of oneself or to more readily accept ideas that align with one's own perspectives. In the context of large language models (LLMs), this bias manifests when models evaluate their own outputs more favorably than those generated by other systems. <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper>\n\nThis phenomenon creates significant challenges for LLM evaluation, particularly when using models to assess their own performance. As Li et al. note, \"The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator).\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> This creates a circular problem similar to a \"chicken-and-egg dilemma,\" where developing better models requires reliable evaluation, yet the evaluators themselves may be biased.\n\nRecent research has documented this behavior in prominent models. For instance, GPT-4V has been observed to exhibit \"a slight degree of Egocentricity\" when evaluating responses, showing a preference for its own outputs that align with its predefined ethical guidelines and training. <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> This bias persists despite efforts to use prompt engineering for neutrality, as these models still rely on judgment criteria established during their post-alignment training. <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>\n\nIt's important to note that not all preference for one's own outputs constitutes bias. As researchers point out, \"some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\" <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper> The challenge lies in distinguishing genuine quality differences from self-preferential bias.", "citations": [{"id": "(Koo et al., 2023)", "paper": {"corpus_id": 263310448, "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ryan Koo", "authorId": "2213239540"}, {"name": "Minhwa Lee", "authorId": "2187932371"}, {"name": "Vipul Raheja", "authorId": "2831377"}, {"name": "Jong Inn Park", "authorId": "2294310015"}, {"name": "Zae Myung Kim", "authorId": "2894340"}, {"name": "Dongyeop Kang", "authorId": "48493368"}], "n_citations": 86}, "snippets": ["Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias."], "score": 0.79541015625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.6328125}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267523079, "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Dongping Chen", "authorId": "2279219833"}, {"name": "Ruoxi Chen", "authorId": "2283244952"}, {"name": "Shilin Zhang", "authorId": "2283311181"}, {"name": "Yinuo Liu", "authorId": "2283150665"}, {"name": "Yaochen Wang", "authorId": "2283314623"}, {"name": "Huichi Zhou", "authorId": "2283313383"}, {"name": "Qihui Zhang", "authorId": "46324457"}, {"name": "Pan Zhou", "authorId": "2221116622"}, {"name": "Yao Wan", "authorId": "2254266993"}, {"name": "Lichao Sun", "authorId": "2267508610"}], "n_citations": 122}, "snippets": ["Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'."], "score": 0.84375}, {"id": "(Ouyang et al., 2022)", "paper": {"corpus_id": 246426909, "title": "Training language models to follow instructions with human feedback", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Long Ouyang", "authorId": "31793034"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Xu Jiang", "authorId": "2115903168"}, {"name": "Diogo Almeida", "authorId": "2061137049"}, {"name": "Carroll L. Wainwright", "authorId": "2064084601"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Chong Zhang", "authorId": null}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Alex Ray", "authorId": "2064770039"}, {"name": "John Schulman", "authorId": "47971768"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke E. Miller", "authorId": "2142365973"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "P. Christiano", "authorId": "145791315"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Ryan J. Lowe", "authorId": "49407415"}], "n_citations": 13203}, "snippets": ["Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."], "score": 0.0}], "table": null}, {"title": "Manifestations of Egocentric Bias in GPT-4", "tldr": "GPT-4 exhibits egocentric bias through several observable patterns including positional preferences, self-rating inflation, and a tendency to align evaluations with its own ethical guidelines and training. These manifestations can significantly impact the reliability of self-evaluation, especially in specialized domains like healthcare. (9 sources)", "text": "\nGPT-4's egocentric bias manifests in several distinct patterns. One prominent form is positional bias, where the model consistently assigns higher scores to responses presented in certain positions. Wang et al. demonstrated that \"GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered,\" and showed that \"merely swapping the presentation order can reverse evaluation outcomes.\" <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\nBeyond positional preferences, GPT-4 demonstrates self-preference by rating its own responses more favorably than comparable outputs from other models. This is particularly evident in evaluations involving ethical considerations, where \"GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines.\" <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> This alignment with its own training parameters occurs \"despite efforts in prompt engineering to ensure neutrality,\" as the model continues to rely on \"judgment criteria set during post-alignment training.\" <Paper corpusId=\"267523079\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>\n\nQuantitative analysis confirms this self-preference tendency. When evaluating its own responses versus those of other models, GPT-4 shows a higher false positive rate (FPR) \u2013 \"erroneously labeling incorrect responses as correct\" \u2013 particularly when \"provided with its own generated reference or no reference at all.\" <Paper corpusId=\"276885275\" paperTitle=\"(Krumdick et al., 2025)\" isShortName></Paper> This bias extends beyond individual model evaluations to a phenomenon termed \"preference leakage,\" where the model's bias can be \"inherited\" by student models trained on its outputs. <Paper corpusId=\"276106991\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>\n\nIn specialized domains like healthcare, this bias can have significant consequences. For example, when GPT-4 is used to generate medical content, it may \"inadequately represent demographic diversity in medical conditions\" and generate differential diagnoses that \"reflect biases associated with race, ethnicity, and gender.\" <Paper corpusId=\"270614044\" paperTitle=\"(Bragazzi et al., 2023)\" isShortName></Paper> This highlights how egocentric bias can compound existing societal biases embedded in the model's training data.\n\nResearch also suggests that the specificity of evaluation criteria influences the degree of bias. LLMs \"tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed.\" <Paper corpusId=\"277621852\" paperTitle=\"(Eldifrawi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273098639\" paperTitle=\"(Ye et al., 2024)\" isShortName></Paper> However, even with improved evaluation protocols, researchers still advise \"approaching results produced by GPT-4 with a certain level of skepticism,\" particularly when the model is generating and evaluating its own outputs. <Paper corpusId=\"272337179\" paperTitle=\"(Ji et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 258960339, "title": "Large Language Models are not Fair Evaluators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Peiyi Wang", "authorId": "144202874"}, {"name": "Lei Li", "authorId": "49192881"}, {"name": "Liang Chen", "authorId": "2146034504"}, {"name": "Dawei Zhu", "authorId": "2116276849"}, {"name": "Binghuai Lin", "authorId": "3186130"}, {"name": "Yunbo Cao", "authorId": "2154235"}, {"name": "Qi Liu", "authorId": "2144831944"}, {"name": "Tianyu Liu", "authorId": "1701889"}, {"name": "Zhifang Sui", "authorId": "3335836"}], "n_citations": 573}, "snippets": ["Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC."], "score": 0.62353515625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267523079, "title": "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Dongping Chen", "authorId": "2279219833"}, {"name": "Ruoxi Chen", "authorId": "2283244952"}, {"name": "Shilin Zhang", "authorId": "2283311181"}, {"name": "Yinuo Liu", "authorId": "2283150665"}, {"name": "Yaochen Wang", "authorId": "2283314623"}, {"name": "Huichi Zhou", "authorId": "2283313383"}, {"name": "Qihui Zhang", "authorId": "46324457"}, {"name": "Pan Zhou", "authorId": "2221116622"}, {"name": "Yao Wan", "authorId": "2254266993"}, {"name": "Lichao Sun", "authorId": "2267508610"}], "n_citations": 122}, "snippets": ["Egocentric Bias. Models tend to assign higher scores to their own responses while scoring others lower (Zheng et al., 2023b;Li et al., 2024). In Figures 19 and 20, GPT-4V exhibits a slight degree of Egocentricity. Conversely, Gemini maintains a uniform scoring distribution across different sources, demonstrating a more equitable approach to judgment. In contrast, GPT-4V shows self-preference, aligning its judgments with its predefined ethical guidelines. For example, GPT-4V consistently emphasizes privacy preservation, leading to higher scores for privacy-related questions based on its own metrics. Despite efforts in prompt engineering to ensure neutrality, these models still rely on judgment criteria set during post-alignment training (Ouyang et al., 2022). This bias can result in judgments that deviate from human preferences, highlighting the complexity of aligning MLLM judgments with humans'."], "score": 0.84375}, {"id": "(Ouyang et al., 2022)", "paper": {"corpus_id": 246426909, "title": "Training language models to follow instructions with human feedback", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Long Ouyang", "authorId": "31793034"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Xu Jiang", "authorId": "2115903168"}, {"name": "Diogo Almeida", "authorId": "2061137049"}, {"name": "Carroll L. Wainwright", "authorId": "2064084601"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Chong Zhang", "authorId": null}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Alex Ray", "authorId": "2064770039"}, {"name": "John Schulman", "authorId": "47971768"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke E. Miller", "authorId": "2142365973"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "P. Christiano", "authorId": "145791315"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Ryan J. Lowe", "authorId": "49407415"}], "n_citations": 13203}, "snippets": ["Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."], "score": 0.0}, {"id": "(Krumdick et al., 2025)", "paper": {"corpus_id": 276885275, "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Michael Krumdick", "authorId": "18171842"}, {"name": "Charles Lovering", "authorId": "2307472942"}, {"name": "Varshini Reddy", "authorId": "2266430123"}, {"name": "Seth Ebner", "authorId": "78150202"}, {"name": "Chris Tanner", "authorId": "2266398345"}], "n_citations": 6}, "snippets": ["One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others."], "score": 0.599609375}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276106991, "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Dawei Li", "authorId": "2161635474"}, {"name": "Renliang Sun", "authorId": "2344419674"}, {"name": "Yue Huang", "authorId": "2324070910"}, {"name": "Ming Zhong", "authorId": "2316709408"}, {"name": "Bohan Jiang", "authorId": "2036355404"}, {"name": "Jiawei Han", "authorId": "2343853966"}, {"name": "Xiangliang Zhang", "authorId": "2307963162"}, {"name": "Wei Wang", "authorId": "2343828587"}, {"name": "Huan Liu", "authorId": "2287545693"}], "n_citations": 29}, "snippets": ["Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios", "Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem."], "score": 0.50439453125}, {"id": "(Bragazzi et al., 2023)", "paper": {"corpus_id": 270614044, "title": "Toward Clinical Generative AI: Conceptual Framework", "year": 2023, "venue": "JMIR AI", "authors": [{"name": "N. Bragazzi", "authorId": "2250832100"}, {"name": "Sergio Garbarino", "authorId": "2277106726"}], "n_citations": 13}, "snippets": ["A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients."], "score": 0.51806640625}, {"id": "(Eldifrawi et al., 2025)", "paper": {"corpus_id": 277621852, "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Islam Eldifrawi", "authorId": "2126996290"}, {"name": "Shengrui Wang", "authorId": "2311878157"}, {"name": "Amine Trabelsi", "authorId": "2311887008"}], "n_citations": 0}, "snippets": ["In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it", "It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed."], "score": 0.85986328125}, {"id": "(Ye et al., 2024)", "paper": {"corpus_id": 273098639, "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jiayi Ye", "authorId": "2325239327"}, {"name": "Yanbo Wang", "authorId": "2324066105"}, {"name": "Yue Huang", "authorId": "2324070910"}, {"name": "Dongping Chen", "authorId": "2279219833"}, {"name": "Qihui Zhang", "authorId": "46324457"}, {"name": "Nuno Moniz", "authorId": "2241357425"}, {"name": "Tian Gao", "authorId": "2324218620"}, {"name": "Werner Geyer", "authorId": "2324052753"}, {"name": "Chao Huang", "authorId": "2324171997"}, {"name": "Pin-Yu Chen", "authorId": "2279077171"}, {"name": "Nitesh V. Chawla", "authorId": "2286872295"}, {"name": "Xiangliang Zhang", "authorId": "2307963162"}], "n_citations": 78}, "snippets": ["LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications."], "score": 0.0}, {"id": "(Ji et al., 2024)", "paper": {"corpus_id": 272337179, "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ziwei Ji", "authorId": "3391272"}, {"name": "Tiezheng Yu", "authorId": "2287917733"}, {"name": "Yan Xu", "authorId": "2285265406"}, {"name": "Nayeon Lee", "authorId": "2314592345"}, {"name": "Albert Q. Jiang", "authorId": "2278435713"}, {"name": "Alexandre Sablayrolles", "authorId": "2256994781"}, {"name": "Arthur Men-655", "authorId": "2319226386"}, {"name": "Chris Bamford", "authorId": "2256994975"}, {"name": "Devendra Singh", "authorId": "2302815701"}, {"name": "Diego Chaplot", "authorId": "2302809975"}, {"name": "laume Lample", "authorId": "2318358390"}, {"name": "L\u00e9lio Lucile Saulnier", "authorId": "2318350171"}, {"name": "Renard Lavaud", "authorId": "2318358843"}, {"name": "M. Lachaux", "authorId": "114952298"}, {"name": "Pierre Stock", "authorId": "2256994779"}, {"name": "Teven Le Scao", "authorId": "1379806208"}, {"name": "Jerry Kang", "authorId": "2319259930"}, {"name": "Mark W. Bennett", "authorId": "2319227308"}, {"name": "Devon Carbado", "authorId": "2295905607"}, {"name": "Pam Casey", "authorId": "2319226400"}, {"name": "P. Liang", "authorId": "2310645453"}, {"name": "Chiyu Wu", "authorId": "2115397918"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}, {"name": "Aman Madaan", "authorId": "21626987"}, {"name": "Niket Tandon", "authorId": "2261389843"}, {"name": "Prakhar Gupta", "authorId": "2302821008"}, {"name": "Skyler Hallinan", "authorId": null}, {"name": "Luyu Gao", "authorId": "2267242298"}, {"name": "Sarah Wiegreffe", "authorId": "35823986"}, {"name": "Uri Alon", "authorId": "2268672727"}, {"name": "Nouha Dziri", "authorId": "46217681"}, {"name": "Shrimai Prabhumoye", "authorId": "9358910"}, {"name": "Yiming Yang", "authorId": "2315571964"}, {"name": "Shashank Gupta", "authorId": "2302819906"}, {"name": "Bodhisattwa Prasad Majumder", "authorId": "3165738"}, {"name": "Katherine Hermann", "authorId": "2273674137"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Amir Yazdan Bakhsh", "authorId": "80489277"}, {"name": "ing Bao", "authorId": "2319225091"}, {"name": "Mo Bavarian", "authorId": "2275251620"}, {"name": "J. Belgum", "authorId": "2275245092"}, {"name": "Ir-wan Bello", "authorId": "2309476543"}, {"name": "Jake Berdine", "authorId": "2275245414"}, {"name": "Gabriel Bernadett-Shapiro", "authorId": "2275245581"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Lenny Bogdonoff", "authorId": "2275251674"}, {"name": "Oleg Boiko", "authorId": "2275246071"}, {"name": "Made-laine Boyd", "authorId": "2275248137"}, {"name": "Anna-Luisa Brakman", "authorId": "2275245419"}, {"name": "Greg Brock-724 man", "authorId": "2319225043"}, {"name": "Tim Brooks", "authorId": "2275219628"}, {"name": "M. Brundage", "authorId": "2265097787"}, {"name": "Kevin Button", "authorId": "2146257251"}, {"name": "Trevor Cai", "authorId": "2275157286"}, {"name": "Rosie Campbell", "authorId": "2274782053"}, {"name": "Andrew Cann", "authorId": "2275245404"}, {"name": "Brittany Carey", "authorId": "2275246368"}, {"name": "Chelsea Carlson", "authorId": "2275120298"}, {"name": "Rory Carmichael", "authorId": "144114446"}, {"name": "Brooke Chan", "authorId": "1466431052"}, {"name": "Che Chang", "authorId": "2275545855"}, {"name": "Fotis Chantzis", "authorId": "2057091285"}, {"name": "Derek Chen", "authorId": "2253841704"}, {"name": "Su-Hong Chen", "authorId": "2256808607"}, {"name": "Ruby Chen", "authorId": "2275179180"}, {"name": "Jason Chen", "authorId": "2275289833"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Chester Cho", "authorId": "2275251158"}, {"name": "Hyung Casey Chu", "authorId": "2309475703"}, {"name": "Won Chung", "authorId": "2282528643"}, {"name": "Dave Cummings", "authorId": "2275231534"}, {"name": "Jeremiah Currier", "authorId": "49645091"}, {"name": "Yunxing Dai", "authorId": "2276187456"}, {"name": "Tarun Goel", "authorId": "2309477435"}, {"name": "Gabriel Gogineni", "authorId": "2309477420"}, {"name": "Rapha Goh", "authorId": "2309475753"}, {"name": "Jonathan Gontijo-738 Lopes", "authorId": "2319225953"}, {"name": "Morgan Gordon", "authorId": "2309478880"}, {"name": "Scott Grafstein", "authorId": "2309480960"}, {"name": "Ryan Gray", "authorId": "2309478956"}, {"name": "Joshua Greene", "authorId": "2309478103"}, {"name": "Shixiang Shane Gross", "authorId": "2309475937"}, {"name": "Yufei Gu", "authorId": "2309896895"}, {"name": "Chris Guo", "authorId": "2309804592"}, {"name": "Jesse Hallacy", "authorId": "2309477491"}, {"name": "Jeff Han", "authorId": "2309667621"}, {"name": "Harris Yuchen", "authorId": "2309475604"}, {"name": "Mike He", "authorId": "2310401628"}, {"name": "Johannes Heaton", "authorId": "2309477353"}, {"name": "C. Heidecke", "authorId": "2309476458"}, {"name": "Alan Hesse", "authorId": "2309475602"}, {"name": "W. Hickey", "authorId": "2275246148"}, {"name": "Peter Hickey", "authorId": "2309477265"}, {"name": "Hoeschele Brandon", "authorId": "2309477475"}, {"name": "Kenny Houghton", "authorId": "2309480952"}, {"name": "Shengli Hsu", "authorId": "2309479202"}, {"name": "Xin Hu", "authorId": "2275777049"}, {"name": "Joost Hu", "authorId": "2309663799"}, {"name": "Shantanu Huizinga", "authorId": "2309477471"}, {"name": "Shawn Jain", "authorId": "2309900251"}, {"name": "Jain Joanne", "authorId": "2309475571"}, {"name": "Angela Jang", "authorId": "2309477467"}, {"name": "Roger Jiang", "authorId": "2275172062"}, {"name": "Haozhun Jiang", "authorId": "2309830920"}, {"name": "Denny Jin", "authorId": "2275203081"}, {"name": "Shino Jin", "authorId": "2309901734"}, {"name": "Billie Jomoto", "authorId": "2309481128"}, {"name": "Hee-woo Jonn", "authorId": "2309480973"}, {"name": "Tomer Jun", "authorId": "2309475461"}, {"name": "\u0141ukasz Kaftan", "authorId": "2309476661"}, {"name": "Ali Kaiser", "authorId": "2309476629"}, {"name": "Ingmar Ka-748 mali", "authorId": "2319225089"}, {"name": "Kanitscheider", "authorId": "2102033721"}, {"name": "Nitish Shirish", "authorId": "2288125393"}, {"name": "Keskar Tabarak", "authorId": "2307452588"}, {"name": "Logan Khan", "authorId": "2307454011"}, {"name": "J. Kilpatrick", "authorId": "2307452560"}, {"name": "Kim", "authorId": "2319227856"}, {"name": "Christina Kim", "authorId": "2149054292"}, {"name": "Yongjik Kim", "authorId": "2275296777"}, {"name": "Jan Hendrik Kirch-751 ner", "authorId": "2319226271"}, {"name": "J. Kiros", "authorId": "51131802"}, {"name": "Matthew Knight", "authorId": "2146257375"}, {"name": "Daniel Kokotajlo", "authorId": "1485556711"}, {"name": "\u0141ukasz Kondraciuk", "authorId": "2319226462"}, {"name": "Andrew Kondrich", "authorId": "1666171360"}, {"name": "Aris Kon-753 stantinidis", "authorId": "2319225191"}, {"name": "Kyle Kosic", "authorId": "2275245594"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "Vishal Kuo", "authorId": "2275229877"}, {"name": "Michael Lampe", "authorId": "2275247085"}, {"name": "Ikai Lan", "authorId": "2275246287"}, {"name": "Teddy Lee", "authorId": "2274915115"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Jade Leung", "authorId": "52152632"}, {"name": "Chak Daniel Levy", "authorId": "2319225189"}, {"name": "Ming Li", "authorId": "2319250141"}, {"name": "Rachel Lim", "authorId": "2275176375"}, {"name": "Molly Lin", "authorId": "2275759230"}, {"name": "Stephanie Lin", "authorId": "2253840098"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Theresa Lopez", "authorId": "2275248327"}, {"name": "Ryan Lowe", "authorId": "2257272397"}, {"name": "Patricia Lue", "authorId": "2275245628"}, {"name": "A. Makanju", "authorId": "119341078"}, {"name": "Kim Malfacini", "authorId": "2275245649"}, {"name": "Sam Manning", "authorId": "46430291"}, {"name": "Todor Markov", "authorId": "14113256"}, {"name": "Yaniv Markovski", "authorId": "2275245336"}, {"name": "Bianca Martin", "authorId": "2114362965"}, {"name": "Katie Mayer", "authorId": "2275231822"}, {"name": "Andrew Mayne", "authorId": "2275247045"}, {"name": "Bob McGrew", "authorId": "39593364"}, {"name": "S. McKinney", "authorId": "2047820455"}, {"name": "Christine McLeavey", "authorId": "3028785"}, {"name": "Paul McMillan", "authorId": "2274772421"}, {"name": "Jake McNeil", "authorId": "2275234856"}, {"name": "David Medina", "authorId": "2275210659"}, {"name": "Aalok Mehta", "authorId": "2275132306"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Luke Metz", "authorId": "2275246330"}, {"name": "An-drey Mishchenko", "authorId": "2275252694"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Vinnie Monaco", "authorId": "2275245453"}, {"name": "Evan Morikawa", "authorId": "1404556973"}, {"name": "Daniel P. Mossing", "authorId": "3407880"}, {"name": "Tong Mu", "authorId": "2319225702"}, {"name": "Mira Murati", "authorId": "2117715631"}, {"name": "O. Murk", "authorId": "147746767"}, {"name": "David M\u00e9ly", "authorId": "2319226404"}, {"name": "Ashvin Nair", "authorId": "2319226971"}, {"name": "Reiichiro Nakano", "authorId": "7406311"}, {"name": "Rajeev Nayak", "authorId": "2057426488"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Richard Ngo", "authorId": "2273886618"}, {"name": "Hyeonwoo Noh", "authorId": "2275115983"}, {"name": "Ouyang Long", "authorId": "2228518120"}, {"name": "Cullen O'Keefe", "authorId": "1435765036"}, {"name": "J. Pachocki", "authorId": "2713380"}, {"name": "A. Paino", "authorId": "34800652"}, {"name": "Joe Palermo", "authorId": "2275244652"}, {"name": "Ashley Pantuliano", "authorId": "2275246178"}, {"name": "Carl Ross", "authorId": "2275207240"}, {"name": "Bob Rotsted", "authorId": "11150265"}, {"name": "Henri Roussez", "authorId": "2275250007"}, {"name": "Nick Ry-779 der", "authorId": "2319225618"}, {"name": "Mario D. Saltarelli", "authorId": "2252840300"}, {"name": "Ted Sanders", "authorId": "2275246803"}, {"name": "Shibani Santurkar", "authorId": "2852106"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Heather Schmidt", "authorId": "2275265666"}, {"name": "David Schnurr", "authorId": "2252874293"}, {"name": "John Schulman", "authorId": "2297873691"}, {"name": "Daniel Selsam", "authorId": "2196579"}, {"name": "Kyla Sheppard", "authorId": "2275244711"}, {"name": "Toki Sherbakov", "authorId": "102475503"}, {"name": "Jessica Shieh", "authorId": "2275246834"}, {"name": "Sarah Shoker", "authorId": "118335789"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Szymon Sidor", "authorId": "2700360"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Jordan Sitkin", "authorId": "2275252299"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Ian Sohl", "authorId": "103422608"}, {"name": "Benjamin Sokolowsky", "authorId": "2901424"}, {"name": "Yang Song", "authorId": "2307592658"}, {"name": "Natalie Staudacher", "authorId": "2275245668"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Samuel Wolrich", "authorId": "2275244177"}, {"name": "Hannah Wong", "authorId": "2275225207"}, {"name": "Lauren Workman", "authorId": "2275245771"}, {"name": "Sherwin Wu", "authorId": "2275299848"}, {"name": "Jeff Wu", "authorId": "2274911253"}, {"name": "Michael Wu", "authorId": "2307456650"}, {"name": "Kai Xiao", "authorId": "2307454769"}, {"name": "Tao Xu", "authorId": "2275452480"}, {"name": "Sarah Yoo", "authorId": "2275310096"}, {"name": "Kevin Yu", "authorId": "2275593618"}, {"name": "Qim-ing Yuan", "authorId": "2275194186"}, {"name": "Wojciech Zaremba", "authorId": "2307452791"}, {"name": "Rowan Zellers", "authorId": "49629836"}, {"name": "Chong Zhang", "authorId": "2315024566"}, {"name": "Marvin Zhang", "authorId": "2281037751"}, {"name": "Tianhao Shengjia Zhao", "authorId": "2307453667"}, {"name": "Xu Jiang", "authorId": "2298950344"}, {"name": "Diogo Almeida", "authorId": "2275252021"}, {"name": "Carroll L. Wainwright", "authorId": "2275245962"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Alex Gray", "authorId": "2319227127"}, {"name": "Jacob Hilton", "authorId": "2286540856"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke Miller", "authorId": "2298421583"}, {"name": "Amanda Askell", "authorId": "2220750220"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "Paul F. Christiano", "authorId": "2261980896"}, {"name": "Joon Sung Park", "authorId": "2197475360"}, {"name": "Joseph C. O\u2019Brien", "authorId": "2213764034"}, {"name": "C. Cai", "authorId": "2276794641"}, {"name": "Ringel Morris", "authorId": "2319287576"}, {"name": "Percy Liang", "authorId": "2256995425"}, {"name": "Michael S. Bern-814", "authorId": "2319226111"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "Karthik Narasimhan", "authorId": "2285784924"}, {"name": "Tim Salimans", "authorId": "2887364"}, {"name": "Rachel Rudinger", "authorId": "2302559920"}, {"name": "Jason Naradowsky", "authorId": "2300343"}, {"name": "Brian Leonard", "authorId": "2319225916"}, {"name": "Nisan Stiennon", "authorId": "1387983862"}, {"name": "Ryan Ziegler", "authorId": "2319225774"}, {"name": "Chelsea Lowe", "authorId": "2314165049"}, {"name": "Alec Voss", "authorId": "2314160468"}, {"name": "Radford", "authorId": "2289348718"}, {"name": "Dario Amodei", "authorId": "2698777"}, {"name": "Christiano. 2020. Learn-842", "authorId": "2319225540"}, {"name": "Tony Sun", "authorId": "2319580593"}, {"name": "Andrew Gaut", "authorId": "146072982"}, {"name": "Shirlyn Tang", "authorId": "148149462"}, {"name": "Yuxin Huang", "authorId": "2154731574"}, {"name": "Mai ElSherief", "authorId": "2288124187"}, {"name": "Jie Zhao", "authorId": "2311580116"}, {"name": "Diba Mirza", "authorId": "2319225642"}, {"name": "Kai-Wei Belding", "authorId": "2319226119"}, {"name": "Chang William", "authorId": "2319225109"}, {"name": "Yang Wang", "authorId": "2319563352"}, {"name": "Yixin Wan", "authorId": "2165227666"}, {"name": "George Pu", "authorId": "2258548444"}, {"name": "Jiao Sun", "authorId": "2261454711"}, {"name": "Aparna Garimella", "authorId": "31099365"}, {"name": "Kai-Wei Chang", "authorId": "2257127887"}, {"name": "Nanyun Peng", "authorId": "2285475879"}, {"name": "\u201ckelly", "authorId": "2319226454"}], "n_citations": 14}, "snippets": ["It is also important to note that most of our data are generated by gpt-4. Therefore, it is advisable to approach the results produced by GPT-4 with a certain level of skepticism."], "score": 0.6328125}], "table": null}, {"title": "Risks of Egocentric Bias", "tldr": "Egocentric bias in GPT-4 creates significant risks including reinforcement of the model's own preferences through self-evaluation, potential amplification of medical and demographic biases, and the inheritance of biases by student models trained on its outputs. (10 sources)", "text": "\nWhen GPT-4 evaluates its own outputs, several substantial risks emerge. Perhaps most concerning is the potential for self-reinforcement, where the model's biases become amplified through continuous self-evaluation. As Dai et al. note, \"The emergence of egocentric bias introduces the risk of self-reinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations,\" which can lead to \"overfitting to their own evaluation criteria, intensifying self-preference in next-generation models.\" <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>\n\nThis self-reinforcement creates a problematic feedback loop that potentially undermines the reliability of evaluation processes. Wu et al. highlight \"concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process,\" suggesting the need for multiple evaluators to establish \"a more comprehensive and unbiased assessment.\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\nIn specialized domains like healthcare, these biases can have particularly harmful consequences. Research by Bragazzi et al. warns that \"LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.\" Their analysis of GPT-4 revealed that it \"inadequately represents demographic diversity in medical conditions\" and generates differential diagnoses that \"reflect biases associated with race, ethnicity, and gender.\" <Paper corpusId=\"270614044\" paperTitle=\"(Bragazzi et al., 2023)\" isShortName></Paper>\n\nAnother significant risk is the propagation of bias through what researchers call \"preference leakage.\" Li et al. empirically confirmed \"the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks.\" This means that biases in GPT-4 can be inherited by models trained on its outputs, creating a compounding effect where \"GPT-4's bias toward LLaMA has been passed on to LLaMA's student models.\" <Paper corpusId=\"276106991\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>\n\nEven when researchers are aware of potential biases, mitigating them completely remains challenging. Mullick et al. acknowledge that \"using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities,\" while noting that \"alternatives, such as human evaluation, also carry their own biases.\" <Paper corpusId=\"270286247\" paperTitle=\"(Mullick et al., 2024)\" isShortName></Paper>\n\nThe fundamental challenge is what Li et al. describe as a \"chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper> Wataoka et al. further warn that \"using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies,\" suggesting that over-reliance on a single model for evaluation can lead to a narrowing of the diversity of acceptable outputs. <Paper corpusId=\"273661820\" paperTitle=\"(Wataoka et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Dai et al., 2024)", "paper": {"corpus_id": 269188154, "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era", "year": 2024, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Sunhao Dai", "authorId": "2155892801"}, {"name": "Chen Xu", "authorId": "2153078929"}, {"name": "Shicheng Xu", "authorId": "2202745"}, {"name": "Liang Pang", "authorId": "2263589454"}, {"name": "Zhenhua Dong", "authorId": "2297820120"}, {"name": "Jun Xu", "authorId": "2266437969"}], "n_citations": 82}, "snippets": ["The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."], "score": 0.84765625}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yang Liu", "authorId": "2152797401"}, {"name": "Dan Iter", "authorId": "3310951"}, {"name": "Yichong Xu", "authorId": "2110197273"}, {"name": "Shuo Wang", "authorId": "2146294891"}, {"name": "Ruochen Xu", "authorId": "8233965"}, {"name": "Chenguang Zhu", "authorId": "8652308"}], "n_citations": 1211}, "snippets": ["The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"], "score": 0.0}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 259360998, "title": "Style Over Substance: Evaluation Biases for Large Language Models", "year": 2023, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Minghao Wu", "authorId": "2145209409"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}], "n_citations": 47}, "snippets": ["This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."], "score": 0.552734375}, {"id": "(Bragazzi et al., 2023)", "paper": {"corpus_id": 270614044, "title": "Toward Clinical Generative AI: Conceptual Framework", "year": 2023, "venue": "JMIR AI", "authors": [{"name": "N. Bragazzi", "authorId": "2250832100"}, {"name": "Sergio Garbarino", "authorId": "2277106726"}], "n_citations": 13}, "snippets": ["A further risk is that LLMs might reinforce existing biases and provide inaccurate medical diagnoses, potentially leading to detrimental effects on health care.Zack et al [26] aimed to evaluate whether GPT-4 harbors biases that could influence its application in health care settings.Using the Azure OpenAI interface, the authors scrutinized GPT-4 for racial and gender biases and assessed the impact of such biases on four clinical applications of LLMs-(1) medical education, (2) diagnostic reasoning, (3) development and implementation of clinical plans, and (4) subjective patient evaluations-involving experiments using prompts mimicking typical GPT-4 use in clinical and medical educational settings and drawing from New England Journal of Medicine Healer clinical vignettes and research on implicit bias in health care.The study compared GPT-4's estimates of demographic distributions of medical conditions against actual US prevalence data.For differential diagnosis and treatment planning, the research analyzed variations across demographic groups using standard statistical methods to identify significant differences.The study revealed that GPT-4 inadequately represents demographic diversity in medical conditions, often resorting to stereotypical demographic portrayals in clinical vignettes.The differential diagnoses generated by GPT-4 for standardized clinical vignettes tended to reflect biases associated with race, ethnicity, and gender.Furthermore, the model's assessments and plans demonstrated a notable correlation between demographic characteristics and recommendations for costlier procedures, as well as varied perceptions of patients."], "score": 0.51806640625}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276106991, "title": "Preference Leakage: A Contamination Problem in LLM-as-a-judge", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Dawei Li", "authorId": "2161635474"}, {"name": "Renliang Sun", "authorId": "2344419674"}, {"name": "Yue Huang", "authorId": "2324070910"}, {"name": "Ming Zhong", "authorId": "2316709408"}, {"name": "Bohan Jiang", "authorId": "2036355404"}, {"name": "Jiawei Han", "authorId": "2343853966"}, {"name": "Xiangliang Zhang", "authorId": "2307963162"}, {"name": "Wei Wang", "authorId": "2343828587"}, {"name": "Huan Liu", "authorId": "2287545693"}], "n_citations": 29}, "snippets": ["Through extensive experiments, we empirically confirm the bias of judges towards their related student models caused by preference leakage across multiple LLM baselines and benchmarks. Further analysis suggests that preference leakage is a pervasive and real-world problem that is harder to detect compared to previously identified biases in LLM-as-a-judge scenarios", "Using the MTBench (Zheng et al., 2023) dataset, which includes pairwise comparison judgments from both humans and GPT-4, we compare GPT-4's and human evaluators' judgments on LLaMA-2 vs other models (including Vicuna, Alpaca, GPT-3.5, and GPT-4, which are preferred by GPT-4 due to preference leakage or egocentric bias) and LLaMA-2 vs Claude. The results, shown in Figure 4, reveal a clear preference for LLaMA-2 by GPT-4. Consequently, we conclude that evaluators' bias can be inherited. In this case, GPT-4's bias toward LLaMA has been passed on to LLaMA's student models. This inheritance, combined with the opaque training data of LLMs, makes preference leakage a more complex and challenging problem."], "score": 0.50439453125}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 259129398, "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Lianmin Zheng", "authorId": "2149970173"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Ying Sheng", "authorId": "2209360681"}, {"name": "Siyuan Zhuang", "authorId": "92721493"}, {"name": "Zhanghao Wu", "authorId": "1390573666"}, {"name": "Yonghao Zhuang", "authorId": "2152482391"}, {"name": "Zi Lin", "authorId": "143872641"}, {"name": "Zhuohan Li", "authorId": "2141335450"}, {"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "E. Xing", "authorId": "143977260"}, {"name": "Haotong Zhang", "authorId": "145140331"}, {"name": "Joseph E. Gonzalez", "authorId": "49988044"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 4439}, "snippets": ["Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."], "score": 0.0}, {"id": "(Mullick et al., 2024)", "paper": {"corpus_id": 270286247, "title": "On The Persona-based Summarization of Domain-Specific Documents", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ankan Mullick", "authorId": "21724468"}, {"name": "Sombit Bose", "authorId": "2238668264"}, {"name": "Rounak Saha", "authorId": "2304954421"}, {"name": "Ayan Kumar Bhowmick", "authorId": "19181085"}, {"name": "Pawan Goyal", "authorId": "2261284157"}, {"name": "Niloy Ganguly", "authorId": "2261284171"}, {"name": "Prasenjit Dey", "authorId": "2287821944"}, {"name": "Ravi Kokku", "authorId": "2247701385"}], "n_citations": 3}, "snippets": ["It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints."], "score": 0.501953125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.6328125}, {"id": "(Sellam et al., 2020)", "paper": {"corpus_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Thibault Sellam", "authorId": "145450400"}, {"name": "Dipanjan Das", "authorId": "143790066"}, {"name": "Ankur P. Parikh", "authorId": "144729897"}], "n_citations": 1505}, "snippets": ["Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "score": 0.0}, {"id": "(Wataoka et al., 2024)", "paper": {"corpus_id": 273661820, "title": "Self-Preference Bias in LLM-as-a-Judge", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Koki Wataoka", "authorId": "2007365532"}, {"name": "Tsubasa Takahashi", "authorId": "2325815191"}, {"name": "Ryokan Ri", "authorId": "1466451143"}], "n_citations": 25}, "snippets": ["This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized."], "score": 0.70166015625}], "table": null}, {"title": "Mitigation Strategies: Evaluation Design Approaches", "tldr": "Several evaluation design strategies have been developed to mitigate egocentric bias in GPT-4's self-evaluation, including position calibration techniques, hierarchical rubrics, and specialized prompting methods that encourage rational judgment and multiple evidence collection. (9 sources)", "text": "\n- **Multiple Evidence Calibration (MEC)**: This approach prompts the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. Combined with ensemble techniques, MEC can stabilize evaluation by incorporating multiple evidence calibration results. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Balanced Position Calibration (BPC)**: To reduce positional bias (which can influence egocentric bias), this technique evaluates each candidate in both positions across two runs and computes the final score as the average. This helps counteract GPT-4's tendency to favor responses in certain positions. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Human In The Loop Calibration (HITLC)**: This strategy incorporates human oversight into the evaluation process, using a diversity-based method to identify potentially biased candidates based on results from other calibration approaches. <Paper corpusId=\"258960339\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n- **Hierarchical Rubrics**: By prioritizing certain biases in the evaluation process, this approach helps isolate and address specific forms of bias. For example, if an evaluation shows signs of order bias, it would not be evaluated for other biases like salience or egocentric bias. <Paper corpusId=\"263310448\" paperTitle=\"(Koo et al., 2023)\" isShortName></Paper>\n\n- **Bias Scoring Methods (BSM)**: These methods can improve evaluation even when an LLM judges its own outputs. Research shows BSM can achieve a 3% better correlation with human judgments, suggesting it effectively reduces self-enhancement bias. <Paper corpusId=\"264591429\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>\n\n- **Neutral Evaluator Selection**: Using LLMs independent of the assessed models' training data can mitigate data leakage risks that contribute to egocentric bias. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Evaluator Rotation**: Randomly rotating different evaluators can help reduce the impact of bias from any single model's judgment. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **AwaRe Prompting**: This technique has shown effectiveness in mitigating multiple biases including order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, as well as bandwagon effect and verbosity bias in GPT-4. This approach prompts LLMs to make more rational judgments. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\n- **Human Reference Provision**: Providing human-generated references during evaluation reduces both the overall rate of error and the relative difference between a model's judgment of its own responses versus those of others. This is particularly effective in reducing false positive rates when models judge their own outputs. <Paper corpusId=\"276885275\" paperTitle=\"(Krumdick et al., 2025)\" isShortName></Paper>\n\n- **Detailed Evaluation Criteria**: LLMs tend to show higher correlation with human annotations and lower egocentric bias when evaluation criteria are more specific and detailed. <Paper corpusId=\"277621852\" paperTitle=\"(Eldifrawi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273098639\" paperTitle=\"(Ye et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 258960339, "title": "Large Language Models are not Fair Evaluators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Peiyi Wang", "authorId": "144202874"}, {"name": "Lei Li", "authorId": "49192881"}, {"name": "Liang Chen", "authorId": "2146034504"}, {"name": "Dawei Zhu", "authorId": "2116276849"}, {"name": "Binghuai Lin", "authorId": "3186130"}, {"name": "Yunbo Cao", "authorId": "2154235"}, {"name": "Qi Liu", "authorId": "2144831944"}, {"name": "Tianyu Liu", "authorId": "1701889"}, {"name": "Zhifang Sui", "authorId": "3335836"}], "n_citations": 573}, "snippets": ["Specifically, we demonstrate that GPT-4 exhibits a preference for the first displayed candidate response by consistently assigning it higher scores, even when the order of candidates is subtly altered. As illustrated in Figure 1, merely swapping the presentation order can reverse evaluation outcomes. This bias is also present in Chat-GPT, which typically favors the second response. These findings highlight previously overlooked limitations in the current evaluation paradigm.\n\nTo address this issue, we propose three simple yet effective strategies to calibrate positional bias: 1) Multiple Evidence Calibration (MEC): We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration. We also employ ensemble techniques to incorporate multiple evidence calibration results to further stabilize the evaluation. 2) Balanced Position Calibration (BPC): To further reduce positional bias, we evaluate each candidate in both positions across two runs and compute the final score as the average of the two runs. 3) Human In The Loop Calibration (HITLC): We also explore human-inthe-loop evaluation and consider a diversity-based method to get a cue to indicate biased candidates based on the evaluation results of MEC and BPC."], "score": 0.62353515625}, {"id": "(Koo et al., 2023)", "paper": {"corpus_id": 263310448, "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ryan Koo", "authorId": "2213239540"}, {"name": "Minhwa Lee", "authorId": "2187932371"}, {"name": "Vipul Raheja", "authorId": "2831377"}, {"name": "Jong Inn Park", "authorId": "2294310015"}, {"name": "Zae Myung Kim", "authorId": "2894340"}, {"name": "Dongyeop Kang", "authorId": "48493368"}], "n_citations": 86}, "snippets": ["Egocentric Bias (Self-Preference). (Ross and Sicoly, 1979) is a cognitive bias that refers to the tendency to have a higher opinion of oneself or to more easily accept ideas if they match one's own. We define an evaluator to be egocentrically biased if, for each instance, the evaluator prefers its own response over others. We note that an unbiased evaluator would choose between themselves and other comparand models equally in proportion. However, we highlight that some models would naturally generate higher quality responses (e.g., GPT4 vs. KOALA), resulting in a stronger inclination for such evaluators to choose their own responses.\n\nWe employ various strategies to mitigate these confounding variables and isolate each analysis as much as possible. For example, we employ a \"hierarchical\" rubric, where some biases take priority in an evaluation. Specifically, if an evaluation shows signs of order bias by choosing A in (A first, then B) and B in (B first, then A), we do not evaluate it for SALIENCE or EGOCEN-TRIC bias."], "score": 0.79541015625}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 264591429, "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Swarnadeep Saha", "authorId": "35106509"}, {"name": "Omer Levy", "authorId": "2253752918"}, {"name": "Asli Celikyilmaz", "authorId": "1709797"}, {"name": "Mohit Bansal", "authorId": "2253762115"}, {"name": "Jason Weston", "authorId": "2243265350"}, {"name": "Xian Li", "authorId": "2243015223"}], "n_citations": 77}, "snippets": ["Self-enhancement Bias Reduction.Table 2 evaluates self-enhancement bias by comparing BSM (with zero-shot GPT-4) for the samples where one of the responses is also generated by GPT-4.We observe a 3% better correlation with humans, suggesting that BSM improves evaluation even when the LLM judges its own outputs."], "score": 0.513671875}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 267760188, "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xiang Li", "authorId": "2261896751"}, {"name": "Yunshi Lan", "authorId": "2257016293"}, {"name": "Chao Yang", "authorId": "2268678836"}], "n_citations": 11}, "snippets": ["Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias."], "score": 0.51025390625}, {"id": "(Sumita et al., 2024)", "paper": {"corpus_id": 274437478, "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments", "year": 2024, "venue": "ACM Symposium on Applied Computing", "authors": [{"name": "Yasuaki Sumita", "authorId": "2333360231"}, {"name": "Koh Takeuchi", "authorId": "2243408877"}, {"name": "Hisashi Kashima", "authorId": "2247886893"}], "n_citations": 5}, "snippets": ["Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments."], "score": 0.71044921875}, {"id": "(Krumdick et al., 2025)", "paper": {"corpus_id": 276885275, "title": "No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Michael Krumdick", "authorId": "18171842"}, {"name": "Charles Lovering", "authorId": "2307472942"}, {"name": "Varshini Reddy", "authorId": "2266430123"}, {"name": "Seth Ebner", "authorId": "78150202"}, {"name": "Chris Tanner", "authorId": "2266398345"}], "n_citations": 6}, "snippets": ["One common judgment bias is self-preference, where a model tends to overrate its own responses. To quantify this bias we computed the false positive rate (FPR) for each judge when evaluating its own responses versus all other models. In this context, the FPR represents the rate at which the model erroneously labels incorrect responses as correct. We also evaluated the false negative rate (FNR), which is the rate at which the model erroneously labels correct responses as incorrect. A model with a strong selfpreference bias would exhibit a high FPR and a low FNR when grading its own references. Figure 4 displays the FPR and FNR aggregated over each judge with the \"Wrong\", \"Random\", \"Self\", \"None\", and \"Human\" reference types (Metrics per judge can be found in Figure 5). For every reference type, we see that on average models have a higher FPR when grading their own responses. The gap is particularly large when the model is provided with its own generated reference or no reference at all. Thus, providing a human reference reduces both the overall rate of error and the relative difference between a model's judgment of its own responses and those of others."], "score": 0.599609375}, {"id": "(Eldifrawi et al., 2025)", "paper": {"corpus_id": 277621852, "title": "FinGrAct: A Framework for FINe-GRrained Evaluation of ACTionability in Explainable Automatic Fact-Checking", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Islam Eldifrawi", "authorId": "2126996290"}, {"name": "Shengrui Wang", "authorId": "2311878157"}, {"name": "Amine Trabelsi", "authorId": "2311887008"}], "n_citations": 0}, "snippets": ["In their study, (Liu et al., 2023) identified a bias in evaluators, where they tend to favor their own model's generations over those from other models, even when the latter are objectively better. (Ohi et al., 2024) introduced a method for detecting this bias, which they termed 'Likelihood-based Evaluation Bias.' However, this approach requires access to the probability distribution of the LLM's generations, which is often unavailable, especially when working with commercial LLMs. (Ye et al., 2024) also addressed this issue, referring to it as 'egocentric Bias,' and we adopt this terminology in our work. Their research primarily focuses on understanding the effects of this bias on performance and strategies for mitigating it", "It is worth noting that LLMs as evaluators tend to exhibit higher correlation with human annotations and lower egocentric bias when the evaluation criteria are more detailed."], "score": 0.85986328125}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yang Liu", "authorId": "2152797401"}, {"name": "Dan Iter", "authorId": "3310951"}, {"name": "Yichong Xu", "authorId": "2110197273"}, {"name": "Shuo Wang", "authorId": "2146294891"}, {"name": "Ruochen Xu", "authorId": "8233965"}, {"name": "Chenguang Zhu", "authorId": "8652308"}], "n_citations": 1211}, "snippets": ["The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"], "score": 0.0}, {"id": "(Ye et al., 2024)", "paper": {"corpus_id": 273098639, "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Jiayi Ye", "authorId": "2325239327"}, {"name": "Yanbo Wang", "authorId": "2324066105"}, {"name": "Yue Huang", "authorId": "2324070910"}, {"name": "Dongping Chen", "authorId": "2279219833"}, {"name": "Qihui Zhang", "authorId": "46324457"}, {"name": "Nuno Moniz", "authorId": "2241357425"}, {"name": "Tian Gao", "authorId": "2324218620"}, {"name": "Werner Geyer", "authorId": "2324052753"}, {"name": "Chao Huang", "authorId": "2324171997"}, {"name": "Pin-Yu Chen", "authorId": "2279077171"}, {"name": "Nitesh V. Chawla", "authorId": "2286872295"}, {"name": "Xiangliang Zhang", "authorId": "2307963162"}], "n_citations": 78}, "snippets": ["LLM-as-a-Judge has been widely utilized as an evaluation method in various benchmarks and served as supervised rewards in model training. However, despite their excellence in many domains, potential issues are under-explored, undermining their reliability and the scope of their utility. Therefore, we identify 12 key potential biases and propose a new automated bias quantification framework-CALM-which systematically quantifies and analyzes each type of bias in LLM-as-a-Judge by using automated and principle-guided modification. Our experiments cover multiple popular language models, and the results indicate that while advanced models have achieved commendable overall performance, significant biases persist in certain specific tasks. Empirical results suggest that there remains room for improvement in the reliability of LLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence of these biases and give some suggestions for the reliable application of LLM-as-a-Judge. Our work highlights the need for stakeholders to address these issues and remind users to exercise caution in LLM-as-a-Judge applications."], "score": 0.0}], "table": null}, {"title": "Mitigation Strategies: Alternative Evaluator Approaches", "tldr": "To counter egocentric bias, researchers recommend using diverse evaluator models rather than relying solely on GPT-4 to assess its own outputs. These approaches include multi-model ensembles, peer discussions, and weighted evaluation strategies that together provide more balanced and comprehensive assessments. (7 sources)", "text": "\n- **Multiple LLM Judges**: Incorporating different LLM evaluators beyond just GPT-4 helps establish \"a more comprehensive and unbiased assessment\" of generated content. This approach reduces the risk of any single model's biases dominating the evaluation process. <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n- **Neutral Evaluator Selection**: Using LLMs that are independent of the assessed models' training data can mitigate potential data leakage risks that contribute to egocentric bias. This separation helps ensure more objective evaluations. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Evaluator Rotation**: Randomly rotating different evaluators during the assessment process helps reduce the influence of any single model's biases, creating a more balanced overall evaluation. <Paper corpusId=\"267760188\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>\n\n- **Peer Discussion Models**: Employing diverse LLMs as evaluators to foster peer discussions reduces preference for any specific LLM and enhances the robustness of evaluation outcomes. This approach helps counterbalance the self-preference tendencies of individual models. <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>\n\n- **Weighted Ensemble Evaluation**: When using multiple models for evaluation, adjusting the weight assigned to each model based on its perplexity for a given sample can help mitigate bias. Specifically, when a model shows low perplexity (high confidence) on a sample, reducing that model's evaluation weight for that sample can contribute to bias mitigation. <Paper corpusId=\"273661820\" paperTitle=\"(Wataoka et al., 2024)\" isShortName></Paper>\n\n- **Broader Evaluation Spectrum**: Addressing the \"chicken-and-egg dilemma\" requires employing a wider range of evaluation methods beyond just LLM-based assessment. This includes diverse benchmarks, varied evaluation criteria, and incorporating human feedback to ensure more balanced and comprehensive assessments. <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper>\n\n- **Cross-Model Evaluation**: Having models evaluate outputs from different models rather than their own can reduce the impact of egocentric bias, though this approach doesn't eliminate other forms of bias that might exist between different model families. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Human-AI Collaborative Evaluation**: Combining human evaluators with AI systems creates a hybrid approach that leverages the strengths of both while mitigating the biases inherent to each system alone. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [{"id": "(Wu et al., 2023)", "paper": {"corpus_id": 259360998, "title": "Style Over Substance: Evaluation Biases for Large Language Models", "year": 2023, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Minghao Wu", "authorId": "2145209409"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}], "n_citations": 47}, "snippets": ["This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."], "score": 0.552734375}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 267760188, "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Xiang Li", "authorId": "2261896751"}, {"name": "Yunshi Lan", "authorId": "2257016293"}, {"name": "Chao Yang", "authorId": "2268678836"}], "n_citations": 11}, "snippets": ["Using LLMs like GPT-4 as judges introduces potential data leakage risks due to biases in their pre-training data. This can be mitigated by selecting neutral evaluators independent of the assessed models' training data or randomly rotating evaluators to reduce bias."], "score": 0.51025390625}, {"id": "(Dai et al., 2024)", "paper": {"corpus_id": 269188154, "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era", "year": 2024, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Sunhao Dai", "authorId": "2155892801"}, {"name": "Chen Xu", "authorId": "2153078929"}, {"name": "Shicheng Xu", "authorId": "2202745"}, {"name": "Liang Pang", "authorId": "2263589454"}, {"name": "Zhenhua Dong", "authorId": "2297820120"}, {"name": "Jun Xu", "authorId": "2266437969"}], "n_citations": 82}, "snippets": ["The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."], "score": 0.84765625}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yang Liu", "authorId": "2152797401"}, {"name": "Dan Iter", "authorId": "3310951"}, {"name": "Yichong Xu", "authorId": "2110197273"}, {"name": "Shuo Wang", "authorId": "2146294891"}, {"name": "Ruochen Xu", "authorId": "8233965"}, {"name": "Chenguang Zhu", "authorId": "8652308"}], "n_citations": 1211}, "snippets": ["The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval"], "score": 0.0}, {"id": "(Wataoka et al., 2024)", "paper": {"corpus_id": 273661820, "title": "Self-Preference Bias in LLM-as-a-Judge", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Koki Wataoka", "authorId": "2007365532"}, {"name": "Tsubasa Takahashi", "authorId": "2325815191"}, {"name": "Ryokan Ri", "authorId": "1466451143"}], "n_citations": 25}, "snippets": ["This finding suggests a potential concern: using GPT-4 as a judge may lead to excessive influence from GPT-4's unique styles and policies.\n\nTo reduce self-preference bias, one possible approach is ensemble evaluation using multiple models. This method is expected to provide a more equitable evaluation by avoiding reliance on a single model. Specifically, when a model exhibits low perplexity on a sample, decreasing the weight assigned to that model's evaluation for that sample may contribute to bias mitigation. To evaluate the effectiveness of bias reduction strategies, our proposed new metric can be utilized."], "score": 0.70166015625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.6328125}, {"id": "(Sellam et al., 2020)", "paper": {"corpus_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Thibault Sellam", "authorId": "145450400"}, {"name": "Dipanjan Das", "authorId": "143790066"}, {"name": "Ankur P. Parikh", "authorId": "144729897"}], "n_citations": 1505}, "snippets": ["Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "score": 0.0}], "table": null}, {"title": "Limitations of Current Mitigation Approaches", "tldr": "Despite progress in mitigating egocentric bias, current approaches face significant limitations including increased computational costs, potential introduction of new biases, and the fundamental chicken-and-egg dilemma where evaluator development depends on more powerful models that themselves need evaluation. (5 sources)", "text": "\nWhile researchers have developed various strategies to address egocentric bias in GPT-4's self-evaluation, these approaches come with important limitations. One significant constraint is the computational and resource burden. As Dai et al. note, employing diverse LLMs as evaluators to foster peer discussions \"inevitably increases the evaluation costs,\" making widespread implementation challenging despite its effectiveness in reducing bias. <Paper corpusId=\"269188154\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\nAnother fundamental limitation is what Li et al. describe as the \"chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator.\" This circular problem creates a structural challenge for any mitigation strategy that relies solely on LLM-based evaluation approaches. <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\nSome mitigation techniques may inadvertently introduce new biases or exacerbate existing ones. For example, while the SoPro technique effectively reduces egocentric bias in GPT-3.5, researchers found that \"the models' susceptibility to bandwagon effect increased,\" suggesting that attempts to align LLMs with others' perspectives may create vulnerability to different cognitive biases. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\nEven the AwaRe prompting approach, which has shown promise in mitigating multiple biases including egocentric bias in GPT-3.5, demonstrates inconsistent performance across different models and bias types. It successfully addresses \"bandwagon effect and verbosity bias in GPT-4\" but doesn't uniformly mitigate all forms of bias across model generations. <Paper corpusId=\"274437478\" paperTitle=\"(Sumita et al., 2024)\" isShortName></Paper>\n\nResearchers also acknowledge that alternative approaches like human evaluation \"carry their own biases\" and may not represent a perfect solution. As Mullick et al. point out, \"using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities,\" but human alternatives may introduce different subjective biases. <Paper corpusId=\"270286247\" paperTitle=\"(Mullick et al., 2024)\" isShortName></Paper>\n\nThe limitations of current approaches underscore the need for what Li et al. describe as \"a broader spectrum of evaluation methods,\" involving various benchmarks, evaluation criteria, and human feedback \"to ensure more balanced and comprehensive assessments.\" <Paper corpusId=\"270391675\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"215548699\" paperTitle=\"(Sellam et al., 2020)\" isShortName></Paper> This suggests that no single mitigation strategy is sufficient, and that combinations of approaches tailored to specific contexts may be necessary to effectively address egocentric bias in GPT-4's self-evaluation.", "citations": [{"id": "(Dai et al., 2024)", "paper": {"corpus_id": 269188154, "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era", "year": 2024, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Sunhao Dai", "authorId": "2155892801"}, {"name": "Chen Xu", "authorId": "2153078929"}, {"name": "Shicheng Xu", "authorId": "2202745"}, {"name": "Liang Pang", "authorId": "2263589454"}, {"name": "Zhenhua Dong", "authorId": "2297820120"}, {"name": "Jun Xu", "authorId": "2266437969"}], "n_citations": 82}, "snippets": ["The emergence of egocentric bias introduces the risk of selfreinforcement for LLMs, particularly when they are further trained using rewards from LLM-based evaluations. This scenario can lead to LLMs overfitting to their own evaluation criteria, intensifying self-preference in next-generation model (Liu et al., 2023). Current strategies for mitigating egocentric bias primarily involve employing diverse LLMs as evaluators to foster peer discussions [79], thereby reducing the preference for any specific LLM and enhancing the robustness of evaluation outcomes. However, this strategy inevitably increases the evaluation costs. Future research must explore more efficient solutions to ensure fair and unbiased evaluation."], "score": 0.84765625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270391675, "title": "Leveraging Large Language Models for NLG Evaluation: Advances and Challenges", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhen Li", "authorId": "2145256331"}, {"name": "Xiaohan Xu", "authorId": "2279658967"}, {"name": "Tao Shen", "authorId": "2279548827"}, {"name": "Can Xu", "authorId": "2284826718"}, {"name": "Jia-Chen Gu", "authorId": "2308241851"}, {"name": "Yuxuan Lai", "authorId": "2308073132"}, {"name": "Chongyang Tao", "authorId": "2287928517"}, {"name": "Shuai Ma", "authorId": "2307142498"}], "n_citations": 15}, "snippets": ["The impartiality of such evaluations is questionable if the evaluator (LLM-as-evaluator) possesses capabilities comparable to the model being evaluated (LLM-as-generator). This is compounded by the egocentric bias of LLMs, including GPT-4, to exhibit biases like favoring their own generated responses (Bai et al., 2023). This scenario mirrors the chicken-and-egg dilemma: an LLM-based evaluator relies on a more powerful LLM, yet the development of a more powerful LLM depends on having a robust evaluator. To address this dilemma, a broader spectrum of evaluation methods is necessary, involving various benchmark (Srivastava et al., 2022; Liang et al., 2022), evaluation criteria (Sellam et al., 2020), and human feedback (Xu et al., 2023; Ouyang et al., 2022) to ensure more balanced and comprehensive assessments."], "score": 0.6328125}, {"id": "(Sumita et al., 2024)", "paper": {"corpus_id": 274437478, "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments", "year": 2024, "venue": "ACM Symposium on Applied Computing", "authors": [{"name": "Yasuaki Sumita", "authorId": "2333360231"}, {"name": "Koh Takeuchi", "authorId": "2243408877"}, {"name": "Hisashi Kashima", "authorId": "2247886893"}], "n_citations": 5}, "snippets": ["Although SoPro mitigates egocentric bias in GPT-3.5 and verbosity bias in GPT-4, the models' susceptibility to bandwagon effect increased. SoPro, which aligns LLMs with others' perspectives, may not be effective for cognitive biases. This result is inconsistent with the claim of the study for humans. AwaRe mitigates order bias, compassion fade, egocentric bias, bandwagon effect, and attentional bias in GPT-3.5, and bandwagon effect and verbosity bias in GPT-4. This result suggests that AwaRe prompts LLMs to make rational judgments."], "score": 0.71044921875}, {"id": "(Mullick et al., 2024)", "paper": {"corpus_id": 270286247, "title": "On The Persona-based Summarization of Domain-Specific Documents", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ankan Mullick", "authorId": "21724468"}, {"name": "Sombit Bose", "authorId": "2238668264"}, {"name": "Rounak Saha", "authorId": "2304954421"}, {"name": "Ayan Kumar Bhowmick", "authorId": "19181085"}, {"name": "Pawan Goyal", "authorId": "2261284157"}, {"name": "Niloy Ganguly", "authorId": "2261284171"}, {"name": "Prasenjit Dey", "authorId": "2287821944"}, {"name": "Ravi Kokku", "authorId": "2247701385"}], "n_citations": 3}, "snippets": ["It is acknowledged that using summaries generated exclusively by GPT-4 could introduce biases inherent in its summarization capabilities, it may also be noted that alternatives, such as human evaluation, also carry their own biases.Despite the potential for bias, leveraging GPT-4 for summarization may still be a pragmatic choice, especially in scenarios access to diverse datasets or sophisticated validation methods is limited.However, in this work, we remain vigilant, recognizing the limitations inherent in both automated and humangenerated summaries, and take proactive steps such as human intervention to validate and contextualise the results to mitigate biases to the best extent possible within the given constraints."], "score": 0.501953125}, {"id": "(Sellam et al., 2020)", "paper": {"corpus_id": 215548699, "title": "BLEURT: Learning Robust Metrics for Text Generation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Thibault Sellam", "authorId": "145450400"}, {"name": "Dipanjan Das", "authorId": "143790066"}, {"name": "Ankur P. Parikh", "authorId": "144729897"}], "n_citations": 1505}, "snippets": ["Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution."], "score": 0.0}], "table": null}], "cost": 0.31400400000000006}}
