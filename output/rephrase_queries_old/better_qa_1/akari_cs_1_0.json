{"better_query": "What are the comparative effects of FastText-based versus BERT-based classifiers on the quality and efficiency of pre-training data filtering for large language models?", "better_answer": {"sections": [{"title": "Introduction and Background", "tldr": "Pre-training data filtering is crucial for large language model (LLM) development, with FastText and BERT representing two fundamentally different approaches. FastText uses shallow word embeddings and linear classifiers while BERT leverages deep transformer architecture with contextual understanding. (LLM Memory)", "text": "\nPre-training data filtering is a critical step in the development pipeline of large language models (LLMs), directly impacting the quality, capabilities, and biases of the final models. As training datasets continue to grow into the trillions of tokens, efficient and effective filtering methods become increasingly important to maintain quality while managing computational resources. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFastText and BERT represent two fundamentally different approaches to text classification that are commonly used for data filtering tasks. FastText, developed by Facebook AI Research, uses shallow word embeddings and bag-of-words representations with a simple linear classifier. It treats words as atomic units and enriches them with subword information, making it particularly efficient for large-scale filtering tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nIn contrast, BERT (Bidirectional Encoder Representations from Transformers) employs a deep transformer-based architecture that creates contextual word representations based on surrounding text. This allows BERT to capture more nuanced semantic relationships and contextual meaning, potentially leading to more sophisticated filtering decisions. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe choice between these approaches presents significant tradeoffs. FastText offers computational efficiency and speed at the cost of contextual understanding, while BERT provides richer linguistic representations at the expense of computational resources. These tradeoffs become particularly significant when filtering the massive datasets required for modern LLM pre-training. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Performance Comparison", "tldr": "BERT generally outperforms FastText across most text classification tasks due to its contextual understanding capabilities, though FastText shows competitive performance in specific scenarios involving small datasets or when strong memorization is needed. (15 sources)", "text": "\nWhen directly comparing the classification performance of BERT-based and FastText-based models for data filtering tasks, multiple studies consistently show BERT's superior accuracy and F1 scores across diverse domains. BERT clearly outperforms FastText in nearly all measures, with one study noting that FastText sometimes fails to perform better than naive baselines <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>. This performance gap is particularly pronounced in tasks requiring deep contextual understanding, where BERT's transformer architecture excels.\n\nThe performance advantage of BERT stems primarily from its ability to generate context-dependent word embeddings. Unlike FastText, which produces the same representation for a word regardless of context, BERT can differentiate between multiple meanings of the same word based on surrounding text <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>. This capability is especially valuable for morphologically complex languages where word meaning depends heavily on context, with BERT showing superior vectorization compared to FastText for languages like Latvian, Lithuanian, and Russian <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nIn word sense disambiguation tasks, BERT-based models consistently achieve high F1 scores exceeding 90% in most cases, significantly outperforming FastText <Paper corpusId=\"232278102\" paperTitle=\"(Loureiro et al., 2021)\" isShortName></Paper>. Similarly, in software engineering domains, domain-adapted BERT models show approximately 11% higher F1 scores than FastText for issue type prediction tasks <Paper corpusId=\"237485425\" paperTitle=\"(Mosel et al., 2021)\" isShortName></Paper>.\n\nHowever, the performance gap narrows or even reverses in specific scenarios. With small training datasets, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. For certain financial datasets, FastText outperforms BERT entirely <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper> <Paper corpusId=\"209475786\" paperTitle=\"(Zhu et al., 2019)\" isShortName></Paper>.\n\nAn interesting pattern emerges in e-commerce search query classification, where FastText excels at memorization while BERT demonstrates better generalization. One study found that FastText performs better on test queries that appeared in the training set, while BERT handles unseen or low-frequency queries more effectively <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This suggests complementary strengths that could be leveraged in different filtering scenarios.\n\nThe precision-recall tradeoff also differs between models. BERT-based classifiers typically show higher precision (0.92 in one study) but lower recall (0.8), while FastText-based approaches demonstrate superior recall (0.93) <Paper corpusId=\"246608158\" paperTitle=\"(Ivanov et al., 2022)\" isShortName></Paper>. This characteristic makes FastText potentially more valuable for applications where maximizing the extraction of relevant content is prioritized over precision.\n\nFor bias and discrimination detection, random forest classifiers with BERT word embeddings achieved the best performance, outperforming those using non-contextual embeddings like FastText <Paper corpusId=\"253015767\" paperTitle=\"(Frissen et al., 2022)\" isShortName></Paper>. In classification of unseen data, BERT outperformed FastText by 9.4% in F1 score <Paper corpusId=\"253860591\" paperTitle=\"(Melnyk et al., 2022)\" isShortName></Paper>, further highlighting its superior generalization capabilities.\n\nFor essay answer correction, BERT-based approaches demonstrate good performance compared to traditional methods using FastText word embeddings <Paper corpusId=\"275704171\" paperTitle=\"(Sani, 2024)\" isShortName></Paper>. In more challenging classification tasks, BERT combined with dimension reduction techniques like PCA demonstrates the highest performance metrics <Paper corpusId=\"277940080\" paperTitle=\"(Abdelmotaleb et al., 2025)\" isShortName></Paper>.\n\nWhile BERT generally outperforms FastText in accuracy, it's worth noting that in some specialized domains like clinical text analysis, pre-trained BERT models without domain adaptation don't show marked advantages over simpler approaches <Paper corpusId=\"269973968\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>, emphasizing the importance of domain adaptation for optimal performance.", "citations": [{"id": "(Breit et al., 2020)", "paper": {"corpus_id": 216914042, "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Anna Breit", "authorId": "1453876854"}, {"name": "Artem Revenko", "authorId": "144688662"}, {"name": "Kiamehr Rezaee", "authorId": "1667035673"}, {"name": "Mohammad Taher Pilehvar", "authorId": "1717641"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}], "n_citations": 26}, "snippets": ["As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true", "Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes."], "score": 0.7080078125}, {"id": "(Hussain et al., 2021)", "paper": {"corpus_id": 237253665, "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM", "year": 2021, "venue": "Computational and Mathematical Methods in Medicine", "authors": [{"name": "Sajid Hussain", "authorId": "2113619919"}, {"name": "H. Afzal", "authorId": "1777100"}, {"name": "Ramsha Saeed", "authorId": "5644298"}, {"name": "N. Iltaf", "authorId": "2740595"}, {"name": "M. Umair", "authorId": "33827057"}], "n_citations": 24}, "snippets": ["BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word."], "score": 0.71142578125}, {"id": "(Kapociute-Dzikiene et al., 2020)", "paper": {"corpus_id": 226337596, "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization", "year": 2020, "venue": "Applied Sciences", "authors": [{"name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117", "authorId": "1403992181"}, {"name": "K. Balodis", "authorId": "3288489"}, {"name": "Raivis Skadins", "authorId": "3283640"}], "n_citations": 13}, "snippets": ["As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."], "score": 0.689453125}, {"id": "(Loureiro et al., 2021)", "paper": {"corpus_id": 232278102, "title": "Analysis and Evaluation of Language Models for Word Sense Disambiguation", "year": 2021, "venue": "International Conference on Computational Logic", "authors": [{"name": "Daniel Loureiro", "authorId": "144653901"}, {"name": "Kiamehr Rezaee", "authorId": "1667035673"}, {"name": "Mohammad Taher Pilehvar", "authorId": "1717641"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}], "n_citations": 72}, "snippets": ["We also include two FastText linear classifiers (Joulin et al., 2016)) as baselines: FTX-B (base model without pretrained embeddings) and FTX-C (using pretrained embeddings from Common Crawl). We chose FastText as the baseline given its efficiency and competitive results for sentence classification", "In general, results are high for all Transformerbased models, over 90% in most cases. This reinforces the potential of language models for WSD, both in its light-weight 1NN and in the fine-tuning settings", "Most Frequent Sense (MFS) bias. As expected, macro-F1 results degrade for the purely supervised classification models (FastText and fine-tuned BERT), indicating the inherent sense bias", "In addition to our BERT-based model, we include results for two Fast-Text supervised classifiers (Joulin et al., 2016)) as baselines: a basic one with random initialization (FastText-B) and another initialized with FastText embeddings trained on the Common Crawl (FastText-C)", "We can see that BERT significantly outperforms the FastText static word embedding."], "score": 0.67626953125}, {"id": "(Mosel et al., 2021)", "paper": {"corpus_id": 237485425, "title": "On the Validity of Pre-Trained Transformers for Natural Language Processing in the Software Engineering Domain", "year": 2021, "venue": "IEEE Transactions on Software Engineering", "authors": [{"name": "Julian von der Mosel", "authorId": "2214761830"}, {"name": "Alexander Trautsch", "authorId": "8058979"}, {"name": "Steffen Herbold", "authorId": "3063461"}], "n_citations": 68}, "snippets": ["The results show that seBERT and BERToverflow achieve the best performance for the issue type prediction tasks, outperforming both fastText and the general-domain BERT models. The improvement over fastText is very large with an about 11% higher F1 score for the issue type prediction. The violin plot in Figure 5 indicates that the performance improvement in F1 score is due to an improvement of both recall and precision, which means the models reduced both false positives and false negatives in comparison to fastText. The Bayesian signed rank test determined that this improvement of the SE-specific models over the other models is significant."], "score": 0.703125}, {"id": "(Edwards et al., 2020)", "paper": {"corpus_id": 227231089, "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification", "year": 2020, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "A. Edwards", "authorId": "1383074767"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}, {"name": "H\u00e9l\u00e8ne de Ribaupierre", "authorId": "2750681"}, {"name": "A. Preece", "authorId": "1762890"}], "n_citations": 25}, "snippets": ["In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."], "score": 0.9541015625}, {"id": "(Wildemann et al., 2023)", "paper": {"corpus_id": 265162335, "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking", "year": 2023, "venue": "Social science computer review", "authors": [{"name": "Sergej Wildemann", "authorId": "3041146"}, {"name": "Claudia Nieder\u00e9e", "authorId": "2257974973"}, {"name": "Erick Elejalde", "authorId": "2098749"}], "n_citations": 0}, "snippets": ["Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank", "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."], "score": 0.71484375}, {"id": "(Zhu et al., 2019)", "paper": {"corpus_id": 209475786, "title": "FreeLB: Enhanced Adversarial Training for Natural Language Understanding", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chen Zhu", "authorId": "1431754650"}, {"name": "Yu Cheng", "authorId": "145215470"}, {"name": "Zhe Gan", "authorId": "144702900"}, {"name": "S. Sun", "authorId": "2419809"}, {"name": "T. Goldstein", "authorId": "1962083"}, {"name": "Jingjing Liu", "authorId": "46700348"}], "n_citations": 442}, "snippets": ["Adversarial training, which minimizes the maximal risk for label-preserving input perturbations, has proved to be effective for improving the generalization of language models. In this work, we propose a novel adversarial training algorithm, FreeLB, that promotes higher invariance in the embedding space, by adding adversarial perturbations to word embeddings and minimizing the resultant adversarial risk inside different regions around input samples. To validate the effectiveness of the proposed approach, we apply it to Transformer-based models for natural language understanding and commonsense reasoning tasks. Experiments on the GLUE benchmark show that when applied only to the finetuning stage, it is able to improve the overall test scores of BERT-base model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8. In addition, the proposed approach achieves state-of-the-art single-model test accuracies of 85.44\\% and 67.75\\% on ARC-Easy and ARC-Challenge. Experiments on CommonsenseQA benchmark further demonstrate that FreeLB can be generalized and boost the performance of RoBERTa-large model on other tasks as well. Code is available at \\url{this https URL ."], "score": 0.0}, {"id": "(Ning et al., 2023)", "paper": {"corpus_id": 260379057, "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kun-Peng Ning", "authorId": "66266326"}, {"name": "Ming Pang", "authorId": "2053434618"}, {"name": "Zheng Fang", "authorId": "2072874946"}, {"name": "Xue Jiang", "authorId": "2226458237"}, {"name": "Xi-Wei Zhao", "authorId": "2226511841"}, {"name": "Changping Peng", "authorId": "1949218014"}, {"name": "Zhangang Lin", "authorId": "2146396439"}, {"name": "Jinghe Hu", "authorId": "22528137"}, {"name": "Jingping Shao", "authorId": "2118926502"}], "n_citations": 0}, "snippets": ["We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."], "score": 0.91552734375}, {"id": "(Ivanov et al., 2022)", "paper": {"corpus_id": 246608158, "title": "Extracting Software Requirements from Unstructured Documents", "year": 2022, "venue": "International Joint Conference on the Analysis of Images, Social Networks and Texts", "authors": [{"name": "V. Ivanov", "authorId": "2072422334"}, {"name": "Andrey Sadovykh", "authorId": "2883654"}, {"name": "Alexandr Naumchev", "authorId": "3361049"}, {"name": "A. Bagnato", "authorId": "36000045"}, {"name": "K. Yakovlev", "authorId": "2117465706"}], "n_citations": 11}, "snippets": ["Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements."], "score": 0.775390625}, {"id": "(Frissen et al., 2022)", "paper": {"corpus_id": 253015767, "title": "A machine learning approach to recognize bias and discrimination in job advertisements", "year": 2022, "venue": "Ai & Society", "authors": [{"name": "Richard Frissen", "authorId": "2188338310"}, {"name": "K. Adebayo", "authorId": "2848544"}, {"name": "Rohan Nanda", "authorId": "39418246"}], "n_citations": 19}, "snippets": ["The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec. We also observe that tree-based (Random Forest and Decision Tree) classifiers had a better performance in classifying biased and discriminatory language as compared to the remaining classifiers. Among the textual features, word embedding representations BERT, FastText and ELMo in combination with the RF classifier had the best performance. This was followed by FastText, ELMo and Flair word embeddings in combination with the DT classifier."], "score": 0.77197265625}, {"id": "(Melnyk et al., 2022)", "paper": {"corpus_id": 253860591, "title": "Sentiment Analysis and Stance Detection on German YouTube Comments on Gender Diversity", "year": 2022, "venue": "Journal of Computer-Assisted Linguistic Research", "authors": [{"name": "Lidiia Melnyk", "authorId": "78466463"}, {"name": "Linda Feld", "authorId": "2192124742"}], "n_citations": 1}, "snippets": ["the BERT model scored better in all of these settings and in classifying unseen data, reaching an F1 score of 0.80 and outperforming FastText by 9.4%."], "score": 0.78955078125}, {"id": "(Sani, 2024)", "paper": {"corpus_id": 275704171, "title": "A Random Oversampling and BERT-based Model Approach for Handling Imbalanced Data in Essay Answer Correction", "year": 2024, "venue": "Jurnal Infotel", "authors": [{"name": "Dian Ahkam Sani", "authorId": "2341084544"}], "n_citations": 0}, "snippets": ["In this paper, we propose a novel approach that combines random oversampling with a BERT-base uncased model for essay answer correction. This research explores various scenario of text pre-processing techniques to optimize model accuracy. Using a dataset of essay answers obtained from eighth-grade middle school students in Indonesian language, our approach demonstrates good performance in terms of precision, recall, F1-score and accuracy compared to traditional methods such as Backpropagation Neural Network, Na\u00efve Bayes and Random Forest Classifier using FastText word embedding with Wikipedia 300 vector size pretrained model. The best performance was obtained using the BERT-base uncased model with 2e-5 learning rate and a simplified pre-processing approach."], "score": 0.65869140625}, {"id": "(Abdelmotaleb et al., 2025)", "paper": {"corpus_id": 277940080, "title": "Word Embedding Techniques for Classification of Star Ratings", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hesham Abdelmotaleb", "authorId": "2233683143"}, {"name": "Craig McNeile", "authorId": "2342295801"}, {"name": "Ma\u0142gorzata Wojty\u015b", "authorId": "2273117131"}], "n_citations": 0}, "snippets": ["Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average", "Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding", "for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics."], "score": 0.67138671875}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 269973968, "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data", "year": 2024, "venue": "Health Informatics Journal", "authors": [{"name": "Jingna Xie", "authorId": "2302859526"}, {"name": "Yingshuo Wang", "authorId": "2364687744"}, {"name": "Qiuyang Sheng", "authorId": "1576956902"}, {"name": "Xiaoqing Liu", "authorId": "2283403594"}, {"name": "Jing Li", "authorId": "2336703688"}, {"name": "Fenglei Sun", "authorId": "47450065"}, {"name": "Yuqi Wang", "authorId": "2155376102"}, {"name": "Shuxian Li", "authorId": "2302784979"}, {"name": "Yiming Li", "authorId": "2110531396"}, {"name": "Yizhou Yu", "authorId": "2302875109"}, {"name": "Gang Yu", "authorId": "2303601854"}], "n_citations": 1}, "snippets": ["When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities", "Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities."], "score": 0.69921875}], "table": null}, {"title": "Computational Efficiency", "tldr": "FastText significantly outperforms BERT in computational efficiency, requiring fewer resources and faster training/inference times, making it more suitable for real-time applications and resource-constrained environments despite its generally lower accuracy. (7 sources)", "text": "\nWhen considering computational efficiency, FastText and BERT represent opposite ends of the spectrum in the tradeoff between performance and resource requirements. FastText consistently demonstrates superior computational efficiency compared to transformer-based models like BERT across multiple studies. Classic word embedding models including FastText learn representations significantly faster than transformer-based models due to their simpler architecture <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>. Even when compared to other traditional word embeddings like Skip-gram, FastText takes slightly more time due to its processing of subword information, but still remains dramatically faster than BERT variants <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nThe resource requirements between these approaches differ substantially. FastText has been shown to perform well with minimal computational resources in standard server environments, while BERT-based models demand significantly more computational power <Paper corpusId=\"244895506\" paperTitle=\"(Durairaj et al., 2021)\" isShortName></Paper>. This difference becomes particularly important in real-time applications, where FastText's efficiency enables practical deployment in scenarios where BERT would be prohibitively expensive or slow <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nThe efficiency advantage of FastText is so significant that even lightweight BERT variants like DistilBERT, while faster than BERT, still exceed practical time constraints for real-time processing applications <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>. For example, processing tweets from a 2-minute window required approximately 7.2 minutes with DistilBERT, making it unsuitable for real-time analysis <Paper corpusId=\"219558245\" paperTitle=\"(Hettiarachchi et al., 2020)\" isShortName></Paper>.\n\nIn terms of model size and inference speed, studies have shown that BERT-based models can have up to 160% more parameters than competitive FastText-based architectures, resulting in significantly higher inference times <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>. This performance-runtime tradeoff becomes particularly important in production environments where both accuracy and speed are critical considerations <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>.\n\nEnergy consumption analysis further highlights this efficiency gap, with FastText being among the most computationally demanding embedding techniques while still requiring far fewer resources than BERT <Paper corpusId=\"277940080\" paperTitle=\"(Abdelmotaleb et al., 2025)\" isShortName></Paper>. Despite transformer-based classifiers and BERT significantly outperforming FastText in accuracy, they are \"significantly slower, and require significantly more compute\" <Paper corpusId=\"274464991\" paperTitle=\"(Mansour et al., 2024)\" isShortName></Paper>.\n\nThe computational efficiency advantage of FastText has made it an attractive candidate for knowledge distillation approaches, where a more powerful but computationally expensive model like BERT or ERNIE serves as a teacher to improve FastText's accuracy while maintaining its computational advantages <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>. This approach \"guarantees the superiority of its original computational performance\" while significantly improving classification accuracy <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n\nFor many practical applications, FastText's combination of reasonable performance and computational efficiency makes it an attractive option, particularly when GPU resources are limited or unavailable <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>. Its fast training times and lack of dependency on specialized hardware create significant practical advantages that may outweigh the performance benefits of BERT in resource-constrained environments <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Hettiarachchi et al., 2020)", "paper": {"corpus_id": 219558245, "title": "Embed2Detect: temporally clustered embedded words for event detection in social media", "year": 2020, "venue": "Machine-mediated learning", "authors": [{"name": "Hansi Hettiarachchi", "authorId": "1453653937"}, {"name": "Mariam Adedoyin-Olowe", "authorId": "1403921831"}, {"name": "Jagdev Bhogal", "authorId": "3181018"}, {"name": "M. Gaber", "authorId": "1698684"}], "n_citations": 33}, "snippets": ["According to the obtained results, classic word embedding models (e.g. Skip-gram and fastText) learn the representations faster than transformer-based models (e.g. BERT and DistilBERT). \n\nComparing fastText and Skip-gram, fastText took more time because it processes subword information. But, incorporation of subwords allows this model to capture connections between modified words. For example, consider the goal-related words found within the top 20 words with high cluster change during a goal score: Skip-gramgoal, goalll, rashyyy, scores fastTextgoalll, goooaaalll, rashford, rashyyy, @marcusrashford, scored, scores fastText captures more modified words than Skip-gram. We could not run a complete evaluation using fastText embeddings, because it requires a manual process since GT keywords only contain the words in actual form. \n\nTransformer-based models took more time than both Skip-gram and fastText due to their complex architecture to learn contextualised word embeddings. DistilBERT is found to be faster than BERT, however, the learning time of DistilBERT is not fast enough for real-time processing because it exceeds the tweet generation time. For example to learn from tweets posted during a 2-minute time window, it took approximately 7.2 minutes. If this model can be further distilled, there is a possibility to achieve the required efficiency to become suitable for real-time processing. However, further distillation can reduce the language understanding capability of the model as there is a 3% reduction in DistilBERT compared to BERT (Sanh et al. 2019)."], "score": 0.7333984375}, {"id": "(Durairaj et al., 2021)", "paper": {"corpus_id": 244895506, "title": "Transformer based Contextual Model for Sentiment Analysis of Customer Reviews: A Fine-tuned BERT", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "A. Durairaj", "authorId": "90290875"}, {"name": "Anandan Chinnalagu", "authorId": "2048021830"}], "n_citations": 29}, "snippets": ["The proposed BERT model outperforms in terms of accuracy and model performance compare to other models. The results of the fastText model showed low accuracy when unigram and bigram methods were used for training the model. The overall model training and data preparation tasks took less time for BERT model in comparison to others. This experiment reveals that the BERT model required more computational resources to train compared with other traditional models. The fastText model performed well with a standard server environment with minimal computational resources compare to other models."], "score": 0.7607421875}, {"id": "(Groger et al., 2022)", "paper": {"corpus_id": 252901000, "title": "Assessing Guest Nationality Composition from Hotel Reviews", "year": 2022, "venue": "Swiss Conference on Data Science", "authors": [{"name": "Fabian Gr\u00f6ger", "authorId": "2026999646"}, {"name": "M. Pouly", "authorId": "1715783"}, {"name": "Flavia Tinner", "authorId": "2187827840"}, {"name": "Leif Brandes", "authorId": "39589520"}], "n_citations": 0}, "snippets": ["The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff."], "score": 0.76904296875}, {"id": "(Abdelmotaleb et al., 2025)", "paper": {"corpus_id": 277940080, "title": "Word Embedding Techniques for Classification of Star Ratings", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hesham Abdelmotaleb", "authorId": "2233683143"}, {"name": "Craig McNeile", "authorId": "2342295801"}, {"name": "Ma\u0142gorzata Wojty\u015b", "authorId": "2273117131"}], "n_citations": 0}, "snippets": ["Regarding feature engineering, for Word2Vec and FastText, our proposed PCA approach of combining word vectors using the first principal component shows clear advantages in performance over the traditional approach of taking the average", "Additionally, energy consumption analysis revealed that computational efficiency varies significantly among embedding techniques, with TF-IDF being the most resource-efficient and FastText the most computationally de-manding", "for the more challenging classification tasks, BERT combined with PCA stood out with the highest performance metrics."], "score": 0.67138671875}, {"id": "(Mansour et al., 2024)", "paper": {"corpus_id": 274464991, "title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Youssef Mansour", "authorId": "2294362620"}, {"name": "Reinhard Heckel", "authorId": "2294361310"}], "n_citations": 2}, "snippets": ["We plot FastText's performance as a function of the number of training sequences in Figure 9. The transformer-based classifier and BERT significantly outperform FastText, but are significantly slower, and require significantly more compute."], "score": 0.8291015625}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 246738423, "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition", "year": 2022, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Shiguang Guo", "authorId": "2119112458"}, {"name": "Qing Wang", "authorId": "2117944486"}], "n_citations": 8}, "snippets": ["In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved."], "score": 0.6806640625}, {"id": "(Wildemann et al., 2023)", "paper": {"corpus_id": 265162335, "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking", "year": 2023, "venue": "Social science computer review", "authors": [{"name": "Sergej Wildemann", "authorId": "3041146"}, {"name": "Claudia Nieder\u00e9e", "authorId": "2257974973"}, {"name": "Erick Elejalde", "authorId": "2098749"}], "n_citations": 0}, "snippets": ["Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank", "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."], "score": 0.71484375}], "table": null}, {"title": "Contextual Understanding Capabilities", "tldr": "BERT's advanced contextual understanding capabilities enable it to effectively handle polysemy and contextual meaning variations, while FastText produces static word embeddings regardless of context. This fundamental difference makes BERT significantly more powerful for languages with complex morphology and free word order. (7 sources)", "text": "\nThe most significant advantage of BERT over FastText lies in its sophisticated contextual understanding capabilities. While FastText produces static word embeddings where each word has a single representation regardless of its usage context, BERT generates dynamic, context-dependent embeddings that can represent different meanings of the same word based on surrounding text <Paper corpusId=\"227230531\" paperTitle=\"(Baruah et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>.\n\nThis contextual awareness makes BERT particularly effective at handling polysemy\u2014words with multiple potential meanings. Traditional embedding approaches like Word2Vec, GloVe, and FastText fail to capture these semantic nuances since they produce identical representations for a word regardless of its contextual meaning <Paper corpusId=\"237253665\" paperTitle=\"(Hussain et al., 2021)\" isShortName></Paper>. BERT, however, directly encodes word meanings based on contextual information through its transformer architecture and attention mechanisms, effectively addressing the polysemy issue <Paper corpusId=\"266566817\" paperTitle=\"(Bo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"239459434\" paperTitle=\"(Xinxi, 2021)\" isShortName></Paper>.\n\nFor morphologically complex languages, BERT's contextual understanding provides even greater advantages. Languages with fusional morphology often suffer from disambiguation problems that BERT can handle effectively. Unlike FastText, which considers only a limited context of a few surrounding words, BERT processes a much broader context including words, sentences, and their order <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>. This capability makes BERT significantly more suitable for languages with relatively free word order such as Latvian, Lithuanian, and Russian, while FastText remains adequate primarily for languages with strict word order like English <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nThe fundamental architectural differences between these approaches reflect their distinct capabilities in handling contextual meaning. FastText relies on subword information to address out-of-vocabulary terms, while contextualized models like BERT and its variants (such as RoBERTa) learn word contexts from large-scale text corpora <Paper corpusId=\"272359317\" paperTitle=\"(Malik et al., 2024)\" isShortName></Paper>. This more sophisticated approach to context awareness explains why BERT-based embeddings consistently generate better results than FastText across diverse datasets <Paper corpusId=\"238648863\" paperTitle=\"(Jbene et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Baruah et al., 2020)", "paper": {"corpus_id": 227230531, "title": "IIITG-ADBU at SemEval-2020 Task 12: Comparison of BERT and BiLSTM in Detecting Offensive Language", "year": 2020, "venue": "International Workshop on Semantic Evaluation", "authors": [{"name": "Arup Baruah", "authorId": "48140293"}, {"name": "K. Das", "authorId": "2006187174"}, {"name": "F. Barbhuiya", "authorId": "2568004"}, {"name": "K. Dey", "authorId": "144710196"}], "n_citations": 2}, "snippets": ["The word embeddings produced by fastText is static in nature. Each word has a single embedding irrespective of the context in which the word appears. Static embeddings fail to handle polysemy. The embeddings produced by BERT are contextualized embeddings. The same word may have multiple embeddings depending on the context in which it appears."], "score": 0.6982421875}, {"id": "(Hussain et al., 2021)", "paper": {"corpus_id": 237253665, "title": "Pharmacovigilance with Transformers: A Framework to Detect Adverse Drug Reactions Using BERT Fine-Tuned with FARM", "year": 2021, "venue": "Computational and Mathematical Methods in Medicine", "authors": [{"name": "Sajid Hussain", "authorId": "2113619919"}, {"name": "H. Afzal", "authorId": "1777100"}, {"name": "Ramsha Saeed", "authorId": "5644298"}, {"name": "N. Iltaf", "authorId": "2740595"}, {"name": "M. Umair", "authorId": "33827057"}], "n_citations": 24}, "snippets": ["BERT outperforms both CNN and LSTM. The reason for the better performance of BERT is that it learns contextualized embeddings in a bidirectional way. In natural language, a word is likely to convey multiple meanings based on the context in which it is used. Word2vec, fasttext, and glove produce the same representations of a word even if it has different meanings in different contexts. BERT, on the other hand, produces context-dependent embeddings of a word."], "score": 0.71142578125}, {"id": "(Bo et al., 2024)", "paper": {"corpus_id": 266566817, "title": "Empowering Medical Data Analysis: An Advanced Deep Fusion Model for Sorting Medicine Document", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Guan Bo", "authorId": "2276550856"}, {"name": "Shanshan Wang", "authorId": "2276574822"}, {"name": "Zhang Qing", "authorId": "2276553236"}, {"name": "Pang Bo", "authorId": "2276554085"}, {"name": "Zuo Yan", "authorId": "2276746559"}], "n_citations": 2}, "snippets": ["In contrast, BERT is a pre-trained language model trained on a large-scale corpus, based on the multi-layer Transformer encoder architecture, utilizing attention mechanisms to directly encode word meanings, effectively addressing the polysemy issue based on contextual information (Xinxi, 2021). BERT places greater emphasis on pre-training word meanings, allowing downstream NLP tasks to perform fine-tuning operations based on the specific task's requirements."], "score": 0.82958984375}, {"id": "(Xinxi, 2021)", "paper": {"corpus_id": 239459434, "title": "Single task fine-tune BERT for text classification", "year": 2021, "venue": "Other Conferences", "authors": [{"name": "Z. Xinxi", "authorId": "2096537143"}], "n_citations": 5}, "snippets": ["Over the past decades, natural language processing (NLP) has been a hot topic in many fields, e.g., sentiment analysis and news topic classification. As a very powerful language pre-training model, Bidirectional Encoder Representations from Transformers (BERT) has achieved promising results in many language understanding tasks including text classification. However, fine-tune BERT to adapt different text classification task efficiently is a critical problem that needs improvement. In this paper, a general solution is proposed for BERT fine-tuning on single text classification task. Compared with other traditional fine-tune strategies without any pre-training step, the performance of BERT is boosted by pre-training withintask data. Moreover, the proposed solution obtains superior results on six widely-used text classification datasets."], "score": 0.0}, {"id": "(Kapociute-Dzikiene et al., 2020)", "paper": {"corpus_id": 226337596, "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization", "year": 2020, "venue": "Applied Sciences", "authors": [{"name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117", "authorId": "1403992181"}, {"name": "K. Balodis", "authorId": "3288489"}, {"name": "Raivis Skadins", "authorId": "3283640"}], "n_citations": 13}, "snippets": ["As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."], "score": 0.689453125}, {"id": "(Malik et al., 2024)", "paper": {"corpus_id": 272359317, "title": "Attention-aware with stacked embedding for sentiment analysis of student feedback through deep learning techniques", "year": 2024, "venue": "PeerJ Computer Science", "authors": [{"name": "Shanza Zafar Malik", "authorId": "2327353221"}, {"name": "Khalid Iqbal", "authorId": "2294254945"}, {"name": "Muhammad Sharif", "authorId": "2319267821"}, {"name": "Yaser Ali Shah", "authorId": "2151313189"}, {"name": "Amaad Khalil", "authorId": "2060253868"}, {"name": "Muhammad Abeer Irfan", "authorId": "2327345984"}, {"name": "Joann Rosak-Szyrocka", "authorId": "2316178914"}], "n_citations": 1}, "snippets": ["In the proposed work, three important issues in the field of natural language processing: polysemy, contextual meaning, and out-of-vocabulary terms were addressed. We employed three cutting-edge models to solve these problems: FastText, Elmo, and RoBERTa. \n\nFastText is a text categorization and representation learning library that is intended to be quick and efficient. It handles out-of-vocabulary terms using sub-word information, making it appropriate for NLP jobs. Embeddings from Language Models (ELMO) is a contextualized word representation model that learns word contexts from large-scale text corpora. It captures the contextual meaning of words, making it ideally suited for NLP jobs requiring context awareness. A Robustly Optimized BERT Pre-training Approach (Roberta) is a transformer-based language model that has been refined using large-scale text corpora."], "score": 0.7294921875}, {"id": "(Jbene et al., 2021)", "paper": {"corpus_id": 238648863, "title": "Deep Neural Network and Boosting Based Hybrid Quality Ranking for e-Commerce Product Search", "year": 2021, "venue": "Big Data and Cognitive Computing", "authors": [{"name": "Mourad Jbene", "authorId": "1471736056"}, {"name": "Smail Tigani", "authorId": "3377410"}, {"name": "Rachid Saadane", "authorId": "1916966"}, {"name": "Abdellah Chehri", "authorId": "2782896"}], "n_citations": 6}, "snippets": ["Comparing the results of the model using the different kinds of embeddings, the version with BERT embeddings generates results higher than FastText ones for all the datasets, which can be explained by the difference in dataset size on which the two models were pre-trained on, and most importantly, the capability of BERT in capturing and generating context-dependent word embeddings."], "score": 0.84765625}], "table": null}, {"title": "Domain Adaptation and Generalization", "tldr": "BERT demonstrates superior domain adaptation capabilities compared to FastText, showing more robust performance when transferring between general and specific domains. FastText excels at memorization of seen data while BERT shows stronger generalization to unseen examples, creating complementary strengths for different filtering applications. (5 sources)", "text": "\nWhen comparing domain adaptation and generalization capabilities, BERT and FastText show significant differences that affect their suitability for data filtering applications. BERT consistently demonstrates superior robustness when adapting to new domains, while FastText struggles significantly when transferring between general and specific domains <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>. This domain adaptation gap is substantial enough that in some studies, FastText failed to outperform even naive baselines when applied to new domains <Paper corpusId=\"216914042\" paperTitle=\"(Breit et al., 2020)\" isShortName></Paper>.\n\nThe generalization capabilities of these models reveal complementary strengths. In e-commerce search query classification, FastText exhibits stronger memorization abilities, performing better on test queries that had appeared in the training set. In contrast, BERT shows superior generalization to unseen or low-frequency queries <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This pattern suggests FastText excels at \"remembering\" while BERT excels at \"generalizing,\" which creates different use cases depending on whether the filtering task prioritizes recognizing previously seen patterns or adapting to novel content.\n\nFor languages with complex morphology and relatively free word order (such as Latvian, Lithuanian, and Russian), BERT's generalization capabilities prove particularly valuable. Its ability to process broader contextual information makes it substantially more suitable for these languages compared to FastText, which performs adequately primarily for languages with strict word order like English <Paper corpusId=\"226337596\" paperTitle=\"(Kapociute-Dzikiene et al., 2020)\" isShortName></Paper>.\n\nHowever, the generalization advantage of BERT diminishes in certain scenarios. With small training datasets, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This suggests that in data-constrained environments, the simpler FastText approach may offer more reliable generalization.\n\nIn specialized domains like clinical text analysis, pre-trained BERT models without domain adaptation show limited advantages over simpler approaches. Studies have found that in clinical text classification, standard BERT without domain-specific pre-training can be outperformed by simpler approaches like TextCNN and doesn't demonstrate marked advantages over Word2Vec-based methods <Paper corpusId=\"269973968\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. This highlights the critical importance of domain adaptation for BERT to fully leverage its capabilities in specialized fields.\n\nThese findings suggest that optimal pre-training data filtering may involve a hybrid approach\u2014using FastText for efficiency in recognizing common patterns and BERT for its superior generalization to novel or ambiguous content requiring deeper contextual understanding.", "citations": [{"id": "(Breit et al., 2020)", "paper": {"corpus_id": 216914042, "title": "WiC-TSV: An Evaluation Benchmark for Target Sense Verification of Words in Context", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Anna Breit", "authorId": "1453876854"}, {"name": "Artem Revenko", "authorId": "144688662"}, {"name": "Kiamehr Rezaee", "authorId": "1667035673"}, {"name": "Mohammad Taher Pilehvar", "authorId": "1717641"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}], "n_citations": 26}, "snippets": ["As can be observed, BERT is clearly better than FastText in all measures. In fact, perhaps surprisingly, FastText does not perform better than a naive baseline that retrieves all instances as true", "Interestingly, FastText faces a massive challenge in adapting domains and generalising from the general to the specific domains. However, BERT shows to be much more robust to domain changes."], "score": 0.7080078125}, {"id": "(Ning et al., 2023)", "paper": {"corpus_id": 260379057, "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kun-Peng Ning", "authorId": "66266326"}, {"name": "Ming Pang", "authorId": "2053434618"}, {"name": "Zheng Fang", "authorId": "2072874946"}, {"name": "Xue Jiang", "authorId": "2226458237"}, {"name": "Xi-Wei Zhao", "authorId": "2226511841"}, {"name": "Changping Peng", "authorId": "1949218014"}, {"name": "Zhangang Lin", "authorId": "2146396439"}, {"name": "Jinghe Hu", "authorId": "22528137"}, {"name": "Jingping Shao", "authorId": "2118926502"}], "n_citations": 0}, "snippets": ["We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."], "score": 0.91552734375}, {"id": "(Kapociute-Dzikiene et al., 2020)", "paper": {"corpus_id": 226337596, "title": "Intent Detection Problem Solving via Automatic DNN Hyperparameter Optimization", "year": 2020, "venue": "Applied Sciences", "authors": [{"name": "J. Kapo\u010di\u016bt\u0117-Dzikien\u0117", "authorId": "1403992181"}, {"name": "K. Balodis", "authorId": "3288489"}, {"name": "Raivis Skadins", "authorId": "3283640"}], "n_citations": 13}, "snippets": ["As seen from Figure 2, BERT vectorization is a better choice compared to fastText for all morphologically complex languages for all datasets, and this is not surprising. Morphologically complex languages (especially fusional languages) suffer from disambiguation problems, but BERT has mechanisms that are able to vectorize even those words that are written the same but have different meanings, depending on their context, differently. Despite the fact that fastText embeddings are also trained to consider a context around the target word, that context is restricted to only a few words. Despite this, fastText is a suitable vectorization solution for languages (such as English) with strict word order in a sentence. In contrast, BERT is able to consider a much broader context (words, sentences, their order) compared to fastText and is, therefore, more suitable for languages that have a relatively free word order in a sentence (such as Latvian, Lithuanian, and Russian)."], "score": 0.689453125}, {"id": "(Edwards et al., 2020)", "paper": {"corpus_id": 227231089, "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification", "year": 2020, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "A. Edwards", "authorId": "1383074767"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}, {"name": "H\u00e9l\u00e8ne de Ribaupierre", "authorId": "2750681"}, {"name": "A. Preece", "authorId": "1762890"}], "n_citations": 25}, "snippets": ["In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."], "score": 0.9541015625}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 269973968, "title": "Identification of mycoplasma pneumonia in children based on fusion of multi-modal clinical free-text description and structured test data", "year": 2024, "venue": "Health Informatics Journal", "authors": [{"name": "Jingna Xie", "authorId": "2302859526"}, {"name": "Yingshuo Wang", "authorId": "2364687744"}, {"name": "Qiuyang Sheng", "authorId": "1576956902"}, {"name": "Xiaoqing Liu", "authorId": "2283403594"}, {"name": "Jing Li", "authorId": "2336703688"}, {"name": "Fenglei Sun", "authorId": "47450065"}, {"name": "Yuqi Wang", "authorId": "2155376102"}, {"name": "Shuxian Li", "authorId": "2302784979"}, {"name": "Yiming Li", "authorId": "2110531396"}, {"name": "Yizhou Yu", "authorId": "2302875109"}, {"name": "Gang Yu", "authorId": "2303601854"}], "n_citations": 1}, "snippets": ["When comparing VSM-based methods with Word2Vec-based methods, we equate MLP and FastText due to their similar complexities as shallow neural networks. FastText, however, demonstrates better performance than MLP (0.696 vs 0.670 in Acc and 0.767 vs 0.727 in F 1 ), indicating Word2Vec's superior word representation capabilities", "Interestingly, original BERT, despite its wide-ranging successes in natural language processing, does not show a marked advantage over Word2Vec in our analysis and is slightly outperformed by TextCNN. This observation is further evidenced by the improved performance of MLM BERT over the original BERT, suggesting that contextual word embeddings benefit from additional data and training. This leads us to conclude that for specialized domain language tasks like clinical text analysis, pre-training is essential to fully leverage BERT's capabilities."], "score": 0.69921875}], "table": null}, {"title": "Practical Applications and Use Cases", "tldr": "FastText and BERT offer complementary strengths for different data filtering scenarios, with FastText excelling in resource-constrained environments and real-time applications, while BERT delivers superior performance for complex linguistic tasks requiring deep contextual understanding. (5 sources)", "text": "\n- **High-volume data filtering with limited computational resources**: FastText provides a practical solution when processing massive datasets with constrained computational resources, offering fast training and no dependency on GPUs while maintaining reasonable performance. <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>\n\n- **Real-time content moderation and filtering**: For applications requiring immediate filtering decisions, FastText's computational efficiency makes it suitable for real-time processing where BERT would be prohibitively slow or resource-intensive. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Requirements extraction in software engineering**: FastText's superior recall characteristics (0.93 compared to BERT's 0.8) make it particularly valuable for extracting more relevant sentences and text patterns associated with requirements, where comprehensiveness is prioritized over precision. <Paper corpusId=\"246608158\" paperTitle=\"(Ivanov et al., 2022)\" isShortName></Paper>\n\n- **Small dataset scenarios**: In settings with limited training data, FastText coupled with domain-specific word embeddings can perform equally well or better than BERT, even when BERT is pre-trained on domain-specific data. <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>\n\n- **Knowledge distillation applications**: FastText can serve as a student model distilled from more powerful but computationally expensive models like BERT or ERNIE, significantly improving classification accuracy while maintaining computational efficiency. This approach is particularly useful for applications like chatbot intention classification. <Paper corpusId=\"246738423\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>\n\n- **Production environments with strict performance-runtime constraints**: FastText-based models can offer a better performance-runtime tradeoff in production environments, with one study showing a LSTM architecture with FastText embeddings having 160% fewer parameters than BERT while achieving comparable or slightly better performance. <Paper corpusId=\"252901000\" paperTitle=\"(Groger et al., 2022)\" isShortName></Paper>\n\n- **Financial text analysis**: FastText has demonstrated superior performance over BERT specifically for banking datasets, suggesting particular value in financial domain applications. <Paper corpusId=\"265162335\" paperTitle=\"(Wildemann et al., 2023)\" isShortName></Paper>\n\n- **Hybrid filtering approaches**: Combining FastText for efficient processing of straightforward content with BERT for more ambiguous or context-dependent filtering decisions can optimize both performance and computational efficiency in large-scale data filtering pipelines. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [{"id": "(Wildemann et al., 2023)", "paper": {"corpus_id": 265162335, "title": "Bridging Qualitative Data Silos: The Potential of Reusing Codings Through Machine Learning Based Cross-Study Code Linking", "year": 2023, "venue": "Social science computer review", "authors": [{"name": "Sergej Wildemann", "authorId": "3041146"}, {"name": "Claudia Nieder\u00e9e", "authorId": "2257974973"}, {"name": "Erick Elejalde", "authorId": "2098749"}], "n_citations": 0}, "snippets": ["Overall, BERT achieves the best performance on the F 1 measure in all datasets except for Bank", "fastText proves to be the second-best classifier and even outperforms BERT on the Bank dataset. The additional use of pre-trained word embeddings clearly benefits the task and shows an average increase in F 1 -score of 15.3% across all datasets. This suggests a further improvement when using larger domain-specific corpora. Moreover, its fast training and the lack of dependency on GPUs may be advantageous for many practical applications."], "score": 0.71484375}, {"id": "(Ivanov et al., 2022)", "paper": {"corpus_id": 246608158, "title": "Extracting Software Requirements from Unstructured Documents", "year": 2022, "venue": "International Joint Conference on the Analysis of Images, Social Networks and Texts", "authors": [{"name": "V. Ivanov", "authorId": "2072422334"}, {"name": "Andrey Sadovykh", "authorId": "2883654"}, {"name": "Alexandr Naumchev", "authorId": "3361049"}, {"name": "A. Bagnato", "authorId": "36000045"}, {"name": "K. Yakovlev", "authorId": "2117465706"}], "n_citations": 11}, "snippets": ["Result of the first phase of evaluation is presented in Table 1. As it was expected, more advanced model (BERT) showed better results in terms of F1-score. BERT-based model showed high precision (0.92) and lower recall (0.8). Results of the BERT model is available at https://bit.ly/3oPElMm. However, the values of precision and recall metrics behave differently for the fastText and ELMobased baselines. fastText-based classifier showed better Recall (0.93) comparing with other architectures. This property might be useful in some cases when it is necessary to extract more relevant sentences and text patterns associated with requirements."], "score": 0.775390625}, {"id": "(Edwards et al., 2020)", "paper": {"corpus_id": 227231089, "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification", "year": 2020, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "A. Edwards", "authorId": "1383074767"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}, {"name": "H\u00e9l\u00e8ne de Ribaupierre", "authorId": "2750681"}, {"name": "A. Preece", "authorId": "1762890"}], "n_citations": 25}, "snippets": ["In settings with small training datasets a simple method like fastText coupled with domain-specific word embeddings performs equally well or better than BERT, even when pre-trained on domain-specific data."], "score": 0.9541015625}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 246738423, "title": "Application of Knowledge Distillation Based on Transfer Learning of ERNIE Model in Intelligent Dialogue Intention Recognition", "year": 2022, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Shiguang Guo", "authorId": "2119112458"}, {"name": "Qing Wang", "authorId": "2117944486"}], "n_citations": 8}, "snippets": ["In this paper, we use knowledge of the ERNIE model to distill the FastText model; the ERNIE model works as a teacher model to predict the massive online unlabeled data for data enhancement, and then guides the training of the student model of FastText with better computational efficiency. The FastText model is distilled by the ERNIE model in chatbot intention classification. This not only guarantees the superiority of its original computational performance, but also the intention classification accuracy has been significantly improved."], "score": 0.6806640625}, {"id": "(Groger et al., 2022)", "paper": {"corpus_id": 252901000, "title": "Assessing Guest Nationality Composition from Hotel Reviews", "year": 2022, "venue": "Swiss Conference on Data Science", "authors": [{"name": "Fabian Gr\u00f6ger", "authorId": "2026999646"}, {"name": "M. Pouly", "authorId": "1715783"}, {"name": "Flavia Tinner", "authorId": "2187827840"}, {"name": "Leif Brandes", "authorId": "39589520"}], "n_citations": 0}, "snippets": ["The best performing model consists of pre-trained FastText embeddings followed by a stack of bidirectional LSTM layers. This rather simple architecture performs more that 20% better in F 1 compared to a support vector machine baseline with TF-IDF feature engineering and also beats transfer learning with a pretrained BERT model 3 by a small margin. On the one hand, this indicates that such large language models as BERT can generalize pretty well even in the presence of small data. On the other hand, BERT comes with an increase of 160% in the number of parameters and thus significantly higher inference time compared to the winning model. Hence, even with more data and an expected increase in the BERT model performance, the LSTM architecture will show the better performance-runtime tradeoff."], "score": 0.76904296875}], "table": null}], "cost": 0.37251300000000004}}
