{"better_query": "What were the main technical innovations introduced in each major LLaVA variant (LLaVA 1.5, LLaVA-NeXT, and LLaVA-OneVision) that drove their performance improvements?", "better_answer": {"sections": [{"title": "Introduction to LLaVA Models", "tldr": "LLaVA models represent a significant advancement in large multimodal models (LMMs) that combine vision capabilities with language understanding. The evolution from the original LLaVA to LLaVA-1.5, LLaVA-NeXT, and LLaVA-OneVision demonstrates progressive improvements in multimodal capabilities through innovative techniques and training approaches. (5 sources)", "text": "\nThe LLaVA (Large Language and Vision Assistant) family of models has emerged as a leading framework for multimodal AI systems that can process both visual and textual information. The original LLaVA model demonstrated impressive multimodal chat abilities, exhibiting behaviors similar to GPT-4V when processing previously unseen images and instructions <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This breakthrough was achieved by combining CLIP-based image encoders with large language models (LLMs) for multi-modal instruction tuning <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nThe LLaVA architecture evolved rapidly through several major versions. LLaVA-1.5 significantly expanded the model's capabilities by incorporating more academic-related instruction data, achieving state-of-the-art performance across numerous benchmarks through a data-efficient recipe <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. A key insight from this version was that the fully-connected vision-language connector in LLaVA proved surprisingly powerful and data-efficient <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nBuilding upon these foundations, subsequent models including LLaVA-NeXT and LLaVA-OneVision further pushed performance boundaries through broader training datasets and advanced training strategies <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper>. These improvements have positioned the LLaVA family alongside other notable vision-language foundation models such as ShareGPT4V, InternVL, SPHINX, and Qwen-VL <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266521410\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. The evolution of these models demonstrates the rapid advancement in multimodal AI systems that can process and reason about visual and textual information in increasingly sophisticated ways.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 271719914, "title": "LLaVA-OneVision: Easy Visual Task Transfer", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Bo Li", "authorId": "2310709478"}, {"name": "Yuanhan Zhang", "authorId": "2145784327"}, {"name": "Dong Guo", "authorId": "2325209062"}, {"name": "Renrui Zhang", "authorId": "2310650738"}, {"name": "Feng Li", "authorId": "2310758205"}, {"name": "Hao Zhang", "authorId": "2267467406"}, {"name": "Kaichen Zhang", "authorId": "2300086932"}, {"name": "Yanwei Li", "authorId": "2315071527"}, {"name": "Ziwei Liu", "authorId": "2315193840"}, {"name": "Chunyuan Li", "authorId": "2264692022"}], "n_citations": 867}, "snippets": ["The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."], "score": 0.84716796875}, {"id": "(Guo et al., 2025)", "paper": {"corpus_id": 276961504, "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyu Guo", "authorId": "2237599228"}, {"name": "Ray Zhang", "authorId": "2350073299"}, {"name": "Hao Chen", "authorId": "2280286298"}, {"name": "Jialin Gao", "authorId": "2350025512"}, {"name": "Dongzhi Jiang", "authorId": "2293242031"}, {"name": "Jiaze Wang", "authorId": "2254323687"}, {"name": "Pheng-Ann Heng", "authorId": "2274486861"}], "n_citations": 7}, "snippets": ["Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies."], "score": 0.5654296875}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 266521410, "title": "Intern VL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Zhe Chen", "authorId": "66350249"}, {"name": "Jiannan Wu", "authorId": "2109215916"}, {"name": "Wenhai Wang", "authorId": "2257133501"}, {"name": "Weijie Su", "authorId": "2276207009"}, {"name": "Guo Chen", "authorId": "2155229619"}, {"name": "Sen Xing", "authorId": "2191075284"}, {"name": "Zhong Muyan", "authorId": "2276203785"}, {"name": "Qinglong Zhang", "authorId": "2276279994"}, {"name": "Xizhou Zhu", "authorId": "2578924"}, {"name": "Lewei Lu", "authorId": "152309485"}, {"name": "Bin Li", "authorId": "2218579598"}, {"name": "Ping Luo", "authorId": "2253674868"}, {"name": "Tong Lu", "authorId": "2276323159"}, {"name": "Yu Qiao", "authorId": "2258755556"}, {"name": "Jifeng Dai", "authorId": "3304536"}], "n_citations": 1215}, "snippets": ["The exponential growth of large language models (LLMs) has opened up numerous possibilities for multi-modal AGI systems. However, the progress in vision and vision-language foundation models, which are also critical elements of multi-modal AGI, has not kept pace with LLMs. In this work, we design a large-scale vision-language foun-dation model (Intern VL), which scales up the vision foun-dation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources. This model can be broadly applied to and achieve state-of-the-art performance on 32 generic visual-linguistic benchmarks including visual perception tasks such as image-level or pixel-level recognition, vision-language tasks such as zero-shot image/video classification, zero-shot image/video-text retrieval, and link with LLMs to create multi-modal dialogue systems. It has powerful visual capabilities and can be a good alternative to the ViT-22B. We hope that our research could contribute to the development of multi-modal large models."], "score": 0.0}], "table": null}, {"title": "LLaVA-1.5 Technical Innovations", "tldr": "LLaVA-1.5 introduced several key technical innovations including an upgraded MLP vision-language connector, increased image resolution from 224px to 336px, and the incorporation of academic task-oriented datasets to enhance performance across multiple benchmarks. (5 sources)", "text": "\nLLaVA-1.5 represented a significant advancement over the original LLaVA model through several technical innovations. One of the most important changes was replacing the original linear layer with a more sophisticated MLP (Multi-Layer Perceptron) vision-language connector, which proved to be \"surprisingly powerful and data-efficient\" <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This architectural improvement enabled better integration between visual and textual information.\n\nThe model also increased the input image resolution from 224px to 336px, using CLIP ViT-L/14 as its vision encoder, which resulted in 576 visual tokens (calculated as (336/14)\u00b2) <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This higher resolution allowed the model to capture more detailed visual information from images.\n\nPerhaps most significantly, LLaVA-1.5 incorporated more academic task-oriented visual question answering (VQA) datasets with response formatting prompts <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>. This data enhancement strategy was remarkably efficient - the final 13B checkpoint used only 1.2M publicly available data points and could be fully trained in approximately one day on a single 8-A100 node <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nThese improvements collectively enabled LLaVA-1.5 to achieve state-of-the-art performance across 11 benchmarks <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. The model also laid important groundwork for addressing open challenges in large multimodal models, including \"scaling to higher resolution inputs, compositional capabilities, and model hallucination\" <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273186838, "title": "Intriguing Properties of Large Language and Vision Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Young-Jun Lee", "authorId": "2119566888"}, {"name": "ByungSoo Ko", "authorId": "9726578"}, {"name": "Han-Gyu Kim", "authorId": "2118626092"}, {"name": "Yechan Hwang", "authorId": "2296345851"}, {"name": "Ho-Jin Choi", "authorId": "2260282476"}], "n_citations": 0}, "snippets": ["\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."], "score": 0.6640625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."], "score": 0.88232421875}, {"id": "(Qu et al., 2024)", "paper": {"corpus_id": 274131643, "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tingyu Qu", "authorId": "2173939633"}, {"name": "Mingxiao Li", "authorId": "2112132080"}, {"name": "T. Tuytelaars", "authorId": "1704728"}, {"name": "M. Moens", "authorId": "1802161"}], "n_citations": 2}, "snippets": ["LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM."], "score": 0.64013671875}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.908203125}], "table": null}, {"title": "LLaVA-NeXT Technical Innovations", "tldr": "LLaVA-NeXT (also known as LLaVA-1.6) introduced three key innovations: AnyRes for dynamic high-resolution image processing, expanded high-quality instruction datasets, and integration with advanced open-source LLMs, which collectively enhanced its visual reasoning, OCR, and multimodal document understanding capabilities. (8 sources)", "text": "\nLLaVA-NeXT represented a significant evolution from LLaVA-1.5, with several technical innovations that substantially improved performance across multiple domains. One of the most important advancements was the introduction of the AnyRes technique for handling high-resolution images <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Unlike LLaVA-1.5's fixed resolution increase, LLaVA-NeXT implemented a dynamic high-resolution design that could scale input image resolution by up to four times, reaching 1344\u00d7336 compared to the 336\u00d7336 used in LLaVA-1.5 <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThis approach enabled the model to select the optimal aspect ratio based on the input image's resolution. Technically, high-resolution images were split into several sub-images of the same size as the original image, which were individually encoded and then concatenated before being fed into the language model <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This patch-based processing of large images significantly enhanced the model's ability to capture finer-grained visual details <Paper corpusId=\"276647578\" paperTitle=\"(Alyakin et al., 2025)\" isShortName></Paper>.\n\nLLaVA-NeXT also featured an expanded and more diverse visual instruction-tuning dataset, incorporating ShareGPT-4V and LAION-GPT-V data, along with specialized OCR, document, and chart datasets <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This broader training data significantly contributed to the model's enhanced capabilities in visual reasoning, optical character recognition, and multimodal document understanding <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe third key innovation was the utilization of the most advanced open-source LLMs available at the time <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. LLaVA-NeXT's architecture continued to use CLIP ViT-L/14 as its vision encoder but paired it with Vicuna models ranging from 7B to 34B parameters as backbone language models <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. The model also retained and enhanced the multilayer projection from LLaVA-1.5 <Paper corpusId=\"276647578\" paperTitle=\"(Alyakin et al., 2025)\" isShortName></Paper>.\n\nThese technical innovations collectively positioned LLaVA-NeXT as part of a broader trend in vision-language models that expanded capabilities through broader training datasets and advanced training strategies <Paper corpusId=\"276961504\" paperTitle=\"(Guo et al., 2025)\" isShortName></Paper>. The model's improvements in handling high-resolution images and enhanced training approaches also influenced subsequent multimodal models that incorporated both image and video processing <Paper corpusId=\"277313523\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 271719914, "title": "LLaVA-OneVision: Easy Visual Task Transfer", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Bo Li", "authorId": "2310709478"}, {"name": "Yuanhan Zhang", "authorId": "2145784327"}, {"name": "Dong Guo", "authorId": "2325209062"}, {"name": "Renrui Zhang", "authorId": "2310650738"}, {"name": "Feng Li", "authorId": "2310758205"}, {"name": "Hao Zhang", "authorId": "2267467406"}, {"name": "Kaichen Zhang", "authorId": "2300086932"}, {"name": "Yanwei Li", "authorId": "2315071527"}, {"name": "Ziwei Liu", "authorId": "2315193840"}, {"name": "Chunyuan Li", "authorId": "2264692022"}], "n_citations": 867}, "snippets": ["The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."], "score": 0.84716796875}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271855335, "title": "Revisiting Multi-Modal LLM Evaluation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jian Lu", "authorId": "2316020150"}, {"name": "Shikhar Srivastava", "authorId": "2315987232"}, {"name": "Junyu Chen", "authorId": "2315947134"}, {"name": "Robik Shrestha", "authorId": "153677280"}, {"name": "Manoj Acharya", "authorId": "47309247"}, {"name": "Kushal Kafle", "authorId": "33315685"}, {"name": "Christopher Kanan", "authorId": "2303258758"}], "n_citations": 3}, "snippets": ["LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets."], "score": 0.724609375}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."], "score": 0.88232421875}, {"id": "(Alyakin et al., 2025)", "paper": {"corpus_id": 276647578, "title": "Repurposing the scientific literature with vision-language models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Anton Alyakin", "authorId": "2328643882"}, {"name": "Jaden Stryker", "authorId": "2325728846"}, {"name": "D. Alber", "authorId": "1941269137"}, {"name": "Karl L. Sangwon", "authorId": "2248607147"}, {"name": "Brandon Duderstadt", "authorId": "2347532646"}, {"name": "Akshay V. Save", "authorId": "2269042919"}, {"name": "David B. Kurland", "authorId": "5861736"}, {"name": "Spencer Frome", "authorId": "2331397171"}, {"name": "Shrutika Singh", "authorId": "2347581153"}, {"name": "Jeff Zhang", "authorId": "2347550668"}, {"name": "Eunice Yang", "authorId": "2321510385"}, {"name": "Ki Yun Park", "authorId": "2347589607"}, {"name": "C. Orillac", "authorId": "6068765"}, {"name": "Aly A. Valliani", "authorId": "49558131"}, {"name": "Sean Neifert", "authorId": "2321514213"}, {"name": "Albert Liu", "authorId": "2148770758"}, {"name": "Aneek Patel", "authorId": "2273924323"}, {"name": "Christopher Livia", "authorId": "2219711063"}, {"name": "Darryl Lau", "authorId": "2291574490"}, {"name": "Ilya Laufer", "authorId": "2347528853"}, {"name": "P. Rozman", "authorId": "47034244"}, {"name": "E. Hidalgo", "authorId": "39243010"}, {"name": "H. Riina", "authorId": "145327270"}, {"name": "Rui Feng", "authorId": "2281737315"}, {"name": "T. Hollon", "authorId": "2249367961"}, {"name": "Yindalon Aphinyanagphongs", "authorId": "8340776"}, {"name": "J. Golfinos", "authorId": "6473601"}, {"name": "Laura Snyder", "authorId": "2347529122"}, {"name": "Eric C. Leuthardt", "authorId": "2334000180"}, {"name": "Douglas Kondziolka", "authorId": "2247415145"}, {"name": "E. Oermann", "authorId": "2181708076"}], "n_citations": 0}, "snippets": ["We built on LLaVA-Next's improvements -including its multilayer projection, patch-based processing of large images, and enhanced pre-training (Fig. S6a)."], "score": 0.6435546875}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.908203125}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273186838, "title": "Intriguing Properties of Large Language and Vision Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Young-Jun Lee", "authorId": "2119566888"}, {"name": "ByungSoo Ko", "authorId": "9726578"}, {"name": "Han-Gyu Kim", "authorId": "2118626092"}, {"name": "Yechan Hwang", "authorId": "2296345851"}, {"name": "Ho-Jin Choi", "authorId": "2260282476"}], "n_citations": 0}, "snippets": ["\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."], "score": 0.6640625}, {"id": "(Guo et al., 2025)", "paper": {"corpus_id": 276961504, "title": "SciVerse: Unveiling the Knowledge Comprehension and Visual Reasoning of LMMs on Multi-modal Scientific Problems", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyu Guo", "authorId": "2237599228"}, {"name": "Ray Zhang", "authorId": "2350073299"}, {"name": "Hao Chen", "authorId": "2280286298"}, {"name": "Jialin Gao", "authorId": "2350025512"}, {"name": "Dongzhi Jiang", "authorId": "2293242031"}, {"name": "Jiaze Wang", "authorId": "2254323687"}, {"name": "Pheng-Ann Heng", "authorId": "2274486861"}], "n_citations": 7}, "snippets": ["Early opensource LMMs like LLaVA (Liu et al., 2023b) and MiniGPT-4 (Zhu et al., 2023) paired CLIP-based image encoders (Radford et al., 2021) with LLMs for multi-modal instruction tuning. Later models such as LLaVA-NeXT (Li et al., 2024a), LLaVA-OneVision (Li et al., 2024b), ShareGPT4V (Chen et al., 2023b), InternVL (Chen et al., 2023), SPHINX (Lin et al., 2023), and Qwen-VL (Qwen Team, 2024) expanded these capabilities through broader training datasets and advanced training strategies."], "score": 0.5654296875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 277313523, "title": "PAVE: Patching and Adapting Video Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhuoming Liu", "authorId": "2333863752"}, {"name": "Yiquan Li", "authorId": "2321684254"}, {"name": "Khoi Duc Nguyen", "authorId": "2342927060"}, {"name": "Yiwu Zhong", "authorId": "1828787912"}, {"name": "Yin Li", "authorId": "2333742645"}], "n_citations": 1}, "snippets": ["Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance."], "score": 0.57421875}], "table": null}, {"title": "LLaVA-OneVision Technical Innovations", "tldr": "LLaVA-OneVision introduced a unified architecture capable of handling single images, multiple images, and video within the same framework, enabling powerful cross-modal transfer learning. Its key innovations include dynamic image resolution handling, the SigLIP vision encoder with higher base resolution, and integration with Qwen2 language models. (7 sources)", "text": "\nLLaVA-OneVision represents a significant advancement in large multimodal models by addressing a critical limitation of previous versions: the ability to handle different visual modalities simultaneously. While LLaVA-1.5 and LLaVA-NeXT focused primarily on single-image processing, LLaVA-OneVision was designed to unify \"single-image, multi-image, and video scenarios\" within a single model architecture <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>.\n\nThe model introduced several key technical innovations. First, it employed the SigLIP vision encoder, which processes images at a higher base resolution of 384\u00d7384 pixels, producing 729 visual tokens - a substantial increase from LLaVA-1.5's 576 tokens <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This enhanced resolution capability, combined with a dynamic image resolution approach similar to LLaVA-NeXT's AnyRes technique, allowed the model to better handle visual details across different scenarios <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nSecond, LLaVA-OneVision incorporated more powerful language models, specifically the Qwen2 family ranging from 8B to 72B parameters, connected to the vision encoder through a 2-layer MLP projector <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This architecture choice provided a stronger foundation for cross-modal reasoning.\n\nPerhaps most importantly, LLaVA-OneVision pioneered an approach that enables powerful transfer learning across different visual modalities and scenarios <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This cross-modal transfer learning capability resulted in \"the emergence of new capabilities\" that weren't present in previous LLaVA versions <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nThe model's unified approach to handling multiple visual formats represented a significant shift in multimodal AI development strategy. Rather than creating separate specialized models for different visual tasks, LLaVA-OneVision demonstrated that a single architecture could effectively process single images, multiple images, videos, and even audio within the same framework <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This multi-stage training approach with both video and image data has become increasingly common in subsequent vision-language models, as it \"substantially improves the model performance\" across diverse tasks <Paper corpusId=\"277313523\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273186838, "title": "Intriguing Properties of Large Language and Vision Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Young-Jun Lee", "authorId": "2119566888"}, {"name": "ByungSoo Ko", "authorId": "9726578"}, {"name": "Han-Gyu Kim", "authorId": "2118626092"}, {"name": "Yechan Hwang", "authorId": "2296345851"}, {"name": "Ho-Jin Choi", "authorId": "2260282476"}], "n_citations": 0}, "snippets": ["\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."], "score": 0.6640625}, {"id": "(Dai et al., 2024)", "paper": {"corpus_id": 273821149, "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Dawei Dai", "authorId": "2082462168"}, {"name": "Xu Long", "authorId": "2329189750"}, {"name": "Yutang Li", "authorId": "2136494548"}, {"name": "Yuanhui Zhang", "authorId": "2310835404"}, {"name": "Shuy Xia", "authorId": "2147222435"}], "n_citations": 2}, "snippets": ["LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios."], "score": 0.8681640625}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276724729, "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiao Wang", "authorId": "2261886750"}, {"name": "Jingyun Hua", "authorId": "2348096687"}, {"name": "Weihong Lin", "authorId": "2348609405"}, {"name": "Yuanxing Zhang", "authorId": "2145784116"}, {"name": "Fuzheng Zhang", "authorId": "2257136363"}, {"name": "Jianlong Wu", "authorId": "2292205089"}, {"name": "Di Zhang", "authorId": "2323902668"}, {"name": "Liqiang Nie", "authorId": "2284688853"}], "n_citations": 0}, "snippets": ["LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format."], "score": 0.591796875}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276938164, "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoda Yang", "authorId": "2308224151"}, {"name": "JunYu Lu", "authorId": "2350336954"}, {"name": "Hongshun Qiu", "authorId": "2220747584"}, {"name": "Sijing Li", "authorId": "2350180388"}, {"name": "Hao Li", "authorId": "2349632427"}, {"name": "Shengpeng Ji", "authorId": "72890649"}, {"name": "Xudong Tang", "authorId": "2349737557"}, {"name": "Jiayang Xu", "authorId": "2349670795"}, {"name": "Jiaqi Duan", "authorId": "2329894630"}, {"name": "Ziyue Jiang", "authorId": "2112347676"}, {"name": "Cong Lin", "authorId": "2349737916"}, {"name": "Sihang Cai", "authorId": "2328348412"}, {"name": "Zejian Xie", "authorId": "2266912737"}, {"name": "Zhuoyang Song", "authorId": "2352067468"}, {"name": "Songxin Zhang", "authorId": "2266803682"}], "n_citations": 0}, "snippets": ["LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]."], "score": 0.66064453125}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.66552734375}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 277313523, "title": "PAVE: Patching and Adapting Video Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhuoming Liu", "authorId": "2333863752"}, {"name": "Yiquan Li", "authorId": "2321684254"}, {"name": "Khoi Duc Nguyen", "authorId": "2342927060"}, {"name": "Yiwu Zhong", "authorId": "1828787912"}, {"name": "Yin Li", "authorId": "2333742645"}], "n_citations": 1}, "snippets": ["Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance."], "score": 0.57421875}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.908203125}], "table": null}, {"title": "Comparative Performance Improvements", "tldr": "Each iteration of the LLaVA model family delivered significant performance improvements across various benchmarks, with LLaVA-1.5 establishing state-of-the-art results across 11 benchmarks, LLaVA-NeXT enhancing visual reasoning and OCR capabilities, and LLaVA-OneVision unifying performance across single-image, multi-image, and video tasks. (10 sources)", "text": "\nThe evolution of the LLaVA model family demonstrates clear progression in multimodal capabilities, with each version introducing innovations that directly translated to performance improvements. LLaVA-1.5 significantly expanded the capabilities of the original model by incorporating more academic-related instruction data, which helped it achieve state-of-the-art performance across 11 benchmarks through its data-efficient recipe <Paper corpusId=\"271719914\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This version's performance improvements were particularly notable given its resource efficiency - the final 13B checkpoint used only 1.2M publicly available data points and could be fully trained in approximately one day on a single 8-A100 node <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nLLaVA-NeXT (also known as LLaVA-1.6) built upon these foundations to deliver enhanced performance in specific domains. Its four-fold increase in image resolution (up to 1344\u00d7336 compared to 336\u00d7336 in LLaVA-1.5) significantly improved the model's ability to capture fine-grained visual details <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273186838\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This resolution enhancement, combined with training on more diverse and realistic visual instruction-tuning datasets including ShareGPT-4V and specialized OCR, document, and chart datasets, resulted in substantial improvements in visual reasoning, optical character recognition, and multimodal document understanding <Paper corpusId=\"271855335\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper>. The dynamic high-resolution design allowed LLaVA-NeXT to select the optimal aspect ratio based on input image resolution, providing flexibility that further enhanced performance <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nLLaVA-OneVision represented a paradigm shift in performance by simultaneously addressing multiple visual modalities. While previous versions showed improvements in single-image tasks, LLaVA-OneVision pushed performance boundaries across three key computer vision scenarios: single image, multiple images, and video <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. This unified approach not only improved performance across individual modalities but also enabled powerful transfer learning between different scenarios, resulting in the emergence of entirely new capabilities <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. By encoding different types of data into vectors of the same dimension, LLaVA-OneVision extended the model's capabilities to handle multiple modalities simultaneously, including audio <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nAcross the LLaVA family evolution, we observe a consistent pattern of performance improvements driven by three key factors: increased resolution and enhanced visual processing, expanded high-quality training data across diverse tasks, and integration with increasingly powerful language models <Paper corpusId=\"274131643\" paperTitle=\"(Qu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276724729\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. These technical innovations collectively advanced the state of multimodal AI systems, with each iteration addressing specific limitations of its predecessors.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 271719914, "title": "LLaVA-OneVision: Easy Visual Task Transfer", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Bo Li", "authorId": "2310709478"}, {"name": "Yuanhan Zhang", "authorId": "2145784327"}, {"name": "Dong Guo", "authorId": "2325209062"}, {"name": "Renrui Zhang", "authorId": "2310650738"}, {"name": "Feng Li", "authorId": "2310758205"}, {"name": "Hao Zhang", "authorId": "2267467406"}, {"name": "Kaichen Zhang", "authorId": "2300086932"}, {"name": "Yanwei Li", "authorId": "2315071527"}, {"name": "Ziwei Liu", "authorId": "2315193840"}, {"name": "Chunyuan Li", "authorId": "2264692022"}], "n_citations": 867}, "snippets": ["The first LLaVA model [83] demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors similar to GPT-4V on previously unseen images and instructions for the first time. LLaVA-1.5 (Liu et al., 2023) significantly expands and improves the capabilities by incorporating more academicrelated instruction data, achieving SoTA performance on a dozens of benchmarks with a data-efficient recipe. LLaVA-NeXT [82] inherits this property, further pushing performance boundaries through three key techniques: AnyRes for handling high-resolution images, expanding high-quality instruction data, and utilizing the best open LLM available at the time."], "score": 0.84716796875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this paper, we present the first systematic study to investigate the design choices of LMMs in a controlled setting under the LLaVA framework. We show that the fully-connected vision-language connector in LLaVA is surprisingly power-ful and data-efficient. With simple modifications to LLa VA, namely, using CLIP- ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ~ 1 day on a single 8-AI00 node. Furthermore, we present some early exploration of open problems in LMMs, including scaling to higher resolution inputs, compositional capabilities, and model hallucination, etc. We hope this makes state-of-the-art LMM research more accessible. Code and model will be publicly available."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 273186838, "title": "Intriguing Properties of Large Language and Vision Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Young-Jun Lee", "authorId": "2119566888"}, {"name": "ByungSoo Ko", "authorId": "9726578"}, {"name": "Han-Gyu Kim", "authorId": "2118626092"}, {"name": "Yechan Hwang", "authorId": "2296345851"}, {"name": "Ho-Jin Choi", "authorId": "2260282476"}], "n_citations": 0}, "snippets": ["\u2022 LLaVA-1.5 (Liu et al., 2024a) incorporates academic task-oriented datasets to enhance performance in VQA tasks and features an MLP vision-language connector, which improves upon the original linear layer utilized in LLaVA (Liu et al., 2024c). It uses CLIP ViT-L/14 (Radford et al., 2021) with a 336px resolution as its vision encoder, resulting in a total of (336/14) 2 = 576 visual tokens.\n\n\u2022 LLaVA-NeXT (Liu et al., 2024b) (also known as LLaVA-1.6) enhances visual reasoning, OCR, and world knowledge, offering four times higher image resolution (up to 1344x336) and improved performance in visual conversations. Its architecture includes a CLIP ViT-L/14 as a vision encoder, paired with Vicuna models ranging from 7B to 34B as a backbone language model.\n\n\u2022 LLaVA-OneVision (Li et al., 2024b) is a LVLM designed for task transfer across singleimage, multi-image, and video scenarios, with strong capabilities in video understanding through image-to-video task transfer. Its architecture consists of a Qwen2 language model (Yang et al., 2024) with 8B to 72B parameters, and the SigLIP vision encoder (Zhai et al., 2023), which processes images at a base resolution of 384x384, producing 729 visual tokens. The model employs a 2-layer MLP projector."], "score": 0.6640625}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271855335, "title": "Revisiting Multi-Modal LLM Evaluation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jian Lu", "authorId": "2316020150"}, {"name": "Shikhar Srivastava", "authorId": "2315987232"}, {"name": "Junyu Chen", "authorId": "2315947134"}, {"name": "Robik Shrestha", "authorId": "153677280"}, {"name": "Manoj Acharya", "authorId": "47309247"}, {"name": "Kushal Kafle", "authorId": "33315685"}, {"name": "Christopher Kanan", "authorId": "2303258758"}], "n_citations": 3}, "snippets": ["LLaVA-NeXT [24] is an improved version of LLaVA 1.5, with a focus on enhanced visual reasoning, optical character recognition (OCR), and multi-modal document understanding. LLaVA-NeXT scales the input image resolution of input images by 4\u00d7, up to 1344 \u00d7 336 compared to 336 \u00d7 336 in LLaVA 1.5 to enhance its ability to grasp finer-grained visual cues. LLaVA-NeXT is also trained on a more diverse and realistic visual instruction-tuning dataset (ShareGPT-4V and LAION-GPT-V), as well as a range of OCR, document, and chart datasets."], "score": 0.724609375}, {"id": "(Qu et al., 2024)", "paper": {"corpus_id": 274131643, "title": "TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for Training-Free Video Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tingyu Qu", "authorId": "2173939633"}, {"name": "Mingxiao Li", "authorId": "2112132080"}, {"name": "T. Tuytelaars", "authorId": "1704728"}, {"name": "M. Moens", "authorId": "1802161"}], "n_citations": 2}, "snippets": ["LLaVA-v1.5 (Liu et al., 2023) and LLaVA-v1.6(NeXT) [23] further improve LLaVA's performance through better data, higher resolution and stronger LLM."], "score": 0.64013671875}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 [34]. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement.\n\nLLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM."], "score": 0.88232421875}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276938164, "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoda Yang", "authorId": "2308224151"}, {"name": "JunYu Lu", "authorId": "2350336954"}, {"name": "Hongshun Qiu", "authorId": "2220747584"}, {"name": "Sijing Li", "authorId": "2350180388"}, {"name": "Hao Li", "authorId": "2349632427"}, {"name": "Shengpeng Ji", "authorId": "72890649"}, {"name": "Xudong Tang", "authorId": "2349737557"}, {"name": "Jiayang Xu", "authorId": "2349670795"}, {"name": "Jiaqi Duan", "authorId": "2329894630"}, {"name": "Ziyue Jiang", "authorId": "2112347676"}, {"name": "Cong Lin", "authorId": "2349737916"}, {"name": "Sihang Cai", "authorId": "2328348412"}, {"name": "Zejian Xie", "authorId": "2266912737"}, {"name": "Zhuoyang Song", "authorId": "2352067468"}, {"name": "Songxin Zhang", "authorId": "2266803682"}], "n_citations": 0}, "snippets": ["LLaVA-OneVision simultaneously pushes the performance boundaries of open-domain Large Multimodal Models (LMMs) across three key computer vision scenarios-single image, multiple images, and video-while enabling powerful transfer learning across different modalities and scenarios, resulting in the emergence of new capabilities [28]."], "score": 0.66064453125}, {"id": "(Dai et al., 2024)", "paper": {"corpus_id": 273821149, "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Dawei Dai", "authorId": "2082462168"}, {"name": "Xu Long", "authorId": "2329189750"}, {"name": "Yutang Li", "authorId": "2136494548"}, {"name": "Yuanhui Zhang", "authorId": "2310835404"}, {"name": "Shuy Xia", "authorId": "2147222435"}], "n_citations": 2}, "snippets": ["LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios."], "score": 0.8681640625}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.66552734375}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276724729, "title": "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiao Wang", "authorId": "2261886750"}, {"name": "Jingyun Hua", "authorId": "2348096687"}, {"name": "Weihong Lin", "authorId": "2348609405"}, {"name": "Yuanxing Zhang", "authorId": "2145784116"}, {"name": "Fuzheng Zhang", "authorId": "2257136363"}, {"name": "Jianlong Wu", "authorId": "2292205089"}, {"name": "Di Zhang", "authorId": "2323902668"}, {"name": "Liqiang Nie", "authorId": "2284688853"}], "n_citations": 0}, "snippets": ["LLaVA-1.5 (Liu et al., 2024) explored adding high-quality multitask training data, and scaling up the resolution and LLM size to boost MLLM performance. LLaVA-OneVision (Li et al., 2024a) explored to unify dynamic image resolution, multi-image, and video into a unified input format."], "score": 0.591796875}], "table": null}], "cost": 0.256149}}
