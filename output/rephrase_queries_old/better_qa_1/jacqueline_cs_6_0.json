{"better_query": "What are the most effective post-training mitigation techniques for reducing racial and gender bias in large language models, and how do they compare in terms of performance trade-offs?", "better_answer": {"sections": [{"title": "Introduction to Bias Mitigation in Large Language Models", "tldr": "Large language models (LLMs) often perpetuate racial and gender biases present in their training data, requiring post-training mitigation techniques to address these issues. Effective bias mitigation is critical to ensure LLMs deliver fair and equitable outputs while maintaining their utility across various applications. (LLM Memory)", "text": "\nLarge language models have demonstrated impressive capabilities across numerous domains, but they also inherit and sometimes amplify social biases present in their training data. These biases can manifest as gender stereotypes, racial prejudices, and other forms of discrimination in model outputs. Post-training bias mitigation techniques have emerged as important solutions for addressing these issues in already-trained LLMs without requiring complete retraining of these massive models.\n\nBias in LLMs stems primarily from patterns in the training data that reflect historical and societal inequities. For example, models may associate certain professions predominantly with specific genders or exhibit varying sentiment when discussing different racial groups. These biases can have harmful real-world consequences when LLMs are deployed in sensitive contexts such as hiring, content moderation, or healthcare applications. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nPost-training mitigation approaches offer practical solutions for developers and organizations that need to address bias in existing models without access to the original training data or the resources required for complete retraining. These techniques target the model after its initial training phase, applying various methods to reduce harmful biases while attempting to preserve general model performance and capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe growing emphasis on responsible AI development has accelerated research into effective post-training debiasing methods. These approaches vary widely in their complexity, effectiveness, and impact on other aspects of model performance, leading to important trade-offs that must be considered when selecting and implementing bias mitigation strategies. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Types of Post-Training Bias Mitigation Techniques", "tldr": "Post-training bias mitigation techniques for LLMs fall into three main categories: intra-processing (modifying inference without retraining), post-processing (adjusting model outputs), and model-driven approaches (directly altering model parameters). Each approach offers different tradeoffs between implementation complexity, effectiveness, and preservation of model capabilities. (14 sources)", "text": "\nPost-training bias mitigation techniques operate on already trained language models and can be categorized into three main approaches based on when and how they intervene in the model processing pipeline <Paper corpusId=\"270878797\" paperTitle=\"(Greco et al., 2024)\" isShortName></Paper>.\n\n## Intra-Processing Approaches\n\nIntra-processing methods modify a model's behavior during inference without requiring additional training <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>. These techniques operate primarily at the decoding stage and include:\n\n1. **Token Search Restriction**: Constraining the decoding process to prevent the generation of biased outputs <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>.\n\n2. **Token Distribution Adjustment**: Manipulating probability distributions during token selection to generate less biased or more diverse outputs <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper>. This can involve techniques like logit suppression or temperature sampling to flatten token probability distributions <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>.\n\n3. **Attention Redistribution**: Reallocating the model's attention to focus on less stereotypical aspects of the input <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268512691\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n4. **Standalone Debiasing Networks**: Implementing modular bias mitigation components that can be integrated into the core model on-demand at inference time without altering the original model <Paper corpusId=\"270370977\" paperTitle=\"(Deng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258461365\" paperTitle=\"(Hauzenberger et al., 2022)\" isShortName></Paper>.\n\nA notable example is DEXPERTS, which uses two models in parallel\u2014one promoting non-toxic text and another discouraging harmful content\u2014to improve output fairness without retraining <Paper corpusId=\"272599907\" paperTitle=\"(Peng et al., 2024)\" isShortName></Paper>. Similarly, Saunders et al. proposed constraining beam search to improve gender diversity in machine translation outputs <Paper corpusId=\"233240748\" paperTitle=\"(Saunders et al., 2021)\" isShortName></Paper>.\n\n## Post-Processing Approaches\n\nPost-processing techniques modify the model's outputs after generation without accessing the model's internal parameters <Paper corpusId=\"267770177\" paperTitle=\"(Yan et al., 2024)\" isShortName></Paper>. These methods include:\n\n1. **Output Filtering**: Identifying and blocking harmful or biased terms in generated content <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>.\n\n2. **Content Replacement**: Detecting biased tokens and replacing them with less stereotypical alternatives <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"246210255\" paperTitle=\"(Tokpo et al., 2022)\" isShortName></Paper>.\n\n3. **Output Redistribution**: Creating a new set of less biased predictions from the original model outputs <Paper corpusId=\"258832694\" paperTitle=\"(Zayed et al., 2023)\" isShortName></Paper>.\n\n4. **Counterfactual Inference**: Generating counterfactual examples to distill and mitigate biases captured by the model <Paper corpusId=\"236459953\" paperTitle=\"(Qian et al., 2021)\" isShortName></Paper>.\n\nHe et al. proposed a gradient-based rewriting framework called DEPEN (Detect and Perturb to Neutralize) that first detects sensitive components in text and masks them for regeneration under a neutralizing constraint <Paper corpusId=\"237634972\" paperTitle=\"(He et al., 2021)\" isShortName></Paper>.\n\n## Model-Driven Approaches\n\nModel-driven approaches directly modify the internal parameters or representations of the language model <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. For models using word embeddings, these techniques may:\n\n1. **Embedding Manipulation**: Directly modifying embedding values to decrease cosine similarities between category terms associated with bias <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n2. **Model Neutralization**: Applying techniques to neutralize the model's internal representations, though these may be less effective for transformer models with contextual embeddings <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\nWhile these post-training approaches offer practical solutions for addressing bias in existing models, their effectiveness can vary significantly. Intra-processing and post-processing methods are particularly valuable when retraining is not feasible due to resource constraints or lack of access to training data <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>. However, achieving the right balance between bias mitigation and maintaining model performance remains a challenge across all approaches <Paper corpusId=\"268512691\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Greco et al., 2024)", "paper": {"corpus_id": 270878797, "title": "NLPGuard: A Framework for Mitigating the Use of Protected Attributes by NLP Classifiers", "year": 2024, "venue": "Proc. ACM Hum. Comput. Interact.", "authors": [{"name": "Salvatore Greco", "authorId": "2149942497"}, {"name": "Ke Zhou", "authorId": "2310191590"}, {"name": "L. Capra", "authorId": "47809306"}, {"name": "Tania Cerquitelli", "authorId": "2203091524"}, {"name": "D. Quercia", "authorId": "144041798"}], "n_citations": 2}, "snippets": ["To address biases in NLP, techniques can be developed that act at the three main stages of the NLP pipeline (Kozodoi et al., 2021)[75]: pre-processing (modifying training data), in-processing (imposing fairness constraints during model training), and post-processing (adjusting classifier predictions based on fairness metrics)."], "score": 0.865234375}, {"id": "(Deng et al., 2024)", "paper": {"corpus_id": 270370977, "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chengyuan Deng", "authorId": "2307568934"}, {"name": "Yiqun Duan", "authorId": "2305729796"}, {"name": "Xin Jin", "authorId": "2308077589"}, {"name": "Heng Chang", "authorId": "2305625227"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Han Liu", "authorId": "2305875824"}, {"name": "Henry Peng Zou", "authorId": "2261285492"}, {"name": "Yiqiao Jin", "authorId": "2087723977"}, {"name": "Yijia Xiao", "authorId": "95289709"}, {"name": "Yichen Wang", "authorId": "2305636181"}, {"name": "Shenghao Wu", "authorId": "2305627329"}, {"name": "Zongxing Xie", "authorId": "2307409629"}, {"name": "Kuofeng Gao", "authorId": "2345701402"}, {"name": "Sihong He", "authorId": "2309117237"}, {"name": "Jun Zhuang", "authorId": "2305613890"}, {"name": "Lu Cheng", "authorId": "2305654768"}, {"name": "Haohan Wang", "authorId": "2305650253"}], "n_citations": 24}, "snippets": ["Intra-processing Mitigation.These approaches modify a trained model's behavior without additional training to generate debiased predictions during inference.There are mainly four types of methods.The first method adds restrictions during token search decoding to prevent biased outputs [245; 194].The second method adjusts token distributions to enhance output diversity or sample less biased outputs [58; 95].The third method redistributes the model's attention to less stereotypical aspects [321].The last method implements standalone networks with original models for specific debiasing tasks, such as reducing gender or racial biases (Hauzenberger et al., 2022).\n\nPost-processing Mitigation.The techniques address bias in generated outputs, especially relevant for black-box models with inaccessible training data or internal processes."], "score": 0.90283203125}, {"id": "(Chung et al., 2023)", "paper": {"corpus_id": 259096160, "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "John Joon Young Chung", "authorId": "152836325"}, {"name": "Ece Kamar", "authorId": "1783184"}, {"name": "Saleema Amershi", "authorId": "1719124"}], "n_citations": 121}, "snippets": ["Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."], "score": 0.0}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 268512691, "title": "Detecting Bias in Large Language Models: Fine-tuned KcBERT", "year": 2024, "venue": "International Conferences on Pattern Recognition and Artificial Intelligence", "authors": [{"name": "J. K. Lee", "authorId": "2292166326"}, {"name": "T. M. Chung", "authorId": "2292024530"}], "n_citations": 0}, "snippets": ["Post-processing mitigation methods adjust the probability distribution during the decoding phase to select tokens with less bias, using methods such as adjusting, filtering, or inserting tokens [31]. Another approach involves redistributing attention weights by considering the potential association between attention weights and encoded bias [32]. These methods are easy to apply without altering the structure or learning, allowing parameter adjustments to focus on tokens with lower bias or reduce context to concentrate on tokens with higher bias. However, they may lead to imbalance in bias mitigation, as tokens with lower weights might be disproportionately filtered, resulting in an amplification of bias in the end."], "score": 0.92822265625}, {"id": "(Hauzenberger et al., 2022)", "paper": {"corpus_id": 258461365, "title": "Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Lukas Hauzenberger", "authorId": "2167029006"}, {"name": "Shahed Masoudian", "authorId": "2184114298"}, {"name": "Deepak Kumar", "authorId": "2116415191"}, {"name": "M. Schedl", "authorId": "144125621"}, {"name": "Navid Rekabsaz", "authorId": "2844293"}], "n_citations": 18}, "snippets": ["Societal biases are reflected in large pre-trained language models and their fine-tuned versions on downstream tasks. Common in-processing bias mitigation approaches, such as adversarial training and mutual information removal, introduce additional optimization criteria, and update the model to reach a new debiased state. However, in practice, end-users and practitioners might prefer to switch back to the original model, or apply debiasing only on a specific subset of protected attributes. To enable this, we propose a novel modular bias mitigation approach, consisting of stand-alone highly sparse debiasing subnetworks, where each debiasing module can be integrated into the core model on-demand at inference time. Our approach draws from the concept of \\emph{diff} pruning, and proposes a novel training regime adaptable to various representation disentanglement optimizations. We conduct experiments on three classification tasks with gender, race, and age as protected attributes. The results show that our modular approach, while maintaining task performance, improves (or at least remains on-par with) the effectiveness of bias mitigation in comparison with baseline finetuning. Particularly on a two-attribute dataset, our approach with separately learned debiasing subnetworks shows effective utilization of either or both the subnetworks for selective bias mitigation."], "score": 0.93701171875}, {"id": "(Peng et al., 2024)", "paper": {"corpus_id": 272599907, "title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Benji Peng", "authorId": "2319606006"}, {"name": "Keyu Chen", "authorId": "2319648854"}, {"name": "Ming Li", "authorId": "2321058649"}, {"name": "Pohsun Feng", "authorId": "2319607771"}, {"name": "Ziqian Bi", "authorId": "2319608188"}, {"name": "Junyu Liu", "authorId": "2319615017"}, {"name": "Qian Niu", "authorId": "2319611972"}], "n_citations": 15}, "snippets": ["During intra-processing, models are tweaked at the inference stage without retraining. Model editing enables targeted updates to model behavior, ensuring that biases in specific areas are corrected without affecting overall model performance [140,141]. Decoding modification like DEXPERTS directly affects text generation by adjusting token probabilities. DEXPERTS uses two models, one to promote non-toxic text and another to discourage harmful content, to improving output fairness [142].\n\nPost-processing methods focus on modifying the model's outputs."], "score": 0.955078125}, {"id": "(Saunders et al., 2021)", "paper": {"corpus_id": 233240748, "title": "First the Worst: Finding Better Gender Translations During Beam Search", "year": 2021, "venue": "Findings", "authors": [{"name": "D. Saunders", "authorId": "2069873842"}, {"name": "Rosie Sallis", "authorId": "1993979140"}, {"name": "B. Byrne", "authorId": "36126076"}], "n_citations": 28}, "snippets": ["Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach\u2019s utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary."], "score": 0.0}, {"id": "(Yan et al., 2024)", "paper": {"corpus_id": 267770177, "title": "Potential and Challenges of Model Editing for Social Debiasing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jianhao Yan", "authorId": "134233854"}, {"name": "Futing Wang", "authorId": "2285582246"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Yue Zhang", "authorId": "2249762135"}], "n_citations": 9}, "snippets": ["Preprocessing techniques aim to detect and eliminate bias and unfairness early on, either within the dataset (Zmigrod et al., 2019)(Dinan et al., 2019)Abid et al., 2021;(Qian et al., 2022)(Ghanbarzadeh et al., 2023) or prompt (Mattern et al., 2022;(Fatemi et al., 2021)(Yang et al., 2022). In-training bias mitigation techniques focus on reducing bias and unfairness during model training, by adjusting model architecture (Bartl et al., 2020)(Han et al., 2021), modifying loss functions (Liu et al., 2019)Webster et al., 2020;(Ouyang et al., 2022)(Woo et al., 2023)(Park et al., 2023)(Zhou et al., 2023)(Li et al., 2023), or selectively updating parameters (Qian et al., 2022)Ranaldi et al., 2023;(Yu et al., 2023). \n\nIntraprocessing approaches alter decoding behavior (Saunders et al., 2021)Meade et al., 2023;(Kim et al., 2022)(Chung et al., 2023)(Hallinan et al., 2022) without additional training or fine-tuning. \n\nPost-processing techniques primarily adjust model outputs to address bias and unfairness, without directly accessing the model itself (He et al., 2021)(Tokpo et al., 2022)Majumder et al., 2022;Dhingra et al., 2023)."], "score": 0.90380859375}, {"id": "(Dasu et al., 2025)", "paper": {"corpus_id": 277150560, "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Vishnu Asutosh Dasu", "authorId": "152123953"}, {"name": "Md. Rafi Ur Rashid", "authorId": "2150249269"}, {"name": "Vipul Gupta", "authorId": "2110652561"}, {"name": "Saeid Tizpaz-Niari", "authorId": "1404887808"}, {"name": "Gang Tan", "authorId": "2309172099"}], "n_citations": 1}, "snippets": ["Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train."], "score": 0.9658203125}, {"id": "(Tokpo et al., 2022)", "paper": {"corpus_id": 246210255, "title": "Text Style Transfer for Bias Mitigation using Masked Language Modeling", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "E. Tokpo", "authorId": "2145259446"}, {"name": "T. Calders", "authorId": "1709830"}], "n_citations": 34}, "snippets": ["It is well known that textual data on the internet and other digital platforms contain significant levels of bias and stereotypes. Various research findings have concluded that biased texts have significant effects on target demographic groups. For instance, masculine-worded job advertisements tend to be less appealing to female applicants. In this paper, we present a text-style transfer model that can be trained on non-parallel data and be used to automatically mitigate bias in textual data. Our style transfer model improves on the limitations of many existing text style transfer techniques such as the loss of content information. Our model solves such issues by combining latent content encoding with explicit keyword replacement. We will show that this technique produces better content preservation whilst maintaining good style transfer accuracy."], "score": 0.0}, {"id": "(Zayed et al., 2023)", "paper": {"corpus_id": 258832694, "title": "Should We Attend More or Less? Modulating Attention for Fairness", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "A. Zayed", "authorId": "2077390471"}, {"name": "Gon\u00e7alo Mordido", "authorId": "24039720"}, {"name": "S. Shabanian", "authorId": "3197429"}, {"name": "Sarath Chandar", "authorId": "123607932"}], "n_citations": 10}, "snippets": ["While relatively less explored, post-processing bias mitigation methods modify the predictions of a biased model and generate a new set of less biased predictions."], "score": 0.88232421875}, {"id": "(Qian et al., 2021)", "paper": {"corpus_id": 236459953, "title": "Counterfactual Inference for Text Classification Debiasing", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Chen Qian", "authorId": "2082474054"}, {"name": "Fuli Feng", "authorId": "2163400298"}, {"name": "L. Wen", "authorId": "40650846"}, {"name": "Chunping Ma", "authorId": "48168083"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 89}, "snippets": ["Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness."], "score": 0.0}, {"id": "(He et al., 2021)", "paper": {"corpus_id": 237634972, "title": "Detect and Perturb: Neutral Rewriting of Biased and Sensitive Text via Gradient-based Decoding", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zexue He", "authorId": "2116458151"}, {"name": "Bodhisattwa Prasad Majumder", "authorId": "3165738"}, {"name": "Julian McAuley", "authorId": "35660011"}], "n_citations": 29}, "snippets": ["Written language carries explicit and implicit biases that can distract from meaningful signals. For example, letters of reference may describe male and female candidates differently, or their writing style may indirectly reveal demographic characteristics. At best, such biases distract from the meaningful content of the text; at worst they can lead to unfair outcomes. We investigate the challenge of re-generating input sentences to 'neutralize' sensitive attributes while maintaining the semantic meaning of the original text (e.g. is the candidate qualified?). We propose a gradient-based rewriting framework, Detect and Perturb to Neutralize (DEPEN), that first detects sensitive components and masks them for regeneration, then perturbs the generation model at decoding time under a neutralizing constraint that pushes the (predicted) distribution of sensitive attributes towards a uniform distribution. Our experiments in two different scenarios show that DEPEN can regenerate fluent alternatives that are neutral in the sensitive attribute while maintaining the semantics of other attributes."], "score": 0.0}, {"id": "(Magee et al., 2021)", "paper": {"corpus_id": 236034024, "title": "Intersectional Bias in Causal Language Models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "L. Magee", "authorId": "2733075"}, {"name": "Lida Ghahremanlou", "authorId": "9557084"}, {"name": "K. Soldati\u0107", "authorId": "13714096"}, {"name": "S. Robertson", "authorId": "97868921"}], "n_citations": 33}, "snippets": ["Model-driven approaches treat the language model post-training. For models involving word embeddings, these embeddings can be inspected, and following a debiasing heuristic, modified directly. [10] for example conducts a component analysis to identify bias, and then manipulates embedding values, to either increase or decrease cosine values between category terms in accordance with the analysis. Such techniques appear more difficult to apply to transformer models with contextual embeddings. [35] for example examines both data augmentation and model neutralisation, and finds model neutralisation less effective."], "score": 0.89697265625}], "table": null}, {"title": "Specific Post-Training Mitigation Methods", "tldr": "Various post-training techniques have been developed to mitigate gender and racial bias in large language models, ranging from data manipulation approaches like Counterfactual Data Augmentation to embedding-focused methods like projection and clipping. These methods differ in their implementation complexity, computational requirements, and effectiveness at preserving model performance while reducing bias. (20 sources)", "text": "\n## Counterfactual Approaches\n\n1. **Counterfactual Data Augmentation (CDA)**: Creates balanced training datasets by replacing gender or racial terms with their counterparts (e.g., \"he\" with \"she\") to reduce biased associations <Paper corpusId=\"253734850\" paperTitle=\"(Zayed et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256390197\" paperTitle=\"(Tokpo et al., 2023)\" isShortName></Paper>. While effective, traditional CDA requires retraining and is more resource-intensive than some alternatives <Paper corpusId=\"256390197\" paperTitle=\"(Tokpo et al., 2023)\" isShortName></Paper>.\n\n2. **Counterfactual Data Substitution (CDS)**: An improvement over CDA that randomly substitutes potentially biased text rather than duplicating content, helping to mitigate indirect gender bias by reducing cluster purity by up to 49% <Paper corpusId=\"202541569\" paperTitle=\"(Maudslay et al., 2019)\" isShortName></Paper>.\n\n3. **Few-Shot Debiasing**: Fine-tuning on as few as 10 carefully debiased examples can significantly reduce gender bias while maintaining language modeling capabilities, offering a highly practical approach <Paper corpusId=\"259095603\" paperTitle=\"(Thakur et al., 2023)\" isShortName></Paper>.\n\n4. **Contrastive Debiasing**: Co\u00b2PT (Counterfactual Contrastive Prompt Tuning) efficiently mitigates bias by training different debiasing prompts for various bias dimensions that can be combined to address intersectional bias <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>.\n\n## Embedding Manipulation Techniques\n\n1. **Hard Debiasing**: Identifies and neutralizes a gender subspace in word embeddings while maintaining performance on evaluation tasks <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>.\n\n2. **Iterative Null-space Projection (INLP)**: Trains linear classifiers to predict gender from embeddings, then projects embeddings to the null-space of these classifiers to remove gender information <Paper corpusId=\"215786522\" paperTitle=\"(Ravfogel et al., 2020)\" isShortName></Paper> <Paper corpusId=\"271769656\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n3. **Feature Clipping**: A post-processing method that clips or prunes embedding dimensions that highly correlate with gender information, though this can cause unavoidable performance degradation in the main task <Paper corpusId=\"237490811\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"262828449\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>.\n\n4. **Sent-Debias**: Removes bias at the sentence level by identifying and removing biased dimensions in sentence embeddings, though some research suggests this may obscure rather than eliminate bias <Paper corpusId=\"207996257\" paperTitle=\"(Liang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"277150794\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\n## Inference-Time Methods\n\n1. **Self-Debias**: Adjusts next-token probabilities during generation based on the model's own prediction of how biased each potential token might be, requiring no manually curated word lists or parameter changes <Paper corpusId=\"232075876\" paperTitle=\"(Schick et al., 2021)\" isShortName></Paper> <Paper corpusId=\"271769656\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n2. **DEXPERTS**: Uses two models in parallel\u2014one promoting non-toxic text and another discouraging harmful content\u2014to improve output fairness without retraining <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n3. **Fair Filter (FairFil)**: Transforms pretrained encoder outputs into debiased representations via a neural network that minimizes correlation between filtered embeddings and bias words while preserving semantic information <Paper corpusId=\"232185104\" paperTitle=\"(Cheng et al., 2021)\" isShortName></Paper>.\n\n4. **Name-Scrambling**: Replaces gender-specific names with randomly selected alternatives, proving especially effective for reducing bias in dialogue models while maintaining performance <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>.\n\n## Causal and Knowledge-Based Approaches\n\n1. **Causal-Debias**: Integrates causal learning principles to distinguish between causal (label-relevant) and non-causal (bias-related) factors in token representations, directly addressing bias resurgence during fine-tuning <Paper corpusId=\"259370743\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273501862\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>.\n\n2. **DEPEN (Detect and Perturb to Neutralize)**: A gradient-based rewriting framework that detects sensitive components in text and masks them for regeneration under a neutralizing constraint <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\n3. **Auto-Debias**: Automatically crafts biased prompts to identify completions with high disagreement across demographic groups, then applies a distribution alignment loss to reduce bias, achieving 90-96% reduction in bias probability scores with only 3-16% reduction in performance <Paper corpusId=\"248780440\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268064051\" paperTitle=\"(Doughman et al., 2023)\" isShortName></Paper>.\n\n4. **LSDM (Least Square Debias Method)**: Uses causal mediation analysis and knowledge editing to mitigate gender bias in occupational pronouns by solving a matrix equation with constraint terms, avoiding catastrophic forgetting while targeting specific biases <Paper corpusId=\"268553687\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper>.\n\nThese techniques vary significantly in their implementation complexity, computational requirements, and effectiveness. Practitioners should select methods based on their specific use case, considering whether invariance to names is essential (name-scrambling), performance preservation is critical (controlled generation), or generalizable bias suppression is needed (unlikelihood training) <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Zayed et al., 2022)", "paper": {"corpus_id": 253734850, "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "A. Zayed", "authorId": "2077390471"}, {"name": "Prasanna Parthasarathi", "authorId": "32899078"}, {"name": "Gon\u00e7alo Mordido", "authorId": "24039720"}, {"name": "Hamid Palangi", "authorId": "2542427"}, {"name": "S. Shabanian", "authorId": "3197429"}, {"name": "Sarath Chandar", "authorId": "123607932"}], "n_citations": 22}, "snippets": ["To mitigate gender bias in language models, several techniques have been proposed. These methods can be broadly classified into three main categories: data-based methods (Lu et al., 2018)(Maudslay et al., 2019)(De-Arteaga et al., 2019); regularization-based methods (Maudslay et al., 2019)(Garg et al., 2018); and adversarial-based methods (Song et al. 2019;(Manzini et al., 2019)(Isabelle et al., 2017). Data-based methods, which are our focus in this work, change the training data to balance the bias through targeted data augmentation (Lu et al., 2018)(Maudslay et al., 2019), constructing counterfactual examples (Garg et al., 2018), or removing protected attributes from the input (De-Arteaga et al., 2019) to disallow models from learning any correlation between labels and gender words. Regularization-based methods add an auxiliary loss term to the objective function that reduces the amount of bias in the model. Adversarial-based methods use adversarial learning for bias mitigation."], "score": 0.900390625}, {"id": "(Tokpo et al., 2023)", "paper": {"corpus_id": 256390197, "title": "How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification", "year": 2023, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "E. Tokpo", "authorId": "2145259446"}, {"name": "Pieter Delobelle", "authorId": "150258834"}, {"name": "Bettina Berendt", "authorId": "2990203"}, {"name": "T. Calders", "authorId": "1709830"}], "n_citations": 8}, "snippets": ["We select three popular mitigation methods to represent all three types, namely Counterfactual Data Augmentation (CDA), Context-debias, and Sentdebias. Notice that these methods create debiased pretrained language models, as is illustrated in Figure 1. These models still need to be finetuned on a downstream task.\n\nCDA pretraining. The idea behind counterfactual data augmentation (Zmigrod et al., 2019;Lu et al., 2020) is to generate a counterfactual for each example in the training corpus by replacing attribute terms with their complimentary equivalent from the other demographic classes. For example, she will map to he in the case of binary gender. To mitigate intrinsic bias, this counterfactual augmentation has to be done as a pretraining step. Since CDA involves retraining the model, it is more resource-intensive compared to Sent-debias and Context-debias. We use the pretrained CDA models based on BERT and ALBERT from Webster et al. (2020) for our implementation."], "score": 0.8740234375}, {"id": "(Maudslay et al., 2019)", "paper": {"corpus_id": 202541569, "title": "It\u2019s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "R. Maudslay", "authorId": "1388061045"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Ryan Cotterell", "authorId": "1750769"}, {"name": "Simone Teufel", "authorId": "2480901"}], "n_citations": 172}, "snippets": ["This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation."], "score": 0.0}, {"id": "(Thakur et al., 2023)", "paper": {"corpus_id": 259095603, "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Himanshu Thakur", "authorId": "2221493995"}, {"name": "Atishay Jain", "authorId": "1819271266"}, {"name": "Praneetha Vaddamanu", "authorId": "2127734657"}, {"name": "Paul Pu Liang", "authorId": "28130078"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}], "n_citations": 37}, "snippets": ["While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability."], "score": 0.87109375}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 264305744, "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiangjue Dong", "authorId": "1716200686"}, {"name": "Ziwei Zhu", "authorId": "9725200"}, {"name": "Zhuoer Wang", "authorId": "96309563"}, {"name": "Maria Teleki", "authorId": "121003684"}, {"name": "James Caverlee", "authorId": "1697232"}], "n_citations": 11}, "snippets": ["Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks", "We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance", "Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks."], "score": 0.89501953125}, {"id": "(Bolukbasi et al., 2016)", "paper": {"corpus_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "year": 2016, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tolga Bolukbasi", "authorId": "2843215"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}, {"name": "James Y. Zou", "authorId": "145085305"}, {"name": "Venkatesh Saligrama", "authorId": "1699322"}, {"name": "A. Kalai", "authorId": "2186481"}], "n_citations": 3153}, "snippets": ["The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."], "score": 0.0}, {"id": "(Ravfogel et al., 2020)", "paper": {"corpus_id": 215786522, "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shauli Ravfogel", "authorId": "51432464"}, {"name": "Yanai Elazar", "authorId": "51131518"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Michael Twiton", "authorId": "102707804"}, {"name": "Yoav Goldberg", "authorId": "79775260"}], "n_citations": 388}, "snippets": ["The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 271769656, "title": "A Parameter-Efficient Multi-Objective Approach to Mitigate Stereotypical Bias in Language Models", "year": 2024, "venue": "GEBNLP", "authors": [{"name": "Yifan Wang", "authorId": "2293409512"}, {"name": "Vera Demberg", "authorId": "2293393732"}], "n_citations": 1}, "snippets": ["Post-hoc: Iterative null-space projection (INLP) (Ravfogel et al., 2020) trains a set of linear classifiers to predict genders from embeddings and then projects embeddings to the null-space of learned classifiers. Self-Debias (Schick et al., 2021) adjusts next token probabilities at each step according to model's prediction to what extent the next token is biased."], "score": 0.8642578125}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 237490811, "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jialu Wang", "authorId": "46583583"}, {"name": "Yang Liu", "authorId": "2152799147"}, {"name": "X. Wang", "authorId": "47120131"}], "n_citations": 95}, "snippets": ["Internet search affects people\u2019s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models."], "score": 0.0}, {"id": "(Lee et al., 2023)", "paper": {"corpus_id": 262828449, "title": "Survey of Social Bias in Vision-Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Nayeon Lee", "authorId": "40221187"}, {"name": "Yejin Bang", "authorId": "23672613"}, {"name": "Holy Lovenia", "authorId": "116344405"}, {"name": "Samuel Cahyawijaya", "authorId": "66986482"}, {"name": "Wenliang Dai", "authorId": "47653392"}, {"name": "Pascale Fung", "authorId": "1683412"}], "n_citations": 18}, "snippets": ["\u2022 Feature Clipping. Wang et al. (Wang et al., 2021) propose a post-processing mitigation method that clips/prunes the dimensions of feature embeddings that are highly correlated with gender information. This idea is motivated by the fact that an unbiased retrieve implies the independence between the covariates (active features) and sensitive attributes (gender). However, the limitation of this approach is that it results in unavoidable performance degradation in the main task performance. \u2022 Instruction/Prompt Engineering. Friedrich et al. [62] propose a post-processing mitigation approach, Fair Diffusion, that is inspired by advances in instructing AI systems based on human feedback. Fair Diffusion \"instructs\" the pre-trained diffusion models to be fair during the deployment stage; it enables precise guidance to reduce biases in model outcomes based on pre-defined instructions stored in a lookup table. \u2022 Bias vector projection. Chuang et al. [38] de-bias VL foundation models by projecting out biased directions (gender bias direction, racial bias direction) in the text embedding. These bias directions are obtained from embeddings of spurious prompts such as \"a photo of a [irrelevant attribute]\". The authors also calibrate the projection matrix by introducing one additional regularization constraint that ensures the debiased prompt representation still has the same semantic meaning after the projection. To elaborate, \"a photo of a [class name] with [spurious attribute]\" should still have the same semantic meaning as \"a photo of a [class name]\". The biggest advantage of this method is that it is computationally lightweight and simple, as it only manipulates text embedding."], "score": 0.90625}, {"id": "(Liang et al., 2020)", "paper": {"corpus_id": 207996257, "title": "Towards Debiasing Sentence Representations", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Paul Pu Liang", "authorId": "28130078"}, {"name": "Irene Z Li", "authorId": "47841931"}, {"name": "Emily Zheng", "authorId": "2064345025"}, {"name": "Y. Lim", "authorId": "144529448"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}], "n_citations": 240}, "snippets": ["As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP."], "score": 0.0}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 277150794, "title": "Enforcing Consistency and Fairness in Multi-level Hierarchical Classification with a Mask-based Output Layer", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shijing Chen", "authorId": "2301225724"}, {"name": "Shoaib Jameel", "authorId": "38797620"}, {"name": "Mohamed Reda Bouadjenek", "authorId": "3080469"}, {"name": "Feilong Tang", "authorId": "2338268075"}, {"name": "Usman Naseem", "authorId": "2301210198"}, {"name": "Basem Suleiman", "authorId": "2260341001"}, {"name": "Hakim Hacid", "authorId": "2296782788"}, {"name": "Flora D. Salim", "authorId": "2253483164"}, {"name": "Imran Razzak", "authorId": "2240180680"}], "n_citations": 0}, "snippets": ["Post-processing methods such as Sent-debias work by removing gender bias from pretrained model embeddings, though research suggests that these methods often obscure rather than fully eliminate bias (Liang et al., 2020;Gonen and Goldberg, 2019)."], "score": 0.9091796875}, {"id": "(Schick et al., 2021)", "paper": {"corpus_id": 232075876, "title": "Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Timo Schick", "authorId": "32246932"}, {"name": "Sahana Udupa", "authorId": "40941940"}, {"name": "Hinrich Sch\u00fctze", "authorId": "144418438"}], "n_citations": 387}, "snippets": ["Abstract \u26a0 This paper contains prompts and model outputs that are offensive in nature. When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model\u2019s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1"], "score": 0.0}, {"id": "(Cheng et al., 2021)", "paper": {"corpus_id": 232185104, "title": "FairFil: Contrastive Neural Debiasing Method for Pretrained Text Encoders", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Pengyu Cheng", "authorId": "144533942"}, {"name": "Weituo Hao", "authorId": "3314779"}, {"name": "Siyang Yuan", "authorId": "2087092426"}, {"name": "Shijing Si", "authorId": "81065454"}, {"name": "L. Carin", "authorId": "145006560"}], "n_citations": 104}, "snippets": ["Pretrained text encoders, such as BERT, have been applied increasingly in various natural language processing (NLP) tasks, and have recently demonstrated significant performance gains. However, recent studies have demonstrated the existence of social bias in these pretrained NLP models. Although prior works have made progress on word-level debiasing, improved sentence-level fairness of pretrained encoders still lacks exploration. In this paper, we proposed the first neural debiasing method for a pretrained sentence encoder, which transforms the pretrained encoder outputs into debiased representations via a fair filter (FairFil) network. To learn the FairFil, we introduce a contrastive learning framework that not only minimizes the correlation between filtered embeddings and bias words but also preserves rich semantic information of the original sentences. On real-world datasets, our FairFil effectively reduces the bias degree of pretrained text encoders, while continuously showing desirable performance on downstream tasks. Moreover, our post-hoc method does not require any retraining of the text encoders, further enlarging FairFil's application space."], "score": 0.0}, {"id": "(Smith et al., 2021)", "paper": {"corpus_id": 237442178, "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Eric Michael Smith", "authorId": "51324296"}, {"name": "Adina Williams", "authorId": "81840293"}], "n_citations": 28}, "snippets": ["We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion."], "score": 0.89208984375}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 259370743, "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Fan Zhou", "authorId": "144315735"}, {"name": "Yuzhou Mao", "authorId": "2221367541"}, {"name": "Liu Yu", "authorId": "38057162"}, {"name": "Yi Yang", "authorId": "2143686211"}, {"name": "Ting Zhong", "authorId": "46456474"}], "n_citations": 41}, "snippets": ["Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."], "score": 0.86865234375}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 273501862, "title": "Causality for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Anpeng Wu", "authorId": "1748975704"}, {"name": "Kun Kuang", "authorId": "2315590986"}, {"name": "Minqin Zhu", "authorId": "2292675528"}, {"name": "Yingrong Wang", "authorId": "2290028937"}, {"name": "Yujia Zheng", "authorId": "2309464222"}, {"name": "Kairong Han", "authorId": "2315988838"}, {"name": "Baohong Li", "authorId": "2313166010"}, {"name": "Guan-Hong Chen", "authorId": "2155315836"}, {"name": "Fei Wu", "authorId": "2290035617"}, {"name": "Kun Zhang", "authorId": "2316094914"}], "n_citations": 9}, "snippets": ["Causal-Debias (Zhou et al., 2023). The Causal-Debias framework offers a pioneering solution to mitigating biases in pretrained language models (PLMs) by integrating causal learning principles into the fine-tuning process, which could be directly applied to the pre-training processes. Unlike conventional methods that separate bias mitigation and task performance optimization, Causal-Debias merges these goals through the use of causal interventions and invariant risk minimization (IRM). The framework effectively distinguishes between causal (label-relevant) and non-causal (bias-related) factors embedded in token representations, addressing spurious correlations resulting from demographic biases and social stereotypes. By generating counterfactual data-modifying bias-related attributes such as gendered or racial terms-and training the model with an invariant loss across diverse environments, Causal-Debias ensures that the model generalizes well across tasks while mitigating biases. This approach directly tackles the challenge of bias resurgence, where previously mitigated biases reappear during fine-tuning, a limitation observed in many existing debiasing techniques."], "score": 0.95703125}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 248780440, "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yue Guo", "authorId": null}, {"name": "Yi Yang", "authorId": "46285693"}, {"name": "A. Abbasi", "authorId": "144849629"}], "n_citations": 167}, "snippets": ["Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark."], "score": 0.9501953125}, {"id": "(Doughman et al., 2023)", "paper": {"corpus_id": 268064051, "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models", "year": 2023, "venue": "EVAL4NLP", "authors": [{"name": "Jad Doughman", "authorId": "1724523030"}, {"name": "Shady Shehata", "authorId": "38510157"}, {"name": "L. A. Qadi", "authorId": "1452348515"}, {"name": "Youssef Nafea", "authorId": "2292591921"}, {"name": "F. Karray", "authorId": "1696863"}], "n_citations": 0}, "snippets": ["Auto-Debias. Auto-Debias (Guo et al., 2022)) is a debiasing technique for masked language models that does not entail referencing external corpora. Auto-Debias contains two stages: First, automatically crafting biased prompts, such that the clozestyle completions have the highest disagreement in generating stereotype words with respect to demographic groups. Second, debiasing the language model by a distribution alignment loss", "Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks."], "score": 0.9267578125}, {"id": "(Cai et al., 2024)", "paper": {"corpus_id": 268553687, "title": "Locating and Mitigating Gender Bias in Large Language Models", "year": 2024, "venue": "International Conference on Intelligent Computing", "authors": [{"name": "Yuchen Cai", "authorId": "2292681106"}, {"name": "Ding Cao", "authorId": "2237226610"}, {"name": "Rongxi Guo", "authorId": "2237261799"}, {"name": "Yaqin Wen", "authorId": "2293239741"}, {"name": "Guiquan Liu", "authorId": "2237403462"}, {"name": "Enhong Chen", "authorId": "2292390890"}], "n_citations": 5}, "snippets": ["In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compare it against two baselines on three gender bias datasets and seven knowledge competency test datasets", "LSDM modifies parameters by solving a matrix equation with constraint terms, enabling us to minimize interference with other aspects of the model while specifically mitigating gender bias associated with certain occupation words. LSDM overcomes the catastrophic forgetting problem that exists in all other debiasing methods and stands out by avoiding additional reinforcement learning or human annotations, basing its approach on causal trace conclusions rather than just black-box fine-tuning."], "score": 0.87158203125}], "table": null}, {"title": "Performance Trade-offs and Effectiveness", "tldr": "Post-training bias mitigation techniques typically involve trade-offs between bias reduction and model performance, with most methods showing some degradation in language modeling capabilities when bias is reduced. The effectiveness of different mitigation approaches varies significantly based on bias type, downstream task, and implementation details, highlighting the challenge of developing a universal debiasing solution. (5 sources)", "text": "\nWhen implementing post-training bias mitigation techniques, practitioners face inevitable trade-offs between reducing bias and maintaining model performance. Multiple studies have empirically demonstrated that while these approaches effectively reduce bias, they often compromise language modeling performance, as indicated by increased perplexity on unbiased text <Paper corpusId=\"271404523\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"239015827\" paperTitle=\"(Meade et al., 2021)\" isShortName></Paper>. This performance degradation is particularly notable when measuring impacts on downstream natural language understanding tasks.\n\nThe extent of performance trade-offs varies across mitigation methods. Surprisingly, combining multiple techniques doesn't always yield better results. For instance, Park et al. found that applying both debiased embedding and gender swap techniques to a GRU model reduced equality differences by up to 98% while sacrificing only 1.5% of the original performance <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>. However, when they combined three methods together, they observed the largest decrease in original performance despite improvements in reducing bias <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>.\n\nAn important insight is that the effectiveness of bias mitigation techniques depends on several contextual factors:\n\n1. **Bias type complexity**: Token-based mitigation works better for simpler biases like gender bias than for more complex cultural biases <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n2. **Task characteristics**: Classification tasks typically respond better to mitigation efforts than detailed generation tasks. For example, mask-based mitigation is particularly effective for cultural classification tasks at lower bias ratios (5%), where explicit biased terms have more direct impact <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n3. **Bias ratio and data distribution**: Loss-based mitigation effectiveness depends significantly on the distribution distance between augmentation and original data, with smaller proportions (5-10%) of augmentation data often yielding better results by introducing subtle shifts without overfitting <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nOne concerning finding is that debiasing techniques can sometimes create a disconnect between surface-level outputs and internal representations. Zakizadeh et al. discovered that some methods inadvertently increase encoded bias in internal representations while successfully reducing bias in model output distributions <Paper corpusId=\"276902427\" paperTitle=\"(Zakizadeh et al., 2025)\" isShortName></Paper>. This suggests that current approaches may be masking rather than truly eliminating bias.\n\nAdditionally, some post-hoc techniques manage to retain language modeling performance but fail to detect and mitigate more subtle and implicit toxic content <Paper corpusId=\"271404523\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This highlights the need for more sophisticated methods that can address nuanced forms of bias without compromising model utility.\n\nThe performance trade-offs encountered in bias mitigation efforts stem largely from how these methods modify data or model architecture. As Park et al. observed, the performance loss likely occurs because mitigation methods sometimes deter models from discriminating important \"unbiased\" features while addressing the biased ones <Paper corpusId=\"52070035\" paperTitle=\"(Park et al., 2018)\" isShortName></Paper>. \n\nThese nuanced effects across different scenarios highlight the significant challenge in developing a unified mitigation solution that works consistently across all bias types, tasks, and model architectures <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Future research must continue to explore this complex balance between bias reduction and maintaining model capabilities.", "citations": [{"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271404523, "title": "Towards Transfer Unlearning: Empirical Evidence of Cross-Domain Bias Mitigation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Huimin Lu", "authorId": "2312881526"}, {"name": "Masaru Isonuma", "authorId": "24905917"}, {"name": "Junichiro Mori", "authorId": "49010536"}, {"name": "Ichiro Sakata", "authorId": "2265001653"}], "n_citations": 1}, "snippets": ["Numerous studies have been conducted to mitigate the bias and toxicity inherent in LLMs (Zhao et al., 2018)(Barikeri et al., 2021)(Liang et al., 2020)(Ravfogel et al., 2020)(Schick et al., 2021).However, recent studies (Meade et al., 2021) empiri-cally show that these approaches are effective in reducing bias while compromising the language modeling performance, as indicated by increased perplexity on unbiased text.Notably, existing posthoc techniques manage to retain language modeling performance, but they fail to detect more subtle and implicit toxic content."], "score": 0.92041015625}, {"id": "(Meade et al., 2021)", "paper": {"corpus_id": 239015827, "title": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nicholas Meade", "authorId": "150247363"}, {"name": "Elinor Poole-Dayan", "authorId": "2133330526"}, {"name": "Siva Reddy", "authorId": "145732771"}], "n_citations": 128}, "snippets": ["Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model\u2019s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective."], "score": 0.88671875}, {"id": "(Park et al., 2018)", "paper": {"corpus_id": 52070035, "title": "Reducing Gender Bias in Abusive Language Detection", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ji Ho Park", "authorId": "2116023415"}, {"name": "Jamin Shin", "authorId": "51228826"}, {"name": "Pascale Fung", "authorId": "1683412"}], "n_citations": 341}, "snippets": ["To our surprise, the most effective method was applying both debiased embedding and gender swap to GRU, which reduced the equality differences by 98% & 89% while losing only 1.5% of the original performance. We assume that this may be related to the influence of \"attending\" model architectures on biases as discussed in Section 4.3. On the other hand, using the three methods together improved both generated unbiased set performance and equality differences, but had the largest decrease in the original performance. \n\nAll methods involved some performance loss when gender biases were reduced. Especially, fine-tuning had the largest decrease in original test set performance. This could be attributed to the difference in the source and target tasks (abusive & sexist). However, the decrease was marginal (less than 4%), while the drop in bias was significant. We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important \"unbiased\" features."], "score": 0.8779296875}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276235453, "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Miaomiao Li", "authorId": "2344641665"}, {"name": "Hao Chen", "authorId": "2261741520"}, {"name": "Yang Wang", "authorId": "2344936960"}, {"name": "Tingyuan Zhu", "authorId": "2331767016"}, {"name": "Weijia Zhang", "authorId": "2344790620"}, {"name": "Kaijie Zhu", "authorId": "2313725635"}, {"name": "Kam-Fai Wong", "authorId": "2345348279"}, {"name": "Jindong Wang", "authorId": "2285254341"}], "n_citations": 4}, "snippets": ["The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."], "score": 0.8974609375}, {"id": "(Zakizadeh et al., 2025)", "paper": {"corpus_id": 276902427, "title": "Gender Encoding Patterns in Pretrained Language Model Representations", "year": 2025, "venue": "Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025)", "authors": [{"name": "Mahdi Zakizadeh", "authorId": "2261278375"}, {"name": "Mohammad Taher Pilehvar", "authorId": "1717641"}], "n_citations": 0}, "snippets": ["Despite growing awareness, there is a lack of comprehensive investigation into how different models internally represent and propagate such biases", "Surprisingly, debiasing techniques often exhibit limited efficacy, sometimes inadvertently increasing the encoded bias in internal representations while reducing bias in model output distributions. This highlights a disconnect between mitigating bias in output distributions and addressing its internal representations."], "score": 0.85791015625}], "table": null}, {"title": "Comparative Analysis of Mitigation Techniques", "tldr": "The effectiveness of bias mitigation techniques varies significantly based on the type of bias, downstream task, and implementation context, with no single universal solution emerging as optimal across all scenarios. Different approaches have distinct strengths and limitations in terms of effectiveness, transparency, and cultural sensitivity, requiring practitioners to select methods based on their specific use cases. (3 sources)", "text": "\nWhen comparing post-training bias mitigation techniques for LLMs, research reveals that effectiveness varies substantially across different contexts, making it challenging to identify a single best approach. Smith et al. found that while name-scrambling techniques most effectively reduced gender bias as measured by classifier detection and token distribution metrics, controlled-generation approaches were rated more favorably by human evaluators for maintaining natural dialogue quality, and unlikelihood-training demonstrated superior performance on downstream tasks where gender was explicitly stated <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>. This variation suggests that the optimal choice of mitigation technique depends heavily on the specific application requirements.\n\nThe effectiveness of different mitigation strategies also varies based on the complexity of the bias being addressed. Token-based mitigation approaches work better for addressing simpler biases like gender compared to more complex cultural biases, and they tend to perform better in classification tasks than in detailed generation tasks <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This is likely because token-based techniques rely on the model's inherent understanding of implicit cues, making them less effective for nuanced biases that require more sophisticated handling.\n\nThe implementation context also significantly impacts effectiveness. Mask-based mitigation techniques show more noticeable effects at lower bias ratios (around 5%), particularly in tasks like cultural classification where explicit biased terms have direct impact. In contrast, loss-based mitigation approaches depend primarily on the distribution distance between augmentation and original data, with smaller proportions (5-10%) of augmentation data often yielding better results by introducing subtle shifts without overfitting <Paper corpusId=\"276235453\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\nBeyond effectiveness, mitigation techniques also differ in transparency and cultural sensitivity. Adversarial training methods, while effective at reducing certain explicit biases, often score lower on transparency due to their complex and opaque implementation. Conversely, model auditing approaches tend to rate higher for transparency but show limitations in addressing subtle or implicit biases <Paper corpusId=\"270869465\" paperTitle=\"(Lim et al., 2024)\" isShortName></Paper>. These multidimensional considerations highlight the importance of holistic evaluation frameworks that consider not just bias reduction but also practical implementation factors.\n\nThe comparative evidence strongly suggests that practitioners should select mitigation techniques based on their specific use cases. For applications where name invariance is critical (such as task-oriented dialogue for scheduling), name-scrambling approaches may be most appropriate. When preserving natural dialogue quality is essential, controlled-generation techniques might be preferable. For applications requiring broader bias suppression beyond names, unlikelihood training can provide more generalizable results <Paper corpusId=\"237442178\" paperTitle=\"(Smith et al., 2021)\" isShortName></Paper>. This context-dependent effectiveness underscores the current reality that no universal debiasing solution exists that works consistently across all bias types, tasks, and model architectures.", "citations": [{"id": "(Smith et al., 2021)", "paper": {"corpus_id": 237442178, "title": "Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Eric Michael Smith", "authorId": "51324296"}, {"name": "Adina Williams", "authorId": "81840293"}], "n_citations": 28}, "snippets": ["We investigate several sources of bias in large dialogue models, and employ a few methods to reduce bias by gender and race/ethnicity in Blender-Bot while maintaining similar levels of performance. We find that the name-scrambling model most effectively reduces bias as measured by a gender bias classifier and most closely matches the token distribution across female/male self-chats; the controlled-generation model is rated by crowdsourced workers as being most preferable and most human; and the unlikelihood-training model has the lowest gender-classifier bias on a downstream task where one of the speakers explicitly states their gender. \n\nPractitioners should choose mitigation techniques that match their use case: the namescrambling method might be most applicable for a model where invariance to names is of most essential importance (e.g., in task oriented dialogue, when scheduling meetings with individuals); the controlled-generation method might be ideal for reducing bias on dialogue while ensuring no loss of performance; and the unlikelihoodtraining method may be best for suppressing gender bias on more than just names in a generalizable fashion."], "score": 0.89208984375}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276235453, "title": "Understanding and Mitigating the Bias Inheritance in LLM-based Data Augmentation on Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Miaomiao Li", "authorId": "2344641665"}, {"name": "Hao Chen", "authorId": "2261741520"}, {"name": "Yang Wang", "authorId": "2344936960"}, {"name": "Tingyuan Zhu", "authorId": "2331767016"}, {"name": "Weijia Zhang", "authorId": "2344790620"}, {"name": "Kaijie Zhu", "authorId": "2313725635"}, {"name": "Kam-Fai Wong", "authorId": "2345348279"}, {"name": "Jindong Wang", "authorId": "2285254341"}], "n_citations": 4}, "snippets": ["The average mitigation results of severe negative influences for each task and bias are shown in Figure 9. A detailed analysis of the mitigation effects on gender and cultural biases across different tasks, as well as the results on GPT-4o-mini, are provided in Appendix E.2. Our key conclusion is that, similar to the nuanced effects on downstream tasks, the effectiveness of different mitigation approaches is also nuanced, depending on various factors such as different types of bias, downstream tasks, bias ratios, and more, highlighting the difficulty in devising a unified mitigation solution. Specifically, token-based mitigation provides implicit cues and works best with simple biases and tasks, as it depends on the model's own understanding. It is more effective for gender bias than complex cultural bias and performs better in classification tasks than in detailed generation tasks. For gender generation tasks, salary recommendation benefits more than complex hiring recommendation. More augmentation data (50%) further enhance its performance. \n\nMask-based mitigation shows noticeable effects at lower bias ratios (5%), especially in tasks like cultural classification. As at this ratio, explicit biased terms have a more direct and pronounced impact on the model's performance. By masking these terms, the model is less likely to rely on biased information or features that could skew its decision-making, thereby reducing bias inheritance. However, as bias ratios increase, the influence of more subtle and implicit biases grows, necessitating more complex mitigation strategies. It also proves effective in contrastive explicit bias, where direct bias information is more clearly identifiable. \n\nLoss-based mitigation primarily depends on the distribution distance between the augmentation and the original data, showing significant effectiveness when the distance is large. It is more suitable for coarse-grained tasks. For example, it works well in classification tasks where decisions often rely on broader patterns. In generation tasks, coarse-grained contexts like salary recommendations perform better than finer-grained ones like hiring recommendation. Additionally, smaller proportions (e.g., 5% and 10%) of augmentation data yield better results, by introducing subtle shifts that avoid overfitting to the augmented distribution while effectively influencing the original bias."], "score": 0.8974609375}, {"id": "(Lim et al., 2024)", "paper": {"corpus_id": 270869465, "title": "The African Woman is Rhythmic and Soulful: An Investigation of Implicit Biases in LLM Open-ended Text Generation", "year": 2024, "venue": "", "authors": [{"name": "Serene Lim", "authorId": "2309199058"}, {"name": "Mar'ia P'erez-Ortiz", "authorId": "2323507494"}], "n_citations": 2}, "snippets": ["We therefore developed a scoring methodology used to evaluate bias mitigation techniques in Large Language Models (LLMs) to offer a comparative analysis that highlights both the strengths and limitations of each approach. Each technique was assessed across multiple dimensions -transparency, cultural sensitivity, and effectiveness. The scores for these dimensions were derived through a combination of literature review, empirical findings, and qualitative evaluations of how these techniques perform in practical settings. For example, adversarial training received a high score for its effectiveness in reducing certain explicit biases but was marked lower for transparency due to the complexity and opacity of its implementation. In contrast, model auditing scored higher for transparency but displayed limitations in direct bias mitigation, particularly in subtle or implicit bias contexts."], "score": 0.87353515625}], "table": null}, {"title": "Implementation Considerations", "tldr": "Implementing post-training bias mitigation techniques for LLMs requires careful consideration of various practical factors including computational resources, technical complexity, and contextual appropriateness. Organizations must evaluate trade-offs between immediate deployment needs and long-term bias reduction goals when selecting and implementing debiasing approaches. (3 sources)", "text": "\n## Resource Requirements and Complexity\n- **Computational Resources**: Post-processing approaches are often preferred when retraining is not feasible due to limited computational resources, as they can be applied after model training without requiring extensive recomputation. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>\n\n- **Implementation Difficulty**: Different mitigation techniques vary significantly in their implementation complexity, with some requiring specialized expertise in adversarial methods or causal inference.\n\n- **Deployment Flexibility**: Some techniques like Co\u00b2PT (Counterfactual Contrastive Prompt Tuning) offer flexibility to mitigate different bias types without retraining the entire model, allowing practitioners to develop separate debiasing prompts for different dimensions (gender, race, religion) that can be combined as needed. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n## Practical Application Considerations\n- **Domain Specificity**: Bias mitigation approaches should be selected based on the specific domain and application context, as effectiveness varies across different tasks and bias types.\n\n- **Monitoring Requirements**: Implementing bias mitigation requires ongoing monitoring systems to track effectiveness and potential unintended consequences over time. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n- **Maintenance Overhead**: Some techniques require regular updates as language evolves or as new biases are identified, creating ongoing maintenance requirements. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## Ethical and Organizational Factors\n- **Transparency Trade-offs**: Organizations must consider whether the selected mitigation approach provides sufficient transparency to explain how bias is being addressed.\n\n- **Cost-Benefit Analysis**: Post-processing approaches can be particularly valuable when dataset collection and LLM training are prohibitively expensive processes that cannot be repeated if fairness issues are discovered. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper>\n\n- **Regulatory Compliance**: Different jurisdictions may have varying requirements regarding AI fairness and bias mitigation, necessitating specific implementation approaches to ensure compliance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\n## Integration Challenges\n- **Pipeline Compatibility**: Techniques like keyword-based distillation for inference-time bias removal must be integrated carefully into existing inference pipelines. <Paper corpusId=\"277150560\" paperTitle=\"(Dasu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"236459953\" paperTitle=\"(Qian et al., 2021)\" isShortName></Paper>\n\n- **Multi-bias Handling**: Organizations implementing debiasing systems should consider how different techniques can be combined to address intersectional biases across multiple dimensions simultaneously. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n- **Performance Monitoring**: Implementing mitigation techniques requires systems to monitor both bias reduction and potential performance impacts on downstream tasks. <Paper corpusId=\"264305744\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>", "citations": [{"id": "(Dasu et al., 2025)", "paper": {"corpus_id": 277150560, "title": "Attention Pruning: Automated Fairness Repair of Language Models via Surrogate Simulated Annealing", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Vishnu Asutosh Dasu", "authorId": "152123953"}, {"name": "Md. Rafi Ur Rashid", "authorId": "2150249269"}, {"name": "Vipul Gupta", "authorId": "2110652561"}, {"name": "Saeid Tizpaz-Niari", "authorId": "1404887808"}, {"name": "Gang Tan", "authorId": "2309172099"}], "n_citations": 1}, "snippets": ["Post-processing techniques modify the model's inference behavior after complete training. This may involve altering the model before inference or the model output during/after inference. For instance, Gehman et al. [24] proposed token-blocking methods during decoding to prevent the generation of harmful or biased terms. \n\nHauzenberger et al. [33] introduced sparse debiasing subnetworks that are trained separately and can be applied to the model at inference time. Qian et al. (Qian et al., 2021) performed keyword-based distillation to remove bias during inference. Tokpo and Calders [61] identify biased tokens and replace them with less stereotypical terms. Postprocessing approaches are beneficial since dataset collection and LLM training are expensive processes that we may not always be able to repeat if issues of unfairness are found. Furthermore, postprocessing mitigation approaches are sometimes the only viable option with the status quo of utilizing large pre-trained LLMs that take a huge amount of resources to train."], "score": 0.9658203125}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 264305744, "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiangjue Dong", "authorId": "1716200686"}, {"name": "Ziwei Zhu", "authorId": "9725200"}, {"name": "Zhuoer Wang", "authorId": "96309563"}, {"name": "Maria Teleki", "authorId": "121003684"}, {"name": "James Caverlee", "authorId": "1697232"}], "n_citations": 11}, "snippets": ["Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks", "We evaluate its effectiveness on bias mitigation and applicability to existing debiased upstream models, and investigate how the design of each component and the selection of hyperparameters impact both its bias reduction capabilities and downstream task performance", "Without re-training the model, Co 2 PT is flexible to apply in order to mitigate different bias types in downstream applications. One can train different debiasing prompts to tackle different bias dimensions such as gender, race, and religion. Furthermore, these debiasing prompts can be applied to mitigate intersectional bias by simply combining the corresponding prompts in downstream tasks."], "score": 0.89501953125}, {"id": "(Qian et al., 2021)", "paper": {"corpus_id": 236459953, "title": "Counterfactual Inference for Text Classification Debiasing", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Chen Qian", "authorId": "2082474054"}, {"name": "Fuli Feng", "authorId": "2163400298"}, {"name": "L. Wen", "authorId": "40650846"}, {"name": "Chunping Ma", "authorId": "48168083"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 89}, "snippets": ["Today\u2019s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models\u2019 generalization. Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases. Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies. Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations. Inspired by this, we propose a model-agnostic text classification debiasing framework \u2013 Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms. Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases \u2018poison\u2019 the trained model. In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model. Extensive experiments demonstrate Corsair\u2019s effectiveness, generalizability and fairness."], "score": 0.0}], "table": null}], "cost": 0.47095499999999996}}
