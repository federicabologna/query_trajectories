{"better_query": "How does temperature scaling in the softmax function facilitate the transfer of dark knowledge during knowledge distillation from teacher to student models?", "better_answer": {"sections": [{"title": "Introduction to Knowledge Distillation and Dark Knowledge", "tldr": "Knowledge distillation transfers knowledge from a larger teacher model to a smaller student model using soft probability distributions. The \"dark knowledge\" embedded in these soft targets reveals inter-class relationships that aren't captured by hard labels alone. (11 sources)", "text": "\nKnowledge distillation, introduced by Hinton et al., is a technique for transferring knowledge from a complex teacher model to a simpler student model while preserving generalization ability <Paper corpusId=\"234353710\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper> <Paper corpusId=\"256900863\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This approach addresses the challenge of deploying large ensemble models, which despite their superior performance, can be computationally expensive and cumbersome for widespread use <Paper corpusId=\"234353710\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\nAt the core of knowledge distillation is the concept of \"dark knowledge,\" which refers to the information embedded in the probability distributions produced by the teacher model <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper>. Unlike traditional training methods that rely solely on hard labels (one-hot encodings), knowledge distillation leverages soft targets\u2014probability distributions that reveal how the teacher model relates different classes to each other <Paper corpusId=\"212644537\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper>. These soft targets contain valuable information about inter-class relationships that aren't captured by one-hot labels alone <Paper corpusId=\"212644537\" paperTitle=\"(Deng et al., 2020)\" isShortName></Paper> <Paper corpusId=\"235421655\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper>.\n\nThe distillation process typically involves training the student model using a weighted sum of two losses: (1) a standard cross-entropy loss with hard labels and (2) a cross-entropy loss with soft labels produced by the teacher model <Paper corpusId=\"239616535\" paperTitle=\"(Jin et al., 2021)\" isShortName></Paper>. This dual optimization approach allows the student to both learn the correct classifications and mimic the rich inter-class knowledge captured by the teacher <Paper corpusId=\"267335043\" paperTitle=\"(Ralambomihanta et al., 2024)\" isShortName></Paper>.\n\nA key challenge in knowledge distillation is that well-trained models tend to produce very high probabilities for the correct class and near-zero probabilities for others, making the soft labels nearly identical to hard labels and limiting the transfer of dark knowledge <Paper corpusId=\"237002008\" paperTitle=\"(Lin et al., 2021)\" isShortName></Paper>. To address this issue, knowledge distillation employs temperature scaling in the softmax function, which controls the \"softness\" of the probability distributions and reveals the dark knowledge embedded in the teacher model <Paper corpusId=\"234336288\" paperTitle=\"(Jaiswal et al., 2021)\" isShortName></Paper> <Paper corpusId=\"265444951\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nBeyond classification tasks, dark knowledge also serves important functions in other domains. For example, in communication systems involving lossy compression, the teacher model's output can provide a feasible sub-optimal solution that is more beneficial for training than learning directly from ground-truth data that represents an infeasible perfect solution <Paper corpusId=\"260447668\" paperTitle=\"(Cui et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2019)", "paper": {"corpus_id": 234353710, "title": "Certainty driven consistency loss on multi-teacher networks for semi-supervised learning", "year": 2019, "venue": "Pattern Recognition", "authors": [{"name": "Lu Liu", "authorId": "2145288039"}, {"name": "R. Tan", "authorId": "1726720"}], "n_citations": 32}, "snippets": ["Hinton et al. (Hinton et al., 2015) apply the concept of temperature in model distillation, which aims to distill the knowledge from a large pre-trained network to a much smaller network without lossing much of the generalization ability. The temperature, a hyperparameter inside softmax function, is used to soften the probability distributions of softmax, which encourages the small model to learn more \"dark knowledge\" distributions from the large model, rather than the hard label."], "score": 0.6943359375}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 256900863, "title": "Fuzzy Knowledge Distillation from High-Order TSK to Low-Order TSK", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiongtao Zhang", "authorId": "2135847074"}, {"name": "Zezong Yin", "authorId": "2206403890"}, {"name": "Yunliang Jiang", "authorId": "3247526"}, {"name": "Yizhang Jiang", "authorId": "1390650781"}, {"name": "Da-Song Sun", "authorId": "2340356"}, {"name": "Yong Liu", "authorId": "2189281"}], "n_citations": 1}, "snippets": ["Knowledge distillation (Hinton et al., 2015) transfers the dark knowledge from complex/large model, namely the teacher model, to simple/small model, namely the student model, and hence improve the performance of student model. Specifically, knowledge distillation introduces the hyper-parameter temperature in softmax function to obtain soft labels at first, then calculates the KL divergence of soft labels and the cross-entropy of student model output and the ground-truth label, finally transfers the dark knowledge via soft labels from teacher model to student model, improves the performance of student model, as shown in Fig. 1."], "score": 0.783203125}, {"id": "(Jaiswal et al., 2021)", "paper": {"corpus_id": 234336288, "title": "Performance Analysis of Deep Neural Network based on Transfer Learning for Pet Classification", "year": 2021, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Bhavesh Jaiswal", "authorId": "2139643077"}, {"name": "Nagendra Gajjar", "authorId": "32136431"}], "n_citations": 0}, "snippets": ["For this, Hinton [30], introduced the concept of \"softmax temperature\". As it grows, the probability distribution generated by the softmax function becomes softer, providing more information as to which classes 'T' found more like the predicted class. This is the \"dark knowledge\" embedded in the 'T' and transferred to 'S' in the distillation process."], "score": 0.70556640625}, {"id": "(Deng et al., 2020)", "paper": {"corpus_id": 212644537, "title": "Multitask Emotion Recognition with Incomplete Labels", "year": 2020, "venue": "IEEE International Conference on Automatic Face & Gesture Recognition", "authors": [{"name": "Didan Deng", "authorId": "46203817"}, {"name": "Zhaokang Chen", "authorId": "2808872"}, {"name": "Bertram E. Shi", "authorId": "2075335081"}], "n_citations": 95}, "snippets": ["Hinton et al. [16] proposed Knowledge Distillation for model compression. The knowledge of a larger network is transferred to a relatively smaller network using a modified cross entropy loss function. They introduce a new hyperparameter called temperature T into the softmax function, and suggest that setting T > 1 can increase the weight of smaller logit values, thus providing dark knowledge. In other words, the relative probabilities can reveal more information about inter-class relations than the one-hot labels."], "score": 0.87353515625}, {"id": "(Lee et al., 2021)", "paper": {"corpus_id": 235421655, "title": "Energy-efficient Knowledge Distillation for Spiking Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Dongjin Lee", "authorId": "2109519891"}, {"name": "Seongsik Park", "authorId": "120425550"}, {"name": "Jongwan Kim", "authorId": "2157129258"}, {"name": "Wuhyeong Doh", "authorId": "2111845679"}, {"name": "Sungroh Yoon", "authorId": "2999019"}], "n_citations": 12}, "snippets": ["Soft labels are the probabilities of an input belonging to each class, and can be estimated by a softmax function. Here, a temperature factor is introduced to control the importance of each soft label, which contains the informative dark knowledge from the teacher model. By increasing the temperature, the logits can contain richer information than one-hot labels. However, if the temperature becomes too large, the probability of irrelevant classes will also be over-emphasized."], "score": 0.68798828125}, {"id": "(Jin et al., 2021)", "paper": {"corpus_id": 239616535, "title": "MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Woojeong Jin", "authorId": "8844876"}, {"name": "Maziar Sanjabi", "authorId": "2095979"}, {"name": "Shaoliang Nie", "authorId": "35557488"}, {"name": "L Tan", "authorId": "48327785"}, {"name": "Xiang Ren", "authorId": "1384550891"}, {"name": "Hamed Firooz", "authorId": "22593971"}], "n_citations": 6}, "snippets": ["In knowledge distillation (Hinton et al., 2015), a student is trained to minimize a weighted sum of two different losses: (a) cross entropy with hard labels (one-hot encodings on correct labels) using a standard softmax function, (b) cross entropy with soft labels (probability distribution of labels) produced by a teacher with a temperature higher than 1 in the softmax of both models. The temperature controls the softness of the probability distributions", ".The soft targets (or soft labels) are defined as softmax on outputs of f T with temperature \u03c4", ".The temperature parameter \u03c4 controls the entropy of the output distribution (higher temperature \u03c4 means higher entropy in the soft labels)."], "score": 0.6875}, {"id": "(Ralambomihanta et al., 2024)", "paper": {"corpus_id": 267335043, "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tokiniaina Raharison Ralambomihanta", "authorId": "2281945399"}, {"name": "Shahrad Mohammadzadeh", "authorId": "2281945133"}, {"name": "Mohammad Sami Nur Islam", "authorId": "2282084764"}, {"name": "Wassim Jabbour", "authorId": "2281945352"}, {"name": "Laurence Liang", "authorId": "2282098962"}], "n_citations": 3}, "snippets": ["Distillation, a knowledge transfer method in neural networks, leverages temperature-adjusted softmax probabilities. Initially, the cumbersome model generates soft targets by applying a higher temperature in its softmax, aiding the training of a smaller distilled model. Besides mimicking soft targets, optimizing the distilled model with correct labels further enhances learning."], "score": 0.6435546875}, {"id": "(Lin et al., 2021)", "paper": {"corpus_id": 237002008, "title": "In-Network Flow Classification With Knowledge Distillation", "year": 2021, "venue": "IEEE Access", "authors": [{"name": "K. Lin", "authorId": "145446632"}, {"name": "Chen-yang Li", "authorId": "2124901704"}], "n_citations": 0}, "snippets": ["During distillation, knowledge is transferred from the teacher model to the student by minimizing the distillation loss. However, in conventional neural networks, the well-trained model usually gives a very high probability to the correct class, while making the probabilities of all the other classes close to 0. In this case, the soft label output by the teacher model would be very similar to the ground-truth hard label, as a result providing little information beyond the ground truth label. To resolve this issue, knowledge distillation usually incorporates with another technique, called softmax temperature [1], which transforms the logit z j (input of the softmax layer) to the following probability p j of class j: \n\nwhere T is the temperature parameter. When T = 1, the distribution is the standard softmax function. As T grows, the probability distribution becomes softer (i.e., the gap between the correct class and the others becoming smaller). Such softer distribution then provides more information for the student model to distill the knowledge from the teacher model."], "score": 0.86865234375}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 265444951, "title": "EA-KD: Entropy-based Adaptive Knowledge Distillation", "year": 2023, "venue": "", "authors": [{"name": "Chi-Ping Su", "authorId": "2268314644"}, {"name": "Ching-Hsun Tseng", "authorId": "1569686364"}, {"name": "Bin Pu", "authorId": "2338265886"}, {"name": "Lei Zhao", "authorId": "2338506240"}, {"name": "Zhuangzhuang Chen", "authorId": "2328588941"}, {"name": "Shin-Jye Lee", "authorId": "2116351339"}], "n_citations": 2}, "snippets": ["In classification tasks, the softened probabilities are computed via the temperature-scaled softmax function, given by where p i (T ) is the probability output for class i softened by the temperature hyperparameter T , y i represents the logit for class i, and C is the total number of classes. Typically, T is set to greater than 1 in KD. The higher value of T produces softer probabilities, which are crucial for unveiling the dark knowledge hidden in the inter-class relationships captured by the teacher."], "score": 0.95361328125}, {"id": "(Cui et al., 2022)", "paper": {"corpus_id": 260447668, "title": "Lightweight Neural Network With Knowledge Distillation for CSI Feedback", "year": 2022, "venue": "IEEE Transactions on Communications", "authors": [{"name": "Yiming Cui", "authorId": "2172485573"}, {"name": "Jiajia Guo", "authorId": "47093519"}, {"name": "Zheng Cao", "authorId": "2113999930"}, {"name": "Huaze Tang", "authorId": "120710335"}, {"name": "Chao-Kai Wen", "authorId": "2257212132"}, {"name": "Shi Jin", "authorId": "2227268421"}, {"name": "Xin Wang", "authorId": "2288090155"}, {"name": "Xiaolin Hou", "authorId": "2240356164"}], "n_citations": 3}, "snippets": ["Compared to direct learning with labels3 , the outputs of the teacher network contain more inconspicuous knowledge, which may be learned by complex networks but is not easily captured by simpler student networks. This type of knowledge is commonly known as dark knowledge. Here, we delve deeper into the explanation of dark knowledge. In DL-based CSI feedback, which involves lossy compression, perfect CSI reconstruction at the BS is rarely achievable. Learning with the aid of the teacher autoencoder's output, a feasible sub-optimal solution, is intuitively more beneficial for the optimization process compared to learning directly from the ground-truth CSI, which is essentially an infeasible solution. In other words, the dark knowledge in the teacher autoencoder's output additionally indicates the degree of accuracy achievable in CSI reconstruction at a certain compression ratio. To enhance the efficiency of learning dark knowledge, an extended softmax function is introduced [40], formulated as follows: \n\nwhere z, z i , and t represent the outputs of the teacher network, i-th element in the outputs of the teacher network, and a hyper-parameter called temperature, respectively. The extended softmax function is reduced to the ordinary softmax function when t = 1. The outputs of the extended softmax function are also called soft targets. \n\nAs the temperature t increases, the imperceptible small values in the CSI, which may contain dark knowledge, are further enlarged, and the large values are weakened. An appropriate value of t makes the dark knowledge in the outputs of the teacher network more evident without destructing other knowledge, and the student network can better learn different knowledge. However, when the t is overlarge, the outputs of the extended softmax are almost uniform, resulting in information loss and performance degradation. Therefore, selecting an appropriate value of t is significant, which is further discussed in the simulation part."], "score": 0.66455078125}], "table": null}, {"title": "Temperature Scaling Mechanism in Softmax", "tldr": "Temperature scaling modifies the standard softmax function by dividing logits by a temperature parameter T, which controls the \"softness\" of probability distributions. Higher temperature values produce smoother distributions that better reveal inter-class relationships, enabling more effective transfer of dark knowledge. (18 sources)", "text": "\nTemperature scaling is a fundamental mechanism in knowledge distillation that modifies the standard softmax function to reveal the rich inter-class relationships learned by the teacher model. The conventional softmax function, which converts logits to probabilities, is reformulated by introducing a temperature parameter T that divides each logit before applying the exponential function <Paper corpusId=\"268820185\" paperTitle=\"(Kaleem et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269635406\" paperTitle=\"(Hu et al., 2024)\" isShortName></Paper>. The modified softmax formula is expressed as:\n\n$$p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$\n\nwhere $p_i$ represents the probability for class $i$, $z_i$ is the corresponding logit, and $T$ is the temperature parameter <Paper corpusId=\"274269913\" paperTitle=\"(Rizk et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272969060\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273549238\" paperTitle=\"(Dubey et al., 2024)\" isShortName></Paper>.\n\nWhen T=1, this formula reverts to the standard softmax function <Paper corpusId=\"225397480\" paperTitle=\"(Liu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"270257779\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, as T increases, the probability distribution becomes smoother, spreading probabilities more evenly across classes <Paper corpusId=\"274269913\" paperTitle=\"(Rizk et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277104482\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. This scaling mechanism effectively adjusts the slope of the softmax function\u2014higher T values decrease the slope significantly, allowing the output values to carry considerably more information <Paper corpusId=\"252596141\" paperTitle=\"(Nguyen et al., 2022)\" isShortName></Paper>.\n\nThe mathematical basis for temperature scaling lies in its ability to normalize probability vectors. The standard softmax function typically produces non-uniform probability distributions that can vary significantly between different models, even for the same class. By dividing logits by T, both teacher and student models generate more uniformly distributed probability vectors, making it feasible to compute meaningful loss values between them <Paper corpusId=\"258156357\" paperTitle=\"(Ullah et al., 2023)\" isShortName></Paper>.\n\nDuring the knowledge distillation process, the same temperature parameter is applied to both the teacher and student models <Paper corpusId=\"277632659\" paperTitle=\"(Khan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. This ensures that the student learns to mimic the teacher's softened distributions, capturing not just the correct classification but also the subtle relationships between classes <Paper corpusId=\"249151858\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4853851\" paperTitle=\"(Li et al., 2016)\" isShortName></Paper>. \n\nThe temperature parameter is a crucial hyperparameter that can be tuned to optimize knowledge transfer <Paper corpusId=\"258156357\" paperTitle=\"(Ullah et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272341595\" paperTitle=\"(Sarikaya et al., 2024)\" isShortName></Paper>. It serves as the key mechanism that enables the teacher to effectively transfer its \"dark knowledge\" to the student, revealing patterns of similarity among classes that would otherwise remain hidden in standard training approaches <Paper corpusId=\"247486748\" paperTitle=\"(Oh et al., 2022)\" isShortName></Paper> <Paper corpusId=\"233777216\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"198179476\" paperTitle=\"(Tung et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Kaleem et al., 2024)", "paper": {"corpus_id": 268820185, "title": "A Comprehensive Review of Knowledge Distillation in Computer Vision", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Sheikh Musa Kaleem", "authorId": "2294363351"}, {"name": "Tufail Rouf", "authorId": "2294363576"}, {"name": "Gousia Habib", "authorId": "2016381637"}, {"name": "T. Saleem", "authorId": "1468566373"}, {"name": "B. Lall", "authorId": "143632380"}], "n_citations": 13}, "snippets": ["The use of temperature scaling to soften soft logits produced by the softmax function is another important feature of knowledge distillation. This is accomplished by dividing the logits by a temperature parameter T, followed by the softmax function. The temperature parameter governs the probability distribution's \"softness,\" with higher temperatures resulting in softer distributions."], "score": 0.71728515625}, {"id": "(Hu et al., 2024)", "paper": {"corpus_id": 269635406, "title": "Markowitz Meets Bellman: Knowledge-distilled Reinforcement Learning for Portfolio Management", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Gang Hu", "authorId": "2301005855"}, {"name": "Ming Gu", "authorId": "2300397509"}], "n_citations": 0}, "snippets": ["Knowledge distillation involves a teacher model with a temperature-modified softmax output layer q i = exp(zi/T ) j exp(zj /T ) where z i are logits, T is the temperature, and q i are the softened probabilities.A higher T yields a softer probability distribution."], "score": 0.6298828125}, {"id": "(Rizk et al., 2024)", "paper": {"corpus_id": 274269913, "title": "A Precise and Scalable Indoor Positioning System Using Cross-Modal Knowledge Distillation", "year": 2024, "venue": "Italian National Conference on Sensors", "authors": [{"name": "Hamada Rizk", "authorId": "2311724069"}, {"name": "Ahmed M. Elmogy", "authorId": "2677103"}, {"name": "Mohamed Rihan", "authorId": "145195785"}, {"name": "Hirozumi Yamaguchi", "authorId": "2171271984"}], "n_citations": 2}, "snippets": ["A key component of knowledge distillation is the use of softened probability distributions, achieved by introducing a temperature parameter T > 1 in the softmax function. The logits from both the teacher and the student are softened as follows: \n\nThe temperature T controls the softness of the output distribution. When T = 1, the distribution is the same as the original softmax output. When T > 1, the distribution becomes softer, spreading the probabilities more evenly across the classes. This softened distribution contains richer information about the inter-class relationships, which the student model can learn from."], "score": 0.6533203125}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 272969060, "title": "Harmonizing knowledge Transfer in Neural Network with Unified Distillation", "year": 2024, "venue": "European Conference on Computer Vision", "authors": [{"name": "Yaomin Huang", "authorId": "2180087917"}, {"name": "Zaoming Yan", "authorId": "2257133494"}, {"name": "Chaomin Shen", "authorId": "2242115637"}, {"name": "Faming Fang", "authorId": "152786529"}, {"name": "Guixu Zhang", "authorId": "2323414943"}], "n_citations": 0}, "snippets": ["In contrast to direct supervised learning through ground truth, logits-based KD using soft labels can elicit more 'dark knowledge' [9], thereby enhancing the performance of the student network without altering its architecture. The logits distillation losses are represented as follows: \n\nP j denotes the class probability derived from the logits z \u2208 R C after undergoing Softmax. \u03c4 is the temperature scaling hyper-parameter which enables the production of different probability distributions:"], "score": 0.625}, {"id": "(Dubey et al., 2024)", "paper": {"corpus_id": 273549238, "title": "AI Readiness in Healthcare through Storytelling XAI", "year": 2024, "venue": "EXPLIMED@ECAI", "authors": [{"name": "Akshat Dubey", "authorId": "2282540984"}, {"name": "Zewen Yang", "authorId": "2282577952"}, {"name": "Georges Hattab", "authorId": "2282533557"}], "n_citations": 3}, "snippets": ["By using a \"temperature\" scaling function in the softmax, the logits are softened, so smoothing down the probability distribution and exposing the teacher's taught inter-class correlations. The probability p i of class i from the logits z is calculated as: \n\nwhere  is the parameter of temperature and when  = 1 then we get the original softmax function. The probability distribution produced by the softmax function softens with increasing T, giving more insight into which classes the teacher thought were more like the ones that were expected. The discrepancy between the instructor model's soft targets and the student model's predictions is quantified by this loss function. The student model learns the internal representation from the teacher model [13]. The student model learns not only the target outputs but also the internal representations and similarity information from the teacher model."], "score": 0.75244140625}, {"id": "(Liu et al., 2020)", "paper": {"corpus_id": 225397480, "title": "A Fusion Algorithm of Multi-model Pruning and Collaborative Distillation Learning", "year": 2020, "venue": "Journal of Physics: Conference Series", "authors": [{"name": "Zihan Liu", "authorId": "2145253722"}, {"name": "Zhi-yuan Shi", "authorId": "2110398805"}], "n_citations": 1}, "snippets": ["By comparing the size of each output  , it is converted into a probability value  of this class. Here, a temperature parameter T is introduced to softmax to control the smoothness of the output distribution. The modified softmax formula is as follows (softmax-T): \n\nWhen the T parameter is placed at 1, it is the ordinary Softmax formula. The larger the T value, the smoother the curve of the obtained Softmax function. After setting the temperature T to a larger value, the Softmax-T of the teacher model and the student model output  ,  and then set the temperature T to 1 to get the conventional output  ."], "score": 0.712890625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 270257779, "title": "Robust Knowledge Distillation Based on Feature Variance Against Backdoored Teacher Model", "year": 2024, "venue": "Applied Soft Computing", "authors": [{"name": "Jinyin Chen", "authorId": "2283187602"}, {"name": "Xiao-Ming Zhao", "authorId": "2289841303"}, {"name": "Haibin Zheng", "authorId": "1796810"}, {"name": "Xiao Li", "authorId": "2304969928"}, {"name": "Sheng Xiang", "authorId": "2291571475"}, {"name": "Haifeng Guo", "authorId": "2304823063"}], "n_citations": 4}, "snippets": ["When T =1, q i is the standard softmax function. In this situation, the results output by the softmax layer will be more distributed and more information between and within classes will be retained with the increase of the temperature factor."], "score": 0.7236328125}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 277104482, "title": "MaTVLM: Hybrid Mamba-Transformer for Efficient Vision-Language Modeling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yingyue Li", "authorId": "2320013374"}, {"name": "Bencheng Liao", "authorId": "2060439659"}, {"name": "Wenyu Liu", "authorId": "2313037989"}, {"name": "Xinggang Wang", "authorId": "2266175736"}], "n_citations": 0}, "snippets": ["The temperature factor adjusts the smoothness of the probability distributions, allowing the student model to capture finer details from the softened distribution of the teacher model", "The softened probabilities P t (i) and P s (i) are calculated by applying a temperature-scaled softmax function to the logits of the teacher and student models, respectively", "where T is the temperature scaling factor, a higher temperature produces softer distributions, z t is the logit (presoftmax output) from the teacher model, and \u1e91s is the corresponding logit from the student model."], "score": 0.82421875}, {"id": "(Nguyen et al., 2022)", "paper": {"corpus_id": 252596141, "title": "Label driven Knowledge Distillation for Federated Learning with non-IID Data", "year": 2022, "venue": "", "authors": [{"name": "Minh-Duong Nguyen", "authorId": "2162621330"}, {"name": "Viet Quoc Pham", "authorId": "145436642"}, {"name": "D. Hoang", "authorId": "2233724"}, {"name": "Long Tran-Thanh", "authorId": "1389575160"}, {"name": "Diep N. Nguyen", "authorId": "30479205"}, {"name": "W. Hwang", "authorId": "2054135428"}], "n_citations": 2}, "snippets": ["The intention of adding the variable T is to adjust the slope of the softmax function in the classifier as shown in equation 16. As we can see, when we increase the value of T, the slope of the softmax function will decrease significantly. With large temperature scale values, over the same output range of data, the range of values represented by input zj is larger. Then, the output value tuple created by the teacher and student carries considerably more information. Therefore, the learning process between teacher and student is more effective."], "score": 0.921875}, {"id": "(Ullah et al., 2023)", "paper": {"corpus_id": 258156357, "title": "A 3DCNN-Based Knowledge Distillation Framework for Human Activity Recognition", "year": 2023, "venue": "Journal of Imaging", "authors": [{"name": "Hayat Ullah", "authorId": "1491632645"}, {"name": "Arslan Munir", "authorId": "1748235"}], "n_citations": 3}, "snippets": ["Here, the role of temperature T value in the normalized softmax function is to produce normalized or smooth probability vectors. Normally, the probability vector produced by the standard softmax function has non-uniform distribution of probability values. Furthermore, the probability vectors of two different models for the same class can vary to a high extent, which makes it infeasible to compute the generalized loss value. Therefore, each value of logits vector in the softmax function is divided by T to provide the uniformly distributed probability vectors for both teacher and student model as shown in Figure 4. A normalized softmax function with different temperature T values will result in different probability vectors (soft labels). Therefore, we have considered different temperature T values in our experiments to observe its impact on the knowledge distillation performance. Thus, in our proposed framework, T is a hyperparameter which can be tuned to provide the best prediction accuracy."], "score": 0.6474609375}, {"id": "(Khan et al., 2025)", "paper": {"corpus_id": 277632659, "title": "Crowd counting at the edge using weighted knowledge distillation", "year": 2025, "venue": "Scientific Reports", "authors": [{"name": "Muhammad Asif Khan", "authorId": "2115771635"}, {"name": "Hamid Menouar", "authorId": "2330247142"}, {"name": "Ridha Hamila", "authorId": "2311725227"}, {"name": "Adnan Abu-Dayya", "authorId": "2354319497"}], "n_citations": 0}, "snippets": ["Hinton et al. (Hinton et al., 2015) introduced the \"temperature\" parameter T in the softmax function at the output layer used to distill the knowledge from the teacher model to the student model by training the student model using the same dataset as used for training the teacher model with a high value of T. Once the student model is trained, the value of T is set to T = 1 (standard softmax function) during the inference. The student model's accuracy can be highly improved by using two objective (loss) functions i.e., a cross-entropy (CE) function with the soft targets (produced by the teacher) and a CE with the actual ground truth labels. The authors suggest using a high value of T in the first case (same T in student and teacher) and T = 1 for the second case."], "score": 0.658203125}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 249151858, "title": "Geometer: Graph Few-Shot Class-Incremental Learning via Prototype Representation", "year": 2022, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Bin Lu", "authorId": "2064914349"}, {"name": "Xiaoying Gan", "authorId": "1693639"}, {"name": "Lina Yang", "authorId": "2145501350"}, {"name": "Weinan Zhang", "authorId": "2108309275"}, {"name": "Luoyi Fu", "authorId": "1922573"}, {"name": "Xinbing Wang", "authorId": "2107937507"}], "n_citations": 29}, "snippets": ["Temperature-scaled softmax (Li et al., 2016) is utilized to soften the old classes logits of teacher model and student model. The modified logits  \u2032() of class  by applying a temperature scaling function in the softmax are calculated as \n\nwhere  is the temperature factor. Generally, we set  > 1 to increase the weight of smaller logit values and encourages the network to better reveal inter-class relationships learned by the teacher model."], "score": 0.72265625}, {"id": "(Li et al., 2016)", "paper": {"corpus_id": 4853851, "title": "Learning without Forgetting", "year": 2016, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Zhizhong Li", "authorId": "49969902"}, {"name": "Derek Hoiem", "authorId": "2433269"}], "n_citations": 4431}, "snippets": ["When building a unified vision system or gradually adding new apabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and fine-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace fine-tuning with similar old and new task datasets for improved new task performance."], "score": 0.0}, {"id": "(Sarikaya et al., 2024)", "paper": {"corpus_id": 272341595, "title": "Modality- and Subject-Aware Emotion Recognition Using Knowledge Distillation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "M. Sarikaya", "authorId": "152648598"}, {"name": "G\u00f6khan Ince", "authorId": "2146435812"}], "n_citations": 0}, "snippets": ["T is the temperature parameter that scales the softmax function, controlling the smoothness of the output probability distribution and influencing the magnitude of gradients during training."], "score": 0.80517578125}, {"id": "(Oh et al., 2022)", "paper": {"corpus_id": 247486748, "title": "Edge-Cloud Alarm Level of Heterogeneous IIoT Devices Based on Knowledge Distillation in Smart Manufacturing", "year": 2022, "venue": "Electronics", "authors": [{"name": "Seokju Oh", "authorId": "2121943561"}, {"name": "Donghyun Kim", "authorId": "2145183886"}, {"name": "Chae-Suk Lee", "authorId": "2109513069"}, {"name": "Jongpil Jeong", "authorId": "3453343"}], "n_citations": 1}, "snippets": ["The logits apply the Softmax temperature scaling function, which effectively smooths the probability distribution and reveals the relationship between the classes that are learned by the teacher model."], "score": 0.6630859375}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 233777216, "title": "On-Device Learning Systems for Edge Intelligence: A Software and Hardware Synergy Perspective", "year": 2021, "venue": "IEEE Internet of Things Journal", "authors": [{"name": "Qihua Zhou", "authorId": "1993679419"}, {"name": "Zhihao Qu", "authorId": "2034348867"}, {"name": "Song Guo", "authorId": "144123438"}, {"name": "Boyuan Luo", "authorId": "2150444053"}, {"name": "Jingcai Guo", "authorId": "8572939"}, {"name": "Zhenda Xu", "authorId": "150358457"}, {"name": "R. Akerkar", "authorId": "1854500"}], "n_citations": 52}, "snippets": ["An effective method to solve this problem is to employ the temperature-based softmax function [82] to adjust the original logits from FC layers and balance the unscaled log probabilities of shape. The temperature-based softmax can be described as: \n\nwhere p i is the classification probability of each class i and z is corresponding logits inside the softmax function. Besides, T is the hyper-parameter called temperature to control the probability distribution of the output. Note that we will get the standard softmax function when T is 1 and the probability distribution will become softer when setting a higher value of T, so as to make the teacher model provide more effective information for the student model's learning. The key that a teacher can successfully transfer the knowledge to the student is to get the prediction experience from the teacher's loss function, which is called the dark knowledge (Tung et al., 2019). The entire distillation procedure is actually based on the transferring of this dark knowledge, from the teacher to the student."], "score": 0.873046875}, {"id": "(Tung et al., 2019)", "paper": {"corpus_id": 198179476, "title": "Similarity-Preserving Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Frederick Tung", "authorId": "2065573607"}, {"name": "Greg Mori", "authorId": "10771328"}], "n_citations": 981}, "snippets": ["Knowledge distillation is a widely applicable technique for training a student neural network under the guidance of a trained teacher network. For example, in neural network compression, a high-capacity teacher is distilled to train a compact student; in privileged learning, a teacher trained with privileged data is distilled to train a student without access to that data. The distillation loss determines how a teacher's knowledge is captured and transferred to the student. In this paper, we propose a new form of knowledge distillation loss that is inspired by the observation that semantically similar inputs tend to elicit similar activation patterns in a trained network. Similarity-preserving knowledge distillation guides the training of a student network such that input pairs that produce similar (dissimilar) activations in the teacher network produce similar (dissimilar) activations in the student network. In contrast to previous distillation methods, the student is not required to mimic the representation space of the teacher, but rather to preserve the pairwise similarities in its own representation space. Experiments on three public datasets demonstrate the potential of our approach."], "score": 0.0}], "table": null}, {"title": "How Temperature Affects Probability Distributions", "tldr": "Temperature scaling fundamentally alters probability distributions by controlling their \"softness\" - higher temperatures produce more uniform distributions that reveal subtle inter-class relationships, while lower temperatures create sharper distributions that emphasize dominant classes. (11 sources)", "text": "\nTemperature scaling in the softmax function directly manipulates how probability mass is distributed across classes, which is crucial for effective knowledge transfer in distillation. When using standard softmax (T=1), well-trained models typically produce very peaked distributions with high confidence in the predicted class <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>. However, increasing the temperature parameter T>1 creates \"softer\" probability distributions that spread more evenly across classes, revealing the teacher model's uncertainty and confidence levels across all categories <Paper corpusId=\"227228186\" paperTitle=\"(Liu et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"243832889\" paperTitle=\"(Eyono et al., 2021)\" isShortName></Paper>.\n\nThis softening effect occurs because temperature scaling penalizes larger logit values more than smaller ones <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>. As temperature increases, the probability distribution becomes smoother, conveying more nuanced information about the interrelationships between different categories according to the teacher model <Paper corpusId=\"265384964\" paperTitle=\"(Xie et al., 2023)\" isShortName></Paper>. For example, a truck image might have hard labels with 0.99 probability for the truck class and near-zero for others, but with higher temperatures, it might show softer logits like 0.65 for truck, 0.25 for car, and smaller values for other classes - revealing that trucks and cars share similarities <Paper corpusId=\"252968120\" paperTitle=\"(Thapa, 2022)\" isShortName></Paper>.\n\nConversely, lowering the temperature (T<1) creates \"sharper\" distributions that push the maximum probability closer to 1 and others closer to 0, resulting in more deterministic outputs <Paper corpusId=\"267535483\" paperTitle=\"(Al-Ahmadi, 2024)\" isShortName></Paper>. This can be beneficial in reinforcement learning contexts where the teacher's output represents expected returns for different actions, and a sharper distribution provides more unambiguous information for action selection <Paper corpusId=\"221284805\" paperTitle=\"(Jang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"1923568\" paperTitle=\"(Rusu et al., 2015)\" isShortName></Paper>.\n\nThe temperature parameter effectively controls the balance between true label knowledge and dark knowledge <Paper corpusId=\"274436184\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Lower temperatures make the distillation assign less weight to logits that are significantly smaller than the average, while larger temperatures pay more attention to the seemingly unimportant parts of the logit distribution <Paper corpusId=\"276725399\" paperTitle=\"(Xuan et al., 2025)\" isShortName></Paper>. This focus on the \"unimportant\" parts is actually valuable because the hard-target term in distillation already ensures the dominant part of the logit is correct, so capturing the fine-grained information in the remaining logits allows the student model to learn more subtle patterns from the teacher <Paper corpusId=\"276725399\" paperTitle=\"(Xuan et al., 2025)\" isShortName></Paper>.\n\nBy adjusting temperature, the entropy of the softmax output probability distribution can be increased, amplifying the information carried by negative labels <Paper corpusId=\"270377282\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. When both teacher and student models use the same temperature value during training, the student can effectively learn from the teacher's relative confidences across classes rather than just the hard predictions <Paper corpusId=\"267535483\" paperTitle=\"(Al-Ahmadi, 2024)\" isShortName></Paper>. This temperature-based smoothing enhances the efficacy of knowledge distillation by enabling the transfer of comprehensive information regarding class relationships, simplifying the learning process and potentially accelerating training <Paper corpusId=\"275993890\" paperTitle=\"(Murtada et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Thapa, 2022)", "paper": {"corpus_id": 252968120, "title": "On effects of Knowledge Distillation on Transfer Learning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Sushil Thapa", "authorId": "2093329661"}], "n_citations": 1}, "snippets": ["The temperature parameter T in Equation 1.2 controls the softness of the output probabilities in the Softmax scores. T = 1 is the special case that outputs vanilla softmax scores, i.e., hard labels from Equation 1.1. However, using values higher than 1 produces a softer probability distribution between classes. For an intuitive understanding of the hardness and softness of logits, let us take an example of hard labels of a truck image sample with a score of 0.99 for the truck class and 0 or 1 everywhere else. On the contrary, soft labels provide a more nuanced representation of classes with a probability distribution. The same truck image would have softer logits of 0.65 for the truck, 0.25 for the car, and a nominal score for other classes with higher temperatures. With this information, it is easier to learn that cars and trucks are more similar to each other compared to other classes.\n\nFurther exploration of how the temperature value changes the output logits of the same network output is shown in Figure 1.10. Softmax with higher T yields softer probabilities that are less confident in the model's prediction. With a lower value of T, i.e., more hard labels, the network tends to be more confident in its predictions. This happens because Softmax uses an exponential function, and the temperature value of Softmax penalizes larger logit values more than smaller logit values. Lowering T makes the model more overconfident with harder labels, and increasing T for knowledge distillation makes the logits softer. Note that the maximum score value is divided and decreases with higher temperatures."], "score": 0.9638671875}, {"id": "(Liu et al._1, 2020)", "paper": {"corpus_id": 227228186, "title": "Bringing AI To Edge: From Deep Learning's Perspective", "year": 2020, "venue": "Neurocomputing", "authors": [{"name": "Di Liu", "authorId": "2115296404"}, {"name": "Hao Kong", "authorId": "2815982"}, {"name": "Xiangzhong Luo", "authorId": "2041283357"}, {"name": "Weichen Liu", "authorId": "2248636"}, {"name": "Ravi Subramaniam", "authorId": "2062739126"}], "n_citations": 123}, "snippets": ["To alleviate this issue, Hinton et al. [140] propose softmax temperature in which temperature T is to soften the generated probability distribution. Intuitively, a larger T leads to a 'softer' probability distribution (e.g., [0.4, 0.2, 0.2, 0.[2])."], "score": 0.70458984375}, {"id": "(Eyono et al., 2021)", "paper": {"corpus_id": 243832889, "title": "AUTOKD: Automatic Knowledge Distillation Into A Student Architecture Family", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Roy Henha Eyono", "authorId": "35369096"}, {"name": "Fabio Maria Carlucci", "authorId": "2264326"}, {"name": "P. Esperan\u00e7a", "authorId": "27245978"}, {"name": "Binxin Ru", "authorId": "88739736"}, {"name": "Phillip Torr", "authorId": "73007356"}], "n_citations": 3}, "snippets": ["Hinton et al. (2015b) propose \"softening\" the probabilities using temperature scaling with \u03c4 \u2265 1."], "score": 0.64404296875}, {"id": "(Xie et al., 2023)", "paper": {"corpus_id": 265384964, "title": "Forest Fire Object Detection Analysis Based on Knowledge Distillation", "year": 2023, "venue": "Fire", "authors": [{"name": "Jinzhou Xie", "authorId": "2268047558"}, {"name": "Hongmin Zhao", "authorId": "2268031861"}], "n_citations": 6}, "snippets": ["With the increase in the temperature parameter T, the softmax function's probability distribution becomes smoother, thereby conveying more nuanced particulars about the interrelation of different categories according to the teacher model. This information, referred to as \"dark knowledge\" by Hinton, is what we aim to impart to the student model in distillation. To compute the loss function for the teacher's soft targets, we use the same T value to calculate the softmax function based on the student logits. This kind of loss is frequently called \"distillation loss.\" Therefore, with the increase in T, we are better able to impart the knowledge of the teacher model to the student model, aiding the latter in learning and generalization."], "score": 0.8193359375}, {"id": "(Al-Ahmadi, 2024)", "paper": {"corpus_id": 267535483, "title": "Knowledge Distillation Based Deep Learning Model for User Equipment Positioning in Massive MIMO Systems Using Flying Reconfigurable Intelligent Surfaces", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Abdullah Al-Ahmadi", "authorId": "2283215857"}], "n_citations": 1}, "snippets": ["In higher temperature T > 1, the probabilities become ''softer'', meaning they move closer to being uniform. This can reveal more information about the model's uncertainties and confidences across classes. It's particularly useful in knowledge distillation, as it allows the student model to learn more from the teacher's outputs, including the nuances and relative confidences. Whereas in lower temperature T < 1, the probabilities become ''sharper'', pushing the maximum probability closer to 1 and the rest closer to 0 resulting in more deterministic outputs. The softened probabilities from the teacher model (obtained using a higher temperature) are used to train the student model. This allows the student model to learn from the teacher's relative confidences across classes, not just the hard predictions. The same temperature value is typically used to soften the student's outputs during training. The loss is then computed between the softened outputs of the teacher and student, helping the student model to generalize better."], "score": 0.82568359375}, {"id": "(Jang et al., 2020)", "paper": {"corpus_id": 221284805, "title": "Knowledge Transfer for On-Device Deep Reinforcement Learning in Resource Constrained Edge Computing Systems", "year": 2020, "venue": "IEEE Access", "authors": [{"name": "Ingook Jang", "authorId": "2941497"}, {"name": "Hyunseok Kim", "authorId": "2118020583"}, {"name": "Donghun Lee", "authorId": "2115475643"}, {"name": "Young-Sung Son", "authorId": "120595891"}, {"name": "Seonghyun Kim", "authorId": "4328349"}], "n_citations": 31}, "snippets": ["In the classification problem, raising the temperature enables more of the knowledge to be transferred to the student network since the teacher's output typically tends to be very peaked. However, in the RL setting, the softmax function is used to make the distribution sharper by lowering the temperature \u03c4 < 1 because the teacher's output is a set of the expected discounted return values for its action space (Rusu et al., 2015). If the temperature goes to 0, a softmax function becomes greedy. Otherwise, it is computed as a softmax function with a Boltzmann distribution shown in Equation (3). The sharpened distribution not only provides more unambiguous information for action selection but also serves as a regression target for the student training. These characteristics enable an RL training process using distillation to be accelerated."], "score": 0.67529296875}, {"id": "(Rusu et al., 2015)", "paper": {"corpus_id": 1923568, "title": "Policy Distillation", "year": 2015, "venue": "International Conference on Learning Representations", "authors": [{"name": "Andrei A. Rusu", "authorId": "2228824"}, {"name": "Sergio Gomez Colmenarejo", "authorId": "2016840"}, {"name": "\u00c7aglar G\u00fcl\u00e7ehre", "authorId": "1854385"}, {"name": "Guillaume Desjardins", "authorId": "2755582"}, {"name": "J. Kirkpatrick", "authorId": "2066516991"}, {"name": "Razvan Pascanu", "authorId": "1996134"}, {"name": "Volodymyr Mnih", "authorId": "3255983"}, {"name": "K. Kavukcuoglu", "authorId": "2645384"}, {"name": "R. Hadsell", "authorId": "2315504"}], "n_citations": 696}, "snippets": ["Abstract: Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 274436184, "title": "Toward Fair Graph Neural Networks Via Dual-Teacher Knowledge Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chengyu Li", "authorId": "2327960212"}, {"name": "Debo Cheng", "authorId": "2320341"}, {"name": "Guixian Zhang", "authorId": "2152233401"}, {"name": "Yi Li", "authorId": "2316406036"}, {"name": "Shichao Zhang", "authorId": "2186753918"}], "n_citations": 0}, "snippets": ["In conventional knowledge distillation, a predefined temperature parameter is used to generate soft targets. The teacher model's informative dark knowledge is embedded in the soft targets, which infer the probability that a node belongs to a given class. The softmax function with a fixed temperature is defined as follows: \n\nwhere the temperature parameter \u03c4 is used to control the softness of each prediction. The distribution of predictions becomes smoother as the temperature rises, and sharper as the temperature falls. Thus, the temperature is responsible for regulating the balance between the true label knowledge and the dark knowledge."], "score": 0.88623046875}, {"id": "(Xuan et al., 2025)", "paper": {"corpus_id": 276725399, "title": "Exploring the Impact of Temperature Scaling in Softmax for Classification and Adversarial Robustness", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hao Xuan", "authorId": "2235060849"}, {"name": "Bokai Yang", "authorId": "2348692399"}, {"name": "Xingyu Li", "authorId": "2348481539"}], "n_citations": 4}, "snippets": ["Knowledge Distillation proposed by [8] is one innovative way to transfer knowledge from a teacher model to a student model. Temperature is utilized during training to control both the student and teacher model's output. The author argues that lower temperatures make the distillation assign less weight to logits that are much smaller than the average. Conversely, employing larger temperatures softens the probability distribution and pays more attention to the unimportant part of the logit. Larger temperatures are proven to be beneficial in the distillation process since the hard-target term already ensures the dominant part of the logit (target class) is correct. By focusing on the remaining logit, the student model can capture more fine-grained information from the teacher model."], "score": 0.97607421875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270377282, "title": "Research on temperature detection method of liquor distilling pot feeding operation based on a compressed algorithm", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Xiaolian Liu", "authorId": "2305667661"}, {"name": "Shaopeng Gong", "authorId": "2305658388"}, {"name": "Xiangxu Hua", "authorId": "2305657615"}, {"name": "Taotao Chen", "authorId": "2305821873"}, {"name": "Chunjiang Zhao", "authorId": "2306058720"}], "n_citations": 2}, "snippets": ["By adjusting the value of T, the entropy of the Softmax output probability distribution can be increased, thus amplifying the information carried by negative labels", "The knowledge distillation process was shown in the knowledge distillation model in Fig. 1 22,(Chen et al., 2023) . First, the teacher network model is trained and the logits output of the teacher network is divided by the T after doing Softmax calculation to get the soft label value. Then, the same training as for the teacher network is performed to get the logits output. Next, a two-step calculation is performed. The first step is to perform a Softmax calculation by dividing the logits output of the student network by the same T as the teacher model to obtain the soft prediction. Soft predictions were compared to soft labels, and the difference between the two probability distributions was measured using the distillation loss function."], "score": 0.76171875}, {"id": "(Murtada et al., 2025)", "paper": {"corpus_id": 275993890, "title": "Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered Design", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Amna Murtada", "authorId": "2342923579"}, {"name": "Omnia Abdelrhman", "authorId": "88856585"}, {"name": "T. Attia", "authorId": "30665494"}], "n_citations": 1}, "snippets": ["In the mechanism of Soft Target Distillation, temperature scaling is implemented via the temperature hyper-parameter T to smooth the probability distributions, thereby enhancing the efficacy of knowledge distillation. This smoothing is achieved by dividing the logits by T before the application of the softmax function. The teacher model's softened logits convey comprehensive information regarding class relationships, thereby simplifying the learning process to a certain degree. The use of the KL divergence loss facilitates the transfer of knowledge between the teacher and student soft predictions, seemingly enabling accelerated training."], "score": 0.654296875}], "table": null}, {"title": "Dark Knowledge Transfer Process", "tldr": "The knowledge distillation process transfers dark knowledge from teacher to student through temperature-scaled softmax distributions and Kullback-Leibler divergence loss. This mechanism enables the student to learn not just correct classifications but also the nuanced inter-class relationships captured by the teacher. (5 sources)", "text": "\nThe dark knowledge transfer process begins with applying temperature scaling to both the teacher and student models' logits before computing softmax probabilities. This scaling mechanism is essential for revealing the hidden patterns and relationships the teacher model has learned during its training <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>. The temperature parameter \u03c4 (or T) effectively controls how much of this dark knowledge is exposed and transferred to the student model\u2014higher values produce softer distributions that better reveal which classes the teacher model considers similar to the predicted class <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nTo facilitate this knowledge transfer, the student model is trained to mimic the teacher's softened predictions through a Kullback-Leibler divergence loss term <Paper corpusId=\"253655030\" paperTitle=\"(Borza et al., 2022)\" isShortName></Paper>. This approach ensures that the student not only learns to predict the correct class but also internalizes the nuanced relationships between classes that the teacher has captured. Importantly, research indicates that using the same temperature value for both teacher and student models typically yields optimal results, creating a consistent framework for knowledge transfer <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of temperature scaling in the knowledge distillation process has been empirically demonstrated, with studies showing that this smoothing strategy can improve knowledge distillation accuracy by approximately 1.2% compared to distillation without temperature scaling <Paper corpusId=\"220280430\" paperTitle=\"(Pouransari et al., 2020)\" isShortName></Paper>. This improvement underscores the value of properly revealing and transferring dark knowledge through carefully controlled temperature parameters.\n\nThe temperature parameter selection is crucial for effective knowledge transfer. While higher temperatures produce softer probability distributions that encourage the student to learn nuanced relationships, excessively high values can result in overly smoothed distributions that may be less informative <Paper corpusId=\"268060109\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>. This creates a balancing act in the distillation process, where the temperature must be carefully tuned to maximize the transfer of meaningful dark knowledge while maintaining the informativeness of the distributions. Empirical experiments are typically conducted to determine the optimal temperature value, with research suggesting that effective values can range from 1 to 20 depending on the specific task and model architectures <Paper corpusId=\"254910794\" paperTitle=\"(Amin et al., 2023)\" isShortName></Paper>.\n\nSince its introduction in Hinton's original knowledge distillation work, this method of transferring knowledge through temperature-scaled softmax outputs has become a fundamental approach in model compression and knowledge transfer <Paper corpusId=\"271218098\" paperTitle=\"(Giakoumoglou et al., 2024)\" isShortName></Paper>. The process allows smaller student models to benefit from the rich internal representations learned by larger teacher models, making advanced AI capabilities more accessible for deployment in resource-constrained environments.", "citations": [{"id": "(Amin et al., 2023)", "paper": {"corpus_id": 254910794, "title": "Data Driven Classification of Opioid Patients Using Machine Learning\u2013An Investigation", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Saddam Al Amin", "authorId": "2164150522"}, {"name": "Md. Saddam Hossain Mukta", "authorId": "1806836"}, {"name": "Md. Sezan Mahmud Saikat", "authorId": "2197620015"}, {"name": "Md. Ismail Hossain", "authorId": "2197241772"}, {"name": "Md. Adnanul Islam", "authorId": "7484275"}, {"name": "Mohiuddin Ahmed", "authorId": "2115615362"}, {"name": "S. Azam", "authorId": "25447315"}], "n_citations": 2}, "snippets": ["When \u03c4 = 1, softmax produces its typical output. However, when we raise, the softmax output softens and reveals which classes our teacher model discovered to be more similar to the predicted class. Hinton et al. [27] called it dark knowledge. The teacher model itself implant the dark knowledge during training. However, during the distillation process, this dark knowledge is transmitted to the student model which is built from unstructured dataset, D us . According to the experiments of the authors [27], the value of \u03c4 could be from 1 to 20. Authors find that the same value of \u03c4 to the student and teacher models likely return the maximum results."], "score": 0.61328125}, {"id": "(Borza et al., 2022)", "paper": {"corpus_id": 253655030, "title": "Effective Online Knowledge Distillation via Attention-Based Model Ensembling", "year": 2022, "venue": "Mathematics", "authors": [{"name": "D. Borza", "authorId": "144583200"}, {"name": "A. Darabant", "authorId": "1821352"}, {"name": "Tudor Alexandru Ileni", "authorId": "74810447"}, {"name": "Alexandru-Ion Marinescu", "authorId": "150020994"}], "n_citations": 2}, "snippets": ["In the classical setup, during training, a Kullback-Libeler divergence loss term is employed to ensure that the student network mimics the teacher's softened predictions."], "score": 0.90185546875}, {"id": "(Pouransari et al., 2020)", "paper": {"corpus_id": 220280430, "title": "Extracurricular Learning: Knowledge Transfer Beyond Empirical Distribution", "year": 2020, "venue": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)", "authors": [{"name": "H. Pouransari", "authorId": "1842915"}, {"name": "Oncel Tuzel", "authorId": "2577513"}], "n_citations": 5}, "snippets": ["In KD [25], logits of the student and the teacher are inversely scaled by a temperature parameter T before softmax probabilities are computed. This smoothing strategy can slightly improve the knowledge distillation accuracy (+1.2% compared to KD without temperature scaling)."], "score": 0.66650390625}, {"id": "(Cao et al., 2024)", "paper": {"corpus_id": 268060109, "title": "Advanced hybrid LSTM-transformer architecture for real-time multi-task prediction in engineering systems", "year": 2024, "venue": "Scientific Reports", "authors": [{"name": "Kangjie Cao", "authorId": "2259997326"}, {"name": "Ting Zhang", "authorId": "2288621104"}, {"name": "Jueqiao Huang", "authorId": "2269063771"}], "n_citations": 46}, "snippets": ["The choice of temperature T in the softmax function is crucial. A higher T produces softer probability distributions, encouraging the student model to learn the nuanced relationships captured by the teacher. However, too high a value of T can lead to an overly smoothed distribution, which might be less informative. Therefore, we empirically determine the optimal value of T through a series of experiments, aiming to find the right balance for effective knowledge transfer."], "score": 0.95849609375}, {"id": "(Giakoumoglou et al., 2024)", "paper": {"corpus_id": 271218098, "title": "Discriminative and Consistent Representation Distillation", "year": 2024, "venue": "", "authors": [{"name": "Nikolaos Giakoumoglou", "authorId": "2196360101"}, {"name": "Tania Stathaki", "authorId": "2292259667"}], "n_citations": 0}, "snippets": ["The original knowledge distillation work by [33] introduced transferring knowledge through softened logit outputs using temperature scaling in the softmax."], "score": 0.7919921875}], "table": null}, {"title": "Practical Considerations for Temperature Values", "tldr": "Selecting appropriate temperature values is crucial for effective knowledge distillation, with empirical evidence suggesting optimal ranges between 2-20 depending on the specific task and model. While conventional knowledge distillation often uses T=4 as a starting point, practitioners should experiment with different values to maximize knowledge transfer. (1 source)", "text": "\nWhen implementing temperature scaling in knowledge distillation, choosing an appropriate temperature value is critical for maximizing the effectiveness of dark knowledge transfer. While there is no universal optimal temperature that works across all scenarios, empirical evidence suggests that certain ranges tend to be more effective than others. In conventional knowledge distillation approaches, setting the softmax temperature to approximately T = 4 has been shown to improve performance <Paper corpusId=\"277994315\" paperTitle=\"(Hasegawa et al., 2025)\" isShortName></Paper>.\n\nThe optimal temperature value typically depends on several factors, including the complexity of the task, the architecture of both teacher and student models, and the dataset characteristics <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Temperature values that are too low (close to 1) may not sufficiently soften the probability distributions to reveal meaningful dark knowledge, while excessively high values might over-smooth the distributions, diluting the useful information contained within them <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nPractitioners should adopt an experimental approach when determining the optimal temperature for their specific knowledge distillation task. This typically involves training student models with various temperature values (often ranging from 2 to 20) and comparing their performance on validation datasets <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Some researchers recommend starting with the conventional T = 4 setting and then exploring higher or lower values based on initial results <Paper corpusId=\"277994315\" paperTitle=\"(Hasegawa et al., 2025)\" isShortName></Paper>.\n\nIt's also worth noting that the temperature parameter used during the training phase of knowledge distillation does not affect the inference phase\u2014once training is complete, the student model typically uses the standard softmax function (T = 1) for making predictions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This ensures that the final model maintains appropriate confidence levels in its predictions while still benefiting from the rich inter-class relationships learned during temperature-scaled training.\n\nFor more complex distillation scenarios, some advanced approaches dynamically adjust the temperature parameter throughout the training process or use different temperatures for different layers or components of the model <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. However, for most practical applications, a carefully selected static temperature value is sufficient to achieve effective knowledge transfer from teacher to student models.", "citations": [{"id": "(Hasegawa et al., 2025)", "paper": {"corpus_id": 277994315, "title": "Analytical Softmax Temperature Setting from Feature Dimensions for Model- and Domain-Robust Classification", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tatsuhito Hasegawa", "authorId": "2354312339"}, {"name": "Shunsuke Sakai", "authorId": "2348382817"}], "n_citations": 0}, "snippets": ["In conventional KD, it is well known that setting the softmax temperature to approximately T = 4 during distillation improves performance."], "score": 0.83154296875}], "table": null}], "cost": 0.41913600000000006}}
