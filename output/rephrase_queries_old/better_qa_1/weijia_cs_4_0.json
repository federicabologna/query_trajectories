{"better_query": "What specific technical methods have been proposed to detect and prevent copyright infringement in text-to-image diffusion models?", "better_answer": {"sections": [{"title": "Introduction to Copyright Issues in Text-to-Image Models", "tldr": "Text-to-image diffusion models can generate images that closely resemble copyrighted works when prompted with specific artist names or style descriptions. This capability has raised significant legal and ethical concerns regarding copyright infringement, attribution, and fair compensation for original creators. (LLM Memory)", "text": "\nText-to-image diffusion models like DALL-E, Midjourney, and Stable Diffusion have revolutionized image generation capabilities, allowing users to create high-quality images from textual descriptions. These models are trained on vast datasets of images scraped from the internet, which inevitably include copyrighted works from artists, photographers, and other visual creators. The fundamental copyright issue arises because these models can generate derivative works that mimic specific artists' styles when prompted with names like \"in the style of Greg Rutkowski\" or \"similar to Picasso,\" often without permission or compensation to the original creators. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe legal landscape surrounding these models remains unclear, as traditional copyright frameworks were not designed for AI-generated content. Courts and legal scholars are grappling with questions about whether training on copyrighted data constitutes fair use, who owns the rights to AI-generated images, and whether style imitation constitutes infringement. Several lawsuits have been filed against companies developing these models, with artists claiming their work was used without consent and that the models can produce near-replicas of their distinctive styles, potentially devaluing their original work and causing market harm. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe technical dimension of this issue is equally complex, as these models don't simply store and reproduce images but learn underlying patterns and features. This creates a challenging situation where the boundary between inspiration, transformation, and infringement becomes technically difficult to define and detect. Model developers must now consider implementing technical safeguards to prevent copyright infringement while maintaining model utility, spurring research into various detection and prevention methods. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Watermarking-Based Protection Methods", "tldr": "Watermarking techniques embed invisible signals into original artwork that can be detected in images generated by models trained on those works, providing concrete evidence of copyright infringement. These methods include specialized watermark generation algorithms that transfer from training data to outputs, with various implementations like DiffusionShield, FT-Shield, and DIAGNOSIS showing promising results for protecting visual artists' intellectual property. (10 sources)", "text": "\nWatermarking has emerged as a promising approach for protecting copyright in text-to-image diffusion models by embedding imperceptible signals in original images that persist through the model training process and appear in generated outputs. Unlike other protection methods, watermarking provides concrete evidence of copyright infringement that can potentially serve as legal proof when unauthorized use occurs <Paper corpusId=\"271600759\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. The fundamental principle involves embedding distinctive patterns that are invisible to humans but detectable through specialized algorithms after generation.\n\nSeveral watermarking frameworks have been developed specifically for diffusion models. FT-Shield introduces innovative watermark generation and detection strategies that ensure watermarks transfer seamlessly from training images to generated outputs. This system integrates a Mixture of Experts (MoE) approach for watermark detection to handle variability in fine-tuning methods <Paper corpusId=\"263622213\" paperTitle=\"(Cui et al., 2023)\" isShortName></Paper>. Similarly, DiffusionShield employs blockwise watermarks designed to convey greater amounts of information, allowing distinct copyright information to be more readily decoded through a joint optimization strategy that enhances both the pixel values of watermark patches and the decoding model <Paper corpusId=\"267412857\" paperTitle=\"(Ren et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231979499\" paperTitle=\"(Nichol et al., 2021)\" isShortName></Paper>.\n\nAnother approach, DIAGNOSIS, protects images by applying an image warping function that creates unique features detectable in generated outputs <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Luo et al. proposed a framework that embeds subtle watermarks into digital artworks to protect their copyrights while preserving the artist's visual expression, with the capability to detect mimicry through fine-tuning by analyzing the distribution of watermarks in generated images <Paper corpusId=\"265445146\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>.\n\nSome watermarking techniques build upon earlier work in adversarial examples and deep neural networks. For instance, approaches like those developed by Yu et al. demonstrate that neural networks can learn to encode rich information through invisible perturbations, which provides a foundation for data hiding techniques applicable to copyright protection <Paper corpusId=\"238419552\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper>. These methods can be made robust against various transformations including blurring, cropping, and even JPEG compression <Paper corpusId=\"50784854\" paperTitle=\"(Zhu et al., 2018)\" isShortName></Paper>.\n\nWatermarking approaches offer advantages over perturbation-based methods (which prevent learning entirely) by allowing legitimate use while providing a means to detect infringement. However, they typically require preemptive action before images are published online, as they must be applied to original works before model training occurs <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. When implemented effectively, these watermarking techniques can trace unauthorized data usage in both training and fine-tuning processes of text-to-image diffusion models, providing a promising technical solution to the challenge of copyright protection <Paper corpusId=\"274235104\" paperTitle=\"(Datta et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Wang et al., 2024)", "paper": {"corpus_id": 271600759, "title": "Replication in Visual Diffusion Models: A Survey and Outlook", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Wenhao Wang", "authorId": "2108977186"}, {"name": "Yifan Sun", "authorId": "2267731577"}, {"name": "Zongxin Yang", "authorId": "15556978"}, {"name": "Zhengdong Hu", "authorId": "2125634511"}, {"name": "Zhentao Tan", "authorId": "2296990692"}, {"name": "Yi Yang", "authorId": "2297815073"}], "n_citations": 9}, "snippets": ["By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks", "Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft."], "score": 0.98779296875}, {"id": "(Cui et al., 2023)", "paper": {"corpus_id": 263622213, "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models", "year": 2023, "venue": "SIGKDD Explorations", "authors": [{"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Yuping Lin", "authorId": "2254140893"}, {"name": "Han Xu", "authorId": "2253881697"}, {"name": "Pengfei He", "authorId": "2185740224"}, {"name": "Yue Xing", "authorId": "2253469617"}, {"name": "Wenqi Fan", "authorId": "2255025428"}, {"name": "Hui Liu", "authorId": "2253533415"}, {"name": "Jiliang Tang", "authorId": "2115879611"}], "n_citations": 12}, "snippets": ["FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model."], "score": 0.99365234375}, {"id": "(Ren et al., 2024)", "paper": {"corpus_id": 267412857, "title": "Copyright Protection in Generative AI: A Technical Perspective", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Han Xu", "authorId": "2253881697"}, {"name": "Pengfei He", "authorId": "2185740224"}, {"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Shenglai Zeng", "authorId": "2253682835"}, {"name": "Jiankun Zhang", "authorId": "2282560420"}, {"name": "Hongzhi Wen", "authorId": "2256788829"}, {"name": "Jiayuan Ding", "authorId": "46496977"}, {"name": "Hui Liu", "authorId": "2253533415"}, {"name": "Yi Chang", "authorId": "2267019992"}, {"name": "Jiliang Tang", "authorId": "2115879611"}], "n_citations": 42}, "snippets": ["The \"watermarking\" strategy is alternatively studied. This technique involves encoding sophisticated \"identifiable information\" into the copyrighted source data, such that this information also exists in the generated samples which are trained on the watermarked images. Subsequently, a detector is leveraged to assess whether a suspect image contains this encoded information, to trace and verify the ownership of copyright", "Focusing on DDPM (Nichol et al., 2021), Cui et al. [24] evaluated whether the injected watermarks via previous methods for traditional image watermarks (Navas et al., 2008)(Yu et al., 2020)(Zhu et al., 2018) can still be preserved in the generated samples. The empirical results show that these methods are either partially preserved in generated images or requires large perturbation budgets. Therefore, they proposed DiffusionShield [24], a watermarking method designed for diffusion models. To elaborate, blockwise watermarks, are engineered to convey a greater amount of information, allowing distinct copyright information to be more readily decoded. Then, a joint optimization strategy is leveraged to optimize both the pixel values of watermark patches, as well as a decoding model, which is utilized to detect and decode the encoded information from the generated images."], "score": 0.96044921875}, {"id": "(Nichol et al., 2021)", "paper": {"corpus_id": 231979499, "title": "Improved Denoising Diffusion Probabilistic Models", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alex Nichol", "authorId": "38967461"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}], "n_citations": 3724}, "snippets": ["Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion"], "score": 0.0}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 277043466, "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhenguang Liu", "authorId": "2243376899"}, {"name": "Chao Shuai", "authorId": "2243277462"}, {"name": "Shaojing Fan", "authorId": "2342623076"}, {"name": "Ziping Dong", "authorId": "2350529533"}, {"name": "Jinwu Hu", "authorId": "2350852841"}, {"name": "Zhongjie Ba", "authorId": "36890675"}, {"name": "Kui Ren", "authorId": "2286244627"}], "n_citations": 0}, "snippets": ["Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one."], "score": 0.98193359375}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 265445146, "title": "Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Ge Luo", "authorId": "2056101029"}, {"name": "Junqiang Huang", "authorId": "2268330053"}, {"name": "Manman Zhang", "authorId": "2260829743"}, {"name": "Zhenxing Qian", "authorId": "3243117"}, {"name": "Sheng Li", "authorId": "2153698673"}, {"name": "Xinpeng Zhang", "authorId": "2238534596"}], "n_citations": 9}, "snippets": ["In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed."], "score": 0.96728515625}, {"id": "(Yu et al., 2020)", "paper": {"corpus_id": 238419552, "title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data", "year": 2020, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Ning Yu", "authorId": "2052212417"}, {"name": "Vladislav Skripniuk", "authorId": "1818812352"}, {"name": "Sahar Abdelnabi", "authorId": "1383113350"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 219}, "snippets": ["Photorealistic image generation has reached a new level of quality due to the breakthroughs of generative adversarial networks (GANs). Yet, the dark side of such deepfakes, the malicious use of generated media, raises concerns about visual misinformation. While existing research work on deepfake detection demonstrates high accuracy, it is subject to advances in generation techniques and adversarial iterations on detection countermeasure techniques. Thus, we seek a proactive and sustainable solution on deepfake detection, that is agnostic to the evolution of generative models, by introducing artificial fingerprints into the models.Our approach is simple and effective. We first embed artificial fingerprints into training data, then validate a surprising discovery on the transferability of such fingerprints from training data to generative models, which in turn appears in the generated deepfakes. Experiments show that our fingerprinting solution (1) holds for a variety of cutting-edge generative models, (2) leads to a negligible side effect on generation quality, (3) stays robust against image-level and model-level perturbations, (4) stays hard to be detected by adversaries, and (5) converts deepfake detection and attribution into trivial tasks and outperforms the recent state-of-the-art baselines. Our solution closes the responsibility loop between publishing pre-trained generative model inventions and their possible misuses, which makes it independent of the current arms race."], "score": 0.0}, {"id": "(Zhu et al., 2018)", "paper": {"corpus_id": 50784854, "title": "HiDDeN: Hiding Data With Deep Networks", "year": 2018, "venue": "European Conference on Computer Vision", "authors": [{"name": "Jiren Zhu", "authorId": "2108812092"}, {"name": "Russell Kaplan", "authorId": "31121905"}, {"name": "Justin Johnson", "authorId": "2115231104"}, {"name": "Li Fei-Fei", "authorId": "48004138"}], "n_citations": 755}, "snippets": ["Recent work has shown that deep neural networks are highly sensitive to tiny perturbations of input images, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, we explore whether it can be beneficial. We find that neural networks can learn to use invisible perturbations to encode a rich amount of useful information. In fact, one can exploit this capability for the task of data hiding. We jointly train encoder and decoder networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the decoder can recover the original message. We show that these encodings are competitive with existing data hiding algorithms, and further that they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of Gaussian blurring, pixel-wise dropout, cropping, and JPEG compression. Even though JPEG is non-differentiable, we show that a robust model can be trained using differentiable approximations. Finally, we demonstrate that adversarial training improves the visual quality of encoded images."], "score": 0.0}, {"id": "(Du et al., 2025)", "paper": {"corpus_id": 277856857, "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models", "year": 2025, "venue": "The Web Conference", "authors": [{"name": "L. Du", "authorId": "151483943"}, {"name": "Zheng Zhu", "authorId": "2288042024"}, {"name": "Min Chen", "authorId": "2238153157"}, {"name": "Zhou Su", "authorId": "2328027909"}, {"name": "Shouling Ji", "authorId": "2237990407"}, {"name": "Peng Cheng", "authorId": "2147335888"}, {"name": "Jiming Chen", "authorId": "2238129188"}, {"name": "Zhikun Zhang", "authorId": "2238124154"}], "n_citations": 0}, "snippets": ["To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."], "score": 0.98486328125}, {"id": "(Datta et al., 2024)", "paper": {"corpus_id": 274235104, "title": "Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Soumil Datta", "authorId": "2277608846"}, {"name": "Shih-Chieh Dai", "authorId": "2332319316"}, {"name": "Leo Yu", "authorId": "2332299474"}, {"name": "Guanhong Tao", "authorId": "2332096509"}], "n_citations": 0}, "snippets": ["A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features."], "score": 0.96728515625}], "table": null}, {"title": "Adversarial Perturbation-Based Methods", "tldr": "Adversarial perturbation techniques protect artistic works by introducing subtle, imperceptible changes to images that disrupt how diffusion models learn or generate content. These methods actively prevent unauthorized style mimicry by modifying the latent representation during the diffusion process, causing models to generate unrealistic or distorted images when attempting to replicate protected content. (4 sources)", "text": "\nAdversarial perturbation-based methods represent a proactive approach to copyright protection that fundamentally differs from watermarking techniques. Rather than detecting infringement after it occurs, these methods aim to prevent unauthorized learning and generation entirely by introducing carefully designed perturbations to the original artwork. These perturbations are imperceptible to humans but significantly disrupt the learning process of text-to-image diffusion models. <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper>\n\nThe core principle behind these methods is to alter the latent representation of images in the diffusion process, causing models to be unable to generate images as expected when trained on protected content. One notable implementation is Glaze, which allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>\n\nAnother approach, proposed by Kumari et al., focuses on \"ablating concepts\" within pretrained models to prevent the generation of target concepts. Their algorithm works by matching the image distribution for a target style, instance, or text prompt to the distribution of an anchor concept, effectively making the model incapable of generating the protected content even when explicitly prompted. <Paper corpusId=\"257687839\" paperTitle=\"(Kumari et al., 2023)\" isShortName></Paper>\n\nSpecialized systems like Anti-DreamBooth provide defense against the misuse of fine-tuning techniques by adding noise perturbations to images before publication. This degrades the quality of images generated by models trained on these perturbed datasets, preventing successful style imitation. Similarly, EditShield introduces imperceptible perturbations that shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects when attempting to use protected content. <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>\n\nUnlike watermarking techniques which allow for detection of infringement after it occurs, adversarial perturbation methods actively prevent unauthorized use by making the protected content unusable for model training or generation. This represents a more aggressive approach to copyright protection that prioritizes prevention over detection. <Paper corpusId=\"263622213\" paperTitle=\"(Cui et al., 2023)\" isShortName></Paper>\n\nHowever, these methods face several limitations. They typically require modification of the original artwork before publication, making them less applicable to content that has already been published online. Additionally, they may be vulnerable to various image pre-processing techniques that could potentially neutralize the perturbations, and they generally cannot be applied retroactively to artwork that has already been shared publicly. <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>", "citations": [{"id": "(Du et al., 2024)", "paper": {"corpus_id": 268048573, "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models", "year": 2024, "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing", "authors": [{"name": "L. Du", "authorId": "151483943"}, {"name": "Zheng Zhu", "authorId": "2288042024"}, {"name": "Min Chen", "authorId": "2238153157"}, {"name": "Shouling Ji", "authorId": "2237990407"}, {"name": "Peng Cheng", "authorId": "2147335888"}, {"name": "Jiming Chen", "authorId": "2138800088"}, {"name": "Zhikun Zhang", "authorId": "2238124154"}], "n_citations": 3}, "snippets": ["To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork."], "score": 0.9697265625}, {"id": "(Du et al., 2025)", "paper": {"corpus_id": 277856857, "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models", "year": 2025, "venue": "The Web Conference", "authors": [{"name": "L. Du", "authorId": "151483943"}, {"name": "Zheng Zhu", "authorId": "2288042024"}, {"name": "Min Chen", "authorId": "2238153157"}, {"name": "Zhou Su", "authorId": "2328027909"}, {"name": "Shouling Ji", "authorId": "2237990407"}, {"name": "Peng Cheng", "authorId": "2147335888"}, {"name": "Jiming Chen", "authorId": "2238129188"}, {"name": "Zhikun Zhang", "authorId": "2238124154"}], "n_citations": 0}, "snippets": ["To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."], "score": 0.98486328125}, {"id": "(Kumari et al., 2023)", "paper": {"corpus_id": 257687839, "title": "Ablating Concepts in Text-to-Image Diffusion Models", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Nupur Kumari", "authorId": "46373847"}, {"name": "Bin Zhang", "authorId": "2119454239"}, {"name": "Sheng-Yu Wang", "authorId": "12782331"}, {"name": "Eli Shechtman", "authorId": "2177801"}, {"name": "Richard Zhang", "authorId": "2109976035"}, {"name": "Jun-Yan Zhu", "authorId": "1922024303"}], "n_citations": 201}, "snippets": ["To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition."], "score": 0.962890625}, {"id": "(Cui et al., 2023)", "paper": {"corpus_id": 263622213, "title": "FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models", "year": 2023, "venue": "SIGKDD Explorations", "authors": [{"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Yuping Lin", "authorId": "2254140893"}, {"name": "Han Xu", "authorId": "2253881697"}, {"name": "Pengfei He", "authorId": "2185740224"}, {"name": "Yue Xing", "authorId": "2253469617"}, {"name": "Wenqi Fan", "authorId": "2255025428"}, {"name": "Hui Liu", "authorId": "2253533415"}, {"name": "Jiliang Tang", "authorId": "2115879611"}], "n_citations": 12}, "snippets": ["FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection.\n\nTo protect images' IP from unauthorized learning by text-to-image models, in literature, two predominant methods are employed: (1) Adversarial methods which design perturbations in the data to prevent any model learning from the data; and (2) Watermarking techniques which introduce imperceptible signals to the image to enable protectors to detect infringement.\n\nGLAZE (Shan et al., 2023) is the first adversarial method which focuses on attacking the features extracted by the encoder in Stable Diffusion to prevent the learning of image styles. The work of Van Le et al. (2023) and Liang et al. (2023) introduces methods to generate adversarial examples to evade the infringement from DreamBooth (Ruiz et al., 2023) and Textual Inversion (Gal et al., 2022), respectively. Additionally, Salman et al. (2023) proposed to alter the pictures to protect them from image editing applications by Stable Diffusion in case the pictures are used to generate images with illegal or abnormal scenarios.\n\nWang et al. (2024a) proposed to apply an existing backdoor method (Nguyen & Tran, 2021) to embed unique signatures into the protected images. It aims to inject extra memorization into the text-to-image models fine-tuned on the protected dataset so that unauthorized data usage can be detected by checking whether the extra Figure 2: An overview of the two-stage watermarking protection process memorization exists in the suspected model."], "score": 0.99365234375}], "table": null}, {"title": "Detection Mechanisms for Copyright Infringement", "tldr": "Detection mechanisms for copyright infringement in text-to-image models utilize techniques like attention map analysis, watermark detection, and similarity comparison to identify unauthorized use of protected content. These methods range from examining model attention patterns and comparing image features to implementing specialized algorithms that can trace the origin of potentially infringing images. (9 sources)", "text": "\nCopyright infringement detection in text-to-image diffusion models has evolved to include several technical approaches that can identify when protected content has been used without authorization. One effective method analyzes attention maps during the diffusion process to identify regions of interest that may contain copyrighted elements. Zhang et al. proposed a copyright test that leverages the tendency of diffusion models to \"overattend\" to copyrighted areas, aggregating attention maps from the last reverse diffusion step to efficiently identify regions for similarity checking. This approach applies Gaussian blur filtering and standardization to generate binary masks of regions of interest, followed by cosine-similarity comparison of CLIP embeddings to detect substantial similarities with copyrighted content. <Paper corpusId=\"265352103\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>\n\nWatermarking has emerged as a particularly promising detection mechanism, providing concrete evidence of copyright infringement that can potentially serve as legal proof. Several systems have been developed to embed imperceptible watermarks into images that persist through the model training process and appear in generated outputs. Notable implementations include DIAGNOSIS, which detects unauthorized data usage by applying image warping functions; DiffusionShield, which embeds invisible watermarks containing copyright information; and FT-SHIELD, which verifies if data has been misused in training or fine-tuning of text-to-image models. <Paper corpusId=\"271600759\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>\n\nSome detection approaches are specifically designed to address the limitations of existing watermarking methods. Traditional watermarking techniques often modify large portions of datasets and may degrade image quality, while black-box Membership Inference (MI) methods require extensive queries to obtain significant results. <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper> More sophisticated approaches like GenWatermark target personalized text-to-image models, though it is primarily restricted to subject-driven synthesis. <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>\n\nRecent innovations include Wu et al.'s Contrasting Gradient Inversion for Diffusion Models (CGI-DM), which recovers missing details of partially obscured images by exploiting conceptual differences between pretrained and fine-tuned models, with the similarity between original and recovered images indicating potential infringement. This approach has demonstrated high accuracy in digital copyright authentication. <Paper corpusId=\"277043466\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268513090\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>\n\nA particularly comprehensive approach is Copyright-Shield, which addresses copyright infringement arising from backdoor injection attacks. This system employs spatial similarity detection by segmenting sample images based on prompts and comparing the segmented features with copyrighted images. It calculates a poisoning score using Intersection over Union (IoU) <Paper corpusId=\"67855581\" paperTitle=\"(Rezatofighi et al., 2019)\" isShortName></Paper> and Self-Supervised Copy Detection (SSCD) <Paper corpusId=\"247011159\" paperTitle=\"(Pizzi et al., 2022)\" isShortName></Paper> similarity scores to filter out poisoned samples. The method also implements infringement feature inversion to trace the origin of poisoned images, helping determine infringement liability. <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n\nThese detection mechanisms represent significant progress in addressing copyright concerns in text-to-image diffusion models, providing rights holders with technical means to identify unauthorized use of their intellectual property. While current solutions also involve removing copyrighted images from training datasets to prevent models from learning these images <Paper corpusId=\"257050406\" paperTitle=\"(Vyas et al., 2023)\" isShortName></Paper>, detection mechanisms remain crucial for identifying infringement in models that have already been trained on protected content. <Paper corpusId=\"274436785\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 265352103, "title": "On Copyright Risks of Text-to-Image Diffusion Models", "year": 2023, "venue": "", "authors": [{"name": "Yang Zhang", "authorId": "2267877984"}, {"name": "Teoh Tze Tzun", "authorId": "2267728071"}, {"name": "Lim Wei Hern", "authorId": "2267727392"}, {"name": "Haonan Wang", "authorId": "2267866973"}, {"name": "Kenji Kawaguchi", "authorId": "2256995496"}], "n_citations": 10}, "snippets": ["In addition to prompt generation, we propose a copyright test for identifying substantial similarities. Previously, we show the tendency of Text-to-Image (T2I) diffusion models to overattend to copyrighted areas in Figure 3. We apply this observation to find regions of interest for similarity check efficiently. Specifically, we aggregate attention maps from the last reverse diffusion step using a reduction function R(\u2022) to generate an aggregated attention map for each token. Suppose the prompt has t tokens, then there are t two-dimensional maps aggregated over attention heads in different layers of the diffusion model. Among the t aggregated attention maps, we apply a ranking process (detailed in Appendix C.4) to select the top m aggregated attention maps that are most likely to correspond to copyrighted features in the generated image. We then smooth the selected maps with a Gaussian blur filter G(\u2022, k, \u03c3) and apply Min-Max standardization to the maps. For selecting regions of interest, we transform the maps into two-dimensional binary masks B, with B i,j = 1 for values over 0.5, to isolate regions of interest in the generated image. \n\nGiven regions of interest, we can efficiently apply similarity check with copyrighted images using cosine-similarity of CLIP-embeddings. Sections from the generated images with similarity scores above 0.85 are considered to have substantial similarity with copyrighted content."], "score": 0.970703125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 271600759, "title": "Replication in Visual Diffusion Models: A Survey and Outlook", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Wenhao Wang", "authorId": "2108977186"}, {"name": "Yifan Sun", "authorId": "2267731577"}, {"name": "Zongxin Yang", "authorId": "15556978"}, {"name": "Zhengdong Hu", "authorId": "2125634511"}, {"name": "Zhentao Tan", "authorId": "2296990692"}, {"name": "Yi Yang", "authorId": "2297815073"}], "n_citations": 9}, "snippets": ["By embedding imperceptible watermarks into the data, one can detect the presence of these watermarks in the generated images if a visual diffusion model uses the data during training or fine-tuning processes. In this way, unveiling possible replication is simplified to detecting and verifying the occurrence of watermarks", "Unlike comparing similarities, which aligns with common sense but is difficult to use as legal evidence, watermarking techniques provide concrete evidence of copyright infringement and protect the intellectual property of rights holders. Several methods have been proposed to embed such watermarks into images. For instance, DIAGNOSIS [84] detects unauthorized data usage in text-to-image diffusion models by injecting unique behaviors into models via modified datasets; DiffusionShield [85] embeds invisible watermarks containing copyright information into images; and FT-SHIELD [86] uses imperceptible watermarks embedded in data to verify if it has been misused in the training or fine-tuning of textto-image diffusion models. Beyond watermarking general images, [87] embeds robust, invisible watermarks into artworks to trace art theft."], "score": 0.98779296875}, {"id": "(Ren et al._1, 2024)", "paper": {"corpus_id": 270620522, "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Chen Chen", "authorId": "2288619145"}, {"name": "Vikash Sehwag", "authorId": "3482535"}, {"name": "Yue Xing", "authorId": "2253469617"}, {"name": "Jiliang Tang", "authorId": "2115879611"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}], "n_citations": 1}, "snippets": ["Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2)."], "score": 0.98828125}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 277043466, "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhenguang Liu", "authorId": "2243376899"}, {"name": "Chao Shuai", "authorId": "2243277462"}, {"name": "Shaojing Fan", "authorId": "2342623076"}, {"name": "Ziping Dong", "authorId": "2350529533"}, {"name": "Jinwu Hu", "authorId": "2350852841"}, {"name": "Zhongjie Ba", "authorId": "36890675"}, {"name": "Kui Ren", "authorId": "2286244627"}], "n_citations": 0}, "snippets": ["Zhao et al. [48] used the pretrained watermark encoder (Yu et al., 2020) to embed a bit string into training images, aiming to track unauthorized image usage in diffusion models. However, this method is limited to unconditional diffusion models and hardly applied to more widely used textto-image models. Wang et al. [44] proposed DIAGNOSIS, coating protected images with an image warping function [28], and determined whether text-to-image models were trained or fine-tuned with protected images by detecting if the generated image contains same features as warping image. However, their method severely degrades image quality, and when coating rate of training dataset is small, the prediction is unreliable. Ma et al. [27] proposed GenWatermark for personalized text-to-image models, but it is restricted to subject-driven synthesis. Additionally, Wu et al. (Wu et al., 2024) leveraged reconstruction residuals of masked images to predict whether the model has been trained on a specific image, but it is limited to few-shot generation models and then examine suspected infringing images one by one."], "score": 0.98193359375}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 268513090, "title": "CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Xiaoyu Wu", "authorId": "2108069960"}, {"name": "Yang Hua", "authorId": "2147311278"}, {"name": "Chumeng Liang", "authorId": "2186858424"}, {"name": "Jiaru Zhang", "authorId": "2118001291"}, {"name": "Hao Wang", "authorId": "2144220882"}, {"name": "Tao Song", "authorId": "2055312951"}, {"name": "Haibing Guan", "authorId": "2292035375"}], "n_citations": 6}, "snippets": ["Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot generation where a pretrained model is fine-tuned on a small set of images to capture a specific style or object. Despite their success, concerns exist about potential copyright violations stemming from the use of unauthorized data in this process. In response, we present Contrasting Gradient Inversion for Diffusion Models (CGI-DM), a novel method featuring vivid visual representations for digital copyright authentication. Our approach involves removing partial information of an image and recovering missing details by exploiting conceptual differences between the pretrained and fine-tuned models. We formulate the differences as KL divergence between latent variables of the two models when given the same input image, which can be maximized through Monte Carlo sampling and Projected Gradient Descent (PGD). The similarity between original and recovered images serves as a strong indicator of potential infringements. Extensive experiments on the WikiArt and Dream-booth datasets demonstrate the high accuracy of CGI-DM in digital copyright authentication, surpassing alternative validation techniques. Code implementation is available at https://github.com/Nicholas0228/Revelio."], "score": 0.97119140625}, {"id": "(Rezatofighi et al., 2019)", "paper": {"corpus_id": 67855581, "title": "Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression", "year": 2019, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "S. H. Rezatofighi", "authorId": "73768948"}, {"name": "Nathan Tsoi", "authorId": "39282796"}, {"name": "JunYoung Gwak", "authorId": "39813007"}, {"name": "Amir Sadeghian", "authorId": "145759966"}, {"name": "I. Reid", "authorId": "145950884"}, {"name": "S. Savarese", "authorId": "1702137"}], "n_citations": 4181}, "snippets": ["Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks. However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value. The optimal objective for a metric is the metric itself. In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss. However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes. In this paper, we address the this weakness by introducing a generalized version of IoU as both a new loss and a new metric. By incorporating this generalized IoU ( GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO."], "score": 0.0}, {"id": "(Pizzi et al., 2022)", "paper": {"corpus_id": 247011159, "title": "A Self-Supervised Descriptor for Image Copy Detection", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Ed Pizzi", "authorId": "1900487455"}, {"name": "Sreya . Dutta Roy", "authorId": "2109836437"}, {"name": "Sugosh Nagavara Ravindra", "authorId": "2155482435"}, {"name": "Priya Goyal", "authorId": "47316088"}, {"name": "Matthijs Douze", "authorId": "3271933"}], "n_citations": 126}, "snippets": ["Image copy detection is an important task for content moderation. We introduce SSCD, a model that builds on a recent self-supervised contrastive training objective. We adapt this method to the copy detection task by changing the architecture and training objective, including a pooling operator from the instance matching literature, and adapting contrastive learning to augmentations that combine images. Our approach relies on an entropy regularization term, promoting consistent separation between descriptor vectors, and we demonstrate that this significantly improves copy detection accuracy. Our method produces a compact descriptor vector, suitable for real-world web scale applications. Statistical information from a background image distribution can be incorporated into the descriptor. On the recent DISC2021 benchmark, SSCD is shown to outperform both baseline copy detection models and self-supervised architectures designed for image classification by huge margins, in all settings. For example, SSCD out-performs SimCLR descriptors by 48% absolute. Code is available at https://github.com/facebookresearch/sscd-copy-detection."], "score": 0.0}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 274436785, "title": "CopyrightShield: Spatial Similarity Guided Backdoor Defense against Copyright Infringement in Diffusion Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhixiang Guo", "authorId": "2333463963"}, {"name": "Siyuan Liang", "authorId": "2325884825"}, {"name": "Aishan Liu", "authorId": "2257572247"}, {"name": "Dacheng Tao", "authorId": "2237906923"}], "n_citations": 3}, "snippets": ["Current solutions to copyright issues primarily involve the removal of copyrighted images from training datasets to prevent diffusion models from inadvertently learning these images, thus avoiding potential copyright infringement [7,(Vyas et al., 2023)[76]. However, effective defense mechanisms against existing backdoor injection attacks [21,28,36,74]77], which can result in copyright violations, have yet to be developed", ".we proposed Copyright-Shield, a backdoor defense method against copyright infringement based on the spatial similarity of replication phenomena. First, we designed a poisoning data detection method by leveraging the spatial similarity. We segment sample images based on prompts and compare the segmented features with copyrighted images. By calculating the poisoning score using the Intersection over Union (IoU) (Rezatofighi et al., 2019) and Self-Supervised Copy Detection (SSCD) (Pizzi et al., 2022) similarity scores of the features, we filter out the poisoned samples. Subsequently, we fine-tuned the model using the detected poisoned images. To reduce the image-prompt association characteristic of replication, we introduced a protective constraint term into the loss function. This involved comparing the similarity between poisoned images without paired prompts and generated samples, thereby limiting the model's memorization capability. Finally, we introduced the concept of infringement feature inversion to trace the origin of poisoned images through their features, thereby extending the application scenarios of the defense strategy and assisting in determining infringement liability."], "score": 0.98583984375}, {"id": "(Vyas et al., 2023)", "paper": {"corpus_id": 257050406, "title": "On Provable Copyright Protection for Generative Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Vyas", "authorId": "145603901"}, {"name": "S. Kakade", "authorId": "144695232"}, {"name": "B. Barak", "authorId": "1697211"}], "n_citations": 94}, "snippets": ["There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data $C$ that was in their training set. We give a formal definition of $\\textit{near access-freeness (NAF)}$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to $C$, even if $C$ is included in its training set. Roughly speaking, a generative model $p$ is $\\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of $p$ diverges by at most $k$-bits from the output of a model $q$ that $\\textit{did not access $C$ at all}$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content."], "score": 0.0}], "table": null}, {"title": "Unlearning and Model Adaptation Approaches", "tldr": "Unlearning and model adaptation techniques aim to remove or modify diffusion models' ability to generate copyright-infringing content without completely retraining them. These approaches include gradient ascent-based methods, reinforcement learning with copyright metrics, and inference-time frameworks that can dynamically detect and prevent the generation of protected content. (7 sources)", "text": "\nAs copyright concerns with text-to-image diffusion models have grown, researchers have developed various unlearning and model adaptation approaches to address the issue of copyright infringement without requiring complete model retraining. These methods aim to modify existing models to prevent them from generating content that infringes on copyrighted works, particularly addressing the problem of model memorization.\n\nModel memorization has been identified as a critical issue where diffusion models can reproduce parts of images (local memorization) or entire images (global memorization) from their training data during inference, potentially leading to copyright infringement without awareness from either the model's users or the copyright holders <Paper corpusId=\"278129333\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper> <Paper corpusId=\"254366634\" paperTitle=\"(Somepalli et al., 2022)\" isShortName></Paper> <Paper corpusId=\"256389993\" paperTitle=\"(Carlini et al., 2023)\" isShortName></Paper>. To address this, researchers have implemented gradient ascent-based and response-based pruning methods for unlearning, particularly targeting Stable Diffusion models <Paper corpusId=\"268532352\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. These approaches evaluate the effectiveness of unlearning by measuring similarities between original copyright images and their unlearned counterparts, as well as tracking changes in CLIP scores to determine how effectively the model has forgotten specific content.\n\nA more sophisticated approach is Reinforcement Learning-based Copyright Protection (RLCP), which utilizes a novel copyright metric grounded in copyright law and court precedents on infringement. This method employs the Denoising Diffusion Policy Optimization (DDPO) framework to guide models through a multi-step decision-making process, optimizing them using a reward function that incorporates copyright considerations while maintaining generated image quality <Paper corpusId=\"272146279\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. By including KL divergence as a regularization term, this approach stabilizes reinforcement learning fine-tuning and reduces undesirable outcomes.\n\nAnother innovative solution is the \"Guardians of Generation\" framework, which operates at inference time without requiring model retraining or weight modification. This model-agnostic approach integrates with existing diffusion pipelines and implements an adaptive guidance mechanism comprising three components: detection, prompt rewriting, and guidance adjustment. The system monitors user prompts and intermediate generation steps to identify potential copyright infringement before it appears in the final output. When problematic content is detected, it dynamically transforms the user's prompt and steers the diffusion process away from protected content while preserving the prompt's intended meaning <Paper corpusId=\"277151077\" paperTitle=\"(Roy et al., 2025)\" isShortName></Paper>.\n\nDespite these advances, there are limitations to current approaches. Some existing methods focus primarily on detecting illegally generated content but cannot effectively prevent or mitigate illegal adaptations of diffusion models. Similarly, standard model unlearning and reinitialization approaches may not prevent users from relearning protected content through custom data <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This highlights the need for ongoing research into more robust unlearning and adaptation techniques that can address the full spectrum of copyright infringement scenarios.", "citations": [{"id": "(Chen et al., 2025)", "paper": {"corpus_id": 278129333, "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chen Chen", "authorId": "1696291"}, {"name": "Daochang Liu", "authorId": "51023221"}, {"name": "Mubarak Shah", "authorId": "2302950741"}, {"name": "Chang Xu", "authorId": "2288626806"}], "n_citations": 1}, "snippets": ["Recent research (Carlini et al., 2023)8,(Somepalli et al., 2022)[34] has revealed a critical issue: these models can memorize training data, leading them to reproduce parts of images, such as foregrounds or backgrounds (local memorization, see Fig. 1), or even entire images (global memorization, see Fig. 2) during inference, instead of generating genuinely novel content. When the training data includes sensitive or copyrighted material, these memorization issues can infringe on copyright laws without notifying either the model's owners or users or the copyright holders of the replicated content."], "score": 0.96435546875}, {"id": "(Somepalli et al., 2022)", "paper": {"corpus_id": 254366634, "title": "Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Gowthami Somepalli", "authorId": "2003112028"}, {"name": "Vasu Singla", "authorId": "1824188732"}, {"name": "Micah Goldblum", "authorId": "121592562"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 329}, "snippets": ["Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, ImageNet, and LAION, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data. Project page: https://somepago.github.io/diffrep.html"], "score": 0.0}, {"id": "(Carlini et al., 2023)", "paper": {"corpus_id": 256389993, "title": "Extracting Training Data from Diffusion Models", "year": 2023, "venue": "USENIX Security Symposium", "authors": [{"name": "Nicholas Carlini", "authorId": "2483738"}, {"name": "Jamie Hayes", "authorId": "9200194"}, {"name": "Milad Nasr", "authorId": "3490923"}, {"name": "Matthew Jagielski", "authorId": "40844378"}, {"name": "Vikash Sehwag", "authorId": "3482535"}, {"name": "Florian Tram\u00e8r", "authorId": "2444919"}, {"name": "Borja Balle", "authorId": "1718064"}, {"name": "Daphne Ippolito", "authorId": "7975935"}, {"name": "Eric Wallace", "authorId": "145217343"}], "n_citations": 617}, "snippets": ["Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."], "score": 0.0}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 268532352, "title": "A Dataset and Benchmark for Copyright Infringement Unlearning from Text-to-Image Diffusion Models", "year": 2024, "venue": "", "authors": [{"name": "Rui Ma", "authorId": "2210435658"}, {"name": "Qiang Zhou", "authorId": "2257338567"}, {"name": "Yizhu Jin", "authorId": "2268733263"}, {"name": "Daquan Zhou", "authorId": "2292161412"}, {"name": "Bangjun Xiao", "authorId": "2292177829"}, {"name": "Xiuyu Li", "authorId": "2292217065"}, {"name": "Yi Qu", "authorId": "2292690089"}, {"name": "Aishani Singh", "authorId": "2261448160"}, {"name": "Kurt Keutzer", "authorId": "2242659602"}, {"name": "Jingtong Hu", "authorId": "2328473437"}, {"name": "Xiaodong Xie", "authorId": "2307915260"}, {"name": "Zhen Dong", "authorId": "2293731776"}, {"name": "Shanghang Zhang", "authorId": "2257020214"}, {"name": "Shiji Zhou", "authorId": "2275298300"}], "n_citations": 2}, "snippets": ["We reuse the CM metric to indicate the similarities between the original copyright images and their unlearned counterparts after processed by unlearning methods. Additionally, we evaluate changes of CLIP scores, denoted as \u2206CLIP, for text-image similarity. This indicates the extent to which the prompt that generates potential infringement is nullified", "In our experiments, we utilize gradient ascent-based and response-based pruning methods for unlearning, as comparison baselines for other unlearning approaches, specifically targeting the Stable Diffusion models."], "score": 0.96240234375}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 272146279, "title": "RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Jing Yan", "authorId": "2318391138"}, {"name": "Xiaoli Tang", "authorId": "2318236128"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 1}, "snippets": ["To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning."], "score": 0.99365234375}, {"id": "(Roy et al., 2025)", "paper": {"corpus_id": 277151077, "title": "Guardians of Generation: Dynamic Inference-Time Copyright Shielding with Adaptive Guidance for AI Image Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Soham Roy", "authorId": "2351229051"}, {"name": "Abhishek Mishra", "authorId": "2351593773"}, {"name": "S. Karande", "authorId": "40151143"}, {"name": "Murari Mandal", "authorId": "2316561345"}], "n_citations": 0}, "snippets": ["We introduce Guardians of Generation, a model agnostic inference time framework for dynamic copyright shielding in AI image generation. Our approach requires no retraining or modification of the generative model weights, instead integrating seamlessly with existing diffusion pipelines. It augments the generation process with an adaptive guidance mechanism comprising three components: a detection module, a prompt rewriting module, and a guidance adjustment module. The detection module monitors user prompts and intermediate generation steps to identify features indicative of copyrighted content before they manifest in the final output. If such content is detected, the prompt rewriting mechanism dynamically transforms the user's prompt by sanitizing or replacing references that could trigger copyrighted material while preserving the prompt's intended semantics. The adaptive guidance module adaptively steers the diffusion process away from flagged content by modulating the model's sampling trajectory."], "score": 0.9873046875}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 270067889, "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Huang", "authorId": "2112769493"}, {"name": "Wei Gao", "authorId": "2274008610"}], "n_citations": 2}, "snippets": ["Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data."], "score": 0.9677734375}], "table": null}, {"title": "Automated Copyright Judgment Systems", "tldr": "Automated copyright judgment systems employ large vision-language models to assess infringement in AI-generated images by simulating legal reasoning processes. These systems can identify potential copyright violations by comparing similarities between original and generated content, while also offering mitigation strategies to modify infringing prompts or generation parameters. (1 source)", "text": "\nRecent advances in AI technology have led to the development of automated copyright judgment systems that can evaluate potential infringement in text-to-image diffusion models. Unlike detection mechanisms that focus solely on identifying unauthorized content, these systems aim to replicate the legal reasoning processes used in copyright infringement cases to provide more nuanced assessments.\n\nA notable example is CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes. This system implements an abstraction-filtration-comparison test framework that mimics legal reasoning, employing multi-LVLM debate to assess the likelihood of infringement between copyrighted images and those generated by text-to-image diffusion models. Beyond simply detecting potential infringement, CopyJudge provides detailed judgment rationales that explain the basis for its decisions <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nWhat distinguishes CopyJudge from other approaches is its integrated mitigation strategy. Upon identifying potentially infringing content, the system can automatically optimize problematic prompts by avoiding sensitive expressions while preserving non-infringing content. This capability allows users to generate similar but legally compliant images without significantly compromising their creative intent. Additionally, CopyJudge can be enhanced through reinforcement learning techniques that explore non-infringing noise vectors within the diffusion latent space, enabling copyright compliance even without modifying the original prompts <Paper corpusId=\"276558342\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nThese automated judgment systems represent a significant advancement in addressing copyright concerns in text-to-image models, as they not only identify potential infringement but also provide actionable guidance for creating compliant content. By incorporating legal frameworks and reasoning processes, these systems help bridge the gap between technical detection and legal assessment, offering a more comprehensive approach to managing copyright issues in generative AI <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Liu et al._1, 2025)", "paper": {"corpus_id": 276558342, "title": "CopyJudge: Automated Copyright Infringement Identification and Mitigation in Text-to-Image Diffusion Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shunchang Liu", "authorId": "2346891526"}, {"name": "Zhuan Shi", "authorId": "2319419519"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}, {"name": "Yaochu Jin", "authorId": "2344619001"}, {"name": "Boi Faltings", "authorId": "2054858128"}], "n_citations": 2}, "snippets": ["In this paper, we propose CopyJudge, an automated copyright infringement identification framework that leverages large vision-language models (LVLMs) to simulate practical court processes for determining substantial similarity between copyrighted images and those generated by text-to-image diffusion models. Specifically, we employ an abstraction-filtration-comparison test framework with multi-LVLM debate to assess the likelihood of infringement and provide detailed judgment rationales. Based on the judgments, we further introduce a general LVLM-based mitigation strategy that automatically optimizes infringing prompts by avoiding sensitive expressions while preserving the non-infringing content. Besides, our approach can be enhanced by exploring non-infringing noise vectors within the diffusion latent space via reinforcement learning, even without modifying the original prompts."], "score": 0.99462890625}], "table": null}, {"title": "Limitations and Challenges of Current Methods", "tldr": "Current copyright protection methods for text-to-image models face significant limitations including the requirement to modify original artwork before publication, vulnerability to preprocessing techniques, and inability to address already published content. Many approaches also struggle with balancing protection effectiveness against image quality degradation and face practical challenges in large-scale implementation. (4 sources)", "text": "\nDespite the advances in watermarking, adversarial perturbation, detection mechanisms, and automated judgment systems, several fundamental limitations restrict the effectiveness of current copyright protection methods for text-to-image diffusion models. One of the most significant challenges is that many techniques require preemptive modification of original artwork before it is published online <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. This requirement makes these methods impractical for the vast amount of visual content already available on the internet, which continues to be scraped for training datasets.\n\nBoth perturbation-based and watermarking-based approaches share this limitation, as they typically cannot be applied retroactively to published artwork without access to and modification of the original files <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>. This creates a substantial gap in protection for artists whose work is already widely distributed online, effectively leaving their intellectual property vulnerable to being incorporated into training datasets without consent.\n\nAnother critical weakness is the vulnerability of many protection methods to image preprocessing techniques. Simple transformations such as cropping, resizing, or format conversion can potentially neutralize the protective effects of subtle perturbations or watermarks <Paper corpusId=\"268048573\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper>. This vulnerability significantly undermines the reliability of these methods, as sophisticated users can potentially circumvent protection mechanisms through basic image manipulation.\n\nExisting watermarking approaches face additional practical challenges. Many watermarking methods require modifying large portions or even entire datasets, which becomes increasingly impractical as dataset sizes grow <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. Furthermore, these modifications can inadvertently degrade image quality, creating an undesirable tradeoff between protection and aesthetic integrity <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. \n\nAlternative approaches like black-box Membership Inference (MI) techniques avoid modifying the original data but require extensive querying to produce significant results, making them computationally expensive and time-consuming <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>. Similarly, poison-only backdoor attack methods adapted for detecting dataset usage show reduced robustness when subjected to re-captioning <Paper corpusId=\"270620522\" paperTitle=\"(Ren et al._1, 2024)\" isShortName></Paper>.\n\nFrom a model-centric perspective, current methods primarily focus on detecting illegally generated content but often fail to prevent or mitigate illegal adaptations of diffusion models <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. Standard model unlearning and reinitialization approaches face a similar limitation\u2014they cannot prevent determined users from relearning protected content through custom datasets <Paper corpusId=\"270067889\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This creates a persistent vulnerability where even after protected content is removed from a model, it can be reintroduced through fine-tuning with a small dataset of collected examples.\n\nThe technical challenges are compounded by the rapid evolution of text-to-image models and the diverse ecosystem of model architectures, making it difficult to develop universal protection methods that work across all implementations. As diffusion models continue to advance and become more accessible, the need for more robust, scalable, and adaptable copyright protection methods becomes increasingly urgent, particularly for content that is already publicly available <Paper corpusId=\"277856857\" paperTitle=\"(Du et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Du et al., 2024)", "paper": {"corpus_id": 268048573, "title": "WIP: Auditing Artist Style Pirate in Text-to-image Generation Models", "year": 2024, "venue": "Proceedings 2024 Workshop on AI Systems with Confidential COmputing", "authors": [{"name": "L. Du", "authorId": "151483943"}, {"name": "Zheng Zhu", "authorId": "2288042024"}, {"name": "Min Chen", "authorId": "2238153157"}, {"name": "Shouling Ji", "authorId": "2237990407"}, {"name": "Peng Cheng", "authorId": "2147335888"}, {"name": "Jiming Chen", "authorId": "2138800088"}, {"name": "Zhikun Zhang", "authorId": "2238124154"}], "n_citations": 3}, "snippets": ["To tackle these issues, previous works have proposed strategies such as adversarial perturbation-based and watermarking-based methods. The former involves introducing subtle changes to disrupt the image generation process, while the latter involves embedding detectable marks in the artwork. The existing methods face limitations such as requiring modifications of the original image, being vulnerable to image pre-processing, and facing difficulties in applying them to the published artwork."], "score": 0.9697265625}, {"id": "(Du et al., 2025)", "paper": {"corpus_id": 277856857, "title": "ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models", "year": 2025, "venue": "The Web Conference", "authors": [{"name": "L. Du", "authorId": "151483943"}, {"name": "Zheng Zhu", "authorId": "2288042024"}, {"name": "Min Chen", "authorId": "2238153157"}, {"name": "Zhou Su", "authorId": "2328027909"}, {"name": "Shouling Ji", "authorId": "2237990407"}, {"name": "Peng Cheng", "authorId": "2147335888"}, {"name": "Jiming Chen", "authorId": "2238129188"}, {"name": "Zhikun Zhang", "authorId": "2238124154"}], "n_citations": 0}, "snippets": ["To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.\n\nThe existing solutions can be classified into two categories by the underlying technologies, i.e., the perturbation-based methods [5,56,63,75] and the watermark-based methods [11,36,39,65,77]. The perturbation-based methods introduce subtle perturbations that alter the latent representation in the diffusion process, causing models to be unable to generate images as expected. The watermark-based methods inject imperceptible watermarks into artworks before they are shared. The diffusion model collects and learns the watermarked artworks. The artists can then validate the infringements by checking if the watermarks exist in the generated images. Membership inference (MI) [2,4,6,58] is another technique to determine whether specific data was used to train or fine-tune the diffusion model [15,26,43,67].\n\nPerturbation-based Method. The artists can introduce slight perturbations that modify the latent representation during the diffusion process, preventing models from generating the expected images. Shan et al. [56] introduce Glaze, a tool that allows artists to apply \"style cloaks\" to their artwork, introducing subtle perturbations that mislead generative models attempting to replicate a specific artist's style. Similarly, Anti-DreamBooth [63] is a defense system designed to protect against the misuse of DreamBooth by adding slight noise perturbations to images before they are published, thereby degrading the quality of images generated by models trained on these perturbed datasets. Chen et al. [5] propose EditShield, a protection method that introduces imperceptible perturbations to shift the latent representation during the diffusion process, causing models to produce unrealistic images with mismatched subjects.\n\nWatermark-based Method. This framework adds subtle watermarks to digital artworks to protect copyrights while preserving the artist's expression. Cui et al. [11] construct the watermark by converting the copyright message into an ASCII-based binary sequence and then translating it into a quaternary sequence. During the copyright auditing, they adopt a ResNet-based decoder to recover the watermarks from the images generated by a third-party propose GenWatermark, a novel system that jointly trains a watermark generator and detector."], "score": 0.98486328125}, {"id": "(Ren et al._1, 2024)", "paper": {"corpus_id": 270620522, "title": "EnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models with Minimal and Robust Alterations", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jie Ren", "authorId": "2256589810"}, {"name": "Yingqian Cui", "authorId": "2218740984"}, {"name": "Chen Chen", "authorId": "2288619145"}, {"name": "Vikash Sehwag", "authorId": "3482535"}, {"name": "Yue Xing", "authorId": "2253469617"}, {"name": "Jiliang Tang", "authorId": "2115879611"}, {"name": "Lingjuan Lyu", "authorId": "2287820224"}], "n_citations": 1}, "snippets": ["Observing the above, techniques like watermarking [10,11,12,13] and black-box Membership Inference (MI) [14,15] have been employed to protect data specifically against unauthorized finetuning in text-to-image diffusion models.Nevertheless, existing watermark methods often face some common problems.For example, they usually modify a large portion [12] or even the whole of the dataset [11], which is not realistic for large-scale datasets.They also unexpectedly affect the quality [13,11].Meanwhile, as black-box MI does not alter the data to boost the detection, it needs highly extensive queries to get a significant result.Another line of techniques, poison-only backdoor attack [16]17], can be adapted for detecting dataset usage by verifying the attacked behavior.However, they are inherently designed for malicious attacking and demonstrate reduced robustness when subjected to re-captioning (as shown by Sec 5.2)."], "score": 0.98828125}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 270067889, "title": "FreezeAsGuard: Mitigating Illegal Adaptation of Diffusion Models via Selective Tensor Freezing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Huang", "authorId": "2112769493"}, {"name": "Wei Gao", "authorId": "2274008610"}], "n_citations": 2}, "snippets": ["Existing work focused on detecting the illegally generated contents, but cannot prevent or mitigate illegal adaptations of diffusion models. Other schemes of model unlearning and reinitialization, similarly, cannot prevent users from relearning the knowledge of illegal model adaptation with custom data."], "score": 0.9677734375}], "table": null}], "cost": 0.34997100000000003}}
