{"better_query": "What are the key architectural differences between autoregressive language models and traditional dual-encoder models in dense passage retrieval, and how do these differences affect retrieval accuracy and computational efficiency?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Information retrieval systems have evolved from traditional sparse retrieval methods to more sophisticated approaches using dual-encoder architectures and, more recently, autoregressive language models. This evolution addresses limitations in representation capabilities and interaction between queries and documents. (4 sources)", "text": "\nInformation retrieval systems have undergone significant evolution in recent years, particularly in the context of open-domain question answering where identifying relevant passages is crucial for answer extraction. Traditional approaches relied on term-based passage retrievers such as TF-IDF and BM25, which match questions and passages based on lexical overlap <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"3618568\" paperTitle=\"(Chen et al., 2017)\" isShortName></Paper> <Paper corpusId=\"207178704\" paperTitle=\"(Robertson et al., 2009)\" isShortName></Paper>. These sparse retrieval methods, while foundational, suffer from limited representation capabilities as they primarily focus on exact term matches rather than semantic understanding <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>.\n\nTo address these limitations, researchers have developed dense passage retrieval approaches that represent both questions and documents as dense vectors (embeddings) using dual-encoder neural architectures <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>. Notable implementations such as ORQA and DPR employ dense context vectors for passage indexing, significantly enhancing retrieval performance compared to traditional methods <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. However, dual-encoder models have their own constraints, particularly because the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions between queries and documents <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nMore recently, research interest has surged in utilizing autoregressive language models for retrieval tasks, offering a new paradigm that can potentially overcome the interaction bottleneck inherent in dual-encoder models <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These models generate identifiers to simplify the retrieval process while enabling deeper interactions between queries and documents. This shift represents a fundamental architectural change in how retrieval systems are designed and implemented <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Qu et al., 2020)", "paper": {"corpus_id": 231815627, "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yingqi Qu", "authorId": "51281403"}, {"name": "Yuchen Ding", "authorId": "2111070044"}, {"name": "Jing Liu", "authorId": "46700619"}, {"name": "Kai Liu", "authorId": "2146384872"}, {"name": "Ruiyang Ren", "authorId": "1708171825"}, {"name": "Xin Zhao", "authorId": "2145734278"}, {"name": "Daxiang Dong", "authorId": "9532787"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 617}, "snippets": ["For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever", "Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a)."], "score": 0.64111328125}, {"id": "(Chen et al., 2017)", "paper": {"corpus_id": 3618568, "title": "Reading Wikipedia to Answer Open-Domain Questions", "year": 2017, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Adam Fisch", "authorId": "2064150446"}, {"name": "J. Weston", "authorId": "145183709"}, {"name": "Antoine Bordes", "authorId": "1713934"}], "n_citations": 2019}, "snippets": ["This paper proposes to tackle open-domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task."], "score": 0.0}, {"id": "(Robertson et al., 2009)", "paper": {"corpus_id": 207178704, "title": "The Probabilistic Relevance Framework: BM25 and Beyond", "year": 2009, "venue": "Foundations and Trends in Information Retrieval", "authors": [{"name": "S. Robertson", "authorId": "144430625"}, {"name": "H. Zaragoza", "authorId": "2833561"}], "n_citations": 3760}, "snippets": ["The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970\u20141980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268031876, "title": "Generative Retrieval with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ye Wang", "authorId": "2185022832"}, {"name": "Xinrun Xu", "authorId": "2290204960"}, {"name": "Rui Xie", "authorId": "2143721734"}, {"name": "Wenxin Hu", "authorId": "2288018918"}, {"name": "Wei Ye", "authorId": "2052980435"}], "n_citations": 1}, "snippets": ["Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."], "score": 0.70654296875}], "table": null}, {"title": "Dual-Encoder Architecture", "tldr": "Dual-encoder architectures represent the dominant approach in dense passage retrieval, independently encoding queries and documents into vector representations that enable efficient similarity computation. This architecture allows for pre-computation of document embeddings, facilitating fast retrieval through approximate nearest neighbor search techniques at query time. (25 sources)", "text": "\nDual-encoder architectures form the backbone of modern dense retrieval systems, offering a balance between efficiency and effectiveness that has made them the standard approach for information retrieval tasks <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>. Unlike traditional term-based methods, these models encode both queries and documents as dense vectors in a shared semantic space <Paper corpusId=\"243865399\" paperTitle=\"(Lu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"173990818\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.\n\nThe fundamental characteristic of dual-encoder models is their use of two separate encoders\u2014one for queries and one for documents\u2014that map inputs into a common representation space <Paper corpusId=\"254685782\" paperTitle=\"(Zhou et al., 2022)\" isShortName></Paper>. The relevance between a query and document is typically calculated using lightweight similarity functions such as dot product or cosine similarity <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"258833383\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This architecture provides a crucial efficiency advantage: document representations can be pre-computed and indexed offline, with only the query needing to be encoded at inference time <Paper corpusId=\"252816088\" paperTitle=\"(Cheng et al., 2022)\" isShortName></Paper>.\n\nDuring inference, when a new query arrives, it is encoded by the query encoder, and relevant documents are retrieved through approximate nearest neighbor (ANN) search techniques <Paper corpusId=\"265457188\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"926364\" paperTitle=\"(Johnson et al., 2017)\" isShortName></Paper>. This approach dramatically reduces retrieval time compared to methods that require computing interactions between queries and all possible documents <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper>.\n\nMost prominent implementations of dense retrievers, such as DPR <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>, RocketQA <Paper corpusId=\"231815627\" paperTitle=\"(Qu et al., 2020)\" isShortName></Paper>, TAS-B <Paper corpusId=\"233231706\" paperTitle=\"(Hofstatter et al., 2021)\" isShortName></Paper>, and ColBERT <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>, utilize various forms of dual-encoder architectures <Paper corpusId=\"258331649\" paperTitle=\"(Salemi et al., 2023)\" isShortName></Paper>. These models generally start with pre-trained language models (PLMs) that are then fine-tuned for the retrieval task <Paper corpusId=\"258557604\" paperTitle=\"(Zhuang et al., 2023)\" isShortName></Paper>.\n\nHowever, dual-encoder architectures face a fundamental limitation: they encode queries and documents independently, without modeling the interactions between them <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. This independent encoding causes information loss and can lead to suboptimal retrieval performance compared to models that incorporate cross-attention mechanisms <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. In contrast, cross-encoder models that jointly process query-document pairs achieve higher accuracy but are computationally impractical for direct retrieval from large collections <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"201307832\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nTo address this trade-off, some researchers have developed hybrid approaches that maintain the efficiency of dual-encoders while incorporating limited forms of interaction. For example, ColBERT introduces a late interaction mechanism that retains fine-grained token-level interactions while still allowing for efficient pre-computation <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper> <Paper corpusId=\"275931943\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. Others have explored multi-representation approaches <Paper corpusId=\"267751308\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> or asymmetric architectures with stronger query encoders <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>.\n\nDespite these limitations, dual-encoder architectures remain the preferred choice for large-scale retrieval systems due to their computational efficiency and scalability <Paper corpusId=\"259203703\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>. Their ability to support fast retrieval from millions or billions of documents makes them indispensable for practical applications, even as research continues to improve their effectiveness <Paper corpusId=\"273026177\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Qu et al., 2020)", "paper": {"corpus_id": 231815627, "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yingqi Qu", "authorId": "51281403"}, {"name": "Yuchen Ding", "authorId": "2111070044"}, {"name": "Jing Liu", "authorId": "46700619"}, {"name": "Kai Liu", "authorId": "2146384872"}, {"name": "Ruiyang Ren", "authorId": "1708171825"}, {"name": "Xin Zhao", "authorId": "2145734278"}, {"name": "Daxiang Dong", "authorId": "9532787"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 617}, "snippets": ["For open-domain QA, passage retriever is an important component to identify relevant passages for answer extraction. Traditional approaches (Chen et al., 2017) implemented term-based passage retriever (e.g. TF-IDF and BM25), which have limited representation capabilities. Recently, researchers have utilized deep learning to improve traditional passage retriever", "Different from the above term-based approaches, dense passage retrieval has been proposed to represent both questions and documents as dense vectors (i.e., embeddings), typically in a dual-encoder neural architecture (as shown in Figure 1a)."], "score": 0.64111328125}, {"id": "(Lu et al., 2021)", "paper": {"corpus_id": 243865399, "title": "Less is More: Pretrain a Strong Siamese Encoder for Dense Text Retrieval Using a Weak Decoder", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shuqi Lu", "authorId": "1830381674"}, {"name": "Di He", "authorId": "1391126980"}, {"name": "Chenyan Xiong", "authorId": "2139787803"}, {"name": "Guolin Ke", "authorId": "35286545"}, {"name": "Waleed Malik", "authorId": "2060300532"}, {"name": "Zhicheng Dou", "authorId": "1897235"}, {"name": "Paul N. Bennett", "authorId": "144609235"}, {"name": "Tie-Yan Liu", "authorId": "2110264337"}, {"name": "Arnold Overwijk", "authorId": "2734525"}], "n_citations": 74}, "snippets": ["In the first-stage retrieval of these scenarios, DR models generally employ a Siamese/Dual-Encoder architecture in practice. The encoder model first separately encodes the user side (query, browsing history, or question) and the corpus side (document or passages) as individual embeddings in a learned representation space (Lee et al., 2019), where retrieval with simple similarity metrics are conducted effectively (Johnson et al., 2017;Guo et al., 2020)."], "score": 0.6240234375}, {"id": "(Lee et al., 2019)", "paper": {"corpus_id": 173990818, "title": "Latent Retrieval for Weakly Supervised Open Domain Question Answering", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 1018}, "snippets": ["Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match."], "score": 0.0}, {"id": "(Zhou et al., 2022)", "paper": {"corpus_id": 254685782, "title": "MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers", "year": 2022, "venue": "ECML/PKDD", "authors": [{"name": "Kun Zhou", "authorId": "1423651904"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Daxin Jiang", "authorId": "71790825"}, {"name": "Nan Duan", "authorId": "46429989"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 16}, "snippets": ["In this work, we aim to propose a more effective multi-task pre-training framework specially for the dense retrieval task, which learns to compress more useful information into the [CLS] representations", "It consists of a query encoder E_q and a passage encoder E_p, mapping the query q and passage p into k-dimensional dense vectors h_q and h_p, respectively. Then, the semantic relevance score of q and p will be computed using dot product"], "score": 0.8369140625}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 253157959, "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 4}, "snippets": ["To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair."], "score": 0.71728515625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 258833383, "title": "Challenging Decoder helps in Masked Auto-Encoder Pre-training for Dense Passage Retrieval", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Zehan Li", "authorId": "2109967721"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 3}, "snippets": ["To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture, where query and passage are encoded into continuous vector representations by PLMs respectively, and then a lightweight score function such as dot product or cosine similarity between two vectors is used to estimate the semantic similarity between the query-passage pair", "Currently, the Masked Auto-Encoder (MAE) is arguably the most effective pre-training framework in retrieval tasks. As illustrated in Figure 1, MAE utilizes the encoder-decoder architecture in which the sentence is randomly masked twice as the input to the encoder and decoder, respectively, and the sentence embedding pooled from the encoder is concatenated with the masked input of the decoder to reconstruct the original input."], "score": 0.6904296875}, {"id": "(Cheng et al., 2022)", "paper": {"corpus_id": 252816088, "title": "Task-Aware Specialization for Efficient and Robust Dense Retrieval for Open-Domain Question Answering", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hao Cheng", "authorId": "47413820"}, {"name": "Hao Fang", "authorId": "145204655"}, {"name": "Xiaodong Liu", "authorId": "46522098"}, {"name": "Jianfeng Gao", "authorId": "48441311"}], "n_citations": 6}, "snippets": ["Specifically, the de-facto architecture for open-domain question answering uses two isomorphic encoders that are initialized from the same pretrained model but separately parameterized for questions and passages. This biencoder architecture is parameter-inefficient in that there is no parameter sharing between encoders", "During inference, all passages are pre-converted into vectors using the passage encoder. Then, each incoming question is encoded using the question encoder, and a top-K list of most relevant passages are retrieved based on their relevance scores with respect to the question."], "score": 0.8310546875}, {"id": "(Jiang et al., 2023)", "paper": {"corpus_id": 265457188, "title": "Boot and Switch: Alternating Distillation for Zero-Shot Dense Retrieval", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fan Jiang", "authorId": "51511290"}, {"name": "Qiongkai Xu", "authorId": "2257020484"}, {"name": "Tom Drummond", "authorId": "2268408815"}, {"name": "Trevor Cohn", "authorId": "2256997782"}], "n_citations": 2}, "snippets": ["The dense retrieval model (retriever) encodes both queries and passages into dense vectors using a dual-encoder architecture (Karpukhin et al., 2020). Two distinct encoders are applied to transform queries and passages separately, then, a relevance score is calculated by a dot product, \n\nwhere E(\u2022; \u03b8) are encoders parameterised by \u03b8 p for passages and \u03b8 q for queries. The asymmetric dualencoder works better than the shared-encoder architecture in our preliminary study. For efficiency, all passages in P are encoded offline, and an efficient nearest neighbour search (Johnson et al., 2017) is employed to fetch top-k relevant passages."], "score": 0.734375}, {"id": "(Johnson et al., 2017)", "paper": {"corpus_id": 926364, "title": "Billion-Scale Similarity Search with GPUs", "year": 2017, "venue": "IEEE Transactions on Big Data", "authors": [{"name": "Jeff Johnson", "authorId": "2115354049"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}], "n_citations": 3738}, "snippets": ["Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."], "score": 0.0}, {"id": "(Jiang et al._1, 2023)", "paper": {"corpus_id": 265457282, "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fan Jiang", "authorId": "51511290"}, {"name": "Tom Drummond", "authorId": "2268408815"}, {"name": "Trevor Cohn", "authorId": "2256997782"}], "n_citations": 2}, "snippets": ["In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings", "The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages."], "score": 0.7822265625}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Hofstatter et al., 2021)", "paper": {"corpus_id": 233231706, "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Sebastian Hofst\u00e4tter", "authorId": "97393346"}, {"name": "Sheng-Chieh Lin", "authorId": "122045993"}, {"name": "Jheng-Hong Yang", "authorId": "2109723027"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}, {"name": "A. Hanbury", "authorId": "1699657"}], "n_citations": 402}, "snippets": ["A vital step towards the widespread adoption of neural retrieval models is their resource efficiency throughout the training, indexing and query workflows. The neural IR community made great advancements in training effective dual-encoder dense retrieval (DR) models recently. A dense text retrieval model uses a single vector representation per query and passage to score a match, which enables low-latency first-stage retrieval with a nearest neighbor search. Increasingly common, training approaches require enormous compute power, as they either conduct negative passage sampling out of a continuously updating refreshing index or require very large batch sizes. Instead of relying on more compute capability, we introduce an efficient topic-aware query and balanced margin sampling technique, called TAS-Balanced. We cluster queries once before training and sample queries out of a cluster per batch. We train our lightweight 6-layer DR model with a novel dual-teacher supervision that combines pairwise and in-batch negative teachers. Our method is trainable on a single consumer-grade GPU in under 48 hours. We show that our TAS-Balanced training method achieves state-of-the-art low-latency (64ms per query) results on two TREC Deep Learning Track query sets. Evaluated on NDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by 11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces the first dense retriever that outperforms every other method on recall at any cutoff on TREC-DL and allows more resource intensive re-ranking models to operate on fewer passages to improve results further."], "score": 0.0}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}, {"id": "(Salemi et al., 2023)", "paper": {"corpus_id": 258331649, "title": "A Symmetric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual Question Answering", "year": 2023, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Alireza Salemi", "authorId": "2073044451"}, {"name": "Juan Altmayer Pizzorno", "authorId": "2137765299"}, {"name": "Hamed Zamani", "authorId": "2499986"}], "n_citations": 14}, "snippets": ["A property of this retrieval task is that it deals with asymmetric input modalities: the user information need is multimodal (question-image pair) while the information items (passages) are uni-modal. As a result of this property, Qu et al. (Qu et al., 2021) recently showed that a KI-VQA dense retrieval model that uses a multimodal encoder for representing the question-image pair and a text encoder for representing the passages in the collection leads to state-of-the-art passage retrieval performance. We argue that using such an asymmetric bi-encoder architecture is sub-optimal, since the encoders produce outputs in different semantic spaces and fine-tuning the encoders cannot always close this gap", "Since such asymmetric architectures start from fundamentally different embedding spaces, they suffer from slow convergence speed and sub-optimal dense retrieval performance. Conversely, extensive research on dense retrieval for uni-modal data (textual queries and documents) suggests that symmetric architectures lead to significantly better performance. State-of-the-art dense passage retrieval models, such as TAS-B (Hofst\u00e4tter et al., 2021), ColBERT (Khattab et al., 2020)(Santhanam et al., 2021), RocketQA (Qu et al., 2020)(Ren et al., 2021), and CLDRD (Zeng et al., 2022), use symmetric architectures."], "score": 0.7646484375}, {"id": "(Zhuang et al., 2023)", "paper": {"corpus_id": 258557604, "title": "Augmenting Passage Representations with Query Generation for Enhanced Cross-Lingual Dense Retrieval", "year": 2023, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Shengyao Zhuang", "authorId": "1630489015"}, {"name": "Linjun Shou", "authorId": "24962156"}, {"name": "G. Zuccon", "authorId": "1692855"}], "n_citations": 8}, "snippets": ["These models use a dual-encoder architecture that encodes both queries and passages with a PLM encoder into dense embeddings. They then perform approximate nearest neighbor (ANN) searching in the embedding space. Compared to traditional bag-of-words approaches, DRs benefit from semantic soft matching, which helps overcome the problem of word mismatch in passage retrieval [33]45]."], "score": 0.6689453125}, {"id": "(He et al., 2022)", "paper": {"corpus_id": 254853896, "title": "Curriculum Sampling for Dense Retrieval with Document Expansion", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Anlei Dong", "authorId": "3300216"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "S. Yiu", "authorId": "145964453"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 3}, "snippets": ["In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."], "score": 0.79345703125}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 260656514, "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiduan Liu", "authorId": "1656672968"}, {"name": "Jiahao Liu", "authorId": "2108421184"}, {"name": "Yang Yang", "authorId": "2152915671"}, {"name": "Jingang Wang", "authorId": "2109593338"}, {"name": "Wei Wu", "authorId": "2118256028"}, {"name": "Dongyan Zhao", "authorId": "144060462"}, {"name": "Rui Yan", "authorId": "144539156"}], "n_citations": 6}, "snippets": ["Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."], "score": 0.84033203125}, {"id": "(He et al._1, 2022)", "paper": {"corpus_id": 253080873, "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Weizhen Qi", "authorId": "15629561"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Bartuer Zhou", "authorId": "2109061043"}, {"name": "Biao Cheng", "authorId": "2055922979"}, {"name": "Sm Yiu", "authorId": "2060901199"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 11}, "snippets": ["Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers", "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."], "score": 0.82861328125}, {"id": "(Wang et al., 2019)", "paper": {"corpus_id": 201307832, "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhiguo Wang", "authorId": "40296541"}, {"name": "Patrick Ng", "authorId": "145878390"}, {"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 244}, "snippets": ["BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models."], "score": 0.0}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 275931943, "title": "Enhanced Retrieval of Long Documents: Leveraging Fine-Grained Block Representations with Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghan Li", "authorId": "2291199148"}, {"name": "\u00c9. Gaussier", "authorId": "2288256282"}, {"name": "Guodong Zhou", "authorId": "2331371334"}], "n_citations": 0}, "snippets": ["Karpukhin et al. (Karpukhin et al., 2020) show that retrieval can be practically implemented and greatly outperform BM25 using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple BERT-based dual-encoder framework. Similarly, Sentence-BERT [18] uses siamese BERT network to derive semantically meaningful sentence embeddings which can be compared using cosine-similarity. These kinds of approaches are called dense retrieval or bi-encoder", "In the late-interaction based approach, ColBERT (Khattab et al., 2020) is also a siamese architecture with a query encoder and a passage encoder, both based on BERT. Each token in the query and passage is encoded into its own vector representation, and the passage token representations are precomputed and stored. During online search, these stored passage token representations are later compared, or \"interacted\", with the query token representations to calculate relevance scores. Though this approach balances effectiveness and efficiency, it relies on bi-directional token representations from BERT and can not be directly applied to current decoder-only LLMs."], "score": 0.6279296875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 267751308, "title": "TriSampler: A Better Negative Sampling Principle for Dense Retrieval", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhen Yang", "authorId": "2149234508"}, {"name": "Zhou Shao", "authorId": "2284682533"}, {"name": "Yuxiao Dong", "authorId": "2243402027"}, {"name": "Jie Tang", "authorId": "2238207092"}], "n_citations": 4}, "snippets": ["The primary paradigm is to model semantic interaction between queries and passages based on the learned representations. Most dense retrieval models leverage the pretrained language models to learn latent semantic representations for both queries and passages. Lee, Chang, and Toutanova (2019) first proposed the dual-encoder retrieval architecture based on BERT, paving the way for a new retrieval approach. In order to model fine-grained semantic interaction between queries and passages, Poly-encoder (Humeau et al. 2019), ColBERT (Khattab and Zaharia 2020), and ME-BERT (Luan et al. 2021) explored multi-representation dual-encoder to enhance retrieval performance."], "score": 0.67626953125}, {"id": "(Rajapakse, 2023)", "paper": {"corpus_id": 259949811, "title": "Dense Passage Retrieval: Architectures and Augmentation Methods", "year": 2023, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "T. Rajapakse", "authorId": "2091044662"}], "n_citations": 7}, "snippets": ["The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries", "While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder."], "score": 0.837890625}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 259203703, "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency", "year": 2023, "venue": "SUSTAINLP", "authors": [{"name": "Yuxuan Wang", "authorId": "2115829412"}, {"name": "Hong Lyu", "authorId": "2220304036"}], "n_citations": 2}, "snippets": ["The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings."], "score": 0.640625}, {"id": "(Sidiropoulos et al., 2024)", "paper": {"corpus_id": 272770506, "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Georgios Sidiropoulos", "authorId": "2267237867"}, {"name": "Evangelos Kanoulas", "authorId": "2314138767"}], "n_citations": 0}, "snippets": ["On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers", "At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings."], "score": 0.66845703125}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 273026177, "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chao-Wei Huang", "authorId": "47396497"}, {"name": "Yun-Nung Chen", "authorId": "2286884447"}], "n_citations": 1}, "snippets": ["In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval."], "score": 0.77197265625}], "table": null}, {"title": "Autoregressive Models for Retrieval", "tldr": "Autoregressive models offer a fundamentally different approach to retrieval by directly generating document identifiers rather than computing similarity between independently encoded query and document vectors. This approach enables deeper query-document interactions through cross-attention mechanisms, potentially overcoming the limited interaction bottleneck of dual-encoder architectures. (9 sources)", "text": "\nAutoregressive models represent a paradigm shift in information retrieval, moving away from the traditional dual-encoder architecture toward a generation-based approach. While dual-encoder models independently encode queries and documents to compute similarity scores, autoregressive search engines directly generate identifiers for relevant documents in the candidate pool <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. This fundamental difference allows autoregressive models to perform deep token-level cross-attention between queries and documents, resulting in more sophisticated interactions than the shallow vector comparisons used in dense retrievers <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nThe surge of interest in autoregressive retrieval models stems from their ability to address a critical limitation of dual-encoder architectures: the independent encoding of queries and documents that leads to limited interactions between them <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. By reformulating retrieval as a sequence generation problem, these models can leverage the powerful token-level interactions inherent in autoregressive language models <Paper corpusId=\"274822530\" paperTitle=\"(Trung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. The generative retrieval paradigm enables end-to-end modeling of document retrieval tasks, with models like GenRet learning to tokenize documents into short discrete representations (docids) via discrete auto-encoding approaches <Paper corpusId=\"274822530\" paperTitle=\"(Trung et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258048596\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\nArchitecturally, these systems often consist of two major components: a retriever model (such as a dual-encoder) and a generative language model or reader model <Paper corpusId=\"272330251\" paperTitle=\"(Monath et al., 2024)\" isShortName></Paper>. The generative component typically operates in an autoregressive fashion, where the model takes both the encoder's representations and previously generated tokens as input when generating text <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>. This architecture enables deeper semantic processing than is possible with the lightweight similarity functions used in dual-encoder models.\n\nIn reranking scenarios, formulating the task as sequence generation allows models to take advantage of the language model's autoregressive capabilities, such as conducting listwise sorting or generating rationales <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. This approach has shown advantages in zero-shot retrieval settings, demonstrating strong generalization abilities that can outperform traditional methods like BM25 without task-specific training <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nDespite these advantages, implementing autoregressive search at the scale of large language models (LLMs) presents significant computational challenges <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. To address efficiency concerns, some approaches have introduced non-autoregressive decoder architectures that are compatible with existing transformer-based language models, allowing for parallel scoring of vocabulary items in a single decode step <Paper corpusId=\"258865354\" paperTitle=\"(Soares et al., 2023)\" isShortName></Paper>. This makes document indexing with these models approximately as efficient as indexing with document encoders used in dual-encoder retrieval systems.\n\nThe historical progression from term-matching techniques like TF-IDF and BM25 to dense retrievers with dual-encoder architectures, and now to autoregressive generation models, represents an evolution toward more sophisticated semantic processing in information retrieval <Paper corpusId=\"268889861\" paperTitle=\"(Abdi et al., 2024)\" isShortName></Paper>. As research continues to advance, these autoregressive approaches offer promising directions for overcoming the limitations of previous retrieval paradigms while maintaining practical efficiency.", "citations": [{"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."], "score": 0.81787109375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268031876, "title": "Generative Retrieval with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ye Wang", "authorId": "2185022832"}, {"name": "Xinrun Xu", "authorId": "2290204960"}, {"name": "Rui Xie", "authorId": "2143721734"}, {"name": "Wenxin Hu", "authorId": "2288018918"}, {"name": "Wei Ye", "authorId": "2052980435"}], "n_citations": 1}, "snippets": ["Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."], "score": 0.70654296875}, {"id": "(Trung et al., 2024)", "paper": {"corpus_id": 274822530, "title": "Adaptive Two-Phase Finetuning LLMs for Japanese Legal Text Retrieval", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Quang Hoang Trung", "authorId": "2335861500"}, {"name": "Nguyen Van Hoang Phuc", "authorId": "2335861321"}, {"name": "Le Trung Hoang", "authorId": "2335859619"}, {"name": "Quang Huu Hieu", "authorId": "2281214899"}, {"name": "Vo Nguyen Le Duy", "authorId": "113438599"}], "n_citations": 0}, "snippets": ["The dual-encoder approach involves two backbone language models, typically transformer encoder models or, more recently, Large Language Models (LLMs). One model is responsible for encoding queries, while the other encodes documents. This method maps both queries and documents into a shared vector space, where the inner product of their respective embeddings serves as an efficient similarity measure. Dual-encoders are highly scalable for large datasets due to two key mechanisms: (1) sharing weights among targets via a parametric encoder, and (2) utilizing a computationally efficient scoring function based on inner products (Monath et al., 2023)(Fu et al., 2023)", "Generative retrieval is an emerging paradigm in text retrieval that utilizes generative models to directly produce relevant document identifiers (docids) or content for a given query GENRET (Sun et al., 2023), DSI (Tay et al., 2022), DSI-QG (Zhuang et al., 2022). Unlike traditional retrieval methods (such as sparse or dense retrieval ) that rely on pre-encoded document embeddings and matching them with queries via similarity measures, generative retrieval models treat the task as a sequence generation problem. These models are capable of generating document identifiers or text based on the input query by leveraging large language models (LLMs) or autoregressive language models.The key innovation of generative retrieval lies in its end-to-end nature, where the model generates a ranked list of results directly, without the need for explicit document indexing or vector search."], "score": 0.65087890625}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 258048596, "title": "Learning to Tokenize for Generative Retrieval", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Zheng Chen", "authorId": "2117203270"}, {"name": "Shuaiqiang Wang", "authorId": "2386396"}, {"name": "Haichao Zhu", "authorId": "2387872"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Dawei Yin", "authorId": "2136400100"}, {"name": "M. de Rijke", "authorId": "1696030"}, {"name": "Z. Ren", "authorId": "2780667"}], "n_citations": 75}, "snippets": ["Conventional document retrieval techniques are mainly based on the index-retrieve paradigm. It is challenging to optimize pipelines based on this paradigm in an end-to-end manner. As an alternative, generative retrieval represents documents as identifiers (docid) and retrieves documents by generating docids, enabling end-to-end modeling of document retrieval tasks. However, it is an open question how one should define the document identifiers. Current approaches to the task of defining document identifiers rely on fixed rule-based docids, such as the title of a document or the result of clustering BERT embeddings, which often fail to capture the complete semantic information of a document. We propose GenRet, a document tokenization learning method to address the challenge of defining document identifiers for generative retrieval. GenRet learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. Three components are included in GenRet: (i) a tokenization model that produces docids for documents; (ii) a reconstruction model that learns to reconstruct a document based on a docid; and (iii) a sequence-to-sequence retrieval model that generates relevant document identifiers directly for a designated query. By using an auto-encoding framework, GenRet learns semantic docids in a fully end-to-end manner. We also develop a progressive training scheme to capture the autoregressive nature of docids and to stabilize training. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets to assess the effectiveness of GenRet. GenRet establishes the new state-of-the-art on the NQ320K dataset. Especially, compared to generative retrieval baselines, GenRet can achieve significant improvements on the unseen documents. GenRet also outperforms comparable baselines on MS MARCO and BEIR, demonstrating the method's generalizability."], "score": 0.0}, {"id": "(Monath et al., 2024)", "paper": {"corpus_id": 272330251, "title": "A Fresh Take on Stale Embeddings: Improving Dense Retriever Training with Corrector Networks", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nicholas Monath", "authorId": "2121348263"}, {"name": "Will Sussman Grathwohl", "authorId": "2319130233"}, {"name": "Michael Boratko", "authorId": "51020741"}, {"name": "Rob Fergus", "authorId": "2300098510"}, {"name": "Andrew McCallum", "authorId": "2286335051"}, {"name": "M. Zaheer", "authorId": "1771307"}], "n_citations": 0}, "snippets": ["Retrieval augmented language models (RLMs) typically consist of two major architectural components, a retriever model (e.g., a dual-encoder) and a generative language model or reader model (Guu et al., 2020)Izacard & Grave, 2021;Izacard et al., 2022)", "P (a|y, x) is an autoregressive language model. P (y|x) is computed by the softmax with logits from Equation 2 using the encoder models f (x) and g(y)."], "score": 0.6982421875}, {"id": "(Ma et al., 2022)", "paper": {"corpus_id": 248366563, "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Xinyu Ma", "authorId": "121875983"}, {"name": "J. Guo", "authorId": "1777025"}, {"name": "Ruqing Zhang", "authorId": "2109960367"}, {"name": "Yixing Fan", "authorId": "7888704"}, {"name": "Xueqi Cheng", "authorId": "1717004"}], "n_citations": 53}, "snippets": ["Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."], "score": 0.7001953125}, {"id": "(Yoon et al., 2024)", "paper": {"corpus_id": 267938301, "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Soyoung Yoon", "authorId": "2287336807"}, {"name": "Eunbi Choi", "authorId": "2287970016"}, {"name": "Jiyeon Kim", "authorId": "2287064006"}, {"name": "Yireun Kim", "authorId": "2181032855"}, {"name": "Hyeongu Yun", "authorId": "2286896884"}, {"name": "Seung-won Hwang", "authorId": "2287694374"}], "n_citations": 16}, "snippets": ["In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities."], "score": 0.72216796875}, {"id": "(Soares et al., 2023)", "paper": {"corpus_id": 258865354, "title": "NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Livio Baldini Soares", "authorId": "7353832"}, {"name": "D. Gillick", "authorId": "2396669"}, {"name": "Jeremy R. Cole", "authorId": "30859623"}, {"name": "T. Kwiatkowski", "authorId": "15652489"}], "n_citations": 1}, "snippets": ["To overcome this challenge, we introduce a novel use of nonautoregressive decoder architecture that is compatible with existing Transfomer-based language models (whether Encoder-Decoder or Decoder-only [2]). It allows the model, in a single decode step, to score all vocabulary items in parallel. This makes document indexing with our model approximately as expensive as indexing with document encoders used in recent dual-encoder retrieval systems [6,14]26]."], "score": 0.64794921875}, {"id": "(Abdi et al., 2024)", "paper": {"corpus_id": 268889861, "title": "NL2KQL: From Natural Language to Kusto Query", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Amir H. Abdi", "authorId": "2294879238"}, {"name": "Xinye Tang", "authorId": "2295304070"}, {"name": "Jeremias Eichelbaum", "authorId": "50821288"}, {"name": "Mahan Das", "authorId": "2294847106"}, {"name": "Alex Klein", "authorId": "2294855455"}, {"name": "Nihal Irmak Pakis", "authorId": "2294876696"}, {"name": "William Blum", "authorId": "2294877213"}, {"name": "Daniel L Mace", "authorId": "2294875530"}, {"name": "Tanvi Raja", "authorId": "2294879377"}, {"name": "Namrata Padmanabhan", "authorId": "2294871555"}, {"name": "Ye Xing", "authorId": "2294870992"}], "n_citations": 2}, "snippets": ["Historical and recent studies have demonstrated that retrieval mechanisms to enhance performance of auto-regressive models, including question answering (Chen et al., 2017)(Kwiatkowski et al., 2019)(Voorhees, 2001), fact-checking [28], dialogue systems (Dinan et al., 2018), and citation recommendation [2]. Initially, retrieval was predominantly conducted through term-matching techniques like TF-IDF and BM25 [5]. The advent of neural networks ushered the era of dense retrievers, adopting dual-encoder architectures [13,(Shen et al., 2014)(Yih et al., 2011). Notable advancements include DPR, aimed at distinguishing relevant passages among non-relevant ones, and its enhancements like ANCE which refine the process of mining hard negatives [15]36]."], "score": 0.73681640625}], "table": null}, {"title": "Architectural Differences", "tldr": "Dual-encoder architectures and autoregressive models represent fundamentally different approaches to retrieval, with dual-encoders encoding queries and documents independently while autoregressive models generate document identifiers using cross-attention mechanisms. The key architectural distinction lies in how query-document interactions are processed, affecting the depth of semantic understanding and computational efficiency. (11 sources)", "text": "\nThe architectural differences between dual-encoder models and autoregressive language models for retrieval reflect fundamentally different approaches to the retrieval task. The most significant distinction is in how these architectures process and model interactions between queries and documents.\n\nDual-encoder architectures, as their name suggests, utilize two separate encoders that independently encode queries and documents into dense vector representations <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. After encoding, these models calculate relevance scores using lightweight similarity functions such as dot product or cosine similarity <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper>. This independent encoding approach, while computationally efficient, creates a fundamental limitation: it only permits shallow interactions between queries and documents, leading to information loss that can compromise retrieval accuracy <Paper corpusId=\"248227479\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>.\n\nIn contrast, autoregressive models for retrieval adopt a generation-based paradigm that directly produces document identifiers rather than computing similarity scores between independently encoded vectors <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Unlike dual-encoders, these models employ deep token-level cross-attention mechanisms that allow for much richer interactions between queries and documents <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268031876\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. When generating text in an autoregressive fashion, the model considers both the encoder's representations and previously generated tokens as input, enabling more sophisticated semantic processing <Paper corpusId=\"248366563\" paperTitle=\"(Ma et al., 2022)\" isShortName></Paper>.\n\nCross-encoder architectures represent a middle ground that processes query-document pairs jointly, providing full interaction through attention mechanisms <Paper corpusId=\"254564747\" paperTitle=\"(Sun et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. However, these models face scalability issues as they cannot pre-compute document representations, making them impractical for direct retrieval from large collections <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Autoregressive models adapt this cross-attention capability while reformulating retrieval as a sequence generation problem.\n\nThe computational architecture also differs significantly between these approaches. Dual-encoders allow for offline pre-computation of document representations, enabling efficient retrieval through approximate nearest neighbor search at query time <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Autoregressive models, while potentially more expressive, face greater computational challenges due to their sequential processing nature <Paper corpusId=\"267938301\" paperTitle=\"(Yoon et al., 2024)\" isShortName></Paper>. Some recent approaches have introduced non-autoregressive decoder architectures to address these efficiency concerns while maintaining the benefits of deeper query-document interactions <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.\n\nAnother key architectural difference emerges in how these models handle complex queries. Reranking models, which include both cross-encoders and generative autoregressive models, demonstrate superior performance for simple queries by capturing fine-grained relevance signals. However, their performance declines more sharply than retrieval models as query complexity increases <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the architectural differences between these approaches create distinct trade-offs that vary depending on query characteristics.\n\nThe computational cost implications of these architectural differences are substantial. While dual-encoder models like DPR significantly increase latency compared to traditional methods like BM25 (from 36ms to 293ms), cross-encoder models further increase computational demands, with models like ColBERT requiring approximately 13 times the latency of BM25 <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. Autoregressive models, especially those scaled to the size of large language models, face even greater computational challenges that can make practical implementation difficult <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Sun et al., 2022)", "paper": {"corpus_id": 254564747, "title": "LEAD: Liberal Feature-based Distillation for Dense Retrieval", "year": 2022, "venue": "Web Search and Data Mining", "authors": [{"name": "Hao-Lun Sun", "authorId": "2118181226"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Anlei Dong", "authorId": "3300216"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Jing Lu", "authorId": "2115404379"}, {"name": "Yan Zhang", "authorId": "2152822477"}, {"name": "Daxin Jiang", "authorId": "71790825"}, {"name": "Linjun Yang", "authorId": "7866194"}, {"name": "Rangan Majumder", "authorId": "32431940"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 2}, "snippets": ["Dual Encoder (DE) (Karpukhin et al., 2020) is the most widely used dense retrieval architecture, which encodes queries and passages into dense vectors separately, calculating the relevance score through the inner product. For DE,  1 is the query encoder and  2 is the passage encoder. Both of them are Transformer encoders. The similarity calculation function  DE (\u2022) is defined as: \n\nColBERT (CB) (Khattab et al., 2020) can be viewed as a more expressive dualencoder, which delays the interaction between query and passage after encoding. The instantiation of  1 and  2 is the same as DE. \n\nBut the similarity calculation function  CB (\u2022) is defined as: \n\nwhere  and  denote the length of the query and passage token sequence, respectively. Please note that, following [16], we remove the punctuation filter and the last linear layer of the encoders to focus on distillation. \n\nCross Encoder (CE) (Qu et al., 2020) has strong abilities to capture the fine-grained relationships between queries and passages within the Transformer encoding. Much different from DE and CB, for CE,  1 is the query-passage pair encoder  CE and  2 is the projection layer  after the Transformer encoder, which is used in a shared manner. The similarity calculation function  CE (\u2022) is defined as: \n\nwhere [; ] is the concatenation operation."], "score": 0.66748046875}, {"id": "(He et al., 2022)", "paper": {"corpus_id": 254853896, "title": "Curriculum Sampling for Dense Retrieval with Document Expansion", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Anlei Dong", "authorId": "3300216"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "S. Yiu", "authorId": "145964453"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 3}, "snippets": ["In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."], "score": 0.79345703125}, {"id": "(Ma et al., 2022)", "paper": {"corpus_id": 248366563, "title": "Pre-train a Discriminative Text Encoder for Dense Retrieval via Contrastive Span Prediction", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Xinyu Ma", "authorId": "121875983"}, {"name": "J. Guo", "authorId": "1777025"}, {"name": "Ruqing Zhang", "authorId": "2109960367"}, {"name": "Yixing Fan", "authorId": "7888704"}, {"name": "Xueqi Cheng", "authorId": "1717004"}], "n_citations": 53}, "snippets": ["Dense retrieval usually utilizes a Siamese or bi-encoder architecture to encode queries and documents into low-dimensional representations to abstract their semantic information [18,19,21,38,40,41]. With the learned representations, a dot-product or cosine function is conducted to measure the similarity between queries and documents.\n\nTo boost the dense retrieval performance, recent studies begin to focus on the autoencoder-based language models, which are inspired by the information bottleneck [37] to force the encoder to provide better text representations [25]29]. As shown in Figure 1 (a), these methods pair a decoder on top of the encoder and then train the decoder to reconstruct the input texts solely from the representations given by the encoder. When generating text in the autoregressive fashion, the model takes not only the encoder's encodings but also the previous tokens as input."], "score": 0.7001953125}, {"id": "(Lin et al., 2022)", "paper": {"corpus_id": 248227479, "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bill Yuchen Lin", "authorId": "51583409"}, {"name": "Kangmin Tan", "authorId": "2162783506"}, {"name": "Chris Miller", "authorId": "2111522090"}, {"name": "Beiwen Tian", "authorId": "2143694337"}, {"name": "Xiang Ren", "authorId": "1384550891"}], "n_citations": 49}, "snippets": ["Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4)."], "score": 0.76220703125}, {"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."], "score": 0.81787109375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 268031876, "title": "Generative Retrieval with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ye Wang", "authorId": "2185022832"}, {"name": "Xinrun Xu", "authorId": "2290204960"}, {"name": "Rui Xie", "authorId": "2143721734"}, {"name": "Wenxin Hu", "authorId": "2288018918"}, {"name": "Wei Ye", "authorId": "2052980435"}], "n_citations": 1}, "snippets": ["Traditional methods of obtaining reference include sparse and dense retrieval. Sparse retrieval, using TF-IDF and BM25, matches questions and passages (Robertson et al., 2009)(Chen et al., 2017)(Yang et al., 2019). Recent approaches, such as ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020), employ dense context vectors for passage indexing to enhance performance. However, in dual-encoder dense retrieval models, the representations of questions and passages are obtained independently, leading to performance limitations due to shallow vector interactions (Khattab and Zaharia, 2020). \n\nInterest has surged in using autoregressive language models to generate identifiers to simplify the retrieval process and address the bottleneck of limited interactions in dual-encoder models."], "score": 0.70654296875}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 260656514, "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiduan Liu", "authorId": "1656672968"}, {"name": "Jiahao Liu", "authorId": "2108421184"}, {"name": "Yang Yang", "authorId": "2152915671"}, {"name": "Jingang Wang", "authorId": "2109593338"}, {"name": "Wei Wu", "authorId": "2118256028"}, {"name": "Dongyan Zhao", "authorId": "144060462"}, {"name": "Rui Yan", "authorId": "144539156"}], "n_citations": 6}, "snippets": ["Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."], "score": 0.84033203125}, {"id": "(Yoon et al., 2024)", "paper": {"corpus_id": 267938301, "title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Soyoung Yoon", "authorId": "2287336807"}, {"name": "Eunbi Choi", "authorId": "2287970016"}, {"name": "Jiyeon Kim", "authorId": "2287064006"}, {"name": "Yireun Kim", "authorId": "2181032855"}, {"name": "Hyeongu Yun", "authorId": "2286896884"}, {"name": "Seung-won Hwang", "authorId": "2287694374"}], "n_citations": 16}, "snippets": ["In the reranking scenario, rather than dual encoder models (Karpukhin et al., 2020) which separately encode query and passage information, models that see query and passage information jointly at inference time (Reimers and Gurevych, 2019;(Nogueira et al., 2020) are shown to be effective for zero-shot retrieval (Rosa et al., 2022). Among those, formulating reranking as sequence generation, such as conducting listwise sorting (Ma et al., 2023;Sun et al., 2023b;Pradeep et al., 2023a) or generating rationales (Ferraretto et al., 2023), has shown an advantage in application to zero-shot retrieval by leveraging the language model's auto-regressive generation capabilities."], "score": 0.72216796875}, {"id": "(Lu et al., 2025)", "paper": {"corpus_id": 276928453, "title": "MultiConIR: Towards multi-condition Information Retrieval", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xuan Lu", "authorId": "2349551985"}, {"name": "Sifan Liu", "authorId": "2349802894"}, {"name": "Bochao Yin", "authorId": "2349465013"}, {"name": "Yongqi Li", "authorId": "2349749730"}, {"name": "Xinghao Chen", "authorId": "2325183464"}, {"name": "Hui Su", "authorId": "2349762405"}, {"name": "Yaohui Jin", "authorId": "2349879907"}, {"name": "Wenjun Zeng", "authorId": "2349955598"}, {"name": "Xiaoyu Shen", "authorId": "2287612923"}], "n_citations": 0}, "snippets": ["Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query", "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format", "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."], "score": 0.673828125}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 266573365, "title": "A Multi-level Distillation based Dense Passage Retrieval Model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Haifeng Li", "authorId": "2276748536"}, {"name": "Mo Hai", "authorId": "2157052"}, {"name": "Dong Tang", "authorId": "2276610303"}], "n_citations": 1}, "snippets": ["The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."], "score": 0.841796875}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}], "table": null}, {"title": "Retrieval Accuracy Comparison", "tldr": "While dual-encoder models offer computational efficiency, they consistently underperform compared to cross-encoder and autoregressive models in retrieval accuracy due to their limited query-document interactions. Autoregressive models achieve superior accuracy through deep token-level cross-attention, though this advantage diminishes as query complexity increases. (9 sources)", "text": "\nWhen comparing the retrieval accuracy of dual-encoder architectures and autoregressive models, a clear performance hierarchy emerges. Dual-encoder models, despite their widespread adoption in real-world applications, consistently demonstrate suboptimal retrieval performance compared to both cross-encoder and autoregressive approaches <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>.\n\nThe primary factor limiting dual-encoder accuracy is the independent encoding of queries and documents, which prevents fine-grained interactions between them. This independent processing leads to information loss that distorts similarity calculations and ultimately compromises retrieval performance <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>. The weakness of this architecture is particularly evident when handling nuanced semantic relationships that require deeper token-to-token attention across query-document pairs <Paper corpusId=\"248227479\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>.\n\nCross-encoder models, which take the concatenation of queries and documents as input, significantly outperform dual-encoders in retrieval accuracy by capturing deep correlations through full attention mechanisms <Paper corpusId=\"254853896\" paperTitle=\"(He et al., 2022)\" isShortName></Paper>. This improved performance comes from their ability to model token-level interactions that dual-encoders simply cannot capture <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"201307832\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nAutoregressive search engines push this advantage further by performing deep token-level cross-attention during the generation of document identifiers. This approach results in superior retrieval accuracy compared to the shallow interactions in dense retrievers <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. Notably, these models have demonstrated strong generalization abilities, outperforming traditional methods like BM25 even in zero-shot settings without task-specific training <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>.\n\nHowever, the accuracy advantage of both cross-encoder and autoregressive models is context-dependent. While these reranking-oriented architectures excel at capturing fine-grained relevance signals for simple, single-condition queries, their performance declines more sharply than dual-encoder retrieval models as query complexity increases <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the deeper interaction mechanisms that provide accuracy advantages in straightforward retrieval scenarios may become less effective when handling complex, multi-condition queries.\n\nDespite their accuracy limitations, dual-encoder models remain competitive in practical settings, particularly when combined with additional components. Two-stage retrieval pipelines that use dual-encoders for initial retrieval followed by cross-encoder reranking can achieve superior retrieval quality compared to single-stage approaches <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>. Some researchers have also proposed modifications to the dual-encoder architecture, such as using multiple representations to rerank passages without requiring a separate reranking model <Paper corpusId=\"259949811\" paperTitle=\"(Rajapakse, 2023)\" isShortName></Paper>.\n\nThe retrieval accuracy trade-offs between these architectural approaches highlight the continuing challenge in information retrieval: balancing the superior semantic understanding capabilities of cross-attention mechanisms with the practical efficiency requirements of real-world applications.", "citations": [{"id": "(Liu et al., 2022)", "paper": {"corpus_id": 260656514, "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiduan Liu", "authorId": "1656672968"}, {"name": "Jiahao Liu", "authorId": "2108421184"}, {"name": "Yang Yang", "authorId": "2152915671"}, {"name": "Jingang Wang", "authorId": "2109593338"}, {"name": "Wei Wu", "authorId": "2118256028"}, {"name": "Dongyan Zhao", "authorId": "144060462"}, {"name": "Rui Yan", "authorId": "144539156"}], "n_citations": 6}, "snippets": ["Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."], "score": 0.84033203125}, {"id": "(He et al., 2022)", "paper": {"corpus_id": 254853896, "title": "Curriculum Sampling for Dense Retrieval with Document Expansion", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Anlei Dong", "authorId": "3300216"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "S. Yiu", "authorId": "145964453"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 3}, "snippets": ["In recent years, the dual-encoder architecture has been a standard workhorse for dense retrieval. One major disadvantage of this architecture is that it can only partially extract the interactions between the query and document, since it encodes them separately. By comparison, the cross-encoder architecture can effectively capture the deep correlation between them by taking the concatenation of the query and document as input. By directly concatenating the query and document, the cross-encoder gains an advantage in capturing interactions, but also loses the advantage of pre-computing document representations during inference. Therefore, cross-encoder cannot wholly replace dual-encoder."], "score": 0.79345703125}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 266573365, "title": "A Multi-level Distillation based Dense Passage Retrieval Model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Haifeng Li", "authorId": "2276748536"}, {"name": "Mo Hai", "authorId": "2157052"}, {"name": "Dong Tang", "authorId": "2276610303"}], "n_citations": 1}, "snippets": ["The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."], "score": 0.841796875}, {"id": "(He et al._1, 2022)", "paper": {"corpus_id": 253080873, "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Weizhen Qi", "authorId": "15629561"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Bartuer Zhou", "authorId": "2109061043"}, {"name": "Biao Cheng", "authorId": "2055922979"}, {"name": "Sm Yiu", "authorId": "2060901199"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 11}, "snippets": ["Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers", "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."], "score": 0.82861328125}, {"id": "(Lin et al., 2022)", "paper": {"corpus_id": 248227479, "title": "Unsupervised Cross-Task Generalization via Retrieval Augmentation", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bill Yuchen Lin", "authorId": "51583409"}, {"name": "Kangmin Tan", "authorId": "2162783506"}, {"name": "Chris Miller", "authorId": "2111522090"}, {"name": "Beiwen Tian", "authorId": "2143694337"}, {"name": "Xiang Ren", "authorId": "1384550891"}], "n_citations": 49}, "snippets": ["Weakness of the dense retrieval. Although dense retrieval is very efficient thanks to the MIPS support, the retrieval performance is limited by its two major weakness. First, it is a dual-encoder architecture that encodes the candidate example and the query example separately, which ignores informative features behind token-to-token attention across a pair of examples. Second, it is too costly to frequently update the example encoder, which prevents us from learning to refine the retrieval results with distant supervision (if any). Therefore, we design a re-ranking stage where we train a cross-encoder to further enhance the dense-retrieval results with mined distant supervision (Sec. 3.4)."], "score": 0.76220703125}, {"id": "(Wang et al., 2019)", "paper": {"corpus_id": 201307832, "title": "Multi-passage BERT: A Globally Normalized BERT Model for Open-domain Question Answering", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhiguo Wang", "authorId": "40296541"}, {"name": "Patrick Ng", "authorId": "145878390"}, {"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 244}, "snippets": ["BERT model has been successfully applied to open-domain QA tasks. However, previous work trains BERT by viewing passages corresponding to the same question as independent training instances, which may cause incomparable scores for answers from different passages. To tackle this issue, we propose a multi-passage BERT model to globally normalize answer scores across all passages of the same question, and this change enables our QA model find better answers by utilizing more passages. In addition, we find that splitting articles into passages with the length of 100 words by sliding window improves performance by 4%. By leveraging a passage ranker to select high-quality passages, multi-passage BERT gains additional 2%. Experiments on four standard benchmarks showed that our multi-passage BERT outperforms all state-of-the-art models on all benchmarks. In particular, on the OpenSQuAD dataset, our model gains 21.4% EM and 21.5% F1 over all non-BERT models, and 5.8% EM and 6.5% F1 over BERT-based models."], "score": 0.0}, {"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."], "score": 0.81787109375}, {"id": "(Lu et al., 2025)", "paper": {"corpus_id": 276928453, "title": "MultiConIR: Towards multi-condition Information Retrieval", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xuan Lu", "authorId": "2349551985"}, {"name": "Sifan Liu", "authorId": "2349802894"}, {"name": "Bochao Yin", "authorId": "2349465013"}, {"name": "Yongqi Li", "authorId": "2349749730"}, {"name": "Xinghao Chen", "authorId": "2325183464"}, {"name": "Hui Su", "authorId": "2349762405"}, {"name": "Yaohui Jin", "authorId": "2349879907"}, {"name": "Wenjun Zeng", "authorId": "2349955598"}, {"name": "Xiaoyu Shen", "authorId": "2287612923"}], "n_citations": 0}, "snippets": ["Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query", "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format", "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."], "score": 0.673828125}, {"id": "(Rajapakse, 2023)", "paper": {"corpus_id": 259949811, "title": "Dense Passage Retrieval: Architectures and Augmentation Methods", "year": 2023, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "T. Rajapakse", "authorId": "2091044662"}], "n_citations": 7}, "snippets": ["The dual-encoder model is a dense retrieval architecture, consisting of two encoder models, that has surpassed traditional sparse retrieval methods for open-domain retrieval [1]. But, room exists for improvement, particularly when dense retrievers are exposed to unseen passages or queries", "While dual encoder models can surpass traditional sparse retrieval methods, they lag behind two stage retrieval pipelines in retrieval quality. I propose a modification to the dual encoder model where a second representation is used to rerank the passages retrieved using the first representation. Here, a second stage model is not required and both representations are generated in a single forward pass from the dual encoder."], "score": 0.837890625}], "table": null}, {"title": "Computational Efficiency Comparison", "tldr": "Dual-encoder architectures offer superior computational efficiency by allowing offline pre-computation of document embeddings and fast retrieval through approximate nearest neighbor search, while autoregressive models provide better retrieval accuracy at the cost of significantly higher computational demands. (13 sources)", "text": "\nThe computational efficiency differences between dual-encoder and autoregressive models represent one of the most significant trade-offs in information retrieval system design. Dual-encoder architectures have become the de facto standard for practical retrieval systems primarily due to their computational advantages <Paper corpusId=\"259203703\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>.\n\nThe fundamental efficiency advantage of dual-encoder models stems from their ability to decouple the encoding process. Since queries and documents are encoded independently, document representations can be pre-computed and indexed offline <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This design allows systems to compute embeddings for millions of documents in advance and build efficient indexing structures, significantly reducing the computational burden during retrieval <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"215737187\" paperTitle=\"(Karpukhin et al., 2020)\" isShortName></Paper>.\n\nAt query time, dual-encoder systems only need to encode the new query and conduct an approximate nearest neighbor (ANN) search to locate the most relevant documents <Paper corpusId=\"273026177\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This approach enables efficient retrieval at scale, making it possible to search through millions of documents with reasonable latency <Paper corpusId=\"265457282\" paperTitle=\"(Jiang et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"926364\" paperTitle=\"(Johnson et al., 2017)\" isShortName></Paper>. The computational efficiency of dual-encoders is particularly valuable in latency-sensitive applications, making them widely adopted in real-world deployment scenarios <Paper corpusId=\"253157959\" paperTitle=\"(Long et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.\n\nIn contrast, both cross-encoder and autoregressive models face significant computational challenges. Cross-encoders, which jointly process query-document pairs using full attention mechanisms, require recomputing representations for each document whenever a new query arrives <Paper corpusId=\"260656514\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This makes them impractical for direct retrieval from large document collections, relegating them to reranking a smaller set of candidate documents retrieved by more efficient methods <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper> <Paper corpusId=\"272770506\" paperTitle=\"(Sidiropoulos et al., 2024)\" isShortName></Paper>.\n\nAutoregressive search engines face even greater computational demands. While these models offer superior retrieval accuracy through deep token-level cross-attention, scaling them to the size of large language models creates prohibitive computational overhead <Paper corpusId=\"258714822\" paperTitle=\"(Ziems et al., 2023)\" isShortName></Paper>. The sequential generation process inherent in autoregressive models further limits their computational efficiency compared to the parallelizable nature of dual-encoder approaches.\n\nThe practical implications of these efficiency differences are substantial. Experimental results show that compared to traditional BM25 retrieval with a latency of 36 milliseconds, dual-encoder models like DPR increase latency to 293 milliseconds (an 8-fold increase), while cross-encoder models like ColBERT further increase latency to 458 milliseconds (nearly 13 times that of BM25) <Paper corpusId=\"266573365\" paperTitle=\"(Li et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"216553223\" paperTitle=\"(Khattab et al., 2020)\" isShortName></Paper>. Autoregressive models would likely show even higher latency, especially when scaled to the size of modern large language models.\n\nThese efficiency considerations have direct implications for system design choices. As query complexity increases, reranking models (including both cross-encoders and autoregressive approaches) show a sharper decline in performance compared to retrieval models <Paper corpusId=\"276928453\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>. This suggests that the computational efficiency advantages of dual-encoder architectures become even more valuable in scenarios involving complex, multi-condition queries.\n\nThe substantial efficiency gap between these architectural approaches explains why most practical information retrieval systems adopt a two-stage pipeline: using efficient dual-encoder models for initial retrieval from large collections, followed by more computationally intensive cross-encoder or autoregressive models to rerank a smaller candidate set <Paper corpusId=\"253080873\" paperTitle=\"(He et al._1, 2022)\" isShortName></Paper>. This hybrid approach attempts to balance the computational efficiency of dual-encoders with the superior accuracy of models that enable deeper query-document interactions.", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 259203703, "title": "Query Encoder Distillation via Embedding Alignment is a Strong Baseline Method to Boost Dense Retriever Online Efficiency", "year": 2023, "venue": "SUSTAINLP", "authors": [{"name": "Yuxuan Wang", "authorId": "2115829412"}, {"name": "Hong Lyu", "authorId": "2220304036"}], "n_citations": 2}, "snippets": ["The information retrieval community has made significant progress in improving the efficiency of Dual Encoder (DE) dense passage retrieval systems, making them suitable for latency-sensitive settings."], "score": 0.640625}, {"id": "(Sidiropoulos et al., 2024)", "paper": {"corpus_id": 272770506, "title": "A Multimodal Dense Retrieval Approach for Speech-Based Open-Domain Question Answering", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Georgios Sidiropoulos", "authorId": "2267237867"}, {"name": "Evangelos Kanoulas", "authorId": "2314138767"}], "n_citations": 0}, "snippets": ["On the contrary, even though cross-encoder architectures can achieve higher performance due to jointly encoding questions and passages, they are not indexable and hence are re-rankers", "At this point, we want to highlight that we choose a dual-encoder architecture because it has shown high efficiency as a first-stage ranker in large-scale settings."], "score": 0.66845703125}, {"id": "(He et al._1, 2022)", "paper": {"corpus_id": 253080873, "title": "Metric-guided Distillation: Distilling Knowledge from the Metric to Ranker and Retriever for Generative Commonsense Reasoning", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xingwei He", "authorId": "1754500"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Alex Jin", "authorId": "15796861"}, {"name": "Weizhen Qi", "authorId": "15629561"}, {"name": "Hang Zhang", "authorId": "2119077859"}, {"name": "Jian Jiao", "authorId": "2143968416"}, {"name": "Bartuer Zhou", "authorId": "2109061043"}, {"name": "Biao Cheng", "authorId": "2055922979"}, {"name": "Sm Yiu", "authorId": "2060901199"}, {"name": "Nan Duan", "authorId": "46429989"}], "n_citations": 11}, "snippets": ["Dense passage retrievers are typically based on the dual-encoder architecture, which allows practitioners to compute the representation of each passage in the corpus and built indexes for them in advance. In this way, we only need to calculate the representation for the newly entered query and find the closest passage to the query, thus reducing the retrieval time. \n\nHowever, dual-encoder retrievers model the query and passage independently, thus failing to fully capture the fine-grained interactions between them. To solve this, BERT-based cross-encoder rankers (Wang et al., 2019)Nogueira and Cho, 2019) are used to re-rank the retrieval passages of retrievers", "Although rankers can effectively capture the relationships between the query and passage, the cross-encoder architecture makes it impractical to retrieve directly from the corpus."], "score": 0.82861328125}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 260656514, "title": "GNN-encoder: Learning a Dual-encoder Architecture via Graph Neural Networks for Dense Passage Retrieval", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiduan Liu", "authorId": "1656672968"}, {"name": "Jiahao Liu", "authorId": "2108421184"}, {"name": "Yang Yang", "authorId": "2152915671"}, {"name": "Jingang Wang", "authorId": "2109593338"}, {"name": "Wei Wu", "authorId": "2118256028"}, {"name": "Dongyan Zhao", "authorId": "144060462"}, {"name": "Rui Yan", "authorId": "144539156"}], "n_citations": 6}, "snippets": ["Two paradigms based on fine-tuned language models are typically built for retrieval: crossencoders and dual-encoders. \n\nTypical crossencoders need to recompute the representation of each passage in the corpus once a new query comes, which is difficult to deploy in real-world search systems. In contrast, dual-encoders remove querypassage interaction by representing a query and a passage independently through two separate encoders (Siamese encoders). Hence, passage embeddings can be pre-computed offline, and online latency can be greatly reduced. Thanks to this advantage, dual-encoders are more widely adopted in real-world applications. On the other hand, independent encoding without any interaction causes severe retrieval performance drop due to information loss."], "score": 0.84033203125}, {"id": "(Jiang et al._1, 2023)", "paper": {"corpus_id": 265457282, "title": "Noisy Self-Training with Synthetic Queries for Dense Retrieval", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fan Jiang", "authorId": "51511290"}, {"name": "Tom Drummond", "authorId": "2268408815"}, {"name": "Trevor Cohn", "authorId": "2256997782"}], "n_citations": 2}, "snippets": ["In contrast to traditional IR methods, such as BM25 (Paszke et al., 2019), which represent texts in high dimensional and sparse vectors with inverted index, dense retrieval methods alternatively adopt neural models to encode texts (queries or passages) in dense latent vectors with much smaller dimensions. A dense passage retrieval model (Karpukhin et al., 2020) typically adopts the dual-encoder architecture, where neural models are used to encode the query and passage into dense vectors separately. The relevance is measured by the dot product between their embeddings", "The adoption of this form of 'dual-encoder' architecture decouples the encoding of query and passage. At inference, all passages in P can be encoded offline. When a query q comes in, efficient nearest neighbour search (Johnson et al., 2017) can be performed to fetch the top-k passages."], "score": 0.7822265625}, {"id": "(Karpukhin et al., 2020)", "paper": {"corpus_id": 215737187, "title": "Dense Passage Retrieval for Open-Domain Question Answering", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Barlas O\u011fuz", "authorId": "9185192"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ledell Yu Wu", "authorId": "51183248"}, {"name": "Sergey Edunov", "authorId": "2068070"}, {"name": "Danqi Chen", "authorId": "50536468"}, {"name": "Wen-tau Yih", "authorId": "144105277"}], "n_citations": 3794}, "snippets": ["Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."], "score": 0.0}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 273026177, "title": "PairDistill: Pairwise Relevance Distillation for Dense Retrieval", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chao-Wei Huang", "authorId": "47396497"}, {"name": "Yun-Nung Chen", "authorId": "2286884447"}], "n_citations": 1}, "snippets": ["In order to efficiently retrieve from millions of passages, the most common architecture used for dense retrieval is the dual encoder architecture, where the queries and the passages are encoded by a query encoder and a passage encoder, respectively. We denote the query representation of a query q as q and the passage representation of a passage d as d. This architecture enables offline encoding and indexing of all passages, thus significantly reducing the computation required during retrieval."], "score": 0.77197265625}, {"id": "(Johnson et al., 2017)", "paper": {"corpus_id": 926364, "title": "Billion-Scale Similarity Search with GPUs", "year": 2017, "venue": "IEEE Transactions on Big Data", "authors": [{"name": "Jeff Johnson", "authorId": "2115354049"}, {"name": "Matthijs Douze", "authorId": "3271933"}, {"name": "H. J\u00e9gou", "authorId": "1681054"}], "n_citations": 3738}, "snippets": ["Similarity search finds application in database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data parallel tasks such as distance computation, prior approaches in this domain are bottlenecked by algorithms that expose less parallelism, such as <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq1-2921572.gif\"/></alternatives></inline-formula>-min selection, or make poor use of the memory hierarchy. We propose a novel design for <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq2-2921572.gif\"/></alternatives></inline-formula>-selection. We apply it in different similarity search scenarios, by optimizing brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation operates at up to 55 percent of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5 \u00d7 faster than prior GPU state of the art. It enables the construction of a high accuracy <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"johnson-ieq3-2921572.gif\"/></alternatives></inline-formula>-NN graph on 95 million images from the <sc>Yfcc100M</sc> dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility."], "score": 0.0}, {"id": "(Long et al., 2022)", "paper": {"corpus_id": 253157959, "title": "Retrieval Oriented Masking Pre-training Language Model for Dense Passage Retrieval", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Dingkun Long", "authorId": "8427191"}, {"name": "Yanzhao Zhang", "authorId": "2107949588"}, {"name": "Guangwei Xu", "authorId": "2149131512"}, {"name": "Pengjun Xie", "authorId": "35930962"}], "n_citations": 4}, "snippets": ["To balance efficiency and effectiveness, existing dense passage retrieval methods usually leverage a dual-encoder architecture. Specifically, query and passage are encoded into continuous vector representations by language models (LMs) respectively, then, a score function is applied to estimate the semantic similarity between the query-passage pair."], "score": 0.71728515625}, {"id": "(Ziems et al., 2023)", "paper": {"corpus_id": 258714822, "title": "Large Language Models are Built-in Autoregressive Search Engines", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Noah Ziems", "authorId": "2264184691"}, {"name": "W. Yu", "authorId": "38767143"}, {"name": "Zhihan Zhang", "authorId": "72871419"}, {"name": "Meng Jiang", "authorId": "2152153656"}], "n_citations": 42}, "snippets": ["Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool.\n\nInstead of computing similarity between question and document embeddings, autoregressive search engines aim to directly generate document identifiers then map them to complete documents in the predetermined candidate pool. This approach has attracted increasing interest in information retrieval (IR) and related fields (Tay et al., 2022;Bevilacqua et al., 2022;Wang et al., 2022). Compared to dual-encoder dense retrieval methods, autoregressive search engines enjoy a number of advantages. First, autoregressive generation models produce document identifiers by performing deep token-level cross-attention, resulting in a better esti-mation than shallow interactions in dense retrievers. Second, autoregressive search engines have been shown to have strong generalization abilities, outperforming BM25 in a zero-shot setting (Tay et al., 2022). While it is theoretically possible to scale an autoregressive search engine to the size of a large language model (LLM), such as GPT-3 with 175B parameters, in practice it is not feasible due to the computational overhead of training such a large autoregressive search engine from scratch (Tay et al., 2022)."], "score": 0.81787109375}, {"id": "(Li et al._1, 2023)", "paper": {"corpus_id": 266573365, "title": "A Multi-level Distillation based Dense Passage Retrieval Model", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Haifeng Li", "authorId": "2276748536"}, {"name": "Mo Hai", "authorId": "2157052"}, {"name": "Dong Tang", "authorId": "2276610303"}], "n_citations": 1}, "snippets": ["The dual-encoder model encodes the query and passage separately, resulting in a lack of interaction between them, which can distort the similarity calculation and cause a loss of contextual information, leading to sub-optimal performance. (2) The crossencoder model employs an attention mechanism for full interaction, but this comes at a high computational cost, which is proportional to the square of the text length [16]. While it effectively improves model performance, it also significantly decrease both training and inference computational efficiency. Experimental results from COIL [7] show that using BM25 retrieval has a latency of 36 milliseconds, whereas using the dual-encoder model DPR [14] increases the latency to 293 milliseconds, an 8-fold increase. The use of the crossencoder model ColBert (Khattab et al., 2020) further increases the latency to 458 milliseconds, nearly 13 times of the BM25."], "score": 0.841796875}, {"id": "(Khattab et al., 2020)", "paper": {"corpus_id": 216553223, "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT", "year": 2020, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "O. Khattab", "authorId": "144112155"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 1377}, "snippets": ["Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query."], "score": 0.0}, {"id": "(Lu et al., 2025)", "paper": {"corpus_id": 276928453, "title": "MultiConIR: Towards multi-condition Information Retrieval", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xuan Lu", "authorId": "2349551985"}, {"name": "Sifan Liu", "authorId": "2349802894"}, {"name": "Bochao Yin", "authorId": "2349465013"}, {"name": "Yongqi Li", "authorId": "2349749730"}, {"name": "Xinghao Chen", "authorId": "2325183464"}, {"name": "Hui Su", "authorId": "2349762405"}, {"name": "Yaohui Jin", "authorId": "2349879907"}, {"name": "Wenjun Zeng", "authorId": "2349955598"}, {"name": "Xiaoyu Shen", "authorId": "2287612923"}], "n_citations": 0}, "snippets": ["Retrieval models typically employ a dual-encoder architecture, where queries and documents are independently encoded before computing their similarity using dot-product or cosine similarity. This independent computation ensures that the generation of query and document embeddings remains unaffected by each other. At the same time, bidirectional attention enables the model to better capture the overall semantic meaning of the query", "Reranking models compute relevance by jointly processing the query and document, primarily through: (1) cross-encoders, which perform token-level relevance comparison through crossattention (e.g., bge-reranker-v2-m3), and (2) generative models that estimate relevance using LLMbased agents (e.g., bge-reranker-v2-gemma and FollowIR). Both architectures rely on deep querydocument interaction, making them more sensitive to input complexity, such as changes in condition quantity and query format", "Our experiments show that rerankers outperform retrieval models in ranking effectiveness for singlecondition queries, suggesting their advantage in capturing fine-grained query-document relevance for short and simple queries. However, as query complexity increases, their performance declines more sharply, eventually falling behind retrieval models."], "score": 0.673828125}], "table": null}], "cost": 0.467298}}
