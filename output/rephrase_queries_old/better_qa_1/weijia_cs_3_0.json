{"better_query": "What are the key differences and tradeoffs between parameter optimization, parameter merging, in-context unlearning, and localization-based unlearning methods for removing knowledge from large language models?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Machine unlearning is an emerging field focused on removing specific knowledge from trained models, driven by regulatory guidelines and the need to eliminate harmful content. Various approaches have been developed for large language models (LLMs), including parameter optimization, parameter merging, in-context unlearning, and localization-based methods, each with distinct characteristics and trade-offs. (2 sources)", "text": "\nMachine unlearning has gained significant attention in recent years, particularly in the context of large language models (LLMs), driven by regulatory guidelines such as the \"Right to be Forgotten\" and the need to remove potentially harmful knowledge from deployed models <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. This field focuses on efficiently removing the influence of specific training instances or knowledge from a model without compromising its overall performance on unrelated tasks.\n\nTraditional approaches to unlearning typically require complete model retraining, which is computationally prohibitive for large-scale models like LLMs. This limitation has spurred the development of alternative methods that can approximate the removal of certain knowledge without the computational burden of full retraining <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. These approaches are particularly valuable for LLMs, as they allow for the targeted removal of harmful knowledge while preserving the model's broader capabilities <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nCurrent knowledge unlearning methods for LLMs can be broadly categorized into several approaches: parameter optimization-based methods, which involve updating model weights; parameter merging methods, which combine parameters from different model versions through arithmetic operations; in-context unlearning methods, which operate without parameter updates by providing specific inputs in the context window; and localization-based methods that target specific parts of the model <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper> <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. Each of these approaches offers distinct advantages and limitations, making them suitable for different unlearning scenarios and requirements.", "citations": [{"id": "(Pawelczyk et al., 2023)", "paper": {"corpus_id": 263834631, "title": "In-Context Unlearning: Language Models as Few Shot Unlearners", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Martin Pawelczyk", "authorId": "89583148"}, {"name": "Seth Neel", "authorId": "2273685865"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 132}, "snippets": ["Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters."], "score": 0.654296875}, {"id": "(Si et al., 2023)", "paper": {"corpus_id": 265456592, "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Nianwen Si", "authorId": "73502630"}, {"name": "Hao Zhang", "authorId": "2154930608"}, {"name": "Heyu Chang", "authorId": "2116152318"}, {"name": "Wenlin Zhang", "authorId": "9047584"}, {"name": "Dan Qu", "authorId": "2253591545"}, {"name": "Weiqiang Zhang", "authorId": "2268429659"}], "n_citations": 33}, "snippets": ["Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model", "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."], "score": 0.81982421875}], "table": null}, {"title": "Parameter Optimization Methods", "tldr": "Parameter optimization methods focus on adjusting model weights to selectively unlearn specific knowledge or behaviors, with approaches ranging from gradient-based techniques to knowledge distillation. These methods offer varying trade-offs between unlearning effectiveness, utility preservation, and computational efficiency. (7 sources)", "text": "\nParameter optimization methods form a major category of unlearning approaches that modify model parameters to remove specific knowledge from large language models. These techniques directly adjust model weights by optimizing various objective functions designed to target unwanted information while preserving general model capabilities.\n\nOne of the primary approaches within this category is gradient-based optimization, which can be implemented with or without reversed loss functions. Methods using reversed loss (such as gradient ascent) effectively unlearn targeted knowledge by maximizing the cross-entropy loss for specific samples that need to be forgotten <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This approach has been applied to various unlearning scenarios, including removing harmful responses, erasing copyright-protected content, and eliminating hallucinations <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. However, while effective at unlearning, these methods can inadvertently affect unrelated parameters, compromising the model's overall performance <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nOther parameter optimization techniques include knowledge distillation, which transfers knowledge to a new model while excluding specific data points. This approach maintains locality by preserving essential performance while unlearning undesired knowledge, though it can be resource-intensive and may not achieve optimal accuracy and generalizability <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Knowledge distillation approaches have also been reported to suffer from knowledge relearning issues, where models may reacquire forgotten information during fine-tuning with related or even unrelated samples <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273849875\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nGlobal weight modification approaches\u2014which update all model parameters during unlearning\u2014include techniques like gradient ascent, gradient difference, KL minimization, and weight perturbation <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273901406\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271909642\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>. These methods generally provide strong unlearning guarantees but are resource-intensive for larger models and can face challenges in balancing effective unlearning with preservation of model capabilities <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>.\n\nThe use of extra learnable layers represents another parameter optimization approach, offering highly targeted unlearning but potentially requiring significant computational resources <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Additionally, task vectors and parameter-efficient module operations may perform well in terms of unlearning accuracy, but recent work has highlighted the risk of these approaches leading to model instability and degradation <Paper corpusId=\"271543835\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nA notable challenge across many parameter optimization methods is the inherent tension between forgetting specific information and maintaining general model performance. Methods that optimize for effective unlearning often require careful regularization to mitigate performance loss on unrelated tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. This fundamental trade-off represents one of the key considerations when selecting parameter optimization techniques for unlearning in large language models.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 271543835, "title": "Machine Unlearning in Generative AI: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zheyuan Liu", "authorId": "2122087252"}, {"name": "Guangyao Dou", "authorId": "2174956825"}, {"name": "Zhaoxuan Tan", "authorId": "2093186816"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Meng Jiang", "authorId": "2275403324"}], "n_citations": 19}, "snippets": ["The parameter optimization strategies focus on adjusting specific model parameters to selectively unlearn certain behaviors without affecting other functions. These approaches involve precise alterations to parameters associated with unwanted data influences or biases, ensuring the preservation of essential model performance. Gradient-based approaches with reversed loss are effective for unlearning accuracy and generalizability but can negatively impact model locality by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without reversed loss can maximally preserve locality but may not excel in unlearning accuracy and generalizability. Extra learnable layers provide highly targeted unlearning but may demand significant computational resources. Data sharding methods excel in maintaining locality by partitioning the training data and ensuring specific data points can be unlearned without extensive retraining, although they might struggle with generalizability in very large models. Knowledge distillation is effective in maintaining locality by transferring knowledge to a new model trained to exclude specific data, thus retaining essential performance while unlearning undesired knowledge. However, it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task vector and parameter-efficient module operations may perform well in terms of unlearning accuracy and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches leading to instability due to significant model degradation, resulting in poor locality performance."], "score": 0.6513671875}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 271601132, "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shiji Zhou", "authorId": "2303304464"}, {"name": "Lianzhe Wang", "authorId": "2233281230"}, {"name": "Jiangnan Ye", "authorId": "2277872538"}, {"name": "Yongliang Wu", "authorId": "2314337350"}, {"name": "Heng Chang", "authorId": "2303324907"}], "n_citations": 5}, "snippets": ["Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."], "score": 0.75732421875}, {"id": "(Geng et al., 2025)", "paper": {"corpus_id": 276772996, "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiahui Geng", "authorId": "2266466915"}, {"name": "Qing Li", "authorId": "2295742465"}, {"name": "Herbert Woisetschlaeger", "authorId": "2215923457"}, {"name": "Zongxiong Chen", "authorId": "2109296817"}, {"name": "Yuxia Wang", "authorId": "2241417701"}, {"name": "Preslav Nakov", "authorId": "2026545715"}, {"name": "Hans-Arno Jacobsen", "authorId": "2238271670"}, {"name": "Fakhri Karray", "authorId": "2265969003"}], "n_citations": 5}, "snippets": ["Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."], "score": 0.7509765625}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 273849875, "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yingzi Ma", "authorId": "2269426296"}, {"name": "Jiong Wang", "authorId": "2110170885"}, {"name": "Fei Wang", "authorId": "2306872120"}, {"name": "Siyuan Ma", "authorId": "2295006382"}, {"name": "Jiazhao Li", "authorId": "2286593620"}, {"name": "Xiujun Li", "authorId": "2268736216"}, {"name": "Furong Huang", "authorId": "2329735142"}, {"name": "Lichao Sun", "authorId": "2329521764"}, {"name": "Bo Li", "authorId": "2286730795"}, {"name": "Yejin Choi", "authorId": "2329517334"}, {"name": "Muhao Chen", "authorId": "1998918"}, {"name": "Chaowei Xiao", "authorId": "2256992327"}], "n_citations": 10}, "snippets": ["Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms."], "score": 0.0}, {"id": "(Srivasthav et al., 2025)", "paper": {"corpus_id": 276885223, "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Dinesh P. Srivasthav", "authorId": "2135634919"}, {"name": "B. Garlapati", "authorId": "9266407"}], "n_citations": 0}, "snippets": ["The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024)."], "score": 0.53955078125}, {"id": "(Feng et al., 2024)", "paper": {"corpus_id": 273901406, "title": "Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaohua Feng", "authorId": "2314871557"}, {"name": "Chao-Jun Chen", "authorId": "2251485995"}, {"name": "Yuyuan Li", "authorId": "1527113700"}, {"name": "Zibin Lin", "authorId": "2261893790"}], "n_citations": 10}, "snippets": ["Pre-trained language models acquire knowledge from vast amounts of text data, which can inadvertently contain sensitive information. To mitigate the presence of undesirable knowledge, the task of knowledge unlearning becomes crucial for language models. Previous research relies on gradient ascent methods to achieve knowledge unlearning, which is simple and effective. However, this approach calculates all the gradients of tokens in the sequence, potentially compromising the general ability of language models. To overcome this limitation, we propose an adaptive objective that calculates gradients with fine-grained control specifically targeting sensitive tokens. Our adaptive objective is pluggable, ensuring simplicity and enabling extension to the regularization-based framework that utilizes non-target data or other models to preserve general ability. Through extensive experiments targeting the removal of typical sensitive data, we demonstrate that our proposed method enhances the general ability of language models while achieving knowledge unlearning. Additionally, it demonstrates the capability to adapt to behavior alignment, eliminating all the undesirable knowledge within a specific domain."], "score": 0.5654296875}, {"id": "(Yuan et al., 2024)", "paper": {"corpus_id": 271909642, "title": "Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Hongbang Yuan", "authorId": "2165224410"}, {"name": "Zhuoran Jin", "authorId": "2152843772"}, {"name": "Pengfei Cao", "authorId": "49776272"}, {"name": "Yubo Chen", "authorId": "1763402"}, {"name": "Kang Liu", "authorId": "2284814101"}, {"name": "Jun Zhao", "authorId": "2269147239"}], "n_citations": 9}, "snippets": ["LLM have achieved success in many fields but still troubled by problematic content in the training corpora. LLM unlearning aims at reducing their influence and avoid undesirable behaviours. However, existing unlearning methods remain vulnerable to adversarial queries and the unlearned knowledge resurfaces after the manually designed attack queries. As part of a red-team effort to proactively assess the vulnerabilities of unlearned models, we design Dynamic Unlearning Attack (DUA), a dynamic and automated framework to attack these models and evaluate their robustness. It optimizes adversarial suffixes to reintroduce the unlearned knowledge in various scenarios. We find that unlearned knowledge can be recovered in 55.2% of the questions, even without revealing the unlearned model's parameters. In response to this vulnerability, we propose Latent Adversarial Unlearning (LAU), a universal framework that effectively enhances the robustness of the unlearned process. It formulates the unlearning process as a min-max optimization problem and resolves it through two stages: an attack stage, where perturbation vectors are trained and added to the latent space of LLMs to recover the unlearned knowledge, and a defense stage, where previously trained perturbation vectors are used to enhance unlearned model's robustness. With our LAU framework, we obtain two robust unlearning methods, AdvGA and AdvNPO. We conduct extensive experiments across multiple unlearning benchmarks and various models, and demonstrate that they improve the unlearning effectiveness by over 53.5%, cause only less than a 11.6% reduction in neighboring knowledge, and have almost no impact on the model's general capabilities."], "score": 0.0}], "table": null}, {"title": "Parameter Merging Methods", "tldr": "Parameter merging methods combine parameters from different model versions through simple arithmetic operations without requiring additional training, offering computational efficiency for deployed models. These approaches include task vectors that specify directional changes in weight space, enabling selective knowledge removal through operations like negation and addition. (5 sources)", "text": "\nParameter merging represents a distinct approach to machine unlearning that involves combining parameters from different model versions through arithmetic operations without requiring additional parameter training <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. Unlike parameter optimization methods, these techniques focus on offline composition of model parameters, making them particularly suitable for scenarios where models have already been deployed <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>.\n\nOne of the most notable parameter merging approaches involves task vectors, which specify directions in the weight space of a pre-trained model that, when applied, can steer model behavior toward or away from specific tasks <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper> <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. Task vectors are created by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a particular task <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. These vectors can then be modified through arithmetic operations such as negation and addition to selectively alter model behavior <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of parameter merging methods is their computational efficiency, as they eliminate the need for online updates and extensive parameter training <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. This makes them especially valuable for LLM providers seeking economical unlearning solutions and for applications where fine-tuning already-deployed models is necessary <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. Parameter merging also offers the ability to remove specific knowledge while maintaining stability in other model behaviors <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>.\n\nHowever, parameter merging approaches face several limitations. The effectiveness of simply overlaying model parameters remains debatable, with a need for more theoretical analysis to fully understand their impact <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>. Task vectors, in particular, have been noted to have limitations in precisely localizing knowledge, which restricts their application primarily to AI alignment tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Additionally, while more computationally efficient than parameter optimization methods, parameter merging still incurs some computational costs <Paper corpusId=\"268819249\" paperTitle=\"(Xu, 2024)\" isShortName></Paper>.", "citations": [{"id": "(Si et al., 2023)", "paper": {"corpus_id": 265456592, "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Nianwen Si", "authorId": "73502630"}, {"name": "Hao Zhang", "authorId": "2154930608"}, {"name": "Heyu Chang", "authorId": "2116152318"}, {"name": "Wenlin Zhang", "authorId": "9047584"}, {"name": "Dan Qu", "authorId": "2253591545"}, {"name": "Weiqiang Zhang", "authorId": "2268429659"}], "n_citations": 33}, "snippets": ["Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model", "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."], "score": 0.81982421875}, {"id": "(Xu, 2024)", "paper": {"corpus_id": 268819249, "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yi Xu", "authorId": "2294397567"}], "n_citations": 8}, "snippets": ["\u2022 Parameter merging: LLM providers often also need more economical unlearning methods, so many works are also focusing on the method of parameter merging.This method is simpler than parameter optimization; it usually only involves simple arithmetic operations on parameters (usually addition and subtraction), and there is no need for online updates, thus saving a large number of parameter training steps and costs.If there is a need to fine-tune models that have been deployed, this method is the most practical and effective.However, parameter merging still requires a certain computational cost, and the effectiveness of simply overlaying model parameters is still debatable, with more theoretical analysis yet to be introduced.For example, (Ilharco et al., 2022) proposes a new paradigm for steering the behavior of neural networks, centered around task vectors.A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task, and thus improves the unlearning procedure."], "score": 0.5048828125}, {"id": "(Ilharco et al., 2022)", "paper": {"corpus_id": 254408495, "title": "Editing Models with Task Arithmetic", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "Marco Tulio Ribeiro", "authorId": "78846919"}, {"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Ali Farhadi", "authorId": "143787583"}], "n_citations": 520}, "snippets": ["Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."], "score": 0.0}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 271601132, "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shiji Zhou", "authorId": "2303304464"}, {"name": "Lianzhe Wang", "authorId": "2233281230"}, {"name": "Jiangnan Ye", "authorId": "2277872538"}, {"name": "Yongliang Wu", "authorId": "2314337350"}, {"name": "Heng Chang", "authorId": "2303324907"}], "n_citations": 5}, "snippets": ["Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."], "score": 0.75732421875}, {"id": "(Geng et al., 2025)", "paper": {"corpus_id": 276772996, "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiahui Geng", "authorId": "2266466915"}, {"name": "Qing Li", "authorId": "2295742465"}, {"name": "Herbert Woisetschlaeger", "authorId": "2215923457"}, {"name": "Zongxiong Chen", "authorId": "2109296817"}, {"name": "Yuxia Wang", "authorId": "2241417701"}, {"name": "Preslav Nakov", "authorId": "2026545715"}, {"name": "Hans-Arno Jacobsen", "authorId": "2238271670"}, {"name": "Fakhri Karray", "authorId": "2265969003"}], "n_citations": 5}, "snippets": ["Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."], "score": 0.7509765625}], "table": null}, {"title": "In-Context Unlearning Methods", "tldr": "In-context unlearning methods enable knowledge removal from LLMs without parameter modifications by strategically manipulating input prompts at inference time. While offering accessibility advantages for black-box models, these methods generally demonstrate lower unlearning effectiveness compared to parameter-based approaches and may struggle with generalizability. (6 sources)", "text": "\nIn-context unlearning represents a distinct approach to machine unlearning that operates without modifying model parameters, instead relying on strategic manipulation of input prompts during inference time <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>. This technique has emerged as a valuable solution particularly for scenarios where access to model parameters is limited due to computational constraints or when working with black-box LLMs <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.\n\nThe core mechanism of in-context unlearning involves providing specific inputs within the context window to influence model behavior. Typically, this is implemented by including the instance to be forgotten alongside a flipped label or contradictory information, often accompanied by correctly labeled instances that serve as additional context <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. For example, to unlearn a particular instance, practitioners might prepend inputs with the target instance paired with modified labels or information that contradicts the knowledge to be forgotten <Paper corpusId=\"271601132\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>.\n\nThe primary advantage of in-context unlearning is its accessibility and flexibility. Unlike parameter optimization or merging methods, in-context approaches do not require direct access to model weights, making them applicable to black-box models where only query access is available <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. Additionally, these methods enable test-time unlearning, allowing for dynamic knowledge removal at inference without the need for separate training or preparation phases <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>.\n\nHowever, in-context unlearning faces significant limitations. Most notably, studies indicate that parameter optimization methods typically achieve much higher unlearning effectiveness compared to in-context approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This effectiveness gap stems from the fact that in-context methods do not modify the model's intrinsic mechanisms, instead relying on contextual manipulation that may not fully counteract deeply embedded knowledge <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nAnother critical limitation is the potential for hallucinations. Since in-context unlearning often works by changing ground-truth labels or words within prompts, this approach can inadvertently lead to the generation of hallucinated content <Paper corpusId=\"273022754\" paperTitle=\"(Takashiro et al., 2024)\" isShortName></Paper>. Furthermore, in-context methods typically struggle with generalizability beyond the specific instances included in the context window, limiting their ability to achieve comprehensive unlearning across varied inputs and situations <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nDespite these limitations, in-context unlearning remains a valuable approach in scenarios where parameter access is restricted or when temporary knowledge suppression is sufficient. Its ability to function without parameter modifications makes it particularly suitable for deployed models where more invasive unlearning methods are impractical or unavailable <Paper corpusId=\"263834631\" paperTitle=\"(Pawelczyk et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Pawelczyk et al., 2023)", "paper": {"corpus_id": 263834631, "title": "In-Context Unlearning: Language Models as Few Shot Unlearners", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Martin Pawelczyk", "authorId": "89583148"}, {"name": "Seth Neel", "authorId": "2273685865"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 132}, "snippets": ["Machine unlearning, the study of efficiently removing the impact of specific training instances on a model, has garnered increased attention in recent years due to regulatory guidelines such as the \\emph{Right to be Forgotten}. Achieving precise unlearning typically involves fully retraining the model and is computationally infeasible in case of very large models such as Large Language Models (LLMs). To this end, recent work has proposed several algorithms which approximate the removal of training data without retraining the model. These algorithms crucially rely on access to the model parameters in order to update them, an assumption that may not hold in practice due to computational constraints or having only query access to the LLMs. In this work, we propose a new class of unlearning methods for LLMs called ``In-Context Unlearning.'' This method unlearns instances from the model by simply providing specific kinds of inputs in context, without the need to update model parameters."], "score": 0.654296875}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 271601132, "title": "On the Limitations and Prospects of Machine Unlearning for Generative AI", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shiji Zhou", "authorId": "2303304464"}, {"name": "Lianzhe Wang", "authorId": "2233281230"}, {"name": "Jiangnan Ye", "authorId": "2277872538"}, {"name": "Yongliang Wu", "authorId": "2314337350"}, {"name": "Heng Chang", "authorId": "2303324907"}], "n_citations": 5}, "snippets": ["Machine unlearning for LLMs is a crucial technique to align LLMs with human preferences and values and to ensure their ethical and responsible use. The existing methods for machine unlearning for LLMs can be broadly classified into: \n\nParameter Optimization Methods. These methods update the model parameters by minimizing a loss function that penalizes the undesirable outputs or behaviors of the model. (Yao et al., 2023) proposed a gradient-based unlearning method that minimizes the cross-entropy loss between the model outputs and a predefined target distribution for the data samples that need to be unlearned. They applied their method to three scenarios of unlearning for LLMs: removing harmful responses, erasing copyright-protected content, and eliminating hallucinations. \n\nParameter Merging Methods. These methods reduce the model size and complexity by merging or pruning the model parameters that are most affected by the data samples that need to be unlearned. (Ilharco et al., 2022) proposed the concept of a task vector, which, through arithmetic operations like negation or addition between task vectors, can selectively modify the model's output with minimal impact on other model behaviors. \n\nIn-context Learning Methods. These methods modify the model inputs or outputs by adding or removing certain tokens or features that indicate the data samples or modalities that need to be unlearned. To unlearn a particular instance in the forget set, (Pawelczyk et al., 2023) provided the instance alongside a flipped label and additional correctly labeled instances which are prepended as inputs to the LLM at inference time. These contexts are shown to be able to effectively remove specific information in given instances while maintaining comparable performance with other unlearning methods that need to access the LLM parameters."], "score": 0.75732421875}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271064299, "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2286638403"}, {"name": "Jaechan Lee", "authorId": "2261353791"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "Jieyu Zhao", "authorId": "2266698166"}, {"name": "Ari Holtzman", "authorId": "2309248199"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Noah A. Smith", "authorId": "2309424274"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}], "n_citations": 84}, "snippets": ["Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."], "score": 0.73681640625}, {"id": "(Takashiro et al., 2024)", "paper": {"corpus_id": 273022754, "title": "Answer When Needed, Forget When Not: Language Models Pretend to Forget via In-Context Knowledge Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shota Takashiro", "authorId": "2323750981"}, {"name": "Takeshi Kojima", "authorId": "2081836120"}, {"name": "Andrew Gambardella", "authorId": "2304550144"}, {"name": "Qi Cao", "authorId": "2268816164"}, {"name": "Yusuke Iwasawa", "authorId": "1715282"}, {"name": "Yutaka Matsuo", "authorId": "2241471533"}], "n_citations": 2}, "snippets": ["For example, Gradient Ascent (Golatkar et al., 2019) lacks test-time unlearning and only removes global knowledge. ROME (Meng et al., 2022) and Knowledge Sanitization (Ishibashi and Shimodaira, 2024) require separate training to unlearn specific knowledges so that these methods cannot perform test-time unlearning. While ICUL (In-context Unlearning) (Pawelczyk et al., 2023) achieves test-time unlearning, it merely changes a ground-truth label or word of target instance within the in-context prompt, so this approach inevitably outputs hallucinations."], "score": 0.5029296875}, {"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271212701, "title": "On Large Language Model Continual Unlearning", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chongyang Gao", "authorId": "2311833838"}, {"name": "Lixu Wang", "authorId": "2108631414"}, {"name": "Chenkai Weng", "authorId": "2148353350"}, {"name": "Xiao Wang", "authorId": "2276121035"}, {"name": "Qi Zhu", "authorId": "2275773112"}], "n_citations": 5}, "snippets": ["Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."], "score": 0.69482421875}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 278782460, "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models", "year": 2025, "venue": "", "authors": [{"name": "Miao Yu", "authorId": "2323938152"}, {"name": "Liang Lin", "authorId": "2362620538"}, {"name": "Guibin Zhang", "authorId": "2232926268"}, {"name": "Xinfeng Li", "authorId": "2349829666"}, {"name": "Junfeng Fang", "authorId": "2362314471"}, {"name": "Ningyu Zhang", "authorId": "2357266201"}, {"name": "Kun Wang", "authorId": "2363281493"}, {"name": "Yang Wang", "authorId": "2362692320"}], "n_citations": 0}, "snippets": ["However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning."], "score": 0.7744140625}], "table": null}, {"title": "Localization-Based Unlearning Methods", "tldr": "Localization-based unlearning methods target specific model components (like neurons or layers) associated with unwanted knowledge rather than modifying all parameters. While showing promise for precise knowledge removal with reduced computational demands compared to global approaches, these methods face challenges in accurately identifying the relevant parameters to modify. (6 sources)", "text": "\nLocalization-based unlearning represents an emerging approach that focuses on identifying and modifying only specific components of a model associated with the knowledge to be forgotten, rather than updating all parameters. This targeted approach aims to achieve more precise knowledge removal while reducing computational demands compared to global parameter modification methods <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>.\n\nAt its core, localization-based unlearning involves two key steps: first identifying model units (such as layers or neurons) closely related to the unlearning data or tasks, and then locally editing or modifying these specific units <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. This approach draws from research on knowledge localization in transformer models, which has found evidence that factual associations correspond to localized, directly-editable computations, particularly in middle-layer feed-forward modules <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>.\n\nThe primary advantage of localization-based methods is their computational efficiency. By modifying only a subset of parameters, these approaches can significantly reduce the computational resources required for unlearning, making them more feasible for larger models compared to global weight modification techniques <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>. This targeted approach also potentially offers greater precision in removing specific knowledge without broadly affecting model capabilities.\n\nHowever, localization-based methods face several challenges. The most significant is accurately identifying the precise set of parameters that need to be modified, which can vary considerably depending on the diversity and nature of the information to be unlearned <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper>. Incomplete or imprecise localization can leave traces of the targeted knowledge, allowing its influence to persist in the model's behavior <Paper corpusId=\"276885223\" paperTitle=\"(Srivasthav et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264816202\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nAdditionally, recent research has identified limitations in localization-based merging for unlearning. Current localization methods that boost accuracy by sharing information across tasks make exact unlearning computationally infeasible <Paper corpusId=\"277620868\" paperTitle=\"(Kuo et al., 2025)\" isShortName></Paper>. This creates a fundamental tension between parameter efficiency and complete knowledge removal.\n\nDespite these challenges, localization-based unlearning methods show considerable promise. Recent studies have found that while many existing unlearning methods fail to completely remove parametric knowledge (with their effectiveness often overestimated by common behavioral evaluations), localization-based approaches offer a promising direction for more effective knowledge removal <Paper corpusId=\"270560986\" paperTitle=\"(Hong et al., 2024)\" isShortName></Paper>. As research in this area continues to advance, these methods may provide increasingly precise and efficient solutions for knowledge unlearning in large language models.", "citations": [{"id": "(Srivasthav et al., 2025)", "paper": {"corpus_id": 276885223, "title": "Cyber for AI at SemEval-2025 Task 4: Forgotten but Not Lost: The Balancing Act of Selective Unlearning in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Dinesh P. Srivasthav", "authorId": "2135634919"}, {"name": "B. Garlapati", "authorId": "9266407"}], "n_citations": 0}, "snippets": ["The approaches to unlearning in LLMs can be broadly classified into four categories: global weight modification, local weight modification, architecture modification, input/output modification (Blanco-Justicia et al., 2024). Global weight modification involves updating all the model parameters while unlearning, thus, ensuring better guarantee of forgetting the requested information. It includes approaches such as gradient ascent (Feng et al., 2024)Gundavarapu et al., 2024), gradient difference (Bu et al., 2024), knowledge distillation (Zhao et al., 2024), KL minimization (Yao et al., 2024), weight perturbation (Yuan et al., 2024), and so on. These approaches are well suited for smaller models and provide strong unlearning, however, are resource intensive for larger models, as the training costs greatly increase with increase in the number of parameters. Global weight modification for larger models also strengthens the problem of optimizing effective unlearning, and preserving model's capabilities. Local weight modification identifies a subset of parameters that are required to be modified and accordingly updates only those model parameters (Ashuach et al., 2024;(Wu et al., 2023)Jia et al., 2024;Pochinkov and Schoots, 2024), thereby, minimizing the computational efforts needed. Nevertheless, the right set of parameters that are required to be modified might vary based on the diversity of the requested information. Identifying the same is thus, challenging which therefore, has chances of leaving traces of unlearning, or in other words, influence of the requested information could still be observed in the model's behavior (Hong et al., 2024)."], "score": 0.53955078125}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271064299, "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2286638403"}, {"name": "Jaechan Lee", "authorId": "2261353791"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "Jieyu Zhao", "authorId": "2266698166"}, {"name": "Ari Holtzman", "authorId": "2309248199"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Noah A. Smith", "authorId": "2309424274"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}], "n_citations": 84}, "snippets": ["Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."], "score": 0.73681640625}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 264816202, "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xinwei Wu", "authorId": "2051972795"}, {"name": "Junzhuo Li", "authorId": "2138202959"}, {"name": "Minghui Xu", "authorId": "2263533854"}, {"name": "Weilong Dong", "authorId": "2114049130"}, {"name": "Shuangzhi Wu", "authorId": "2263775209"}, {"name": "Chao Bian", "authorId": "2150136794"}, {"name": "Deyi Xiong", "authorId": "2263617516"}], "n_citations": 55}, "snippets": ["Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach."], "score": 0.0}, {"id": "(Kuo et al., 2025)", "paper": {"corpus_id": 277620868, "title": "Exact Unlearning of Finetuning Data via Model Merging at Scale", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kevin Kuo", "authorId": "2305621566"}, {"name": "Amrith Rajagopal Setlur", "authorId": "80366270"}, {"name": "Kartik Srinivas", "authorId": "2302322978"}, {"name": "Aditi Raghunathan", "authorId": "2269982126"}, {"name": "Virginia Smith", "authorId": "2305622207"}], "n_citations": 5}, "snippets": ["We identify a key deficiency in using localization-based merging for unlearning: current localization methods boost accuracy when merging by sharing extra information across tasks, but this information sharing makes exact unlearning computationally infeasible."], "score": 0.58251953125}, {"id": "(Hong et al., 2024)", "paper": {"corpus_id": 270560986, "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yihuai Hong", "authorId": "2306946364"}, {"name": "Lei Yu", "authorId": "2306950395"}, {"name": "Shauli Ravfogel", "authorId": "2143278592"}, {"name": "Haiqin Yang", "authorId": "2307764517"}, {"name": "Mor Geva", "authorId": "22245981"}], "n_citations": 24}, "snippets": ["Overall, our results show that existing unlearning methods fail to remove parametric knowledge and their performance is overestimated by common behavioural evaluations. Moreover, our findings underscore the promise of localization-based unlearning methods."], "score": 0.67138671875}], "table": null}, {"title": "Comparative Analysis of Tradeoffs", "tldr": "Each unlearning method offers distinct tradeoffs between unlearning effectiveness, computational efficiency, and preservation of model utility, with parameter optimization generally providing the strongest unlearning but at higher computational cost, while in-context methods offer accessibility but limited effectiveness. (10 sources)", "text": "\nWhen comparing the various unlearning approaches for large language models, distinct tradeoffs emerge across several critical dimensions including unlearning effectiveness, computational efficiency, and preservation of model utility. Understanding these tradeoffs is essential for selecting the appropriate unlearning method based on specific requirements and constraints.\n\nParameter optimization methods generally achieve the highest unlearning effectiveness among all approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. However, these methods face significant challenges in balancing the removal of targeted knowledge while preserving performance on retained knowledge. As demonstrated in empirical studies, improving performance on data to be retained typically results in decreased unlearning effectiveness on the forget set, and vice versa <Paper corpusId=\"273228310\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This fundamental tension often necessitates careful regularization to mitigate performance degradation on unrelated tasks <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Additionally, parameter optimization methods tend to be computationally intensive, particularly for large-scale models, making them less practical for deployment scenarios with limited resources.\n\nParameter merging approaches offer a middle ground in terms of effectiveness and efficiency. While they provide computational advantages by eliminating the need for extensive retraining, their precision in localizing and removing specific knowledge can be limited <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Task vectors, a common parameter merging technique, face limitations in precisely identifying knowledge locations, restricting their primary application to AI alignment tasks rather than comprehensive knowledge removal <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>.\n\nIn-context unlearning methods stand out for their accessibility and ease of implementation, particularly in scenarios where direct access to model parameters is restricted <Paper corpusId=\"270286011\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273098143\" paperTitle=\"(Satvaty et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265456592\" paperTitle=\"(Si et al., 2023)\" isShortName></Paper>. However, these approaches typically deliver significantly lower unlearning effectiveness compared to parameter-based methods <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. The reliance on contextual manipulation rather than direct parameter modification limits their ability to achieve comprehensive knowledge removal, resulting in poor generalizability beyond specific prompts <Paper corpusId=\"278782460\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper>.\n\nLocalization-based methods attempt to address the efficiency-effectiveness tradeoff by targeting specific model components associated with unwanted knowledge. While theoretically promising for balancing computational demands with unlearning precision <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>, these approaches face challenges in accurately identifying all relevant parameters, potentially leading to incomplete knowledge removal.\n\nEach unlearning approach also presents distinct failure modes. Gradient-based parameter optimization and preference optimization methods often struggle with utility preservation due to optimization conflicts, requiring additional regularization techniques <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>. Relabeling-based fine-tuning and knowledge distillation methods have been reported to suffer from knowledge relearning issues, where forgotten knowledge can be inadvertently reacquired during subsequent fine-tuning <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273849875\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. Representation engineering approaches may be more vulnerable to adversarial attacks, failing to robustly erase knowledge under challenging conditions <Paper corpusId=\"276772996\" paperTitle=\"(Geng et al., 2025)\" isShortName></Paper>.\n\nTraditional approaches like dataset filtering and complete retraining, while conceptually straightforward, remain impractical for large language models due to prohibitive computational costs <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>. Gradient reversal methods, though more efficient, can create broad damage to model capabilities beyond the targeted knowledge, while representation manipulation approaches often introduce obvious behavioral artifacts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\nThe ideal selection of an unlearning method ultimately depends on specific requirements regarding unlearning effectiveness, computational constraints, and the importance of preserving model utility across various tasks. Parameter optimization may be preferred when unlearning effectiveness is paramount and computational resources are available, while in-context approaches offer practical solutions for deployed models with limited parameter access, despite their reduced effectiveness.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271212701, "title": "On Large Language Model Continual Unlearning", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chongyang Gao", "authorId": "2311833838"}, {"name": "Lixu Wang", "authorId": "2108631414"}, {"name": "Chenkai Weng", "authorId": "2148353350"}, {"name": "Xiao Wang", "authorId": "2276121035"}, {"name": "Qi Zhu", "authorId": "2275773112"}], "n_citations": 5}, "snippets": ["Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."], "score": 0.69482421875}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 273228310, "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hyoseo Kim", "authorId": "2296414239"}, {"name": "Dongyoon Han", "authorId": "2325152150"}, {"name": "Junsuk Choe", "authorId": "3338475"}], "n_citations": 3}, "snippets": ["Second, finding a balance between reducing accuracy on the forget set while maintaining accuracy on the retain set is challenging. As shown in Figure 1 (a), improving performance on the retain set tends to result in a clear decrease in performance on the forget set, and vice versa. We argue overfitting the fine-tuned model to the forget set greatly diminishes performance on the retain set when unlearning is applied; conversely, underfitting the model to the forget set leads to ineffective unlearning, where the forgetting performance does not decrease sufficiently."], "score": 0.57080078125}, {"id": "(Geng et al., 2025)", "paper": {"corpus_id": 276772996, "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiahui Geng", "authorId": "2266466915"}, {"name": "Qing Li", "authorId": "2295742465"}, {"name": "Herbert Woisetschlaeger", "authorId": "2215923457"}, {"name": "Zongxiong Chen", "authorId": "2109296817"}, {"name": "Yuxia Wang", "authorId": "2241417701"}, {"name": "Preslav Nakov", "authorId": "2026545715"}, {"name": "Hans-Arno Jacobsen", "authorId": "2238271670"}, {"name": "Fakhri Karray", "authorId": "2265969003"}], "n_citations": 5}, "snippets": ["Brief Summary Table 1 summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning (Ma et al., 2024). Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge [\u0141ucki et al., 2024]. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment."], "score": 0.7509765625}, {"id": "(Zuo et al., 2024)", "paper": {"corpus_id": 270286011, "title": "Federated TrustChain: Blockchain-Enhanced LLM Training and Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuhan Zuo", "authorId": "2005445750"}, {"name": "Minghao Wang", "authorId": "2005212347"}, {"name": "Tianqing Zhu", "authorId": "2185053609"}, {"name": "Lefeng Zhang", "authorId": "48571174"}, {"name": "Dayong Ye", "authorId": "2261513740"}, {"name": "Shui Yu", "authorId": "2304458654"}, {"name": "Wanlei Zhou", "authorId": "2134555583"}], "n_citations": 4}, "snippets": ["Si et al. [21] explores the technical challenges of knowledge unlearning in large language models (LLMs), specifically introducing parameter optimization, parameter merging, and in-context learning as methods to efficiently remove harmful or biased data while maintaining the integrity of the models."], "score": 0.54052734375}, {"id": "(Satvaty et al., 2024)", "paper": {"corpus_id": 273098143, "title": "Undesirable Memorization in Large Language Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ali Satvaty", "authorId": "2165227497"}, {"name": "Suzan Verberne", "authorId": "2280669651"}, {"name": "Fatih Turkmen", "authorId": "2324053570"}], "n_citations": 7}, "snippets": ["Additionally, knowledge unlearning techniques have been categorized into parameter optimization, parameter merging, and in-context learning, each offering unique advantages in efficiently removing harmful or undesirable knowledge from LLMs (Si et al., 2023)."], "score": 0.76611328125}, {"id": "(Si et al., 2023)", "paper": {"corpus_id": 265456592, "title": "Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Nianwen Si", "authorId": "73502630"}, {"name": "Hao Zhang", "authorId": "2154930608"}, {"name": "Heyu Chang", "authorId": "2116152318"}, {"name": "Wenlin Zhang", "authorId": "9047584"}, {"name": "Dan Qu", "authorId": "2253591545"}, {"name": "Weiqiang Zhang", "authorId": "2268429659"}], "n_citations": 33}, "snippets": ["Knowledge unlearning, derived from analogous studies on machine unlearning, presents a promising avenue to address this concern and is notably advantageous in the context of LLMs. It allows for the removal of harmful knowledge at a minimal cost, without affecting unrelated knowledge embedded in the model", "we categorize existing knowledge unlearning methods into three classes: those based on parameter optimization, parameter merging, and incontext learning, and principles and characteristics of each method are elucidated.\n\nDiffering from the methods based on parameter optimization, methods based on parameter merging merely involves the offline composition of previously trained model parameters (e.g., via arithmetic operations like addition and subtraction) without requiring additional parameter training. This process also allows for the removal of specific knowledge from the model while maintaining the stability of other model behaviors. In scenarios where the model has already been deployed, this method proves to be practical, offering a simple and convenient means of implementing knowledge unlearning."], "score": 0.81982421875}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 278782460, "title": "UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models", "year": 2025, "venue": "", "authors": [{"name": "Miao Yu", "authorId": "2323938152"}, {"name": "Liang Lin", "authorId": "2362620538"}, {"name": "Guibin Zhang", "authorId": "2232926268"}, {"name": "Xinfeng Li", "authorId": "2349829666"}, {"name": "Junfeng Fang", "authorId": "2362314471"}, {"name": "Ningyu Zhang", "authorId": "2357266201"}, {"name": "Kun Wang", "authorId": "2363281493"}, {"name": "Yang Wang", "authorId": "2362692320"}], "n_citations": 0}, "snippets": ["However, mainstream fine-tuning-based unlearning methods often fail to balance unlearning efficacy and model ability, frequently resulting in catastrophic model collapse under extensive knowledge removal. Meanwhile, in-context unlearning, which relies solely on contextual prompting without modifying the model's intrinsic mechanisms, suffers from limited generalizability and struggles to achieve true unlearning."], "score": 0.7744140625}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 273849875, "title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yingzi Ma", "authorId": "2269426296"}, {"name": "Jiong Wang", "authorId": "2110170885"}, {"name": "Fei Wang", "authorId": "2306872120"}, {"name": "Siyuan Ma", "authorId": "2295006382"}, {"name": "Jiazhao Li", "authorId": "2286593620"}, {"name": "Xiujun Li", "authorId": "2268736216"}, {"name": "Furong Huang", "authorId": "2329735142"}, {"name": "Lichao Sun", "authorId": "2329521764"}, {"name": "Bo Li", "authorId": "2286730795"}, {"name": "Yejin Choi", "authorId": "2329517334"}, {"name": "Muhao Chen", "authorId": "1998918"}, {"name": "Chaowei Xiao", "authorId": "2256992327"}], "n_citations": 10}, "snippets": ["Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning, we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms. Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms."], "score": 0.0}, {"id": "(Gandikota et al., 2024)", "paper": {"corpus_id": 273098800, "title": "Erasing Conceptual Knowledge from Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rohit Gandikota", "authorId": "52017367"}, {"name": "Sheridan Feucht", "authorId": "2140009998"}, {"name": "Samuel Marks", "authorId": "2225941937"}, {"name": "David Bau", "authorId": "2284996653"}], "n_citations": 11}, "snippets": ["Prior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content. Unfortunately, each of these strategies have limitations that make them impractical for unlearning in large language models: dataset filtering requires retraining that is costly at scale; gradient reversal methods are unstable and create broad damage to the model; and representation manipulation creates obvious behavioral artifacts. These approaches lack a principled objective defining successful concept erasure. They focus on technical mechanisms like reversing gradients, altering training data, or randomizing activations without a clear target for the model's modified behavior."], "score": 0.6376953125}], "table": null}], "cost": 0.34997100000000003}}
