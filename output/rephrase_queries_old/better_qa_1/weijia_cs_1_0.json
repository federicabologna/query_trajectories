{"better_query": "What are the most effective parameter-space merging techniques for combining specialized language models, and how do they address knowledge conflict and task interference?", "better_answer": {"sections": [{"title": "Introduction to Parameter-Space Model Merging", "tldr": "Parameter-space model merging combines multiple language models by directly manipulating their weights to create a single model that preserves the capabilities of all constituent models. This approach offers a computationally efficient alternative to ensemble methods, allowing for knowledge integration without increasing inference costs. (LLM Memory)", "text": "\nParameter-space model merging is a technique that combines multiple pre-trained language models (LMs) at the weight level, rather than at the output level as in ensemble methods. This approach has gained significant attention as an efficient way to integrate specialized knowledge and capabilities from different models into a single model architecture. The fundamental idea is to combine the weights of multiple models that share the same architecture but have been trained on different tasks or domains. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe simplest form of parameter-space merging is linear interpolation, where the weights of the merged model are calculated as a weighted average of the constituent models' parameters. However, this naive approach often leads to interference between different knowledge sources, resulting in performance degradation. More sophisticated techniques have been developed to address these challenges, focusing on identifying and preserving the most important parameters from each source model. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe primary advantages of parameter-space merging include computational efficiency (as only one model needs to be deployed at inference time), the preservation of knowledge from multiple sources, and the potential to mitigate catastrophic forgetting when models are fine-tuned on new tasks. Additionally, this approach allows for the creation of generalist models that perform well across multiple domains without requiring retraining from scratch. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research has shown that effective parameter-space merging can not only combine models without significant performance degradation but can sometimes lead to emergent capabilities not present in any of the constituent models. This has made model merging an important area of study for developing more capable and efficient language models. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Common Sources of Interference in Model Merging", "tldr": "Parameter-space model merging often suffers from two major sources of interference: redundant parameter values that contribute little to task performance, and sign conflicts where parameter updates disagree in direction across different models. These interference sources can significantly degrade performance when merging multiple specialized models. (8 sources)", "text": "\nParameter-space model merging techniques, while promising for creating multi-task models without additional training, frequently encounter performance degradation due to parameter interference. Research has identified several key sources of this interference:\n\n## Redundant Parameter Values\nWhen models are fine-tuned for specific tasks, many parameter changes are minimal or redundant, contributing little to task performance but potentially causing interference during merging. These small, non-essential parameter changes can accumulate and disrupt the knowledge encoded in more important parameters <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Techniques like TIES-Merging address this by \"trimming\" these redundant values, effectively resetting them to their pre-trained values <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper>.\n\n## Sign Conflicts\nPerhaps the most significant source of interference occurs when the same parameter position shows opposite directional changes (sign conflicts) across different fine-tuned models. This indicates fundamental disagreement about how a parameter should be adjusted for different tasks <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. When these conflicting parameters are naively averaged, they can cancel each other out, resulting in significant performance degradation - sometimes up to 10% performance drops with simple averaging <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\n## Task Vector Conflicts\nThe task vector paradigm, which represents the difference between fine-tuned and pre-trained models, has revealed that conflicts in parameter space arise from divergent adaptation directions for different tasks <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. Visualizing these task vectors demonstrates how different models pull parameters in opposing directions, creating interference when merged <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\n## Global vs. Localized Conflicts\nRecent research has found that globally merging models often leads to unnecessary task interference due to parameter redundancy in task vectors. Studies show that much of a model's task-specific knowledge is concentrated in small, localized regions of the parameter space <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. When multiple tasks share highly overlapping localized regions, they compete for parameter control, leading to performance degradation <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Feature-Level Conflicts\nInterference also manifests at the feature level, where task-specific adaptations can conflict. Feature projection weights, normalization scalers, and shifts often require different strategies to mitigate knowledge conflicts <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>. Analysis shows that centering task vectors and applying low-rank approximation can significantly reduce task interference by isolating task-specific knowledge in the top singular vectors <Paper corpusId=\"274788575\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\nTo address these interference sources, researchers have developed various approaches, including parameter selection based on magnitude (dropping insignificant changes), sign conflict resolution by selecting dominant directions, and focusing merging on specific parameter subsets rather than the entire model <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. These strategies aim to preserve the most valuable task-specific knowledge while minimizing destructive interference during the merging process.", "citations": [{"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."], "score": 0.939453125}, {"id": "(Akiba et al., 2024)", "paper": {"corpus_id": 268537290, "title": "Evolutionary Optimization of Model Merging Recipes", "year": 2024, "venue": "Nat. Mac. Intell.", "authors": [{"name": "Takuya Akiba", "authorId": "2292261829"}, {"name": "Makoto Shing", "authorId": "2205654861"}, {"name": "Yujin Tang", "authorId": "2244138850"}, {"name": "Qi Sun", "authorId": "2326487319"}, {"name": "David Ha", "authorId": "2292260083"}], "n_citations": 125}, "snippets": ["A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."], "score": 0.923828125}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271050386, "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinliang Lu", "authorId": "81758928"}, {"name": "Ziliang Pang", "authorId": "2310334594"}, {"name": "Min Xiao", "authorId": "2310342808"}, {"name": "Yaochen Zhu", "authorId": "2290018211"}, {"name": "Rui Xia", "authorId": "2258711381"}, {"name": "Jiajun Zhang", "authorId": "2290006077"}], "n_citations": 27}, "snippets": ["Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."], "score": 0.93212890625}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 275921352, "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Wen Wang", "authorId": "2119293226"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 5}, "snippets": ["Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."], "score": 0.91943359375}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 271957310, "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yifei He", "authorId": "2182670937"}, {"name": "Yuzheng Hu", "authorId": "2317078449"}, {"name": "Yong Lin", "authorId": "2292270783"}, {"name": "Tong Zhang", "authorId": "2306841244"}, {"name": "Han Zhao", "authorId": "2283183420"}], "n_citations": 25}, "snippets": ["In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation", "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."], "score": 0.92138671875}, {"id": "(Sun et al._1, 2025)", "paper": {"corpus_id": 278501405, "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 2}, "snippets": ["Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars."], "score": 0.91552734375}, {"id": "(Choi et al., 2024)", "paper": {"corpus_id": 274788575, "title": "Revisiting Weight Averaging for Model Merging", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiho Choi", "authorId": "2336394540"}, {"name": "Donggyun Kim", "authorId": "122204266"}, {"name": "Chanhyuk Lee", "authorId": "2335812409"}, {"name": "Seunghoon Hong", "authorId": "2335816260"}], "n_citations": 5}, "snippets": ["Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors."], "score": 0.88916015625}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."], "score": 0.91650390625}], "table": null}, {"title": "TIES-Merging: A Leading Parameter-Space Merging Technique", "tldr": "TIES-Merging addresses parameter interference through a three-step approach: trimming redundant parameters, resolving sign conflicts, and merging only aligned parameters. This technique consistently outperforms other merging methods across diverse models, tasks, and domains by specifically targeting the two major sources of interference in model merging. (11 sources)", "text": "\nTIES-Merging (TRIM, ELECT SIGN & MERGE) has emerged as one of the most effective parameter-space model merging techniques, specifically designed to address the fundamental challenges of interference when combining multiple fine-tuned models. Developed by Yadav et al., TIES-Merging implements a three-step approach to mitigate the problems of redundant parameter values and sign conflicts identified in previous sections <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nThe first step, \"Trim,\" focuses on removing redundant parameters by identifying and retaining only the most influential parameter values in each task vector (defined as the difference between fine-tuned and pre-trained model weights). This is accomplished by setting parameters with minimal changes back to their pre-trained values, effectively pruning noise that contributes little to task performance <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This trimming process can be conceptualized as keeping only the top-k% of parameter values based on magnitude, where k is a tunable hyperparameter <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>.\n\nThe second step, \"Elect Sign,\" addresses sign conflicts that persist even after trimming. For each parameter position where task vectors disagree on the direction of change, TIES-Merging elects the dominant sign by selecting the direction with the largest total magnitude across all models <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>. This majority voting approach ensures that parameter updates move consistently in the most significant direction.\n\nFinally, the \"Merge\" step selectively combines only those parameters whose signs align with the elected sign direction <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This disjoint merging process creates a mean value using only parameters that agree with the elected sign, preventing destructive interference from conflicting parameter updates <Paper corpusId=\"271039884\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276961298\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of TIES-Merging stems from its targeted approach to the two primary sources of interference identified in parameter-space model merging. Unlike simpler methods like weighted averaging that often suffer from up to 10% performance drops when merging multiple models <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>, TIES-Merging explicitly aligns model representations while pruning redundant or conflicting parameters <Paper corpusId=\"276961298\" paperTitle=\"(Fang et al., 2025)\" isShortName></Paper>.\n\nTIES-Merging has been extensively evaluated across diverse settings, including various model architectures, sizes, domains, and numbers of tasks, consistently outperforming other merging techniques <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Its effectiveness has made it a foundation for numerous subsequent merging techniques, and it is often used in combination with other approaches like DARE, which further enhances performance by randomly dropping additional parameters and rescaling the remaining ones <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\nThe impact of TIES-Merging extends beyond its original implementation, with numerous subsequent approaches building upon its insights about parameter interference. For example, WIDEN (Weight Disentanglement) extends model merging techniques by disentangling model weights into magnitude and direction components <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>, while EMR-Merging applies task-specific masks and rescalers to further reduce interference <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. Additionally, Model Breadcrumbs employs a dual masking strategy that removes both large outliers and small perturbations in task vectors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nDespite its advantages, it's worth noting that in some specific applications, simpler approaches like Linear merging (equal weight averaging) have occasionally produced better results <Paper corpusId=\"278328055\" paperTitle=\"(Ociepa et al., 2025)\" isShortName></Paper> <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper>. However, TIES-Merging remains particularly valuable when combining models with potentially conflicting knowledge representations, especially when merging across diverse tasks or domains <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."], "score": 0.939453125}, {"id": "(Tao et al., 2024)", "paper": {"corpus_id": 271039884, "title": "Unlocking the Potential of Model Merging for Low-Resource Languages", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mingxu Tao", "authorId": "2053641126"}, {"name": "Chen Zhang", "authorId": "2111574159"}, {"name": "Quzhe Huang", "authorId": "2007771781"}, {"name": "Tianyao Ma", "authorId": "2310297716"}, {"name": "Songfang Huang", "authorId": "2290972467"}, {"name": "Dongyan Zhao", "authorId": "2258097237"}, {"name": "Yansong Feng", "authorId": "2115387922"}], "n_citations": 5}, "snippets": ["In this work, we investigate two commonly-used methods of model merging: weighted averaging (Choshen et al., 2022;(Wortsman et al., 2022) and TIES (Yadav et al., 2023). Here we provide a brief overview of these methods. \n\nWeighted averaging is simply averaging the parameters of two models with a weight tuned on the validation set. \n\nTIES aims to handle the parameter conflicts across multiple models more meticulously. Suppose we have two models specialized for distinct tasks, denoted as \u03b8 1 and \u03b8 2 , both trained from the same initial model \u03b8 init . Task vectors for these models are calculated as follows: \u03c4 1 = \u03b8 1 \u2212 \u03b8 init and \u03c4 2 = \u03b8 2 \u2212 \u03b8 init . The objective is to merge these task vectors and reintegrate them into the initial model. \n\nThe merging process of TIES consists of three steps: (1) Trim: For \u03c4 1 and \u03c4 2 , we trim the redundant parameters by keeping the top-k 1 % and top-k 2 % values, respectively, creating \u03c41 and \u03c42 . \n\n(2) Elect Signs: For each parameter p in \u03c41 and \u03c42 , we select the sign (+1 or -1) with the higher magni-tude, denoted as \u03b3 p = sgn(\u03c4 p 1 + \u03c4 p 2 ). \n\n(3) Disjoint Merge: For each parameter p, we only keep the parameter values from \u03c41 and \u03c42 whose signs are the same as the aggregated elected sign and calculate their mean. Specifically, for each parameter p, its disjoint mean is calculated as \u03c4 p m = avg(S p ), where S p = {\u03c4 p i |sgn(\u03c4 p i ) = \u03b3 p , i = 1, 2}."], "score": 0.93359375}, {"id": "(Fang et al., 2025)", "paper": {"corpus_id": 276961298, "title": "Efficient Multi-Task Inferencing: Model Merging with Gromov-Wasserstein Feature Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Luyang Fang", "authorId": "2263268557"}, {"name": "Ehsan Latif", "authorId": "2258714804"}, {"name": "Haoran Lu", "authorId": "2298332870"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Ping Ma", "authorId": "2276478007"}, {"name": "Xiaoming Zhai", "authorId": "2262445470"}], "n_citations": 0}, "snippets": ["Rather than simply averaging parameters or directly combining task-specific updates, TIES-MERGING enhances the merging process by explicitly aligning model representations and pruning redundant or conflicting parameters. Let \u03b8 t denote the parameters of the fine-tuned model for task t, and let \u03b8 0 represent the shared backbone. We first compute the task-specific update as \u03c4 t = \u03b8 t \u2212 \u03b8 0 . \n\nInstead of merging these updates directly, we align the feature spaces of individual models using techniques such as optimal transport. This alignment ensures that similar features across models are brought into correspondence, leading to a more coherent integration of the learned representations. \n\nAfter alignment, a pruning mechanism is applied to eliminate redundant or conflicting parameters. This step stabilizes the merged model by preserving only the essential task-specific information and mitigating destructive interference."], "score": 0.94189453125}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271050386, "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinliang Lu", "authorId": "81758928"}, {"name": "Ziliang Pang", "authorId": "2310334594"}, {"name": "Min Xiao", "authorId": "2310342808"}, {"name": "Yaochen Zhu", "authorId": "2290018211"}, {"name": "Rui Xia", "authorId": "2258711381"}, {"name": "Jiajun Zhang", "authorId": "2290006077"}], "n_citations": 27}, "snippets": ["Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."], "score": 0.93212890625}, {"id": "(Akiba et al., 2024)", "paper": {"corpus_id": 268537290, "title": "Evolutionary Optimization of Model Merging Recipes", "year": 2024, "venue": "Nat. Mac. Intell.", "authors": [{"name": "Takuya Akiba", "authorId": "2292261829"}, {"name": "Makoto Shing", "authorId": "2205654861"}, {"name": "Yujin Tang", "authorId": "2244138850"}, {"name": "Qi Sun", "authorId": "2326487319"}, {"name": "David Ha", "authorId": "2292260083"}], "n_citations": 125}, "snippets": ["A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."], "score": 0.923828125}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 265034087, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Le Yu", "authorId": "2265527327"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Haiyang Yu", "authorId": "46493167"}, {"name": "Fei Huang", "authorId": "2257407873"}, {"name": "Yongbin Li", "authorId": "1527090216"}], "n_citations": 335}, "snippets": ["In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."], "score": 0.0}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."], "score": 0.91650390625}, {"id": "(Ociepa et al., 2025)", "paper": {"corpus_id": 278328055, "title": "Bielik 11B v2 Technical Report", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Krzysztof Ociepa", "authorId": "2327339379"}, {"name": "Lukasz Flis", "authorId": "2327338025"}, {"name": "Krzysztof Wr'obel", "authorId": "2327337845"}, {"name": "Adrian Gwo'zdziej", "authorId": "2327337329"}, {"name": "Remigiusz Kinas", "authorId": "2327339098"}], "n_citations": 0}, "snippets": ["To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training."], "score": 0.87060546875}, {"id": "(Wortsman et al., 2022)", "paper": {"corpus_id": 247362886, "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "S. Gadre", "authorId": "1387466862"}, {"name": "R. Roelofs", "authorId": "40458654"}, {"name": "Raphael Gontijo-Lopes", "authorId": "2158366935"}, {"name": "Ari S. Morcos", "authorId": "4690624"}, {"name": "Hongseok Namkoong", "authorId": "40281109"}, {"name": "Ali Farhadi", "authorId": "143787583"}, {"name": "Y. Carmon", "authorId": "2444742"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}], "n_citations": 1011}, "snippets": ["The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."], "score": 0.0}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271843401, "title": "ProFuser: Progressive Fusion of Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianyuan Shi", "authorId": "2257014771"}, {"name": "Fanqi Wan", "authorId": "2217614543"}, {"name": "Canbin Huang", "authorId": "2258677979"}, {"name": "Xiaojun Quan", "authorId": "2261363209"}, {"name": "Chenliang Li", "authorId": "143971529"}, {"name": "Mingshi Yan", "authorId": "2114009661"}, {"name": "Ji Zhang", "authorId": "2281901929"}], "n_citations": 3}, "snippets": ["Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."], "score": 0.93359375}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 275921352, "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Wen Wang", "authorId": "2119293226"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 5}, "snippets": ["Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."], "score": 0.91943359375}], "table": null}, {"title": "Other Effective Parameter-Space Merging Techniques", "tldr": "Beyond TIES-Merging, several other effective parameter-space merging techniques have emerged, each with unique approaches to addressing knowledge conflicts. These include Fisher Merging, DARE, Task Arithmetic, Model Breadcrumbs, and other specialized techniques that leverage different mechanisms to reduce interference while preserving task-specific knowledge. (15 sources)", "text": "\n## Task Arithmetic (TA)\nTask Arithmetic introduced the concept of \"task vectors,\" defined as the difference between a fine-tuned model and its pre-trained version in parameter space. By combining these vectors through arithmetic operations, TA enables efficient knowledge transfer and capability steering. This approach allows models to inherit multiple task capabilities simultaneously by adding weighted task vectors to the pre-trained model parameters <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper>. Task Arithmetic provides a foundational framework for many subsequent merging techniques, demonstrating that model behaviors can be effectively steered through weight-space operations <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\n## Fisher Merging\nFisher Merging leverages the Fisher information matrix to estimate parameter importance during the merging process. Rather than treating all parameters equally, this technique approximates each model's posterior as a Gaussian distribution with a precision matrix corresponding to its Fisher information <Paper corpusId=\"244345933\" paperTitle=\"(Matena et al., 2021)\" isShortName></Paper>. This approach has shown performance improvements in robust fine-tuning and model ensembling scenarios, offering a more principled alternative to simple parameter averaging <Paper corpusId=\"271329267\" paperTitle=\"(Hagos et al., 2024)\" isShortName></Paper>. However, computing the Fisher information matrix becomes computationally and memory-intensive as model size increases <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>.\n\n## DARE (Drop And REscale)\nDARE addresses parameter redundancy by randomly dropping delta parameters (the difference between fine-tuned and pre-trained weights) and rescaling the remaining ones. This technique randomly zeros out a large percentage (up to 90-99%) of parameter changes and compensates by rescaling the remaining parameters to approximate the original embeddings <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. DARE effectively mitigates merging conflicts while preserving model performance through essential unscaling operations <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>. It is particularly effective for large language models, where merged models can sometimes exceed the performance of any source model <Paper corpusId=\"271903078\" paperTitle=\"(Dehghan et al., 2024)\" isShortName></Paper>.\n\n## Model Breadcrumbs\nUnlike TIES-Merging which focuses on sign conflicts and low-magnitude parameters, Model Breadcrumbs employs a dual masking strategy that simultaneously removes both large outliers and small perturbations in task vectors. This balanced approach provides more noise-resistant parameter selection, ensuring that neither extreme values nor insignificant changes interfere with the merging process <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## Consensus Merging\nConsensus Merging identifies and eliminates \"selfish\" and \"catastrophic\" weights\u2014parameters that are important exclusively to one task but detrimental to multi-task fusion. This technique improves the general performance of existing model merging approaches while simultaneously enabling efficient model compression <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269757600\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## EMR-Merging\nEMR-Merging (Efficient Model Routing) enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction. It applies task-specific masks to filter conflicting signs and rescalers to adjust parameter magnitudes, adapting the merged model to different tasks during inference without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## WIDEN (Weight Disentanglement)\nWIDEN extends model merging techniques by disentangling model weights into magnitude and direction components. By quantifying the divergence of these components from the backbone model, WIDEN automatically determines the importance of each model in the merging process, eliminating the need for manually assigned scaling factors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\n## RegMean and FroM\nRegMean transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model <Paper corpusId=\"271843401\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. Building upon this, FroM (Frobenius norm-based Merging) directly measures model parameters using the Frobenius norm without requiring training data, introducing an additional hyperparameter for controlling the merging process to alleviate task interference <Paper corpusId=\"279118713\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>.\n\n## Specialized Merging Techniques\nSeveral specialized techniques have emerged to address specific merging scenarios:\n- **Sens-Merging**: Analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n- **LoRS-Merging**: Combines low-rank structure and sparsity for multilingual speech models, minimizing parameter redundancy and conflicts <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n- **1bit-Merging**: Integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency, leveraging the observation that different tasks store knowledge in distinct model layers <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nMost of these techniques can be implemented as plug-and-play enhancements to existing merging methods, offering flexible approaches to address the knowledge conflict and task interference challenges inherent in parameter-space model merging <Paper corpusId=\"271329267\" paperTitle=\"(Hagos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277634330\" paperTitle=\"(Nayak et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Ilharco et al., 2022)", "paper": {"corpus_id": 254408495, "title": "Editing Models with Task Arithmetic", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "Marco Tulio Ribeiro", "authorId": "78846919"}, {"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Ali Farhadi", "authorId": "143787583"}], "n_citations": 520}, "snippets": ["Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277322544, "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Han Wu", "authorId": "2346255376"}, {"name": "Yuxuan Yao", "authorId": "2345985527"}, {"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Zehua Liu", "authorId": "2333317068"}, {"name": "Xiaojin Fu", "authorId": "2221337060"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Xing Li", "authorId": "2344902525"}, {"name": "Hui-Ling Zhen", "authorId": "2267558779"}, {"name": "Tao Zhong", "authorId": "2332348570"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}], "n_citations": 14}, "snippets": ["Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge."], "score": 0.8720703125}, {"id": "(Matena et al., 2021)", "paper": {"corpus_id": 244345933, "title": "Merging Models with Fisher-Weighted Averaging", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 402}, "snippets": ["Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."], "score": 0.0}, {"id": "(Hagos et al., 2024)", "paper": {"corpus_id": 271329267, "title": "Recent Advances in Generative AI and Large Language Models: Current Status, Challenges, and Perspectives", "year": 2024, "venue": "IEEE Transactions on Artificial Intelligence", "authors": [{"name": "D. Hagos", "authorId": "1930733"}, {"name": "Rick Battle", "authorId": "2312327093"}, {"name": "Danda B. Rawat", "authorId": "2260694752"}], "n_citations": 26}, "snippets": ["Techniques such as parameter averaging (Matena et al., 2021) or knowledge distillation [46], [47] are then employed to integrate the knowledge from these models. Additionally, various algorithms, including task vector arithmetic [48], TIES [44], and DARE [49] can be used for parameter merging, each with its own advantages and considerations, such as computational complexity and the ability to handle models trained on different tasks."], "score": 0.9287109375}, {"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271843401, "title": "ProFuser: Progressive Fusion of Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianyuan Shi", "authorId": "2257014771"}, {"name": "Fanqi Wan", "authorId": "2217614543"}, {"name": "Canbin Huang", "authorId": "2258677979"}, {"name": "Xiaojun Quan", "authorId": "2261363209"}, {"name": "Chenliang Li", "authorId": "143971529"}, {"name": "Mingshi Yan", "authorId": "2114009661"}, {"name": "Ji Zhang", "authorId": "2281901929"}], "n_citations": 3}, "snippets": ["Fisher Merging (Matena and Raffel, 2021) uses the Fisher information matrix to measure the importance of individual model parameters, guiding the merging process. However, computing the Fisher information matrix becomes computationally and memory-intensive with a large number of model parameters. RegMean (Jin et al., 2023) transforms merging into an optimization problem, finding a closed-form solution by minimizing the L2 distance between the merged model and each individual model. Task Arithmetic introduces \"task vectors\", showing that merging task vectors to create a consolidated model can effectively facilitate MTL. PEM Composition (Zhang et al., 2023) extends Task Arithmetic to merge LoRA models (Hu et al., 2021). Ties-Merging (Yadav et al., 2023) addresses task conflicts within Task Arithmetic by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency."], "score": 0.93359375}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 265034087, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Le Yu", "authorId": "2265527327"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Haiyang Yu", "authorId": "46493167"}, {"name": "Fei Huang", "authorId": "2257407873"}, {"name": "Yongbin Li", "authorId": "1527090216"}], "n_citations": 335}, "snippets": ["In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."], "score": 0.0}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 274423281, "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feng Xiong", "authorId": "2310819377"}, {"name": "Runxi Cheng", "authorId": "2315308131"}, {"name": "Wang Chen", "authorId": "2333239207"}, {"name": "Zhanqiu Zhang", "authorId": "2316516702"}, {"name": "Yiwen Guo", "authorId": "2326995130"}, {"name": "Chun Yuan", "authorId": "2333235284"}, {"name": "Ruifeng Xu", "authorId": "2333328707"}], "n_citations": 8}, "snippets": ["Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality."], "score": 0.87353515625}, {"id": "(Dehghan et al., 2024)", "paper": {"corpus_id": 271903078, "title": "MergeRepair: An Exploratory Study on Merging Task-Specific Adapters in Code LLMs for Automated Program Repair", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Meghdad Dehghan", "authorId": "2203935492"}, {"name": "Jie Jw Wu", "authorId": "2304517862"}, {"name": "Fatemeh H. Fard", "authorId": "2304472005"}, {"name": "Ali Ouni", "authorId": "1994554"}], "n_citations": 2}, "snippets": ["In this paper, we will employ three merging methods, i.e., weight-space merging [27], TIES-Merging [28], and DARE [22], to merge multiple task-specific adapters. Specifically, we will train multiple LoRA instances on various SE tasks and assess the performance of the merged LoRA in two equalweight merging and continual merging scenarios for all three merging methods."], "score": 0.873046875}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."], "score": 0.91650390625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269757600, "title": "Localizing Task Information for Improved Model Merging and Compression", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Ke Wang", "authorId": "2257324343"}, {"name": "Nikolaos Dimitriadis", "authorId": "153670517"}, {"name": "Guillermo Ortiz-Jim\u00e9nez", "authorId": "2257000703"}, {"name": "Franccois Fleuret", "authorId": "116272138"}, {"name": "Pascal Frossard", "authorId": "2256985187"}], "n_citations": 60}, "snippets": ["Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve>99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."], "score": 0.0}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 279118713, "title": "FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Zijian Li", "authorId": "2239036328"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Huixin Liu", "authorId": "2365043614"}, {"name": "Yi-Chong Huang", "authorId": "2118138548"}, {"name": "Ting Liu", "authorId": "2274093523"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 0}, "snippets": ["In this paper, we introduce an improvement to the RegMean method, which indirectly leverages the training data to approximate the outputs of the linear layers before and after merging. We propose an adaptive merging method called FroM, which directly measures the model parameters using the Frobenius norm, without any training data. By introducing an additional hyperparameter for control, FroM outperforms baseline methods across various fine-tuning scenarios, alleviating the task interference problem."], "score": 0.888671875}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276422064, "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 3}, "snippets": ["Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."], "score": 0.98046875}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276575632, "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qiuming Zhao", "authorId": "2241702793"}, {"name": "Guangzhi Sun", "authorId": "2107310187"}, {"name": "Chao Zhang", "authorId": "2256775692"}, {"name": "Mingxing Xu", "authorId": "2241950375"}, {"name": "Thomas Fang Zheng", "authorId": "2241350908"}], "n_citations": 1}, "snippets": ["We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model."], "score": 0.9384765625}, {"id": "(Liu et al._1, 2025)", "paper": {"corpus_id": 276409347, "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Zehua Liu", "authorId": "2333317068"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 2}, "snippets": ["We present \\texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies."], "score": 0.9404296875}, {"id": "(Nayak et al., 2025)", "paper": {"corpus_id": 277634330, "title": "Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nikhil Shivakumar Nayak", "authorId": "2353385023"}, {"name": "Krishnateja Killamsetty", "authorId": "2273669923"}, {"name": "Ligong Han", "authorId": "2249472971"}, {"name": "Abhishek Bhandwaldar", "authorId": "150894502"}, {"name": "Prateek Chanda", "authorId": "2290484410"}, {"name": "Kai Xu", "authorId": "2261102120"}, {"name": "Hao Wang", "authorId": "2336343895"}, {"name": "Aldo Pareja", "authorId": "2288530668"}, {"name": "Oleg Silkin", "authorId": "2354335460"}, {"name": "Mustafa Eyceoz", "authorId": "2166051120"}, {"name": "Akash Srivastava", "authorId": "2265502573"}], "n_citations": 0}, "snippets": ["On the other hand, model merging approaches, such as PATCHING (Ilharco et al., 2022), TIES (Yadav et al., 2023), represent an alternative strategy where models are finetuned separately for each task and subsequently combined into a unified multitask model by resolving parameter conflicts post-hoc. While effective, these methods incur higher computational costs due to multiple rounds of training and merging."], "score": 0.93896484375}], "table": null}, {"title": "Comparative Performance and Effectiveness", "tldr": "Empirical evaluations show that parameter-space merging techniques vary significantly in effectiveness across different contexts, with TIES-Merging and its variants generally outperforming simple averaging for multiple tasks, though simpler methods like Linear merging can sometimes excel in specific scenarios. (8 sources)", "text": "\nComparative analyses of parameter-space merging techniques reveal significant performance differences based on merging context, task diversity, and model architectures. Simple averaging methods, while straightforward to implement, often suffer from substantial performance degradation due to parameter interference, with studies showing performance drops of up to 10% when merging multiple models <Paper corpusId=\"271050386\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This performance degradation occurs primarily because weighted averaging approaches emphasize parameter importance but overlook task-specific properties of those parameters.\n\nTIES-Merging consistently demonstrates superior performance across diverse settings compared to simple averaging techniques. Its effectiveness stems from directly addressing the two primary sources of interference: redundant parameter values and sign conflicts <Paper corpusId=\"268537290\" paperTitle=\"(Akiba et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. Empirical evaluations show TIES-Merging outperforms earlier methods across multiple modalities, domains, task quantities, model sizes, and architectures <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nThe performance benefits of advanced merging techniques appear most pronounced when combining models from diverse tasks with potentially conflicting knowledge representations. The Localize-and-Stitch approach, which performs localized merging via sparse task arithmetic, demonstrates that much of the task-specific knowledge is concentrated in small regions of the parameter space. By stitching only these essential regions back onto the pretrained model, this approach significantly reduces task interference compared to global merging methods <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nRecent analysis reveals that centering task vectors around their weight average implicitly reduces task interference, with most task-specific knowledge concentrated in the top singular vectors. Applying low-rank approximation to these centered task vectors significantly improves merging performance by further reducing conflicts <Paper corpusId=\"274788575\" paperTitle=\"(Choi et al., 2024)\" isShortName></Paper>.\n\nSensitivity-guided approaches like Sens-Merging enhance existing task vector-based merging techniques by moving beyond uniform coefficients. By analyzing parameter sensitivity both within individual tasks and across different tasks, these methods determine optimal merging coefficients that account for varying parameter importance, leading to more effective knowledge integration <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nInterestingly, despite the theoretical advantages of sophisticated merging techniques, simpler approaches can sometimes yield superior results in specific contexts. For instance, when merging models obtained from different phases of the same training process, Linear merging with equal weights has produced the most favorable outcomes in some Polish language benchmarks <Paper corpusId=\"278328055\" paperTitle=\"(Ociepa et al., 2025)\" isShortName></Paper>. This suggests that the optimal merging strategy may depend on the specific characteristics of the models being combined, including their training history and the degree of task diversity.\n\nThe comparative effectiveness of different merging techniques also varies based on inference requirements. Methods like EMR-Merging enable tuning-free model adaptation across multiple tasks during inference by applying task-specific masks and rescalers, achieving high accuracy across vision, NLP, and multi-modal models without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nWhen considering the trade-off between performance and implementation complexity, TIES-Merging and its derivatives (such as when combined with DARE) offer an attractive balance for most general merging scenarios. However, the optimal choice ultimately depends on specific application requirements, the diversity of tasks being merged, and computational constraints.", "citations": [{"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271050386, "title": "Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinliang Lu", "authorId": "81758928"}, {"name": "Ziliang Pang", "authorId": "2310334594"}, {"name": "Min Xiao", "authorId": "2310342808"}, {"name": "Yaochen Zhu", "authorId": "2290018211"}, {"name": "Rui Xia", "authorId": "2258711381"}, {"name": "Jiajun Zhang", "authorId": "2290006077"}], "n_citations": 27}, "snippets": ["Merging methods based on weighted average emphasize the importance of parameters but overlook their task-specific properties, leading to significant performance degradation in certain tasks.Ilharco et al. (2023) find that \"Simple Averaging suffers from a 10% performance drop\".To address this issue, recent studies introduce a new paradigm known as the task vector.Ilharco et al. (2023) define the task vector \u03c4 t as \"a vector specifies a direction in the parameter space of a pre-trained model, such that movement in that direction improves performance on the task\", which can be formalized as:\n\nwhere \u03b8 ft t refers to the parameters fine-tuned with the specific data for task t, and \u03b8 pre refers to the original parameters of the pre-trained model.\n\nTask vector can more effectively resolve parameter conflicts during the model merging.As illustrated in Figure 4, using the pre-trained model as a reference, the variation in the direction of task vectors of the fine-tuned models indicates the presence of conflicts in the parameter space.To address parameter conflicts problem, recent studies aim to exploring methods to mitigate conflicts and strike a balance between the different models.Parameter Conflict methods resolve parameter conflicts at same position of parameters, while Fewer Parameter methods identify and prune redundant parameters to reduce conflict.\n\nResolving Parameter Conflicts TASK ARITH-METIC (Ilharco et al., 2023) initially derives task vectors through arithmetic operations between finetuned and pre-trained parameters, as shown in equation (6).These task vectors are subsequently used to merge models for enhancing performance on target tasks.To further address the issue of parameter conflicts in model merging, TIES-MERGING (Yadav et al., 2023) identifies two primary causes of interference: redundant parameter values and sign inconsistencies across models.Building on these advancements, ADAMERGING (Yang et al., 2023) reduces conflicts even further by considering the importance of different model parameters.METAGPT (Zhou et al., 2024a) proposes efficient methods based on ADAMERGING and TASK ARITHMETIC."], "score": 0.93212890625}, {"id": "(Akiba et al., 2024)", "paper": {"corpus_id": 268537290, "title": "Evolutionary Optimization of Model Merging Recipes", "year": 2024, "venue": "Nat. Mac. Intell.", "authors": [{"name": "Takuya Akiba", "authorId": "2292261829"}, {"name": "Makoto Shing", "authorId": "2205654861"}, {"name": "Yujin Tang", "authorId": "2244138850"}, {"name": "Qi Sun", "authorId": "2326487319"}, {"name": "David Ha", "authorId": "2292260083"}], "n_citations": 125}, "snippets": ["A key problem of weight interpolation-based methods is that they neglect parameter interference, leading to performance degradation. A recent work [49] identifies two key sources of interference: redundant parameter values and conflicting parameter signs across models, and proposes a method that addresses these issues to achieve improved merging performance. The proposed TIES-Merging method addresses information loss in existing merging methods by incorporating three steps: resetting minimal parameter changes, resolving sign conflicts, and merging only aligned parameters.\n\nAnother recent work [50] proposes the DARE method goes further by zeroing out small differences between the fine-tuned model and the original base model, while amplifying the differences. In practice, DARE [50] is often used together with Task Arithmetic [21] or TIES-Merging [49]."], "score": 0.923828125}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["To address these sources of interference, we propose TIES-MERGING (TRIM, ELECT SIGN & MERGE) method, a method for merging models by combining task vectors that has three steps (visualized in Fig. 1): First, we trim each task vector to retain only the influential parameter values by setting the redundant values in each task vector to zero (or, equivalently, resetting the fine-tuned parameter value back to the value from the pre-trained model). After this step, sign conflicts may still persist among influential parameter values, as visualized in Fig. 4. Our second step therefore resolves the sign conflicts between different values and our last step only averages parameters whose sign agrees with the direction of the largest total movement across models."], "score": 0.939453125}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."], "score": 0.91650390625}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 271957310, "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yifei He", "authorId": "2182670937"}, {"name": "Yuzheng Hu", "authorId": "2317078449"}, {"name": "Yong Lin", "authorId": "2292270783"}, {"name": "Tong Zhang", "authorId": "2306841244"}, {"name": "Han Zhao", "authorId": "2283183420"}], "n_citations": 25}, "snippets": ["In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation", "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."], "score": 0.92138671875}, {"id": "(Choi et al., 2024)", "paper": {"corpus_id": 274788575, "title": "Revisiting Weight Averaging for Model Merging", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiho Choi", "authorId": "2336394540"}, {"name": "Donggyun Kim", "authorId": "122204266"}, {"name": "Chanhyuk Lee", "authorId": "2335812409"}, {"name": "Seunghoon Hong", "authorId": "2335816260"}], "n_citations": 5}, "snippets": ["Model merging aims to build a multi-task learner by combining the parameters of individually fine-tuned models without additional training. While a straightforward approach is to average model parameters across tasks, this often results in suboptimal performance due to interference among parameters across tasks. In this paper, we present intriguing results that weight averaging implicitly induces task vectors centered around the weight averaging itself and that applying a low-rank approximation to these centered task vectors significantly improves merging performance. Our analysis shows that centering the task vectors effectively reduces task interference and most of task-specific knowledge is concentrated in the top singular vectors."], "score": 0.88916015625}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276422064, "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 3}, "snippets": ["Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."], "score": 0.98046875}, {"id": "(Ociepa et al., 2025)", "paper": {"corpus_id": 278328055, "title": "Bielik 11B v2 Technical Report", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Krzysztof Ociepa", "authorId": "2327339379"}, {"name": "Lukasz Flis", "authorId": "2327338025"}, {"name": "Krzysztof Wr'obel", "authorId": "2327337845"}, {"name": "Adrian Gwo'zdziej", "authorId": "2327337329"}, {"name": "Remigiusz Kinas", "authorId": "2327339098"}], "n_citations": 0}, "snippets": ["To leverage the diverse capabilities of models fine-tuned under varying conditions, we developed our framework for merging models and tested multiple strategies, including Linear (Model Soups) (Wortsman et al., 2022), TIES (TIES-Merging: Resolving Interference When Merging Models) (Yadav et al., 2023), Model Stock Jang et al. [2024]. \n\nEach of these methods offers distinct approaches to combining model parameters, aiming to integrate strengths from different models while mitigating potential conflicts or redundancies. \n\nWe systematically evaluated these merging techniques on a suite of Polish language benchmarks. Among the methods tested, the Linear merging approach, assigning equal weights (1.0) to each model, consistently yielded the most favorable results. This method involved merging three models obtained from different phases of the same DPO-Positive (DPO-P) training process, effectively capturing varied response characteristics developed during training."], "score": 0.87060546875}], "table": null}, {"title": "Recent Advancements in Addressing Knowledge Conflicts", "tldr": "Recent advancements in parameter-space merging focus on identifying and mitigating specific sources of knowledge conflicts through techniques like localized merging, sensitivity-guided coefficient adjustment, and feature-centric conflict resolution. These approaches move beyond uniform coefficient strategies to address task interference at different granularity levels, from parameter-specific to feature-level conflicts. (8 sources)", "text": "\nRecent research in parameter-space model merging has shifted toward more targeted approaches for addressing knowledge conflicts and task interference. These advancements build upon foundational techniques like TIES-Merging and Task Arithmetic while introducing more sophisticated mechanisms for conflict resolution.\n\n## Localized Merging for Reduced Task Interference\n\nOne significant advancement comes from the Localize-and-Stitch approach, which challenges the practice of global merging by recognizing that task-specific knowledge is often concentrated in small regions of parameter space. This method first identifies tiny regions in fine-tuned models that contain essential skills acquired during fine-tuning and stitches only those regions back onto the pre-trained model. By reducing the overlap between localized regions, this approach decreases task conflicts and enhances overall performance. The key insight is that \"including more parameters than necessary only introduces additional task interference\" as long as the localized region contains sufficient task-specific knowledge <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\n## Parameter Selection Based on Consensus\n\nConsensus Merging represents another significant advancement that identifies and eliminates \"selfish\" and \"catastrophic\" weights\u2014parameters that are important exclusively to one task but detrimental to multi-task fusion. This approach reveals that \"different tasks mostly use non-overlapping sets of weights,\" suggesting that multi-task performance degradation stems from specific problematic parameters rather than general incompatibility <Paper corpusId=\"274423281\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269757600\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. By removing these problematic weights, Consensus Merging consistently improves existing model merging approaches while simultaneously enabling model compression.\n\n## Sensitivity-Guided Coefficient Adjustment\n\nTraditional merging techniques often apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks. Sens-Merging addresses this limitation by analyzing parameter sensitivity at two levels:\n\n1. Within individual tasks: Identifying critical layers that significantly impact task performance\n2. Across different tasks: Prioritizing models that enhance the performance of others\n\nThis sensitivity-guided approach provides \"fine-grained control over parameter importance\" during merging, enhancing existing task vector-based methods by making them more adaptive to the specific importance of different parameters <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\n## Modality-Specific Merging Techniques\n\nRecent work has also extended parameter-space merging to new modalities. LoRS-Merging represents the first application of model merging techniques to speech models, exploiting the combination of low-rank structure and sparsity of language-specific and task-specific weights. This approach minimizes parameter redundancy and conflicts while providing an efficient way to incorporate new knowledge from task or language-specialized models <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\n## Feature-Centric Conflict Resolution\n\nMoving beyond parameter-level interventions, Conflict-Aware Task Merging (CAT Merging) introduces a feature-centric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. This approach analyzes task vector components layer by layer and applies tailored operations for different parameter types: feature projection for linear weights and masking for normalization scalers and shifts. This strategy effectively mitigates knowledge conflicts in a training-free manner, requiring only a lightweight forward pass with a few unlabeled examples <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>.\n\n## Enhanced Parameter Representation\n\nWeight Disentanglement (WIDEN) extends model merging techniques by separating model weights into magnitude and direction components. By quantifying the divergence of these components from the backbone model, WIDEN automatically determines each model's importance in the merging process, eliminating the need for manually assigned scaling factors <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>. This approach demonstrates that the orthogonality among task vectors is key to improving performance in model merging <Paper corpusId=\"275921352\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\n## Inference-Time Adaptation\n\nEMR-Merging enables tuning-free model adaptation across multiple tasks during inference by selecting the maximum absolute value of each parameter while preserving the dominant sign direction. This method applies task-specific masks to filter conflicting signs and rescalers to adjust parameter magnitudes, adapting the merged model to different tasks during inference without additional training <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nThese recent advancements collectively demonstrate a shift toward more nuanced, targeted approaches to knowledge conflict resolution in parameter-space model merging. By addressing conflicts at different levels of granularity\u2014from parameter-specific to feature-level conflicts\u2014these techniques provide more effective solutions for combining specialized language models while preserving their distinct capabilities.", "citations": [{"id": "(He et al., 2024)", "paper": {"corpus_id": 271957310, "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yifei He", "authorId": "2182670937"}, {"name": "Yuzheng Hu", "authorId": "2317078449"}, {"name": "Yong Lin", "authorId": "2292270783"}, {"name": "Tong Zhang", "authorId": "2306841244"}, {"name": "Han Zhao", "authorId": "2283183420"}], "n_citations": 25}, "snippets": ["In this work, we study the problem of task interference in the context of model merging. We find that globally merging models typically leads to task interference, due to the parameter redundancy in task vectors. To tackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse task arithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired during finetuning, and stitch only those regions back onto the pretrained model.\n\nThis has the intuitive explanation that when two conflicting tasks share highly overlapping localized regions, they will compete to steer the parameters within these regions to their advantage in the merged model, leading to performance degradation", "However, if the localized regions are small yet sufficiently informative about their respective tasks, the reduced overlap between these regions decreases task conflicts and enhances overall performance after merging. In other words, as long as the localized region contains sufficient task-specific knowledge, including more parameters than necessary in them only introduces additional task interference."], "score": 0.92138671875}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 274423281, "title": "Multi-Task Model Merging via Adaptive Weight Disentanglement", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feng Xiong", "authorId": "2310819377"}, {"name": "Runxi Cheng", "authorId": "2315308131"}, {"name": "Wang Chen", "authorId": "2333239207"}, {"name": "Zhanqiu Zhang", "authorId": "2316516702"}, {"name": "Yiwen Guo", "authorId": "2326995130"}, {"name": "Chun Yuan", "authorId": "2333235284"}, {"name": "Ruifeng Xu", "authorId": "2333328707"}], "n_citations": 8}, "snippets": ["Recently, Ilharco et al. (Ilharco et al., 2022) demonstrated that efficient capability transfer can be achieved by combining task vectors through arithmetic operations. However, task interference remains a significant challenge. Ties-Merging (Yadav et al., 2023) resolves this challenge by trimming redundant parameters, resolving the sign conflicts, and averaging parameters that align with the predominant direction. DARE (Yu et al., 2023) mitigates merging conflicts by randomly dropping parameters and preserves model performance through essential unscaling operations. \n\nConsensus Merging (Wang et al., 2024) eliminates selfish and catastrophic weights, thereby enhancing the overall performance of existing model merging methods while simultaneously compressing the model. In our work, we have demonstrated that the orthogonality among task vectors is the key to improving performance in model merging and introducing adaptive weight disentanglement to improve orthogonality."], "score": 0.87353515625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269757600, "title": "Localizing Task Information for Improved Model Merging and Compression", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Ke Wang", "authorId": "2257324343"}, {"name": "Nikolaos Dimitriadis", "authorId": "153670517"}, {"name": "Guillermo Ortiz-Jim\u00e9nez", "authorId": "2257000703"}, {"name": "Franccois Fleuret", "authorId": "116272138"}, {"name": "Pascal Frossard", "authorId": "2256985187"}], "n_citations": 60}, "snippets": ["Model merging and task arithmetic have emerged as promising scalable approaches to merge multiple single-task checkpoints to one multi-task model, but their applicability is reduced by significant performance loss. Previous works have linked these drops to interference in the weight space and erasure of important task-specific features. Instead, in this work we show that the information required to solve each task is still preserved after merging as different tasks mostly use non-overlapping sets of weights. We propose TALL-masks, a method to identify these task supports given a collection of task vectors and show that one can retrieve>99% of the single task accuracy by applying our masks to the multi-task vector, effectively compressing the individual checkpoints. We study the statistics of intersections among constructed masks and reveal the existence of selfish and catastrophic weights, i.e., parameters that are important exclusively to one task and irrelevant to all tasks but detrimental to multi-task fusion. For this reason, we propose Consensus Merging, an algorithm that eliminates such weights and improves the general performance of existing model merging approaches. Our experiments in vision and NLP benchmarks with up to 20 tasks, show that Consensus Merging consistently improves existing approaches. Furthermore, our proposed compression scheme reduces storage from 57Gb to 8.2Gb while retaining 99.7% of original performance."], "score": 0.0}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276422064, "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 3}, "snippets": ["Recent advances in large language models have led to numerous task-specialized fine-tuned variants, creating a need for efficient model merging techniques that preserve specialized capabilities while avoiding costly retraining. While existing task vector-based merging methods show promise, they typically apply uniform coefficients across all parameters, overlooking varying parameter importance both within and across tasks.\n\nWe present Sens-Merging, a sensitivity-guided coefficient adjustment method that enhances existing model merging techniques by operating at both task-specific and cross-task levels. Our method analyzes parameter sensitivity within individual tasks and evaluates cross-task transferability to determine optimal merging coefficients.\n\nWhile task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."], "score": 0.98046875}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276575632, "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qiuming Zhao", "authorId": "2241702793"}, {"name": "Guangzhi Sun", "authorId": "2107310187"}, {"name": "Chao Zhang", "authorId": "2256775692"}, {"name": "Mingxing Xu", "authorId": "2241950375"}, {"name": "Thomas Fang Zheng", "authorId": "2241350908"}], "n_citations": 1}, "snippets": ["We propose LoRS-Merging, a low-rank and sparse model merging method for multi-lingual ASR and speech translation. To the best of our knowledge, LoRS-Merging is the first work that explores model merging for speech models.\n\nLoRS-Merging exploits the combination of lowrank structure and sparsity of language-specific and task-specific weights in model merging, minimising the parameter redundancy and conflicts as well as providing an efficient way to incorporate new knowledge from a task or languagespecialised model."], "score": 0.9384765625}, {"id": "(Sun et al._1, 2025)", "paper": {"corpus_id": 278501405, "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 2}, "snippets": ["Motivated by the dual objectives outlined above, we propose Conflict-Aware Task Merging (CAT Merging), a featurecentric framework that addresses knowledge conflicts by trimming conflict-prone components from task vectors. Specifically, we focus on feature-level conflicts by analyzing task vector components layer by layer. By adhering to the dual objectives above, CAT Merging involves tailored operations for different types of parameters: feature projection for linear weights, and masking for normalization scalers and shifts. These strategies ensure that CAT Merging effectively mitigates knowledge conflicts in a training-free manner, relying solely on a lightweight forward pass with few unlabeled exemplars."], "score": 0.91552734375}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["TIES-MERGING: This method focuses on minimizing parameter redundancy and resolving sign conflicts, two primary sources of performance degradation in model merging. TIES-MERGING reduces interference in model merging by selectively keeping important parameters and resolving sign conflicts. First, it drops low-magnitude parameters, resetting them to pre-trained values while preserving only the most significant ones. Then, it resolves conflicting parameter signs by selecting the dominant direction across models. Finally, it merges only the aligned parameters, preventing destructive interference. This method improves merging quality by ensuring that only the most relevant and consistent updates are retained [Yadav et al., 2024].\n\nModel Breadcrumbs: Unlike TIES-Merging, which resolves sign conflicts and trims low-magnitude parameters to address interference (Figure 3), Model Breadcrumbs employs a dual masking strategy that simultaneously removes large outliers and small perturbations in task vectors. This ensures a more balanced and noise-resistant parameter selection [Davari and Belilovsky, 2024].\n\nEMR-MERGING: The EMR-Merging method enables tuning-free model merging by selecting the maximum absolute value of each parameter while preserving the dominant sign direction, reducing interference. It then applies taskspecific masks to filter conflicting signs and rescalers to adjust parameter magnitudes. During inference, these modulators adapt the merged model to different tasks, achieving high accuracy across vision, NLP, and multi-modal models without additional training [Huang et al., 2024].\n\nWIDEN(Weight Disentanglement): A novel approach to extending model merging techniques beyond fine-tuned (FT) models to also include pre-trained (PT) models. The key idea behind WIDEN is to disentangle model weights into two components: magnitude and direction. By quantifying the divergence of these components from the backbone model, WIDEN can automatically determine the importance of each model in the merging process, eliminating the need for manually assigned scaling factors [Yu et al., 2024a]."], "score": 0.91650390625}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 275921352, "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Wen Wang", "authorId": "2119293226"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 5}, "snippets": ["Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations.\n\nTask Arithmetic (TA) (Ilharco et al., 2023b) innovatively proposes the concept of \"task vector\", defined as the vector from a pre-trained model to its fine-tuned counterpart in the parameter space. By weighting these task vectors and adding them back to the pre-trained model, TA strikes a harmonious balance between generalized knowledge from the pre-train model and the task-specific knowledge in the task vectors. Following this insight, Ties-Merging (Yadav et al., 2023) refines the fusion process by discarding parameters deemed insignificant or of low magnitude. PEFT (Zhang et al., 2023a) and MoLE (Wu et al., 2024) further extend TA by integrating it with LoRA (Hu et al., 2022) modules. Furthermore, Ortiz-Jimenez et al. (2023) suggests fine-tuning models in the tangent space, which can effectively mitigate conflict between task vectors."], "score": 0.91943359375}], "table": null}], "cost": 0.45768}}
