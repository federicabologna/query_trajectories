{"better_query": "What are the main differences in compression ratio, performance impact, and hardware compatibility between recent unstructured and structured pruning methods for large language models?", "better_answer": {"sections": [{"title": "Introduction to Pruning Methods for LLMs", "tldr": "Pruning methods for large language models fall into two main categories: structured pruning, which removes entire architectural components, and unstructured pruning, which eliminates individual weights. These approaches offer different trade-offs in terms of compression, performance, and hardware compatibility. (10 sources)", "text": "\nThe exponential growth in size of large language models (LLMs) has created significant challenges for their deployment in resource-constrained environments. Model compression techniques have emerged as essential solutions to this problem, with pruning being one of the primary approaches alongside quantization and distillation <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231740691\" paperTitle=\"(Hoefler et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nPruning methods for LLMs can be broadly categorized into two main approaches:\n\n1. **Structured pruning** removes entire architectural components such as neurons, attention heads, or even complete layers from the model <Paper corpusId=\"267751193\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper>. This approach maintains a regular, dense structure in the resulting model, which makes it more deployment-friendly and hardware-compatible <Paper corpusId=\"272693912\" paperTitle=\"(Lv et al., 2024)\" isShortName></Paper>. A key advantage of structured pruning is that the model can be physically reshaped to new dimensions, allowing computational savings to be leveraged on standard hardware without specialized support <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. However, models are typically highly sensitive to structured compression, often requiring gradual compression and retraining cycles to recover accuracy.\n\n2. **Unstructured pruning** targets individual weights within the model without considering any specific structural patterns <Paper corpusId=\"267751193\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258865530\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This approach can achieve higher accuracy for a given compression ratio by enabling more targeted weight removal <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper>. Recent work has shown that even massive GPT-family models can be pruned to at least 50% sparsity in one shot without retraining, with minimal accuracy loss <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. However, the resulting irregular sparsity patterns lead to inefficient memory access and limited speed-ups on standard hardware, often requiring specialized libraries or hardware to fully leverage the compression benefits <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper>.\n\nThe fundamental trade-off between these approaches lies in their practical applicability: structured pruning delivers more immediate hardware efficiency but may sacrifice more accuracy, while unstructured pruning can retain more accuracy but requires specialized hardware support to realize performance gains <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Kurtic et al., 2023)", "paper": {"corpus_id": 256662263, "title": "ZipLM: Inference-Aware Structured Pruning of Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Eldar Kurtic", "authorId": "40992614"}, {"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 26}, "snippets": ["The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."], "score": 0.90576171875}, {"id": "(Hoefler et al., 2021)", "paper": {"corpus_id": 231740691, "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks", "year": 2021, "venue": "Journal of machine learning research", "authors": [{"name": "T. Hoefler", "authorId": "1713648"}, {"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "Tal Ben-Nun", "authorId": "1402921119"}, {"name": "Nikoli Dryden", "authorId": "2134146"}, {"name": "Alexandra Peste", "authorId": "3341722"}], "n_citations": 725}, "snippets": ["The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field."], "score": 0.0}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}, {"id": "(Yuan et al., 2024)", "paper": {"corpus_id": 267751193, "title": "Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shuzhou Yuan", "authorId": "2201687496"}, {"name": "Ercong Nie", "authorId": "2197254657"}, {"name": "Bolei Ma", "authorId": "2188764477"}, {"name": "Michael Farber", "authorId": "2281825175"}], "n_citations": 3}, "snippets": ["Pruning, a prevalent technique for model compression method, seeks to discard non-essential parameters within language models, thereby reducing computational and storage costs while maintaining performance (LeCun et al., 1989), (Wang et al., 2023), (Xia et al., 2022). Pruning methods can be broadly categorized into unstructured pruning, which removes individual weights, and structured pruning, which removes higher-granularity structures such as neurons, attention heads, or entire layers."], "score": 0.7646484375}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 247922354, "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 187}, "snippets": ["The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."], "score": 0.0}, {"id": "(Lv et al., 2024)", "paper": {"corpus_id": 272693912, "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large Language Models", "year": 2024, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Bo Lv", "authorId": "2321454694"}, {"name": "Quan Zhou", "authorId": "2321495086"}, {"name": "Xuanang Ding", "authorId": "2216237674"}, {"name": "Yan Wang", "authorId": "2321964263"}, {"name": "Zeming Ma", "authorId": "2321489070"}], "n_citations": 2}, "snippets": ["Currently, optimization methods for large models include pruning (structured pruning [17]-[19] and unstructured pruning [20], [21]), quantization [22]-[24], and distillation [25], [26]. This work focuses on structured pruning, making it more deployment-friendly and hardware-friendly."], "score": 0.76953125}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 258865530, "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xinpeng Wang", "authorId": "2108254820"}, {"name": "Leonie Weissweiler", "authorId": "31832571"}, {"name": "Hinrich Schutze", "authorId": "2130001188"}, {"name": "Barbara Plank", "authorId": "2065013761"}], "n_citations": 8}, "snippets": ["Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies."], "score": 0.0}, {"id": "(Garg et al., 2025)", "paper": {"corpus_id": 278033481, "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Muskan Garg", "authorId": "2258141722"}, {"name": "Shaina Raza", "authorId": "2278330619"}, {"name": "Shebuti Rayana", "authorId": "3023076"}, {"name": "Xingyi Liu", "authorId": "2278394763"}, {"name": "Sunghwan Sohn", "authorId": "2267490593"}], "n_citations": 2}, "snippets": ["Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive."], "score": 0.822265625}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"], "score": 0.0}], "table": null}, {"title": "Compression Ratio Comparison", "tldr": "Unstructured pruning achieves higher compression ratios with less performance degradation compared to structured pruning, but the resulting sparse matrices are difficult to accelerate without specialized hardware. Structured pruning yields lower compression rates but delivers immediate hardware efficiency gains. (11 sources)", "text": "\nUnstructured pruning consistently achieves higher compression ratios while maintaining model performance compared to structured pruning. This approach can reach extreme sparsity levels\u2014up to 60-70% on large language models like LLaMA-7B with minimal performance degradation <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> and even 97% compression on some BERT models with small accuracy drops <Paper corpusId=\"245131431\" paperTitle=\"(Xu et al., 2021)\" isShortName></Paper>. The superior compression capability stems from unstructured pruning's ability to selectively remove only the least important individual weights while preserving critical ones <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.\n\nIn contrast, structured pruning typically achieves lower compression ratios before significant performance degradation occurs. This approach faces inherent limitations because it removes entire components (neurons, attention heads, or layers), which may include both essential and non-essential weights <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. As a result, structured pruning tends to cause higher accuracy drops than unstructured pruning at equivalent compression levels <Paper corpusId=\"247794014\" paperTitle=\"(Yang et al., 2022)\" isShortName></Paper>.\n\nRecent research illustrates this gap in compression capabilities. For example, SparseGPT has demonstrated the ability to compress LLMs with billions of parameters by up to 60% with negligible performance impact using unstructured techniques <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Similarly, Wanda achieves 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. These unstructured pruning methods consistently outperform structured approaches in terms of raw compression potential.\n\nThe fundamental challenge with unstructured pruning, however, is that the resulting irregular sparse matrices do not translate to practical efficiency gains without specialized hardware or software support <Paper corpusId=\"256662734\" paperTitle=\"(Santacroce et al., 2023)\" isShortName></Paper>. In fact, directly applying sparse kernels to unstructured pruned models can lead to significant slowdowns\u2014up to 60\u00d7 compared to dense kernels on GPUs <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper>. This represents the central trade-off in pruning approaches: unstructured pruning yields higher theoretical compression but requires specialized implementations to realize practical benefits, while structured pruning provides immediate efficiency gains but with more limited compression potential <Paper corpusId=\"275920740\" paperTitle=\"(Sander et al., 2025)\" isShortName></Paper>.\n\nSemi-structured approaches, like N:M sparsity patterns where N out of every M consecutive weights are pruned, attempt to strike a balance between the two extremes by enforcing regular pruning patterns that can be more efficiently implemented in hardware <Paper corpusId=\"278501529\" paperTitle=\"(Laborde et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. For example, 2:4 sparse networks can achieve 2\u00d7 speedup without performance drops on specialized GPUs like NVIDIA A100s <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."], "score": 0.0}, {"id": "(Xu et al., 2021)", "paper": {"corpus_id": 245131431, "title": "From Dense to Sparse: Contrastive Pruning for Better Pre-trained Language Model Compression", "year": 2021, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Runxin Xu", "authorId": "1748844142"}, {"name": "Fuli Luo", "authorId": "2140495101"}, {"name": "Chengyu Wang", "authorId": "121899912"}, {"name": "Baobao Chang", "authorId": "7267809"}, {"name": "Jun Huang", "authorId": "2078113"}, {"name": "Songfang Huang", "authorId": "2410938"}, {"name": "Fei Huang", "authorId": "143857288"}], "n_citations": 26}, "snippets": ["Pre-trained Language Models (PLMs) have achieved great success in various Natural Language Processing (NLP) tasks under the pre-training and fine-tuning paradigm. \n With large quantities of parameters, PLMs are computation-intensive and resource-hungry. Hence, model pruning has been introduced to compress large-scale PLMs. \n However, most prior approaches only consider task-specific knowledge towards downstream tasks, but ignore the essential task-agnostic knowledge during pruning, which may cause catastrophic forgetting problem and lead to poor generalization ability. \n To maintain both task-agnostic and task-specific knowledge in our pruned model, we propose ContrAstive Pruning (CAP) under the paradigm of pre-training and fine-tuning. \n It is designed as a general framework, compatible with both structured and unstructured pruning. \n Unified in contrastive learn- ing, CAP enables the pruned model to learn from the pre-trained model for task-agnostic knowledge, and fine-tuned model for task-specific knowledge. \n Besides, to better retain the performance of the pruned model, the snapshots (i.e., the intermediate models at each pruning iteration) also serve as effective supervisions for pruning. \n Our extensive experiments show that adopting CAP consistently yields significant improvements, especially in extremely high sparsity scenarios. \n With only 3% model parameters reserved (i.e., 97% sparsity), CAP successfully achieves 99.2% and 96.3% of the original BERT performance in QQP and MNLI tasks. \n In addition, our probing experiments demonstrate that the model pruned by CAP tends to achieve better generalization ability."], "score": 0.0}, {"id": "(Belhaouari et al., 2025)", "paper": {"corpus_id": 277275922, "title": "Efficient self-attention with smart pruning for sustainable large language models", "year": 2025, "venue": "Scientific Reports", "authors": [{"name": "S. Belhaouari", "authorId": "102804035"}, {"name": "Insaf Kraidia", "authorId": "2292003273"}], "n_citations": 1}, "snippets": ["Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations."], "score": 0.96142578125}, {"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 246276158, "title": "Iterative Activation-based Structured Pruning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Kaiqi Zhao", "authorId": "1995855"}, {"name": "Animesh Jain", "authorId": "101682296"}, {"name": "Ming Zhao", "authorId": "2152527896"}], "n_citations": 0}, "snippets": ["Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."], "score": 0.89111328125}, {"id": "(Yang et al., 2022)", "paper": {"corpus_id": 247794014, "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ziqing Yang", "authorId": "48599077"}, {"name": "Yiming Cui", "authorId": "3043830"}, {"name": "Zhigang Chen", "authorId": "2156610145"}], "n_citations": 12}, "snippets": ["In the unstructured pruning, each model parameter is individually removed if it reaches some criteria based on the magnitude or importance score (Han et al., 2015;(Zhu et al., 2017). The unstructured pruning results in sparse matrices and allows for significant model compression, but the inference speed can hardly be improved without specialized devices. While in the structured pruning, rows or columns of the parameters are removed from the weight matrices (McCarley, 2019;(Michel et al., 2019)(Voita et al., 2019)(Lagunas et al., 2021)(Hou et al., 2020). Thus, the resulting model speeds up on the common CPU and GPU devices."], "score": 0.83837890625}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 267412232, "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models", "year": 2024, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Xindi Wang", "authorId": "2108048327"}, {"name": "Mahsa Salmani", "authorId": "1904419"}, {"name": "Parsa Omidi", "authorId": "2282534833"}, {"name": "Xiangyu Ren", "authorId": "2283447900"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "A. Eshaghi", "authorId": "50782111"}], "n_citations": 45}, "snippets": ["In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models."], "score": 0.83740234375}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]. However, unstructured pruning often results in irregular sparse patterns in the weight matrix, necessitating specialized hardware accelerators (e.g., sparse matrix multiplication units) to efficiently handle sparse matrix computations and fully exploit the benefits of sparsity in terms of storage and computation."], "score": 0.94482421875}, {"id": "(Santacroce et al., 2023)", "paper": {"corpus_id": 256662734, "title": "What Matters In The Structured Pruning of Generative Language Models?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Michael Santacroce", "authorId": "1413038175"}, {"name": "Zixin Wen", "authorId": "2051054583"}, {"name": "Yelong Shen", "authorId": "1752875"}, {"name": "Yuan-Fang Li", "authorId": "152244300"}], "n_citations": 34}, "snippets": ["Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs."], "score": 0.77490234375}, {"id": "(Sander et al., 2025)", "paper": {"corpus_id": 275920740, "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jacob Sander", "authorId": "2342530769"}, {"name": "Achraf Cohen", "authorId": "2342484279"}, {"name": "Venkateswara Dasari", "authorId": "69387233"}, {"name": "K. Venable", "authorId": "1712010"}, {"name": "B. Jalaeian", "authorId": "3116427"}], "n_citations": 1}, "snippets": ["While structured pruning offers consistent inference acceleration across hardware, unstructured pruning often achieves higher compression ratios but requires specialized hardware or compiler optimizations to realize speedups."], "score": 0.80126953125}, {"id": "(Laborde et al., 2025)", "paper": {"corpus_id": 278501529, "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "year": 2025, "venue": "", "authors": [{"name": "Stanislas Laborde", "authorId": "2360373404"}, {"name": "Martin Cousseau", "authorId": "2360359994"}, {"name": "Antoun Yaacoub", "authorId": "40605834"}, {"name": "Lionel Prevost", "authorId": "2266474578"}], "n_citations": 0}, "snippets": ["Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12]."], "score": 0.89892578125}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 231847094, "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Aojun Zhou", "authorId": "9548994"}, {"name": "Yukun Ma", "authorId": "2289831903"}, {"name": "Junnan Zhu", "authorId": "24925751"}, {"name": "Jianbo Liu", "authorId": "2124809722"}, {"name": "Zhijie Zhang", "authorId": "1490508571"}, {"name": "Kun Yuan", "authorId": "50492964"}, {"name": "Wenxiu Sun", "authorId": "8397576"}, {"name": "Hongsheng Li", "authorId": "47893312"}], "n_citations": 248}, "snippets": ["Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity."], "score": 0.0}], "table": null}, {"title": "Performance Impact on Model Accuracy", "tldr": "Unstructured pruning preserves model accuracy better than structured pruning at equivalent compression rates, but structured pruning's hardware benefits come at the cost of greater performance degradation. Recent methods like LLM-Pruner attempt to mitigate accuracy loss in structured pruning through targeted component removal and lightweight fine-tuning. (18 sources)", "text": "\nThe impact of pruning methods on model accuracy reveals a clear trade-off between compression and performance. Unstructured pruning consistently outperforms structured pruning in maintaining model accuracy at equivalent compression ratios <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>. This advantage stems from unstructured pruning's ability to selectively remove only the least important individual weights while preserving critical ones, offering a more fine-grained approach to model compression <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"277275922\" paperTitle=\"(Belhaouari et al., 2025)\" isShortName></Paper>.\n\nStructured pruning, while beneficial for hardware efficiency, typically causes more substantial accuracy degradation because it removes entire architectural components (neurons, attention heads, layers) that may contain both essential and non-essential weights <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>. Models are generally highly sensitive to structured compression, often requiring gradual compression and retraining cycles to recover accuracy <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. The coarser granularity of structured pruning makes it more challenging to preserve model capabilities, especially at higher compression ratios <Paper corpusId=\"276482745\" paperTitle=\"(Qin et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277244079\" paperTitle=\"(Lu et al., 2025)\" isShortName></Paper>.\n\nRecent approaches have attempted to mitigate the performance impact of structured pruning. For example, LLM-Pruner employs structured pruning to remove non-critical grouped structures based on gradient information, followed by LoRA fine-tuning to recover performance <Paper corpusId=\"270621063\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. This method can restore model capabilities with minimal data (just 50K samples) and short fine-tuning periods (approximately 3 hours) <Paper corpusId=\"275921475\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper> <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>.\n\nUnstructured pruning methods have shown impressive results in preserving model accuracy even at high sparsity levels. Recent techniques like SparseGPT can prune large language models to at least 50% sparsity in one shot without retraining, while maintaining minimal accuracy loss <Paper corpusId=\"278033481\" paperTitle=\"(Garg et al., 2025)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. Similarly, Wanda achieves 60% sparsity on LLaMA-7B with negligible performance degradation across multiple downstream tasks <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>.\n\nThe performance impact also varies by pruning implementation. Semi-structured approaches like N:M sparsity patterns (where N out of every M consecutive weights are retained) offer a middle ground, balancing performance preservation with some degree of hardware compatibility <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. These patterns enable more efficient implementation in hardware while maintaining better accuracy than fully structured methods <Paper corpusId=\"278208127\" paperTitle=\"(Mugnaini et al., 2025)\" isShortName></Paper>.\n\nThe fundamental challenge remains that while unstructured pruning preserves accuracy better, the resulting irregular sparse matrices require specialized hardware or software to translate into computational efficiency <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269588232\" paperTitle=\"(Klein et al., 2024)\" isShortName></Paper>. Structured pruning, despite its greater impact on accuracy, provides immediate hardware efficiency gains and deployment benefits without requiring specialized support <Paper corpusId=\"273963228\" paperTitle=\"(Rostami et al., 2024)\" isShortName></Paper> <Paper corpusId=\"260887757\" paperTitle=\"(Cheng et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 246276158, "title": "Iterative Activation-based Structured Pruning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Kaiqi Zhao", "authorId": "1995855"}, {"name": "Animesh Jain", "authorId": "101682296"}, {"name": "Ming Zhao", "authorId": "2152527896"}], "n_citations": 0}, "snippets": ["Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."], "score": 0.89111328125}, {"id": "(Gong et al., 2024)", "paper": {"corpus_id": 267103825, "title": "A Review of Neural Network Lightweighting Techniques", "year": 2024, "venue": "Innovation &amp; Technology Advances", "authors": [{"name": "Ziyi Gong", "authorId": "2280410840"}, {"name": "Huifu Zhang", "authorId": "2280726529"}, {"name": "Hao Yang", "authorId": "2280414619"}, {"name": "Fangjun Liu", "authorId": "2280959623"}, {"name": "Fan Luo", "authorId": "2280411835"}], "n_citations": 0}, "snippets": ["By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."], "score": 0.8623046875}, {"id": "(Belhaouari et al., 2025)", "paper": {"corpus_id": 277275922, "title": "Efficient self-attention with smart pruning for sustainable large language models", "year": 2025, "venue": "Scientific Reports", "authors": [{"name": "S. Belhaouari", "authorId": "102804035"}, {"name": "Insaf Kraidia", "authorId": "2292003273"}], "n_citations": 1}, "snippets": ["Structure pruning typically removes entire groups of parameters, such as whole neurons, channels, or even layers, which can limit its flexibility. Furthermore, this method can achieve a different level of sparsity than unstructured pruning, limiting its ability to compress models efficiently without retraining or redesigning the model architecture. On the other hand, unstructured pruning operates at the level of individual weights in a neural network, meaning it can remove any specific weight, regardless of its position. This allows for more fine-grained control over which weights to prune. This can target only the least essential weights, leading to a more refined reduction in model size while retaining critical model capacity. It can achieve higher sparsity levels without compromising model performance because it is not constrained by the rigid structures to which structured pruning must adhere.\n\nDespite preserving model performance, unstructured pruning often results in sparse weight matrices, which may not fully utilize hardware efficiency, as specialized hardware (such as GPUs) is typically optimized for dense matrix operations."], "score": 0.96142578125}, {"id": "(Kurtic et al., 2023)", "paper": {"corpus_id": 256662263, "title": "ZipLM: Inference-Aware Structured Pruning of Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Eldar Kurtic", "authorId": "40992614"}, {"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 26}, "snippets": ["The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."], "score": 0.90576171875}, {"id": "(Qin et al., 2025)", "paper": {"corpus_id": 276482745, "title": "MaskPrune: Mask-based LLM Pruning for Layer-wise Uniform Structures", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiayu Qin", "authorId": "2290611525"}, {"name": "Jianchao Tan", "authorId": "2326256572"}, {"name": "Kefeng Zhang", "authorId": "2326248013"}, {"name": "Xunliang Cai", "authorId": "2326248599"}, {"name": "Wei Wang", "authorId": "2338695871"}], "n_citations": 0}, "snippets": ["Unstructured pruning compresses models by removing individual parameters, resulting in sparse weight matrices that consume less memory. However, without dedicated hardware support, the updated models do not achieve faster inference, thereby still imposing computational burdens during the inference process. Semi-structured pruning offers some speed improvements, but these are limited compared to those achieved by structured pruning. Structured pruning adopts a more modular approach to remove modules from models, typically targeting attention heads, embedding dimensions, FFN intermediate dimensions, experts in Mixture-of-Experts (MoE) networks, or layers. After structured pruning, the weight matrices of the models remain dense, and their reduced dimensions typically lead to greater inference acceleration. However, the coarser granularity of this pruning method makes it more challenging to preserve model capabilities after pruning."], "score": 0.92919921875}, {"id": "(Lu et al., 2025)", "paper": {"corpus_id": 277244079, "title": "Large Language Model Compression via the Nested Activation-Aware Decomposition", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jun Lu", "authorId": "2316872781"}, {"name": "Tianyi Xu", "authorId": "2351716503"}, {"name": "Bill Ding", "authorId": "2316639822"}, {"name": "David Li", "authorId": "2316673285"}, {"name": "Yu Kang", "authorId": "2316671224"}], "n_citations": 1}, "snippets": ["However, the irregular sparsification achieved through unstructured pruning often fails to achieve the desired speedup or memory savings. Structured pruning, in contrast, removes entire channels or components from LLMs, making it easier to implement on hardware but potentially leading to significant accuracy degradation, especially under high compression ratios as seen with LLM-Pruner (Ma et al., 2023)."], "score": 0.89013671875}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 270621063, "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Zhiyu Guo", "authorId": "2300138950"}, {"name": "Hidetaka Kamigaito", "authorId": "2300756"}, {"name": "Taro Wanatnabe", "authorId": "2299941873"}], "n_citations": 1}, "snippets": ["Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern."], "score": 0.76953125}, {"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"], "score": 0.0}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 275921475, "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Siddhant Chaudhary", "authorId": "2261280847"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 4}, "snippets": ["Unstructured pruning removes individual weights, as seen in SparseGPT (Frantar & Alistarh, 2023), which leverages Hessian matrix inversion to identify and eliminate less critical weights. However, unstructured pruning often requires hardware-specific optimizations and may not always result in significant computational gains (Yang et al., 2024;Wang et al., 2024b). In contrast, structured pruning removes entire channels or components, making it more compatible with various hardware setups. For example, LLM-Pruner (Ma et al., 2023) evaluates weight group importance and uses LoRA fine-tuning to recover lost accuracy. While structured pruning is more hardware-friendly, it can lead to greater accuracy loss at higher compression ratios."], "score": 0.923828125}, {"id": "(Garg et al., 2025)", "paper": {"corpus_id": 278033481, "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Muskan Garg", "authorId": "2258141722"}, {"name": "Shaina Raza", "authorId": "2278330619"}, {"name": "Shebuti Rayana", "authorId": "3023076"}, {"name": "Xingyi Liu", "authorId": "2278394763"}, {"name": "Sunghwan Sohn", "authorId": "2267490593"}], "n_citations": 2}, "snippets": ["Structured pruning streamlines LLM by eliminating entire architectural components-such as neurons, filter channels, attention heads, or even layers-instead of removing individual parameters (Ma et al., 2023)[198]. By selectively pruning structurally coherent groups of parameters, this approach yields a smaller yet regularly structured model-maintaining dense connectivity while reducing inference costs, for instance, LLM-Pruner (Ma et al., 2023). Unstructured pruning removes individual weights from LLM without considering any specific structure within the model (Frantar et al., 2023). Unstructured pruning removes less important weights based on a threshold, resulting in a sparse model where the architecture remains unchanged, as long as enough weights stay to maintain performance. Unstructured pruning can deliver higher accuracy for a given compression ratio by allowing more targeted weight removal. However, its irregular sparsity pattern leads to inefficient memory access and limited speed-ups on standard hardware. To fully leverage the compression benefits, specialized libraries or hardware are needed, and even then, acceleration gains are modest unless the model is highly sparse. Post unstructured pruning, significant retraining or fine-tuning is often required to recover lost accuracy that can be resource-intensive."], "score": 0.822265625}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Feng et al., 2025)", "paper": {"corpus_id": 275993741, "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingkuan Feng", "authorId": "2332540527"}, {"name": "Jinyang Wu", "authorId": "2141911656"}, {"name": "Shuai Zhang", "authorId": "2298428469"}, {"name": "Pengpeng Shao", "authorId": "2221575807"}, {"name": "Ruihan Jin", "authorId": "2300370837"}, {"name": "Zhengqi Wen", "authorId": "1718662"}, {"name": "Jianhua Tao", "authorId": "2298423822"}, {"name": "Feihu Che", "authorId": "1471057495"}], "n_citations": 1}, "snippets": ["Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."], "score": 0.8154296875}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["As their size increases, Large Languages Models (LLMs) are natural candidates for network pruning methods: approaches that drop a subset of network weights while striving to preserve performance. Existing methods, however, require either retraining, which is rarely affordable for billion-scale LLMs, or solving a weight reconstruction problem reliant on second-order information, which may also be computationally expensive. In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update. Code is available at https://github.com/locuslab/wanda."], "score": 0.0}, {"id": "(Cheng et al., 2024)", "paper": {"corpus_id": 271217883, "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hongrong Cheng", "authorId": "2230189096"}, {"name": "Miao Zhang", "authorId": "2311635513"}, {"name": "J. Q. Shi", "authorId": "2262197686"}], "n_citations": 3}, "snippets": ["Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."], "score": 0.7734375}, {"id": "(Mugnaini et al., 2025)", "paper": {"corpus_id": 278208127, "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leandro Giusti Mugnaini", "authorId": "2331613426"}, {"name": "B. Yamamoto", "authorId": "2303405382"}, {"name": "Lucas Lauton de Alcantara", "authorId": "2358264111"}, {"name": "Victor Zacarias", "authorId": "2358264489"}, {"name": "Edson Bollis", "authorId": "2358263602"}, {"name": "Lucas F. A. O. Pellicer", "authorId": "2344249909"}, {"name": "A. H. R. Costa", "authorId": "2303454322"}, {"name": "Artur Jordao", "authorId": "2303404119"}], "n_citations": 1}, "snippets": ["Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17]."], "score": 0.7861328125}, {"id": "(Klein et al., 2024)", "paper": {"corpus_id": 269588232, "title": "Structural Pruning of Pre-trained Language Models via Neural Architecture Search", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Aaron Klein", "authorId": "2238461878"}, {"name": "Jacek Golebiowski", "authorId": "2238481381"}, {"name": "Xingchen Ma", "authorId": "2238532717"}, {"name": "Valerio Perrone", "authorId": "2299943113"}, {"name": "C\u00e9dric Archambeau", "authorId": "2262457089"}], "n_citations": 2}, "snippets": ["Unstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as the weight's magnitude, and removes weights with scores below a predetermined threshold. This approach often achieves high pruning rates with minimal performance degradation, but it also leads to sparse weight matrices, which are not well-supported by commonly used machine learning frameworks. Structured pruning (Michel et al., 2019;Sajjad et al., 2022) removes larger components of the network, such as layers or heads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunes entire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks and hardware."], "score": 0.82666015625}, {"id": "(Rostami et al., 2024)", "paper": {"corpus_id": 273963228, "title": "CULL-MT: Compression Using Language and Layer pruning for Machine Translation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pedram Rostami", "authorId": "2279336018"}, {"name": "M. Dousti", "authorId": "1702695"}], "n_citations": 1}, "snippets": ["While unstructured pruning allows for the sparsification of specific weights, it often necessitates specialized hardware for speed improve- ments (Cheng et al., 2023). In contrast, structural pruning (Molchanov et al., 2016) focuses on removing entire filters, blocks, or layers, providing efficiency that is not dependent on specific devices (Cheng et al., 2023)."], "score": 0.77880859375}, {"id": "(Cheng et al., 2023)", "paper": {"corpus_id": 260887757, "title": "A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Hongrong Cheng", "authorId": "2230189096"}, {"name": "Miao Zhang", "authorId": "2211872272"}, {"name": "Javen Qinfeng Shi", "authorId": "3177281"}], "n_citations": 154}, "snippets": ["Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and to accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. More than three thousand pruning papers have been published from 2020 to 2024. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of contrast settings for pruning (e.g., unstructured/structured, one-shot/iterative, data-free/data-driven, initialized/pre-trained weights, etc.) and explore several emerging topics, including pruning for large language models, vision transformers, diffusion models, and large multimodal models, post-training pruning, and different levels of supervision for pruning to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. Finally, we provide some valuable recommendations on selecting pruning methods and prospect several promising research directions for neural network pruning. To facilitate future research on deep neural network pruning, we summarize broad pruning applications (e.g., adversarial robustness, natural language understanding, etc.) and build a curated collection of datasets, networks, and evaluations on different applications. We maintain a repository on https://github.com/hrcheng1066/awesome-pruning that serves as a comprehensive resource for neural network pruning papers and corresponding open-source codes. We will keep updating this repository to include the latest advancements in the field."], "score": 0.0}], "table": null}, {"title": "Hardware Compatibility and Acceleration", "tldr": "Structured pruning offers immediate hardware efficiency gains on standard devices by physically reducing model dimensions, while unstructured pruning achieves higher compression but requires specialized hardware support to translate sparsity into actual speedups. Semi-structured approaches like N:M sparsity patterns attempt to balance compression benefits with hardware compatibility. (17 sources)", "text": "\nThe hardware compatibility of pruning methods represents a critical trade-off between theoretical compression and practical acceleration benefits. Structured pruning has a distinct advantage in hardware compatibility, as it removes entire architectural components (channels, heads, or layers), allowing models to be physically reshaped to smaller dimensions <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper>. This approach enables immediate computational efficiency gains on standard hardware without requiring specialized support <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52048008\" paperTitle=\"(He et al., 2018)\" isShortName></Paper>. The resulting dense matrices with reduced dimensions maintain regular memory access patterns that work efficiently with existing deep learning frameworks and hardware architectures <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>.\n\nIn contrast, unstructured pruning, while achieving higher compression ratios, faces significant hardware implementation challenges. The irregular sparsity patterns resulting from individual weight removal lead to inefficient memory access patterns that standard hardware cannot exploit <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. Several studies have demonstrated that directly applying sparse kernels to unstructured pruned models can lead to significant performance degradation\u2014up to 60\u00d7 slowdown on GPUs compared to dense kernels <Paper corpusId=\"246276158\" paperTitle=\"(Zhao et al., 2022)\" isShortName></Paper> <Paper corpusId=\"27494814\" paperTitle=\"(Zhu et al., 2017)\" isShortName></Paper>. Without specialized hardware or software support, the theoretical compression benefits of unstructured pruning do not translate into practical efficiency gains <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRecent hardware developments have attempted to address this gap. Semi-structured pruning approaches, such as N:M sparsity patterns where N out of every M consecutive weights are pruned, offer a middle ground <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. These patterns create more regular sparsity that can be more efficiently implemented in hardware <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. For example, 2:4 sparse networks can achieve 2\u00d7 speedup on specialized GPUs like NVIDIA A100s that support this specific pattern <Paper corpusId=\"233296249\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper>. However, these semi-structured approaches still require specialized hardware support and may not deliver the same level of acceleration as structured pruning at equivalent sparsity levels <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>.\n\nThe hardware compatibility gap has significant implications for deployment scenarios. For resource-constrained devices or general deployment where specialized hardware is unavailable, structured pruning remains the most practical approach despite its lower compression potential <Paper corpusId=\"256662734\" paperTitle=\"(Santacroce et al., 2023)\" isShortName></Paper>. As noted by multiple researchers, structured pruning is particularly advantageous for deployment on resource-constrained devices or for providing fast services based on LLMs <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\nEmpirical measurements confirm these theoretical distinctions. Several studies report concrete speedups from structured pruning\u2014ranging from 1.19\u00d7 to 1.87\u00d7 faster inference compared to dense models on standard hardware <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. In contrast, unstructured pruning methods like SparseGPT, while achieving up to 60% parameter reduction, require specialized support to translate this into computational gains <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. For deployment scenarios requiring immediate efficiency gains without specialized hardware, structured pruning remains the preferred approach <Paper corpusId=\"276259010\" paperTitle=\"(Tang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Kurtic et al., 2023)", "paper": {"corpus_id": 256662263, "title": "ZipLM: Inference-Aware Structured Pruning of Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Eldar Kurtic", "authorId": "40992614"}, {"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 26}, "snippets": ["The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."], "score": 0.90576171875}, {"id": "(Zhao et al., 2022)", "paper": {"corpus_id": 246276158, "title": "Iterative Activation-based Structured Pruning", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Kaiqi Zhao", "authorId": "1995855"}, {"name": "Animesh Jain", "authorId": "101682296"}, {"name": "Ming Zhao", "authorId": "2152527896"}], "n_citations": 0}, "snippets": ["Unstructured pruning is a fine-grained approach that prunes individual unimportant elements in weight tensors. It allows better accuracy with higher memory footprint reduction, compared to structured pruning, as it selectively prunes unimportant elements from a weight filter while retaining the important elements. However, unstructured pruning is hardware-inefficient because it is difficult to map random/indirect memory in the pruned weight representation on general-purpose hardware platforms like Intel/ARM CPU vector units or NVDIA GPU CUDA cores (He et al., 2018)), e.g, (Hill et al., 2017) shows directly applying NVDIA cuS-PARSE on unstructured pruned models can lead to 60\u015d lowdown on GPU compared to dense kernels. \n\nOn the other hand, structured pruning is a coarse-grained approach that prunes entire regular regions of weight tensors (e.g., filters or output channels) (Han, Mao, and Dally 2015). It tends to cause higher drop in accuracy than unstructured pruning because by removing entire regions, it might remove weight elements that are important to the final accuracy (Li et al. 2016). However, structurally pruned models can be mapped easily to general-purpose hardware and accelerated directly with off-the-shelf hardware and libraries (He et al., 2018)."], "score": 0.89111328125}, {"id": "(He et al., 2018)", "paper": {"corpus_id": 52048008, "title": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices", "year": 2018, "venue": "European Conference on Computer Vision", "authors": [{"name": "Yihui He", "authorId": "39838894"}, {"name": "Ji Lin", "authorId": "46698300"}, {"name": "Zhijian Liu", "authorId": "47781592"}, {"name": "Hanrui Wang", "authorId": "35446689"}, {"name": "Li-Jia Li", "authorId": "2040091191"}, {"name": "Song Han", "authorId": "143840275"}], "n_citations": 1349}, "snippets": ["Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy."], "score": 0.0}, {"id": "(Cheng et al., 2024)", "paper": {"corpus_id": 271217883, "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hongrong Cheng", "authorId": "2230189096"}, {"name": "Miao Zhang", "authorId": "2311635513"}, {"name": "J. Q. Shi", "authorId": "2262197686"}], "n_citations": 3}, "snippets": ["Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."], "score": 0.7734375}, {"id": "(Gong et al., 2024)", "paper": {"corpus_id": 267103825, "title": "A Review of Neural Network Lightweighting Techniques", "year": 2024, "venue": "Innovation &amp; Technology Advances", "authors": [{"name": "Ziyi Gong", "authorId": "2280410840"}, {"name": "Huifu Zhang", "authorId": "2280726529"}, {"name": "Hao Yang", "authorId": "2280414619"}, {"name": "Fangjun Liu", "authorId": "2280959623"}, {"name": "Fan Luo", "authorId": "2280411835"}], "n_citations": 0}, "snippets": ["By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."], "score": 0.8623046875}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 272368391, "title": "On-Device Language Models: A Comprehensive Review", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiajun Xu", "authorId": "2316519813"}, {"name": "Zhiyuan Li", "authorId": "2294674012"}, {"name": "Wei Chen", "authorId": "2294845809"}, {"name": "Qun Wang", "authorId": "2316514278"}, {"name": "Xin Gao", "authorId": "2319809164"}, {"name": "Qi Cai", "authorId": "2364055424"}, {"name": "Ziyuan Ling", "authorId": "2319410023"}], "n_citations": 35}, "snippets": ["Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation."], "score": 0.85986328125}, {"id": "(Zhu et al., 2017)", "paper": {"corpus_id": 27494814, "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "year": 2017, "venue": "International Conference on Learning Representations", "authors": [{"name": "Michael Zhu", "authorId": "2152183723"}, {"name": "Suyog Gupta", "authorId": "2116011472"}], "n_citations": 1281}, "snippets": ["Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 267412232, "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models", "year": 2024, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Xindi Wang", "authorId": "2108048327"}, {"name": "Mahsa Salmani", "authorId": "1904419"}, {"name": "Parsa Omidi", "authorId": "2282534833"}, {"name": "Xiangyu Ren", "authorId": "2283447900"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "A. Eshaghi", "authorId": "50782111"}], "n_citations": 45}, "snippets": ["In general, pruning a model can be categorized into structured and unstructured pruning. \n\nStructured pruning aims at removing higher-granularity structures, such as entire neurons, layers, or rows/columns of weight matrices, which can result in a model that retains its original structure but with fewer number of parameters.\n\nUnstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models."], "score": 0.83740234375}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 272910976, "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Hongxu Yin", "authorId": "1989015"}, {"name": "Saurav Muralidharan", "authorId": "31225166"}, {"name": "Greg Heinrich", "authorId": "2273650910"}, {"name": "Jeff Pool", "authorId": "2322991313"}, {"name": "Jan Kautz", "authorId": "2273651410"}, {"name": "Pavlo Molchanov", "authorId": "2824500"}, {"name": "Xinchao Wang", "authorId": "2322993154"}], "n_citations": 10}, "snippets": ["According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."], "score": 0.8720703125}, {"id": "(Mishra et al., 2021)", "paper": {"corpus_id": 233296249, "title": "Accelerating Sparse Deep Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Asit K. Mishra", "authorId": "35769149"}, {"name": "J. Latorre", "authorId": "2060797517"}, {"name": "Jeff Pool", "authorId": "47325862"}, {"name": "Darko Stosic", "authorId": "33749574"}, {"name": "D. Stosic", "authorId": "2737605"}, {"name": "Ganesh Venkatesh", "authorId": "145595812"}, {"name": "Chong Yu", "authorId": "2116145799"}, {"name": "P. Micikevicius", "authorId": "1802359"}], "n_citations": 235}, "snippets": ["As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores."], "score": 0.0}, {"id": "(Hu et al., 2025)", "paper": {"corpus_id": 278338989, "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hanyu Hu", "authorId": "2283405371"}, {"name": "Xiaoming Yuan", "authorId": "2283433779"}], "n_citations": 0}, "snippets": ["While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."], "score": 0.93359375}, {"id": "(Santacroce et al., 2023)", "paper": {"corpus_id": 256662734, "title": "What Matters In The Structured Pruning of Generative Language Models?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Michael Santacroce", "authorId": "1413038175"}, {"name": "Zixin Wen", "authorId": "2051054583"}, {"name": "Yelong Shen", "authorId": "1752875"}, {"name": "Yuan-Fang Li", "authorId": "152244300"}], "n_citations": 34}, "snippets": ["Unstructured pruning removes individual weights from the network based on some criteria, resulting in sparse weight matrices that can be stored and processed more efficiently. Structured pruning, on the other hand, eliminates whole components, such as neurons, channels, or blocks, leading to smaller architectures to reduce end-to-end inference latency. While unstructured pruning has been extensively studied and applied to LLMs (Wang et al., 2019)Xu et al., 2021;Zafrir et al., 2021;Li et al., 2022), structured pruning is more challenging and less explored. However, structured pruning is also more desirable in many practical scenarios, such as deploying these models on resource-constrained devices or providing fast services based on LLMs."], "score": 0.77490234375}, {"id": "(Feng et al., 2025)", "paper": {"corpus_id": 275993741, "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingkuan Feng", "authorId": "2332540527"}, {"name": "Jinyang Wu", "authorId": "2141911656"}, {"name": "Shuai Zhang", "authorId": "2298428469"}, {"name": "Pengpeng Shao", "authorId": "2221575807"}, {"name": "Ruihan Jin", "authorId": "2300370837"}, {"name": "Zhengqi Wen", "authorId": "1718662"}, {"name": "Jianhua Tao", "authorId": "2298423822"}, {"name": "Feihu Che", "authorId": "1471057495"}], "n_citations": 1}, "snippets": ["Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."], "score": 0.8154296875}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 278367624, "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yanbiao Liang", "authorId": "2325996847"}, {"name": "Huihong Shi", "authorId": "30984015"}, {"name": "Haikuo Shao", "authorId": "2146641923"}, {"name": "Zhongfeng Wang", "authorId": "2275509567"}], "n_citations": 0}, "snippets": ["Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021)."], "score": 0.939453125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271909421, "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Guanchen Li", "authorId": "2301331844"}, {"name": "Xiandong Zhao", "authorId": "2270847262"}, {"name": "Lian Liu", "authorId": "2316517251"}, {"name": "Zeping Li", "authorId": "2307589652"}, {"name": "Dong Li", "authorId": "2279335698"}, {"name": "Lu Tian", "authorId": "2279539118"}, {"name": "Jie He", "authorId": "2316522396"}, {"name": "Ashish Sirasao", "authorId": "2316484957"}, {"name": "E. Barsoum", "authorId": "2271751612"}], "n_citations": 1}, "snippets": ["Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]", "Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts."], "score": 0.77734375}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Tang et al., 2025)", "paper": {"corpus_id": 276259010, "title": "DarwinLM: Evolutionary Structured Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shengkun Tang", "authorId": "2331680259"}, {"name": "Oliver Sieberling", "authorId": "2326834165"}, {"name": "Eldar Kurtic", "authorId": "40992614"}, {"name": "Zhiqiang Shen", "authorId": "2344981603"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 3}, "snippets": ["Unlike unstructured pruning (Frantar & Alistarh, 2023), the model produced by structured pruning can be accelerated on mainstream hardware without any specific design for computation."], "score": 0.779296875}], "table": null}, {"title": "Semi-Structured Pruning as a Middle Ground", "tldr": "Semi-structured pruning approaches like N:M sparsity patterns attempt to balance the compression benefits of unstructured pruning with the hardware compatibility of structured pruning. These methods enforce regular sparsity patterns where N out of every M consecutive weights are pruned, enabling acceleration on specialized hardware while maintaining better accuracy than fully structured approaches. (13 sources)", "text": "\nSemi-structured pruning has emerged as a compromise between the high compression ratios of unstructured pruning and the immediate hardware efficiency of structured pruning. This approach implements specific regular patterns of sparsity, most commonly N:M patterns where N out of every M consecutive weights are pruned <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. The 2:4 sparsity pattern, where 2 out of every 4 consecutive weights are retained, has gained particular prominence due to hardware support in NVIDIA's Ampere GPU architecture <Paper corpusId=\"233296249\" paperTitle=\"(Mishra et al., 2021)\" isShortName></Paper> <Paper corpusId=\"278367624\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\nThe key advantage of semi-structured pruning is that it creates more regular sparsity patterns that can be efficiently implemented in specialized hardware <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. For instance, 2:4 sparse networks can achieve up to 2\u00d7 speedup on NVIDIA A100 GPUs that are specifically designed to support these patterns <Paper corpusId=\"271217883\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"231847094\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>. This approach provides a balance between compression benefits and hardware compatibility that neither fully structured nor unstructured methods can achieve alone <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nIn terms of performance preservation, semi-structured pruning maintains better accuracy than fully structured methods while still enabling some degree of hardware acceleration <Paper corpusId=\"270621063\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. Methods like SparseGPT have demonstrated that semi-structured pruning patterns (2:4 and 4:8) can be effectively applied to large language models with minimal performance degradation <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This approach harmonizes \"the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity\" <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nHowever, semi-structured pruning still has limitations. It requires specialized hardware support to realize practical speedups, making it less suitable for deployment on consumer-grade GPUs or general hardware <Paper corpusId=\"278208127\" paperTitle=\"(Mugnaini et al., 2025)\" isShortName></Paper>. Some empirical studies suggest that semi-structured pruning may underperform structured methods in inference acceleration at equivalent sparsity levels <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>. Additionally, while semi-structured approaches can achieve computational speedups, they typically do not reduce the model's memory footprint as structured pruning does <Paper corpusId=\"274981759\" paperTitle=\"(Sy et al., 2024)\" isShortName></Paper>.\n\nThe choice between pruning approaches ultimately depends on the specific deployment scenario and hardware availability. Semi-structured pruning represents a valuable middle ground for cases where specialized hardware support is available and both compression ratio and performance preservation are priorities <Paper corpusId=\"278501529\" paperTitle=\"(Laborde et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271909421\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Fang et al., 2024)", "paper": {"corpus_id": 272910976, "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Hongxu Yin", "authorId": "1989015"}, {"name": "Saurav Muralidharan", "authorId": "31225166"}, {"name": "Greg Heinrich", "authorId": "2273650910"}, {"name": "Jeff Pool", "authorId": "2322991313"}, {"name": "Jan Kautz", "authorId": "2273651410"}, {"name": "Pavlo Molchanov", "authorId": "2824500"}, {"name": "Xinchao Wang", "authorId": "2322993154"}], "n_citations": 10}, "snippets": ["According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."], "score": 0.8720703125}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 278367624, "title": "AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yanbiao Liang", "authorId": "2325996847"}, {"name": "Huihong Shi", "authorId": "30984015"}, {"name": "Haikuo Shao", "authorId": "2146641923"}, {"name": "Zhongfeng Wang", "authorId": "2275509567"}], "n_citations": 0}, "snippets": ["Pruning boosts model compactness at the architectural level by removing redundant parameters, ranging from individual weights (unstructured pruning (Shao et al., 2023)- (Xu et al., 2024)) to entire channels or layers (structured pruning (Ma et al., 2023)- (Li et al., 2023)). Although unstructured pruning can achieve significant compression ratios, the resulting irregular sparsity is not conducive to hardware implementation (Molchanov et al., 2016). In contrast, structured pruning is more compatible with hardware acceleration but often results in model accuracy degradation and limited sparsity (He et al., 2017). To balance model accuracy and hardware efficiency, N :M semi-structured pruning (Frantar et al., 2023), (Sun et al., 2023), where N out of every M elements are pruned, is commonly adopted in prevalent LLMs (Zeng et al., 2024), (Mishra et al., 2021)."], "score": 0.939453125}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Mishra et al., 2021)", "paper": {"corpus_id": 233296249, "title": "Accelerating Sparse Deep Neural Networks", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Asit K. Mishra", "authorId": "35769149"}, {"name": "J. Latorre", "authorId": "2060797517"}, {"name": "Jeff Pool", "authorId": "47325862"}, {"name": "Darko Stosic", "authorId": "33749574"}, {"name": "D. Stosic", "authorId": "2737605"}, {"name": "Ganesh Venkatesh", "authorId": "145595812"}, {"name": "Chong Yu", "authorId": "2116145799"}, {"name": "P. Micikevicius", "authorId": "1802359"}], "n_citations": 235}, "snippets": ["As neural network model sizes have dramatically increased, so has the interest in various techniques to reduce their parameter counts and accelerate their execution. An active area of research in this field is sparsity - encouraging zero values in parameters that can then be discarded from storage or computations. While most research focuses on high levels of sparsity, there are challenges in universally maintaining model accuracy as well as achieving significant speedups over modern matrix-math hardware. To make sparsity adoption practical, the NVIDIA Ampere GPU architecture introduces sparsity support in its matrix-math units, Tensor Cores. We present the design and behavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern that leads to twice the math throughput of dense matrix units. We also describe a simple workflow for training networks that both satisfy 2:4 sparsity pattern requirements and maintain accuracy, verifying it on a wide range of common tasks and model architectures. This workflow makes it easy to prepare accurate models for efficient deployment on Sparse Tensor Cores."], "score": 0.0}, {"id": "(Cheng et al., 2024)", "paper": {"corpus_id": 271217883, "title": "MINI-LLM: Memory-Efficient Structured Pruning for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hongrong Cheng", "authorId": "2230189096"}, {"name": "Miao Zhang", "authorId": "2311635513"}, {"name": "J. Q. Shi", "authorId": "2262197686"}], "n_citations": 3}, "snippets": ["Unstructured pruning achieves substantial sparsity by directly setting weights or their masks to zero while maintaining a comparable performance compared to the vanilla models. However, the irregular sparsity results in no compression in the model size, and actual acceleration necessitates the support of specialized software/hardware. In contrast, structured pruning discards the whole grouped parameters (such as channels and attention heads), leading to physically reduced model size and enabling inference acceleration without any special requirements of software/hardware ([Zhou et al., 2022;Frantar and Alistarh, 2023]). Semi-structure pruning, such as 2:4 or 4:8 patterns in [Frantar and Alistarh, 2023], provides a balance between performance and hardware speedup."], "score": 0.7734375}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 231847094, "title": "Learning N: M Fine-grained Structured Sparse Neural Networks From Scratch", "year": 2021, "venue": "International Conference on Learning Representations", "authors": [{"name": "Aojun Zhou", "authorId": "9548994"}, {"name": "Yukun Ma", "authorId": "2289831903"}, {"name": "Junnan Zhu", "authorId": "24925751"}, {"name": "Jianbo Liu", "authorId": "2124809722"}, {"name": "Zhijie Zhang", "authorId": "1490508571"}, {"name": "Kun Yuan", "authorId": "50492964"}, {"name": "Wenxiu Sun", "authorId": "8397576"}, {"name": "Hongsheng Li", "authorId": "47893312"}], "n_citations": 248}, "snippets": ["Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured fine-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarse-grained sparsity which prunes blocks of sub-networks of a neural network. Fine-grained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot simultaneously achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the first to study training from scratch an N:M fine-grained structured sparse network, which can maintain the advantages of both unstructured fine-grained sparsity and structured coarse-grained sparsity simultaneously on specifically designed GPUs. Specifically, a 2 : 4 sparse network could achieve 2\u00d7 speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-refined straight-through estimator (SR-STE), to alleviate the negative influence of the approximated gradients computed by vanilla STE during optimization. We also define a metric, Sparse Architecture Divergence (SAD), to measure the sparse network\u2019s topology change during the training process. Finally, We justify SR-STE\u2019s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Anonymous code and model will be at available at https://github.com/anonymous-NM-sparsity/NM-sparsity."], "score": 0.0}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 271744772, "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pengxiang Zhao", "authorId": "2114873379"}, {"name": "Hanyu Hu", "authorId": "2283405371"}, {"name": "Ping Li", "authorId": "2293474362"}, {"name": "Yi Zheng", "authorId": "2293396844"}, {"name": "Zhefeng Wang", "authorId": "2293231668"}, {"name": "Xiaoming Yuan", "authorId": "2283433779"}], "n_citations": 1}, "snippets": ["For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation."], "score": 0.76611328125}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 270621063, "title": "Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Zhiyu Guo", "authorId": "2300138950"}, {"name": "Hidetaka Kamigaito", "authorId": "2300756"}, {"name": "Taro Wanatnabe", "authorId": "2299941873"}], "n_citations": 1}, "snippets": ["Neural network pruning in LLM can be broadly categorized into two groups: structured pruning (Ma et al., 2023)Zhang et al., 2023) and unstructured pruning (Frantar & Alistarh, 2023;(Sun et al., 2023). (Ma et al., 2023) proposes a dependency detection algorithm to detect and prune non-critical grouped structures followed by LoRA fine-tuning. Although structured pruning can usually achieve better hardware efficiency, the accuracy drops a lot even at a low compression rate. Unstructured pruning can yield a higher compression rate and achieve acceleration on Nvidia's GPUs by employing a hardware-friendly N:M sparsity pattern."], "score": 0.76953125}, {"id": "(Mugnaini et al., 2025)", "paper": {"corpus_id": 278208127, "title": "Efficient LLMs with AMP: Attention Heads and MLP Pruning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leandro Giusti Mugnaini", "authorId": "2331613426"}, {"name": "B. Yamamoto", "authorId": "2303405382"}, {"name": "Lucas Lauton de Alcantara", "authorId": "2358264111"}, {"name": "Victor Zacarias", "authorId": "2358264489"}, {"name": "Edson Bollis", "authorId": "2358263602"}, {"name": "Lucas F. A. O. Pellicer", "authorId": "2344249909"}, {"name": "A. H. R. Costa", "authorId": "2303454322"}, {"name": "Artur Jordao", "authorId": "2303404119"}], "n_citations": 1}, "snippets": ["Within the field of LLMs, pruning techniques fall into three main categories: structured, semistructured and unstructured pruning.\n\nStructured pruning removes entire components -such as attention heads or layers -while preserving the overall network structure without introducing sparsity (i.e., without zeroing out a significant portion of the model's parameters) [6]. However, the removal of larger and potentially more critical components may result in performance degradation, typically requiring Parameter-Efficient Fine-Tuning (PEFT) techniques for performance recovery [14]. Due to the removal of complete components, structured pruning usually achieves inference acceleration and memory reduction without the need for specialized hardware or software [15].\n\nSemi-structured (a.k.a. structured N : M ) pruning promotes model sparsity by removing groups of consecutive parameters following a pruning mask [16]. Specifically, structured N : M sparsity requires that at least N out of every M consecutive weights be non-zero [17], [18]. While this is a promising technique, it requires specialized hardware to achieve practical speedup, making it less suitable for deployment on consumergrade GPUs [17]."], "score": 0.7861328125}, {"id": "(Hu et al., 2025)", "paper": {"corpus_id": 278338989, "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hanyu Hu", "authorId": "2283405371"}, {"name": "Xiaoming Yuan", "authorId": "2283433779"}], "n_citations": 0}, "snippets": ["While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."], "score": 0.93359375}, {"id": "(Sy et al., 2024)", "paper": {"corpus_id": 274981759, "title": "Lillama: Large Language Models Compression via Low-Rank Feature Distillation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yaya Sy", "authorId": "2336865596"}, {"name": "Christophe Cerisara", "authorId": "2257152061"}, {"name": "I. Illina", "authorId": "1696945"}], "n_citations": 0}, "snippets": ["Structured Pruning removes entire groups of parameters, which results in a smaller and faster model (Xia et al., 2024;Ma et al., 2023). Ma et al. (2023) propose a new gradient-based criterion to eliminate substructures in LLMs, while Xia et al. (2024) use a joint loss combining a pruning mask loss with the language modeling loss. However, optimizing these criteria can be computationally intensive. For example, the pruning step of Sheared-LLaMA (Xia et al., 2023) is 5x expensive compared to standard LM training, according to the authors. In contrast, thanks to the local gradient updates, our approach is computationally efficient, allowing us to compress a 47B model within minutes on a single A100 GPU. Regarding unstructured pruning, these methods do not provide any gains in terms of memory or speedup, at least with current algorithmic implementations. Semi-structured pruning (e.g., 2:4 and 4:8) (Sun et al., 2024;Frantar and Alistarh, 2023;Liu et al., 2024) does not lead to memory gain but can speed up processing on kernels optimized for such matrix structures. On the other hand, our method, which directly shrinks matrices, saves memory across all hardware and leads to speed up, as fewer computations are performed."], "score": 0.80615234375}, {"id": "(Laborde et al., 2025)", "paper": {"corpus_id": 278501529, "title": "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?", "year": 2025, "venue": "", "authors": [{"name": "Stanislas Laborde", "authorId": "2360373404"}, {"name": "Martin Cousseau", "authorId": "2360359994"}, {"name": "Antoun Yaacoub", "authorId": "40605834"}, {"name": "Lionel Prevost", "authorId": "2266474578"}], "n_citations": 0}, "snippets": ["Unstructured pruning offers maximum theoretical compression by removing individual weights, but often results in irregular sparsity patterns that are challenging to accelerate on current hardware. Semi-structured approaches, like N:M sparsity patterns (Zhou et al., 2021), balance compression rates with hardware efficiency by enforcing regular pruning patterns, where N out of every M consecutive weights are pruned. Structured pruning (Ma et al., 2023) takes this further by removing entire structures, channels, or attention heads, with recent work showing that up to 50% of attention layers in large models can be removed while preserving performance [12]."], "score": 0.89892578125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271909421, "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Guanchen Li", "authorId": "2301331844"}, {"name": "Xiandong Zhao", "authorId": "2270847262"}, {"name": "Lian Liu", "authorId": "2316517251"}, {"name": "Zeping Li", "authorId": "2307589652"}, {"name": "Dong Li", "authorId": "2279335698"}, {"name": "Lu Tian", "authorId": "2279539118"}, {"name": "Jie He", "authorId": "2316522396"}, {"name": "Ashish Sirasao", "authorId": "2316484957"}, {"name": "E. Barsoum", "authorId": "2271751612"}], "n_citations": 1}, "snippets": ["Pruning methods can be broadly classified into structured and unstructured approaches. Structured pruning is more hardware-friendly, as it directly prunes entire segments of weights, thereby removing consecutive computations [17,(Liu et al., 2023). Unstructured pruning (sparsification) is also receiving interest, particularly as hardware advancements increasingly support the acceleration of sparse patterns such as 2:4 or 4:8 sparse [20]", "Table 7 indicates that the pruned models can achieve 1.19x \u223c 1.87x speedup compared to their dense counterparts."], "score": 0.77734375}], "table": null}, {"title": "Recent Methods and Techniques", "tldr": "Recent pruning methods include both structured approaches like LLM-Pruner and SliceGPT that remove entire components for hardware efficiency, and unstructured approaches like SparseGPT and Wanda that target individual weights for higher compression. Semi-structured methods like N:M sparsity patterns attempt to balance the benefits of both approaches. (14 sources)", "text": "\n## Structured Pruning Methods\n- **LLM-Pruner**: Employs gradient-based structured pruning to selectively remove non-critical coupled structures while preserving the model's core functionality. Performance recovery is achieved through LoRA fine-tuning in just 3 hours with only 50K samples. <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"272368391\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- **SliceGPT**: Implements a post-training structured pruning scheme that replaces each weight matrix with a smaller dense matrix, reducing the embedding dimension. Can remove up to 25% of parameters while maintaining 99% zero-shot performance for LLAMA2-70B and OPT-66B models. <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al._1, 2025)\" isShortName></Paper>\n\n- **ZipLM**: Adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Achieves state-of-the-art accuracy-vs-speedup trade-offs while matching desired target runtime speedups in any inference environment. <Paper corpusId=\"256662263\" paperTitle=\"(Kurtic et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\n- **CoFi**: A task-specific structured pruning method that jointly prunes coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules. Delivers highly parallelizable subnetworks without resorting to unlabeled data. <Paper corpusId=\"247922354\" paperTitle=\"(Xia et al., 2022)\" isShortName></Paper> <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>\n\n## Unstructured Pruning Methods\n- **SparseGPT**: Demonstrates that large-scale GPT-family models can be pruned to at least 50% sparsity in one shot without retraining, with minimal accuracy loss. Can be executed on models as large as OPT-175B and BLOOM-176B in under 4.5 hours and is compatible with semi-structured patterns and weight quantization. <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"267103825\" paperTitle=\"(Gong et al., 2024)\" isShortName></Paper>\n\n- **Wanda**: Implements a heuristic approach that removes weights based on the product of their magnitudes and activations without compensation. Achieves 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks. <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278338989\" paperTitle=\"(Hu et al., 2025)\" isShortName></Paper>\n\n- **ISC (Improved Sparse Compression)**: Leverages the OBS framework to calculate saliency for each weight using the inverse Hessian of the loss metric. This approach generates pruning masks and updates weights to maintain performance at high sparsity levels. <Paper corpusId=\"264146174\" paperTitle=\"(Shao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\n## Semi-Structured Pruning Methods\n- **N:M Sparsity Patterns**: Semi-structured approaches that enforce regular sparsity patterns where N out of every M consecutive weights are retained. The 2:4 sparsity pattern is particularly notable for its hardware support in NVIDIA's Ampere architecture, enabling up to 2\u00d7 speedup on A100 GPUs. <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"278310893\" paperTitle=\"(Sengupta et al._1, 2025)\" isShortName></Paper>\n\n- **FISTA-Pruner**: Focuses on unstructured and semi-structured pruning, enabling further model compression and is orthogonal to structured pruning methods. Compatible with different sparsity patterns to balance accuracy and hardware efficiency. <Paper corpusId=\"271744772\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>\n\nThe research trend shows increasing interest in developing pruning methods that balance theoretical compression benefits with practical hardware acceleration. While unstructured pruning techniques like SparseGPT continue to achieve impressive compression ratios, structured approaches like LLM-Pruner are gaining popularity due to their immediate hardware efficiency benefits without requiring specialized support <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper> <Paper corpusId=\"275993741\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper>. Semi-structured approaches serve as a middle ground, offering more regular sparsity patterns that can be efficiently implemented in specialized hardware <Paper corpusId=\"272910976\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. The code is available at: https://github.com/horseee/LLM-Pruner"], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 272368391, "title": "On-Device Language Models: A Comprehensive Review", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiajun Xu", "authorId": "2316519813"}, {"name": "Zhiyuan Li", "authorId": "2294674012"}, {"name": "Wei Chen", "authorId": "2294845809"}, {"name": "Qun Wang", "authorId": "2316514278"}, {"name": "Xin Gao", "authorId": "2319809164"}, {"name": "Qi Cai", "authorId": "2364055424"}, {"name": "Ziyuan Ling", "authorId": "2319410023"}], "n_citations": 35}, "snippets": ["Structured Pruning: This approach removes entire subsets of parameters like layers, channels, or filters, which is beneficial for hardware optimization due to more regular memory access patterns and simplified computations. The 'LLM-Pruner' (Kaddour et al., 2023) employs structured pruning to eliminate non-essential groups based on gradient data, thus maintaining critical functionalities. It also facilitates performance recovery through techniques such as LoRA, allowing efficient restoration with minimal data. 2. Unstructured Pruning: Unlike structured pruning, unstructured pruning removes individual weights across the model, offering finer granularity and potentially higher compression rates (Li et al., 2023a). However, this method typically results in sparse matrices, which can be less compatible with traditional hardware architectures, compromising computational efficiency. It is most suitable where maximum compression is needed without constraints on structural preservation."], "score": 0.85986328125}, {"id": "(Ashkboos et al., 2024)", "paper": {"corpus_id": 267301573, "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Saleh Ashkboos", "authorId": "9543395"}, {"name": "Maximilian L. Croci", "authorId": "2008063761"}, {"name": "Marcelo Gennari do Nascimento", "authorId": "2281641743"}, {"name": "Torsten Hoefler", "authorId": "2258547286"}, {"name": "James Hensman", "authorId": "2266803418"}], "n_citations": 184}, "snippets": ["Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"], "score": 0.0}, {"id": "(Sengupta et al._1, 2025)", "paper": {"corpus_id": 278310893, "title": "Position: Enough of Scaling LLMs! Lets Focus on Downscaling", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["Unstructured pruning (Sun et al., 2023) removes individual weights, producing sparse matrices that maintain performance but are less hardwareefficient. Semi-structured pruning (Frantar & Alistarh, 2023), such as the 2:4 sparsity pattern (Pool et al., 2021), introduces a hardware-friendly structured sparsity that accelerates computation. Structured pruning (Ashkboos et al., 2024;Yuan et al., 2023;Sengupta et al., 2025) takes a broader approach by removing entire components, such as Transformer layers (depth pruning) (Fan et al., 2019) or reducing embedding dimensions and attention heads (width pruning) (Zhu et al., 2021)."], "score": 0.78271484375}, {"id": "(Kurtic et al., 2023)", "paper": {"corpus_id": 256662263, "title": "ZipLM: Inference-Aware Structured Pruning of Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Eldar Kurtic", "authorId": "40992614"}, {"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 26}, "snippets": ["The high accuracy of modern language models from the Transformer family [1] comes at the price of massive computational cost, which hinders their practical adoption in resource-constrained settings. This has motivated the development of model compression techniques, which can be categorized into pruning (Hoefler et al., 2021), quantization [3], and distillation (Gou et al., 2020). In this paper, we focus on structural compression, whose goal is to reduce model size by removing entire sub-components, such as rows or columns from the model's weight matrices. The key advantage of structured pruning, relative to unstructured pruning of individual weights, is that the model can be reshaped to new dimensions, and the resulting computational savings can be leveraged on any hardware, without specialized computational support. At the same time, structured pruning introduces significant challenges. First, models are usually highly-sensitive to structured compression, and most methods require gradual compression, including retraining cycles designed to allow the model to recover accuracy."], "score": 0.90576171875}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 271744772, "title": "A Convex-optimization-based Layer-wise Post-training Pruner for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pengxiang Zhao", "authorId": "2114873379"}, {"name": "Hanyu Hu", "authorId": "2283405371"}, {"name": "Ping Li", "authorId": "2293474362"}, {"name": "Yi Zheng", "authorId": "2293396844"}, {"name": "Zhefeng Wang", "authorId": "2293231668"}, {"name": "Xiaoming Yuan", "authorId": "2283433779"}], "n_citations": 1}, "snippets": ["For structured pruning, SliceGPT (Ashkboos et al., 2024) and Eigenpruning (Vergara-Browne et al., 2024) utilize singular value decompositions to prune singular values of weight matrices and reduce model dimensions. ZipLM (Kurtic et al., 2023) adopts an OBS-based approach for structured pruning and updates remaining weights to maintain performance. Our proposed FISTAPruner focuses on unstructured and semi-structured pruning, and thus is orthogonal to these structured pruning methods, enabling further model compression. For unstructured and semi-structured pruning, SparseGPT (Frantar et al., 2023) and ISC (Shao et al., 2023) leverage the OBS framework to calculate saliency for each entry using the inverse Hessian of the loss metric, based on which pruning masks are generated and weights updated. Wanda (Sun et al., 2023) implements a heuristic approach, removing weights based on the product of their magnitudes and activations without compensation."], "score": 0.76611328125}, {"id": "(Xia et al., 2022)", "paper": {"corpus_id": 247922354, "title": "Structured Pruning Learns Compact and Accurate Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mengzhou Xia", "authorId": "67284811"}, {"name": "Zexuan Zhong", "authorId": "49164966"}, {"name": "Danqi Chen", "authorId": "50536468"}], "n_citations": 187}, "snippets": ["The growing size of neural language models has led to increased attention in model compression. The two predominant approaches are pruning, which gradually removes weights from a pre-trained model, and distillation, which trains a smaller compact model to match a larger one. Pruning methods can significantly reduce the model size but hardly achieve large speedups as distillation. However, distillation methods require large amounts of unlabeled data and are expensive to train. In this work, we propose a task-specific structured pruning method CoFi (Coarse- and Fine-grained Pruning), which delivers highly parallelizable subnetworks and matches the distillation methods in both accuracy and latency, without resorting to any unlabeled data. Our key insight is to jointly prune coarse-grained (e.g., layers) and fine-grained (e.g., heads and hidden units) modules, which controls the pruning decision of each parameter with masks of different granularity. We also devise a layerwise distillation strategy to transfer knowledge from unpruned to pruned models during optimization. Our experiments on GLUE and SQuAD datasets show that CoFi yields models with over 10X speedups with a small accuracy drop, showing its effectiveness and efficiency compared to previous pruning and distillation approaches."], "score": 0.0}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 272910976, "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Hongxu Yin", "authorId": "1989015"}, {"name": "Saurav Muralidharan", "authorId": "31225166"}, {"name": "Greg Heinrich", "authorId": "2273650910"}, {"name": "Jeff Pool", "authorId": "2322991313"}, {"name": "Jan Kautz", "authorId": "2273651410"}, {"name": "Pavlo Molchanov", "authorId": "2824500"}, {"name": "Xinchao Wang", "authorId": "2322993154"}], "n_citations": 10}, "snippets": ["According to the granularity of pruning, existing methods can be classified into three categories: Structured Pruning (Ma et al., 2023)43,(Liu et al., 2023), Unstructured Pruning (Han et al., 2015)[15], and Semi-Structured Pruning (Frantar et al., 2023)38,29,32,(Pool et al., 2021). Structured pruning physically eliminates substructures like attention heads (Ma et al., 2023), embeddings or depth [43] in the model, facilitating acceleration independent of specialized hardware or software infrastructure [32]. However, structured approaches typically necessitate huge retraining efforts to recover network quality due to coarse removal of parameters (Ma et al., 2023)43,27,2]. Conversely, unstructured methods aim to find a sparse model by zeroing out parameters in LLMs, which is characterized by its flexibility and minimal detrimental effect on LLMs' accuracy (Frantar et al., 2023)38,20,42,44]. The acceleration of sparse models is typically impeded by the irregular nature of the resulting sparse patterns, presenting challenges in achieving computational efficiency. Positioned between structured and unstructured methods, the semi-structured approach introduces hardware-friendly patterns such as N:M sparsity, which leaves only N nonzero values in each group of M values and thereby harmonizes the acceleration benefits of a structured pattern with the flexibility of fine-grained sparsity [32](Pool et al., 2021)(Frantar et al., 2023)."], "score": 0.8720703125}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Gong et al., 2024)", "paper": {"corpus_id": 267103825, "title": "A Review of Neural Network Lightweighting Techniques", "year": 2024, "venue": "Innovation &amp; Technology Advances", "authors": [{"name": "Ziyi Gong", "authorId": "2280410840"}, {"name": "Huifu Zhang", "authorId": "2280726529"}, {"name": "Hao Yang", "authorId": "2280414619"}, {"name": "Fangjun Liu", "authorId": "2280959623"}, {"name": "Fan Luo", "authorId": "2280411835"}], "n_citations": 0}, "snippets": ["By analyzing pruning algorithms from different periods, including the latest ones, we can observe significant advantages of unstructured pruning. The most notable advantage is its ability to directly zero out or trim a large number of parameters, resulting in a highly sparse model that does not significantly affect model accuracy. Additionally, unstructured pruning can modify parameters based on the underlying logic of different hardware, leading to improved acceleration. However, unstructured pruning also has noticeable drawbacks. Firstly, due to its consideration of the impact of individual neurons on the network, unstructured pruning algorithms can be computationally intensive. Secondly, simply applying unstructured pruning does not directly accelerate sparse matrix computations, as the size of the pruned matrix remains unchanged. This means that sparse matrix multiplication and other computations are still required, which may not yield substantial acceleration on certain hardware. Moreover, unstructured pruning algorithms may rely on specific software or hardware implementations, limiting their flexibility and portability across different deep-learning frameworks. In contrast, structured pruning algorithms have advantages in these aspects. Structured pruning reduces computational complexity, simplifies sparse matrix computations, and is easier to use across different deep learning frameworks. Consequently, recent research has been inclined towards employing structured pruning algorithms for model pruning [62][63][64][65][66][67].\n\nStructured pruning algorithms have advantages in terms of hardware acceleration and prediction accuracy because they consider a more comprehensive set of factors. Compared to unstructured pruning, structured pruning can achieve model compression and acceleration by pruning entire convolutional kernels or channels. However, structured pruning algorithms also have some limitations. Firstly, in convolutional kernel pruning algorithms, the relationships between kernels are often overlooked. Kernels sometimes work together in a coordinated manner to achieve accurate predictions. Pruning based solely on the individual significance of each kernel may not lead to the optimal pruning results. Secondly, for new models, one-time pruning with structured pruning algorithms often struggles to maintain the same level of accuracy as the original model. Therefore, algorithm-level optimizations are needed to achieve better accuracy preservation."], "score": 0.8623046875}, {"id": "(Hu et al., 2025)", "paper": {"corpus_id": 278338989, "title": "SPAP: Structured Pruning via Alternating Optimization and Penalty Methods", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hanyu Hu", "authorId": "2283405371"}, {"name": "Xiaoming Yuan", "authorId": "2283433779"}], "n_citations": 0}, "snippets": ["While unstructured pruning (Frantar and Alistarh, 2023;Sun et al., 2023;Dong et al., 2024;Zhao et al., 2024) provides flexibility by zeroing out individual weights, it relies on specialized hardware for sparse computations and often yields marginal speedups due to sparsity patterns. Similarly, semi-structured pruning methods (Holmes et al., 2021;Meng et al., 2024;Fang et al., 2024) depend on specialized kernels and hardware supports like NVIDIA's 2:4 pattern (Mishra et al., 2021), making them highly hardware-dependent and thus limiting their applicability. Empirical studies further show that semi-structured pruning underperforms structured methods in inference acceleration at equivalent sparsity levels (Ashkboos et al., 2024). In contrast, structured pruning distinguishes itself by eliminating entire network components (e.g., channels, heads, or layers) rather than individual weight entries, enabling direct computational speedups and broad hardware compatibility without relying on specialized hardware or sparse computations."], "score": 0.93359375}, {"id": "(Shao et al., 2023)", "paper": {"corpus_id": 264146174, "title": "One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models", "year": 2023, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Hang Shao", "authorId": "2216418068"}, {"name": "Bei Liu", "authorId": "2168549481"}, {"name": "Yanmin Qian", "authorId": "2259050251"}], "n_citations": 21}, "snippets": ["Various Large Language Models (LLMs) from the Generative Pretrained Transformer (GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs."], "score": 0.0}, {"id": "(An et al., 2023)", "paper": {"corpus_id": 266362404, "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yongqi An", "authorId": "2167834971"}, {"name": "Xu Zhao", "authorId": "2118489444"}, {"name": "Tao Yu", "authorId": "40418746"}, {"name": "Ming Tang", "authorId": "2113727378"}, {"name": "Jinqiao Wang", "authorId": "2241943585"}], "n_citations": 61}, "snippets": ["Structured pruning (He and Xiao 2023), which prunes entire rows or columns of weights, offers a promising solution to the deployment challenges of LLMs. Unlike unstructured pruning, structured pruning reduces both parameters and inference time without relying on specific hardware, making it more widely applicable (Anwar et al., 2015). For effective structured pruning, it's crucial to have a metric that captures the collective significance of an entire row or column. However, current unstructured pruning techniques for LLMs, as seen in methods like (Sun et al. 2023;Frantar and Alistarh 2023), primarily focus on the importance of individual elements of each row in isolation. This absence of structured metrics that evaluate entire rows or columns makes them less suitable for structured pruning."], "score": 0.90380859375}, {"id": "(Feng et al., 2025)", "paper": {"corpus_id": 275993741, "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingkuan Feng", "authorId": "2332540527"}, {"name": "Jinyang Wu", "authorId": "2141911656"}, {"name": "Shuai Zhang", "authorId": "2298428469"}, {"name": "Pengpeng Shao", "authorId": "2221575807"}, {"name": "Ruihan Jin", "authorId": "2300370837"}, {"name": "Zhengqi Wen", "authorId": "1718662"}, {"name": "Jianhua Tao", "authorId": "2298423822"}, {"name": "Feihu Che", "authorId": "1471057495"}], "n_citations": 1}, "snippets": ["Pruning techniques are generally classified into two primary categories: unstructured pruning (Frantar et al., 2023)Sun et al., 2023) and structured pruning (Ma et al., 2023)An et al., 2024). Compared to unstructured pruning, structured pruning offers the flexibility to do recovery fine-tuning (RFT) for specific downstream tasks without relying on specialized hardware (Zhu et al., 2024). Moreover, the model obtained through structured pruning typically achieves much faster inference speed due to the regular data patterns."], "score": 0.8154296875}], "table": null}], "cost": 0.633891}}
