{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in the existence of formal scaling laws for inference-time compute cost (analogous to those for training) in language models?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in whether there are formal, quantitative scaling laws for inference-time compute cost in language models, similar to the established scaling laws for training."}}, {"clarifying_question2": "Do you want to know about optimal strategies for balancing model size and inference efficiency when deploying language models in production settings?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in optimal strategies for balancing model size and inference efficiency, particularly practical guidelines or principles for selecting model architectures and inference methods in real-world production deployments."}}, {"clarifying_question3": "Are you asking whether techniques like self-consistency sampling can make smaller or less compute-intensive models as effective as larger models during inference?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in whether approaches such as self-consistency sampling\u2014where multiple outputs are sampled and the answer is chosen by aggregation\u2014can allow smaller or less compute-intensive models to achieve inference performance comparable to that of larger models. Specifically, do these inference-time strategies narrow the gap in effectiveness typically seen between smaller and larger models?"}}], "better_query": "Are there formal, quantitative scaling laws for inference-time compute cost in language models, and how do these compare to the established training compute scaling laws?", "better_answer": {"sections": [{"title": "Introduction to Scaling Laws", "tldr": "Scaling laws describe predictable relationships between model performance and factors like model size, data volume, and compute resources. These mathematical formulations have guided LLM development by revealing power-law relationships that help optimize resource allocation for training. (7 sources)", "text": "\nScaling laws have emerged as fundamental principles governing the performance of large language models (LLMs), establishing mathematical relationships between model capabilities and the resources used to develop them. These laws typically express model performance as a power-law function of computational resources, model parameters, and training data volume <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"276421795\" paperTitle=\"(Mayilvahanan et al., 2025)\" isShortName></Paper>. The predictable nature of these relationships has become a cornerstone for strategic resource allocation in LLM development.\n\nThe seminal work in this field revealed that neural language models scale in a predictable manner with the Transformer architecture, demonstrating that loss follows a power-law relationship with the amount of computation used for training <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This predictability enables researchers to determine optimal allocation of computational resources and forecast the performance of larger models based on smaller ones.\n\nTraditional scaling laws focus on several key parameters: model size (number of parameters N), dataset size (number of tokens D), and the total compute budget measured in floating-point operations (FLOPs) <Paper corpusId=\"276421795\" paperTitle=\"(Mayilvahanan et al., 2025)\" isShortName></Paper>. Researchers have identified relationships such as the token multiplier (M = N/D), which represents the ratio of training tokens to parameters and helps quantify the degree of model overtraining <Paper corpusId=\"268379614\" paperTitle=\"(Gadre et al., 2024)\" isShortName></Paper>.\n\nThese scaling patterns have demonstrated remarkable consistency across multiple orders of magnitude and have been observed in both language and vision domains <Paper corpusId=\"275993956\" paperTitle=\"(Xie et al., 2025)\" isShortName></Paper>. However, it's important to note that the idealized assumptions of infinite training resources used in theoretical scaling laws may not accurately reflect real-world constraints, potentially limiting their applicability <Paper corpusId=\"269449894\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\nRecent developments have challenged some established scaling principles. The \"Chinchilla-optimal\" scaling law, which recommended specific ratios of model size to training data, has been questioned by models like Llama 3 and 3.1, which have been trained on significantly more tokens than recommended yet still demonstrate excellent performance <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This has led to proposals for a unified scaling law suggesting that model performance is primarily driven by total compute, regardless of how it's distributed between model size and dataset size.\n\nWhile training scaling laws have been extensively studied, inference-time scaling\u2014which focuses on computational investments at model run-time rather than during training\u2014represents a promising new frontier <Paper corpusId=\"273850391\" paperTitle=\"(Nori et al., 2024)\" isShortName></Paper>. Understanding how model performance scales with inference-time compute has significant implications for deployment efficiency and accessibility of language models.", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264172438, "title": "BitNet: Scaling 1-bit Transformers for Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Hongyu Wang", "authorId": "2127328758"}, {"name": "Shuming Ma", "authorId": "2118866998"}, {"name": "Li Dong", "authorId": "145307652"}, {"name": "Shaohan Huang", "authorId": "3110003"}, {"name": "Huaijie Wang", "authorId": "2216204941"}, {"name": "Lingxiao Ma", "authorId": "2259609416"}, {"name": "Fan Yang", "authorId": "2321493763"}, {"name": "Ruiping Wang", "authorId": "2282983050"}, {"name": "Yi Wu", "authorId": "2336830631"}, {"name": "Furu Wei", "authorId": "2253471545"}], "n_citations": 117}, "snippets": ["Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once."], "score": 0.86083984375}, {"id": "(Mayilvahanan et al., 2025)", "paper": {"corpus_id": 276421795, "title": "LLMs on the Line: Data Determines Loss-to-Loss Scaling Laws", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Prasanna Mayilvahanan", "authorId": "2258718382"}, {"name": "Thadd\u00e4us Wiedemer", "authorId": "2179052180"}, {"name": "Sayak Mallick", "authorId": "2345922971"}, {"name": "Matthias Bethge", "authorId": "2261937896"}, {"name": "Wieland Brendel", "authorId": "40634590"}], "n_citations": 3}, "snippets": ["Scaling laws have long guided Large Language Model (LLM) pretraining, determining model and data size under a fixed compute budget (Kaplan et al., 2020;Hoffmann et al., 2022;Grattafiori et al., 2024). Typically, scaling laws relate model performance, usually measured as training or validation loss, to total compute measured in floating point operations (FLOPs). FLOPs account for both parameter count and the number of training tokens."], "score": 0.76416015625}, {"id": "(Gadre et al., 2024)", "paper": {"corpus_id": 268379614, "title": "Language models scale reliably with over-training and on downstream tasks", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "S. Gadre", "authorId": "1387466862"}, {"name": "G. Smyrnis", "authorId": "1438310376"}, {"name": "Vaishaal Shankar", "authorId": "34961417"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Rulin Shao", "authorId": "2287946530"}, {"name": "Jean-Pierre Mercat", "authorId": "72847120"}, {"name": "Alex Fang", "authorId": "46372713"}, {"name": "Jeffrey Li", "authorId": "2287821949"}, {"name": "Sedrick Scott Keh", "authorId": "150299584"}, {"name": "Rui Xin", "authorId": "2291068532"}, {"name": "Marianna Nezhurina", "authorId": "2174178585"}, {"name": "Igor Vasiljevic", "authorId": "2291068185"}, {"name": "J. Jitsev", "authorId": "2191688"}, {"name": "Alexandros G. Dimakis", "authorId": "2257301126"}, {"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "Shuran Song", "authorId": "2265621012"}, {"name": "Thomas Kollar", "authorId": "2283843631"}, {"name": "Y. Carmon", "authorId": "2444742"}, {"name": "Achal Dave", "authorId": "2298523"}, {"name": "Reinhard Heckel", "authorId": "145639495"}, {"name": "Niklas Muennighoff", "authorId": "2037383772"}, {"name": "Ludwig Schmidt", "authorId": "2253541812"}], "n_citations": 48}, "snippets": ["We fit a scaling law for model validation loss, parameterized by (i) a token multiplier M = N/D, which is the ratio of training tokens D to parameters N and (ii) the compute C in FLOPs used to train a model, approximated by C = 6N D. Larger values of M specify more over-training."], "score": 0.8525390625}, {"id": "(Xie et al., 2025)", "paper": {"corpus_id": 275993956, "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Enze Xie", "authorId": "2320149516"}, {"name": "Junsong Chen", "authorId": "2212250873"}, {"name": "Yuyang Zhao", "authorId": "2109814772"}, {"name": "Jincheng Yu", "authorId": "2193887687"}, {"name": "Ligeng Zhu", "authorId": "20515689"}, {"name": "Yujun Lin", "authorId": "49417466"}, {"name": "Zhekai Zhang", "authorId": "2286139423"}, {"name": "Muyang Li", "authorId": "2288740166"}, {"name": "Junyu Chen", "authorId": "2325895662"}, {"name": "Han Cai", "authorId": "2114069742"}, {"name": "Bingchen Liu", "authorId": "2342993953"}, {"name": "Daquan Zhou", "authorId": "2344575401"}, {"name": "Song Han", "authorId": "2283171540"}], "n_citations": 28}, "snippets": ["Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship."], "score": 0.837890625}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 269449894, "title": "Temporal Scaling Law for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizhe Xiong", "authorId": "2249971338"}, {"name": "Xiansheng Chen", "authorId": "2298904872"}, {"name": "Xin Ye", "authorId": "2299108794"}, {"name": "Hui Chen", "authorId": "2298921971"}, {"name": "Zijia Lin", "authorId": "1818920"}, {"name": "Haoran Lian", "authorId": "2298903058"}, {"name": "Jianwei Niu", "authorId": "2293626051"}, {"name": "Guiguang Ding", "authorId": "2242661989"}], "n_citations": 10}, "snippets": ["Despite previous advancements, a critical point that remains underexplored is the temporal trajectory of LLM performance throughout training. According to (Kaplan et al., 2020), when LLMs are pre-trained with infinite dataset size and training iterations, the test loss follows the power-law. However, this assumption of infinite training resource cannot be fulfilled in real-world, thus the powerlaw may not be accurate to portrait the temporal behaviors of LLM performance during pre-training."], "score": 0.79296875}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272593336, "title": "Scaling Law Hypothesis for Multimodal Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qingyun Sun", "authorId": "2320938961"}, {"name": "Zhen Guo", "authorId": "2320723029"}], "n_citations": 0}, "snippets": ["Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance", "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]", "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."], "score": 0.802734375}, {"id": "(Nori et al., 2024)", "paper": {"corpus_id": 273850391, "title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Harsha Nori", "authorId": "2268494857"}, {"name": "N. Usuyama", "authorId": "2637252"}, {"name": "Nicholas King", "authorId": "2214807281"}, {"name": "S. McKinney", "authorId": "2047820455"}, {"name": "Xavier Fernandes", "authorId": "2329372335"}, {"name": "Sheng Zhang", "authorId": "72655349"}, {"name": "Eric Horvitz", "authorId": "2268493614"}], "n_citations": 13}, "snippets": ["Scaling laws [KMH + 20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scaling-which focuses on the value of investing in additional computation at model run-time-is a promising new area of study."], "score": 0.85986328125}], "table": null}, {"title": "Formal Inference-Time Scaling Laws", "tldr": "Inference-time scaling laws reveal predictable relationships between model performance and computational resources used during inference, often following log-linear patterns. These laws demonstrate that performance can be enhanced by increasing inference compute through multiple attempts or sophisticated inference strategies, without requiring additional training. (11 sources)", "text": "\nWhile training scaling laws have been extensively studied, research into inference-time (or test-time) scaling laws has emerged as a significant new direction in language model research. These laws describe how model performance scales with computational resources allocated during inference rather than training <Paper corpusId=\"275570556\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. A foundational discovery in this area is that the relationship between coverage (the fraction of problems solved by at least one attempt) and the number of samples often follows a log-linear pattern, suggesting the existence of formal inference-time scaling laws <Paper corpusId=\"271571035\" paperTitle=\"(Brown et al., 2024)\" isShortName></Paper> <Paper corpusId=\"272827424\" paperTitle=\"(Saad-Falcon et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275993956\" paperTitle=\"(Xie et al., 2025)\" isShortName></Paper>.\n\nResearchers have formalized these observations into quantitative expressions. Wu et al. conducted regression analysis on inference FLOPs (C) and model sizes (N) to establish a relationship between computational budget and optimal model size, expressed as: log\u2081\u2080(C) = 1.19 log\u2081\u2080(N) + 2.03 <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This equation allows practitioners to estimate the optimal inference model size for a specific compute budget, providing a practical guideline for deployment decisions.\n\nA key insight from inference scaling laws is that performance can be amplified through multiple inference attempts without further training the model <Paper corpusId=\"273507997\" paperTitle=\"(Levi, 2024)\" isShortName></Paper>. This has been demonstrated across challenging tasks such as coding and formal proofs, where verification of correct answers is possible. The \"inference loss\" exhibits a power-law decay as the number of trials increases, establishing a direct connection between performance improvements and prompting costs <Paper corpusId=\"273507997\" paperTitle=\"(Levi, 2024)\" isShortName></Paper>.\n\nThe empirical evidence consistently shows that increasing inference compute can enhance performance without significantly increasing model parameters <Paper corpusId=\"277271576\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>. This enhancement can be achieved through various strategies, such as generating more reasoning chains or longer ones <Paper corpusId=\"276813519\" paperTitle=\"(Aggarwal et al., 2025)\" isShortName></Paper>. Li et al. demonstrated that the chance of finding the correct answer improves with an increasing number of generated responses, empirically summarized by a log-linear scaling law <Paper corpusId=\"278714770\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>.\n\nRecent work has further refined these concepts for specific paradigms. For instance, Singhi et al. derived inference scaling laws for the Generation with Rejection Mechanism (GenRM) paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications <Paper corpusId=\"277467695\" paperTitle=\"(Singhi et al., 2025)\" isShortName></Paper>. This suggests that sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more cost-effective than using larger models with simple inference methods <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>.\n\nThe discovery and formalization of inference-time scaling laws represent a significant advancement in understanding how to optimize language model deployment. These laws provide a framework for making informed decisions about allocating computational resources during inference, potentially enabling more efficient and effective use of language models across various applications.", "citations": [{"id": "(Ma et al., 2025)", "paper": {"corpus_id": 275570556, "title": "Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nanye Ma", "authorId": "2279750963"}, {"name": "Shangyuan Tong", "authorId": "2058178039"}, {"name": "Haolin Jia", "authorId": "2340654494"}, {"name": "Hexiang Hu", "authorId": "2307548497"}, {"name": "Yu-Chuan Su", "authorId": "2269866136"}, {"name": "Mingda Zhang", "authorId": "2326256475"}, {"name": "Xuan Yang", "authorId": "2340726220"}, {"name": "Yandong Li", "authorId": "2324838111"}, {"name": "T. Jaakkola", "authorId": "35132120"}, {"name": "Xuhui Jia", "authorId": "2325917399"}, {"name": "Saining Xie", "authorId": "2324769373"}], "n_citations": 69}, "snippets": ["Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference."], "score": 0.8232421875}, {"id": "(Brown et al., 2024)", "paper": {"corpus_id": 271571035, "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Bradley Brown", "authorId": "2283198901"}, {"name": "Jordan Juravsky", "authorId": "50875781"}, {"name": "Ryan Ehrlich", "authorId": "2283134957"}, {"name": "Ronald Clark", "authorId": "2313919316"}, {"name": "Quoc V. Le", "authorId": "2151097303"}, {"name": "Christopher R'e", "authorId": "2313917068"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}], "n_citations": 330}, "snippets": ["Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws."], "score": 0.88818359375}, {"id": "(Saad-Falcon et al., 2024)", "paper": {"corpus_id": 272827424, "title": "Archon: An Architecture Search Framework for Inference-Time Techniques", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jon Saad-Falcon", "authorId": "2283840040"}, {"name": "Adrian Gamarra Lafuente", "authorId": "2298071127"}, {"name": "Shlok Natarajan", "authorId": "2322445633"}, {"name": "Nahum Maru", "authorId": "2322449133"}, {"name": "Hristo Todorov", "authorId": "2322445297"}, {"name": "E. Guha", "authorId": "2179104354"}, {"name": "E. K. Buchanan", "authorId": "2336877105"}, {"name": "Mayee Chen", "authorId": "2322453298"}, {"name": "Neel Guha", "authorId": "2820009"}, {"name": "Christopher R\u00e9", "authorId": "2287938277"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}], "n_citations": 20}, "snippets": ["A number of recent works show the effectiveness of scaling compute at test time [6,7,24,47]. In particular, Large Language Monkeys [6] characterizes inference-time scaling laws, showing a log-linear relationship between coverage-the fraction of problems solved by at least one attempt-and the number of samples drawn from the model across a broad range of reasoning tasks and LLMs."], "score": 0.80419921875}, {"id": "(Xie et al., 2025)", "paper": {"corpus_id": 275993956, "title": "SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute in Linear Diffusion Transformer", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Enze Xie", "authorId": "2320149516"}, {"name": "Junsong Chen", "authorId": "2212250873"}, {"name": "Yuyang Zhao", "authorId": "2109814772"}, {"name": "Jincheng Yu", "authorId": "2193887687"}, {"name": "Ligeng Zhu", "authorId": "20515689"}, {"name": "Yujun Lin", "authorId": "49417466"}, {"name": "Zhekai Zhang", "authorId": "2286139423"}, {"name": "Muyang Li", "authorId": "2288740166"}, {"name": "Junyu Chen", "authorId": "2325895662"}, {"name": "Han Cai", "authorId": "2114069742"}, {"name": "Bingchen Liu", "authorId": "2342993953"}, {"name": "Daquan Zhou", "authorId": "2344575401"}, {"name": "Song Han", "authorId": "2283171540"}], "n_citations": 28}, "snippets": ["Training scaling laws have been extensively studied in both language [32,51] and vision [33,34,52] domains. For language models, research has revealed power-law relationships between model accuracy and factors like model size, dataset size, and compute [32]. These scaling patterns have been consistently observed across several orders of magnitude. Recently, similar scaling properties have been discovered in diffusion-based text-to-image generation. Studies show that DiT's pre-training loss follows power-law relationships with computational resources [34].\n\nRecent studies have revealed significant insights into inference scaling laws for large language models. The pioneering work \"Large Language Monkeys\" [4] discovered that coverage (the fraction of problems solved) scales with the number of samples following a log-linear relationship."], "score": 0.837890625}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 271601023, "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yangzhen Wu", "authorId": "2314347409"}, {"name": "Zhiqing Sun", "authorId": "48064856"}, {"name": "Shanda Li", "authorId": "2257057803"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Yiming Yang", "authorId": "2257099254"}], "n_citations": 130}, "snippets": ["While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."], "score": 0.92431640625}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 276421468, "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03."], "score": 0.78857421875}, {"id": "(Levi, 2024)", "paper": {"corpus_id": 273507997, "title": "A Simple Model of Inference Scaling Laws", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Noam Levi", "authorId": "2261494026"}], "n_citations": 13}, "snippets": ["Neural scaling laws have garnered significant interest due to their ability to predict model performance as a function of increasing parameters, data, and compute. In this work, we propose a simple statistical ansatz based on memorization to study scaling laws in the context of inference, specifically how performance improves with multiple inference attempts.\n\nRecent works have shown empirically that LLMs can gain substantial benefits from repeated prompts to perform better on difficult tasks such as coding and formal proofs, where verification of the correct answer can be done [Brown et al., 2024, Snell et al., 2024, Bansal et al., 2024]. These works demonstrate that the performance of weaker models can be amplified without further training, by simply repeating inference trials.\n\nWe then define an\"inference loss\", which exhibits a power law decay as the number of trials increases, and connect this result with prompting costs."], "score": 0.81494140625}, {"id": "(Shen et al., 2025)", "paper": {"corpus_id": 277271576, "title": "Long Is More Important Than Difficult for Training Reasoning Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Si Shen", "authorId": "2283092321"}, {"name": "Fei Huang", "authorId": "2353326337"}, {"name": "Zhixiao Zhao", "authorId": "2214963769"}, {"name": "Chang Liu", "authorId": "2351851812"}, {"name": "Tiansheng Zheng", "authorId": "2351808837"}, {"name": "Danhao Zhu", "authorId": "2283097884"}], "n_citations": 0}, "snippets": ["In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy."], "score": 0.88623046875}, {"id": "(Aggarwal et al., 2025)", "paper": {"corpus_id": 276813519, "title": "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Pranjal Aggarwal", "authorId": "2114841965"}, {"name": "S. Welleck", "authorId": "2129663"}], "n_citations": 101}, "snippets": ["Test-time scaling laws indicate predictable performance gains from increasing inference computation, either by generating more reasoning chains or longer ones (Wu et al., 2024;Snell et al., 2024;OpenAI et al., 2024a)."], "score": 0.7724609375}, {"id": "(Xu et al., 2025)", "paper": {"corpus_id": 278714770, "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "year": 2025, "venue": "", "authors": [{"name": "Yige Xu", "authorId": "26339093"}, {"name": "Xu Guo", "authorId": "2325014301"}, {"name": "Zhiwei Zeng", "authorId": "49512311"}, {"name": "Chunyan Miao", "authorId": "2238949687"}], "n_citations": 1}, "snippets": ["Li et al. [2025] suggest that the chance of finding the correct answer improves while increasing the number of generated responses, which is empirically summarized by a log-linear scaling law [Brown et al., 2024]."], "score": 0.806640625}, {"id": "(Singhi et al., 2025)", "paper": {"corpus_id": 277467695, "title": "When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nishad Singhi", "authorId": "1970199394"}, {"name": "Hritik Bansal", "authorId": "2317010356"}, {"name": "Arian Hosseini", "authorId": "2090537547"}, {"name": "Aditya Grover", "authorId": "2263888488"}, {"name": "Kai-Wei Chang", "authorId": "2256646491"}, {"name": "Marcus Rohrbach", "authorId": "2329738651"}, {"name": "Anna Rohrbach", "authorId": "2329738066"}], "n_citations": 6}, "snippets": ["Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications."], "score": 0.9208984375}], "table": null}, {"title": "Comparison with Training Scaling Laws", "tldr": "While training scaling laws focus on model size and dataset volume, inference scaling laws emphasize computational efficiency during model deployment. Research indicates that optimizing for inference can lead to different model configurations than those suggested by training-focused laws, particularly favoring smaller models trained on larger datasets. (12 sources)", "text": "\nTraining and inference scaling laws represent two complementary frameworks for understanding language model performance, each with distinct implications for model design and resource allocation. The foundational training scaling laws, such as those established by Kaplan et al., revealed power-law relationships between model performance, parameter count, and dataset size <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. However, these laws primarily focus on training computation rather than inference costs, which become increasingly significant as models are deployed at scale <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nA key distinction is that while training costs are incurred once, inference costs scale with each use of the model, making them potentially more impactful for widely deployed systems <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This consideration has led researchers to propose \"Inference-Optimal Scaling Laws\" that predict performance in relation to energy consumption during deployment rather than training <Paper corpusId=\"264172438\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. Such approaches recognize that model configurations optimized for training efficiency may not be optimal for inference scenarios.\n\nRecent research challenges some established training scaling principles, particularly the Chinchilla-optimal allocation of compute resources. Models like Llama 3 and 3.1, which were trained on significantly more tokens than recommended by Chinchilla's formula, have demonstrated excellent performance, suggesting that the relationship between model size and dataset size may be more flexible than previously thought <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This has led to proposals for a unified scaling law where total compute, rather than its specific allocation between model size and dataset size, primarily drives performance <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>.\n\nThe trade-offs between training and inference optimization become particularly apparent in specialized architectures. For Mixture-of-Experts (MoE) models, simply increasing the number of experts improves training efficiency without increasing training costs, but this approach creates significant burdens during inference <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>. This discrepancy highlights the need for scaling laws that incorporate inference efficiency alongside validation loss <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>.\n\nInterestingly, the relationship between training and inference optimization can sometimes be counterintuitive. Kumar et al. found that for quantized models, degradation from post-training quantization increases as models are trained on more data, potentially making additional pretraining data actually harmful for inference performance <Paper corpusId=\"273877632\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper>. This suggests that optimization for inference may require different training strategies than those prescribed by traditional scaling laws.\n\nAn emerging solution to balancing these competing objectives involves training smaller models on substantially larger datasets, which can optimize efficiency across both training and deployment phases <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>. Similarly, sparse pre-training approaches can achieve comparable model quality to dense pre-training while providing significant computational savings during inference through reduced model size <Paper corpusId=\"275789021\" paperTitle=\"(Jin et al., 2025)\" isShortName></Paper>.\n\nThe field is now witnessing an evolution from traditional pretraining scaling laws toward inference-time optimization <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This shift recognizes that while pretraining data availability is inherently constrained, performance can still be enhanced by optimizing inference strategies and computational allocation <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>.\n\nRecent theoretical work also offers insights into why neural scaling laws follow power-law relationships. Schaeffer et al. propose that pretraining cross-entropy loss may be composed of multiple functions decaying at different rates, with the slowest-decaying term eventually dominating as compute scales up <Paper corpusId=\"276580891\" paperTitle=\"(Schaeffer et al., 2025)\" isShortName></Paper>. This perspective may explain why power-law relationships become apparent only at sufficient scale, potentially connecting the mathematical foundations of both training and inference scaling laws.\n\nLooking forward, determining optimal allocation of parameters and computation under various inference constraints (such as memory, latency, and batch size) represents a promising direction for extending inference-optimal scaling laws <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. These approaches acknowledge that sophisticated inference methods with smaller models may offer more cost-effective solutions than larger models with simpler inference techniques <Paper corpusId=\"270703266\" paperTitle=\"(Welleck et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264172438, "title": "BitNet: Scaling 1-bit Transformers for Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Hongyu Wang", "authorId": "2127328758"}, {"name": "Shuming Ma", "authorId": "2118866998"}, {"name": "Li Dong", "authorId": "145307652"}, {"name": "Shaohan Huang", "authorId": "3110003"}, {"name": "Huaijie Wang", "authorId": "2216204941"}, {"name": "Lingxiao Ma", "authorId": "2259609416"}, {"name": "Fan Yang", "authorId": "2321493763"}, {"name": "Ruiping Wang", "authorId": "2282983050"}, {"name": "Yi Wu", "authorId": "2336830631"}, {"name": "Furu Wei", "authorId": "2253471545"}], "n_citations": 117}, "snippets": ["Neural language models have proven to scale predictably [KMH + 20] with vanilla Transformer architecture. The loss scales as the power law with the amount of computation used for training. This allows us to determine the optimal allocation of a computation budget as well as predict the performance of large language models from smaller models.\n\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model the relationship between the loss and the actual compute. Previous work [KMH + 20, HKK + 20, HBM + 22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit models whose cost is dominated by integer computation. Moreover, it mainly measures the training computation rather than the inference. To have a better understanding of the scaling efficiency of neural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the energy consumption. We focus on the inference energy cost as it scales with the usage of the model, while the training cost is only once."], "score": 0.86083984375}, {"id": "(Sardana et al., 2023)", "paper": {"corpus_id": 266693796, "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Sardana", "authorId": "2277217297"}, {"name": "Sasha Doubov", "authorId": "2040790531"}, {"name": "Jonathan Frankle", "authorId": "2277215716"}], "n_citations": 88}, "snippets": ["However, the Chinchilla scaling laws only account for the computational costs of training."], "score": 0.818359375}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272593336, "title": "Scaling Law Hypothesis for Multimodal Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qingyun Sun", "authorId": "2320938961"}, {"name": "Zhen Guo", "authorId": "2320723029"}], "n_citations": 0}, "snippets": ["Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance", "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]", "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."], "score": 0.802734375}, {"id": "(Yun et al., 2024)", "paper": {"corpus_id": 268875826, "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Longfei Yun", "authorId": "2294718538"}, {"name": "Yonghao Zhuang", "authorId": "2152482391"}, {"name": "Yao Fu", "authorId": "2294813888"}, {"name": "Eric P. Xing", "authorId": "2243336934"}, {"name": "Hao Zhang", "authorId": "2294828709"}], "n_citations": 8}, "snippets": ["We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss."], "score": 0.796875}, {"id": "(Kumar et al., 2024)", "paper": {"corpus_id": 273877632, "title": "Scaling Laws for Precision", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tanishq Kumar", "authorId": "2256990733"}, {"name": "Zachary Ankner", "authorId": "2172356226"}, {"name": "B. Spector", "authorId": "2259914006"}, {"name": "Blake Bordelon", "authorId": "77327149"}, {"name": "Niklas Muennighoff", "authorId": "2037383772"}, {"name": "Mansheej Paul", "authorId": "1690452"}, {"name": "Cengiz Pehlevan", "authorId": "2577481"}, {"name": "Christopher R'e", "authorId": "2313917068"}, {"name": "Aditi Raghunathan", "authorId": "2334471571"}], "n_citations": 29}, "snippets": ["For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful."], "score": 0.89892578125}, {"id": "(Lu, 2025)", "paper": {"corpus_id": 275336968, "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chien-Ping Lu", "authorId": "2338865687"}], "n_citations": 1}, "snippets": ["Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."], "score": 0.89794921875}, {"id": "(Jin et al., 2025)", "paper": {"corpus_id": 275789021, "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tian Jin", "authorId": "2341534973"}, {"name": "Ahmed Imtiaz Humayun", "authorId": "34657588"}, {"name": "Utku Evci", "authorId": "3399348"}, {"name": "Suvinay Subramanian", "authorId": "1929462"}, {"name": "Amir Yazdanbakhsh", "authorId": "2296991252"}, {"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "G. Dziugaite", "authorId": "2533850"}], "n_citations": 1}, "snippets": ["Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference."], "score": 0.83154296875}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 276249712, "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling", "year": 2025, "venue": "", "authors": [{"name": "Weizhe Chen", "authorId": "2278582297"}, {"name": "Sven Koenig", "authorId": "2256845651"}, {"name": "B. Dilkina", "authorId": "1796375"}], "n_citations": 1}, "snippets": ["The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities."], "score": 0.77197265625}, {"id": "(Schaeffer et al., 2025)", "paper": {"corpus_id": 276580891, "title": "How Do Large Language Monkeys Get Their Power (Laws)?", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rylan Schaeffer", "authorId": "1749176844"}, {"name": "Joshua Kazdan", "authorId": "2327048379"}, {"name": "John Hughes", "authorId": "2294572631"}, {"name": "Jordan Juravsky", "authorId": "50875781"}, {"name": "Sara Price", "authorId": "2333593482"}, {"name": "Aengus Lynch", "authorId": "2287830769"}, {"name": "Erik Jones", "authorId": "2334069920"}, {"name": "Robert Kirk", "authorId": "2311693657"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}, {"name": "Oluwasanmi Koyejo", "authorId": "143812875"}], "n_citations": 4}, "snippets": ["Despite focusing on scaling inference compute, our paper contributes is a new hypothesis for an open question in scaling pretraining compute: why are neural scaling laws power laws? Just as the scaling behavior of \u2212 log(pass D @k) only becomes clear for large k, so too might the scaling behavior of pretraining cross entropy with pretraining compute C. Specifically, suppose the pretraining cross entropy L as a function of pretraining compute C is a sum of many functions which decay at different rates: \n\nwhere \u03b1 is the smallest (positive) polynomial exponent and \u03c9(1/C \u03b1 ) represents functions that decay more slowly than any polynomial. Initially, for small C, the dominant term may be unclear, but as pretraining compute is scaled up across 8 \u2212 10 orders of magnitude, the leading order term dominates and an approximate power law emerges: \n\nThus, a power law relationship may only be reasonable for sufficiently large pretraining compute C, which in turn may require excluding the lowest pretraining compute models in order to obtain good predictions, justifying a widespread empirical practice (Kaplan et al., 2020). We designate possible functions hiding in \u03c9(1/C \u03b1 ) and o(1/C \u03b1 ) as the dark matter of neural scaling laws."], "score": 0.80908203125}, {"id": "(Chen et al._1, 2025)", "paper": {"corpus_id": 278636433, "title": "Parallel Scaling Law for Language Models", "year": 2025, "venue": "", "authors": [{"name": "Mouxiang Chen", "authorId": "2125101083"}, {"name": "Binyuan Hui", "authorId": "2321578848"}, {"name": "Zeyu Cui", "authorId": "2248072386"}, {"name": "Jiaxin Yang", "authorId": "2328943044"}, {"name": "Dayiheng Liu", "authorId": "2248487202"}, {"name": "Jianling Sun", "authorId": "2362357192"}, {"name": "Junyang Lin", "authorId": "2326803484"}, {"name": "Zhongxin Liu", "authorId": "2361700209"}], "n_citations": 2}, "snippets": ["Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction."], "score": 0.88671875}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 271601023, "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yangzhen Wu", "authorId": "2314347409"}, {"name": "Zhiqing Sun", "authorId": "48064856"}, {"name": "Shanda Li", "authorId": "2257057803"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Yiming Yang", "authorId": "2257099254"}], "n_citations": 130}, "snippets": ["While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."], "score": 0.92431640625}, {"id": "(Welleck et al., 2024)", "paper": {"corpus_id": 270703266, "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "S. Welleck", "authorId": "2129663"}, {"name": "Amanda Bertsch", "authorId": "2138301112"}, {"name": "Matthew Finlayson", "authorId": "1580418311"}, {"name": "Hailey Schoelkopf", "authorId": "2184031883"}, {"name": "Alex Xie", "authorId": "2253395527"}, {"name": "Graham Neubig", "authorId": "2265547593"}, {"name": "Ilia Kulikov", "authorId": "2308102420"}, {"name": "Zaid Harchaoui", "authorId": "2265540561"}], "n_citations": 77}, "snippets": ["However, there is also another lesser-mentioned scaling phenomenon, where adopting more sophisticated methods or scaling compute at inference time (Jones, 2021)"], "score": 0.86376953125}], "table": null}, {"title": "Applications and Implications of Inference Scaling Laws", "tldr": "Inference scaling laws are driving practical changes in model design, encouraging smaller models trained on larger datasets for deployment efficiency. These laws are reshaping resource allocation strategies across the AI industry and opening new research directions in synthetic data generation and advanced inference algorithms. (10 sources)", "text": "\nInference scaling laws have significant practical applications that are reshaping how language models are designed, deployed, and optimized. A key implication is the shift toward prioritizing smaller models trained on substantially larger datasets to optimize efficiency during deployment, especially in resource-constrained environments <Paper corpusId=\"272593336\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. This approach balances efficiency across both training and deployment phases, acknowledging that inference costs\u2014which scale with each model use\u2014can ultimately outweigh one-time training costs <Paper corpusId=\"275336968\" paperTitle=\"(Lu, 2025)\" isShortName></Paper>.\n\nThe practical benefits of this approach are becoming increasingly apparent. Research indicates that sparse pre-training can achieve comparable model quality to dense pre-training while enabling significant computational savings during inference through reduced model size <Paper corpusId=\"275789021\" paperTitle=\"(Jin et al., 2025)\" isShortName></Paper>. These findings are challenging traditional scaling paradigms and leading to more deployment-optimized model architectures.\n\nFor specialized architectures like Mixture-of-Experts (MoE) models, inference scaling laws have particularly important implications. The tendency to increase expert count to improve training efficiency without increasing training costs creates significant challenges during inference, necessitating the incorporation of inference efficiency as an additional metric alongside validation loss <Paper corpusId=\"268875826\" paperTitle=\"(Yun et al., 2024)\" isShortName></Paper>. This represents a broader trend toward more holistic evaluation frameworks that consider both training and inference performance.\n\nThe equation established by Wu et al., log\u2081\u2080(C) = 1.19 log\u2081\u2080(N) + 2.03, provides a practical guideline for estimating optimal inference model size for specific compute budgets <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. This relationship allows practitioners to make more informed decisions about model selection based on available computational resources, potentially reducing unnecessary overhead in deployment scenarios.\n\nA particularly promising application of inference scaling laws is the development of sophisticated inference algorithms that can enhance performance without increasing model size. Research suggests that employing advanced test-time computation strategies\u2014such as iterative refinement or tree search\u2014with smaller models may be more cost-effective than using larger models with simple inference methods <Paper corpusId=\"276421468\" paperTitle=\"(Sengupta et al., 2025)\" isShortName></Paper>. This approach offers a path to improved performance without the substantial resource requirements of larger models.\n\nThe implications of inference scaling laws extend to data generation strategies as well. As the availability of high-quality training data becomes increasingly constrained, researchers are exploring methods to synthesize data using existing models to extend the limits of pretraining scaling laws <Paper corpusId=\"276249712\" paperTitle=\"(Chen et al., 2025)\" isShortName></Paper>. This creates a virtuous cycle where inference-optimized models can be used to generate data that improves future model performance.\n\nLooking forward, determining how to allocate parameters and parallel computation under various inference constraints\u2014such as memory limitations, latency requirements, and batch size considerations\u2014represents a promising direction for extending inference-optimal scaling laws <Paper corpusId=\"278636433\" paperTitle=\"(Chen et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"266693796\" paperTitle=\"(Sardana et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271601023\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper>. These efforts acknowledge that the ideal model architecture depends heavily on the specific deployment context and constraints.\n\nThe evolution from traditional pretraining scaling laws toward inference-time optimization reflects a maturing understanding that performance can be enhanced by optimizing inference strategies without significantly increasing model parameters <Paper corpusId=\"277271576\" paperTitle=\"(Shen et al., 2025)\" isShortName></Paper>. This shift recognizes that while model size has dominated scaling discussions, inference compute offers an additional dimension for performance improvement that has been comparatively underexplored.", "citations": [{"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272593336, "title": "Scaling Law Hypothesis for Multimodal Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qingyun Sun", "authorId": "2320938961"}, {"name": "Zhen Guo", "authorId": "2320723029"}], "n_citations": 0}, "snippets": ["Scaling laws in large language models (LLMs) have unveiled fundamental relationships between model performance, size, and the volume of training data [1,2,3,4,5,6]. These laws serve as a guide for resource allocation in LLM development, helping to balance model size and data volume to optimize performance", "Despite these insights, recent trends challenge the Chinchilla-optimal law. For instance, models like Llama 3 and 3.1 have been trained on significantly more tokens (up to 10 times more than Chinchilla's recommendations), yet still demonstrate outstanding performance [7]. This discrepancy has prompted researchers to reconsider the optimal allocation of compute resources in autoregressive pre-training [8].\n\nRecent work suggests a unified scaling law, where model performance is driven primarily by total compute, regardless of how it is distributed between model size and dataset size [8]", "This unified scaling law suggests that smaller models trained on larger datasets may be prioritized for inference efficiency, especially in settings where resource constraints in inference are significant."], "score": 0.802734375}, {"id": "(Lu, 2025)", "paper": {"corpus_id": 275336968, "title": "The Race to Efficiency: A New Perspective on AI Scaling Laws", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chien-Ping Lu", "authorId": "2338865687"}], "n_citations": 1}, "snippets": ["Building on these foundations, recent research has explored additional factors influencing scaling laws. Sardana et al. [12] incorporated inference-time compute costs, proposing methods in which smaller models-trained with much larger (potentially synthetic) datasets-can balance efficiency across both training and deployment phases. Snell et al. [13] investigated strategies for optimizing compute specifically at test time."], "score": 0.89794921875}, {"id": "(Jin et al., 2025)", "paper": {"corpus_id": 275789021, "title": "The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws", "year": 2025, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tian Jin", "authorId": "2341534973"}, {"name": "Ahmed Imtiaz Humayun", "authorId": "34657588"}, {"name": "Utku Evci", "authorId": "3399348"}, {"name": "Suvinay Subramanian", "authorId": "1929462"}, {"name": "Amir Yazdanbakhsh", "authorId": "2296991252"}, {"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "G. Dziugaite", "authorId": "2533850"}], "n_citations": 1}, "snippets": ["Our findings indicate that while sparse pre-training achieves the same final model quality as dense pre-training for equivalent compute budgets, it provides substantial benefits through reduced model size, enabling significant potential computational savings during inference."], "score": 0.83154296875}, {"id": "(Yun et al., 2024)", "paper": {"corpus_id": 268875826, "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Longfei Yun", "authorId": "2294718538"}, {"name": "Yonghao Zhuang", "authorId": "2152482391"}, {"name": "Yao Fu", "authorId": "2294813888"}, {"name": "Eric P. Xing", "authorId": "2243336934"}, {"name": "Hao Zhang", "authorId": "2294828709"}], "n_citations": 8}, "snippets": ["We study the scaling law of MoE-based LLMs regarding the relations between the model performance, model size, dataset size, and the expert degree. Echoing previous research studying MoE in different contexts, we observe the diminishing return of increasing the number of experts, but this seems to suggest we should scale the number of experts until saturation, as the training cost would remain constant, which is problematic during inference time. We propose to amend the scaling law of MoE by introducing inference efficiency as another metric besides the validation loss."], "score": 0.796875}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 271601023, "title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yangzhen Wu", "authorId": "2314347409"}, {"name": "Zhiqing Sun", "authorId": "48064856"}, {"name": "Shanda Li", "authorId": "2257057803"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Yiming Yang", "authorId": "2257099254"}], "n_citations": 130}, "snippets": ["While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We study inference scaling laws (aka test-time scaling laws) and compute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies.\n\nWe performed a regression analysis on inference FLOPs C and model sizes N to establish a relationship between a given computational budget and its optimal model size. The resulting equation, log 10 (C) = 1.19 log 10 (N ) + 2.03, lets us estimate the optimal inference model size for a specific compute budget."], "score": 0.92431640625}, {"id": "(Sengupta et al., 2025)", "paper": {"corpus_id": 276421468, "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ayan Sengupta", "authorId": "34920835"}, {"name": "Yash Goel", "authorId": "2345922770"}, {"name": "Tanmoy Chakraborty", "authorId": "2249914540"}], "n_citations": 0}, "snippets": ["Further exploration by Wu et al. (2024) suggested that employing sophisticated test-time computation strategies (such as iterative refinement or tree search) with smaller models may be more costeffective than using larger models with simple inference methods. Their work establishes a relationship between inference computational budget and optimal model size for compute-efficient inference, expressed as: log 10 (C) = 1.19 log 10 (N ) + 2.03."], "score": 0.78857421875}, {"id": "(Chen et al., 2025)", "paper": {"corpus_id": 276249712, "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling", "year": 2025, "venue": "", "authors": [{"name": "Weizhe Chen", "authorId": "2278582297"}, {"name": "Sven Koenig", "authorId": "2256845651"}, {"name": "B. Dilkina", "authorId": "1796375"}], "n_citations": 1}, "snippets": ["The remarkable capabilities of large language models (LLMs) have largely been driven by the pretraining scaling laws, which demonstrate that increasing the amount of data and model size leads to predictable improvements in performance. However, the availability of high-quality training data is inherently constrained-there is only one Internet from which to source such data. As a result, a growing research focus is on synthesizing high-quality data using existing models to further extend the limits of the pretraining scaling laws. Concurrently, following the release of Ope-nAI's o1 series (Jaech et al., 2024), researchers have been exploring a new class of scaling laws that govern inferencetime performance. These laws aim to optimize LLM performance given a larger computational budget at inference time, enabling improvements in complex problem-solving capabilities."], "score": 0.77197265625}, {"id": "(Chen et al._1, 2025)", "paper": {"corpus_id": 278636433, "title": "Parallel Scaling Law for Language Models", "year": 2025, "venue": "", "authors": [{"name": "Mouxiang Chen", "authorId": "2125101083"}, {"name": "Binyuan Hui", "authorId": "2321578848"}, {"name": "Zeyu Cui", "authorId": "2248072386"}, {"name": "Jiaxin Yang", "authorId": "2328943044"}, {"name": "Dayiheng Liu", "authorId": "2248487202"}, {"name": "Jianling Sun", "authorId": "2362357192"}, {"name": "Junyang Lin", "authorId": "2326803484"}, {"name": "Zhongxin Liu", "authorId": "2361700209"}], "n_citations": 2}, "snippets": ["Recent inference-time scaling efforts attempt to provide a computation-optimal strategy during the inference phase (Wu et al., 2024)(Snell et al., 2025), but most rely on specific scenarios and datasets. Leveraging the proposed PARSCALE, determining how to allocate the number of parameters and parallel computation under various inference budgets (e.g., memory, latency, and batch size) to extend inference-optimal scaling laws (Sardana et al., 2023) is a promising direction."], "score": 0.88671875}, {"id": "(Sardana et al., 2023)", "paper": {"corpus_id": 266693796, "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Nikhil Sardana", "authorId": "2277217297"}, {"name": "Sasha Doubov", "authorId": "2040790531"}, {"name": "Jonathan Frankle", "authorId": "2277215716"}], "n_citations": 88}, "snippets": ["However, the Chinchilla scaling laws only account for the computational costs of training."], "score": 0.818359375}, {"id": "(Shen et al., 2025)", "paper": {"corpus_id": 277271576, "title": "Long Is More Important Than Difficult for Training Reasoning Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Si Shen", "authorId": "2283092321"}, {"name": "Fei Huang", "authorId": "2353326337"}, {"name": "Zhixiao Zhao", "authorId": "2214963769"}, {"name": "Chang Liu", "authorId": "2351851812"}, {"name": "Tiansheng Zheng", "authorId": "2351808837"}, {"name": "Danhao Zhu", "authorId": "2283097884"}], "n_citations": 0}, "snippets": ["In the inference phase, the focus of Scaling Law research has shifted towards optimizing inference compute. Studies [3] [24] [3] show that increasing inference compute can enhance inference performance without significantly increasing model parameters, and it is proposed that optimizing inference strategies can significantly improve inference accuracy."], "score": 0.88623046875}], "table": null}], "cost": 0.294744}}
