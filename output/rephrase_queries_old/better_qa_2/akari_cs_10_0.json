{"clarifying_information": [{"clarifying_question1": "Are you interested in comparing the effectiveness and biases of translation-based multilingual evaluation datasets versus those collected natively in target languages?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in a comparison of the effectiveness and biases between translation-based multilingual evaluation datasets and those that are natively collected in the target languages."}}, {"clarifying_question2": "Do you want to know best practices specifically for ensuring quality and cultural relevance when translating English NLP benchmarks into multiple languages?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in best practices specifically for ensuring both quality and cultural relevance when translating English NLP benchmarks into multiple languages."}}, {"clarifying_question3": "Are you seeking detailed examples of pitfalls and reliability issues\u2014such as translation artifacts, cultural biases, or annotation challenges\u2014that arise when using translation as the primary method for non-English benchmarks?", "clarifying_answer3": {"clarifying_answer": "Yes, I am seeking detailed examples of pitfalls and reliability issues\u2014including translation artifacts, cultural biases, and annotation challenges\u2014that specifically arise when using translation as the main method for constructing non-English evaluation benchmarks."}}], "better_query": "How do translation-based multilingual evaluation datasets compare in effectiveness and bias against datasets natively collected in target languages, particularly regarding their impact on the reliability of cross-lingual evaluations?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Multilingual evaluation datasets are crucial for assessing cross-lingual capabilities of NLP models, but their creation methods can significantly impact their quality and reliability. Translation-based datasets differ fundamentally from datasets natively collected in target languages, raising important questions about their comparative effectiveness and potential biases. (LLM Memory)", "text": "\nMultilingual evaluation datasets serve as critical benchmarks for assessing the cross-lingual capabilities of natural language processing (NLP) models. As AI systems increasingly aim to serve users across diverse linguistic backgrounds, the quality and representativeness of these evaluation datasets directly impact our understanding of models' true multilingual abilities. However, the methods used to create these datasets vary significantly, with two primary approaches emerging: translation-based datasets (where content is originally created in one language and translated to others) and natively collected datasets (where content is originally authored in each target language). <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe distinction between these approaches is not merely methodological but has profound implications for evaluation validity. Translation-based datasets offer practical advantages, including cost-effectiveness, consistent content across languages, and scalability to many languages simultaneously. Yet, they may introduce translation artifacts, cultural misalignments, and fail to capture natural language patterns specific to each language community. These limitations raise fundamental questions about whether assessments based on translated content accurately reflect a model's performance on naturally occurring text in target languages. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe research community has increasingly recognized this tension, prompting investigations into how these different dataset creation methodologies affect evaluation outcomes. Understanding these differences is essential not only for interpreting current benchmark results but also for designing more robust evaluation frameworks that can reliably assess models across the linguistic diversity of the world's languages. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Methods of Creating Multilingual Evaluation Datasets", "tldr": "Two primary approaches exist for creating multilingual evaluation datasets: translation-based methods (converting existing English datasets into other languages) and native collection methods (creating datasets directly in target languages). Each approach offers distinct advantages and limitations that significantly impact the quality and representativeness of the resulting evaluations. (23 sources)", "text": "\nMultilingual evaluation datasets are typically created through one of two main approaches: translation of existing resources or direct collection in target languages. Translation-based approaches, which dominate the multilingual benchmarking landscape, involve converting English datasets into other languages either through machine translation or human translators <Paper corpusId=\"52271711\" paperTitle=\"(Conneau et al., 2018)\" isShortName></Paper> <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>. This approach offers several practical advantages, including cost-effectiveness, time efficiency, and scalability across multiple languages simultaneously <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper>. Additionally, translation creates parallel evaluation sets that provide a meaningful measure of the cross-lingual transfer gap between different models <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nProminent examples of translation-based evaluation resources include XNLI (for natural language inference), XQuAD (for question answering), and XCOPA (for commonsense reasoning) <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261242630\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. These datasets typically begin with carefully selected English examples and translate them to ensure comparable concept coverage across languages <Paper corpusId=\"227231710\" paperTitle=\"(Glavas et al., 2020)\" isShortName></Paper> <Paper corpusId=\"196183388\" paperTitle=\"(Vulic et al., 2019)\" isShortName></Paper>. For instance, XQuAD comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators <Paper corpusId=\"204901567\" paperTitle=\"(Artetxe et al., 2019)\" isShortName></Paper>.\n\nTranslation quality varies significantly depending on the method employed. Machine translation offers rapid scaling but may introduce quality issues, particularly for low-resource languages <Paper corpusId=\"273323209\" paperTitle=\"(Thellmann et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246680398\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Some researchers have evaluated different translation approaches, including Google Translate, MarianMT, and even large language models like ChatGPT, using fluency and meaning preservation metrics to ensure quality <Paper corpusId=\"264405758\" paperTitle=\"(Jin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"4623739\" paperTitle=\"(Junczys-Dowmunt et al., 2018)\" isShortName></Paper>.\n\nDespite their prevalence, translation-based datasets have significant limitations. They often contain translation artifacts or \"translationese\" - linguistic features that make translated text distinguishable from naturally occurring text in the target language <Paper corpusId=\"261557946\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper> <Paper corpusId=\"940724\" paperTitle=\"(Rabinovich et al., 2015)\" isShortName></Paper>. These artifacts can lead to unreliable evaluation results, particularly when models are both trained and evaluated on translated data <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper> <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.\n\nThe alternative approach involves direct collection of data in target languages, creating natively authored datasets without translation intermediaries. TyDi QA exemplifies this method, featuring questions written directly by speakers of 11 typologically diverse languages <Paper corpusId=\"212657414\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. Similarly, region-specific evaluations like IndoMMLU for Indonesian <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper> contain questions written by curriculum experts that reflect local educational standards and cultural contexts.\n\nNative collection approaches better capture cultural nuances and linguistic particularities but require significantly more resources, especially for gathering expert annotations in multiple languages <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. However, they avoid cultural biases introduced through translation and produce more authentic evaluations of model capabilities in target languages <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"237503047\" paperTitle=\"(Yin et al., 2021)\" isShortName></Paper>.\n\nSome researchers have suggested hybrid approaches as a practical compromise. For instance, translating only high-quality evaluation data while using native data for training represents a middle ground that balances practicality with cultural appropriateness <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper>. Others recommend ensuring consistent test sets across languages by either using original annotations in all languages or translating from non-English languages to avoid English-centric biases <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>.\n\nRecent studies have demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to translated counterparts (0.47), underscoring the importance of creating culturally and linguistically tailored evaluation datasets <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This finding has motivated an increasing focus on developing native evaluation benchmarks that better represent local cultural knowledge and linguistic phenomena <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Conneau et al., 2018)", "paper": {"corpus_id": 52271711, "title": "XNLI: Evaluating Cross-lingual Sentence Representations", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Alexis Conneau", "authorId": "2480903"}, {"name": "Guillaume Lample", "authorId": "1830914"}, {"name": "Ruty Rinott", "authorId": "1905713"}, {"name": "Adina Williams", "authorId": "81840293"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}, {"name": "Holger Schwenk", "authorId": "144518416"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}], "n_citations": 1388}, "snippets": ["State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines."], "score": 0.0}, {"id": "(Artetxe et al., 2019)", "paper": {"corpus_id": 204901567, "title": "On the Cross-lingual Transferability of Monolingual Representations", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Dani Yogatama", "authorId": "1755465"}], "n_citations": 799}, "snippets": ["State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators."], "score": 0.0}, {"id": "(Thellmann et al., 2024)", "paper": {"corpus_id": 273323209, "title": "Towards Multilingual LLM Evaluation for European Languages", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Klaudia Thellmann", "authorId": "2269539"}, {"name": "Bernhard Stadler", "authorId": "2325726089"}, {"name": "Michael Fromm", "authorId": "2258551638"}, {"name": "Jasper Schulze Buschhoff", "authorId": "2258550838"}, {"name": "Alex Jude", "authorId": "2325726070"}, {"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Johannes Leveling", "authorId": "2258552277"}, {"name": "Nicolas Flores-Herr", "authorId": "2347259072"}, {"name": "Joachim K\u00f6hler", "authorId": "2330411614"}, {"name": "Ren\u00e9 J\u00e4kel", "authorId": "2330412312"}, {"name": "Mehdi Ali", "authorId": "2258668067"}], "n_citations": 10}, "snippets": ["Creating a multilingual dataset with human annotators requires a significant amount of time, which impacts both the duration of production and the cost of the dataset (Yang et al., 2019). In addition, homogeneous benchmarks are required in each language instead of heterogeneous multilingual benchmarks for a cross-lingual analysis (Lewkowycz et al., 2022;Tiedemann, 2012)", "An alternative is the machine translation of existing benchmarks. This approach is both more cost-effective and timeefficient. However, the main disadvantage is the translation quality, which is highly dependent on the capability of the language model used (Meng et al., 2022). In the past, some data sets have shown quality gaps in translation accuracy, with special consideration given to languages with medium and low resources (Team et al., 2022). Those languages have only a few to very few data sets available for pre-training and evaluation. They are much more difficult to be translated by an LLM, when taking cultural nuances of each language in consideration. Translating these languages, in particular, can lead to imbalanced evaluations across the benchmarks (Team et al., 2022;Goyal et al., 2022;Conneau et al., 2018)."], "score": 0.9345703125}, {"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models", "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)", "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."], "score": 0.9716796875}, {"id": "(Etxaniz et al., 2024)", "paper": {"corpus_id": 270380088, "title": "BertaQA: How Much Do Language Models Know About Local Culture?", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Julen Etxaniz", "authorId": "2226458991"}, {"name": "Gorka Azkune", "authorId": "2481918"}, {"name": "A. Soroa", "authorId": "2260104163"}, {"name": "Oier L\u00f3pez de Lacalle", "authorId": "2251043402"}, {"name": "Mikel Artetxe", "authorId": "2347956"}], "n_citations": 11}, "snippets": ["Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets."], "score": 0.94482421875}, {"id": "(Ranaldi et al., 2023)", "paper": {"corpus_id": 261242630, "title": "Empowering Cross-lingual Abilities of Instruction-tuned Large Language Models by Translation-following demonstrations", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leonardo Ranaldi", "authorId": "2008183566"}, {"name": "Giulia Pucci", "authorId": "2199247500"}, {"name": "A. Freitas", "authorId": "145528474"}], "n_citations": 34}, "snippets": ["XQUAD and MLQA focus on understanding questions and answers through translation into different languages", "Cross-lingual Question Answering Dataset (XQUAD) (Artetxe et al., 2019) consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development set of SQuAD v1.1 (Rajpurkar et al., 2016) with their manual translations into several languages. Consequently, the dataset is entirely parallel across 11 languages", "Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) measures knowledge of the world and problem-solving problems in multiple subjects with 57 subjects across STEM, humanities, social sciences, and other areas. The benchmark is native in English; however, we translated it into five additional languages4."], "score": 0.92578125}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 258865558, "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Sneha Kudugunta", "authorId": "35871436"}, {"name": "Xinyan Velocity Yu", "authorId": "2118211280"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Machel Reid", "authorId": "1557386977"}, {"name": "Yulia Tsvetkov", "authorId": "2073587169"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 62}, "snippets": ["Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."], "score": 0.9326171875}, {"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."], "score": 0.0}, {"id": "(Glavas et al., 2020)", "paper": {"corpus_id": 227231710, "title": "SemEval-2020 Task 2: Predicting Multilingual and Cross-Lingual (Graded) Lexical Entailment", "year": 2020, "venue": "International Workshop on Semantic Evaluation", "authors": [{"name": "Goran Glavas", "authorId": "2472657"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}, {"name": "Simone Paolo Ponzetto", "authorId": "2029669151"}], "n_citations": 8}, "snippets": ["We first created monolingual HyperLex datasets in three target languages: German (DE), Italian (IT), and Croatian (HR), as described in (Vulic et al., 2019). For this shared task, we repeated the procedure for two more languages: Turkish (TR), and our surprise test language -Albanian (SQ). We first translated word pairs from the EN HyperLex dataset and re-scored the translated pairs in the target language. The translation approach has been selected because (1) the original EN HyperLex pairs were already carefully selected through a controlled sampling procedure (ensuring a wide coverage of diverse relations). Moreover, (2) we wanted the datasets in different languages to be as comparable as possible in terms of concept coverage. The translation approach has been validated in previous work for creating multilingual semantic similarity datasets (Leviant and Reichart, 2015;(Camacho-Collados et al., 2017). Most importantly, it allows for the automatic construction of cross-lingual graded LE datasets."], "score": 0.89306640625}, {"id": "(Vulic et al., 2019)", "paper": {"corpus_id": 196183388, "title": "Multilingual and Cross-Lingual Graded Lexical Entailment", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "Simone Paolo Ponzetto", "authorId": "1801255"}, {"name": "Goran Glavas", "authorId": "2472657"}], "n_citations": 16}, "snippets": ["Grounded in cognitive linguistics, graded lexical entailment (GR-LE) is concerned with fine-grained assertions regarding the directional hierarchical relationships between concepts on a continuous scale. In this paper, we present the first work on cross-lingual generalisation of GR-LE relation. Starting from HyperLex, the only available GR-LE dataset in English, we construct new monolingual GR-LE datasets for three other languages, and combine those to create a set of six cross-lingual GR-LE datasets termed CL-HYPERLEX. We next present a novel method dubbed CLEAR (Cross-Lingual Lexical Entailment Attract-Repel) for effectively capturing graded (and binary) LE, both monolingually in different languages as well as across languages (i.e., on CL-HYPERLEX). Coupled with a bilingual dictionary, CLEAR leverages taxonomic LE knowledge in a resource-rich language (e.g., English) and propagates it to other languages. Supported by cross-lingual LE transfer, CLEAR sets competitive baseline performance on three new monolingual GR-LE datasets and six cross-lingual GR-LE datasets. In addition, we show that CLEAR outperforms current state-of-the-art on binary cross-lingual LE detection by a wide margin for diverse language pairs."], "score": 0.0}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 246680398, "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Yu Meng", "authorId": "145391513"}, {"name": "Jiaxin Huang", "authorId": "3488341"}, {"name": "Yu Zhang", "authorId": "49891156"}, {"name": "Jiawei Han", "authorId": "2111759643"}], "n_citations": 235}, "snippets": ["Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class."], "score": 0.0}, {"id": "(Jin et al., 2023)", "paper": {"corpus_id": 264405758, "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries", "year": 2023, "venue": "The Web Conference", "authors": [{"name": "Yiqiao Jin", "authorId": "2087723977"}, {"name": "Mohit Chandra", "authorId": "32556330"}, {"name": "Gaurav Verma", "authorId": "145816931"}, {"name": "Yibo Hu", "authorId": "49995036"}, {"name": "Munmun De Choudhury", "authorId": "2256118804"}, {"name": "Srijan Kumar", "authorId": "2254102372"}], "n_citations": 76}, "snippets": ["Observing the lack of existing multilingual QA datasets in healthcare domains, we curate a novel benchmark. To ensure the quality of the dataset, we conduct a human evaluation on the translation quality of three popular approaches commonly adopted in translating academic documents: Google Translate [93], MarianMT [94], and ChatGPT [7]. To comprehensively evaluate the capability of each model in translating different datasets, we randomly selected 50 questions from each dataset, resulting in a total of 150 questions. Our evaluation of translation quality aligns with established standards in previous works [15]. A total of 450 translation pairs (150 questions across 3 languages) were evaluated. Each example was reviewed by three independent annotators who scored the translations using a five-point Likert scale (1: strongly disagree -5: strongly agree) on two critical dimensions: \n\n(1) Fluency. Is the [TARGET LANGUAGE] version a good translation of the English text? (2) Meaning. Does the [TARGET LANGUAGE] version faithfully convey the same meaning as the English text?"], "score": 0.8759765625}, {"id": "(Junczys-Dowmunt et al., 2018)", "paper": {"corpus_id": 4623739, "title": "Marian: Fast Neural Machine Translation in C++", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Marcin Junczys-Dowmunt", "authorId": "1733933"}, {"name": "Roman Grundkiewicz", "authorId": "3272639"}, {"name": "Tomasz Dwojak", "authorId": "3407177"}, {"name": "Hieu T. Hoang", "authorId": "152378023"}, {"name": "Kenneth Heafield", "authorId": "1702066"}, {"name": "Tom Neckermann", "authorId": "1410712818"}, {"name": "F. Seide", "authorId": "1745715"}, {"name": "Ulrich Germann", "authorId": "50655248"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}, {"name": "Nikolay Bogoychev", "authorId": "3444222"}, {"name": "Andr\u00e9 F. T. Martins", "authorId": "145644643"}, {"name": "Alexandra Birch", "authorId": "2539211"}], "n_citations": 718}, "snippets": ["We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 261557946, "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Xinyu Crystina Zhang", "authorId": "2118895402"}, {"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Odunayo Ogundepo", "authorId": "2166106776"}, {"name": "Ehsan Kamalloo", "authorId": "2023642"}, {"name": "David Alfonso-Hermelo", "authorId": "1419474794"}, {"name": "Xiaoguang Li", "authorId": "2238110973"}, {"name": "Qun Liu", "authorId": "1688015"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}], "n_citations": 77}, "snippets": ["However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value", "Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches."], "score": 0.908203125}, {"id": "(Clark et al., 2020)", "paper": {"corpus_id": 212657414, "title": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "J. Clark", "authorId": "144797264"}, {"name": "Eunsol Choi", "authorId": "2890423"}, {"name": "Michael Collins", "authorId": "123052390"}, {"name": "Dan Garrette", "authorId": "2758616"}, {"name": "T. Kwiatkowski", "authorId": "15652489"}, {"name": "Vitaly Nikolaev", "authorId": "48942032"}, {"name": "J. Palomaki", "authorId": "52578817"}], "n_citations": 612}, "snippets": ["Abstract Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA\u2014a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology\u2014the set of linguistic features each language expresses\u2014such that we expect models performing well on this set to generalize across a large number of the world\u2019s languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don\u2019t know the answer yet, and the data is collected directly in each language without the use of translation."], "score": 0.0}, {"id": "(Rabinovich et al., 2015)", "paper": {"corpus_id": 940724, "title": "Unsupervised Identification of Translationese", "year": 2015, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Ella Rabinovich", "authorId": "2653682"}, {"name": "S. Wintner", "authorId": "2524073"}], "n_citations": 55}, "snippets": ["Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domain-related features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy."], "score": 0.0}, {"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 248780386, "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages", "year": 2022, "venue": "NLPPOWER", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."], "score": 0.95751953125}, {"id": "(Koto et al., 2023)", "paper": {"corpus_id": 263829211, "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fajri Koto", "authorId": "2789148"}, {"name": "Nurul Aisyah", "authorId": "2256987672"}, {"name": "Haonan Li", "authorId": "49404498"}, {"name": "Timothy Baldwin", "authorId": "2256987316"}], "n_citations": 46}, "snippets": ["Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."], "score": 0.0}, {"id": "(Yuksel et al., 2024)", "paper": {"corpus_id": 271245129, "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Arda Yuksel", "authorId": "2311698426"}, {"name": "Abdullatif K\u00f6ksal", "authorId": "1999179692"}, {"name": "Lutfi Kerem cSenel", "authorId": "2126865294"}, {"name": "Anna Korhonen", "authorId": "2311700614"}, {"name": "Hinrich Schutze", "authorId": "2130001188"}], "n_citations": 14}, "snippets": ["Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."], "score": 0.0}, {"id": "(Hershcovich et al., 2022)", "paper": {"corpus_id": 247594499, "title": "Challenges and Strategies in Cross-Cultural NLP", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Daniel Hershcovich", "authorId": "2064295987"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Heather Lent", "authorId": "49568895"}, {"name": "Miryam de Lhoneux", "authorId": "3295381"}, {"name": "Mostafa Abdou", "authorId": "30671790"}, {"name": "Stephanie Brandl", "authorId": "6547490"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Laura Cabello Piqueras", "authorId": "2093582149"}, {"name": "Ilias Chalkidis", "authorId": "2125376289"}, {"name": "Ruixiang Cui", "authorId": "1717462692"}, {"name": "Constanza Fierro", "authorId": "50110151"}, {"name": "Katerina Margatina", "authorId": "82259306"}, {"name": "Phillip Rust", "authorId": "1660797358"}, {"name": "Anders S\u00f8gaard", "authorId": "1700187"}], "n_citations": 182}, "snippets": ["Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture", "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations", "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."], "score": 0.90380859375}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 238198104, "title": "Visually Grounded Reasoning across Languages and Cultures", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fangyu Liu", "authorId": "144097210"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Siva Reddy", "authorId": "145732771"}, {"name": "Nigel Collier", "authorId": "50638196"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 180}, "snippets": ["The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."], "score": 0.0}, {"id": "(Yin et al., 2021)", "paper": {"corpus_id": 237503047, "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Da Yin", "authorId": "144508458"}, {"name": "Liunian Harold Li", "authorId": "2108904535"}, {"name": "Ziniu Hu", "authorId": "3407296"}, {"name": "Nanyun Peng", "authorId": "3157053"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}], "n_citations": 56}, "snippets": ["Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models\u2019 ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277993848, "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghao Wu", "authorId": "2327995656"}, {"name": "Weixuan Wang", "authorId": "2258789284"}, {"name": "Sinuo Liu", "authorId": "2349802912"}, {"name": "Huifeng Yin", "authorId": "2331767275"}, {"name": "Xintong Wang", "authorId": "2356794252"}, {"name": "Yu Zhao", "authorId": "2331686159"}, {"name": "Chenyang Lyu", "authorId": "2266387313"}, {"name": "Longyue Wang", "authorId": "2302342302"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 5}, "snippets": ["Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)", "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."], "score": 0.91796875}], "table": null}, {"title": "Effectiveness Comparison", "tldr": "Studies comparing translation-based and natively collected multilingual evaluation datasets show mixed results, with translation-based methods offering scalability advantages but suffering from translation artifacts that affect performance. The effectiveness of each approach varies significantly depending on the task, language pairs, and evaluation setup, with domain transferability sometimes proving more crucial than language transferability. (13 sources)", "text": "\nComparative analyses of translation-based and natively collected multilingual evaluation datasets reveal significant differences in their effectiveness across various tasks and languages. Translation-based approaches demonstrate considerable utility for cross-lingual transfer, with studies showing that models trained on translated datasets often outperform source-language baselines. For instance, in semantic role labeling tasks, translated training data led to an average F1 score increase of 6.7 points compared to source-only baselines, demonstrating that \"corpus translation is one effective way for cross-lingual SRL\" <Paper corpusId=\"215754572\" paperTitle=\"(Fei et al., 2020)\" isShortName></Paper> <Paper corpusId=\"7805664\" paperTitle=\"(Tiedemann et al., 2016)\" isShortName></Paper> <Paper corpusId=\"202541341\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nHowever, the effectiveness of translation-based methods varies considerably across languages and depends on several factors. Liu et al. found that performance on translated question-answering datasets \"depends on not only how similar the target language and English are, but also how difficult the question set of the target language is\" <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>. Additionally, translation-based methods often struggle with named entities, which can significantly impact performance in knowledge-intensive tasks <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nSeveral evaluation frameworks have been developed to assess translation-based approaches, including translate-test (where evaluation sets are translated to English and a single English model is used) and translate-train (where English training data is translated to target languages) <Paper corpusId=\"207880568\" paperTitle=\"(Conneau et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233481097\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>. These methods serve as critical baselines for cross-lingual transfer assessment, with translate-train-all (training on all translated datasets) often producing strong results <Paper corpusId=\"233481097\" paperTitle=\"(Goyal et al., 2021)\" isShortName></Paper>.\n\nInterestingly, recent research has uncovered a counterintuitive phenomenon: models trained on translated texts sometimes outperform those trained on human-authored texts when evaluated on translate-test sets <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. This suggests that translation artifacts may actually benefit performance when both training and evaluation involve similar translation processes. However, on original English evaluation sets, models trained on human texts typically perform best <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of translation-based approaches also varies by task type. For sentiment analysis on the SemEval dataset, augmenting training data with translations consistently improved performance over multilingual baselines, with translating source language data to all target languages (Tr S\u2192all) proving most effective <Paper corpusId=\"241583550\" paperTitle=\"(Jacqmin et al., 2021)\" isShortName></Paper>. However, this advantage did not generalize to in-house datasets, highlighting the context-dependent nature of translation effectiveness <Paper corpusId=\"241583550\" paperTitle=\"(Jacqmin et al., 2021)\" isShortName></Paper>.\n\nDespite their utility, translation-based approaches face significant limitations. Translation errors can percolate to the classification phase, and maximum-likelihood translations may lack sufficient expressiveness to capture nuanced meanings <Paper corpusId=\"236318351\" paperTitle=\"(Ponti et al., 2021)\" isShortName></Paper>. These issues can compromise evaluation reliability, particularly for complex semantic tasks.\n\nA critical consideration in evaluating these approaches is domain transferability. Studies show that cross-lingual rankers trained on Wikipedia-based QA datasets generalize poorly to product domains, performing worse than multilingual rankers trained on in-domain English data <Paper corpusId=\"258715321\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>. This suggests that \"domain transferability is even more crucial than language transferability\" in some applications <Paper corpusId=\"258715321\" paperTitle=\"(Shen et al., 2023)\" isShortName></Paper>.\n\nTranslation-based approaches have proven particularly valuable for creating multilingual versions of important benchmark datasets. For example, the multilingual MS MARCO passage ranking dataset (mMARCO) demonstrated that \"multilingual models finetuned on translated datasets achieve superior effectiveness to models finetuned on the original English version alone\" in zero-shot scenarios on the Mr. TyDi dataset <Paper corpusId=\"237364084\" paperTitle=\"(Bonifacio et al., 2021)\" isShortName></Paper>. Similarly, machine translation has been applied as cross-lingual data augmentation for bias detection in Hindi, Italian, and Korean datasets <Paper corpusId=\"259859119\" paperTitle=\"(Sahoo et al., 2023)\" isShortName></Paper> and has shown improvements on multilingual benchmarks <Paper corpusId=\"235097287\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>.\n\nOverall, while translation-based approaches offer practical advantages for cross-lingual evaluation, their effectiveness compared to natively collected datasets varies significantly by language, task, and evaluation methodology. The ideal approach depends on specific research goals, available resources, and the languages being studied.", "citations": [{"id": "(Fei et al., 2020)", "paper": {"corpus_id": 215754572, "title": "Cross-Lingual Semantic Role Labeling with High-Quality Translated Training Corpus", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hao Fei", "authorId": "46959445"}, {"name": "Meishan Zhang", "authorId": "2678094"}, {"name": "Donghong Ji", "authorId": "145628086"}], "n_citations": 108}, "snippets": ["Translated target. Next, We consider taking the translated target as only the training data to examine the effectiveness of the pseudo datasets. As shown in Table 2, we find that the translated datasets can bring significantly better performances than the source baseline overall languages, resulting in an averaged F1 score increase of 51.1 \u2212 44.4 = 6.7. The results demonstrate that corpus translation is one effective way for crosslingual SRL. The observation is in line with the previous work for cross-lingual dependency parsing (Tiedemann et al., 2016)(Zhang et al., 2019). By direct gold-standard corpus translation, the produced pseudo training data can not only remain high-quality SRL annotations but also capture the language divergences effectively, which leads to better performance than the source baseline model."], "score": 0.94775390625}, {"id": "(Tiedemann et al., 2016)", "paper": {"corpus_id": 7805664, "title": "Synthetic Treebanking for Cross-Lingual Dependency Parsing", "year": 2016, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "J. Tiedemann", "authorId": "143675545"}, {"name": "Zeljko Agic", "authorId": "1806948"}], "n_citations": 56}, "snippets": ["How do we parse the languages for which no treebanks are available? This contribution addresses the cross-lingual viewpoint on statistical dependency parsing, in which we attempt to make use of resource-rich source language treebanks to build and adapt models for the under-resourced target languages. We outline the benefits, and indicate the drawbacks of the current major approaches. We emphasize synthetic treebanking: the automatic creation of target language treebanks by means of annotation projection and machine translation. We present competitive results in cross-lingual dependency parsing using a combination of various techniques that contribute to the overall success of the method. We further include a detailed discussion about the impact of part-of-speech label accuracy on parsing results that provide guidance in practical applications of cross-lingual methods for truly under-resourced languages."], "score": 0.0}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 202541341, "title": "Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Meishan Zhang", "authorId": "2678094"}, {"name": "Zhang Meishan", "authorId": "2096969631"}, {"name": "Zhang Yue", "authorId": "2068672157"}, {"name": "Fu Guo-hong", "authorId": "31060997"}], "n_citations": 37}, "snippets": ["Treebank translation is a promising method for cross-lingual transfer of syntactic dependency knowledge. The basic idea is to map dependency arcs from a source treebank to its target translation according to word alignments. This method, however, can suffer from imperfect alignment between source and target words. To address this problem, we investigate syntactic transfer by code mixing, translating only confident words in a source treebank. Cross-lingual word embeddings are leveraged for transferring syntactic knowledge to the target from the resulting code-mixed treebank. Experiments on University Dependency Treebanks show that code-mixed treebanks are more effective than translated treebanks, giving highly competitive performances among cross-lingual parsing methods."], "score": 0.0}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 196174566, "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jiahua Liu", "authorId": "46701066"}, {"name": "Yankai Lin", "authorId": "2427350"}, {"name": "Zhiyuan Liu", "authorId": "49293587"}, {"name": "Maosong Sun", "authorId": "1753344"}], "n_citations": 91}, "snippets": ["The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is."], "score": 0.9013671875}, {"id": "(Conneau et al., 2019)", "paper": {"corpus_id": 207880568, "title": "Unsupervised Cross-lingual Representation Learning at Scale", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Alexis Conneau", "authorId": "2480903"}, {"name": "Kartikay Khandelwal", "authorId": "40267343"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Vishrav Chaudhary", "authorId": "113810201"}, {"name": "Guillaume Wenzek", "authorId": "2293203"}, {"name": "Francisco Guzm\u00e1n", "authorId": "144204682"}, {"name": "Edouard Grave", "authorId": "3024698"}, {"name": "Myle Ott", "authorId": "40511414"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}], "n_citations": 6589}, "snippets": ["Cross-lingual Natural Language Inference (XNLI). The XNLI dataset comes with groundtruth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project."], "score": 0.8798828125}, {"id": "(Goyal et al., 2021)", "paper": {"corpus_id": 233481097, "title": "Larger-Scale Transformers for Multilingual Masked Language Modeling", "year": 2021, "venue": "Workshop on Representation Learning for NLP", "authors": [{"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Jingfei Du", "authorId": "3048577"}, {"name": "Myle Ott", "authorId": "40511414"}, {"name": "Giridhar Anantharaman", "authorId": "4168565"}, {"name": "Alexis Conneau", "authorId": "2480903"}], "n_citations": 129}, "snippets": ["We evaluate our model on cross-lingual transfer from English to other languages. We also consider two machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train-all: the English training set is machine-translated to each language and we fine-tune a multilingual model on all training sets. For translations, we use the original XNLI data for consistency."], "score": 0.9072265625}, {"id": "(Park et al., 2024)", "paper": {"corpus_id": 270226445, "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "chaeHun Park", "authorId": "1484057728"}, {"name": "Ko-tik Lee", "authorId": "2110019663"}, {"name": "Hyesu Lim", "authorId": "2151202457"}, {"name": "Jaeseok Kim", "authorId": "2293573807"}, {"name": "Junmo Park", "authorId": "2305630071"}, {"name": "Yu-Jung Heo", "authorId": "2293447208"}, {"name": "Du-Seong Chang", "authorId": "2293430063"}, {"name": "Jaegul Choo", "authorId": "2260653165"}], "n_citations": 3}, "snippets": ["We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes", "Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer", "In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts."], "score": 0.89453125}, {"id": "(Jacqmin et al., 2021)", "paper": {"corpus_id": 241583550, "title": "SpanAlign: Efficient Sequence Tagging Annotation Projection into Translated Data applied to Cross-Lingual Opinion Mining", "year": 2021, "venue": "WNUT", "authors": [{"name": "L\u00e9o Jacqmin", "authorId": "66957859"}, {"name": "Gabriel Marzinotto", "authorId": "2483876"}, {"name": "Justyna Gromada", "authorId": "2101678"}, {"name": "Ewelina Szczekocka", "authorId": "1736936"}, {"name": "Robert Kolodynski", "authorId": "2611308"}, {"name": "G\u00e9raldine Damnati", "authorId": "1722225"}], "n_citations": 2}, "snippets": ["When comparing the two non-augmented baselines, i.e. Monolingual and Multilingual, we observe a significant improvement when fine-tuning a model on the combination of all languages (on average, +11.7 points for SemEval and +16.6 points on the in-house dataset), highlighting the ability of mBERT for cross-lingual transfer learning.\n\nFor SemEval, augmenting the training corpus with translated data consistently provides an improvement over the O all baseline. Using the translations of the non-English corpora into English (O all + Tr all\u2192S ) is detrimental to the performance on the English test set, while other languages are not impacted as much by this translation direction. The reason could be that, similarly to the cross-lingual adaptation experiments, it is beneficial to use the translated data in the target language specifically. Overall, O all + Tr S\u2192all seems to be the most effective configuration for all languages.\n\nRegarding the in-house datasets, the synthetic data is not as beneficial as in the case of SemEval.\n\nResults are comparable to O all for all translation directions, and no data configuration stands out as most effective on average."], "score": 0.91943359375}, {"id": "(Ponti et al., 2021)", "paper": {"corpus_id": 236318351, "title": "Modelling Latent Translations for Cross-Lingual Transfer", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Julia Kreutzer", "authorId": "3422710"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "Siva Reddy", "authorId": "145732771"}], "n_citations": 18}, "snippets": ["While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation."], "score": 0.892578125}, {"id": "(Shen et al., 2023)", "paper": {"corpus_id": 258715321, "title": "xPQA: Cross-Lingual Product Question Answering in 12 Languages", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xiaoyu Shen", "authorId": "2562211"}, {"name": "Akari Asai", "authorId": "35584853"}, {"name": "B. Byrne", "authorId": "36126076"}, {"name": "A. Gispert", "authorId": "1786911"}], "n_citations": 8}, "snippets": ["We find that applying a cross-lingual ranker trained on a Wikipedia-based QA dataset generalizes poorly to the product domain. The performance is even worse than training a multilingual ranker on the English in-domain data, suggesting that domain transferability is even more crucial than language transferability."], "score": 0.89208984375}, {"id": "(Bonifacio et al., 2021)", "paper": {"corpus_id": 237364084, "title": "mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset", "year": 2021, "venue": "", "authors": [{"name": "L. Bonifacio", "authorId": "2003019597"}, {"name": "Israel Campiotti", "authorId": "1472908948"}, {"name": "R. Lotufo", "authorId": "1809633"}, {"name": "Rodrigo Nogueira", "authorId": "143744603"}], "n_citations": 113}, "snippets": ["More recently, researchers observed that multilingual models finetuned on monolingual datasets (usually English) achieve good effectiveness in a zero-shot manner when evaluated on other languages (Conneau et al., 2020;Wu and Dredze, 2019;Xue et al., 2020;MacAvaney et al., 2019). Yet, many languages are underrepresented and the evaluation process is most of the time limited to monolingual datasets. We believe that having a high-quality labeled resource available in multiple languages allows researchers and practitioners to explore different aspects of the design space such as model architectures and training algorithms. Additionally, a way to further explore multilingual model capabilities beyond zero-shot learning is to finetune them on multilingual data. Considering this, we adopted an automatic translation approach to create a multilingual version of the MS MARCO passage ranking dataset, named mMARCO", "We evaluated these models in a zero-shot scenario on the Mr. TyDi dataset, showing that multilingual models finetuned on our translated dataset achieve superior effectiveness to models finetuned on the original English version alone."], "score": 0.89111328125}, {"id": "(Sahoo et al., 2023)", "paper": {"corpus_id": 259859119, "title": "With Prejudice to None: A Few-Shot, Multilingual Transfer Learning Approach to Detect Social Bias in Low Resource Languages", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nihar Ranjan Sahoo", "authorId": "2119502119"}, {"name": "Niteesh Mallela", "authorId": "2166204873"}, {"name": "P. Bhattacharyya", "authorId": "145532184"}], "n_citations": 1}, "snippets": ["For this study, we translate all the non-English datasets into English using Google translate10 api. As there are abudant of resources (datasets and models) already available for English, a general approach is to do classification followed by English translation. We investigate the effectiveness of this approach for bias detection using Hindi, Italian and Korean datasets."], "score": 0.90283203125}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 235097287, "title": "Practical Transformer-based Multilingual Text Classification", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Cindy Wang", "authorId": "2108725400"}, {"name": "Michele Banko", "authorId": "2339397"}], "n_citations": 28}, "snippets": ["In real applications, labeled data is often available in high resource languages such as English but sparse or nonexistent in others. We experiment with machine translation as a form of cross-lingual data augmentation, which has been shown to improve performance on multilingual benchmarks (Singh et al., 2019)."], "score": 0.86669921875}], "table": null}, {"title": "Translation Artifacts and Biases", "tldr": "Translation-based multilingual datasets often contain subtle artifacts that make translated text systematically different from naturally authored text, leading to evaluation bias. These \"translationese\" features can create artificially inflated performance metrics when models are both trained and evaluated on translated data, resulting in unreliable assessments of true cross-lingual capabilities. (19 sources)", "text": "\nTranslation-based approaches to creating multilingual evaluation datasets introduce distinct linguistic patterns known as \"translationese\" or translation artifacts that make translated text systematically different from naturally authored text in target languages <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper> <Paper corpusId=\"261557946\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>. These artifacts appear in both human and machine translations and can manifest as differences in lexical characteristics, including the frequency of functional words and pronouns <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. Such patterns make translated datasets less representative of authentic language use in real-world contexts, particularly for dialogue and culturally-specific content <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper> <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nTranslation quality significantly affects the reliability of cross-lingual evaluations, with high-resource languages appearing to outperform low-resource languages partly due to better translation systems rather than actual model capabilities <Paper corpusId=\"235683093\" paperTitle=\"(Turc et al., 2021)\" isShortName></Paper>. This bias is particularly problematic because it creates an artificially optimistic estimation of models' cross-lingual abilities <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. Recent work has empirically demonstrated this effect, finding that sentence representations of translated data are unnaturally more similar to their English sources than independently generated examples in the target language <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nThe impact of translation artifacts on evaluation extends to various NLP tasks. For natural language inference (NLI), independent translation of premises and hypotheses can reduce the lexical overlap between them, affecting models that are highly sensitive to such patterns <Paper corpusId=\"252968346\" paperTitle=\"(Oh et al., 2022)\" isShortName></Paper> <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Similarly, in summarization, quality evaluations have been found not to \"survive translation,\" suggesting that translated summaries may be assessed differently than their original counterparts <Paper corpusId=\"253762032\" paperTitle=\"(Krubinski et al., 2022)\" isShortName></Paper> <Paper corpusId=\"237532546\" paperTitle=\"(Iskender et al., 2021)\" isShortName></Paper>.\n\nTranslation inconsistencies disproportionately affect low-resource languages, as demonstrated in the XNLI dataset. Researchers have found poor agreement between human-translated test instances and the original English labels they were supposed to inherit, particularly for languages like Hindi and Urdu <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. These inconsistencies can be identified by measuring performance gaps between evaluations on human-translated versus machine-translated target text across multiple languages <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nAn intriguing finding is that models trained on translated texts often outperform those trained on human-authored texts when evaluated on translate-test sets <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. This suggests that translation artifacts may actually benefit performance when both training and evaluation involve similar translation processes, further confirming that relying solely on translated evaluation data can yield misleading results <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>. However, when evaluated on original English datasets, models trained on human-authored texts typically perform best <Paper corpusId=\"270226445\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\nWhile translation-based multilingual datasets are valuable for enabling cross-language comparisons, they introduce simpler syntax and lexical choices that differ distributionally from native data and may not reflect native users' preferences <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270562911\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"57174748\" paperTitle=\"(Baker et al., 1993)\" isShortName></Paper>. For relation extraction tasks, researchers have identified specific error categories in translation and alignment that can affect model performance, including pronoun-dropping, coordination, and compounding issues <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper>.\n\nTo mitigate these biases, researchers have proposed several approaches. One recommendation is to ensure consistent test sets across languages by either using original annotations in all languages or translating from non-English sources to avoid English-centric biases <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. Others suggest using large language models for self-translation to construct training data, as this can reduce translationese effects and produce more natural translations <Paper corpusId=\"273323302\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Additionally, complementing translation-based benchmarks with natively authored multilingual benchmarks can provide a more comprehensive evaluation of cross-lingual capabilities <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, machine-translated data has shown benefits for classification and structured prediction tasks <Paper corpusId=\"258557740\" paperTitle=\"(Hennig et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233189585\" paperTitle=\"(Ozaki et al., 2021)\" isShortName></Paper>. However, researchers emphasize the importance of using target-language originating evaluation data in multimodal setups rather than translated data to avoid the risk of fitting to translationese <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This is particularly crucial when evaluating large language models that are expected to serve diverse linguistic communities <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models", "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)", "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."], "score": 0.9716796875}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 261557946, "title": "MIRACL: A Multilingual Retrieval Dataset Covering 18 Diverse Languages", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Xinyu Crystina Zhang", "authorId": "2118895402"}, {"name": "Nandan Thakur", "authorId": "47583894"}, {"name": "Odunayo Ogundepo", "authorId": "2166106776"}, {"name": "Ehsan Kamalloo", "authorId": "2023642"}, {"name": "David Alfonso-Hermelo", "authorId": "1419474794"}, {"name": "Xiaoguang Li", "authorId": "2238110973"}, {"name": "Qun Liu", "authorId": "1688015"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "Jimmy J. Lin", "authorId": "145580839"}], "n_citations": 77}, "snippets": ["However, translation is known to cause inadvertent artifacts such as ''translationese'' (Clark et al., 2020)(Lembersky et al., 2011)Volansky et al., 2015;Avner et al., 2016;(Eetemadi et al., 2014)(Rabinovich et al., 2015) and may lead to training data of questionable value", "Constructing datasets automatically by exploiting heuristics has the virtue of not requiring expensive human annotations and can be easily scaled up to cover many languages. However, such datasets are inherently limited by the original resource they are built from. For instance, in CLIRMatrix, the queries are the titles of Wikipedia articles, which tend to be short phrases such as named entities. Also, multi-degree judgments in the dataset are directly converted from BM25 scores, which creates an evaluation bias towards lexical approaches."], "score": 0.908203125}, {"id": "(Razumovskaia et al., 2021)", "paper": {"corpus_id": 235313293, "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems", "year": 2021, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "E. Razumovskaia", "authorId": "66879943"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "A. Korhonen", "authorId": "145762466"}, {"name": "Ivan Vulic", "authorId": "1747849"}], "n_citations": 34}, "snippets": ["These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level."], "score": 0.8642578125}, {"id": "(Majewska et al., 2022)", "paper": {"corpus_id": 246430787, "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Olga Majewska", "authorId": "46963731"}, {"name": "E. Razumovskaia", "authorId": "66879943"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 28}, "snippets": ["The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation", ".the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets", ".Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable."], "score": 0.9873046875}, {"id": "(Turc et al., 2021)", "paper": {"corpus_id": 235683093, "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Iulia Turc", "authorId": "1388156275"}, {"name": "Kenton Lee", "authorId": "2110237268"}, {"name": "Jacob Eisenstein", "authorId": "144154709"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 58}, "snippets": ["In practice however, multilingual training data was obtained by machinetranslating an originally human-curated dataset (most often in English) to other languages. Inescapably, this introduces the confound of MT quality; high-resource languages are likely to have good translation systems and therefore merely appear to outperform others on zero-shot crosslingual transfer."], "score": 0.90625}, {"id": "(Oh et al., 2022)", "paper": {"corpus_id": 252968346, "title": "Synergy with Translation Artifacts for Training and Inference in Multilingual Tasks", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jaehoon Oh", "authorId": "46299565"}, {"name": "Jongwoo Ko", "authorId": "2051385328"}, {"name": "Se-Young Yun", "authorId": "70509252"}], "n_citations": 9}, "snippets": ["Both baselines improve the performance compared to ZSXLT; however, they are sensitive to the translator, including translation artifacts, which are characteristics stylized by the translator (Conneau et al., 2018;(Artetxe et al., 2020). Artetxe et al. (2020) showed that matching the types of text (i.e., origin or translationese1 ) between training and inference is essential due to the presence of translation artifacts under translate-test."], "score": 0.92724609375}, {"id": "(Krubinski et al., 2022)", "paper": {"corpus_id": 253762032, "title": "From COMET to COMES \u2013 Can Summary Evaluation Benefit from Translation Evaluation?", "year": 2022, "venue": "EVAL4NLP", "authors": [{"name": "Mateusz Krubi\u0144ski", "authorId": "2040867021"}, {"name": "Pavel Pecina", "authorId": "1758528"}], "n_citations": 3}, "snippets": ["Surprisingly, both the COMES_MT and the COMES variants perform better than the multilingual COMES_MT_ML variant. This is in line with recent findings by (Iskender et al., 2021), which indicate that summary evaluations do not survive translation."], "score": 0.89892578125}, {"id": "(Iskender et al., 2021)", "paper": {"corpus_id": 237532546, "title": "Does Summary Evaluation Survive Translation to Other Languages?", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Neslihan Iskender", "authorId": "147675294"}, {"name": "Oleg V. Vasilyev", "authorId": "2753049"}, {"name": "Tim Polzehl", "authorId": "1912566"}, {"name": "John Bohannon", "authorId": "1995240446"}, {"name": "Sebastian Moller", "authorId": "2058899254"}], "n_citations": 4}, "snippets": ["The creation of a quality summarization dataset is an expensive, time-consuming effort, requiring the production and evaluation of summaries by both trained humans and machines. The returns to such an effort would increase significantly if the dataset could be used in additional languages without repeating human annotations. To investigate how much we can trust machine translation of summarization datasets, we translate the English SummEval dataset to seven languages and compare performances across automatic evaluation measures. We explore equivalence testing as the appropriate statistical paradigm for evaluating correlations between human and automated scoring of summaries. We also consider the effect of translation on the relative performance between measures. We find some potential for dataset reuse in languages similar to the source and along particular dimensions of summary quality. Our code and data can be found at https://github.com/PrimerAI/primer-research/."], "score": 0.0}, {"id": "(Agrawal et al., 2024)", "paper": {"corpus_id": 267413041, "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ashish Agrawal", "authorId": "2282542641"}, {"name": "Barah Fazili", "authorId": "2187454108"}, {"name": "P. Jyothi", "authorId": "144859542"}], "n_citations": 3}, "snippets": ["We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."], "score": 0.962890625}, {"id": "(Park et al., 2024)", "paper": {"corpus_id": 270226445, "title": "Translation Deserves Better: Analyzing Translation Artifacts in Cross-lingual Visual Question Answering", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "chaeHun Park", "authorId": "1484057728"}, {"name": "Ko-tik Lee", "authorId": "2110019663"}, {"name": "Hyesu Lim", "authorId": "2151202457"}, {"name": "Jaeseok Kim", "authorId": "2293573807"}, {"name": "Junmo Park", "authorId": "2305630071"}, {"name": "Yu-Jung Heo", "authorId": "2293447208"}, {"name": "Du-Seong Chang", "authorId": "2293430063"}, {"name": "Jaegul Choo", "authorId": "2260653165"}], "n_citations": 3}, "snippets": ["We find that these artifacts can significantly affect the models, confirmed by extensive experiments across diverse models, languages, and translation processes", "Evaluation results are shown in Table 5. Notably, models trained on translated texts usually outperform those trained on human texts in translate-test sets.These results suggest that, despite a mismatch between the MT systems used for RT translation and the translate-test, leveraging RT translation for training remains advantageous for cross-lingual transfer", "In the original English evaluation set, models with human texts perform best, followed by the ones with NLLB-200-3.3B texts."], "score": 0.89453125}, {"id": "(Bland'on et al., 2025)", "paper": {"corpus_id": 276575630, "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mar'ia Andrea Cruz Bland'on", "authorId": "2346978280"}, {"name": "Jayasimha Talur", "authorId": "114938178"}, {"name": "Bruno Charron", "authorId": "2346980693"}, {"name": "Dong Liu", "authorId": "2343588002"}, {"name": "Saab Mansour", "authorId": "39674628"}, {"name": "Marcello Federico", "authorId": "2346980678"}], "n_citations": 0}, "snippets": ["Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks."], "score": 0.97509765625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 270562911, "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pinzhen Chen", "authorId": "143616669"}, {"name": "Simon Yu", "authorId": "2307212816"}, {"name": "Zhicheng Guo", "authorId": "2307224542"}, {"name": "B. Haddow", "authorId": "2259100"}], "n_citations": 3}, "snippets": ["Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks."], "score": 0.0}, {"id": "(Baker et al., 1993)", "paper": {"corpus_id": 57174748, "title": "'Corpus Linguistics and Translation Studies: Implications and Applications'", "year": 1993, "venue": "", "authors": [{"name": "Mona Baker", "authorId": "103983301"}, {"name": "G. Francis", "authorId": "46880608"}, {"name": "E. Tognini-Bonelli", "authorId": "1404600818"}], "n_citations": 1061}, "snippets": ["The rise of corpus linguistics has serious implications for any discipline in which language plays a major role. This paper explores the impact that the availability of corpora is likely to have on the study of translation as an empirical phenomenon. It argues that the techniques and methodology developed in the field of corpus linguistics will have a direct impact on the emerging discipline of translation studies, particularly with respect to its theoretical and descriptive branches. The nature of this impact is discussed in some detail and brief reference is made to some of the applications of corpus techniques in the applied branch of the dis\u00ad cipline."], "score": 0.0}, {"id": "(Hennig et al., 2023)", "paper": {"corpus_id": 258557740, "title": "MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leonhard Hennig", "authorId": "36943315"}, {"name": "Philippe E. Thomas", "authorId": "143680610"}, {"name": "Sebastian M\u00f6ller", "authorId": "145733288"}], "n_citations": 8}, "snippets": ["The MultiTACRED dataset we present in this work covers 12 languages from 9 language families. We automatically and manually analyze translation and annotation projection quality in all target languages, both in general terms and with respect to the RE task, and identify typical error categories for alignment and translation that may affect model performance. We find that overall translation quality is judged to be quite good with respect to the RE task, but that e.g. pronoun-dropping, coordination and compounding may cause alignment and semantic errors that result in erroneous instances", ".Although the quality of machine-translated data may be lower due to translation and alignment errors (Yarmohammadi et al., 2021), it has been shown to be beneficial for classification and structured prediction tasks (Hu et al., 2020;(Ozaki et al., 2021)Yarmohammadi et al., 2021)."], "score": 0.90185546875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273323302, "title": "Language Imbalance Driven Rewarding for Multilingual Self-improving", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Wen Yang", "authorId": "2218735807"}, {"name": "Junhong Wu", "authorId": "2237788942"}, {"name": "Chen Wang", "authorId": "2188130997"}, {"name": "Chengqing Zong", "authorId": "2064100826"}, {"name": "Jiajun Zhang", "authorId": "2303798059"}], "n_citations": 7}, "snippets": ["Due to the expense and scarcity of multilingual benchmarks, most benchmarks in multilingualrelated work, including both open-ended and structured tests, are predominantly machine-translated from English into other languages. Since the preference data is also constructed using translation, there is a possibility that \"translationese bias\" could be exploited. However, our approach leverages LLMs for self-translation to construct training data, which offers key advantages to avoid translationese bias: \n\n(1) Different Data Distributions: Our method uses LLM self-translation to construct training data, while multilingual benchmarks are derived from machine translation of English datasets. This ensures that the training data and benchmark data have different distributions, effectively minimizing the risk of translationese bias influencing evaluation. \n\n(2) Reduction of Translationese Artifacts: LLM self-translation significantly reduces translationese effects, producing fluent and natural translations that align closely with native text. This is supported by prior works (Chen et al., 2023c;(Kunilovskaya et al., 2024), which highlights the high-quality outputs of LLMs."], "score": 0.931640625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 273532366, "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junhua Liu", "authorId": "2327246234"}, {"name": "Bin Fu", "authorId": "2325881142"}], "n_citations": 1}, "snippets": ["In general, the construction of multilingual evaluation datasets lacks linguistic diversity and rarely covers lowresource languages. Before the emergence of large models, English evaluation datasets were usually translated into multiple target languages through machine translation to create evaluation datasets. This method cannot generate natural, representative target languages, which affects the validity of the evaluation."], "score": 0.92822265625}, {"id": "(Ozaki et al., 2021)", "paper": {"corpus_id": 233189585, "title": "Project-then-Transfer: Effective Two-stage Cross-lingual Transfer for Semantic Dependency Parsing", "year": 2021, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Hiroaki Ozaki", "authorId": "36904068"}, {"name": "Gaku Morio", "authorId": "29347584"}, {"name": "Terufumi Morishita", "authorId": "1379579811"}, {"name": "Toshinori Miyoshi", "authorId": "2468213"}], "n_citations": 7}, "snippets": ["This paper describes the first report on cross-lingual transfer for semantic dependency parsing. We present the insight that there are twodifferent kinds of cross-linguality, namely sur-face level and mantic level, and try to cap-ture both kinds of cross-linguality by combin-ing annotation projection and model transferof pre-trained language models. Our exper-iments showed that the performance of our graph-based semantic dependency parser almost achieved the approximated upper bound."], "score": 0.0}, {"id": "(Qiu et al., 2022)", "paper": {"corpus_id": 253098629, "title": "Multilingual Multimodal Learning with Machine Translated Text", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Qiu", "authorId": "3383271"}, {"name": "Dan Onea\u0163\u0103", "authorId": "3095774"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 15}, "snippets": ["We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."], "score": 0.8955078125}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268819377, "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias", "year": 2024, "venue": "Frontiers Comput. Sci.", "authors": [{"name": "Yuemei Xu", "authorId": "2257136845"}, {"name": "Ling Hu", "authorId": "2258334185"}, {"name": "Jiayi Zhao", "authorId": "2294513520"}, {"name": "Zihan Qiu", "authorId": "2294361104"}, {"name": "Yuqi Ye", "authorId": "2294363807"}, {"name": "Hanwen Gu", "authorId": "2294933103"}], "n_citations": 43}, "snippets": ["Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."], "score": 0.92724609375}], "table": null}, {"title": "Impact on Cross-lingual Evaluation Reliability", "tldr": "The reliability of cross-lingual evaluation is significantly compromised when using translation-based datasets due to factors such as translation artifacts, cultural misalignment, and uneven translation quality across languages. These limitations can lead to overly optimistic performance estimates that do not accurately reflect models' true capabilities when serving users in different languages and cultural contexts. (18 sources)", "text": "\nThe impact of dataset creation methodology on cross-lingual evaluation reliability extends beyond mere methodological preferences to fundamentally shape our understanding of multilingual model capabilities. Translation-based evaluation approaches introduce systematic biases that can significantly distort performance assessments, often producing excessively optimistic estimates of models' cross-lingual abilities <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper> <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>. This \"unnaturally skewed\" evaluation data creates unreliable benchmarks that may not reflect real-world performance in target languages <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nA key factor undermining evaluation reliability is the presence of translation inconsistencies, which disproportionately affect low-resource languages. Studies of the XNLI dataset revealed poor agreement between human-translated test instances and their original English labels, particularly for languages like Hindi and Urdu <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>. These inconsistencies can be identified by measuring performance gaps between evaluations on human-translated versus machine-translated target text across languages <Paper corpusId=\"267413041\" paperTitle=\"(Agrawal et al., 2024)\" isShortName></Paper>.\n\nTranslation quality significantly influences evaluation outcomes, with high-resource languages appearing to outperform low-resource languages partly due to better translation systems rather than actual model capabilities <Paper corpusId=\"235313293\" paperTitle=\"(Razumovskaia et al., 2021)\" isShortName></Paper>. This creates artificial performance hierarchies that may not reflect true linguistic competence. Additionally, translation accuracy varies by task type and content domain, with name entities posing particular challenges for translation-based methods in question answering tasks <Paper corpusId=\"196174566\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nCultural representation presents another significant challenge for evaluation reliability. Translated benchmarks often introduce biases stemming from cultural differences <Paper corpusId=\"268819377\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. These datasets typically reflect the source culture rather than authentically representing the target culture, compromising their validity for evaluating models intended to serve diverse linguistic communities <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>. Cultural biases cannot be mitigated through automatic translation alone; they require either human annotators to evaluate and address cultural sensitivities or the development of benchmarks based on regional resources <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe limitations of translation-based evaluation are particularly evident in task-oriented dialogue (TOD) systems, where researchers have demonstrated that models perform better on translation-based evaluation sets than on more natural, natively generated examples <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>. This finding reveals that \"using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications\" <Paper corpusId=\"246430787\" paperTitle=\"(Majewska et al., 2022)\" isShortName></Paper>.\n\nTo address these reliability concerns, researchers have proposed several approaches. Using target-language originating evaluation data rather than translated data is critical, particularly in multimodal setups where \"fitting to translationese is a risk when using translation data at training time\" <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This risk can only be identified if evaluation data does not also contain translations, especially automatically generated ones <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>.\n\nCreating culturally and linguistically tailored evaluation datasets represents another important strategy. Recent research has demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to their translated counterparts (0.47) <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This finding has motivated an increasing focus on developing native evaluation benchmarks that better represent local cultural knowledge <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270380088\" paperTitle=\"(Etxaniz et al., 2024)\" isShortName></Paper>.\n\nA balanced approach involves combining translation-based and natively collected evaluation data. Projects like XOR-TYDI QA exemplify this hybrid methodology, incorporating both in-language data (where answers are extracted from Wikipedia in the question's language) and cross-lingual data (where English spans are translated to target languages) <Paper corpusId=\"236428949\" paperTitle=\"(Asai et al., 2021)\" isShortName></Paper>. Similarly, researchers have suggested that human translation or original data from target cultures, at least for high-quality evaluation data, represents a practical middle ground that \"allows us to judge the success of cross-lingual transfer in a culturally appropriate way\" <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"237503047\" paperTitle=\"(Yin et al., 2021)\" isShortName></Paper> <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nAs multilingual evaluation evolves, researchers increasingly recognize that the reliability of cross-lingual assessments depends on addressing both translation artifacts and cultural representation. While translation-based datasets offer valuable parallel structures for cross-language comparisons, they must be complemented with natively authored resources to provide a comprehensive and authentic evaluation of models' cross-lingual capabilities <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. Without such complementary approaches, evaluations risk failing to detect cultural biases or distribution shifts that affect real-world model performance <Paper corpusId=\"274149965\" paperTitle=\"(Seto et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248780386\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Razumovskaia et al., 2021)", "paper": {"corpus_id": 235313293, "title": "Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems", "year": 2021, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "E. Razumovskaia", "authorId": "66879943"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "A. Korhonen", "authorId": "145762466"}, {"name": "Ivan Vulic", "authorId": "1747849"}], "n_citations": 34}, "snippets": ["These artefacts, introduced by the translation procedure, could make the dataset not representative of real-life dialogue and cultural context of the target language (Hershcovich, Frank, Lent, de Lhoneux, Abdou, Brandl, Bugliarello, Piqueras, Chalkidis, Cui, et al., 2022) and instead give an edge to translation-based cross-lingual transfer. Hence, the evaluation performance becomes unreliable and excessively optimistic (Artetxe et al., 2020). Koppel and Ordan (2011) studied the differences between translated-into-English and original English texts. They demonstrate that there is a significant difference in lexical characteristics of the texts: e.g., there are some stark differences in the frequency of usage of functional words and pronouns. Recent work by Majewska et al. (2022) presents a qualitative analysis in the context of dataset creation for multilingual ToD, comparing dialogue data obtained via translation and free-form generation by native speakers of the target language. The paper presents multiple examples of the bias from English on both lexical and structural syntactic level."], "score": 0.8642578125}, {"id": "(Majewska et al., 2022)", "paper": {"corpus_id": 246430787, "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Olga Majewska", "authorId": "46963731"}, {"name": "E. Razumovskaia", "authorId": "66879943"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 28}, "snippets": ["The results in Table 8 indicate that the stronger performance is observed on translation-based evaluation sets than on more natural, outline-based generated examples. The results corroborate previous observations in other areas of NLP, e.g., machine translation (Graham et al., 2020), now for TOD. Crucially, this experiment verifies that using solely translation-based TOD evaluation data might yield an overly optimistic estimation of models' cross-lingual capabilities and, consequently, too optimistic performance expectations in real-life applications. This further validates our proposed outline-based approach to (more natural and targetgrounded) multilingual TOD data creation", ".the translation-based data are encoded into sentence representations that are much more similar to their English source than the corresponding outline-generated examples. The difference holds across dev and test splits and across different multilingual sentence encoders (see also Appendix C). This indicates that, as expected, the utterances obtained via translation are artificially more similar to their English counterparts than the outline-generated ones. This again underlines the finding from Table 8: multilingual TOD datasets collected via outline-based generation should lead to more realistic assessments of multilingual TOD models than translation-based multilingual TOD datasets", ".Direct translation has the benefit of re-using already annotated and verified data entries, moreover, it is a well-defined task which does not require task-specific guidelines or training. However, as we demonstrated here, it unnaturally skews the data towards the source language. This makes evaluation results unreliable."], "score": 0.9873046875}, {"id": "(Agrawal et al., 2024)", "paper": {"corpus_id": 267413041, "title": "Translation Errors Significantly Impact Low-Resource Languages in Cross-Lingual Learning", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ashish Agrawal", "authorId": "2282542641"}, {"name": "Barah Fazili", "authorId": "2187454108"}, {"name": "P. Jyothi", "authorId": "144859542"}], "n_citations": 3}, "snippets": ["We find that translation inconsistencies do exist and interestingly they disproportionally impact low-resource languages in XNLI. To identify such inconsistencies, we propose measuring the gap in performance between zero-shot evaluations on the human-translated and machine-translated target text across multiple target languages; relatively large gaps are indicative of translation errors. We also corroborate that translation errors exist for two target languages, namely Hindi and Urdu, by doing a manual reannotation of human-translated test instances in these two languages and finding poor agreement with the original English labels these instances were supposed to inherit."], "score": 0.962890625}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 196174566, "title": "XQA: A Cross-lingual Open-domain Question Answering Dataset", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jiahua Liu", "authorId": "46701066"}, {"name": "Yankai Lin", "authorId": "2427350"}, {"name": "Zhiyuan Liu", "authorId": "49293587"}, {"name": "Maosong Sun", "authorId": "1753344"}], "n_citations": 91}, "snippets": ["The experimental results demonstrate that there is a gap between the performance in English and that in cross-lingual setting. The multilingual BERT model achieves the best performance in al-most all target languages, while translation-based methods suffer from the problem of translating name entities. We show that the performance on the XQA dataset depends on not only how similar the target language and English are, but also how difficult the question set of the target language is."], "score": 0.9013671875}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268819377, "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias", "year": 2024, "venue": "Frontiers Comput. Sci.", "authors": [{"name": "Yuemei Xu", "authorId": "2257136845"}, {"name": "Ling Hu", "authorId": "2258334185"}, {"name": "Jiayi Zhao", "authorId": "2294513520"}, {"name": "Zihan Qiu", "authorId": "2294361104"}, {"name": "Yuqi Ye", "authorId": "2294363807"}, {"name": "Hanwen Gu", "authorId": "2294933103"}], "n_citations": 43}, "snippets": ["Nevertheless, it is important to note that translated benchmarks may introduce additional biases due to translation errors and cultural differences."], "score": 0.92724609375}, {"id": "(Etxaniz et al., 2024)", "paper": {"corpus_id": 270380088, "title": "BertaQA: How Much Do Language Models Know About Local Culture?", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Julen Etxaniz", "authorId": "2226458991"}, {"name": "Gorka Azkune", "authorId": "2481918"}, {"name": "A. Soroa", "authorId": "2260104163"}, {"name": "Oier L\u00f3pez de Lacalle", "authorId": "2251043402"}, {"name": "Mikel Artetxe", "authorId": "2347956"}], "n_citations": 11}, "snippets": ["Research in NLP evaluation has predominantly focused in English, with most multilingual benchmarks being translated from this language, such as XNLI [Conneau et al., 2018], XQUAD [Artetxe et al., 2019], MLQA [Lewis et al., 2019] and Belebele [Bandarkar et al., 2023]. This parallel nature facilitates monolingual, multilingual, and cross-lingual experiments, enabling valuable comparisons across languages. However, this approach introduces biases related to translations and cultural representation, affecting experimental conclusions by reflecting the origin culture. \n\nRecently, there has been a focus on creating native evaluation benchmarks to assess local cultural knowledge, rather than relying on translations from English. These native datasets, which resemble popular English benchmarks, include unique cultural elements that are generally more challenging for current models. They usually are of higher quality than machine or human-translated datasets."], "score": 0.94482421875}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."], "score": 0.90966796875}, {"id": "(Qiu et al., 2022)", "paper": {"corpus_id": 253098629, "title": "Multilingual Multimodal Learning with Machine Translated Text", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Qiu", "authorId": "3383271"}, {"name": "Dan Onea\u0163\u0103", "authorId": "3095774"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 15}, "snippets": ["We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."], "score": 0.8955078125}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277993848, "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghao Wu", "authorId": "2327995656"}, {"name": "Weixuan Wang", "authorId": "2258789284"}, {"name": "Sinuo Liu", "authorId": "2349802912"}, {"name": "Huifeng Yin", "authorId": "2331767275"}, {"name": "Xintong Wang", "authorId": "2356794252"}, {"name": "Yu Zhao", "authorId": "2331686159"}, {"name": "Chenyang Lyu", "authorId": "2266387313"}, {"name": "Longyue Wang", "authorId": "2302342302"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 5}, "snippets": ["Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)", "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."], "score": 0.91796875}, {"id": "(Asai et al., 2021)", "paper": {"corpus_id": 236428949, "title": "One Question Answering Model for Many Languages with Cross-lingual Dense Passage Retrieval", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Xinyan Velocity Yu", "authorId": "2118211280"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 73}, "snippets": ["Multilingual open QA datasets differ in covered languages, annotation schemes, and target application scenarios", "XOR-TYDI QA. XOR-TYDI QA (Asai et al., 2021) is a multilingual open QA dataset consisting of 7 typologically diverse languages, where questions are originally from TYDI QA (Clark et al., 2020) and posed by information-seeking native speakers. The answers are annotated by extracting spans from Wikipedia in the same language as the question (in-language data) or by translating English spans extracted from English Wikipedia to the target language (cross-lingual data)", "MKQA. MKQA (Longpre et al., 2020) is an evaluation dataset created by translating 10k Natural Questions (Kwiatkowski et al., 2019) to 25 target languages. The parallel data enables us to compare the models' performance across typologically diverse languages, in contrast to XOR-TYDI QA."], "score": 0.91357421875}, {"id": "(Hershcovich et al., 2022)", "paper": {"corpus_id": 247594499, "title": "Challenges and Strategies in Cross-Cultural NLP", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Daniel Hershcovich", "authorId": "2064295987"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Heather Lent", "authorId": "49568895"}, {"name": "Miryam de Lhoneux", "authorId": "3295381"}, {"name": "Mostafa Abdou", "authorId": "30671790"}, {"name": "Stephanie Brandl", "authorId": "6547490"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Laura Cabello Piqueras", "authorId": "2093582149"}, {"name": "Ilias Chalkidis", "authorId": "2125376289"}, {"name": "Ruixiang Cui", "authorId": "1717462692"}, {"name": "Constanza Fierro", "authorId": "50110151"}, {"name": "Katerina Margatina", "authorId": "82259306"}, {"name": "Phillip Rust", "authorId": "1660797358"}, {"name": "Anders S\u00f8gaard", "authorId": "1700187"}], "n_citations": 182}, "snippets": ["Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture", "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations", "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."], "score": 0.90380859375}, {"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."], "score": 0.0}, {"id": "(Yin et al., 2021)", "paper": {"corpus_id": 237503047, "title": "Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Da Yin", "authorId": "144508458"}, {"name": "Liunian Harold Li", "authorId": "2108904535"}, {"name": "Ziniu Hu", "authorId": "3407296"}, {"name": "Nanyun Peng", "authorId": "3157053"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}], "n_citations": 56}, "snippets": ["Commonsense is defined as the knowledge on which everyone agrees. However, certain types of commonsense knowledge are correlated with culture and geographic locations and they are only shared locally. For example, the scenes of wedding ceremonies vary across regions due to different customs influenced by historical and religious factors. Such regional characteristics, however, are generally omitted in prior work. In this paper, we construct a Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test vision-and-language models\u2019 ability to understand cultural and geo-location-specific commonsense. In particular, we study two state-of-the-art Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard benchmark with images primarily from Western regions. We then evaluate how well the trained models can generalize to answering the questions in GD-VCR. We find that the performance of both models for non-Western regions including East Asia, South Asia, and Africa is significantly lower than that for Western region. We analyze the reasons behind the performance disparity and find that the performance gap is larger on QA pairs that: 1) are concerned with culture-related scenarios, e.g., weddings, religious activities, and festivals; 2) require high-level geo-diverse commonsense reasoning rather than low-order perception and recognition. Dataset and code are released at https://github.com/WadeYin9712/GD-VCR."], "score": 0.0}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 238198104, "title": "Visually Grounded Reasoning across Languages and Cultures", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fangyu Liu", "authorId": "144097210"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Siva Reddy", "authorId": "145732771"}, {"name": "Nigel Collier", "authorId": "50638196"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 180}, "snippets": ["The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."], "score": 0.0}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 258865558, "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Sneha Kudugunta", "authorId": "35871436"}, {"name": "Xinyan Velocity Yu", "authorId": "2118211280"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Machel Reid", "authorId": "1557386977"}, {"name": "Yulia Tsvetkov", "authorId": "2073587169"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 62}, "snippets": ["Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."], "score": 0.9326171875}, {"id": "(Artetxe et al._1, 2020)", "paper": {"corpus_id": 216914383, "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Gorka Labaka", "authorId": "3255091"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 72}, "snippets": ["We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."], "score": 0.0}, {"id": "(Seto et al., 2024)", "paper": {"corpus_id": 274149965, "title": "Training Bilingual LMs with Data Constraints in the Targeted Language", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Skyler Seto", "authorId": "2324783252"}, {"name": "Maartje ter Hoeve", "authorId": "41096186"}, {"name": "He Bai", "authorId": "2331509164"}, {"name": "Natalie Schluter", "authorId": "3344220"}, {"name": "David Grangier", "authorId": "2529182"}], "n_citations": 1}, "snippets": ["Another limitation in evaluating language models for languages other than English is that many datasets have been translated from English. These datasets may contain cultural biases or information that is not available on the web in other languages. As a result, certain aspects of the evaluation may lead to improved performance when using English auxiliary or translated data. Additionally, translated data often exhibits a distribution different from that of real data in the target languages. Therefore, an important direction for future work is the development of evaluation datasets that are not based on translation, which is essential for more accurate evaluation of multilingual language models."], "score": 0.9091796875}, {"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 248780386, "title": "Beyond Static models and test sets: Benchmarking the potential of pre-trained models across tasks and languages", "year": 2022, "venue": "NLPPOWER", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["Machine Translation can be one way to extend test sets in different benchmarks to a much larger set of languages. Hu et al. (2020) provides pseudo test sets for tasks like XQUAD and XNLI, obtained by translating English test data into different languages, and shows reasonable estimates of the actual performance by evaluating on translated data but cautions about their reliability when the model is trained on translated data. The accuracy of translation based evaluation can be affected by the quality of translation and the technique incurs non-zero costs to obtain reliable translations. Moreover, transferring labels with translation might also be non-trivial for certain tasks like Part of Speech Tagging and Named Entity Recognition."], "score": 0.95751953125}], "table": null}, {"title": "Recommendations and Best Practices", "tldr": "To improve multilingual evaluation reliability, researchers recommend complementing translation-based datasets with natively collected resources, using consistent test sets across languages, and prioritizing cultural sensitivity. Best practices include creating culturally tailored evaluation datasets for high-stakes assessments, using target-language originating data for evaluation, and implementing hybrid approaches that balance practicality with authenticity. (15 sources)", "text": "\nDespite the limitations of translation-based approaches to multilingual evaluation, researchers have proposed several evidence-based recommendations to improve cross-lingual assessment reliability. These recommendations aim to balance practical constraints with the need for culturally appropriate and linguistically authentic evaluation.\n\nA fundamental recommendation is to complement translation-based benchmarks with natively authored multilingual benchmarks to provide a more comprehensive evaluation of cross-lingual capabilities <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper>. This dual approach acknowledges that while translation-based datasets enable valuable cross-language comparisons, they often contain \"translationese\" phenomena that make them distributionally different from native data <Paper corpusId=\"276575630\" paperTitle=\"(Bland'on et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270562911\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper> <Paper corpusId=\"57174748\" paperTitle=\"(Baker et al., 1993)\" isShortName></Paper>.\n\nFor translation-based evaluation datasets, researchers strongly recommend ensuring consistent test sets across languages. This can be achieved through: (1) using original annotations in all languages, (2) using original annotations in a non-English language and translating them into English and other languages, or (3) if translating from English, doing so at the document level to minimize translation inconsistencies <Paper corpusId=\"215548041\" paperTitle=\"(Artetxe et al., 2020)\" isShortName></Paper>. These approaches help address the English-centric bias common in multilingual benchmarks.\n\nCultural representation requires particular attention in multilingual evaluation. Researchers emphasize that cultural biases cannot be mitigated through automatic translation alone and recommend two primary approaches: (1) using human annotators to evaluate cultural sensitivity in translated content, or (2) developing benchmarks based on regional resources that inherently reflect local knowledge <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>. The latter approach has been implemented in benchmarks like Include, which uses local exam sources rather than translating benchmarks with inherent cultural bias <Paper corpusId=\"276421738\" paperTitle=\"(Barth et al., 2025)\" isShortName></Paper>.\n\nThe importance of target-language originating evaluation data is consistently emphasized, particularly for multimodal setups. Researchers warn that \"fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones\" <Paper corpusId=\"253098629\" paperTitle=\"(Qiu et al., 2022)\" isShortName></Paper>. This recommendation reflects growing concerns about the reliability of assessments based solely on translated content.\n\nA pragmatic middle-ground approach involves using human translation or original data from target cultures, at least for high-quality evaluation data. This balanced strategy \"allows us to judge the success of cross-lingual transfer in a culturally appropriate way\" <Paper corpusId=\"247594499\" paperTitle=\"(Hershcovich et al., 2022)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper> <Paper corpusId=\"238198104\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. It acknowledges resource constraints while prioritizing culturally sensitive assessment where it matters most.\n\nRecent empirical evidence strongly supports these recommendations. Research has demonstrated that localized benchmarks show significantly higher alignment with local human judgments (0.68 correlation) compared to their translated counterparts (0.47 correlation) <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. This substantial difference in correlation underscores the importance of creating culturally and linguistically tailored evaluation datasets rather than relying solely on translations.\n\nThe development of native evaluation benchmarks has already begun to address these concerns, with resources like IndoMMLU for Indonesian <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263829211\" paperTitle=\"(Koto et al., 2023)\" isShortName></Paper> and TurkishMMLU <Paper corpusId=\"277993848\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271245129\" paperTitle=\"(Yuksel et al., 2024)\" isShortName></Paper> providing culturally appropriate assessments inspired by established methodologies like MMLU <Paper corpusId=\"221516475\" paperTitle=\"(Hendrycks et al., 2020)\" isShortName></Paper>. These resources reflect a growing recognition that multilingual evaluation must extend beyond translation to include native cultural knowledge and linguistic patterns.\n\nUltimately, researchers recommend being transparent about the creation process of multilingual datasets and their potential limitations <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>. This includes acknowledging when datasets might exhibit undesired biases such as translation artifacts or unnatural topic distributions <Paper corpusId=\"258865558\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper> <Paper corpusId=\"216914383\" paperTitle=\"(Artetxe et al._1, 2020)\" isShortName></Paper> <Paper corpusId=\"218470125\" paperTitle=\"(Ponti et al., 2020)\" isShortName></Paper>. This transparency helps users of these datasets make informed decisions about their applicability to specific research questions and use cases.", "citations": [{"id": "(Bland'on et al., 2025)", "paper": {"corpus_id": 276575630, "title": "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mar'ia Andrea Cruz Bland'on", "authorId": "2346978280"}, {"name": "Jayasimha Talur", "authorId": "114938178"}, {"name": "Bruno Charron", "authorId": "2346980693"}, {"name": "Dong Liu", "authorId": "2343588002"}, {"name": "Saab Mansour", "authorId": "39674628"}, {"name": "Marcello Federico", "authorId": "2346980678"}], "n_citations": 0}, "snippets": ["Translation-based benchmarks, while permitting cross-language comparisons, suffer from translationese phenomena such as introducing simpler syntax and lexical choices (Baker et al., 1993)Graham et al., 2020), thus lead-ing to data distributionally different from native data and not necessarily reflecting native users preferences (Chen et al., 2024). Our position is that translation-based (parallel) benchmarks should be complemented by native multilingual benchmarks."], "score": 0.97509765625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 270562911, "title": "Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models?", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pinzhen Chen", "authorId": "143616669"}, {"name": "Simon Yu", "authorId": "2307212816"}, {"name": "Zhicheng Guo", "authorId": "2307224542"}, {"name": "B. Haddow", "authorId": "2259100"}], "n_citations": 3}, "snippets": ["Multilingual large language models are designed, claimed, and expected to cater to speakers of varied languages. We hypothesise that the current practices of fine-tuning and evaluating these models may not perfectly align with this objective owing to a heavy reliance on translation, which cannot cover language-specific knowledge but can introduce translation defects. It remains unknown whether the nature of the instruction data has an impact on the model output; conversely, it is questionable whether translated test sets can capture such nuances. Due to the often coupled practices of using translated data in both stages, such imperfections could have been overlooked. This work investigates these issues using controlled native or translated data during the instruction tuning and evaluation stages. We show that native or generation benchmarks reveal a notable difference between native and translated instruction data especially when model performance is high, whereas other types of test sets cannot. The comparison between round-trip and single-pass translations reflects the importance of knowledge from language-native resources. Finally, we demonstrate that regularization is beneficial to bridging this gap on structured but not generative tasks."], "score": 0.0}, {"id": "(Baker et al., 1993)", "paper": {"corpus_id": 57174748, "title": "'Corpus Linguistics and Translation Studies: Implications and Applications'", "year": 1993, "venue": "", "authors": [{"name": "Mona Baker", "authorId": "103983301"}, {"name": "G. Francis", "authorId": "46880608"}, {"name": "E. Tognini-Bonelli", "authorId": "1404600818"}], "n_citations": 1061}, "snippets": ["The rise of corpus linguistics has serious implications for any discipline in which language plays a major role. This paper explores the impact that the availability of corpora is likely to have on the study of translation as an empirical phenomenon. It argues that the techniques and methodology developed in the field of corpus linguistics will have a direct impact on the emerging discipline of translation studies, particularly with respect to its theoretical and descriptive branches. The nature of this impact is discussed in some detail and brief reference is made to some of the applications of corpus techniques in the applied branch of the dis\u00ad cipline."], "score": 0.0}, {"id": "(Artetxe et al., 2020)", "paper": {"corpus_id": 215548041, "title": "Translation Artifacts in Cross-lingual Transfer Learning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Gorka Labaka", "authorId": "2064469779"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 120}, "snippets": ["While most NLP resources are English-specific, there have been several recent efforts to build multilingual benchmarks. One possibility is to collect and annotate data in multiple languages separately (Clark et al., 2020), but most existing datasets have been created through translation (Conneau et al., 2018)(Artetxe et al., 2019). This approach has two desirable properties: it relies on existing professional translation services rather than requiring expertise in multiple languages, and it results in parallel evaluation sets that offer a meaningful measure of the cross-lingual transfer gap of different models", "Despite overlooked to date, we show that such mismatch has a notable impact in the performance of existing cross-lingual models. By using back-translation (Sennrich et al., 2016) to paraphrase each training instance, we obtain another English version of the training set that better resembles the test set, obtaining substantial improvements for the TRANSLATE-TEST and ZERO-SHOT approaches in cross-lingual Natural Language Inference (NLI)", "In the absence of more robust datasets, we recommend that future multilingual benchmarks should at least provide consistent test sets for English and the rest of languages. This can be achieved by (i) using original annotations in all languages, (ii) using original annotations in a non-English language and translating them into English and other languages, or (iii) if translating from English, doing so at the document level to minimize translation inconsistencies."], "score": 0.9716796875}, {"id": "(Barth et al., 2025)", "paper": {"corpus_id": 276421738, "title": "Multilingual European Language Models: Benchmarking Approaches and Challenges", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Fabio Barth", "authorId": "2325726142"}, {"name": "Georg Rehm", "authorId": "2302558975"}], "n_citations": 0}, "snippets": ["Cultural biases will not be mitigated if benchmarks are automatically translated from English into other languages. The published benchmarks apply two different options to address this limitation. First, human annotators evaluate cultural biases present in the original dataset. Global-MMLU, for instance, improved the quality of a multilingual MMLU by engaging with professional and community annotators that label samples as culturally-sensitive or culturallyagnostic (Singh et al., 2024). Using annotators to verify translations and evaluate cultural bias supports the effectiveness of a multilingual benchmark. \n\nThe second option is to develop a benchmark based on regional resources. The Include benchmark (Romanou et al., 2024) was created based on local exam sources instead of translating benchmarks with inherent cultural bias and debiasing translations. Local exams contain questions about local history, culture, politics, and geographical and regional knowledge."], "score": 0.90966796875}, {"id": "(Qiu et al., 2022)", "paper": {"corpus_id": 253098629, "title": "Multilingual Multimodal Learning with Machine Translated Text", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Chen Qiu", "authorId": "3383271"}, {"name": "Dan Onea\u0163\u0103", "authorId": "3095774"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 15}, "snippets": ["We would also like to stress the importance of using target-language originating evaluation data in multimodal setups, rather than translated data. Fitting to translationese is a risk when using translation data at training time, and can only be identified if the evaluation data does not also contain translations, especially automatically generated ones."], "score": 0.8955078125}, {"id": "(Hershcovich et al., 2022)", "paper": {"corpus_id": 247594499, "title": "Challenges and Strategies in Cross-Cultural NLP", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Daniel Hershcovich", "authorId": "2064295987"}, {"name": "Stella Frank", "authorId": "37922370"}, {"name": "Heather Lent", "authorId": "49568895"}, {"name": "Miryam de Lhoneux", "authorId": "3295381"}, {"name": "Mostafa Abdou", "authorId": "30671790"}, {"name": "Stephanie Brandl", "authorId": "6547490"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "Laura Cabello Piqueras", "authorId": "2093582149"}, {"name": "Ilias Chalkidis", "authorId": "2125376289"}, {"name": "Ruixiang Cui", "authorId": "1717462692"}, {"name": "Constanza Fierro", "authorId": "50110151"}, {"name": "Katerina Margatina", "authorId": "82259306"}, {"name": "Phillip Rust", "authorId": "1660797358"}, {"name": "Anders S\u00f8gaard", "authorId": "1700187"}], "n_citations": 182}, "snippets": ["Translating this dataset into other languages will require making decisions about how, and whether, to modify these items to make them more intelligible in the target culture", "Automatically translated training data can lead to worse performance than native target language data (Liu et al., 2021). However, if evaluation data is automatically translated too, we have no trivial way of exposing cultural biases introduced by the projection process. Culturallyaware evaluation thus necessitates data annotated directly in the target language, or at least culturallysensitive human translations", "Human translation, or original data from the target culture, is clearly the expensive option, but will often be the only way to avoid cultural bias. Only translating/generating high-quality evaluation data is becoming an attractive middle ground option (Liu et al., 2021)(Ponti et al., 2020)(Yin et al., 2021) that at least allows us to judge the success of crosslingual transfer in a culturally appropriate way."], "score": 0.90380859375}, {"id": "(Ponti et al., 2020)", "paper": {"corpus_id": 218470125, "title": "XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "E. Ponti", "authorId": "3381663"}, {"name": "Goran Glavavs", "authorId": "1666177566"}, {"name": "Olga Majewska", "authorId": "46963731"}, {"name": "Qianchu Liu", "authorId": "50383712"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "A. Korhonen", "authorId": "145762466"}], "n_citations": 327}, "snippets": ["In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at this http URL."], "score": 0.0}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 238198104, "title": "Visually Grounded Reasoning across Languages and Cultures", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fangyu Liu", "authorId": "144097210"}, {"name": "Emanuele Bugliarello", "authorId": "83574123"}, {"name": "E. Ponti", "authorId": "3381663"}, {"name": "Siva Reddy", "authorId": "145732771"}, {"name": "Nigel Collier", "authorId": "50638196"}, {"name": "Desmond Elliott", "authorId": "50369944"}], "n_citations": 180}, "snippets": ["The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277993848, "title": "The Bitter Lesson Learned from 2,000+ Multilingual Benchmarks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Minghao Wu", "authorId": "2327995656"}, {"name": "Weixuan Wang", "authorId": "2258789284"}, {"name": "Sinuo Liu", "authorId": "2349802912"}, {"name": "Huifeng Yin", "authorId": "2331767275"}, {"name": "Xintong Wang", "authorId": "2356794252"}, {"name": "Yu Zhao", "authorId": "2331686159"}, {"name": "Chenyang Lyu", "authorId": "2266387313"}, {"name": "Longyue Wang", "authorId": "2302342302"}, {"name": "Weihua Luo", "authorId": "2305289815"}, {"name": "Kaifu Zhang", "authorId": "2304530663"}], "n_citations": 5}, "snippets": ["Recent research has adopted two primary approaches to multilingual evaluation. The first involves translating existing English evaluation suites into other languages, using either human translators or machine translation systems (Shi et al., 2022)(Lai et al., 2023), Singh et al., 2024a]. The second approach focuses on curating new evaluation suites directly in the target language. For example, inspired by (Hendrycks et al., 2020), several studies have collected human exam questions from specific regions in the target language to assess LLM performance for local users (Koto et al., 2023), Li et al., 2024(Yuksel et al., 2024)", "Moreover, translating English benchmarks into other languages proves insufficient, as localized benchmarks demonstrate significantly higher alignment with local human judgments (0.68) than their translated counterparts (0.47). This underscores the importance of creating culturally and linguistically tailored benchmarks rather than relying solely on translations."], "score": 0.91796875}, {"id": "(Koto et al., 2023)", "paper": {"corpus_id": 263829211, "title": "Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fajri Koto", "authorId": "2789148"}, {"name": "Nurul Aisyah", "authorId": "2256987672"}, {"name": "Haonan Li", "authorId": "49404498"}, {"name": "Timothy Baldwin", "authorId": "2256987316"}], "n_citations": 46}, "snippets": ["Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels."], "score": 0.0}, {"id": "(Yuksel et al., 2024)", "paper": {"corpus_id": 271245129, "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in Turkish", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Arda Yuksel", "authorId": "2311698426"}, {"name": "Abdullatif K\u00f6ksal", "authorId": "1999179692"}, {"name": "Lutfi Kerem cSenel", "authorId": "2126865294"}, {"name": "Anna Korhonen", "authorId": "2311700614"}, {"name": "Hinrich Schutze", "authorId": "2130001188"}], "n_citations": 14}, "snippets": ["Multiple choice question answering tasks evaluate the reasoning, comprehension, and mathematical abilities of Large Language Models (LLMs). While existing benchmarks employ automatic translation for multilingual evaluation, this approach is error-prone and potentially introduces culturally biased questions, especially in social sciences. We introduce the first multitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs' understanding of the Turkish language. TurkishMMLU includes over 10,000 questions, covering 9 different subjects from Turkish high-school education curricula. These questions are written by curriculum experts, suitable for the high-school curricula in Turkey, covering subjects ranging from natural sciences and math questions to more culturally representative topics such as Turkish Literature and the history of the Turkish Republic. We evaluate over 20 LLMs, including multilingual open-source (e.g., Gemma, Llama, MT5), closed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol) models. We provide an extensive evaluation, including zero-shot and few-shot evaluation of LLMs, chain-of-thought reasoning, and question difficulty analysis along with model performance. We provide an in-depth analysis of the Turkish capabilities and limitations of current LLMs to provide insights for future LLMs for the Turkish language. We publicly release our code for the dataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."], "score": 0.0}, {"id": "(Hendrycks et al., 2020)", "paper": {"corpus_id": 221516475, "title": "Measuring Massive Multitask Language Understanding", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dan Hendrycks", "authorId": "3422872"}, {"name": "Collin Burns", "authorId": "90909974"}, {"name": "Steven Basart", "authorId": "104444594"}, {"name": "Andy Zou", "authorId": "1380103052"}, {"name": "Mantas Mazeika", "authorId": "16787428"}, {"name": "D. Song", "authorId": "143711382"}, {"name": "J. Steinhardt", "authorId": "5164568"}], "n_citations": 4568}, "snippets": ["We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings."], "score": 0.0}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 258865558, "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Sneha Kudugunta", "authorId": "35871436"}, {"name": "Xinyan Velocity Yu", "authorId": "2118211280"}, {"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Machel Reid", "authorId": "1557386977"}, {"name": "Yulia Tsvetkov", "authorId": "2073587169"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 62}, "snippets": ["Beyond evaluations on translated data. Prior few-or zero-shot evaluations were often conducted on widely-used datasets translated from English (e.g., XNLI; Conneau et al. 2018, XCOPA;(Ponti et al., 2020). Those datasets might exhibit undesired biases, such as translation artifacts or unnatural topic distributions (Clark et al., 2020;(Artetxe et al., 2020). We collect both translation-based datasets and datasets that are annotated directly in each language (Table 2, Data curation)."], "score": 0.9326171875}, {"id": "(Artetxe et al._1, 2020)", "paper": {"corpus_id": 216914383, "title": "A Call for More Rigor in Unsupervised Cross-lingual Learning", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mikel Artetxe", "authorId": "2347956"}, {"name": "Sebastian Ruder", "authorId": "2884561"}, {"name": "Dani Yogatama", "authorId": "1755465"}, {"name": "Gorka Labaka", "authorId": "3255091"}, {"name": "Eneko Agirre", "authorId": "1733049"}], "n_citations": 72}, "snippets": ["We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world\u2019s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models."], "score": 0.0}], "table": null}], "cost": 0.530064}}
