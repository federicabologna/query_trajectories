{"clarifying_information": [{"clarifying_question1": "Are you interested in evaluating the quality of GPT-4 generated data specifically for general language tasks, or do you have a particular use case or domain (e.g., medical, legal) in mind?", "clarifying_answer1": {"clarifying_answer": "I am interested in evaluating GPT-4 generated data quality for a specific use case: academic scientific writing."}}, {"clarifying_question2": "Do you want to compare the effectiveness and potential biases of using GPT-4 to evaluate its own outputs versus using a different, possibly more or less capable, LLM as the evaluator?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in comparing the effectiveness and potential biases of using GPT-4 to evaluate its own outputs versus using a different LLM as the evaluator, including whether the alternative model is more or less capable than GPT-4."}}, {"clarifying_question3": "Are you seeking best practices for automating the evaluation process with LLMs alone, or would you like recommendations on integrating human feedback and standardized benchmarks alongside LLM-based evaluation?", "clarifying_answer3": {"clarifying_answer": "I am primarily interested in best practices for automating the evaluation process with LLMs alone. However, I would also appreciate brief recommendations on when or how to supplement with human feedback and standardized benchmarks for more robust evaluation if possible."}}], "better_query": "For the automated evaluation of GPT-4 generated scientific academic writing, what are the effectiveness and bias considerations when using GPT-4 as its own evaluator compared to using a different LLM (either more or less capable), and what best practices should guide the choice?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "The evaluation of AI-generated academic writing has become crucial as LLMs like GPT-4 are increasingly used in scientific contexts. This creates a complex evaluation challenge where researchers must consider whether to use the same model as both generator and evaluator or employ different models, each approach having distinct implications for effectiveness and bias. (LLM Memory)", "text": "\nLarge language models (LLMs) like GPT-4 are increasingly being used to generate scientific academic writing, from drafting research papers to summarizing literature and formulating hypotheses. As these AI-generated texts become more prevalent in academic contexts, the need for robust, efficient, and unbiased evaluation methods has emerged as a critical concern. Researchers and institutions face a fundamental question: when evaluating AI-generated academic content, should they use the same model that generated the content (self-evaluation) or employ a different model (cross-evaluation)? This question is particularly relevant when using GPT-4, which possesses advanced capabilities for both generation and evaluation of complex academic content. The choice between self-evaluation and cross-evaluation involves intricate trade-offs related to effectiveness, various forms of bias, and practical implementation considerations. This creates a complex evaluation landscape where researchers must carefully weigh these factors to establish credible assessment frameworks for AI-generated scientific writing. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Effectiveness of GPT-4 as an Evaluator", "tldr": "GPT-4 demonstrates strong capabilities as an evaluator of scientific academic writing, showing high agreement with human evaluators (80-98%) when properly prompted. However, its effectiveness varies based on task complexity, prompt engineering strategies, and implementation approaches like chain-of-thought reasoning and self-consistency mechanisms. (10 sources)", "text": "\nGPT-4 has emerged as a powerful automated evaluator for academic writing, with studies confirming its ability to assess various aspects of scientific content with considerable accuracy. When properly designed, GPT-4-based evaluation systems can achieve remarkable alignment with human judgment. Zou et al. found that GPT-4 demonstrated a 98% agreement with human annotators when evaluating academic responses, significantly outperforming traditional evaluation methods <Paper corpusId=\"271212285\" paperTitle=\"(Zou et al., 2024)\" isShortName></Paper>. Similarly, Wu et al. reported that GPT-4's evaluations exceed 80% consistency with human reviewers and reach up to 85% alignment in pairwise comparisons, approaching near-perfect agreement when performance differences are significant <Paper corpusId=\"271544398\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of GPT-4 as an evaluator is notably influenced by prompt engineering strategies. Tang et al. found that more elaborate and specific prompts, particularly those incorporating criteria-based scoring with justifications, enable GPT-4 to more closely approximate human-level consistency in automated essay scoring tasks <Paper corpusId=\"271126365\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This finding underscores the importance of thoughtful prompt design in maximizing GPT-4's evaluation capabilities.\n\nAdvanced evaluation frameworks have further enhanced GPT-4's effectiveness. The G-EVAL framework, introduced by Liu et al., employs a chain-of-thought paradigm that guides the evaluation process through detailed intermediate steps <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This approach has shown improved correlation with human evaluations for tasks such as text summarization and dialogue generation. G-EVAL also addresses score distribution issues by implementing a self-consistency strategy that generates multiple evaluation samples and averages the scores, mitigating the tendency of LLMs to output integer scores and favor certain values <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper>.\n\nFor scientific synthesis evaluation specifically, Giglou et al. demonstrated the effectiveness of GPT-4-Turbo as an automatic evaluator, implementing a cross-validation approach where the model performed multiple assessments on the same data samples to ensure consistent evaluation <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>. Their detailed prompt specification reduced ambiguity in the evaluation task, resulting in minimal divergence between assessment runs <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>.\n\nDespite these promising capabilities, researchers acknowledge that GPT-4 is not without limitations as an evaluator. Kim et al. cautioned that while GPT-4 is a powerful tool for automated essay scoring, \"it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring\" <Paper corpusId=\"271050803\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. Additionally, Yousefi et al. highlighted that LLMs can exhibit biases similar to human judgments and are susceptible to hallucination\u2014generating plausible but factually incorrect information\u2014which poses particular risks in academic evaluation contexts <Paper corpusId=\"273346449\" paperTitle=\"(Yousefi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265442718\" paperTitle=\"(Buchanan et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Zou et al., 2024)", "paper": {"corpus_id": 271212285, "title": "DOCBENCH: A Benchmark for Evaluating LLM-based Document Reading Systems", "year": 2024, "venue": "Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing", "authors": [{"name": "Anni Zou", "authorId": "2187586628"}, {"name": "Wenhao Yu", "authorId": "2265843326"}, {"name": "Hongming Zhang", "authorId": "2254831297"}, {"name": "Kaixin Ma", "authorId": "2311458574"}, {"name": "Deng Cai", "authorId": "2286805668"}, {"name": "Zhuosheng Zhang", "authorId": "3322871"}, {"name": "Hai Zhao", "authorId": "2287982566"}, {"name": "Dong Yu", "authorId": "2261392681"}], "n_citations": 9}, "snippets": ["(ii) Different LLMs and systems exhibit substantial variations in the organization and style of their outputs, potentially leading to biases in traditional evaluation approaches. Therefore, we capitalize on the prowess of LLMs that have proven to be decent evaluators and can be easily adapted to the assessment of various answer types [14,24,39]. Inspired by Liu et al. [24], we clearly define the evaluation criteria for various types within the instruction prompt and then instruct GPT-4 to assign a score of 0 (incorrect) or 1 (correct). After evaluating 200 examples by both human evaluators and GPT-4, we found that the GPT-4 automatic evaluator shows a 98% agreement with human annotators, significantly exceeding the traditional string matching approach."], "score": 0.580078125}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 271544398, "title": "Automated literature research and review-generation method based on large language models", "year": 2024, "venue": "National Science Review", "authors": [{"name": "Shican Wu", "authorId": "2142349414"}, {"name": "Xiao Ma", "authorId": "2313884864"}, {"name": "Dehui Luo", "authorId": "2313733635"}, {"name": "Lulu Li", "authorId": "2277565237"}, {"name": "Xiangcheng Shi", "authorId": "2289200637"}, {"name": "Xin Chang", "authorId": "2289200103"}, {"name": "Xiaoyun Lin", "authorId": "2152842807"}, {"name": "Ran Luo", "authorId": "2075340202"}, {"name": "Chunlei Pei", "authorId": "48471690"}, {"name": "Zhijian Zhao", "authorId": "12144921"}, {"name": "Jinlong Gong", "authorId": "2258427381"}], "n_citations": 0}, "snippets": ["Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36]."], "score": 0.63232421875}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 259129398, "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Lianmin Zheng", "authorId": "2149970173"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Ying Sheng", "authorId": "2209360681"}, {"name": "Siyuan Zhuang", "authorId": "92721493"}, {"name": "Zhanghao Wu", "authorId": "1390573666"}, {"name": "Yonghao Zhuang", "authorId": "2152482391"}, {"name": "Zi Lin", "authorId": "143872641"}, {"name": "Zhuohan Li", "authorId": "2141335450"}, {"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "E. Xing", "authorId": "143977260"}, {"name": "Haotong Zhang", "authorId": "145140331"}, {"name": "Joseph E. Gonzalez", "authorId": "49988044"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 4439}, "snippets": ["Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."], "score": 0.0}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 271126365, "title": "Harnessing LLMs for multi-dimensional writing assessment: Reliability and alignment with human judgments", "year": 2024, "venue": "Heliyon", "authors": [{"name": "Xiaoyi Tang", "authorId": "2118488377"}, {"name": "Hongwei Chen", "authorId": "2272635431"}, {"name": "Daoyu Lin", "authorId": "2302524052"}, {"name": "Kexin Li", "authorId": "2301924234"}], "n_citations": 7}, "snippets": ["The experimental outcomes corroborate that the types of prompt engineering and the models employed markedly influence the dependability of LLMs in AES tasks. The evidence suggests that more elaborated and specific prompts, encompassing criteria-based scoring with justifications, facilitate LLMs to approximate human-level consistency more closely. GPT-4, as the most advanced model evaluated, exhibited a heightened concordance with human raters, especially when justifications were integrated into the scoring processes."], "score": 0.7041015625}, {"id": "(Pereira et al., 2024)", "paper": {"corpus_id": 271310142, "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jayr Pereira", "authorId": "2257137831"}, {"name": "R.A. Lotufo", "authorId": "2256889299"}], "n_citations": 6}, "snippets": ["G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested."], "score": 0.65576171875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yang Liu", "authorId": "2152797401"}, {"name": "Dan Iter", "authorId": "3310951"}, {"name": "Yichong Xu", "authorId": "2110197273"}, {"name": "Shuo Wang", "authorId": "2146294891"}, {"name": "Ruochen Xu", "authorId": "8233965"}, {"name": "Chenguang Zhu", "authorId": "8652308"}], "n_citations": 1211}, "snippets": ["G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."], "score": 0.55078125}, {"id": "(Giglou et al., 2024)", "paper": {"corpus_id": 272968899, "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis", "year": 2024, "venue": "ACM/IEEE Joint Conference on Digital Libraries", "authors": [{"name": "Hamed Babaei Giglou", "authorId": "2360502238"}, {"name": "Jennifer D'Souza", "authorId": "2261403432"}, {"name": "S. Auer", "authorId": "2268415251"}], "n_citations": 5}, "snippets": ["Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing", "Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported."], "score": 0.837890625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 271050803, "title": "Is GPT-4 Alone Sufficient for Automated Essay Scoring?: A Comparative Judgment Approach Based on Rater Cognition", "year": 2024, "venue": "ACM Conference on Learning @ Scale", "authors": [{"name": "Seungju Kim", "authorId": "2310489304"}, {"name": "Meounggun Jo", "authorId": "2310341618"}], "n_citations": 3}, "snippets": ["This study shows that while GPT-4 is a powerful tool for AES, it is not sufficient on its own, as many factors influence both human raters and LLMs in essay scoring."], "score": 0.6640625}, {"id": "(Yousefi et al., 2024)", "paper": {"corpus_id": 273346449, "title": "Learning the Bitter Lesson: Empirical Evidence from 20 Years of CVPR Proceedings", "year": 2024, "venue": "NLP4SCIENCE", "authors": [{"name": "Mojtaba Yousefi", "authorId": "2325908556"}, {"name": "Jack Collins", "authorId": "2325892124"}], "n_citations": 1}, "snippets": ["In addition to their analytical capabilities, LLMs have been shown to possess a degree of human-like judgment in evaluating the quality of text. The G-EVAL framework, which utilizes LLMs to assess the quality of natural language generation outputs, demonstrates that LLMs can align closely with human evaluators in certain contexts (Liu, 2023). However, the deployment of LLMs in academic evaluation is not without challenges. LLMs can exhibit biases similar to those found in human judgments, which may affect the fairness and accuracy of their evaluations (Acerbi, 2023). Furthermore, the phenomenon of \"hallucination,\" where LLMs produce plausible but factually incorrect information, poses a risk in academic contexts (Buchanan et al., 2023)."], "score": 0.57763671875}, {"id": "(Buchanan et al., 2023)", "paper": {"corpus_id": 265442718, "title": "ChatGPT Hallucinates Non-existent Citations: Evidence from Economics", "year": 2023, "venue": "The American Economist", "authors": [{"name": "Joy Buchanan", "authorId": "2268284776"}, {"name": "Stephen Hill", "authorId": "2268287596"}, {"name": "O. Shapoval", "authorId": "2184736732"}], "n_citations": 15}, "snippets": ["In this study, we generate prompts derived from every topic within the Journal of Economic Literature to assess the abilities of both GPT-3.5 and GPT-4 versions of the ChatGPT large language model (LLM) to write about economic concepts. ChatGPT demonstrates considerable competency in offering general summaries but also cites non-existent references. More than 30% of the citations provided by the GPT-3.5 version do not exist and this rate is only slightly reduced for the GPT-4 version. Additionally, our findings suggest that the reliability of the model decreases as the prompts become more specific. We provide quantitative evidence for errors in ChatGPT output to demonstrate the importance of LLM verification. JEL Codes: B4; O33; I2"], "score": 0.0}], "table": null}, {"title": "Bias Considerations When Using GPT-4 to Evaluate Its Own Outputs", "tldr": "GPT-4 shows significant self-preference bias when evaluating its own outputs or those from similar model families, often rating them higher than human evaluators would. This self-reinforcement tendency creates challenges for objective evaluation and may impede the development of competing models, necessitating mitigation strategies like using alternative evaluator models. (7 sources)", "text": "\nWhen using GPT-4 to evaluate its own outputs in scientific academic writing, researchers have identified several concerning bias patterns. One of the most significant is self-preference bias, where GPT-4 systematically favors outputs generated by itself or models from the same family. Liu et al. highlighted this concern, noting that \"G-EVAL may have a bias towards the LLM-generated summaries because the model could share the same concept of evaluation criteria during generation and evaluation\" <Paper corpusId=\"257804696\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This creates a problematic self-reinforcement cycle, especially if evaluation scores are used as reward signals for further model tuning.\n\nEvidence of this self-preference is substantial across multiple studies. Shi et al. found that \"GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Similarly, Chu et al. observed that \"GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses\" <Paper corpusId=\"267311508\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>. This bias creates a fundamental challenge for comparative evaluation, as Chen et al. demonstrated when they found \"a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods\" suggesting that \"GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability\" <Paper corpusId=\"273374964\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nThe implications of this bias extend beyond simple scoring discrepancies. Wu et al. raised \"concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. More concerning, Chu et al. suggested that this inherent bias \"may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4\" <Paper corpusId=\"267311508\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>, indicating that self-evaluation could potentially impede the progress of competing models.\n\nOrder effects represent another significant bias when using GPT-4 for evaluation. Shi et al. identified \"a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This recency bias can substantially impact evaluation outcomes if not properly controlled for in the evaluation design.\n\nTo address these concerns, researchers have begun implementing various mitigation strategies. For comparative evaluations, Shi et al. recommend \"reporting the mean score over both orders\" to mitigate recency bias <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Wu et al. suggested incorporating \"multiple LLM judges\" to \"establish a more comprehensive and unbiased assessment\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. A particularly promising approach involves using alternative models as evaluators. Chai et al. reported employing \"Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework\" <Paper corpusId=\"273901331\" paperTitle=\"(Chai et al., 2024)\" isShortName></Paper>. Gao et al. also documented \"systematic bias in using GPT evaluation\" when comparing human and GPT-4 rating behaviors <Paper corpusId=\"267783118\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>, reinforcing the need for careful consideration of evaluation design.", "citations": [{"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257804696, "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yang Liu", "authorId": "2152797401"}, {"name": "Dan Iter", "authorId": "3310951"}, {"name": "Yichong Xu", "authorId": "2110197273"}, {"name": "Shuo Wang", "authorId": "2146294891"}, {"name": "Ruochen Xu", "authorId": "8233965"}, {"name": "Chenguang Zhu", "authorId": "8652308"}], "n_citations": 1211}, "snippets": ["G-EVAL may have a bias towards the LLMgenerated summaries because the model could share the same concept of evaluation criteria during generation and evaluation.\n\nOur work should be considered as a preliminary study on this issue, and more research is needed to fully understand the behavior of LLM-based evaluators to reduce its inherent bias towards LLMgenerated text. We highlight this concern in the context that LLM-based evaluators may lead to self-reinforcement of LLMs if the evaluation score is used as a reward signal for further tuning. And this could result in the over-fitting of the LLMs to their own evaluation criteria, rather than the true evaluation criteria of the NLG tasks."], "score": 0.55078125}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 259274988, "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting", "year": 2023, "venue": "Journal of Biomedical Informatics", "authors": [{"name": "Yiwen Shi", "authorId": "2118898475"}, {"name": "Ping Ren", "authorId": "2061530524"}, {"name": "Jing Wang", "authorId": "2152438838"}, {"name": "Biao Han", "authorId": "2220800269"}, {"name": "Taha ValizadehAslani", "authorId": "65986913"}, {"name": "Felix Agbavor", "authorId": "1997966680"}, {"name": "Yi Zhang", "authorId": "2153915340"}, {"name": "Meng Hu", "authorId": "144642598"}, {"name": "Liang Zhao", "authorId": "144010790"}, {"name": "Hualou Liang", "authorId": "2220924999"}], "n_citations": 18}, "snippets": ["When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."], "score": 0.54150390625}, {"id": "(Chu et al., 2024)", "paper": {"corpus_id": 267311508, "title": "PRE: A Peer Review Based Large Language Model Evaluator", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhumin Chu", "authorId": "1381470019"}, {"name": "Qingyao Ai", "authorId": "2256982003"}, {"name": "Yiteng Tu", "authorId": "2275628230"}, {"name": "Haitao Li", "authorId": "2108590438"}, {"name": "Yiqun Liu", "authorId": "2260835922"}], "n_citations": 21}, "snippets": ["Although several works have demonstrated that GPT-4 has decent evaluation capabilities [19]26], we found that GPT-4, so as other LLMs, often prefers responses of LLMs from its own series (i.e., the GPT models) over other LLMs despite of the actual quality of the responses. In other words, if we use GPT-4 as the evaluator, its inherent bias may make it difficult, if possible, to develop an LLM outside the GPT family that outperforms GPT-4."], "score": 0.68798828125}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 273374964, "title": "An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junjie Chen", "authorId": "2326124996"}, {"name": "Weihang Su", "authorId": "2147219374"}, {"name": "Zhumin Chu", "authorId": "1381470019"}, {"name": "Haitao Li", "authorId": "2108590438"}, {"name": "Qinyao Ai", "authorId": "2326111223"}, {"name": "Yiqun Liu", "authorId": "1783406"}, {"name": "Min Zhang", "authorId": "2258690221"}, {"name": "Shaoping Ma", "authorId": "8093158"}], "n_citations": 3}, "snippets": ["From the results, we can observe a significant performance gap between GPT-4 and Auto-PRE, with GPT-4's rate much higher than that of other methods. This suggests that GPT-4's preference for answers from GPT-3.5-turbo compromises its performance and reliability. Conversely, Auto-PRE enhances overall performance and reliability by collaborating with various types of LLMs."], "score": 0.5966796875}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 259360998, "title": "Style Over Substance: Evaluation Biases for Large Language Models", "year": 2023, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Minghao Wu", "authorId": "2145209409"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}], "n_citations": 47}, "snippets": ["This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."], "score": 0.64013671875}, {"id": "(Chai et al., 2024)", "paper": {"corpus_id": 273901331, "title": "Exploring Scientific Hypothesis Generation with Mamba", "year": 2024, "venue": "NLP4SCIENCE", "authors": [{"name": "Miaosen Chai", "authorId": "2329735903"}, {"name": "Emily Herron", "authorId": "2329737480"}, {"name": "Erick Cervantes", "authorId": "2329736173"}, {"name": "Tirthankar Ghosal", "authorId": "2329737470"}], "n_citations": 5}, "snippets": ["To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b)."], "score": 0.6591796875}, {"id": "(Gao et al., 2023)", "paper": {"corpus_id": 267783118, "title": "Evaluating Large Language Models on Wikipedia-Style Survey Generation", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Fan Gao", "authorId": "2232906828"}, {"name": "Hang Jiang", "authorId": "48579520"}, {"name": "Rui Yang", "authorId": "2287835229"}, {"name": "Qingcheng Zeng", "authorId": "2153554138"}, {"name": "Jinghui Lu", "authorId": "2285824559"}, {"name": "Moritz Blum", "authorId": "2285108151"}, {"name": "Dairui Liu", "authorId": "1585849884"}, {"name": "Tianwei She", "authorId": "2106009217"}, {"name": "Yuang Jiang", "authorId": "2285289624"}, {"name": "Irene Li", "authorId": "2275053812"}], "n_citations": 9}, "snippets": ["We compare both human and GPT-based evaluation scores and provide in-depth analysis. While our findings suggest that GPT-created surveys are more contemporary and accessible than human-authored ones, certain limitations were observed. Notably, GPT-4, despite often delivering outstanding content, occasionally exhibited lapses like missing details or factual errors. At last, we compared the rating behavior between humans and GPT-4 and found systematic bias in using GPT evaluation."], "score": 0.5322265625}], "table": null}, {"title": "Position and Ordering Biases in LLM Evaluation", "tldr": "Evaluation using LLMs like GPT-4 shows significant position and ordering biases where models systematically favor outputs based on their position in the prompt rather than their quality. These biases are particularly pronounced when comparing similar-quality content, requiring specific mitigation strategies such as position swapping and averaging results across different orderings. (3 sources)", "text": "\nPosition and ordering biases represent a critical challenge in LLM-based evaluation of scientific writing that extends beyond the self-preference bias previously discussed. These biases occur when an LLM evaluator's judgment is systematically influenced by the position of the content being evaluated rather than its actual quality. Shi et al. identified \"a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt\" <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This recency bias can significantly distort evaluation results, particularly when comparing outputs of similar quality.\n\nThe direction and magnitude of position bias may vary depending on the evaluation task and protocol. Huang et al. observed that \"when GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry\" <Paper corpusId=\"262045067\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. They further noted that this bias was \"particularly pronounced when assessing similar-quality summaries,\" suggesting that position effects become more influential when qualitative differences between outputs are subtle <Paper corpusId=\"262045067\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. Interestingly, they found that these position effects can vary across different evaluation tasks, indicating that the nature and direction of the bias is not consistent and may depend on the specific evaluation context.\n\nTo address these position and ordering biases, researchers have developed several mitigation strategies. Shi et al. recommend \"reporting the mean score over both orders\" to counterbalance the recency bias <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. Taking this approach a step further, Chang et al. implemented a rigorous methodology where \"each pair is evaluated twice with the positions of the two TLDRs swapped\" and \"a method is considered the winner for a sample only if both evaluations favored the same TLDR\" <Paper corpusId=\"271769421\" paperTitle=\"(Chang et al., 2024)\" isShortName></Paper>. This position-swapping protocol helps identify genuinely superior outputs by ensuring that the preference remains consistent regardless of presentation order.\n\nPosition bias can significantly confound evaluation results, especially when GPT-4 \"may favor a summary that includes relevant contextual information not present in the other\" or \"penalize certain aspects based on subjective criteria\" rather than assessing overall quality <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This suggests that position effects interact with other evaluation biases, potentially amplifying inconsistencies in automated assessment of academic writing.", "citations": [{"id": "(Shi et al., 2023)", "paper": {"corpus_id": 259274988, "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting", "year": 2023, "venue": "Journal of Biomedical Informatics", "authors": [{"name": "Yiwen Shi", "authorId": "2118898475"}, {"name": "Ping Ren", "authorId": "2061530524"}, {"name": "Jing Wang", "authorId": "2152438838"}, {"name": "Biao Han", "authorId": "2220800269"}, {"name": "Taha ValizadehAslani", "authorId": "65986913"}, {"name": "Felix Agbavor", "authorId": "1997966680"}, {"name": "Yi Zhang", "authorId": "2153915340"}, {"name": "Meng Hu", "authorId": "144642598"}, {"name": "Liang Zhao", "authorId": "144010790"}, {"name": "Hualou Liang", "authorId": "2220924999"}], "n_citations": 18}, "snippets": ["When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."], "score": 0.54150390625}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 262045067, "title": "Embrace Divergence for Richer Insights: A Multi-document Summarization Benchmark and a Case Study on Summarizing Diverse Information from News Articles", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Kung-Hsiang Huang", "authorId": "1420116116"}, {"name": "Philippe Laban", "authorId": "46180754"}, {"name": "A. R. Fabbri", "authorId": "22281632"}, {"name": "Prafulla Kumar Choubey", "authorId": "3466801"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}, {"name": "Caiming Xiong", "authorId": "2054594326"}, {"name": "Chien-Sheng Wu", "authorId": "30340989"}], "n_citations": 32}, "snippets": ["Position bias is most relevant to the pairwise comparison protocol. While previous work has shown that GPT-4 does exhibit position bias when used to assess text quality in conversational-focused tasks (Wang et al., 2023;Zheng et al., 2023), none of the prior studies have investigated whether such bias is also observed when evaluating faithfulness or coverage. To analyze position bias, we task GPT-4 with assessing a pair of summaries generated by two LLMs on which one is better, and then swap the positions of these two summaries and query GPT-4 again. We compute the percentage of times GPT-4 prefers the first or second summaries. \n\nWhen GPT-4 compared pairs of LLM-generated summaries to evaluate faithfulness and coverage, a strong position bias surfaced, favoring the second entry (Table 3). Position bias was particularly pronounced when assessing similar-quality summaries (see Figure 23a). Hence, we deduce that GPT-4 is unreliable when utilized as a pairwise evaluator in the MDDS task with respect to faithfulness and coverage. Interestingly, this outcome contradicts Zheng et al. (2023), implying that the position of bias for LLM-based evaluators could vary across different tasks."], "score": 0.541015625}, {"id": "(Chang et al., 2024)", "paper": {"corpus_id": 271769421, "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization", "year": 2024, "venue": "SDP", "authors": [{"name": "Yuan Chang", "authorId": "2315438513"}, {"name": "Ziyue Li", "authorId": "2315367785"}, {"name": "Xiaoqiu Le", "authorId": "2315309245"}], "n_citations": 1}, "snippets": ["To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method", "To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie."], "score": 0.8828125}], "table": null}, {"title": "Alternative LLM Evaluators: Considerations", "tldr": "Using alternative LLMs as evaluators can mitigate self-preference bias, with researchers employing models like Claude to evaluate GPT outputs and creating hierarchies based on model capabilities. Specialized reward models trained specifically for evaluation tasks often outperform even the most advanced general-purpose LLMs like GPT-4 and Gemini when it comes to evaluation quality. (4 sources)", "text": "\nWhen considering alternatives to using GPT-4 to evaluate its own outputs, researchers have explored several approaches to address bias concerns. One strategy involves employing different LLM families entirely to mitigate self-preference bias. Chai et al. specifically chose \"Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework\" <Paper corpusId=\"273901331\" paperTitle=\"(Chai et al., 2024)\" isShortName></Paper>. This cross-model evaluation approach offers a promising way to obtain more objective assessments of scientific writing generated by GPT models.\n\nAnother consideration involves creating evaluation hierarchies based on model capabilities. Lu et al. explored this approach by comparing \"three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review,\" noting that \"GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers\" <Paper corpusId=\"270045144\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This capability-based hierarchy allows researchers to simulate different quality levels of evaluation, potentially providing more nuanced assessment frameworks.\n\nMultiple evaluator approaches have also proven effective for achieving more balanced assessments. Wu et al. addressed evaluation bias by incorporating \"multiple LLM judges\" including models from Anthropic \"in addition to GPT-4\" to \"establish a more comprehensive and unbiased assessment\" <Paper corpusId=\"259360998\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. This ensemble approach helps minimize the impact of any single model's biases, creating a more robust evaluation framework.\n\nBeyond using alternative general-purpose LLMs, specialized evaluation models have shown superior performance in specific contexts. Weng et al. noted that \"even the most advanced models, such as GPT-4 and Gemini, still lag behind reward models specifically trained for the task, as seen in RewardBench\" <Paper corpusId=\"273811997\" paperTitle=\"(Weng et al., 2024)\" isShortName></Paper>. This finding suggests that purpose-built evaluation models, despite having less general capability than frontier models like GPT-4, may provide more reliable assessments for scientific writing tasks due to their specialized training.", "citations": [{"id": "(Chai et al., 2024)", "paper": {"corpus_id": 273901331, "title": "Exploring Scientific Hypothesis Generation with Mamba", "year": 2024, "venue": "NLP4SCIENCE", "authors": [{"name": "Miaosen Chai", "authorId": "2329735903"}, {"name": "Emily Herron", "authorId": "2329737480"}, {"name": "Erick Cervantes", "authorId": "2329736173"}, {"name": "Tirthankar Ghosal", "authorId": "2329737470"}], "n_citations": 5}, "snippets": ["To address the limitations of automatic evaluation metrics, we incorporate an LLM evaluation to assess the quality of the generated scientific hypotheses. Specifically, we employ Claude-3.5 instead of the more mainstream GPT-4 to mitigate potential self-enhancement bias, which occurs when a model is evaluated using its own framework (Xu et al., 2024b)."], "score": 0.6591796875}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 270045144, "title": "Eliciting Informative Text Evaluations with Large Language Models", "year": 2024, "venue": "ACM Conference on Economics and Computation", "authors": [{"name": "Yuxuan Lu", "authorId": "2140025107"}, {"name": "Shengwei Xu", "authorId": "2110923845"}, {"name": "Yichi Zhang", "authorId": "2120311160"}, {"name": "Yuqing Kong", "authorId": "2273919664"}, {"name": "Grant Schoenebeck", "authorId": "2268317075"}], "n_citations": 7}, "snippets": ["We compare three types of reviews: the human-written review, the GPT-4-generated review, and the GPT-3.5-generated review. Given that GPT-4 is commonly considered a stronger AI than GPT-3.5, and both models are considered worse than human expert reviewers, we hypothesize that we can utilize the LLM-generated reviews to simulate low-quality human reviews, thereby simulating three levels of quality."], "score": 0.677734375}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 259360998, "title": "Style Over Substance: Evaluation Biases for Large Language Models", "year": 2023, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Minghao Wu", "authorId": "2145209409"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}], "n_citations": 47}, "snippets": ["This raises concerns about potential biases that GPT-4 may have towards its own outputs, which could skew the evaluation process. from Anthropic (Bai et al., 2022b) as an additional LLM judge, in addition to GPT-4. By incorporating multiple LLM judges, we can establish a more comprehensive and unbiased assessment of the generated answers in our study."], "score": 0.64013671875}, {"id": "(Weng et al., 2024)", "paper": {"corpus_id": 273811997, "title": "CycleResearcher: Improving Automated Research via Automated Review", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yixuan Weng", "authorId": "2142839441"}, {"name": "Minjun Zhu", "authorId": "2316827669"}, {"name": "Guangsheng Bao", "authorId": "1993226927"}, {"name": "Hongbo Zhang", "authorId": "2329135444"}, {"name": "Jindong Wang", "authorId": "2273553706"}, {"name": "Yue Zhang", "authorId": "2325943212"}, {"name": "Linyi Yang", "authorId": "2145500840"}], "n_citations": 25}, "snippets": ["when LLMs act as judges, even the most advanced models, such as GPT-4 (Achiam et al., 2023) and Gemini (Reid et al., 2024), still lag behind reward models specifically trained for the task, as seen in RewardBench (Lambert et al., 2024). This gap highlights the challenge of achieving human-level judgment and reasoning in AI-driven peer reviews. In contrast, we train a Generative Reward Model (Zhang et al., 2024) to simulate a comprehensive peer review."], "score": 0.75634765625}], "table": null}, {"title": "Best Practices for LLM-Based Evaluation", "tldr": "Effective LLM-based evaluation of scientific writing requires specific implementation strategies including temperature setting, position randomization, and multiple evaluations to mitigate biases. Techniques such as chain-of-thought reasoning, detailed prompting, and cross-model evaluation help achieve more reliable and consistent assessments that better align with human judgment. (10 sources)", "text": "\nBased on the research literature, the following best practices emerge for using LLMs to evaluate scientific academic writing:\n\n1. **Set deterministic generation parameters**: To address inconsistency issues in LLM evaluations, set temperature = 0 to ensure deterministic outputs, making evaluation results more reproducible. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n2. **Mitigate position and ordering bias**: Implement position swapping by evaluating each comparison twice with the positions of outputs reversed, and report the mean score across both orderings. Only consider a winner when both evaluations favor the same output. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"271769421\" paperTitle=\"(Chang et al., 2024)\" isShortName></Paper>\n\n3. **Use comparative rather than absolute evaluations**: The LLMCompare protocol, which compares two candidate outputs and selects the better one, has been shown to be the most effective evaluation approach when coupled with GPT-4. <Paper corpusId=\"270218703\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>\n\n4. **Implement chain-of-thought reasoning**: Structure evaluations to include detailed intermediate steps that guide the LLM through a systematic assessment process, improving correlation with human evaluations. <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper> <Paper corpusId=\"246411621\" paperTitle=\"(Wei et al., 2022)\" isShortName></Paper>\n\n5. **Apply self-consistency techniques**: Generate multiple evaluation samples with different decoding parameters and average the scores across samples to mitigate score distribution issues and reduce variability in assessment outcomes. <Paper corpusId=\"271310142\" paperTitle=\"(Pereira et al., 2024)\" isShortName></Paper>\n\n6. **Conduct multiple evaluation runs**: Similar to cross-validation, run the same evaluation multiple times on identical data samples and average the results to ensure assessment consistency. <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>\n\n7. **Provide detailed evaluation criteria**: Create comprehensive prompts with explicit evaluation criteria to reduce ambiguity and improve consistency across evaluation runs. <Paper corpusId=\"272968899\" paperTitle=\"(Giglou et al., 2024)\" isShortName></Paper>\n\n8. **Recognize task-specific limitations**: Acknowledge that while LLMs perform well in certain evaluation tasks (like identifying errors or verifying checklists), they may not be sufficient for comprehensive evaluations of scientific papers. <Paper corpusId=\"276296422\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263608784\" paperTitle=\"(Liang et al., 2023)\" isShortName></Paper>\n\n9. **Be aware of model limitations with similar-quality content**: LLMs like GPT-4 may struggle to differentiate between outputs of similar quality, potentially fixating on minor subjective differences rather than substantive quality distinctions. <Paper corpusId=\"259274988\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>\n\n10. **Utilize multiple LLM judges**: Employ different LLMs as evaluators to establish a more comprehensive and unbiased assessment framework that mitigates individual model biases. <Paper corpusId=\"271544398\" paperTitle=\"(Wu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259129398\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>", "citations": [{"id": "(Shi et al., 2023)", "paper": {"corpus_id": 259274988, "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting", "year": 2023, "venue": "Journal of Biomedical Informatics", "authors": [{"name": "Yiwen Shi", "authorId": "2118898475"}, {"name": "Ping Ren", "authorId": "2061530524"}, {"name": "Jing Wang", "authorId": "2152438838"}, {"name": "Biao Han", "authorId": "2220800269"}, {"name": "Taha ValizadehAslani", "authorId": "65986913"}, {"name": "Felix Agbavor", "authorId": "1997966680"}, {"name": "Yi Zhang", "authorId": "2153915340"}, {"name": "Meng Hu", "authorId": "144642598"}, {"name": "Liang Zhao", "authorId": "144010790"}, {"name": "Hualou Liang", "authorId": "2220924999"}], "n_citations": 18}, "snippets": ["When using GPT-4 to rate the performance of different models against the reference summary, there are a number of issues. First, there exists an inconsistency problem whereby there is no guarantee that an LLM will produce the identical output for the same input every time. A good practice to fix the problem is to set temperature = 0; the temperature parameter of the model controls the randomness of the text generated, with 0 being deterministic. Second, we find a strong ordering effect with GPT-4 assigning higher scores to the model appearing earlier in the prompt. To mitigate such recency bias, we recommend reporting the mean score over both orders. Third, we observe from Figure 4 that GPT-4 assigns significantly higher scores to its own outputs compared to human ratings (64% vs. 43%), which represents the propensity of the model to favor its own suggestions. Future work should investigate the existence of potential biases in GPT-4 evaluation as well as possible mitigation strategies.\n\nDespite the potential of GPT-4 for automating evaluation, it is not as flexible as humans, particularly in situations where specific definitions of summary quality are not provided. The performance of GPT-4 hence may vary due to the lack of clarity in evaluation standards. GPT-4 aligns with the human evaluation, concluding that it generates better summaries than ChatGPT. However, GPT-4 encounters difficulty distinguishing scenarios where both models produce equally good summaries. In cases where human assessors perceive the summaries as equally good, GPT-4 may favor a summary that includes relevant contextual information does not present in the other, such as \"explicitly linking it to a high-fat, high-calorie meal\" or \"mention the lack of clinically significant differences\". It also may penalize certain aspects based on subjective criteria, such as \"less concise\"."], "score": 0.54150390625}, {"id": "(Chang et al., 2024)", "paper": {"corpus_id": 271769421, "title": "Guiding Large Language Models via External Attention Prompting for Scientific Extreme Summarization", "year": 2024, "venue": "SDP", "authors": [{"name": "Yuan Chang", "authorId": "2315438513"}, {"name": "Ziyue Li", "authorId": "2315367785"}, {"name": "Xiaoqiu Le", "authorId": "2315309245"}], "n_citations": 1}, "snippets": ["To this end, we employ GPT-4 (version gpt-4-0613), which is currently recognized as one of the most advanced LLMs, to assess the quality of TL-DRs generated by our method compared to a baseline method", "To mitigate position bias (Zheng et al., 2023), each pair is evaluated twice with the positions of the two TLDRs swapped. A method is considered the winner for a sample only if both evaluations favored the same TLDR. In cases of conflicting results or ties, the sample is marked as a tie."], "score": 0.8828125}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 270218703, "title": "Prompt Chaining or Stepwise Prompt? Refinement in Text Summarization", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shichao Sun", "authorId": "2256995981"}, {"name": "Ruifeng Yuan", "authorId": "2273661632"}, {"name": "Ziqiang Cao", "authorId": "2314396"}, {"name": "Wenjie Li", "authorId": "2237591981"}, {"name": "Pengfei Liu", "authorId": "2256991660"}], "n_citations": 20}, "snippets": ["We use the LLMCompare as our evaluation protocol, which compares two candidate outputs and then selects the better one (Zheng et al., 2023)Wang et al., 2023).This is because LLMCompare coupled with GPT-4 is the best evaluation protocol, as mentioned in Liu et al. (2023).The evaluation prompts are shown in Appendix A."], "score": 0.5498046875}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 259129398, "title": "Judging LLM-as-a-judge with MT-Bench and Chatbot Arena", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Lianmin Zheng", "authorId": "2149970173"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Ying Sheng", "authorId": "2209360681"}, {"name": "Siyuan Zhuang", "authorId": "92721493"}, {"name": "Zhanghao Wu", "authorId": "1390573666"}, {"name": "Yonghao Zhuang", "authorId": "2152482391"}, {"name": "Zi Lin", "authorId": "143872641"}, {"name": "Zhuohan Li", "authorId": "2141335450"}, {"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "E. Xing", "authorId": "143977260"}, {"name": "Haotong Zhang", "authorId": "145140331"}, {"name": "Joseph E. Gonzalez", "authorId": "49988044"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 4439}, "snippets": ["Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge."], "score": 0.0}, {"id": "(Pereira et al., 2024)", "paper": {"corpus_id": 271310142, "title": "Check-Eval: A Checklist-based Approach for Evaluating Text Quality", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jayr Pereira", "authorId": "2257137831"}, {"name": "R.A. Lotufo", "authorId": "2256889299"}], "n_citations": 6}, "snippets": ["G-EVAL is another recent approach that leverages the capabilities of LLMs, specifically GPT-4, to improve NLG evaluation. Proposed by Liu et al. (2023) [Liu et al. 2023], G-EVAL introduces a chain-of-thought (CoT) (Wei et al., 2022) paradigm where the evaluation process is guided by detailed intermediate steps generated by the LLM. This method has shown improvements in correlation with human evaluations, particularly in tasks such as text summarization and dialogue generation. To address issues with score distribution and variability, G-EVAL employs a self-consistency strategy. Specifically, it generates multiple samples (n = 20) using different decoding parameters and averages the evaluation scores across these samples. This approach helps mitigate two key problems: the dominance of a single score (such as 3 on a 1-5 scale) and the tendency of LLMs to output integer scores, even when decimal values are requested."], "score": 0.65576171875}, {"id": "(Wei et al., 2022)", "paper": {"corpus_id": 246411621, "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Dale Schuurmans", "authorId": "1714772"}, {"name": "Maarten Bosma", "authorId": "40377863"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "F. Xia", "authorId": "144956443"}, {"name": "Quoc Le", "authorId": "1998340269"}, {"name": "Denny Zhou", "authorId": "65855107"}], "n_citations": 9683}, "snippets": ["We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."], "score": 0.0}, {"id": "(Giglou et al., 2024)", "paper": {"corpus_id": 272968899, "title": "LLMs4Synthesis: Leveraging Large Language Models for Scientific Synthesis", "year": 2024, "venue": "ACM/IEEE Joint Conference on Digital Libraries", "authors": [{"name": "Hamed Babaei Giglou", "authorId": "2360502238"}, {"name": "Jennifer D'Souza", "authorId": "2261403432"}, {"name": "S. Auer", "authorId": "2268415251"}], "n_citations": 5}, "snippets": ["Prior work has demonstrated the effectiveness of LLMs as automatic evaluators for scientific synthesis (Bai et al., 2023)17,19], arguing that they are becoming both the tools and the standards for language assessment. Thus, for the evaluation task, we utilized GPT-4-Turbo, the most advanced LLM available at the time of writing", "Finally, similar to 3-fold cross-validation, the GPT-4 synthesis evaluator was run thrice on the same data samples to ensure consistent assessment. The resulting scores, which did not show significant divergence largely due to our detailed prompt specification of the expected evaluation task leaving little room for ambiguity in interpretation, were averaged and reported."], "score": 0.837890625}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 276296422, "title": "SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers", "year": 2025, "venue": "Expert systems with applications", "authors": [{"name": "Wenqing Wu", "authorId": "2307769002"}, {"name": "Chengzhi Zhang", "authorId": "2267229283"}, {"name": "Tong Bao", "authorId": "2325373803"}, {"name": "Yi Zhao", "authorId": "2284551410"}], "n_citations": 1}, "snippets": ["Liang et al. (Liang et al., 2023) conducted a large-scale empirical analysis to assess comments generated by GPT-4: they tag several metrics and investigated user satisfaction to measure comment quality. They found that while LLM-generated reviews were helpful, they might be non-generic and tended to focus on certain aspects of scientific feedback. Liu and Shah (Liu and Shah, 2023) validated the utility of LLMs across three tasks: identifying errors, verifying checklists, and choosing the \"better\" pa-per. They concluded that LLMs serve well as review assistants for specific reviewing tasks; however, they are not yet sufficient for conducting comprehensive evaluations of papers. Mike Thelwall (Thelwall, 2024) used GPT-4 to evaluate the quality of journal articles on a paper assessment dataset to test its effectiveness. The results indicated that GPT-4 appears to be insufficiently accurate for any formal or informal research quality assessment tasks."], "score": 0.6328125}, {"id": "(Liang et al., 2023)", "paper": {"corpus_id": 263608784, "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis", "year": 2023, "venue": "NEJM AI", "authors": [{"name": "Weixin Liang", "authorId": "151253861"}, {"name": "Yuhui Zhang", "authorId": "49889860"}, {"name": "Hancheng Cao", "authorId": "2256613375"}, {"name": "Binglu Wang", "authorId": "2253999923"}, {"name": "Daisy Ding", "authorId": "2253469362"}, {"name": "Xinyu Yang", "authorId": "2216295251"}, {"name": "Kailas Vodrahalli", "authorId": "52224284"}, {"name": "Siyu He", "authorId": "2257314770"}, {"name": "D. Smith", "authorId": "2199559560"}, {"name": "Yian Yin", "authorId": "2256439343"}, {"name": "Daniel A. McFarland", "authorId": "2251233257"}, {"name": "James Zou", "authorId": "2244005287"}], "n_citations": 145}, "snippets": ["Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations."], "score": 0.888671875}, {"id": "(Wu et al., 2024)", "paper": {"corpus_id": 271544398, "title": "Automated literature research and review-generation method based on large language models", "year": 2024, "venue": "National Science Review", "authors": [{"name": "Shican Wu", "authorId": "2142349414"}, {"name": "Xiao Ma", "authorId": "2313884864"}, {"name": "Dehui Luo", "authorId": "2313733635"}, {"name": "Lulu Li", "authorId": "2277565237"}, {"name": "Xiangcheng Shi", "authorId": "2289200637"}, {"name": "Xin Chang", "authorId": "2289200103"}, {"name": "Xiaoyun Lin", "authorId": "2152842807"}, {"name": "Ran Luo", "authorId": "2075340202"}, {"name": "Chunlei Pei", "authorId": "48471690"}, {"name": "Zhijian Zhao", "authorId": "12144921"}, {"name": "Jinlong Gong", "authorId": "2258427381"}], "n_citations": 0}, "snippets": ["Studies confirm the capability of LLMs to evaluate other LLMs, with verification abilities improving faster than generation quality [29]. GPT-4's evaluations exceeds 80% consistency with human reviewers (Zheng et al., 2023) and exceeds 85% alignment in pairwise comparisons (Bai et al., 2023), reaching nearly 100% agreement when performance differences are significant (Bai et al., 2023). Self-verification benefits LLM performance [32], though biases issues [33], such as position bias [34], length bias (Zheng et al., 2023), and self-bias [35], persist but can be reduced through proper design [36]."], "score": 0.63232421875}], "table": null}], "cost": 0.3705900000000001}}
