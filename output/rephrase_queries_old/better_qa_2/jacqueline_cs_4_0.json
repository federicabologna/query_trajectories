{"clarifying_information": [{"clarifying_question1": "Are you most interested in the specific techniques for measuring data difficulty when designing a curriculum (such as cross-entropy, model uncertainty, or learning percentage) for language model training?", "clarifying_answer1": {"clarifying_answer": "Yes, I am most interested in the concrete techniques for measuring data difficulty\u2014such as cross-entropy, model uncertainty, and learning percentage\u2014as they relate to designing effective curricula for language model training."}}, {"clarifying_question2": "Would you like to focus on recent advances in data selection and curriculum scheduling strategies for large language models, such as skill acquisition frameworks, domain reweighting, or group-level selection?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in focusing on recent advances\u2014specifically in techniques such as skill acquisition frameworks, domain reweighting (e.g., DoReMi), and group-level or multi-granularity data selection strategies for large language models. Please emphasize innovations from the past 2-3 years and their empirical impacts on efficiency and model performance."}}, {"clarifying_question3": "Are you primarily seeking practical implementation guidelines and challenges for applying curriculum learning in large-scale LM pretraining, including computational overhead, redundancy management, and balancing diversity with difficulty?", "clarifying_answer3": {"clarifying_answer": "Yes, I am primarily interested in practical implementation guidelines and the main challenges (such as computational overhead, redundancy management, and balancing diversity vs. difficulty) for deploying curriculum learning in large-scale language model pretraining."}}], "better_query": "What are the most effective and recent techniques for measuring data difficulty\u2014such as cross-entropy, model uncertainty, and learning percentage\u2014for constructing curricula in large language model training?", "better_answer": {"sections": [{"title": "Introduction and Background", "tldr": "Data difficulty measurement is crucial for curriculum learning in LLM training, where models learn from progressively more challenging examples. Recent techniques range from simple heuristics to sophisticated training dynamics measures that help optimize the learning process. (LLM Memory)", "text": "\nCurriculum learning in Large Language Models (LLMs) involves training models on data ordered by difficulty, starting with simpler examples and gradually introducing more complex ones. This approach mimics human learning and has shown potential to improve model performance, convergence speed, and generalization. Measuring data difficulty accurately is therefore a critical component in constructing effective curricula for LLM training.\n\nResearchers have developed various techniques to quantify the difficulty of training examples. Traditional approaches often relied on simple heuristics like sentence length or vocabulary rarity. However, recent advances have introduced more sophisticated metrics that better capture the true learning challenge of each example. These metrics can be broadly categorized into loss-based measures, uncertainty estimates, training dynamics indicators, and predefined heuristics.\n\nThe effectiveness of curriculum learning depends heavily on how accurately we can measure example difficulty. Recent research has shown that dynamic difficulty assessment\u2014where the difficulty is measured during training rather than predetermined\u2014can be particularly effective for large language models. This approach recognizes that difficulty is not an inherent static property of the data but depends on the current state of the model and its learning progress. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nModern curriculum learning strategies for LLMs often employ automatic techniques to measure and adapt to difficulty levels during training. These approaches aim to optimize the presentation order of training examples to maximize learning efficiency and effectiveness. By carefully constructing curricula based on measured difficulty, researchers have demonstrated improvements in final model performance, training efficiency, and capability on complex reasoning tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Loss-based Difficulty Metrics", "tldr": "Loss-based metrics quantify training data difficulty by measuring how challenging it is for a model to learn specific examples, typically using cross-entropy or other loss functions. These metrics enable curriculum learning approaches where models first learn from low-loss \"easy\" examples before progressing to high-loss \"difficult\" ones. (15 sources)", "text": "\nLoss-based difficulty metrics are among the most common and straightforward approaches to quantify data difficulty for curriculum learning in LLM training. These metrics leverage the model's training output to assess how challenging each example is for the model to learn. The core assumption is that samples with higher loss values are more difficult for the model to process and learn from <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nCross-entropy loss is particularly prevalent in this category, serving as a direct indicator of the discrepancy between predicted and actual outcomes. When this loss is high, it indicates that the model struggles to correctly predict the expected output, suggesting greater difficulty <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. Many implementations use this instantaneous loss value from a single training epoch as the difficulty measure, with higher loss corresponding to increased difficulty <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nSelf-paced learning (SPL), a popular curriculum learning approach, directly employs loss values to determine which examples to prioritize during training. In traditional SPL, the model first learns from low-loss \"easy\" examples, and as training progresses, gradually incorporates higher-loss \"difficult\" examples <Paper corpusId=\"266741778\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"94785766\" paperTitle=\"(Gong et al., 2019)\" isShortName></Paper>. This approach is intuitive and has proven effective across various learning tasks <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nHowever, relying solely on instantaneous loss values can be problematic. Training loss can provide noisy estimates of sample difficulty due to the phenomenon of data memorization in neural networks <Paper corpusId=\"264819795\" paperTitle=\"(Elgaar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11455421\" paperTitle=\"(Arpit et al., 2017)\" isShortName></Paper> <Paper corpusId=\"6212000\" paperTitle=\"(Zhang et al., 2016)\" isShortName></Paper>. To address this limitation, researchers have explored variations on basic loss-based metrics:\n\n1. **Average Loss**: Instead of using a single epoch's loss, some approaches calculate the average loss over multiple training epochs or the entire training process to provide a more stable difficulty assessment <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\n2. **Moving Average Loss**: The exponential moving average (EMA) of loss values over time can better capture the temporal aspects of learning difficulty <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\n3. **Loss Change Rate**: Some methods measure the rate of change in loss values between consecutive training iterations, identifying examples that cause significant fluctuations as particularly difficult <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\n4. **Annotation Entropy**: For data with multiple human annotations, the entropy of annotator labels can serve as a proxy for difficulty. Higher entropy (more disagreement among annotators) correlates with examples that models also find difficult <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.\n\n5. **Combined Loss Metrics**: Recent research suggests that using both bias (average loss) and variance terms provides a more comprehensive difficulty measure than using loss alone <Paper corpusId=\"255749213\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>.\n\nFor automatic curriculum learning, these loss-based metrics can be integrated into training through various weighting schemes. Some approaches divide data into discrete difficulty buckets based on loss thresholds <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>, while others implement continuous weighting systems based on normalized loss values <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>.\n\nDespite their limitations, loss-based metrics remain popular due to their simplicity and effectiveness. They provide a model-centric perspective on difficulty that directly relates to the learning process, making them particularly well-suited for curriculum learning in LLM training <Paper corpusId=\"244119160\" paperTitle=\"(Zhu et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 248810776, "title": "Exploring the Learning Difficulty of Data: Theory and Measure", "year": 2022, "venue": "ACM Transactions on Knowledge Discovery from Data", "authors": [{"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ou Wu", "authorId": "2061463125"}, {"name": "Fengguang Su", "authorId": "2165378509"}, {"name": "Yingjun Deng", "authorId": "2165450358"}], "n_citations": 6}, "snippets": ["The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."], "score": 0.89306640625}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."], "score": 0.88037109375}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257937538, "title": "A Review of the Evaluation System for Curriculum Learning", "year": 2023, "venue": "Electronics", "authors": [{"name": "Fengchun Liu", "authorId": "144102068"}, {"name": "Tong Zhang", "authorId": "2213622721"}, {"name": "Chunying Zhang", "authorId": "1806923"}, {"name": "Lu Liu", "authorId": "145412331"}, {"name": "Liya Wang", "authorId": "2108947838"}, {"name": "Bin Liu", "authorId": "2213836973"}], "n_citations": 6}, "snippets": ["In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)", "Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains", "Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples."], "score": 0.79248046875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 266741778, "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Haopeng Li", "authorId": "2266086772"}, {"name": "Qiuhong Ke", "authorId": "2066160726"}, {"name": "Mingming Gong", "authorId": "2146826373"}, {"name": "Tom Drummond", "authorId": "2277600596"}], "n_citations": 1}, "snippets": ["In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL."], "score": 0.7392578125}, {"id": "(Gong et al., 2019)", "paper": {"corpus_id": 94785766, "title": "Decomposition-Based Evolutionary Multiobjective Optimization to Self-Paced Learning", "year": 2019, "venue": "IEEE Transactions on Evolutionary Computation", "authors": [{"name": "Maoguo Gong", "authorId": "144605807"}, {"name": "Hao Li", "authorId": "2152297783"}, {"name": "Deyu Meng", "authorId": "1803714"}, {"name": "Q. Miao", "authorId": "144145103"}, {"name": "Jia Liu", "authorId": "39280862"}], "n_citations": 54}, "snippets": ["Self-paced learning (SPL) is a recently proposed paradigm to imitate the learning process of humans/animals. SPL involves easier samples into training at first and then gradually takes more complex ones into consideration. Current SPL regimes incorporate a self-paced (SP) regularizer into the learning objective with a gradually increasing pace parameter. Therefore, it is difficult to obtain the solution path of the SPL regime and determine where to optimally stop this increasing process. In this paper, a multiobjective SPL method is proposed to optimize the loss function and the SP regularizer simultaneously. A decomposition-based multiobjective particle swarm optimization algorithm is used to simultaneously optimize the two objectives for obtaining the solutions. In the proposed method, a polynomial soft weighting regularizer is proposed to penalize the loss. Theoretical studies are conducted to show that the previous regularizers are roughly particular cases of the proposed polynomial soft weighting regularizer family. Then an implicit decomposition method is proposed to search the solutions with respect to the sample number involved into training. A set of solutions can be obtained by the proposed method and naturally constitute the solution path of the SPL regime. Then a satisfactory solution can be naturally obtained from these solutions by utilizing some effective tools in evolutionary multiobjective optimization. Experiments on matrix factorization and classification problems demonstrate the effectiveness of the proposed technique."], "score": 0.0}, {"id": "(Elgaar et al., 2023)", "paper": {"corpus_id": 264819795, "title": "Ling-CL: Understanding NLP Models through Linguistic Curricula", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 2}, "snippets": ["Second, existing approaches quantify the difficulty of data based on instantaneous training loss. However, training loss provides noisy estimates of sample difficulty due to data memorization (Zhang et al., 2016)(Arpit et al., 2017) in neural models."], "score": 0.76025390625}, {"id": "(Arpit et al., 2017)", "paper": {"corpus_id": 11455421, "title": "A Closer Look at Memorization in Deep Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Devansh Arpit", "authorId": "2309967"}, {"name": "Stanislaw Jastrzebski", "authorId": "40569328"}, {"name": "Nicolas Ballas", "authorId": "2482072"}, {"name": "David Krueger", "authorId": "145055042"}, {"name": "Emmanuel Bengio", "authorId": "2286872375"}, {"name": "Maxinder S. Kanwal", "authorId": "19308176"}, {"name": "Tegan Maharaj", "authorId": "3422058"}, {"name": "Asja Fischer", "authorId": "35988982"}, {"name": "Aaron C. Courville", "authorId": "1760871"}, {"name": "Yoshua Bengio", "authorId": "1751762"}, {"name": "Simon Lacoste-Julien", "authorId": "1388317459"}], "n_citations": 1827}, "snippets": ["We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization."], "score": 0.0}, {"id": "(Zhang et al., 2016)", "paper": {"corpus_id": 6212000, "title": "Understanding deep learning requires rethinking generalization", "year": 2016, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chiyuan Zhang", "authorId": "151505981"}, {"name": "Samy Bengio", "authorId": "1751569"}, {"name": "Moritz Hardt", "authorId": "1775622"}, {"name": "B. Recht", "authorId": "9229182"}, {"name": "O. Vinyals", "authorId": "1689108"}], "n_citations": 4637}, "snippets": ["Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. \nThrough extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. \nWe interpret our experimental findings by comparison with traditional models."], "score": 0.0}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."], "score": 0.7119140625}, {"id": "(Nie et al., 2020)", "paper": {"corpus_id": 222177851, "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yixin Nie", "authorId": "40383658"}, {"name": "Xiang Zhou", "authorId": "144401309"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 138}, "snippets": ["Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"], "score": 0.0}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 255749213, "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure", "year": 2023, "venue": "ECML/PKDD", "authors": [{"name": "Xiaoling Zhou", "authorId": "48667407"}, {"name": "Ou Wu", "authorId": "2061463107"}, {"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ziyang Liang", "authorId": "2200527528"}], "n_citations": 5}, "snippets": ["The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error."], "score": 0.62255859375}, {"id": "(Zeng et al., 2023)", "paper": {"corpus_id": 259593617, "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zijie Zeng", "authorId": "8647899"}, {"name": "D. Ga\u0161evi\u0107", "authorId": "65953975"}, {"name": "Guangliang Chen", "authorId": "49901492"}], "n_citations": 6}, "snippets": ["Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch."], "score": 0.623046875}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 256460262, "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems", "year": 2023, "venue": "ACM Transactions on Software Engineering and Methodology", "authors": [{"name": "Qinghua Xu", "authorId": "2118221146"}, {"name": "Sajid Ali", "authorId": "47495965"}, {"name": "T. Yue", "authorId": "145456500"}], "n_citations": 22}, "snippets": ["To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]", "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."], "score": 0.658203125}, {"id": "(Zhu et al., 2021)", "paper": {"corpus_id": 244119160, "title": "Combining Curriculum Learning and Knowledge Distillation for Dialogue Generation", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Qingqing Zhu", "authorId": "2152206274"}, {"name": "Xiuying Chen", "authorId": "2116950235"}, {"name": "Pengfei Wu", "authorId": "2111192980"}, {"name": "Junfei Liu", "authorId": "2108414477"}, {"name": "Dongyan Zhao", "authorId": "144060462"}], "n_citations": 27}, "snippets": ["Curriculum learning, a machine training strategy that feeds training instances to the model from easy to hard, has been proven to facili-tate the dialogue generation task. Meanwhile, knowledge distillation, a knowledge transformation methodology among teachers and students networks can yield signi\ufb01cant performance boost for student models. Hence, in this paper, we introduce a combination of curriculum learning and knowledge distillation for ef\ufb01cient dialogue generation models, where curriculum learning can help knowledge distillation from data and model aspects. To start with, from the data aspect, we cluster the training cases according to their complexity, which is calculated by various types of features such as sentence length and coherence between dialog pairs. Furthermore, we employ an adversarial training strategy to identify the complexity of cases from model level. The intuition is that, if a discriminator can tell the generated response is from the teacher or the student, then the case is dif\ufb01cult that the student model has not adapted to yet. Finally, we use self-paced learning, which is an extension to curriculum learning to assign weights for distillation. In conclusion, we arrange a hierarchical curriculum based on the above two aspects for the student model under the guidance from the teacher model. Experimental results demon-strate that our methods achieve improvements compared with competitive baselines."], "score": 0.0}], "table": null}, {"title": "Uncertainty-based Difficulty Metrics", "tldr": "Uncertainty-based metrics assess training data difficulty by measuring model confidence and variability in predictions, capturing examples where the model exhibits high uncertainty or inconsistency across training epochs. (12 sources)", "text": "\nUncertainty-based difficulty metrics offer a different approach to measuring data complexity compared to loss-based metrics, focusing on the model's confidence and consistency in its predictions rather than just the magnitude of error. These metrics aim to identify samples where the model exhibits high uncertainty, suggesting they may be more challenging to learn.\n\nOne primary uncertainty metric is **prediction variability**, which measures the standard deviation of probabilities assigned to the gold label across multiple training epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. High variability values indicate greater uncertainty for a training example, suggesting that the model's confidence fluctuates significantly during training. While low variability scores alone don't provide clear information about difficulty, high scores reliably identify uncertain examples that may require special attention in curriculum learning.\n\nAnother widely used metric is **prediction confidence**, calculated as the average probability assigned to the gold label by a model across multiple epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. This is a continuous metric where higher values correspond to easier examples, providing a direct measure of how confidently the model can identify the correct label. Both confidence and variability can be used in combination, with confidence serving as a primary metric and variability as an auxiliary one.\n\nThe **entropy of model predictions** offers another perspective on uncertainty <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264451551\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>. Higher entropy in the prediction vector indicates the model is distributing probability mass across multiple possible outputs rather than confidently predicting a single class, suggesting the example is more difficult or ambiguous. Coleman et al. developed entropy-based metrics that have proven effective for data selection tasks <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nIn addition to model-based uncertainty, some approaches leverage **annotation entropy** as a proxy for difficulty <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. This metric calculates the entropy of human annotator labels, with higher entropy (more disagreement among annotators) indicating samples that are likely more difficult for models as well. Research by Nie et al. has shown a consistent positive correlation between model accuracy and the level of human agreement, supporting annotation entropy as a reliable difficulty metric <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.\n\nFor more sophisticated uncertainty estimation, some methods employ **Bayesian Neural Networks** to infer epistemic uncertainty (also known as model uncertainty) <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>. Alternatively, the **variance of gradients** across different epochs can rank data from difficult to easy, with the assumption that samples with high variance of gradients are more challenging for the model to learn <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\nSome researchers have integrated uncertainty-based metrics into self-paced learning frameworks. Li et al. proposed using uncertainty as the difficulty measure for self-paced learning instead of relying solely on loss values, arguing that difficulty is an intrinsic attribute of samples and should be independent of ground truth labels <Paper corpusId=\"266741778\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. Similarly, Zhou et al. developed an uncertainty-aware curriculum learning approach for neural machine translation that uses model uncertainty to govern the learning schedule <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>.\n\nCross-entropy can also function as an uncertainty measure. Xu et al. defined a Cross Entropy-based Measurer (CEM) that estimates uncertainty by comparing real distribution and prediction <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. This approach can be particularly useful for measuring sample transferability and handling domain relevance in adaptation tasks <Paper corpusId=\"257937538\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"69357784\" paperTitle=\"(Shu et al., 2019)\" isShortName></Paper>.\n\nRecent research has also explored composite metrics that combine different aspects of uncertainty. Swayamdipta et al. introduced a framework that uses both the model's confidence in the true class and the variability of this confidence across epochs to characterize datasets, revealing distinct regions in the data that contribute differently to model learning <Paper corpusId=\"273098749\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Christopoulou et al., 2022)", "paper": {"corpus_id": 247694098, "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fenia Christopoulou", "authorId": "48810605"}, {"name": "Gerasimos Lampouras", "authorId": "2346538"}, {"name": "Ignacio Iacobacci", "authorId": "2676143"}], "n_citations": 4}, "snippets": ["In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own."], "score": 0.72509765625}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 273098749, "title": "Structural-Entropy-Based Sample Selection for Efficient and Effective Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Tianchi Xie", "authorId": "2324006090"}, {"name": "Jiangning Zhu", "authorId": "2316675548"}, {"name": "Guozu Ma", "authorId": "2324704341"}, {"name": "Minzhi Lin", "authorId": "2324761714"}, {"name": "Wei Chen", "authorId": "2324226156"}, {"name": "Weikai Yang", "authorId": "1604645934"}, {"name": "Shixia Liu", "authorId": "2257322120"}], "n_citations": 2}, "snippets": ["Metrics that measure model confidence include the entropy of the prediction vector (Coleman et al., 2019) and the variance of the predicted probabilities across training epochs (Swayamdipta et al., 2020). Metrics that measure model error include EL2N (Paul et al., 2021), which calculates the L 2 norm of the error vector, and the Forgetting score (Toneva et al., 2018), which tracks the frequency of misclassifications after initial correct classifications. AUM (Pleiss et al., 2020) combines both perspectives by measuring the confidence for correct classifications and the error for misclassifications."], "score": 0.75048828125}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 264451551, "title": "Data Optimization in Deep Learning: A Survey", "year": 2023, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Ou Wu", "authorId": "2261492235"}, {"name": "Rujing Yao", "authorId": "7304193"}], "n_citations": 1}, "snippets": ["As shown in Fig. 7, there are five major manners to measure learning difficulty of samples, namely, loss-based, gradient-based, uncertainty-based, multi-training-based, and distance-based", "Model uncertainty can be calculated based on information entropy of the DNN prediction or the variance of multiple predictions output by a DNN with the dropout trick (Kendall et al., 2017). \u2022 Diversity. This type aims to identify the diversity of a subset of training samples. The subset is usually a category. The measurement for subset diversity is useful in the design of data augmentation strategy for the subset (Kumar et al., 2019) and data selection (Su et al., 2022). Friedman and Dieng [80] leveraged the exponential of the Shannon entropy of the eigenvalues of a similarity matrix, namely, vendi score to measure diversity."], "score": 0.85693359375}, {"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."], "score": 0.7119140625}, {"id": "(Nie et al., 2020)", "paper": {"corpus_id": 222177851, "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yixin Nie", "authorId": "40383658"}, {"name": "Xiang Zhou", "authorId": "144401309"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 138}, "snippets": ["Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"], "score": 0.0}, {"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 248810776, "title": "Exploring the Learning Difficulty of Data: Theory and Measure", "year": 2022, "venue": "ACM Transactions on Knowledge Discovery from Data", "authors": [{"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ou Wu", "authorId": "2061463125"}, {"name": "Fengguang Su", "authorId": "2165378509"}, {"name": "Yingjun Deng", "authorId": "2165450358"}], "n_citations": 6}, "snippets": ["The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."], "score": 0.89306640625}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 266741778, "title": "Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Haopeng Li", "authorId": "2266086772"}, {"name": "Qiuhong Ke", "authorId": "2066160726"}, {"name": "Mingming Gong", "authorId": "2146826373"}, {"name": "Tom Drummond", "authorId": "2277600596"}], "n_citations": 1}, "snippets": ["In the case of self-paced CL (SPL, where difficulty is measured during training), the loss function is often used as the difficulty measure (Kumar et al., 2010)- (Gong et al., 2019). Initially, during training, samples with higher losses are excluded from optimization. As training advances, the threshold is gradually increased to incorporate more complex data into the optimization process. However, relying solely on loss might not accurately represent the inherent difficulty of data, as difficulty is an intrinsic attribute of samples and should be independent of ground truth labels. To overcome this limitation, we propose employing uncertainty as the difficulty measure for SPL."], "score": 0.7392578125}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 257937538, "title": "A Review of the Evaluation System for Curriculum Learning", "year": 2023, "venue": "Electronics", "authors": [{"name": "Fengchun Liu", "authorId": "144102068"}, {"name": "Tong Zhang", "authorId": "2213622721"}, {"name": "Chunying Zhang", "authorId": "1806923"}, {"name": "Lu Liu", "authorId": "145412331"}, {"name": "Liya Wang", "authorId": "2108947838"}, {"name": "Bin Liu", "authorId": "2213836973"}], "n_citations": 6}, "snippets": ["In the process of model training, samples with larger losses are harder to learn for the current stage of the model. Conversely, samples with smaller losses prove that the model has been able to correctly predict or classify that sample, which should reduce the sampling probability of that sample. Using sample loss (Gan et al., 2021)[29] as a difficulty evaluation metric, such as Negative Log Likelihood loss [79], square loss (Tang et al., 2019), and cross-entropy loss (Shu et al., 2019)110,(Guo et al., 2018) as difficulty scores, is widely used in self-paced learning (Kumar et al., 2010) and its variant methods (Li et al., 2016)(Fan et al., 2016)", "Cross entropy is used as a measure of transferability (Shu et al., 2019), domain relevance (Zhan et al., 2021), uncertainty (Zhou et al., 2020), and representativeness (Tang et al., 2019)[114], as in Shu et al. (Shu et al., 2019), where cross entropy loss is used as a measure of sample transferability for solving sample noises of the source domain and distribution shift across domains", "Rather than focusing on the instantaneous loss values of a sample, some studies have focused on its loss value during training, calculating the change in model loss over two consecutive training iterations (Zhou et al., 2020) as a difficulty metric, proving that a sample is very difficult when its loss fluctuates between maximum and minimum values during the sequence. Zhou et al. (Zhou et al., 2021) proposed the exponential moving average (EMA) method for the detection of clean and pseudo-labeled samples."], "score": 0.79248046875}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 256460262, "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems", "year": 2023, "venue": "ACM Transactions on Software Engineering and Methodology", "authors": [{"name": "Qinghua Xu", "authorId": "2118221146"}, {"name": "Sajid Ali", "authorId": "47495965"}, {"name": "T. Yue", "authorId": "145456500"}], "n_citations": 22}, "snippets": ["To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]", "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."], "score": 0.658203125}, {"id": "(Shu et al., 2019)", "paper": {"corpus_id": 69357784, "title": "Transferable Curriculum for Weakly-Supervised Domain Adaptation", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yang Shu", "authorId": "2066476988"}, {"name": "Zhangjie Cao", "authorId": "3451430"}, {"name": "Mingsheng Long", "authorId": "35776445"}, {"name": "Jianmin Wang", "authorId": "2144499343"}], "n_citations": 103}, "snippets": ["Domain adaptation improves a target task by knowledge transfer from a source domain with rich annotations. It is not uncommon that \"source-domain engineering\" becomes a cumbersome process in domain adaptation: the high-quality source domains highly related to the target domain are hardly available. Thus, weakly-supervised domain adaptation has been introduced to address this difficulty, where we can tolerate the source domain with noises in labels, features, or both. As such, for a particular target task, we simply collect the source domain with coarse labeling or corrupted data. In this paper, we try to address two entangled challenges of weaklysupervised domain adaptation: sample noises of the source domain and distribution shift across domains. To disentangle these challenges, a Transferable Curriculum Learning (TCL) approach is proposed to train the deep networks, guided by a transferable curriculum informing which of the source examples are noiseless and transferable. The approach enhances positive transfer from clean source examples to the target and mitigates negative transfer of noisy source examples. A thorough evaluation shows that our approach significantly outperforms the state-of-the-art on weakly-supervised domain adaptation tasks."], "score": 0.0}, {"id": "(Swayamdipta et al., 2020)", "paper": {"corpus_id": 221856637, "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Swabha Swayamdipta", "authorId": "2705113"}, {"name": "Roy Schwartz", "authorId": "2279023325"}, {"name": "Nicholas Lourie", "authorId": "35219984"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 452}, "snippets": ["Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."], "score": 0.0}], "table": null}, {"title": "Training Dynamics Metrics", "tldr": "Training dynamics metrics measure model behavior on specific examples throughout the training process, capturing how consistently and confidently models learn from data. These metrics\u2014including confidence, variability, loss change rate, and gradient-based measures\u2014provide insights into learning difficulty that static or instantaneous metrics cannot. (14 sources)", "text": "\nTraining dynamics metrics offer a sophisticated approach to measuring data difficulty by tracking how a model's behavior on specific examples evolves during training. Unlike static heuristics or instantaneous loss values, these metrics provide insights into learning patterns over time, making them particularly valuable for constructing effective curricula for LLM training.\n\nA foundational concept in training dynamics is **confidence**, defined as the average probability assigned to the gold label by a model across multiple training epochs <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. Higher confidence values generally indicate easier examples for the model. Complementing this is **variability**, which measures the standard deviation of probabilities assigned to the gold label across epochs, with higher values revealing examples that create greater uncertainty <Paper corpusId=\"247694098\" paperTitle=\"(Christopoulou et al., 2022)\" isShortName></Paper>. These metrics are particularly valuable in curriculum learning, with confidence serving as a primary metric for difficulty assessment while variability helps identify uncertain examples.\n\nThe **change rate of loss** during training provides another valuable perspective on difficulty. Zhou et al. proposed tracking loss fluctuations between training iterations, suggesting that samples whose loss values fluctuate dramatically between maximum and minimum values are particularly difficult <Paper corpusId=\"220047761\" paperTitle=\"(Zhou et al., 2020)\" isShortName></Paper>. Building on this approach, the exponential moving average (EMA) of loss values over time can offer a more stable assessment of difficulty by smoothing out temporary fluctuations <Paper corpusId=\"244958599\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\n**Learning percentage** represents a novel approach for self-ranking training data. This metric assumes that easier samples show more learning progress in earlier epochs, with the model using this information to prioritize examples accordingly <Paper corpusId=\"267740312\" paperTitle=\"(Mekala et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249060677\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>. This concept aligns with the observation that neural networks tend to learn clean examples before noisy ones, making learning order a valuable signal for difficulty assessment.\n\nGradient-based metrics have emerged as another powerful tool for measuring data difficulty. These include:\n\n1. **Gradient magnitude**: Samples with large gradient magnitudes are considered more difficult for the model to learn <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\n2. **Variance of gradients (VOG)**: This metric ranks data from difficult to easy by measuring the variance of gradients across different training epochs, with high variance indicating examples that are more challenging for the model <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper>.\n\n3. **Gradient-based Difficulty Measure (GDM)**: This approach evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself, offering more fine-grained insights than training loss alone, particularly when different examples yield identical losses <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nThe bias-variance trade-off from statistical learning theory has informed newer difficulty metrics. Zhu et al. argue that both bias (related to average loss) and variance terms should be considered when measuring sample difficulty <Paper corpusId=\"248810776\" paperTitle=\"(Zhu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"255749213\" paperTitle=\"(Zhou et al., 2023)\" isShortName></Paper>. Their theoretical analysis suggests that generalization error, which incorporates both terms, can serve as a more comprehensive difficulty measure than approaches focusing solely on bias or variance.\n\nSeveral metrics combine model confidence and error measurements. Coleman et al. developed entropy-based metrics for data selection <Paper corpusId=\"195750622\" paperTitle=\"(Coleman et al., 2019)\" isShortName></Paper>, while Swayamdipta et al. introduced a framework using both confidence in true class predictions and variability of this confidence across epochs to characterize datasets <Paper corpusId=\"221856637\" paperTitle=\"(Swayamdipta et al., 2020)\" isShortName></Paper>. Other notable methods include EL2N (Error L2-Norm), which calculates the L2 norm of the error vector <Paper corpusId=\"235898952\" paperTitle=\"(Paul et al., 2021)\" isShortName></Paper>, and the Forgetting score, which tracks the frequency of misclassifications after initial correct classifications <Paper corpusId=\"55481903\" paperTitle=\"(Toneva et al., 2018)\" isShortName></Paper>.\n\nFinally, **annotation entropy**\u2014measuring human annotator disagreement\u2014serves as an external difficulty metric that often correlates well with model difficulty <Paper corpusId=\"259370648\" paperTitle=\"(Elgaar et al._1, 2023)\" isShortName></Paper>. Research by Nie et al. has established a consistent positive correlation between model accuracy and the level of human agreement, supporting annotation entropy as a reliable difficulty metric <Paper corpusId=\"222177851\" paperTitle=\"(Nie et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Christopoulou et al., 2022)", "paper": {"corpus_id": 247694098, "title": "Training Dynamics for Curriculum Learning: A Study on Monolingual and Cross-lingual NLU", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Fenia Christopoulou", "authorId": "48810605"}, {"name": "Gerasimos Lampouras", "authorId": "2346538"}, {"name": "Ignacio Iacobacci", "authorId": "2676143"}], "n_citations": 4}, "snippets": ["In this work, instead, we employ CL for NLU by taking advantage of training dynamics as difficulty metrics, i.e., statistics that measure the behavior of the model at hand on specific task-data instances during training and propose modifications of existing CL schedulers based on these statistics.\n\nWe list three difficulty metrics that use statistics during the course of a model's training, as follows: CONFIDENCE (CONF) of an example x i is the average probability assigned to the gold label y i by a model with parameters \u03b8 across a number of epochs E. This is a continuous metric with higher values corresponding to easier examples. \n\nVARIABILITY (VAR) of an example x i is the standard deviation of the probabilities assigned to the gold label y i across E epochs. It is a continuous metric with higher values indicating greater uncertainty for a training example. \n\nConfidence and correctness are the primary metrics that we use in our curricula since low and high values correspond to hard and easy examples respectively. On the other hand, variability is used as an auxiliary metric since only high scores clearly represent uncertain examples while low scores offer no important information on their own."], "score": 0.72509765625}, {"id": "(Zhou et al., 2020)", "paper": {"corpus_id": 220047761, "title": "Uncertainty-Aware Curriculum Learning for Neural Machine Translation", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yikai Zhou", "authorId": "2110349143"}, {"name": "Baosong Yang", "authorId": "21299583"}, {"name": "Derek F. Wong", "authorId": "1758353"}, {"name": "Yu Wan", "authorId": "153379180"}, {"name": "Lidia S. Chao", "authorId": "1774304"}], "n_citations": 96}, "snippets": ["Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule."], "score": 0.0}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 244958599, "title": "AutoBalance: Optimized Loss Functions for Imbalanced Data", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mingchen Li", "authorId": "47629178"}, {"name": "Xuechen Zhang", "authorId": "2108029216"}, {"name": "Christos Thrampoulidis", "authorId": "2751682"}, {"name": "Jiasi Chen", "authorId": "2281075331"}, {"name": "Samet Oymak", "authorId": "3103394"}], "n_citations": 68}, "snippets": ["Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly fit the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Specifically, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the benefits and performance of our approach for the application scenarios of imbalanced and group-sensitive classification. Extensive empirical evaluations demonstrate the benefits of AutoBalance over state-of-the-art approaches. Our experimental findings are complemented with theoretical insights on loss function design and the benefits of train-validation split. All code is available open-source."], "score": 0.0}, {"id": "(Mekala et al., 2024)", "paper": {"corpus_id": 267740312, "title": "Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Alex Nguyen", "authorId": "2284673632"}, {"name": "Jingbo Shang", "authorId": "2284595153"}], "n_citations": 21}, "snippets": ["Drawing inspiration from the learning order metric in (Mekala et al., 2022), we propose a novel data selection method that utilizes the learning percentage as a difficulty metric that the model can use to self-rank its training data. Essentially, the more learning that occurs in earlier epochs, the easier the sample is considered."], "score": 0.861328125}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 249060677, "title": "LOPS: Learning Order Inspired Pseudo-Label Selection for Weakly Supervised Text Classification", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Chengyu Dong", "authorId": "2113540861"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 20}, "snippets": ["Weakly supervised text classification methods typically train a deep neural classifier based on pseudo-labels. The quality of pseudo-labels is crucial to final performance but they are inevitably noisy due to their heuristic nature, so selecting the correct ones has a huge potential for performance boost. One straightforward solution is to select samples based on the softmax probability scores in the neural classifier corresponding to their pseudo-labels. However, we show through our experiments that such solutions are ineffective and unstable due to the erroneously high-confidence predictions from poorly calibrated models. Recent studies on the memorization effects of deep neural models suggest that these models first memorize training samples with clean labels and then those with noisy labels. Inspired by this observation, we propose a novel pseudo-label selection method LOPS that takes learning order of samples into consideration. We hypothesize that the learning order reflects the probability of wrong annotation in terms of ranking, and therefore, propose to select the samples that are learnt earlier. LOPS can be viewed as a strong performance-boost plug-in to most of existing weakly-supervised text classification methods, as confirmed in extensive experiments on four real-world datasets."], "score": 0.0}, {"id": "(Zhu et al., 2022)", "paper": {"corpus_id": 248810776, "title": "Exploring the Learning Difficulty of Data: Theory and Measure", "year": 2022, "venue": "ACM Transactions on Knowledge Discovery from Data", "authors": [{"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ou Wu", "authorId": "2061463125"}, {"name": "Fengguang Su", "authorId": "2165378509"}, {"name": "Yingjun Deng", "authorId": "2165450358"}], "n_citations": 6}, "snippets": ["The partition of training data into different subsets according to their learning difficulties and adoption of separate learning schemes (e.g., weighting) are proven to be useful in many learning tasks [21,43,46,68]. The learning difficulty investigated in this study refers to the degrees of easy or hard to learn of training samples in a given learning task.\n\nNumerous methods are proposed to measure the learning difficulty of a training sample. The most common practice is to leverage the training output (e.g., loss and the predicted value on the true category) of a sample to construct the measurements. In Self-paced Learning (SPL) [26,68], the training loss is used to determine whether a sample is easy or not, and easy samples are first learned. We assume that p i,yi is the prediction on the ground-truth category for a training sample x i . In object detection, the value of (1 \u2212 p i,yi ) is used to indicate the learning difficulty for x i [43] . Given that the training output in an epoch may be unreliable, some methods utilize the average training output of a sample during the training to measure the difficulty. Huang et al. [33] designed a cyclic training procedure, and the model is trained from underfitting to over-fitting in one cycle. The average training loss in the whole cyclic procedure is used as the noisy indicator for a training sample. Feng et al. [20] utilized the magnitude of the loss gradient to measure the learning difficulty of a training sample. A large gradient magnitude indicates a high degree of difficulty.\n\nLearning difficulty is considered as an intrinsic property of data in machine learning [46,75]. Existing measurements are usually based on heuristic cues or inspirations, and they can be divided into the following main categories: \n\n\u2022 Loss-based measurement. This category directly uses the loss as the measure. Most measures fall into this category because it is simple yet effective in various learning tasks. Some methods [68] directly utilize the loss in one epoch as the degree of difficulty. Accordingly, the degrees for the same samples vary in different epochs. Some others utilize the average loss [45] during the partial or whole training procedure for measurement. \u2022 Cross-validation-based measurement. This category adopts a crossvalidation strategy [65]. For example, five-fold cross-validation is performed, and the whole cross-validation is repeated ten times. Consequently, each ELDD training sample receives ten predictions. The value of error predictions is used as the indicator of difficulty. \u2022 Uncertainty-based measurement. This category uses the (model) uncertainty of a sample to measure the difficulty. Aguilar et al. [3] identified hard samples based on the epistemic uncertainty (also known as the model uncertainty). They leveraged the Bayesian Neural Network [67] to infer the epistemic uncertainty. \u2022 Margin-based measurement. This category uses the margin (distance) of a sample to the underlying decision surface as the measurement. The rationale is that a small margin denotes a large difficulty [42,68]. \u2022 Gradient-based measurement. This category uses the loss gradient of a sample to measure the difficulty. Agarwal and Hooker [2] proposed the variance of gradients (VOG) across different epochs to rank data from difficult to easy. They considered that samples with high VOG values are far more difficult for the model to learn. Santiago et al. [54] applied the norm of the gradients to measure the difficulty, and high norms indicate large difficulty for learning."], "score": 0.89306640625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268696658, "title": "Symmetric Self-Paced Learning for Domain Generalization", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Di Zhao", "authorId": "2110600983"}, {"name": "Yun Sing Koh", "authorId": "34930533"}, {"name": "Gillian Dobbie", "authorId": "2276344048"}, {"name": "Hongsheng Hu", "authorId": "2293565669"}, {"name": "Philippe Fournier-Viger", "authorId": "2293453267"}], "n_citations": 5}, "snippets": ["Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."], "score": 0.7685546875}, {"id": "(Zhou et al., 2023)", "paper": {"corpus_id": 255749213, "title": "Understanding Difficulty-based Sample Weighting with a Universal Difficulty Measure", "year": 2023, "venue": "ECML/PKDD", "authors": [{"name": "Xiaoling Zhou", "authorId": "48667407"}, {"name": "Ou Wu", "authorId": "2061463107"}, {"name": "Weiyao Zhu", "authorId": "2152350103"}, {"name": "Ziyang Liang", "authorId": "2200527528"}], "n_citations": 5}, "snippets": ["The commonly used difficulty measures, such as loss (Kumar et al., 2010) and gradient norm (Santiago et al., 2021), are mainly related to the bias term. Shin et al. [27] emphasized that only using loss as the measurement cannot distinguish clean and noisy samples, especially for uniform label noise. There are also a few existing studies that use variance (Chang et al., 2017)[29]. For instance, Agarwal et al. [30] applied the variance of gradient norms as the difficulty measure. Indeed, both the variance and bias terms should not be underestimated when measuring the samples' learning difficulty. Our theoretical analyses support that generalization error including both the two terms can capture four main factors influencing the samples' learning difficulty. Thus, the error can be leveraged as a universal measure that is more reasonable than existing measures. Existing studies generally apply the K-fold cross-validation method (Yang et al., 2020) to calculate the generalization error."], "score": 0.62255859375}, {"id": "(Coleman et al., 2019)", "paper": {"corpus_id": 195750622, "title": "Selection Via Proxy: Efficient Data Selection For Deep Learning", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Cody A. Coleman", "authorId": "2866919"}, {"name": "Christopher Yeh", "authorId": "144088780"}, {"name": "Stephen Mussmann", "authorId": "1776721"}, {"name": "Baharan Mirzasoleiman", "authorId": "2389094"}, {"name": "Peter Bailis", "authorId": "2740804"}, {"name": "Percy Liang", "authorId": "145419642"}, {"name": "J. Leskovec", "authorId": "1702139"}, {"name": "M. Zaharia", "authorId": "143834867"}], "n_citations": 349}, "snippets": ["Data selection methods such as active learning and core-set selection are useful tools for machine learning on large datasets, but they can be prohibitively expensive to apply in deep learning. Unlike in other areas of machine learning, the feature representations that these techniques depend on are learned in deep learning rather than given, which takes a substantial amount of training time. In this work, we show that we can significantly improve the computational efficiency of data selection in deep learning by using a much smaller proxy model to perform data selection for tasks that will eventually require a large target model (e.g., selecting data points to label for active learning). In deep learning, we can scale down models by removing hidden layers or reducing their dimension to create proxies that are an order of magnitude faster. Although these small proxy models have significantly higher error, we find that they empirically provide useful rankings for data selection that have a high correlation with those of larger models. We evaluate this \"selection via proxy\" (SVP) approach on several data selection tasks. For active learning, applying SVP to Sener and Savarese [2018]'s recent method for active learning in deep learning gives a 4x improvement in execution time while yielding the same model accuracy. For core-set selection, we show that a proxy model that trains 10x faster than a target ResNet164 model on CIFAR10 can be used to remove 50% of the training data without compromising the accuracy of the target model, making end-to-end training time improvements via core-set selection possible."], "score": 0.0}, {"id": "(Swayamdipta et al., 2020)", "paper": {"corpus_id": 221856637, "title": "Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Swabha Swayamdipta", "authorId": "2705113"}, {"name": "Roy Schwartz", "authorId": "2279023325"}, {"name": "Nicholas Lourie", "authorId": "35219984"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 452}, "snippets": ["Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of \"ambiguous\" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are \"easy to learn\" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds \"hard to learn\"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization."], "score": 0.0}, {"id": "(Paul et al., 2021)", "paper": {"corpus_id": 235898952, "title": "Deep Learning on a Data Diet: Finding Important Examples Early in Training", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mansheej Paul", "authorId": "1690452"}, {"name": "S. Ganguli", "authorId": "25769960"}, {"name": "G. Dziugaite", "authorId": "2533850"}], "n_citations": 461}, "snippets": ["Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generalize to other configurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples -- we investigate how the data distribution shapes the loss surface and identify subspaces of the model's data representation that are relatively stable over training."], "score": 0.0}, {"id": "(Toneva et al., 2018)", "paper": {"corpus_id": 55481903, "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mariya Toneva", "authorId": "2822168"}, {"name": "Alessandro Sordoni", "authorId": "2041695"}, {"name": "R\u00e9mi Tachet des Combes", "authorId": "15032777"}, {"name": "Adam Trischler", "authorId": "3382568"}, {"name": "Yoshua Bengio", "authorId": "1751762"}, {"name": "Geoffrey J. Gordon", "authorId": "21889436"}], "n_citations": 741}, "snippets": ["Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a \"forgetting event\" to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set\u2019s (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance."], "score": 0.0}, {"id": "(Elgaar et al._1, 2023)", "paper": {"corpus_id": 259370648, "title": "HuCurl: Human-induced Curriculum Discovery", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mohamed Elgaar", "authorId": "1659451954"}, {"name": "Hadi Amiri", "authorId": "143656058"}], "n_citations": 0}, "snippets": ["Annotation entropy is a natural measure of difficulty (for humans) and may serve as a reliable difficulty metric for models. Entropy of each sample x i is calculated as \u2212 l p c log p c (Shannon, 1948), where c is a class category and p c is the fraction of annotators who chose label c for the sample. The use of entropy is supported in (Nie et al., 2020), reporting a consistent positive correlation between model accuracy and level of human agreement.\n\nFurthermore, moving average of a sample's instantaneous loss is a good metric for difficulty (Zhou et al., 2020). Using a baseline model trained with no curriculum and with default hyperparameters, we collect the loss values of all training instances at intervals of 0.5 epochs and use the average loss as prior knowledge about sample difficulty. We obtain twenty observations of the loss and compute the average for each sample."], "score": 0.7119140625}, {"id": "(Nie et al., 2020)", "paper": {"corpus_id": 222177851, "title": "What Can We Learn from Collective Human Opinions on Natural Language Inference Data?", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yixin Nie", "authorId": "40383658"}, {"name": "Xiang Zhou", "authorId": "144401309"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 138}, "snippets": ["Despite the subjective nature of many NLP tasks, most NLU evaluations have focused on using the majority label with presumably high agreement as the ground truth. Less attention has been paid to the distribution of human opinions. We collect ChaosNLI, a dataset with a total of 464,500 annotations to study Collective HumAn OpinionS in oft-used NLI evaluation sets. This dataset is created by collecting 100 annotations per example for 3,113 examples in SNLI and MNLI and 1,532 examples in Abductive-NLI. Analysis reveals that: (1) high human disagreement exists in a noticeable amount of examples in these datasets; (2) the state-of-the-art models lack the ability to recover the distribution over human labels; (3) models achieve near-perfect accuracy on the subset of data with a high level of human agreement, whereas they can barely beat a random guess on the data with low levels of human agreement, which compose most of the common errors made by state-of-the-art models on the evaluation sets. This questions the validity of improving model performance on old metrics for the low-agreement part of evaluation datasets. Hence, we argue for a detailed examination of human agreement in future data collection efforts, and evaluating model outputs against the distribution over collective human opinions. The ChaosNLI dataset and experimental scripts are available at this https URL"], "score": 0.0}], "table": null}, {"title": "Predefined Heuristic Metrics", "tldr": "Predefined heuristic metrics use intuitive data characteristics to estimate training example difficulty without model-based computation. Common metrics include text length, word rarity, sentence complexity, and annotation agreement, which can be applied across various domains. (17 sources)", "text": "\nPredefined heuristic metrics offer straightforward, computationally efficient approaches to measure data difficulty for curriculum learning in LLM training. Unlike model-based metrics that require training or inference passes, these metrics rely on inherent properties of the data that can be calculated prior to training.\n\nIn natural language processing, the most widely used heuristic metrics include:\n\n1. **Text Length**: Sentence or sequence length is perhaps the most common difficulty metric, based on the assumption that longer texts are more challenging for models to process <Paper corpusId=\"26468344\" paperTitle=\"(Kocmi et al., 2017)\" isShortName></Paper> <Paper corpusId=\"166228313\" paperTitle=\"(Tay et al., 2019)\" isShortName></Paper> <Paper corpusId=\"233433844\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. This metric has been extensively used in machine translation and reading comprehension tasks <Paper corpusId=\"265552007\" paperTitle=\"(Ding et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n2. **Word Rarity/Frequency**: Examples containing less frequent words from the training vocabulary are considered more difficult <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"248887633\" paperTitle=\"(Croitoru et al., 2022)\" isShortName></Paper>. Wang et al. (2022) used word frequency as a complexity metric for curriculum-based pre-training of language models <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>.\n\n3. **Linguistic Complexity**: More sophisticated linguistic features such as syntactic complexity, grammatical structures, and text coherence can serve as difficulty indicators <Paper corpusId=\"265351926\" paperTitle=\"(Vakil et al., 2023)\" isShortName></Paper>.\n\n4. **Object Size and Count**: In computer vision, images containing fewer and larger objects are often considered easier to process <Paper corpusId=\"6954583\" paperTitle=\"(Shi et al., 2016)\" isShortName></Paper> <Paper corpusId=\"271051051\" paperTitle=\"(Jarca et al., 2024)\" isShortName></Paper>.\n\n5. **Annotation Agreement**: For datasets with multiple human annotations, the degree of annotator agreement can serve as a proxy for example difficulty <Paper corpusId=\"221995570\" paperTitle=\"(Wei et al., 2020)\" isShortName></Paper>. Higher disagreement among annotators often correlates with examples that models also find difficult <Paper corpusId=\"204539326\" paperTitle=\"(Jimenez-Sanchez et al., 2019)\" isShortName></Paper>.\n\n6. **Perplexity**: Pre-computed perplexity scores from auxiliary language models can estimate text complexity without requiring full model training <Paper corpusId=\"265351926\" paperTitle=\"(Vakil et al., 2023)\" isShortName></Paper>.\n\n7. **Domain-Specific Metrics**: Some approaches develop task-specific heuristics, such as using similarity of parallel sentence pairs in machine translation <Paper corpusId=\"267770178\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\n8. **Comprehensive Complexity Measures**: More recent work has proposed combined metrics that incorporate multiple factors. For example, Ranaldi et al. developed a complexity measure based on length, rarity, and comprehensibility of samples, which outperformed existing curriculum learning methods when used for pre-training BERT and RoBERTa from scratch <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>.\n\n9. **C-Score**: This metric measures how consistently a particular example is learned across various models, indicating its inherent difficulty without relying on loss values that might lead to overfitting <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nWhile predefined heuristic metrics are computationally efficient and intuitive, they may not always align with the actual learning difficulty experienced by models <Paper corpusId=\"248887633\" paperTitle=\"(Croitoru et al., 2022)\" isShortName></Paper>. This limitation has led researchers to explore more sophisticated model-centric approaches that can better capture the true learning challenge of each example <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of these heuristic metrics varies across tasks and domains, with some researchers finding that combined approaches or task-specific metrics yield better results than general-purpose metrics <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>. The selection of appropriate difficulty metrics remains an active area of research in curriculum learning for LLM training.", "citations": [{"id": "(Kocmi et al., 2017)", "paper": {"corpus_id": 26468344, "title": "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation", "year": 2017, "venue": "Recent Advances in Natural Language Processing", "authors": [{"name": "Tom Kocmi", "authorId": "3452584"}, {"name": "Ondrej Bojar", "authorId": "143832874"}], "n_citations": 141}, "snippets": ["We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called \"curriculum learning\"). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our \"curricula\" achieve a small improvement over the baseline."], "score": 0.0}, {"id": "(Tay et al., 2019)", "paper": {"corpus_id": 166228313, "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading Comprehension over Long Narratives", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Shuohang Wang", "authorId": "2992833"}, {"name": "Anh Tuan Luu", "authorId": "26336902"}, {"name": "Jie Fu", "authorId": "49252800"}, {"name": "Minh C. Phan", "authorId": "22209930"}, {"name": "Xingdi Yuan", "authorId": "2854297"}, {"name": "J. Rao", "authorId": "30586030"}, {"name": "S. Hui", "authorId": "144194328"}, {"name": "Aston Zhang", "authorId": "2085709"}], "n_citations": 110}, "snippets": ["This paper tackles the problem of reading comprehension over long narratives where documents easily span over thousands of tokens. We propose a curriculum learning (CL) based Pointer-Generator framework for reading/sampling over large documents, enabling diverse training of the neural model based on the notion of alternating contextual difficulty. This can be interpreted as a form of domain randomization and/or generative pretraining during training. To this end, the usage of the Pointer-Generator softens the requirement of having the answer within the context, enabling us to construct diverse training samples for learning. Additionally, we propose a new Introspective Alignment Layer (IAL), which reasons over decomposed alignments using block-based self-attention. We evaluate our proposed method on the NarrativeQA reading comprehension benchmark, achieving state-of-the-art performance, improving existing baselines by 51% relative improvement on BLEU-4 and 17% relative improvement on Rouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and CL components."], "score": 0.0}, {"id": "(Zhang et al., 2021)", "paper": {"corpus_id": 233433844, "title": "Reducing BERT Computation by Padding Removal and Curriculum Learning", "year": 2021, "venue": "IEEE International Symposium on Performance Analysis of Systems and Software", "authors": [{"name": "Wei Zhang", "authorId": "144142217"}, {"name": "Wei Wei", "authorId": "2149192554"}, {"name": "Wen Wang", "authorId": "2108908267"}, {"name": "Lingling Jin", "authorId": "2152164167"}, {"name": "Zheng Cao", "authorId": "144332880"}], "n_citations": 19}, "snippets": ["BERT [1] is very computationally expensive, which is a hurdle for its training and deployment. This work focuses on removing the unnecessary computation due to input padding in BERT. The input of BERT consists of two concatenated sentences. If the length of the two concatenated sentences is shorter than the maximum sequence length, padding must be added to the end of the sentences to fill the empty slots in the input. Because the lengths of sentences vary greatly, there can be a large amount of padding in input. For the English Wikipedia & BooksCorpus dataset, the percentage of padding among all the input tokens is 17% and 48%, respectively, when the max sequence length is set to 128 and 512. For the Chinese Wikipedia dataset, this percentage is 35% and 79%, respectively, when the max sequence length is 128 and 512. For SQuAD-v1.1 [2], padding accounts for 54% of the total input tokens when the max sequence length is 384. Thus, there is a lot of wasted computation on padding, which significantly increases the training and inference time."], "score": 0.0}, {"id": "(Ding et al., 2023)", "paper": {"corpus_id": 265552007, "title": "The Efficiency Spectrum of Large Language Models: An Algorithmic Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Tianyu Ding", "authorId": "2257191230"}, {"name": "Tianyi Chen", "authorId": "2257252148"}, {"name": "Haidong Zhu", "authorId": "2268495701"}, {"name": "Jiachen Jiang", "authorId": "2257278916"}, {"name": "Yiqi Zhong", "authorId": "2269696935"}, {"name": "Jinxin Zhou", "authorId": "2257235854"}, {"name": "Guangzhi Wang", "authorId": "2269161996"}, {"name": "Zhihui Zhu", "authorId": "2269697742"}, {"name": "Ilya Zharkov", "authorId": "15623770"}, {"name": "Luming Liang", "authorId": "46225943"}], "n_citations": 23}, "snippets": ["Difficulty Metric. In the realm of curriculum learning for natural language processing, the most widely used difficulty metric is perhaps the sequence length (Geiping et al., 2022)157,(Li et al., 2021). The underlying assumption is that processing longer sentences poses greater challenges than shorter ones. Another prevalent metric is vocabulary rarity [285](Zhao et al., 2020), based on the intuition that sentences with less frequently used words in the training set are inherently more complex to comprehend. In addition, this metric could be measured by the uncertainty sampling principle in active learning, where the uncertainty indicated by other pre-trained models could serves as a gauge of difficulty as well."], "score": 0.80517578125}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 269741199, "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Joonho Lee", "authorId": "2116659852"}, {"name": "Jae Oh Woo", "authorId": "2301016207"}, {"name": "Juree Seok", "authorId": "2301014929"}, {"name": "Parisa Hassanzadeh", "authorId": "2301015562"}, {"name": "Wooseok Jang", "authorId": "2301015224"}, {"name": "JuYoun Son", "authorId": "2301016428"}, {"name": "Sima Didari", "authorId": "91748824"}, {"name": "Baruch Gutow", "authorId": "2301014892"}, {"name": "Heng Hao", "authorId": "2065513368"}, {"name": "Hankyu Moon", "authorId": "2301015932"}, {"name": "Wenjun Hu", "authorId": "2301166595"}, {"name": "Yeong-Dae Kwon", "authorId": "2301413463"}, {"name": "Taehee Lee", "authorId": "2301133161"}, {"name": "Seungjai Min", "authorId": "2301015935"}], "n_citations": 2}, "snippets": ["Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)", "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence", "(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks", "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."], "score": 0.6982421875}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 51606954, "title": "Curriculum Learning for Natural Answer Generation", "year": 2018, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Cao Liu", "authorId": "49046540"}, {"name": "Shizhu He", "authorId": "1954845"}, {"name": "Kang Liu", "authorId": "2200096"}, {"name": "Jun Zhao", "authorId": "1390572170"}], "n_citations": 85}, "snippets": ["By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."], "score": 0.0}, {"id": "(Croitoru et al., 2022)", "paper": {"corpus_id": 248887633, "title": "LeRaC: Learning Rate Curriculum", "year": 2022, "venue": "International Journal of Computer Vision", "authors": [{"name": "Florinel-Alin Croitoru", "authorId": "2154573729"}, {"name": "Nicolae-C\u0103t\u0103lin Ristea", "authorId": "103931166"}, {"name": "Radu Tudor Ionescu", "authorId": "1817759"}, {"name": "N. Sebe", "authorId": "1703601"}], "n_citations": 9}, "snippets": ["The main challenge for the methods that build the curriculum at the data level is measuring the difficulty of the data samples, which is required to order the samples from easy to hard. Most studies have addressed the problem with human input (Pentina et al., 2014)(Jim\u00e9nez-S\u00e1nchez et al., 2019)(Wei et al., 2020) or metrics based on domain-specific heuristics. For instance, the text length (Kocmi et al., 2017)[44](Tay et al., 2019)(Zhang et al., 2021) and the word frequency [1,(Liu et al., 2018) have been employed in natural language processing. In computer vision, the samples containing fewer and larger objects have been considered to be easier in some works (Shi et al., 2016)[33]. Other solutions employed difficulty estimators (Ionescu et al., 2016) or even the confidence level of the predictions made by the neural network (Gong et al., 2016)(Hacohen et al., 2019) to approximate the complexity of the data samples. Other studies (Khan et al., 2024)(Khan et al., 2022)(Khan et al., 2023) used the error of a previously trained model to estimate the difficulty of each sample."], "score": 0.869140625}, {"id": "(Vakil et al., 2023)", "paper": {"corpus_id": 265351926, "title": "Complexity-Guided Curriculum Learning for Text Graphs", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Nidhi Vakil", "authorId": "2165570025"}, {"name": "Hadi Amiri", "authorId": "2267488904"}], "n_citations": 3}, "snippets": ["Current curriculum learning methodologies in NLP rely on data properties, e.g., sentence length, word rarity, or syntactic features (Platanios et al., 2019)(Liu et al., 2022), or annotation disagreement (Elgaar et al., 2023); as well as model properties such as training loss and its variations (Graves et al., 2017)(Amiri et al., 2017) to sequence data samples for training. (Elgaar et al., 2023) developed a curriculum discovery framework based on prior knowledge of sample difficulty, utilized annotation entropy and loss values."], "score": 0.63427734375}, {"id": "(Shi et al., 2016)", "paper": {"corpus_id": 6954583, "title": "Weakly Supervised Object Localization Using Size Estimates", "year": 2016, "venue": "European Conference on Computer Vision", "authors": [{"name": "Miaojing Shi", "authorId": "8003001"}, {"name": "V. Ferrari", "authorId": "143865718"}], "n_citations": 81}, "snippets": ["We present a technique for weakly supervised object localization (WSOL), building on the observation that WSOL algorithms usually work better on images with bigger objects. Instead of training the object detector on the entire training set at the same time, we propose a curriculum learning strategy to feed training images into the WSOL learning loop in an order from images containing bigger objects down to smaller ones. To automatically determine the order, we train a regressor to estimate the size of the object given the whole image as input. Furthermore, we use these size estimates to further improve the re-localization step of WSOL by assigning weights to object proposals according to how close their size matches the estimated object size. We demonstrate the effectiveness of using size order and size weighting on the challenging PASCAL VOC 2007 dataset, where we achieve a significant improvement over existing state-of-the-art WSOL techniques."], "score": 0.0}, {"id": "(Jarca et al., 2024)", "paper": {"corpus_id": 271051051, "title": "CBM: Curriculum by Masking", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Andrei Jarca", "authorId": "2310341007"}, {"name": "Florinel-Alin Croitoru", "authorId": "2154573729"}, {"name": "R. Ionescu", "authorId": "2249763264"}], "n_citations": 0}, "snippets": ["Researchers have made significant strides in finding improved metrics for various domains and tasks.For instance, images containing fewer and larger objects in computer vision are deemed easier than other images (Shi et al., 2016)(Soviany et al., 2019).In natural language processing, word frequency (Bengio et al., 2009)(Liu et al., 2018) and sequence length [8](Kocmi et al., 2017)(Tay et al., 2019)(Zhang et al., 2021) are utilized to assess the sample difficulty.In some cases, researchers have also integrated human feedback into their metric design (Jim\u00e9nez-S\u00e1nchez et al., 2019)(Pentina et al., 2014)(Wei et al., 2020)."], "score": 0.69384765625}, {"id": "(Wei et al., 2020)", "paper": {"corpus_id": 221995570, "title": "Learn like a Pathologist: Curriculum Learning by Annotator Agreement for Histopathology Image Classification", "year": 2020, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Jerry W. Wei", "authorId": "47807379"}, {"name": "A. Suriawinata", "authorId": "3583138"}, {"name": "Bing Ren", "authorId": "2064646056"}, {"name": "Xiaoying Liu", "authorId": "49543754"}, {"name": "Mikhail Lisovsky", "authorId": "7013728"}, {"name": "L. Vaickus", "authorId": "37530422"}, {"name": "Charles Brown", "authorId": "2111163034"}, {"name": "Michael Baker", "authorId": "2067799679"}, {"name": "Mustafa Nasir-Moin", "authorId": "1659771244"}, {"name": "Naofumi Tomita", "authorId": "47136049"}, {"name": "L. Torresani", "authorId": "1732879"}, {"name": "Jason Wei", "authorId": "144026731"}, {"name": "S. Hassanpour", "authorId": "145945685"}], "n_citations": 49}, "snippets": ["Applying curriculum learning requires both a range of difficulty in data and a method for determining the difficulty of examples. In many tasks, however, satisfying these requirements can be a formidable challenge.In this paper, we contend that histopathology image classification is a compelling use case for curriculum learning. Based on the nature of histopathology images, a range of difficulty inherently exists among examples, and, since medical datasets are often labeled by multiple annotators, annotator agreement can be used as a natural proxy for the difficulty of a given example. Hence, we propose a simple curriculum learning method that trains on progressively-harder images as determined by annotator agreement.We evaluate our hypothesis on the challenging and clinically-important task of colorectal polyp classification. Whereas vanilla training achieves an AUC of 83.7% for this task, a model trained with our proposed curriculum learning approach achieves an AUC of 88.2%, an improvement of 4.5%. Our work aims to inspire researchers to think more creatively and rigorously when choosing contexts for applying curriculum learning."], "score": 0.0}, {"id": "(Jimenez-Sanchez et al., 2019)", "paper": {"corpus_id": 204539326, "title": "Medical-based Deep Curriculum Learning for Improved Fracture Classification", "year": 2019, "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention", "authors": [{"name": "Amelia Jim\u00e9nez-S\u00e1nchez", "authorId": "1404288037"}, {"name": "D. Mateus", "authorId": "1704770"}, {"name": "S. Kirchhoff", "authorId": "79884102"}, {"name": "C. Kirchhoff", "authorId": "4858301"}, {"name": "P. Biberthaler", "authorId": "5140452"}, {"name": "N. Navab", "authorId": "145587209"}, {"name": "M. Ballester", "authorId": "49463953"}, {"name": "G. Piella", "authorId": "50186507"}], "n_citations": 48}, "snippets": ["Current deep-learning based methods do not easily integrate to clinical protocols, neither take full advantage of medical knowledge. In this work, we propose and compare several strategies relying on curriculum learning, to support the classification of proximal femur fracture from X-ray images, a challenging problem as reflected by existing intra- and inter-expert disagreement. Our strategies are derived from knowledge such as medical decision trees and inconsistencies in the annotations of multiple experts, which allows us to assign a degree of difficulty to each training sample. We demonstrate that if we start learning \"easy\" examples and move towards \"hard\", the model can reach a better performance, even with fewer data. The evaluation is performed on the classification of a clinical dataset of about 1000 X-ray images. Our results show that, compared to class-uniform and random strategies, the proposed medical knowledge-based curriculum, performs up to 15% better in terms of accuracy, achieving the performance of experienced trauma surgeons."], "score": 0.0}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 267770178, "title": "An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuemei Tang", "authorId": "50295874"}, {"name": "Qi Su", "authorId": "2284863194"}], "n_citations": 0}, "snippets": ["For instance, Mohiuddin et al. (2022) developed three deterministic scoring methods for machine translation that leverage the similarity of a parallel sentence pairs and cross-entropy. (Zhu et al., 2021) evaluated sample difficulty in dialogue generation based on model loss. Lobov et al. (2022) utilized metrics such as sentence length, average confidence, and perplexity value to assess difficulty in named entity recognition (NER)."], "score": 0.8076171875}, {"id": "(Ranaldi et al., 2023)", "paper": {"corpus_id": 265068175, "title": "Modeling Easiness for Training Transformers with Curriculum Learning", "year": 2023, "venue": "Recent Advances in Natural Language Processing", "authors": [{"name": "Leonardo Ranaldi", "authorId": "2008183566"}, {"name": "Giulia Pucci", "authorId": "2199247500"}, {"name": "F. M. Zanzotto", "authorId": "103839825"}], "n_citations": 13}, "snippets": ["Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."], "score": 0.0}, {"id": "(Raymond-Saez et al., 2022)", "paper": {"corpus_id": 250408174, "title": "A Study on the Predictability of Sample Learning Consistency", "year": 2022, "venue": "LatinX in AI at International Conference on Machine Learning 2022", "authors": [{"name": "Alain Raymond-S\u00e1ez", "authorId": "2110753585"}, {"name": "J. Hurtado", "authorId": "2064859104"}, {"name": "\u00c1lvaro Soto", "authorId": "2052429926"}], "n_citations": 0}, "snippets": ["Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified."], "score": 0.6787109375}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."], "score": 0.88037109375}, {"id": "(Zhang et al._1, 2021)", "paper": {"corpus_id": 243766208, "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding", "year": 2021, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "authors": [{"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 14}, "snippets": ["With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."], "score": 0.0}], "table": null}, {"title": "Combined and Novel Approaches", "tldr": "Recent research combines multiple difficulty metrics to create more robust measures of training data complexity. These hybrid approaches integrate model-centric metrics with data characteristics to provide comprehensive difficulty assessments that better support curriculum learning in LLM training. (10 sources)", "text": "\nRecognizing the limitations of individual difficulty metrics, researchers have developed innovative combined approaches that leverage multiple signals to measure training data difficulty more comprehensively. These hybrid methods aim to capture different aspects of learning difficulty and provide more nuanced assessments for curriculum learning in LLM training.\n\nOne notable approach combines representation distance with perplexity scores to create a more robust difficulty measure <Paper corpusId=\"266166227\" paperTitle=\"(Zhou et al._1, 2023)\" isShortName></Paper>. This integration leverages both semantic understanding and linguistic complexity to better assess example difficulty from multiple perspectives. Similarly, other researchers have explored ensemble-based methods that measure example difficulty through agreement across predictions from multiple model components, either from final layers or intermediate representations <Paper corpusId=\"250626773\" paperTitle=\"(Peng et al., 2022)\" isShortName></Paper> <Paper corpusId=\"235458180\" paperTitle=\"(Baldock et al., 2021)\" isShortName></Paper>.\n\nThe Gradient-based Difficulty Measure (GDM) represents another innovative approach that evaluates example difficulty through dynamic measurement of gradient magnitude with respect to the example itself. Unlike training loss, which only considers the difference between predictions and ground truth, GDM takes input features into account, allowing for more fine-grained assessment even when different examples yield identical loss values. This is particularly valuable in loss landscapes with plateaus or saddle points, where training loss might remain stable despite substantial shifts in model parameters <Paper corpusId=\"268696658\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nRecent work has also explored model-centric difficulty metrics that move beyond data-specific characteristics. Kim et al. propose calculating difficulty from the model's perspective rather than relying solely on traditional metrics like text length or word rarity <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This approach recognizes that true learning difficulty depends on the interaction between the model and the data, not just inherent data properties.\n\nThe Confidence-guided Curriculum Learning (CCL) framework represents another innovative approach that integrates difficulty measurement with proficiency estimation based on curriculum learning losses observed during both training and validation phases. This method eliminates dependency on task-specific metrics, making implementation more straightforward across different domains <Paper corpusId=\"273901573\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nSeveral researchers have adopted the general curriculum learning framework of \"difficulty measure + training scheduler\" <Paper corpusId=\"261557260\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"232362223\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. In this approach, the difficulty measure determines the relative complexity of each data sample, while the training scheduler decides the sequence of data subsets throughout the training process. This framework has proven effective not only for optimizing learning but also for data cleaning, with studies showing performance improvements on denoised data <Paper corpusId=\"261557260\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"1054586\" paperTitle=\"(Wang et al., 2017)\" isShortName></Paper> <Paper corpusId=\"174798125\" paperTitle=\"(Wang et al., 2019)\" isShortName></Paper>.\n\nThese combined and novel approaches to measuring data difficulty represent a significant advancement over single-metric methods. By integrating multiple signals and considering both data characteristics and model behavior, these techniques provide more comprehensive assessments of training example complexity, enabling more effective curriculum construction for large language model training.", "citations": [{"id": "(Zhou et al._1, 2023)", "paper": {"corpus_id": 266166227, "title": "Non-compositional Expression Generation Based on Curriculum Learning and Continual Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jianing Zhou", "authorId": "102489044"}, {"name": "Ziheng Zeng", "authorId": "41048608"}, {"name": "Hongyu Gong", "authorId": "2008458"}, {"name": "Suma Bhat", "authorId": "2263637139"}], "n_citations": 2}, "snippets": ["Therefore, we use a combination of the representation distance and perplexity score as a measure of examples' difficulty."], "score": 0.69091796875}, {"id": "(Peng et al., 2022)", "paper": {"corpus_id": 250626773, "title": "Angular Gap: Reducing the Uncertainty of Image Difficulty through Model Calibration", "year": 2022, "venue": "ACM Multimedia", "authors": [{"name": "Bohua Peng", "authorId": "2147421343"}, {"name": "Mobarakol Islam", "authorId": "145481136"}, {"name": "Mei Tu", "authorId": "2067051679"}], "n_citations": 9}, "snippets": ["Very recently, deep ensemble methods (Baldock et al., 2021)[18] measure example difficulty with agreement either from last layers' predictions or from intermediate layers' predictions. Reducing estimation uncertainty with ensembling requires selected treatments and controls such as architectures, number of submodels, and number of data splits. In this work, we show faithful image difficulty can be efficiently estimated by deep metric learning. Hyperspherical learning [26], a weakly supervised learning framework, groups instances of the same concept together and pushes instances of different concepts apart by enforcing angular discrimination during training."], "score": 0.6455078125}, {"id": "(Baldock et al., 2021)", "paper": {"corpus_id": 235458180, "title": "Deep Learning Through the Lens of Example Difficulty", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "R. Baldock", "authorId": "37680581"}, {"name": "Hartmut Maennel", "authorId": "50153692"}, {"name": "Behnam Neyshabur", "authorId": "3007442"}], "n_citations": 161}, "snippets": ["Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model's uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first."], "score": 0.0}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268696658, "title": "Symmetric Self-Paced Learning for Domain Generalization", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Di Zhao", "authorId": "2110600983"}, {"name": "Yun Sing Koh", "authorId": "34930533"}, {"name": "Gillian Dobbie", "authorId": "2276344048"}, {"name": "Hongsheng Hu", "authorId": "2293565669"}, {"name": "Philippe Fournier-Viger", "authorId": "2293453267"}], "n_citations": 5}, "snippets": ["Current methods evaluate example difficulty through predefined metrics, such as sentence length, or dynamically update it based on training loss, such as cross-entropy loss. However, predefined difficulty measures do not integrate the model's feedback, and training loss, focusing only on the difference between predictions and ground truth, raises an inaccurate difficulty measurement issue when different examples yield identical training losses.\n\nTo address these limitations, we propose the Gradientbased Difficulty Measure (GDM), which evaluates example difficulty through dynamic measurement of the gradient magnitude with respect to the example itself. Unlike training loss, the gradient avoids inaccurate difficulty assessment by taking input features into account. As a result, even if examples yield the same loss, their gradients can differ. Additionally, loss landscapes can encompass plateaus or saddle points, where training loss remains relatively stable even with substantial shifts in model parameters. In these scenarios, utilizing training loss for evaluating example difficulty can be misleading. Conversely, the gradient provides finergrained insights into changes in model parameters, making the gradient magnitude a more informative approach for evaluating difficulty."], "score": 0.7685546875}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."], "score": 0.88037109375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 273901573, "title": "Curriculum Consistency Learning for Conditional Sentence Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Liangxin Liu", "authorId": "2287881971"}, {"name": "Xuebo Liu", "authorId": "2256344322"}, {"name": "Lian Lian", "authorId": "2324576203"}, {"name": "Shengjun Cheng", "authorId": "2324857773"}, {"name": "Jun Rao", "authorId": "2142545243"}, {"name": "Tengfei Yu", "authorId": "2273704740"}, {"name": "Hexuan Deng", "authorId": "2193388247"}, {"name": "Min Zhang", "authorId": "2269805934"}], "n_citations": 1}, "snippets": ["CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process."], "score": 0.62451171875}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261557260, "title": "Epi-Curriculum: Episodic Curriculum Learning for Low-Resource Domain Adaptation in Neural Machine Translation", "year": 2023, "venue": "IEEE Transactions on Artificial Intelligence", "authors": [{"name": "Keyu Chen", "authorId": "2238164778"}, {"name": "Zhuang Di", "authorId": "2237989461"}, {"name": "Mingchen Li", "authorId": "2260916568"}, {"name": "J. Morris Chang", "authorId": "2334755606"}], "n_citations": 3}, "snippets": ["We follow the general curriculum learning framework of difficulty measure and training scheduler (Wang et al., 2021), where difficulty measure determines the relative \"difficulty\" of each data sample and training scheduler decides the sequence of data subsets throughout the training process. Additionally, curriculum learning has been approved as an effective method for data cleaning, and many works have shown that the performance can be improved on denoised data (Wang et al., 2017)(Wang et al., 2019)."], "score": 0.57958984375}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 232362223, "title": "A Survey on Curriculum Learning", "year": 2021, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Xin Wang", "authorId": "2153687490"}, {"name": "Yudong Chen", "authorId": "51310474"}, {"name": "Wenwu Zhu", "authorId": "145583986"}], "n_citations": 611}, "snippets": ["Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=\"LaTeX\">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=\"wang-ieq1-3069908.gif\"/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations."], "score": 0.0}, {"id": "(Wang et al., 2017)", "paper": {"corpus_id": 1054586, "title": "Instance Weighting for Neural Machine Translation Domain Adaptation", "year": 2017, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Rui Wang", "authorId": "108085542"}, {"name": "M. Utiyama", "authorId": "1802277"}, {"name": "Lemao Liu", "authorId": "2978364"}, {"name": "Kehai Chen", "authorId": "2849740"}, {"name": "E. Sumita", "authorId": "1698363"}], "n_citations": 132}, "snippets": ["Instance weighting has been widely applied to phrase-based machine translation domain adaptation. However, it is challenging to be applied to Neural Machine Translation (NMT) directly, because NMT is not a linear model. In this paper, two instance weighting technologies, i.e., sentence weighting and domain weighting with a dynamic weight learning strategy, are proposed for NMT domain adaptation. Empirical results on the IWSLT English-German/French tasks show that the proposed methods can substantially improve NMT performance by up to 2.7-6.7 BLEU points, outperforming the existing baselines by up to 1.6-3.6 BLEU points."], "score": 0.0}, {"id": "(Wang et al., 2019)", "paper": {"corpus_id": 174798125, "title": "Dynamically Composing Domain-Data Selection with Clean-Data Selection by \u201cCo-Curricular Learning\u201d for Neural Machine Translation", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Wang", "authorId": "40397893"}, {"name": "Isaac Caswell", "authorId": "73769795"}, {"name": "Ciprian Chelba", "authorId": "1802969"}], "n_citations": 59}, "snippets": ["Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a \"co-curricular learning\" method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the \"co-curriculum\". Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum."], "score": 0.0}], "table": null}, {"title": "Automatic Difficulty Measurement", "tldr": "Automatic difficulty measurement techniques dynamically assess training example complexity without requiring predefined heuristics or human expertise. These approaches leverage real-time model feedback such as prediction errors, training loss, and cross-entropy to automatically determine data difficulty during the training process. (6 sources)", "text": "\nAutomatic difficulty measurement addresses a fundamental limitation of curriculum learning: the traditional requirement for manually crafted, task-specific difficulty metrics. Rather than relying on predefined heuristics that may not generalize well across domains, automatic approaches determine sample difficulty dynamically during training without extensive human intervention <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nSelf-paced learning (SPL) represents one of the earliest automatic difficulty measurement techniques, using the current loss of an example as a proxy for difficulty. However, this approach has known limitations, including vulnerability to overfitting and the requirement for additional computation <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>. To address these issues, researchers have developed alternative automatic metrics such as C-Score, which measures how consistently a particular example is learned across various models\u2014essentially capturing the proportion of iterations where a sample is successfully classified <Paper corpusId=\"250408174\" paperTitle=\"(Raymond-Saez et al., 2022)\" isShortName></Paper>.\n\nMore sophisticated automatic approaches modify difficulty scores based on prediction errors, which serve as critical indicators of model uncertainty. For instance, Xu et al. introduced two automatic difficulty measurers: the Hamming Distance-based Measurer (HDM) and the Cross Entropy-based Measurer (CEM). While hamming distance calculates the difference between two equal-length strings, cross-entropy estimates uncertainty by comparing real distribution and prediction <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper>. These methods build upon earlier work in automatic difficulty measurement, including transfer teacher approaches and reinforcement learning-based curriculum selection <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>.\n\nReinforcement learning has proven particularly valuable for automatically selecting the optimal curriculum path. Graves et al. demonstrated a method that uses learning progress signals\u2014such as increases in prediction accuracy or network complexity\u2014as rewards for a multi-armed bandit algorithm that determines a stochastic syllabus <Paper corpusId=\"256460262\" paperTitle=\"(Xu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"11137059\" paperTitle=\"(Graves et al., 2017)\" isShortName></Paper>. This approach has shown significant acceleration in learning efficiency, in some cases reducing training time by half.\n\nRecent research has further refined automatic difficulty measurement for practical applications. Zeng et al. designed an automatic difficulty measurer that dynamically categorizes samples as either \"easy\" or \"difficult\" based on whether their ground truth scores are correctly predicted during a training epoch <Paper corpusId=\"259593617\" paperTitle=\"(Zeng et al., 2023)\" isShortName></Paper>. This binary classification approach enables more flexible training processes without relying on complex difficulty scales.\n\nThe Confidence-guided Curriculum Learning (CCL) framework represents another advancement in automatic difficulty measurement. This approach integrates difficulty measurement with proficiency estimation derived directly from curriculum learning losses observed during both training and validation phases. By eliminating dependency on task-specific metrics, CCL streamlines implementation across different domains <Paper corpusId=\"273901573\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nThese automatic techniques have proven particularly valuable for large-scale pretraining tasks, where they improve both data and training efficiency <Paper corpusId=\"254366532\" paperTitle=\"(Li et al._1, 2022)\" isShortName></Paper>. By dynamically adjusting to the model's current state and learning progress, automatic difficulty measurers provide a more adaptive approach to curriculum construction than static, predefined metrics, making them especially well-suited for the complex training dynamics of large language models.", "citations": [{"id": "(Raymond-Saez et al., 2022)", "paper": {"corpus_id": 250408174, "title": "A Study on the Predictability of Sample Learning Consistency", "year": 2022, "venue": "LatinX in AI at International Conference on Machine Learning 2022", "authors": [{"name": "Alain Raymond-S\u00e1ez", "authorId": "2110753585"}, {"name": "J. Hurtado", "authorId": "2064859104"}, {"name": "\u00c1lvaro Soto", "authorId": "2052429926"}], "n_citations": 0}, "snippets": ["Curriculum Learning (CL) strategies are based on providing a model with an increasingly difficult selection of examples, similar to how children learn in school. CL strategies are popular in Reinforcement Learning -where a curriculum of tasks is progressively learned -and Natural Language Processing applications -where commonly sentence length has been a proxy for difficulty.\n\nThe problem, however, lies in how to determine the difficulty of samples. Typically, an ad hoc difficulty measure needs to be crafted by hand for a specific problem, which limits their application in practice. Even worse, for some applications like image classification, sometimes there's no natural difficulty ordering that can be created. This has led to the development of more general measures of difficulty: Self Paced Learning (Kumar et al., 2010) utilizes the current loss of an example as a proxy for difficulty; however, this requires extra computation and is prone to overfitting (Jiang et al., 2015). As an alternative, a metric called C-Score was proposed (Jiang et al., 2021). This metric measures how consistently a particular example is learned across various models, that is, it measures the proportion of iterations where a sample is successfully classified."], "score": 0.6787109375}, {"id": "(Xu et al., 2023)", "paper": {"corpus_id": 256460262, "title": "Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems", "year": 2023, "venue": "ACM Transactions on Software Engineering and Methodology", "authors": [{"name": "Qinghua Xu", "authorId": "2118221146"}, {"name": "Sajid Ali", "authorId": "47495965"}, {"name": "T. Yue", "authorId": "145456500"}], "n_citations": 22}, "snippets": ["To alleviate these problems, various automatic difficulty measurers have been developed and explored in the literature, including self-paced learning (Kumar et al., 2010), transfer teacher [69], reinforcement learning teacher (Graves et al., 2017), and other automatic difficulty measurers (Jiang et al., 2017)(Sanger, 1994)[54]. Inspired by these methods, we modify difficulty scores automatically with prediction errors, which are critical indicators of CPS uncertainty. Substantial work has been conducted in the literature, demonstrating the importance of handling uncertainties in CPS security and safety [21,(Han et al., 2022)(Ma et al., 2019)(Xu et al., 2022)(Zhang et al., 2019)[68]", "we define the following two types of automatic difficulty measurers: Hamming Distance-based Measurer (HDM, Definition 5.1) and Cross Entropy-based Measurer (CEM, Definition 5.2). Hamming distance is commonly used to calculate the difference of two strings of equal length, while cross-entropy loss estimates uncertainty by comparing real distribution and prediction."], "score": 0.658203125}, {"id": "(Graves et al., 2017)", "paper": {"corpus_id": 11137059, "title": "Automated Curriculum Learning for Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alex Graves", "authorId": "1753223"}, {"name": "Marc G. Bellemare", "authorId": "1792298"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "R. Munos", "authorId": "1708654"}, {"name": "K. Kavukcuoglu", "authorId": "2645384"}], "n_citations": 530}, "snippets": ["We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level."], "score": 0.0}, {"id": "(Zeng et al., 2023)", "paper": {"corpus_id": 259593617, "title": "On the Effectiveness of Curriculum Learning in Educational Text Scoring", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zijie Zeng", "authorId": "8647899"}, {"name": "D. Ga\u0161evi\u0107", "authorId": "65953975"}, {"name": "Guangliang Chen", "authorId": "49901492"}], "n_citations": 6}, "snippets": ["Automatic. Though pre-defined strategies have been demonstrated effective in various application scenarios, they are often plagued by their strong reliance on human expertise to define an appropriate difficulty measurer and an extensive search for effective combinations of difficulty measurer and training scheduler. Therefore, in addition to the four pre-defined CL strategies described above, we further designed an automatic difficulty measurer to dynamically select data samples based on instance-wise training loss and enable a more flexible training process. Specifically, the automatic difficulty measurer used in this study characterizes data samples as either easy and difficult, which represents the samples whose ground truth scores are correctly or incorrectly predicted by a model in a training epoch."], "score": 0.623046875}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 273901573, "title": "Curriculum Consistency Learning for Conditional Sentence Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Liangxin Liu", "authorId": "2287881971"}, {"name": "Xuebo Liu", "authorId": "2256344322"}, {"name": "Lian Lian", "authorId": "2324576203"}, {"name": "Shengjun Cheng", "authorId": "2324857773"}, {"name": "Jun Rao", "authorId": "2142545243"}, {"name": "Tengfei Yu", "authorId": "2273704740"}, {"name": "Hexuan Deng", "authorId": "2193388247"}, {"name": "Min Zhang", "authorId": "2269805934"}], "n_citations": 1}, "snippets": ["CCL integrates a difficulty measure and a proficiency estimator directly derived from the CL losses observed in both the training and validation phases. This innovation eliminates the dependency on task-specific metrics, streamlining the implementation process."], "score": 0.62451171875}, {"id": "(Li et al._1, 2022)", "paper": {"corpus_id": 254366532, "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Conglong Li", "authorId": "2609325"}, {"name": "Z. Yao", "authorId": "9088433"}, {"name": "Xiaoxia Wu", "authorId": "2129511744"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 25}, "snippets": ["These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022)."], "score": 0.57861328125}], "table": null}, {"title": "Applications and Effectiveness in LLM Training", "tldr": "Curriculum learning strategies have shown considerable effectiveness in LLM training across both pre-training and fine-tuning stages, leading to improved model performance and training efficiency. Research demonstrates that organizing training data by difficulty\u2014whether using traditional metrics like sequence length and word frequency or newer model-centric approaches\u2014yields better results than random shuffling. (8 sources)", "text": "\nCurriculum learning (CL) has emerged as a powerful approach for enhancing the training of large language models. By organizing training data from easier to more difficult examples, CL has proven particularly effective for large-scale pre-training tasks, improving both data and training efficiency <Paper corpusId=\"254366532\" paperTitle=\"(Li et al._1, 2022)\" isShortName></Paper>. This approach mimics human learning processes, providing a more structured path for model development than traditional random shuffling methods.\n\nRecent research has implemented curriculum learning at various stages of LLM development. At the pre-training stage, Li et al. employed sequence length as a difficulty metric, starting with shorter sequences and gradually incorporating longer ones <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. Similarly, Wang et al. used word frequency as a complexity metric for curriculum-based pre-training, with less frequent words considered more difficult <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. These linguistically motivated curricula have demonstrated tangible benefits for model performance.\n\nA particularly notable advancement comes from Ranaldi et al., who developed a complexity measure based on length, rarity, and comprehensibility of samples (LRC). When applied to pre-training BERT and RoBERTa from scratch, their curriculum learning approach led to improved performance on downstream tasks compared to existing methods <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"265068175\" paperTitle=\"(Ranaldi et al., 2023)\" isShortName></Paper>. This suggests that more sophisticated, multi-dimensional difficulty metrics can yield even stronger results than single-factor approaches.\n\nFor fine-tuning stages, curriculum learning has also shown significant benefits. Zhang et al. developed a framework consisting of \"Review\" and \"Arrange\" stages to address the challenges of defining instance difficulty and arranging an appropriate curriculum. Their approach yielded substantial performance improvements across various natural language understanding tasks in different languages <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"243766208\" paperTitle=\"(Zhang et al._1, 2021)\" isShortName></Paper>.\n\nBeyond traditional metrics, researchers have explored more model-centric approaches to measuring data difficulty. Kim et al. proposed calculating difficulty from the model's perspective rather than relying solely on data-specific characteristics like text length or word rarity. Their method uses cross-entropy loss as a direct indicator of how challenging the model finds particular examples, with higher loss values correlating with increased difficulty <Paper corpusId=\"269756933\" paperTitle=\"(Kim et al., 2024)\" isShortName></Paper>. This approach acknowledges that true learning difficulty depends on the interaction between the model and the data, not just inherent data properties.\n\nIn data pruning applications, Marion et al. investigated multiple neural heuristics of sample difficulty and concluded that the perplexity of a sample under a reference language model serves as the best pruning metric <Paper corpusId=\"270199394\" paperTitle=\"(Ankner et al., 2024)\" isShortName></Paper>. This finding highlights the value of model-based difficulty metrics for optimizing training data selection.\n\nCurriculum learning principles have also been applied to knowledge distillation processes. Adaptive knowledge distillation techniques adjust the weight of each sample's loss based on its difficulty or use adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions. This targeted approach emphasizes harder samples that the student model struggles with, providing more detailed knowledge transfer from the teacher model and improving performance more efficiently than uniform distillation methods <Paper corpusId=\"277398866\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nIn scenarios with noisy or uneven-quality data, curriculum learning offers additional benefits. Liu et al. demonstrated that a curriculum learning framework for natural answer generation (CL-NAG) could effectively exploit valuable information from noisy corpora by first utilizing simple, low-quality question-answer pairs to learn a basic model, then gradually incorporating more complex, higher-quality examples <Paper corpusId=\"269741199\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper> <Paper corpusId=\"51606954\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper>. This approach increased accuracy for both simple and complex questions compared to state-of-the-art methods.", "citations": [{"id": "(Li et al._1, 2022)", "paper": {"corpus_id": 254366532, "title": "DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing", "year": 2022, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Conglong Li", "authorId": "2609325"}, {"name": "Z. Yao", "authorId": "9088433"}, {"name": "Xiaoxia Wu", "authorId": "2129511744"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 25}, "snippets": ["These techniques improve data and training efficiency, but existing solutions have several limitations: \u2022 Techniques like curriculum learning (CL) improve data efficiency by indexing and sampling training data based on certain difficulty metric (Bengio et al., 2009), and it has recently proved effective on large-scale pretraining tasks (Li, Zhang, and He 2022)."], "score": 0.57861328125}, {"id": "(Lee et al., 2024)", "paper": {"corpus_id": 269741199, "title": "Improving Instruction Following in Language Models through Proxy-Based Uncertainty Estimation", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Joonho Lee", "authorId": "2116659852"}, {"name": "Jae Oh Woo", "authorId": "2301016207"}, {"name": "Juree Seok", "authorId": "2301014929"}, {"name": "Parisa Hassanzadeh", "authorId": "2301015562"}, {"name": "Wooseok Jang", "authorId": "2301015224"}, {"name": "JuYoun Son", "authorId": "2301016428"}, {"name": "Sima Didari", "authorId": "91748824"}, {"name": "Baruch Gutow", "authorId": "2301014892"}, {"name": "Heng Hao", "authorId": "2065513368"}, {"name": "Hankyu Moon", "authorId": "2301015932"}, {"name": "Wenjun Hu", "authorId": "2301166595"}, {"name": "Yeong-Dae Kwon", "authorId": "2301413463"}, {"name": "Taehee Lee", "authorId": "2301133161"}, {"name": "Seungjai Min", "authorId": "2301015935"}], "n_citations": 2}, "snippets": ["Curriculum Learning It is observed that deep learning model training can benefit from the implementation of Curriculum Learning (CL), i.e., using data samples sorted based on a curriculum versus training on the randomly shuffled data (Soviany et al., 2021). Recently, CL methods have been developed and deployed for the LMs as well, at pre-training and post-training stages using a variety of linguistically motivated curricula such as sentence length or term frequency complexity measure based ranking (Liu et al., 2018)(Zhang et al., 2021)Campos, 2021;Weber et al., 2023)", "Li et al., 2021 implemented a CL at the pre-training of LMs using the sequence length as the difficulty metric with the curriculum of starting from the shorter sequence training data toward the longer sequence", "(Ranaldi et al., 2023) proposed a new complexity measure based on the length, rarity, and comprehensibility of the samples and sorted the corpus according to the proposed complexity measure during the pre-training stage and showed that their CL approach led to better performance in downstream tasks", "Wang et al., 2022 used the frequency of words as the complexity metric for the curriculum-based pre-training of LMs."], "score": 0.6982421875}, {"id": "(Ranaldi et al., 2023)", "paper": {"corpus_id": 265068175, "title": "Modeling Easiness for Training Transformers with Curriculum Learning", "year": 2023, "venue": "Recent Advances in Natural Language Processing", "authors": [{"name": "Leonardo Ranaldi", "authorId": "2008183566"}, {"name": "Giulia Pucci", "authorId": "2199247500"}, {"name": "F. M. Zanzotto", "authorId": "103839825"}], "n_citations": 13}, "snippets": ["Directly learning from complex examples is generally problematic for humans and machines. Indeed, a better strategy is exposing learners to examples in a reasonable, pedagogically-motivated order. Curriculum Learning (CL) has been proposed to import this strategy when training machine learning models. In this paper, building on Curriculum Learning, we propose a novel, linguistically motivated measure to determine example complexity for organizing examples during learning. Our complexity measure - LRC- is based on length, rarity, and comprehensibility. Our resulting learning model is CL-LRC, that is, CL with LRC. Experiments on downstream tasks show that CL-LRC outperforms existing CL and non-CL methods for training BERT and RoBERTa from scratch. Furthermore, we analyzed different measures, including perplexity, loss, and learning curve of different models pre-trained from scratch, showing that CL-LRC performs better than the state-of-the-art."], "score": 0.0}, {"id": "(Zhang et al._1, 2021)", "paper": {"corpus_id": 243766208, "title": "Review and Arrange: Curriculum Learning for Natural Language Understanding", "year": 2021, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "authors": [{"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zhendong Mao", "authorId": "1855978"}, {"name": "Benfeng Xu", "authorId": "1754285124"}, {"name": "Quan Wang", "authorId": "143906199"}, {"name": "Yongdong Zhang", "authorId": "1699819"}], "n_citations": 14}, "snippets": ["With the notable success of pretrained language models, the pretraining-fine-tuning paradigm has become a dominant solution for natural language understanding (NLU) tasks. Typically, the training instances of a target NLU task are introduced in a completely random order and treated equally at the fine-tuning stage. However, these instances can vary greatly in difficulty, and similar to human learning procedures, language models can benefit from an easy-to-difficult curriculum. Based on this concept, we propose a curriculum learning (CL) framework. Our framework consists of two stages, Review and Arrange, targeting the two main challenges in curriculum learning, i.e., how to define the difficulty of instances and how to arrange a curriculum based on the difficulty, respectively. In the first stage, we devise a cross-review (CR) method to train several teacher models first and then review the training set in a crossed manner to distinguish easy instances from difficult instances. In the second stage, two sampling algorithms, a coarse-grained arrangement (CGA) and a fine-grained arrangement (FGA), are proposed to arrange a curriculum for language models in which the learning materials start from the easiest instances, and more difficult instances are gradually added into the training procedure. Compared to previous heuristic CL methods, our framework can avoid the errors caused by a gap in difficulty between humans and machines and has strong generalization ability. We conduct comprehensive experiments, and the results show that our curriculum learning framework, without any manual model architecture design or use of external data, obtains significant and universal performance improvements on a wide range of NLU tasks in different languages."], "score": 0.0}, {"id": "(Kim et al., 2024)", "paper": {"corpus_id": 269756933, "title": "Strategic Data Ordering: Enhancing Large Language Model Performance through Curriculum Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jisu Kim", "authorId": "2301165169"}, {"name": "Juhwan Lee", "authorId": "2301167177"}], "n_citations": 10}, "snippets": ["In the development of training data for curriculum learning, accurately measuring data difficulty is crucial. Specifically, for LLMs (Large Language Models), determining the degree of data difficulty is challenging. Traditional metrics, such as text length or word rarity, are often employed to estimate the difficulty of training data (Chang et al., 2021; Nagatsuka et al., 2023). However, these metrics may not fully reflect the complexity of a dataset. It is essential to assess data difficulty from the model's perspective, rather than relying solely on data-specific metrics. Our research proposes a new approach to calculate the degree of difficulty based on a model-centric perspective. By organizing the training dataset according to difficulty using our new metric, we aim to improve the model's performance compared to random shuffling.\n\n3 Methods for Quantitative Difficulty Measurement\n\nIn this section, we introduce a novel methodology for training models that begins with easier tasks and methodically progresses to more challenging ones. This approach requires organizing data by its level of complexity, for which we have established three principal criteria. By arranging the data in an order that goes from less to more difficult, this approach establishes a structured progression for learning.\n\n## Loss\nA high loss signifies a substantial discrepancy between the predicted and actual outcomes, indicating that the model perceives the data as challenging.\n\nWe establish a direct correlation between high loss values and increased difficulty. Therefore, prompts that generate higher loss are considered more challenging for the model to learn. This method computes the cross-entropy loss associated with the answer segment of the prompt."], "score": 0.88037109375}, {"id": "(Ankner et al., 2024)", "paper": {"corpus_id": 270199394, "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Zachary Ankner", "authorId": "2172356226"}, {"name": "Cody Blakeney", "authorId": "73066558"}, {"name": "Kartik K. Sreenivasan", "authorId": "34824875"}, {"name": "Max Marion", "authorId": "2304322161"}, {"name": "Matthew L. Leavitt", "authorId": "2028252288"}, {"name": "Mansheej Paul", "authorId": "1690452"}], "n_citations": 34}, "snippets": ["Marion et al. (2023) investigate data pruning based on multiple neural heuristics of sample difficulty, ultimately concluding that the perplexity of a sample under a reference language model is the best pruning metric."], "score": 0.775390625}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277398866, "title": "Distilling Wisdom: A Review on Optimizing Learning From Massive Language Models", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Dingzong Zhang", "authorId": "2326763710"}, {"name": "Devi Listiyani", "authorId": "2345374431"}, {"name": "Priyanka Singh", "authorId": "2302811064"}, {"name": "Manoranjan Mohanty", "authorId": "2309099465"}], "n_citations": 0}, "snippets": ["For instance, if the student model performs well on easier samples but struggles with harder ones, the distillation process will emphasize these harder samples, providing more detailed and nuanced knowledge transfer from the teacher model. This targeted training helps in improving the student model's performance more efficiently compared to uniform distillation methods. Adaptive KD can be implemented using various techniques, such as adjusting the weight of each sample's loss based on its difficulty or using an adaptive temperature in the softmax function to control the smoothness of the teacher model's predictions."], "score": 0.69091796875}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 51606954, "title": "Curriculum Learning for Natural Answer Generation", "year": 2018, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Cao Liu", "authorId": "49046540"}, {"name": "Shizhu He", "authorId": "1954845"}, {"name": "Kang Liu", "authorId": "2200096"}, {"name": "Jun Zhao", "authorId": "1390572170"}], "n_citations": 85}, "snippets": ["By reason of being able to obtain natural language responses, natural answers are more favored in real-world Question Answering (QA) systems. Generative models learn to automatically generate natural answers from large-scale question answer pairs (QA-pairs). However, they are suffering from the uncontrollable and uneven quality of QA-pairs crawled from the Internet. To address this problem, we propose a curriculum learning based framework for natural answer generation (CL-NAG), which is able to take full advantage of the valuable learning data from a noisy and uneven-quality corpus. Specifically, we employ two practical measures to automatically measure the quality (complexity) of QA-pairs. Based on the measurements, CL-NAG firstly utilizes simple and low-quality QA-pairs to learn a basic model, and then gradually learns to produce better answers with richer contents and more complete syntaxes based on more complex and higher-quality QA-pairs. In this way, all valuable information in the noisy and uneven-quality corpus could be fully exploited. Experiments demonstrate that CL-NAG outperforms the state-of-the-arts, which increases 6.8% and 8.7% in the accuracy for simple and complex questions, respectively."], "score": 0.0}], "table": null}], "cost": 0.562737}}
