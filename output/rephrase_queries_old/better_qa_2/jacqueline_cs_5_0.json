{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in methods for improving the calibration of language models' confidence scores, such as multicalibration or softmax-based estimation?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in methods for improving the calibration of language models' confidence scores, including approaches like multicalibration, softmax-based estimation, and related techniques."}}, {"clarifying_question2": "Do you want to understand the challenges that arise in maintaining calibration, especially after alignment techniques like RLHF, or across various domains?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in understanding both: the challenges in maintaining calibration after applying alignment techniques like RLHF, as well as the issues with calibration across various domains or problem slices."}}, {"clarifying_question3": "Are you seeking evaluation metrics and practical approaches to measure how well a language model's confidence aligns with actual correctness, such as using calibration curves or the area under the curve (AUC)?", "clarifying_answer3": {"clarifying_answer": "Yes, I am seeking evaluation metrics and practical approaches to measure how well a language model's confidence aligns with actual correctness, including calibration curves and AUC."}}], "better_query": "What are the most effective methods for improving the calibration of language model confidence scores, such as multicalibration or softmax-based estimation, and how do these techniques compare in practice?", "better_answer": {"sections": [{"title": "Background and Importance of Calibration", "tldr": "Calibration in language models refers to how well a model's confidence scores correlate with its actual likelihood of being correct. Well-calibrated models are essential for building trustworthy AI systems that can accurately indicate when they are uncertain about their outputs. (9 sources)", "text": "\nCalibration has a long history in probability theory and statistics, predating modern machine learning approaches <Paper corpusId=\"109884250\" paperTitle=\"(Degroot et al., 1983)\" isShortName></Paper> <Paper corpusId=\"122150550\" paperTitle=\"(Murphy et al., 1984)\" isShortName></Paper> <Paper corpusId=\"127203550\" paperTitle=\"(Murphy et al., 1977)\" isShortName></Paper>. In the context of language models (LMs), calibration refers to the alignment between a model's confidence scores and the probability that its predictions are correct <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. A perfectly calibrated confidence measure should satisfy the condition that the expected correctness conditioned on a particular confidence score equals that confidence score <Paper corpusId=\"277150701\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>.\n\nThe importance of calibration has grown with the increasing deployment of language models across various applications. As Jiang et al. note, \"How can we know when language models know, with confidence, the answer to a particular query?\" <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. This question highlights a fundamental issue: despite impressive capabilities, no model is perfect, and they often fail to provide appropriate answers. Poorly calibrated models may be overconfident in incorrect predictions or underconfident in correct ones, potentially leading to unreliable system behavior.\n\nWell-calibrated confidence scores are increasingly recognized as essential for building trustworthy AI systems <Paper corpusId=\"268032879\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"252917981\" paperTitle=\"(Si et al., 2022)\" isShortName></Paper>. Properly calibrated models can:\n\n1. Enable more reliable decision-making in real-world applications\n2. Facilitate better human-AI collaboration by indicating when a model is uncertain\n3. Help identify when a model should defer to human experts or additional information sources\n4. Improve safety and robustness in deployment scenarios\n\nThe evaluation of calibration quality typically relies on metrics such as Expected Calibration Error (ECE), which measures the discrepancy between confidence scores and empirical accuracy <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277150701\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Notably, recent research has shown that while pre-trained language models can be reasonably well-calibrated for in-domain tasks, their calibration often deteriorates when faced with out-of-domain examples or after fine-tuning processes like reinforcement learning from human feedback (RLHF) <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper> <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.\n\nGiven these challenges, researchers have developed various methods to improve calibration, ranging from fine-tuning approaches that modify model parameters to post-hoc techniques that adjust confidence scores without changing the underlying model <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These calibration methods will be explored in detail in subsequent sections.", "citations": [{"id": "(Degroot et al., 1983)", "paper": {"corpus_id": 109884250, "title": "The Comparison and Evaluation of Forecasters.", "year": 1983, "venue": "", "authors": [{"name": "M. Degroot", "authorId": "39416713"}, {"name": "S. Fienberg", "authorId": "1684961"}], "n_citations": 745}, "snippets": ["In this paper we present methods for comparing and evaluating forecasters whose predictions are presented as their subjective probability distributions of various random variables that will be observed in the future, e.g. weather forecasters who each day must specify their own probabilities that it will rain in a particular location. We begin by reviewing the concepts of calibration and refinement, and describiing the relationship between this notion of refinement and the notion of sufficiency in the comparison of statistical experiments. We also consider the question of interrelationships among forecasters and discuss methods by which an observer should combine the predictions from two or more different forecasters. Then we turn our attention to the concept of a proper scoring rule for evaluating forecasters, relating it to the concepts of calibration and refinement. Finally, we discuss conditions under which one fore- caster can exploit the predictions of another forecaster to obtain a better score. In this paper we describe some concepts and methods appropriate for evaluating and com- paring forecasters who repeatedly present their predictions of whether or not various events will occur in terms of their subjective probabilities of those events. The ideas we describe here are relevant in almost any situation in which forecasters must repeatedly make such probabilistic predictions, regardless of the particular subject matter or substantive area of the events being forecast. The forecaster might be an economist who at the beginning of each quarterly period makes predictions about unemployment, the rate of inflation, or Gross National Product in that quarter based on the values of various economic indicators; the forecaster might even make predictions using a large-scale econometric model of the United States economy based on hundreds of variables and econometric relations. In a different field, the forecaster might be the weatherman for a television station who at the beginning of each day must announce his probability that it will rain during the day. For ease of exposition, we present our discussions here in the context of such a weather forecaster who day after day must specify his subjective probability x that there will be at least a certain amount of rain at some given location during a specified time interval of the day. We refer to the occurrence of this well-specified event simply as \"rain\". Thus, at the begin- ning of each day the forecaster must specify his probability of rain and at the end of each day he observes whether or not rain actually occurred. The probability x specified by the forecaster on any particular day is called his prediction for that day. We shall make the realistic, and simultaneously simplifying, assumption that the"], "score": 0.0}, {"id": "(Murphy et al., 1984)", "paper": {"corpus_id": 122150550, "title": "Probability Forecasting in Meteorology", "year": 1984, "venue": "", "authors": [{"name": "A. H. Murphy", "authorId": "32620901"}, {"name": "R. L. Winkler", "authorId": "1830895"}], "n_citations": 338}, "snippets": ["Abstract Efforts to quantify the uncertainty in weather forecasts began more than 75 years ago, and many studies and experiments involving objective and subjective probability forecasting have been conducted in meteorology in the intervening period. Moreover, the U.S. National Weather Service (NWS) initiated a nationwide program in 1965 in which precipitation probability forecasts were formulated on an operational basis and routinely disseminated to the general public. In addition, the NWS now prepares objective probability forecasts for many variables, using statistical procedures. Hence probability forecasting in meteorology is unique in that very large sets of probability forecasts that have been subjected to detailed evaluation are available. This article has four objectives: (a) to review the history of probability forecasting in meteorology to acquaint statisticians with this body of literature; (b) to describe recent methodological, experimental, and operational activities in this field; (c) to exa"], "score": 0.0}, {"id": "(Murphy et al., 1977)", "paper": {"corpus_id": 127203550, "title": "Reliability of Subjective Probability Forecasts of Precipitation and Temperature", "year": 1977, "venue": "", "authors": [{"name": "A. H. Murphy", "authorId": "32620901"}, {"name": "R. L. Winkler", "authorId": "1830895"}], "n_citations": 387}, "snippets": ["SUMMARY This paper briefly describes some results of operational and experimental programmes in the United States involving subjective probability forecasts of precipitation occurrence and of maximum and minimum temperatures. These results indicate that weather forecasters can formulate such forecasts in a reliable manner."], "score": 0.0}, {"id": "(Jiang et al., 2020)", "paper": {"corpus_id": 235078802, "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Zhengbao Jiang", "authorId": "2669515"}, {"name": "J. Araki", "authorId": "50007145"}, {"name": "Haibo Ding", "authorId": "47929135"}, {"name": "Graham Neubig", "authorId": "1700325"}], "n_citations": 436}, "snippets": ["Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."], "score": 0.95947265625}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 277150701, "title": "Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoou Liu", "authorId": "2346334761"}, {"name": "Tiejin Chen", "authorId": "2278853459"}, {"name": "Longchao Da", "authorId": "1387492282"}, {"name": "Chacha Chen", "authorId": "150946029"}, {"name": "Zhen Lin", "authorId": "2351350698"}, {"name": "Hua Wei", "authorId": "2277576540"}], "n_citations": 8}, "snippets": ["Calibration refers to closing the gap between the confidence score and the expected correctness conditioned on confidence score. It has a long history preceding LLM or even modern machine learning (Degroot et al., 1983)(Murphy et al., 1977)(Murphy et al., 1984), but bears slightly different meanings in NLP. In general, we could define a perfectly calibrated confidence measure to achieve the following: \n\nwhere the expectation is taken over the joint distribution of x and generation s. A lot of papers focus on evaluating the calibration quality of specific LMs and tasks (Khojah et al., 2022)63,(Wang et al., 2020). Evaluation typically relies on variants of Expected Calibration Error (ECE) [63,71,(Tian et al., 2023). Oftentimes confidence scores from classification could be directly applied (Jiang et al., 2020)(Stengel-Eskin et al., 2022)(Zhao et al., 2021) in order to evaluate whether an LM is overor under-confident, especially for de facto classification tasks like sentiment analysis or multiple-choice QA. \n\nAs uncertainty and confidence are often intertwined, many approaches used in uncertainty quantification have their counterpart in confidence estimation. For example, for black-box methods, Lin et al. (Lin et al., 2023) computes a similarity matrix of sampled responses and derives confidence estimates for each generation via its degree or distance derived from the graph Laplacian, before using these scores to compute uncertainty. Zhang et al. (Zhang et al., 2024) extends such black-box methods to longer generations. For logit-based methods, Malinin and Gales (Malinin et al., 2021) normalize Eq. ( 1) with the length of s. Further improvements include replacing the logit-sum or mean with weighted sum, by attention values on the important tokens [78] or by importance inferred from natural language inference (NLI) models (Duan et al., 2023). Such variants of sequence likelihood could then be fed for (entropy-style) uncertainty computation (Kuhn et al., 2023)[78]. \n\nAnother popular approach is asking the LM itself whether a particular free-form generation is correct [59]. However, this formulation also poses a restriction on the confidence estimation method, as it is essentially a scalar logit. Thus, many extensions focus on applying calibration methods from classification to calibrate such self-evaluation."], "score": 0.8603515625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 268032879, "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Xinran Zhao", "authorId": "1500662261"}, {"name": "Hongming Zhang", "authorId": "2279761345"}, {"name": "Xiaoman Pan", "authorId": "2243367575"}, {"name": "Wenlin Yao", "authorId": "2087264100"}, {"name": "Dong Yu", "authorId": "2256336899"}, {"name": "Tongshuang Wu", "authorId": "2287820683"}, {"name": "Jianshu Chen", "authorId": "2108276402"}], "n_citations": 7}, "snippets": ["Confidence Calibration of LLMs. Extracting honest and constructive confidence scores of large language models is considered an important step towards building faithful and trustworthy AI systems (Desai et al., 2020)(Si et al., 2022). Many methods have been proposed recently to get reliable confidence scores with different suffix prompts added after outputting possible answers, such as a follow of True or False multiple choice question (Kadavath et al., 2022), asking models to describe the likelihood of the answer (Lin et al., 2022), and describing the likelihood that the answer is correct with demonstrations (Tian et al., 2023). However, it remains unclear how robust the methods are and how good they are comparatively. Our paper proposes FaR prompting as an orthogonal method to improve calibration and compare different extraction methods with our test bed."], "score": 0.91015625}, {"id": "(Si et al., 2022)", "paper": {"corpus_id": 252917981, "title": "Prompting GPT-3 To Be Reliable", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Zhe Gan", "authorId": "144702900"}, {"name": "Zhengyuan Yang", "authorId": "2149231840"}, {"name": "Shuohang Wang", "authorId": "2992833"}, {"name": "Jianfeng Wang", "authorId": "2124948371"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}, {"name": "Lijuan Wang", "authorId": "29957038"}], "n_citations": 302}, "snippets": ["Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3."], "score": 0.0}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Katherine Tian", "authorId": "2218407338"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Allan Zhou", "authorId": "2064472884"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Huaxiu Yao", "authorId": "18307037"}, {"name": "Chelsea Finn", "authorId": "46881670"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 355}, "snippets": ["Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)", "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."], "score": 0.89501953125}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}], "table": null}, {"title": "Softmax-based Calibration Methods", "tldr": "Softmax-based calibration methods directly manipulate the probability distribution produced by language models to achieve better alignment between confidence scores and actual correctness. Temperature scaling is particularly effective, requiring only a single parameter adjustment to significantly improve calibration across various tasks. (8 sources)", "text": "\nSoftmax-based methods represent one of the most widely used approaches for calibrating language model confidence scores. These techniques typically focus on adjusting the softmax function that converts raw model logits into probability distributions. The most prominent among these is temperature scaling, which involves dividing logits by a temperature parameter T before applying the softmax function <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper> <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This simple yet effective approach has been shown to significantly improve calibration across various language tasks without changing the relative ranking of predictions.\n\nTemperature scaling is particularly appealing because it requires optimizing only a single parameter, which can be learned by maximizing log-likelihood on a validation set while keeping the other model parameters fixed <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. Empirical studies have demonstrated that optimal temperature values typically range from 1.5 to 4.0, with a default value of 2.0 often serving as a robust choice across different datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>. Researchers have observed that language models tend to be overconfident in their predictions, with most confidence scores clustered at extremely high or low values. Temperature scaling effectively redistributes these scores to better reflect actual correctness probabilities <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\nAnother softmax-based approach is label smoothing, which penalizes low entropy distributions by assigning a fixed probability to the true label and distributing the remaining probability mass uniformly across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>. This technique has proven competitive with temperature scaling for calibration <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, particularly in out-of-domain settings <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nRecent advancements in softmax-based calibration include the \"invert softmax trick,\" which approximates logits by inverting verbalized probabilities to address issues arising from the scaling of these probabilities <Paper corpusId=\"273228940\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Another innovative approach transforms the multiclass calibration problem into calibrating a single surrogate binary classifier, enabling more efficient use of standard calibration methods <Paper corpusId=\"273821339\" paperTitle=\"(LeCoz et al., 2024)\" isShortName></Paper>.\n\nFor multiple-choice questions, fine-tuning language models with softmax-based or margin-based objective functions that explicitly compare and adjust the probabilities of candidate answers has been shown to improve expected calibration error (ECE) <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These methods can be combined with other techniques, such as using temperature scaling after label smoothing or applying few-shot learning with label smoothing, to further enhance calibration performance <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 253098773, "title": "On the Calibration of Massively Multilingual Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS)."], "score": 0.875}, {"id": "(Tarride et al., 2024)", "paper": {"corpus_id": 269449363, "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library", "year": 2024, "venue": "IEEE International Conference on Document Analysis and Recognition", "authors": [{"name": "Sol\u00e8ne Tarride", "authorId": "116700157"}, {"name": "Yoann Schneider", "authorId": "2298900821"}, {"name": "Marie Generali-Lince", "authorId": "2298903215"}, {"name": "M\u00e9lodie Boillet", "authorId": "1380222130"}, {"name": "Bastien Abadie", "authorId": "1491425010"}, {"name": "Christopher Kermorvant", "authorId": "2285800930"}], "n_citations": 4}, "snippets": ["The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets", "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."], "score": 0.97314453125}, {"id": "(Jiang et al., 2020)", "paper": {"corpus_id": 235078802, "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Zhengbao Jiang", "authorId": "2669515"}, {"name": "J. Araki", "authorId": "50007145"}, {"name": "Haibo Ding", "authorId": "47929135"}, {"name": "Graham Neubig", "authorId": "1700325"}], "n_citations": 436}, "snippets": ["Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."], "score": 0.95947265625}, {"id": "(Muller et al., 2019)", "paper": {"corpus_id": 174802983, "title": "When Does Label Smoothing Help?", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael M\u00fcller", "authorId": "2114054259"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 1954}, "snippets": ["The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."], "score": 0.0}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273228940, "title": "Calibrating Verbalized Probabilities for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cheng Wang", "authorId": "2214132154"}, {"name": "Gyuri Szarvas", "authorId": "72270481"}, {"name": "Georges Balazs", "authorId": "2325041176"}, {"name": "Pavel Danchenko", "authorId": "70407105"}, {"name": "Patrick Ernst", "authorId": "2325095919"}], "n_citations": 0}, "snippets": ["Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities."], "score": 0.923828125}, {"id": "(LeCoz et al., 2024)", "paper": {"corpus_id": 273821339, "title": "Confidence Calibration of Classifiers with Many Classes", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Adrien LeCoz", "authorId": "2329178942"}, {"name": "St\u00e9phane Herbin", "authorId": "2300096291"}, {"name": "Faouzi Adjed", "authorId": "7167973"}], "n_citations": 1}, "snippets": ["To address this issue, we transform the problem of calibrating a multiclass classifier into calibrating a single surrogate binary classifier. This approach allows for more efficient use of standard calibration methods. We evaluate our approach on numerous neural networks used for image or text classification and show that it significantly enhances existing calibration methods."], "score": 0.92236328125}], "table": null}, {"title": "Post-hoc Calibration Techniques", "tldr": "Post-hoc calibration techniques improve model confidence estimates without modifying the underlying language model. These methods include temperature scaling, histogram binning, isotonic regression, feature-based decision trees, and multi-hypothesis generation, which effectively redistribute confidence scores to better align with actual prediction accuracy. (10 sources)", "text": "\nPost-hoc calibration methods are particularly valuable because they can be applied to pre-trained language models without requiring expensive retraining or access to model internals. These techniques focus on manipulating the confidence values or inputs while keeping the underlying model fixed <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\n## Temperature Scaling\n\nTemperature scaling remains one of the most widely used post-hoc calibration methods, where logits are divided by a temperature parameter before applying the softmax function. This simple approach has proven remarkably effective at calibrating predictions across various tasks <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>. Recent innovations include Adaptive Temperature Scaling (ATS), which predicts a temperature scaling parameter for each token prediction rather than using a single global parameter. ATS has been shown to improve calibration by 10-50% across several natural language benchmarks, particularly addressing calibration shifts that occur after RLHF fine-tuning <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\n## Histogram-Based Methods\n\n* **Histogram Binning**: This method adjusts confidence values within different intervals by assessing the correctness of predictions in each bin, providing a simple calibration approach, though potentially limited when confidence distributions are uneven <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\n* **Isotonic Regression**: This technique seeks an optimal monotonically increasing function to map model confidence to correctness while preserving the relative ranking of predictions <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\n## Feature-Based Approaches\n\n* **Feature-Based Decision Trees**: These methods incorporate not only confidence scores but also input-related features to produce calibrated confidence estimates. Research has shown that post-processing raw confidence based on additional features is effective across various tasks, including structured prediction and natural language inference <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\n* **Confidence Estimation Models (CEM)**: Model-based confidence estimators trained on pseudo transcriptions can significantly improve calibration for out-of-domain (OOD) data, especially when trained with additional features from OOD language models <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n## Multi-Hypothesis Generation\n\nGenerating and evaluating multiple hypotheses before providing confidence scores has been shown to significantly improve calibration of verbalized probabilities. When combined with temperature scaling, this approach can reduce expected calibration error (ECE) by over 50% for models like ChatGPT, GPT-4, and Claude 2 across multiple datasets <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper> <Paper corpusId=\"24047702\" paperTitle=\"(Lord et al., 1984)\" isShortName></Paper>.\n\n## Data Augmentation and Ensemble Methods\n\nWhile data augmentation and model ensemble approaches have been successful in computer vision, they have shown limited benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. Nevertheless, exploring diverse expressions through paraphrasing has demonstrated significant improvements in confidence estimation, particularly for multiple-choice datasets <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>.\n\n## Multicalibration\n\nMulticalibration methods ensure not only overall alignment between confidence scores and correctness but also calibration across multiple subgroups through an iterative alignment process <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. This approach is particularly valuable for ensuring fair calibration across different demographic or linguistic groups <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>.\n\nPost-hoc calibration techniques are often preferred for black-box language models where internal model access or fine-tuning is not possible. These methods can successfully re-scale confidence from extremely high or low values to more reasonable ranges, thus improving overall calibration performance <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Jiang et al., 2020)", "paper": {"corpus_id": 235078802, "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Zhengbao Jiang", "authorId": "2669515"}, {"name": "J. Araki", "authorId": "50007145"}, {"name": "Haibo Ding", "authorId": "47929135"}, {"name": "Graham Neubig", "authorId": "1700325"}], "n_citations": 436}, "snippets": ["Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."], "score": 0.95947265625}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 272987064, "title": "Calibrating Language Models with Adaptive Temperature Scaling", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Johnathan Xie", "authorId": "2285068366"}, {"name": "Annie S. Chen", "authorId": "2111073657"}, {"name": "Yoonho Lee", "authorId": "2260289273"}, {"name": "Eric Mitchell", "authorId": "2260106796"}, {"name": "Chelsea Finn", "authorId": "2279838067"}], "n_citations": 17}, "snippets": ["In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."], "score": 0.93994140625}, {"id": "(Xie et al._1, 2024)", "paper": {"corpus_id": 274789494, "title": "A Survey of Calibration Process for Black-Box LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liangru Xie", "authorId": "2336030822"}, {"name": "Hui Liu", "authorId": "2326538488"}, {"name": "Jingying Zeng", "authorId": "2276423058"}, {"name": "Xianfeng Tang", "authorId": "2313288320"}, {"name": "Yan Han", "authorId": "2336079495"}, {"name": "Chen Luo", "authorId": "2305051640"}, {"name": "Jing Huang", "authorId": "2336079074"}, {"name": "Zhen Li", "authorId": "2313299851"}, {"name": "Suhang Wang", "authorId": "2257362558"}, {"name": "Qi He", "authorId": "2315065019"}], "n_citations": 4}, "snippets": ["Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."], "score": 0.93408203125}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 238419596, "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition", "year": 2021, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Qiujia Li", "authorId": "1522347808"}, {"name": "Yu Zhang", "authorId": "2153632494"}, {"name": "David Qiu", "authorId": "150080713"}, {"name": "Yanzhang He", "authorId": "2145999837"}, {"name": "Liangliang Cao", "authorId": "48749954"}, {"name": "P. Woodland", "authorId": "1716393"}], "n_citations": 11}, "snippets": ["Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation", "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."], "score": 0.87939453125}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Katherine Tian", "authorId": "2218407338"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Allan Zhou", "authorId": "2064472884"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Huaxiu Yao", "authorId": "18307037"}, {"name": "Chelsea Finn", "authorId": "46881670"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 355}, "snippets": ["Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)", "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."], "score": 0.89501953125}, {"id": "(Lord et al., 1984)", "paper": {"corpus_id": 24047702, "title": "Considering the opposite: a corrective strategy for social judgment.", "year": 1984, "venue": "Journal of Personality and Social Psychology", "authors": [{"name": "C. G. Lord", "authorId": "6802871"}, {"name": "M. Lepper", "authorId": "81476483"}, {"name": "Elizabeth L. Preston", "authorId": "40299604"}], "n_citations": 663}, "snippets": ["It is proposed that several biases in social judgment result from a failure--first noted by Francis Bacon--to consider possibilities at odds with beliefs and perceptions of the moment. Individuals who are induced to consider the opposite, therefore, should display less bias in social judgment. In two separate but conceptually parallel experiments, this reasoning was applied to two domains--biased assimilation of new evidence on social issues and biased hypothesis testing of personality impressions. Subjects were induced to consider the opposite in two ways: through explicit instructions to do so and through stimulus materials that made opposite possibilities more salient. In both experiments the induction of a consider-the-opposite strategy had greater corrective effect than more demand-laden alternative instructions to be as fair and unbiased as possible. The results are viewed as consistent with previous research on perseverance, hindsight, and logical problem solving, and are thought to suggest an effective method of retraining social judgment."], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 259859030, "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yangyi Chen", "authorId": null}, {"name": "Xingyao Wang", "authorId": "2144803999"}, {"name": "Heng Ji", "authorId": "2072975661"}], "n_citations": 0}, "snippets": ["Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)", "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."], "score": 0.9130859375}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 267617073, "title": "Calibrating Long-form Generations from Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yukun Huang", "authorId": "2145952181"}, {"name": "Yixin Liu", "authorId": "2283783652"}, {"name": "Raghuveer Thirukovalluru", "authorId": "2283771339"}, {"name": "Arman Cohan", "authorId": "2266838179"}, {"name": "Bhuwan Dhingra", "authorId": "2060730422"}], "n_citations": 12}, "snippets": ["Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023)."], "score": 0.9609375}], "table": null}, {"title": "Training-based Calibration Methods", "tldr": "Training-based calibration methods improve confidence score alignment by directly modifying language model parameters during the training or fine-tuning process. These approaches include label smoothing, parameter-efficient tuning, and specialized objective functions that preserve pre-trained features while optimizing for calibration. (10 sources)", "text": "\nTraining-based calibration methods involve directly modifying language model parameters to achieve better calibration, as opposed to post-hoc techniques that adjust confidence scores after training. These approaches integrate calibration goals directly into the training or fine-tuning process.\n\nLabel smoothing is one of the most effective training-based calibration techniques. This regularization method penalizes low entropy distributions by assigning a fixed probability to the true label and distributing the remaining probability mass uniformly across other classes <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. Studies have demonstrated that label smoothing is competitive with temperature scaling for calibration <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, especially in out-of-domain settings <Paper corpusId=\"212747810\" paperTitle=\"(Desai et al., 2020)\" isShortName></Paper>.\n\nFor multiple-choice questions, fine-tuning with specialized objective functions has proven effective. Both softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probabilities of candidate answers, can significantly improve expected calibration error (ECE) on multiple-choice datasets <Paper corpusId=\"235078802\" paperTitle=\"(Jiang et al., 2020)\" isShortName></Paper>. These objective functions are particularly well-suited for questions with potential candidate answers.\n\nA promising recent finding suggests that preserving pre-trained features helps calibrate fine-tuned language models. Several approaches achieve this goal:\n\n1. **Parameter-efficient tuning** methods, which were originally designed to improve performance rather than calibration, have been shown to outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper> <Paper corpusId=\"59599816\" paperTitle=\"(Houlsby et al., 2019)\" isShortName></Paper>.\n\n2. **Pre-trained weight decay** and **Mixout** <Paper corpusId=\"202750126\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>, which regularizes learning to minimize deviation from pre-trained weights, also help maintain better calibration <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\n3. **Masked Language Modeling (MLM) objectives** can be incorporated during fine-tuning to maintain consistency between pre-trained and fine-tuned models, achieving the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both in-domain and out-of-domain settings <Paper corpusId=\"258967945\" paperTitle=\"(He et al., 2023)\" isShortName></Paper>.\n\nFew-shot learning combined with calibration techniques offers another approach. Fine-tuning on a small number of examples in a target language, in addition to data in the source language, can improve calibration by reducing domain shift <Paper corpusId=\"253098773\" paperTitle=\"(Ahuja et al., 2022)\" isShortName></Paper> <Paper corpusId=\"226262344\" paperTitle=\"(Lauscher et al., 2020)\" isShortName></Paper>.\n\nFor out-of-domain scenarios, training confidence estimation models (CEMs) using pseudo transcriptions has proven effective. Including additional features from out-of-domain language models can significantly improve confidence scores on out-of-domain datasets <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nUnlike in computer vision, data augmentation and model ensemble approaches have not shown the same benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. However, an emerging trend involves directly collecting data to train models for reasonable confidence estimations, applying multi-layer perceptrons on the last hidden states of pre-trained language models or training them to verbalize their confidence levels <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Ahuja et al., 2022)", "paper": {"corpus_id": 253098773, "title": "On the Calibration of Massively Multilingual Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "3010457"}, {"name": "Sandipan Dandapat", "authorId": "34725175"}, {"name": "M. Choudhury", "authorId": "143990839"}], "n_citations": 16}, "snippets": ["We briefly review some commonly used methods for calibrating neural network based classifiers. 1. Temperature Scaling (TS and Self-TS) (Guo et al., 2017) is applied by scaling the output logits using a temperature parameter T before applying softmax i.e. : \n\n, where o k denotes the logits corresponding to the k th class. T is a learnable parameter obtained posttraining by maximizing the log-likelihood on the dev set while keeping other network parameters fixed. We experiment with two settings for improving calibration on a target language: using dev data in English to perform temperature scaling (TS) and using the target language's dev data (Self-TS). 2. Label Smoothing (LS) (Pereyra et al., 2017) is a regularization technique that penalizes low entropy distributions by using soft labels that are obtained by assigning a fixed probability q = 1 \u2212 \u03b1 to the true label (0 < \u03b1 < 1), and distributing the remaining probability mass uniformly across the remaining classes. Label smoothing has been empirically shown to be competitive with temperature scaling for calibration (M\u00fcller et al., 2019) especially in out of domain settings (Desai et al., 2020) 3. Few-Shot Learning (FSL) We also investigate if fine-tuning the MMLM on a few examples in a target language in addition to the data in English, leads to any improvement in calibration as it does in the performance (Lauscher et al., 2020). Since these models are expected to be calibrated worse for out-of-domain data compared to in-domain data (Desai et al., 2020), we try to improve calibration by reducing the domain shift through fewshot learning. \n\nApart from these, we also consider combinations of different calibration methods in our experiments, including Label Smoothing with Temperature Scaling (TS + LS or Self-TS + LS) and Few-Shot Learning with Label Smoothing (FSL + LS)."], "score": 0.875}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 259859030, "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yangyi Chen", "authorId": null}, {"name": "Xingyao Wang", "authorId": "2144803999"}, {"name": "Heng Ji", "authorId": "2072975661"}], "n_citations": 0}, "snippets": ["Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)", "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."], "score": 0.9130859375}, {"id": "(Muller et al., 2019)", "paper": {"corpus_id": 174802983, "title": "When Does Label Smoothing Help?", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael M\u00fcller", "authorId": "2114054259"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 1954}, "snippets": ["The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."], "score": 0.0}, {"id": "(Desai et al., 2020)", "paper": {"corpus_id": 212747810, "title": "Calibration of Pre-trained Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Shrey Desai", "authorId": "120777041"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 301}, "snippets": ["Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated. Specifically, do these models' posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example? We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning. For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about. We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain."], "score": 0.0}, {"id": "(Jiang et al., 2020)", "paper": {"corpus_id": 235078802, "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering", "year": 2020, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Zhengbao Jiang", "authorId": "2669515"}, {"name": "J. Araki", "authorId": "50007145"}, {"name": "Haibo Ding", "authorId": "47929135"}, {"name": "Graham Neubig", "authorId": "1700325"}], "n_citations": 436}, "snippets": ["Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \"How can we know when language models know, with confidence, the answer to a particular query?\" We examine this question from the point of view of calibration, the property of a probabilistic model's predicted probabilities actually being well correlated with the probabilities of correctness.\n\nTo alleviate this problem, we propose methods to make LMs' confidence scores correlate better with the likelihood of model prediction being correct. We examined both fine-tuning methods that modify LMs' parameters and post-hoc methods that keep LMs fixed and only manipulate the confidence values or inputs. Specifically, we fine-tune the LM using softmax-or marginbased objective functions based on multiple candidate answers. For post-hoc calibration, we examined temperature-based scaling and feature-based decision trees that take prediction probability and input-related features as input and produce calibrated confidence (Jagannatha and Yu, 2020;Desai and Durrett, 2020;Kamath et al., 2020). We also study the sensitivity of LMs' confidence estimation with respect to language variation by paraphrasing candidate answers and augmenting questions using retrieved context.\n\nBoth softmax-based and margin-based fine-tuning, which explicitly compare and adjust the probability of candidate answers, can further improve ECE on multiplechoice datasets. We argue that the softmax-based and margin-based objective functions are better suited for questions with potential candidates.\n\nPost-processing the raw confidence either solely based on confidence or other features is effective across all datasets, which is consistent with the conclusion on other tasks such as structured prediction and natural language inference (Jagannatha and Yu, 2020; Desai and Durrett, 2020). We demonstrate the histogram of confidence before and after applying temperature-based scaling or feature-based decision trees in Figure 2. LMs tend to be over-confident, with most predictions having either extremely high or low confidence. Both methods can successfully re-scale the confidence to reasonable ranges, thus improving the calibration performance.\n\nThe improvement brought by using paraphrasing is significant on multiple-choice datasets, demonstrating that using diverse expressions can indeed improve confidence estimation."], "score": 0.95947265625}, {"id": "(He et al., 2023)", "paper": {"corpus_id": 258967945, "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guande He", "authorId": "2218509878"}, {"name": "Jianfei Chen", "authorId": "2276707"}, {"name": "Jun Zhu", "authorId": "2155220672"}], "n_citations": 22}, "snippets": ["Based on the observations, we hypothesize that preserving the pre-trained features helps calibrate the fine-tuned LMs. \n\nTo validate our hypothesis, we first evaluate the calibration of some previous methods that can preserve pre-trained features, including (1) Parameter-efficient tuning (Houlsby et al., 2019)Hu et al., 2021;Li & Liang, 2021), (2) Pre-trained weight decay, (3) Mixout (Lee et al., 2019). Although these methods were originally designed to improve the performance beyond uncertainty estimation, our experiment demonstrates that these methods outperform vanilla fine-tuning in terms of calibration, especially under out-of-domain settings. Based on our observation that the PLMs are well-calibrated on the MLM task yet the fine-tuned LMs that forget the pre-trained features struggle with overconfidence under domain shift, we propose a simple baseline that utilizes the MLM objective to maintain the consistency between the pre-trained and fine-tuned models. The proposed method achieves the lowest expected calibration error and competitive accuracy compared to existing calibration methods in both ID and OD settings on three NLU tasks, including natural language inference, paraphrase detection, and commonsense reasoning, showing that preserving the pre-trained features is an effective approach for improving the calibration of fine-tuned LMs."], "score": 0.85791015625}, {"id": "(Houlsby et al., 2019)", "paper": {"corpus_id": 59599816, "title": "Parameter-Efficient Transfer Learning for NLP", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "N. Houlsby", "authorId": "2815290"}, {"name": "A. Giurgiu", "authorId": "1911881"}, {"name": "Stanislaw Jastrzebski", "authorId": "40569328"}, {"name": "Bruna Morrone", "authorId": "68973833"}, {"name": "Quentin de Laroussilhe", "authorId": "51985388"}, {"name": "Andrea Gesmundo", "authorId": "2813347"}, {"name": "Mona Attariyan", "authorId": "2809991"}, {"name": "S. Gelly", "authorId": "1802148"}], "n_citations": 4518}, "snippets": ["Fine-tuning large pre-trained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter's effectiveness, we transfer the recently proposed BERT Transformer model to 26 diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within 0.4% of the performance of full fine-tuning, adding only 3.6% parameters per task. By contrast, fine-tuning trains 100% of the parameters per task."], "score": 0.0}, {"id": "(Lee et al., 2019)", "paper": {"corpus_id": 202750126, "title": "Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Cheolhyoung Lee", "authorId": "81275395"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Wanmo Kang", "authorId": "1742677"}], "n_citations": 209}, "snippets": ["In natural language processing, it has been observed recently that generalization could be greatly improved by finetuning a large-scale language model pretrained on a large unlabeled corpus. Despite its recent success and wide adoption, finetuning a large pretrained language model on a downstream task is prone to degenerate performance when there are only a small number of training instances available. In this paper, we introduce a new regularization technique, to which we refer as \"mixout\", motivated by dropout. Mixout stochastically mixes the parameters of two models. We show that our mixout technique regularizes learning to minimize the deviation from one of the two models and that the strength of regularization adapts along the optimization trajectory. We empirically evaluate the proposed mixout and its variants on finetuning a pretrained language model on downstream tasks. More specifically, we demonstrate that the stability of finetuning and the average accuracy greatly increase when we use the proposed approach to regularize finetuning of BERT on downstream tasks in GLUE."], "score": 0.0}, {"id": "(Lauscher et al., 2020)", "paper": {"corpus_id": 226262344, "title": "From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Anne Lauscher", "authorId": "29891652"}, {"name": "Vinit Ravishankar", "authorId": "24881798"}, {"name": "Ivan Vulic", "authorId": "1747849"}, {"name": "Goran Glavas", "authorId": "2472657"}], "n_citations": 315}, "snippets": ["Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions."], "score": 0.0}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 238419596, "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition", "year": 2021, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Qiujia Li", "authorId": "1522347808"}, {"name": "Yu Zhang", "authorId": "2153632494"}, {"name": "David Qiu", "authorId": "150080713"}, {"name": "Yanzhang He", "authorId": "2145999837"}, {"name": "Liangliang Cao", "authorId": "48749954"}, {"name": "P. Woodland", "authorId": "1716393"}], "n_citations": 11}, "snippets": ["Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation", "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."], "score": 0.87939453125}], "table": null}, {"title": "Multicalibration and Group-wise Calibration", "tldr": "Multicalibration techniques ensure language models maintain appropriate confidence levels across different subgroups or \"slices\" of data, rather than just in aggregate. These methods use techniques like clustering, self-annotation, and slice-specific recalibration to address issues where models might be well-calibrated overall but significantly miscalibrated for specific domains or groups. (6 sources)", "text": "\nTraditional calibration metrics often assess performance in aggregate, potentially masking significant miscalibration within specific subgroups or domains. As Li et al. note, \"while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate)\" <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This observation has motivated the development of group-wise and multicalibration techniques that ensure reliable confidence scores across diverse data segments.\n\nMulticalibration methods aim to simultaneously achieve calibration across various intersecting groupings of data, rather than just overall calibration. The foundational concept builds on earlier work in algorithmic fairness <Paper corpusId=\"12845273\" paperTitle=\"(Kleinberg et al., 2016)\" isShortName></Paper> and has been adapted to address the unique challenges of language models. Xie et al. explain that multicalibration \"ensures not only overall alignment but also calibration across multiple subgroups through iterative alignment\" <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>. This approach is particularly valuable for ensuring fair calibration across different demographic, linguistic, or domain-specific groups.\n\nSeveral innovative techniques have emerged for implementing multicalibration in language models. Detommaso et al. propose forming groupings for prompt/completion pairs that correlate with correctness probability through two primary methods:\n\n1. **Clustering within embedding space**: Grouping similar examples based on their representation in the model's embedding space.\n2. **Self-annotation**: Querying the language model itself with yes-or-no questions about the prompt to derive meaningful groupings <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.\n\nTheir work also introduces novel variants of multicalibration algorithms designed to reduce overfitting tendencies, demonstrating substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods <Paper corpusId=\"269004786\" paperTitle=\"(Detommaso et al., 2024)\" isShortName></Paper>.\n\nFor addressing domain-specific calibration issues, Li et al. propose a few-shot slice-specific recalibration framework. This approach trains a recalibration model that uses a few unlabeled examples from any given slice to predict a curve that remaps confidence scores specifically for that slice. Their method enables identification of domain-specific confidence thresholds, above which the language model's predictions can be trusted and below which it should abstain. Experiments show their few-shot recalibrator consistently outperforming existing calibration methods, improving calibration error for PaLM2-Large on MMLU by 16% compared to temperature scaling <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nSeveral researchers have noted the connection between group-wise calibration and fairness concerns. Traditional group-wise calibration uses predefined groupings, but faces challenges in balancing different fairness criteria <Paper corpusId=\"75455\" paperTitle=\"(Pleiss et al., 2017)\" isShortName></Paper>. In the context of language models, Manggala et al. highlight several complementary approaches:\n\n1. Training models to approximate precision-threshold curves for given groups using few-shot samples\n2. Training auxiliary models using group-specific accuracy as targets\n3. Extending multicalibration algorithms to ensure simultaneous calibration across various intersecting data groupings <Paper corpusId=\"273228151\" paperTitle=\"(Manggala et al., 2024)\" isShortName></Paper>\n\nThese group-wise calibration approaches are particularly important for applications where trust and fairness are paramount, as they help ensure that language model confidence scores are reliable across diverse domains, topics, and demographic groups.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."], "score": 0.966796875}, {"id": "(Kleinberg et al., 2016)", "paper": {"corpus_id": 12845273, "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores", "year": 2016, "venue": "Information Technology Convergence and Services", "authors": [{"name": "J. Kleinberg", "authorId": "3371403"}, {"name": "S. Mullainathan", "authorId": "2062143"}, {"name": "Manish Raghavan", "authorId": "38009222"}], "n_citations": 1776}, "snippets": ["Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them."], "score": 0.0}, {"id": "(Xie et al._1, 2024)", "paper": {"corpus_id": 274789494, "title": "A Survey of Calibration Process for Black-Box LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liangru Xie", "authorId": "2336030822"}, {"name": "Hui Liu", "authorId": "2326538488"}, {"name": "Jingying Zeng", "authorId": "2276423058"}, {"name": "Xianfeng Tang", "authorId": "2313288320"}, {"name": "Yan Han", "authorId": "2336079495"}, {"name": "Chen Luo", "authorId": "2305051640"}, {"name": "Jing Huang", "authorId": "2336079074"}, {"name": "Zhen Li", "authorId": "2313299851"}, {"name": "Suhang Wang", "authorId": "2257362558"}, {"name": "Qi He", "authorId": "2315065019"}], "n_citations": 4}, "snippets": ["Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."], "score": 0.93408203125}, {"id": "(Detommaso et al., 2024)", "paper": {"corpus_id": 269004786, "title": "Multicalibration for Confidence Scoring in LLMs", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Gianluca Detommaso", "authorId": "2295667267"}, {"name": "Martin Bertran", "authorId": "2295665717"}, {"name": "Riccardo Fogliato", "authorId": "2295664744"}, {"name": "Aaron Roth", "authorId": "2295665299"}], "n_citations": 19}, "snippets": ["We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and \"self-annotation\" - querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods."], "score": 0.970703125}, {"id": "(Pleiss et al., 2017)", "paper": {"corpus_id": 75455, "title": "On Fairness and Calibration", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Manish Raghavan", "authorId": "38009222"}, {"name": "Felix Wu", "authorId": "24277779"}, {"name": "J. Kleinberg", "authorId": "3371403"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 880}, "snippets": ["The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be \"fair.\" In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets."], "score": 0.0}, {"id": "(Manggala et al., 2024)", "paper": {"corpus_id": 273228151, "title": "QA-Calibration of Language Model Confidence Scores", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Putra Manggala", "authorId": "2799707"}, {"name": "Atalanti A. Mastakouri", "authorId": "1387253571"}, {"name": "Elke Kirschbaum", "authorId": "2319413371"}, {"name": "S. Kasiviswanathan", "authorId": "7993151"}, {"name": "Aaditya Ramdas", "authorId": "2556942"}], "n_citations": 3}, "snippets": ["Group Notions of Calibration. Previous works highlight the limitations of average-case calibration. Group-wise calibration, which uses predefined groupings (Kleinberg et al., 2016)(Pleiss et al., 2017), has been adapted for language models (LMs). Li et al. ( 2024) train a model that approximates the precision-threshold curve for a given group by using few-shot samples to predict the LM's empirical precision at various confidence thresholds. Ulmer et al. (2024) train an auxiliary model using accuracy per group as target to predict an LM's confidence based on textual input and output. Detommaso et al. (2024) achieves multicalibration -simultaneous calibration across various intersecting groupings of the data. Our work complements multicalibration, and our methods could extend to this by adapting Algorithm 3 in Detommaso et al. (2024). Luo et al. (2022) measure calibration over a set of similar predictions, quantified by a kernel function on feature space."], "score": 0.8896484375}], "table": null}, {"title": "Verbalization and Consistency-based Methods", "tldr": "Verbalization and consistency-based methods enable language models to express their uncertainty directly in natural language or through agreement across multiple generated responses. These approaches are becoming increasingly popular as they don't require access to model internals and can effectively improve calibration, especially when combined with other techniques like temperature scaling. (13 sources)", "text": "\nVerbalization and consistency-based methods represent emerging approaches to calibration that leverage language models' ability to express uncertainty in natural language and generate multiple response options. Unlike traditional calibration techniques that manipulate logits, these methods can be applied to black-box models without requiring access to internal model parameters <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\n## Verbalized Probabilities\n\nVerbalization methods directly prompt language models to express their confidence in natural language, such as stating \"90% confidence\" or \"high confidence\" for their answers <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>. Research shows these verbalized probabilities can be well-calibrated, with models capable of generalizing calibration under distribution shifts <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>. This approach is particularly valuable for commercial black-box models like OpenAI's GPT family, where internal model probabilities are inaccessible <Paper corpusId=\"252917981\" paperTitle=\"(Si et al., 2022)\" isShortName></Paper>.\n\nHowever, verbalized probabilities face challenges related to \"semantic equivalence\" \u2013 different sentences can express the same meaning, making uncertainty measurement more complex <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>. Additionally, verbalized probabilities often require rescaling, as models may express overconfidence. Wang et al. address this through an \"invert softmax trick\" that approximates logits by inverting verbalized probabilities <Paper corpusId=\"273228940\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\n## Consistency-based Methods\n\nConsistency-based approaches assess confidence by measuring agreement across multiple responses generated for the same question <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. This method leverages the principle that when a model consistently produces the same answer across different attempts, it's likely more confident in that answer.\n\nA particularly effective consistency technique is multi-hypothesis generation, where models produce several potential answers before providing a confidence score. Tian et al. demonstrate that generating and evaluating multiple hypotheses significantly improves calibration of verbalized probabilities across various language models, including ChatGPT, GPT-4, and Claude 2 <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This approach echoes earlier findings in human psychology where considering alternative possibilities reduces bias in judgment <Paper corpusId=\"24047702\" paperTitle=\"(Lord et al., 1984)\" isShortName></Paper>.\n\nSelf-consistency, which samples a diverse set of reasoning paths rather than only using greedy decoding, has also shown impressive improvements on reasoning tasks <Paper corpusId=\"247595263\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. Kuhn et al. introduce \"semantic entropy\" to address challenges in measuring uncertainty in natural language, creating an entropy measure that incorporates linguistic invariances created by shared meanings <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>.\n\n## Hybrid Approaches\n\nRecent research increasingly focuses on hybrid approaches that combine verbalization and consistency methods <Paper corpusId=\"267617073\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. For instance, Sun et al. evaluate different confidence estimation methods including softmax-based approaches, raw token scores, verbalized confidences, and combinations of these methods, further enhanced with a self-probing mechanism <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>.\n\nAn emerging trend involves directly collecting data to train models for reasonable confidence estimations. This includes applying multi-layer perceptrons on the last hidden states of pre-trained language models or training them to verbalize their confidence levels <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. These approaches have been successful in significantly reducing overconfidence issues in language models <Paper corpusId=\"249191391\" paperTitle=\"(Lin et al., 2022)\" isShortName></Paper>.\n\nUnlike data augmentation and model ensemble approaches that have shown success in computer vision <Paper corpusId=\"208637407\" paperTitle=\"(Hendrycks et al., 2019)\" isShortName></Paper> <Paper corpusId=\"6294674\" paperTitle=\"(Lakshminarayanan et al., 2016)\" isShortName></Paper>, these methods haven't demonstrated the same benefits for NLP tasks according to empirical studies <Paper corpusId=\"259859030\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. However, when verbalization and consistency approaches are combined with temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, they can reduce expected calibration error by over 50% compared to using model probabilities alone <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Huang et al., 2024)", "paper": {"corpus_id": 267617073, "title": "Calibrating Long-form Generations from Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yukun Huang", "authorId": "2145952181"}, {"name": "Yixin Liu", "authorId": "2283783652"}, {"name": "Raghuveer Thirukovalluru", "authorId": "2283771339"}, {"name": "Arman Cohan", "authorId": "2266838179"}, {"name": "Bhuwan Dhingra", "authorId": "2060730422"}], "n_citations": 12}, "snippets": ["Improving Calibration Traditional calibration methods focus on post-processing logits (Guo et al., 2017), but with LLMs generating unbounded text, logits could fall short. Thus, extracting better confidence scores (i.e., confidence elicitation) has become crucial for improving calibration. These include: verbalization, which directly asks the model to output its confidence (Lin et al., 2022), consistency, which uses the uniformity of multiple responses to gauge confidence (Kadavath et al., 2022, Kuhn et al., 2023, Cole et al., 2023, Chen et al., 2023c, Tian et al., 2023a, Lin et al., 2023), and the hybrid of both (Xiong et al., 2023, Tian et al., 2023b, Chen and Mueller, 2023)."], "score": 0.9609375}, {"id": "(Lin et al., 2022)", "paper": {"corpus_id": 249191391, "title": "Teaching Models to Express Their Uncertainty in Words", "year": 2022, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Stephanie C. Lin", "authorId": "48639938"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Owain Evans", "authorId": "47107786"}], "n_citations": 424}, "snippets": ["We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers."], "score": 0.0}, {"id": "(Si et al., 2022)", "paper": {"corpus_id": 252917981, "title": "Prompting GPT-3 To Be Reliable", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chenglei Si", "authorId": "152358188"}, {"name": "Zhe Gan", "authorId": "144702900"}, {"name": "Zhengyuan Yang", "authorId": "2149231840"}, {"name": "Shuohang Wang", "authorId": "2992833"}, {"name": "Jianfeng Wang", "authorId": "2124948371"}, {"name": "Jordan L. Boyd-Graber", "authorId": "1389036863"}, {"name": "Lijuan Wang", "authorId": "29957038"}], "n_citations": 302}, "snippets": ["Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3."], "score": 0.0}, {"id": "(Kuhn et al., 2023)", "paper": {"corpus_id": 257039062, "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Lorenz Kuhn", "authorId": "39879848"}, {"name": "Y. Gal", "authorId": "2681954"}, {"name": "Sebastian Farquhar", "authorId": "33859827"}], "n_citations": 310}, "snippets": ["We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273228940, "title": "Calibrating Verbalized Probabilities for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cheng Wang", "authorId": "2214132154"}, {"name": "Gyuri Szarvas", "authorId": "72270481"}, {"name": "Georges Balazs", "authorId": "2325041176"}, {"name": "Pavel Danchenko", "authorId": "70407105"}, {"name": "Patrick Ernst", "authorId": "2325095919"}], "n_citations": 0}, "snippets": ["Recent methods have demonstrated improved calibration by applying techniques like Platt scaling or temperature scaling to the confidence scores generated by LLMs. In this paper, we explore the calibration of verbalized probability distributions for discriminative tasks. First, we investigate the capability of LLMs to generate probability distributions over categorical labels. We theoretically and empirically identify the issue of re-softmax arising from the scaling of verbalized probabilities, and propose using the invert softmax trick to approximate the\"logit\"by inverting verbalized probabilities."], "score": 0.923828125}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Katherine Tian", "authorId": "2218407338"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Allan Zhou", "authorId": "2064472884"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Huaxiu Yao", "authorId": "18307037"}, {"name": "Chelsea Finn", "authorId": "46881670"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 355}, "snippets": ["Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)", "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."], "score": 0.89501953125}, {"id": "(Lord et al., 1984)", "paper": {"corpus_id": 24047702, "title": "Considering the opposite: a corrective strategy for social judgment.", "year": 1984, "venue": "Journal of Personality and Social Psychology", "authors": [{"name": "C. G. Lord", "authorId": "6802871"}, {"name": "M. Lepper", "authorId": "81476483"}, {"name": "Elizabeth L. Preston", "authorId": "40299604"}], "n_citations": 663}, "snippets": ["It is proposed that several biases in social judgment result from a failure--first noted by Francis Bacon--to consider possibilities at odds with beliefs and perceptions of the moment. Individuals who are induced to consider the opposite, therefore, should display less bias in social judgment. In two separate but conceptually parallel experiments, this reasoning was applied to two domains--biased assimilation of new evidence on social issues and biased hypothesis testing of personality impressions. Subjects were induced to consider the opposite in two ways: through explicit instructions to do so and through stimulus materials that made opposite possibilities more salient. In both experiments the induction of a consider-the-opposite strategy had greater corrective effect than more demand-laden alternative instructions to be as fair and unbiased as possible. The results are viewed as consistent with previous research on perseverance, hindsight, and logical problem solving, and are thought to suggest an effective method of retraining social judgment."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 247595263, "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "D. Schuurmans", "authorId": "50319359"}, {"name": "Quoc Le", "authorId": "1998340269"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "Denny Zhou", "authorId": "65855107"}], "n_citations": 3734}, "snippets": ["Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."], "score": 0.0}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272689376, "title": "Confidence Estimation For LLM-Based Dialogue State Tracking", "year": 2024, "venue": "Spoken Language Technology Workshop", "authors": [{"name": "Yi-Jyun Sun", "authorId": "2321493889"}, {"name": "Suvodip Dey", "authorId": "29832722"}, {"name": "Dilek Hakkani-Tur", "authorId": "2365041900"}, {"name": "Gokhan Tur", "authorId": "5108268"}], "n_citations": 1}, "snippets": ["We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models."], "score": 0.9814453125}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 259859030, "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yangyi Chen", "authorId": null}, {"name": "Xingyao Wang", "authorId": "2144803999"}, {"name": "Heng Ji", "authorId": "2072975661"}], "n_citations": 0}, "snippets": ["Data augmentation (Hendrycks et al., 2019)(Wang et al., 2021) and model ensemble (Gal and Ghahramani, 2016;(Lakshminarayanan et al., 2016) have been empirically proven to be successful in computer vision. However, they cannot bring the same benefits in NLP according to the empirical study in Chen et al. (2022b)", "Temperature scaling (Platt, 1999)Guo et al., 2017) readjusts the output logits in a posterior way according to the calibration performance on a held-out set. Label smoothing (Szegedy et al., 2015) imposes a confidence regularization during the training process, discouraging models from being overconfident in their predictions.\n\nRecently, there is an emergent trend in NLP, motivating to directly collect data for training models to have reasonable confidence estimations. Kadavath et al. (2022) assume that the last hidden states of PLMs contain the uncertainty information, and directly apply a multi-layer perceptron on them to perform confidence estimations. Lin et al. (2022) also show that PLMs can be directly trained to give their confidence estimations by words. These two methods are proven to be successful in significantly reducing the overconfidence issue and are further extended to exploit the potential in this kind of methods (Chen et al., 2022b)."], "score": 0.9130859375}, {"id": "(Hendrycks et al., 2019)", "paper": {"corpus_id": 208637407, "title": "AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dan Hendrycks", "authorId": "3422872"}, {"name": "Norman Mu", "authorId": "52227748"}, {"name": "E. D. Cubuk", "authorId": "8132903"}, {"name": "Barret Zoph", "authorId": "2368067"}, {"name": "J. Gilmer", "authorId": "2058362"}, {"name": "Balaji Lakshminarayanan", "authorId": "40627523"}], "n_citations": 1308}, "snippets": ["Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AugMix, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AugMix significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half."], "score": 0.0}, {"id": "(Lakshminarayanan et al., 2016)", "paper": {"corpus_id": 6294674, "title": "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles", "year": 2016, "venue": "Neural Information Processing Systems", "authors": [{"name": "Balaji Lakshminarayanan", "authorId": "40627523"}, {"name": "A. Pritzel", "authorId": "1863250"}, {"name": "C. Blundell", "authorId": "1723876"}], "n_citations": 5843}, "snippets": ["Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet."], "score": 0.0}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}], "table": null}, {"title": "Comparative Effectiveness of Calibration Methods", "tldr": "Comparative studies show that different calibration methods vary in effectiveness across contexts, with multi-hypothesis approaches combined with temperature scaling reducing calibration error by over 50% in some cases. Adaptive Temperature Scaling (ATS) offers significant improvements for post-RLHF models, while verbalized confidences often outperform raw model probabilities for black-box language models. (10 sources)", "text": "\nWhen comparing calibration methods for language models, effectiveness varies significantly based on model architecture, task type, and whether the model is accessible or a black-box system. Several comparative studies provide insights into which methods perform best in different scenarios.\n\nFor black-box language models like ChatGPT, GPT-4, and Claude 2, verbalized confidences have been shown to outperform the models' conditional probabilities across multiple benchmarks including TriviaQA, SciQ, and TruthfulQA <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This effectiveness is further enhanced when combining verbalization with multi-hypothesis generation, where prompting models to produce several answer choices before giving confidence scores significantly improves calibration. When this approach is combined with temperature scaling, it can reduce expected calibration error (ECE) by over 50% compared to using model probabilities alone <Paper corpusId=\"258865733\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>.\n\nTemperature scaling remains one of the most consistently effective calibration methods across various contexts. Empirical studies demonstrate that optimal temperature values typically range from 1.5 to 4.0, with a default value of 2.0 often serving as a robust choice across different datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>. This simple approach significantly improves the linear correlation between confidence scores and recognition rates without changing the underlying ranking of predictions.\n\nRecent advances include Adaptive Temperature Scaling (ATS), which predicts a temperature scaling parameter for each token prediction rather than using a single global parameter. This approach addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning and has been shown to improve calibration by 10-50% across multiple natural language benchmarks compared to prior calibration methods <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nWhen comparing training-based and training-free approaches, there are clear tradeoffs. Training-free methods like verbalization or self-consistency can be applied to many language models without requiring access to internal parameters, but they are limited by the models' instruction-following capabilities and can be computationally expensive during inference <Paper corpusId=\"270620078\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Self-consistency approaches, which sample a diverse set of reasoning paths rather than only using greedy decoding, have shown impressive improvements on reasoning tasks <Paper corpusId=\"247595263\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nTraining-based methods directly learn to output a model's uncertainty and can be more efficient at inference time. Approaches like semantic entropy incorporate linguistic invariances created by shared meanings <Paper corpusId=\"257039062\" paperTitle=\"(Kuhn et al., 2023)\" isShortName></Paper>, while other methods leverage token logits or latent representations to predict response correctness <Paper corpusId=\"264811211\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>.\n\nRecent comprehensive evaluations have explored combinations of calibration techniques. Sun et al. evaluated four methods for estimating confidence scores: softmax-based approaches, raw token scores, verbalized confidences, and combinations of these methods, further enhanced with a self-probing mechanism for closed models <Paper corpusId=\"272689376\" paperTitle=\"(Sun et al., 2024)\" isShortName></Paper>. Similarly, Li et al. noted various approaches, including training calibrators to predict response correctness, using latent representations to predict correctness, and multicalibration techniques <Paper corpusId=\"273821704\" paperTitle=\"(Li et al._1, 2024)\" isShortName></Paper>.\n\nIn the context of confidence estimation for factual accuracy, approaches like SelfCheckGPT have demonstrated that sampling-based methods can effectively detect non-factual statements in a zero-resource fashion by leveraging the consistency of multiple samples <Paper corpusId=\"257557820\" paperTitle=\"(Manakul et al., 2023)\" isShortName></Paper>. These methods have shown higher AUC-PR scores in sentence-level hallucination detection compared to grey-box methods that require partial access to model internals.", "citations": [{"id": "(Tian et al., 2023)", "paper": {"corpus_id": 258865733, "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Katherine Tian", "authorId": "2218407338"}, {"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Allan Zhou", "authorId": "2064472884"}, {"name": "Archit Sharma", "authorId": "50465276"}, {"name": "Rafael Rafailov", "authorId": "102801230"}, {"name": "Huaxiu Yao", "authorId": "18307037"}, {"name": "Chelsea Finn", "authorId": "46881670"}, {"name": "Christopher D. Manning", "authorId": "144783904"}], "n_citations": 355}, "snippets": ["Among the methods for verbalizing probabilities directly, we observe that generating and evaluating multiple hypotheses improves calibration (see Figure 1), similarly to humans (Lord et al., 1984), and corroborating a similar finding in LMs (Kadavath et al., 2022)", "We show that prompting a model to produce several answer choices before giving its confidence scores significantly improves calibration of verbalized probabilities. Combined with temperature scaling (Guo et al., 2017), this approach generally provides better calibration than model probabilities for ChatGPT, GPT-4, and Claude 2 across three datasets, often reducing expected calibration error (ECE) by over 50%."], "score": 0.89501953125}, {"id": "(Tarride et al., 2024)", "paper": {"corpus_id": 269449363, "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library", "year": 2024, "venue": "IEEE International Conference on Document Analysis and Recognition", "authors": [{"name": "Sol\u00e8ne Tarride", "authorId": "116700157"}, {"name": "Yoann Schneider", "authorId": "2298900821"}, {"name": "Marie Generali-Lince", "authorId": "2298903215"}, {"name": "M\u00e9lodie Boillet", "authorId": "1380222130"}, {"name": "Bastien Abadie", "authorId": "1491425010"}, {"name": "Christopher Kermorvant", "authorId": "2285800930"}], "n_citations": 4}, "snippets": ["The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets", "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."], "score": 0.97314453125}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 272987064, "title": "Calibrating Language Models with Adaptive Temperature Scaling", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Johnathan Xie", "authorId": "2285068366"}, {"name": "Annie S. Chen", "authorId": "2111073657"}, {"name": "Yoonho Lee", "authorId": "2260289273"}, {"name": "Eric Mitchell", "authorId": "2260106796"}, {"name": "Chelsea Finn", "authorId": "2279838067"}], "n_citations": 17}, "snippets": ["In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."], "score": 0.93994140625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270620078, "title": "Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xin Liu", "authorId": "2120099874"}, {"name": "Farima Fatahi Bayat", "authorId": "2131675694"}, {"name": "Lu Wang", "authorId": "2299208178"}], "n_citations": 6}, "snippets": ["Popular LM calibration methods include training-free, inference-only approaches through verbalization (Tian et al., 2023a) or selfconsistency measurements (Wang et al., 2022) and training-based methods, such as tuning temperature parameters (Liang et al., 2017) or learning uncertainty estimations from LMs' logits (Liu et al., 2023).Despite their potential compatibility with many LMs, training-free methods are limited by the models' instruction-following capabilities and can be computationally expensive during inference.\n\nTraining-based methods, on the other hand, directly learn to output model's uncertainty.For instance, (Kuhn et al., 2023) leverages the token logits predicted by the model to estimate the confidence of the entire response.Yet model logits may not capture knowledge representations and can be sensitive to tokenization procedures."], "score": 0.9462890625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 247595263, "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "D. Schuurmans", "authorId": "50319359"}, {"name": "Quoc Le", "authorId": "1998340269"}, {"name": "Ed H. Chi", "authorId": "2226805"}, {"name": "Denny Zhou", "authorId": "65855107"}], "n_citations": 3734}, "snippets": ["Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%)."], "score": 0.0}, {"id": "(Kuhn et al., 2023)", "paper": {"corpus_id": 257039062, "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Lorenz Kuhn", "authorId": "39879848"}, {"name": "Y. Gal", "authorId": "2681954"}, {"name": "Sebastian Farquhar", "authorId": "33859827"}], "n_citations": 310}, "snippets": ["We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 264811211, "title": "LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xin Liu", "authorId": "2120099874"}, {"name": "Muhammad Khalifa", "authorId": "2261284957"}, {"name": "Lu Wang", "authorId": "2261364835"}], "n_citations": 27}, "snippets": ["A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding<2% of the original model parameters. For evaluation, we construct CaT, a benchmark consisting of eight text generation tasks, covering responses ranging from short phrases to paragraphs. We test LitCab with Llama2-7B, where it improves calibration across all tasks, reducing the average ECE score by as large as 30%. We further conduct a comprehensive evaluation with multiple popular open-sourced LMs from GPT and LLaMA families, yielding the following key findings: (i) Larger models within the same family exhibit better calibration on tasks with short generation tasks, but not necessarily for longer ones. (ii) GPT-family models show superior calibration compared to LLaMA, Llama2, and Vicuna models, despite having much fewer parameters. (iii) Fine-tuning pretrained model (e.g., LLaMA) with samples of limited purpose (e.g., conversations) may lead to worse calibration, highlighting the importance of fine-tuning setups for calibrating LMs."], "score": 0.0}, {"id": "(Sun et al., 2024)", "paper": {"corpus_id": 272689376, "title": "Confidence Estimation For LLM-Based Dialogue State Tracking", "year": 2024, "venue": "Spoken Language Technology Workshop", "authors": [{"name": "Yi-Jyun Sun", "authorId": "2321493889"}, {"name": "Suvodip Dey", "authorId": "29832722"}, {"name": "Dilek Hakkani-Tur", "authorId": "2365041900"}, {"name": "Gokhan Tur", "authorId": "5108268"}], "n_citations": 1}, "snippets": ["We evaluate four methods for estimating confidence scores based on softmax, raw token scores, verbalized confidences, and a combination of these methods, using the area under the curve (AUC) metric to assess calibration, with higher AUC indicating better calibration. We also enhance these with a self-probing mechanism, proposed for closed models."], "score": 0.9814453125}, {"id": "(Li et al._1, 2024)", "paper": {"corpus_id": 273821704, "title": "Graph-based Confidence Calibration for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yukun Li", "authorId": "2329041193"}, {"name": "Sijia Wang", "authorId": "2238971842"}, {"name": "Lifu Huang", "authorId": "2301269508"}, {"name": "Li-Ping Liu", "authorId": "2294764328"}], "n_citations": 2}, "snippets": ["(Mielke et al., 2020) train a calibrator to predict the correctness of a response for a given question. With a similar idea, Ulmer et al. (2024) train a language model (e.g., DeBERTa) based on question-response pairs to predict the probability of responses' correctness. Based on SelfCheckGPT (Manakul et al., 2023) and JAFC (Tian et al., 2023), Chen et al. (2024) train supervised models to reduce grouping losses and improve the confidence estimation. The method by Liu et al. (2024) uses an LLM's latent representations to predict the correctness of responses. Detommaso et al. (2024) use the \"multicalibration\" technique to calibrate the probability of correctness. Fadeeva et al. (2023) offer a detailed comparative study of various confidence estimation methods, providing empirical evidence on their effectiveness across different tasks."], "score": 0.962890625}, {"id": "(Manakul et al., 2023)", "paper": {"corpus_id": 257557820, "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Potsawee Manakul", "authorId": "89355510"}, {"name": "Adian Liusie", "authorId": "2190750613"}, {"name": "M. Gales", "authorId": "1740397"}], "n_citations": 444}, "snippets": ["Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods."], "score": 0.0}], "table": null}, {"title": "Adaptive and Domain-specific Calibration", "tldr": "Adaptive and domain-specific calibration techniques address the challenge that language models may appear well-calibrated overall while being significantly miscalibrated for specific domains or contexts. These approaches include few-shot slice-specific recalibration and adaptive temperature scaling, which dynamically adjust confidence scores based on the specific domain or token-level features. (5 sources)", "text": "\nLanguage models often demonstrate varying levels of calibration across different domains, tasks, or contexts. While a model might appear well-calibrated when evaluated broadly, this can mask significant miscalibration within specific slices of data - for example, \"systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate\" <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>. This observation has led to the development of adaptive and domain-specific calibration techniques that dynamically adjust confidence scores based on specific contexts.\n\nFew-shot slice-specific recalibration represents a significant advancement in this area. This approach trains a recalibration model that takes a few unlabeled examples from any given domain slice and predicts a curve that remaps confidence scores to be more accurate for that specific slice. The trained model can recalibrate for arbitrary new slices without requiring labeled data from those domains, enabling the identification of domain-specific confidence thresholds above which language model predictions can be trusted. Experimental results demonstrate that this few-shot recalibrator consistently outperforms existing calibration methods, improving calibration error for PaLM2-Large on MMLU by 16% compared to temperature scaling <Paper corpusId=\"268723623\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nAnother innovative approach is Adaptive Temperature Scaling (ATS), which addresses the limitations of traditional temperature scaling by predicting a unique temperature scaling parameter for each token prediction rather than applying a single global parameter. This method adapts based on token-level features and is fitted over a standard supervised fine-tuning dataset. The adaptive nature of ATS specifically addresses the varying degrees of calibration shift that can occur after reinforcement learning from human feedback (RLHF) fine-tuning. Studies show that ATS improves calibration by 10-50% across multiple natural language evaluation benchmarks compared to prior calibration methods, without impeding performance improvements from RLHF <Paper corpusId=\"272987064\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nWhen implementing adaptive calibration approaches, practitioners should consider the tradeoffs between open-box and post-hoc methods. While post-hoc methods like temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, Platt scaling, histogram binning, and training with label smoothing <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper> are more straightforward to implement, open-box methods that access model internals can potentially provide more informative calibration signals. However, these open-box approaches are often more complex and difficult to apply to large language models <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>.\n\nDomain-specific calibration is particularly important for applications requiring high reliability in specific contexts, such as medical diagnosis, legal analysis, or educational assessment. By dynamically adjusting confidence scores based on the specific domain or context, these adaptive approaches help ensure that language models provide appropriately calibrated confidence estimates across the diverse range of tasks they may encounter.", "citations": [{"id": "(Li et al., 2024)", "paper": {"corpus_id": 268723623, "title": "Few-Shot Recalibration of Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Lisa Li", "authorId": "2293910776"}, {"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Kelvin Guu", "authorId": "2091768"}], "n_citations": 5}, "snippets": ["Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling."], "score": 0.966796875}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 272987064, "title": "Calibrating Language Models with Adaptive Temperature Scaling", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Johnathan Xie", "authorId": "2285068366"}, {"name": "Annie S. Chen", "authorId": "2111073657"}, {"name": "Yoonho Lee", "authorId": "2260289273"}, {"name": "Eric Mitchell", "authorId": "2260106796"}, {"name": "Chelsea Finn", "authorId": "2279838067"}], "n_citations": 17}, "snippets": ["In this work, we introduce Adaptive Temperature Scaling (ATS), a post-hoc calibration method that predicts a temperature scaling parameter for each token prediction. The predicted temperature values adapt based on token-level features and are fit over a standard supervised fine-tuning (SFT) dataset. The adaptive nature of ATS addresses the varying degrees of calibration shift that can occur after RLHF fine-tuning. ATS improves calibration by over 10-50% across three downstream natural language evaluation benchmarks compared to prior calibration methods and does not impede performance improvements from RLHF."], "score": 0.93994140625}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Muller et al., 2019)", "paper": {"corpus_id": 174802983, "title": "When Does Label Smoothing Help?", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael M\u00fcller", "authorId": "2114054259"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 1954}, "snippets": ["The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."], "score": 0.0}, {"id": "(Abbasli et al., 2025)", "paper": {"corpus_id": 278129791, "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Toghrul Abbasli", "authorId": "2357723383"}, {"name": "Kentaroh Toyoda", "authorId": "2311579070"}, {"name": "Yuan Wang", "authorId": "2285070153"}, {"name": "Leon Witt", "authorId": "2115471566"}, {"name": "Muhammad Asif Ali", "authorId": "2358256611"}, {"name": "Yukai Miao", "authorId": "2239095099"}, {"name": "Dan Li", "authorId": "2302856161"}, {"name": "Qingsong Wei", "authorId": "2285078682"}], "n_citations": 0}, "snippets": ["Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101]."], "score": 0.953125}], "table": null}, {"title": "Implementation Considerations", "tldr": "Implementing effective calibration methods requires careful consideration of model architecture, available access level, and computational efficiency. Practitioners must select appropriate techniques based on whether they're working with black-box, gray-box, or open-box language models, alongside optimizing parameters like temperature values. (6 sources)", "text": "\nWhen implementing calibration techniques for language models, several key considerations should guide your approach:\n\n## Model Access Level\n* **Black-box models**: For commercial LLMs with no internal access, verbalization methods, consistency-based approaches, and post-processing techniques like histogram binning and isotonic regression are most appropriate <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>\n* **Gray-box models**: Temperature scaling, which operates on the logits of model outputs, is well-suited for scenarios with partial access to model internals <Paper corpusId=\"274789494\" paperTitle=\"(Xie et al._1, 2024)\" isShortName></Paper>\n* **Open-box models**: These allow more informative calibration through direct manipulation of model parameters, though these methods can be complex and difficult to apply to large language models <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>\n\n## Parameter Selection\n* **Temperature values**: When implementing temperature scaling, empirical studies show optimal temperatures typically range from 1.5 to 4.0, with 2.0 serving as a robust default choice across diverse datasets <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>\n* **Confidence thresholds**: For domain-specific applications, determining appropriate confidence thresholds may require validation on domain-specific data\n\n## Computational Efficiency\n* **Token-level averaging**: For some applications, taking the softmax value for each token and averaging these values provides an efficient yet effective approach to obtaining confidence scores while maintaining strong correlation with recognition rates <Paper corpusId=\"269449363\" paperTitle=\"(Tarride et al., 2024)\" isShortName></Paper>\n* **Inference costs**: Consider the additional computational burden of methods requiring multiple forward passes or sampling-based approaches\n\n## Domain Adaptation\n* **Out-of-domain (OOD) data**: When calibrating for OOD scenarios, using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD language model can significantly improve confidence scores <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>\n* **Pre-exposure to OOD data**: Exposing confidence estimation models to OOD data during training can improve calibration performance even when the primary model wasn't trained on such data <Paper corpusId=\"238419596\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>\n\n## Method Compatibility\n* **Post-hoc vs. training-based**: Classical post-hoc methods like temperature scaling <Paper corpusId=\"28671436\" paperTitle=\"(Guo et al., 2017)\" isShortName></Paper>, Platt scaling, and histogram binning are generally easier to implement but may provide less informative calibration than training-based approaches <Paper corpusId=\"278129791\" paperTitle=\"(Abbasli et al., 2025)\" isShortName></Paper>\n* **Label smoothing**: This approach prevents networks from becoming overconfident and has been effective in many state-of-the-art models <Paper corpusId=\"174802983\" paperTitle=\"(Muller et al., 2019)\" isShortName></Paper>, but requires model training access\n\n## Evaluation Protocol\n* **Calibration metrics**: Implement appropriate evaluation metrics like Expected Calibration Error (ECE) or linear correlation between confidence scores and recognition rates to properly assess calibration performance\n* **Dataset selection**: Ensure evaluation datasets represent the diversity of domains and tasks the model will encounter in production", "citations": [{"id": "(Xie et al._1, 2024)", "paper": {"corpus_id": 274789494, "title": "A Survey of Calibration Process for Black-Box LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liangru Xie", "authorId": "2336030822"}, {"name": "Hui Liu", "authorId": "2326538488"}, {"name": "Jingying Zeng", "authorId": "2276423058"}, {"name": "Xianfeng Tang", "authorId": "2313288320"}, {"name": "Yan Han", "authorId": "2336079495"}, {"name": "Chen Luo", "authorId": "2305051640"}, {"name": "Jing Huang", "authorId": "2336079074"}, {"name": "Zhen Li", "authorId": "2313299851"}, {"name": "Suhang Wang", "authorId": "2257362558"}, {"name": "Qi He", "authorId": "2315065019"}], "n_citations": 4}, "snippets": ["Calibration aims to align confidence scores with correctness. Commonly used calibration methods include temperature scaling (Guo et al., 2017), label smoothing (Szegedy et al., 2016), and logistic calibration (Kull et al., 2017). However, methods like label smoothing and logistic calibration are unsuitable for black-box LLMs, as they require internal model access or fine-tuning, whereas temperature scaling, which operates on the logits of model outputs, is better suited for gray-box models. In black-box LLMs, common calibration methods include post-processing or transforming the model into a gray-box. Due to the near-well-calibrated nature of Confidence Estimation in black-box LLMs, calibration pressure is reduced.\n\nIn post-processing methods, popular techniques include Histogram Binning (Zadrozny and Elkan, 2001) and Isotonic Regression (Jiang et al., 2011). Histogram Binning (Zadrozny and Elkan, 2001) adjusts confidence values within different intervals by assessing the correctness of predictions in each interval, providing a simple but potentially limited calibration when confidence distributions are uneven. Isotonic Regression (Jiang et al., 2011), on the other hand, seeks an optimal monotonically increasing function f (c) to map the model's confidence c 1 , c 2 , . . . , c n to correctness. f (c) need to satisfy the condition in Formula 2, ensuring that the confidence ranking aligns with the correctness.\n\nTo address more complex scenarios, some methods introduce additional constraints. For instance, H\u00e9bert-Johnson et al. ( 2018) introduced a multicalibration method, ensuring not only overall alignment but also calibration across multiple subgroups S j through iterative alignment. The overall goal can be expressed as Formula 3."], "score": 0.93408203125}, {"id": "(Abbasli et al., 2025)", "paper": {"corpus_id": 278129791, "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Toghrul Abbasli", "authorId": "2357723383"}, {"name": "Kentaroh Toyoda", "authorId": "2311579070"}, {"name": "Yuan Wang", "authorId": "2285070153"}, {"name": "Leon Witt", "authorId": "2115471566"}, {"name": "Muhammad Asif Ali", "authorId": "2358256611"}, {"name": "Yukai Miao", "authorId": "2239095099"}, {"name": "Dan Li", "authorId": "2302856161"}, {"name": "Qingsong Wei", "authorId": "2285078682"}], "n_citations": 0}, "snippets": ["Post-hoc or post-training methods, such as TS (Guo et al., 2017), Platt scaling (Platt, 1999), histogram binning (Zadrozny et al., 2001), and training with label smoothing (M\u00fcller et al., 2019), are classical methods of calibration. Mixup training with learning the complexity of samples was proposed to calibrate the pre-trained language models [100]. We note that open-box methods of calibration can be complex and difficult to apply to LLMs, but they can be more informative of inner states than post-hoc methods [101]."], "score": 0.953125}, {"id": "(Tarride et al., 2024)", "paper": {"corpus_id": 269449363, "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library", "year": 2024, "venue": "IEEE International Conference on Document Analysis and Recognition", "authors": [{"name": "Sol\u00e8ne Tarride", "authorId": "116700157"}, {"name": "Yoann Schneider", "authorId": "2298900821"}, {"name": "Marie Generali-Lince", "authorId": "2298903215"}, {"name": "M\u00e9lodie Boillet", "authorId": "1380222130"}, {"name": "Bastien Abadie", "authorId": "1491425010"}, {"name": "Christopher Kermorvant", "authorId": "2285800930"}], "n_citations": 4}, "snippets": ["The results show that the most effective method is to take the softmax value for each token after the CTC decoding step and average these values for each line to obtain line-level confidence scores. This approach not only ensures computational efficiency, but also yields a strong correlation with the recognition rate. As a result of these findings, we have incorporated this calculation method into PyLaia.\n\nTable 3 provides insight into the impact of model calibration on the linear correlation between confidence scores and recognition rate on the test set of different datasets", "The results show significant improvements in the linear correlation between confidence scores and recognition rates when temperature scaling is applied. Across different datasets, optimal temperatures ranging from 1.5 to 4.0 yield consistently higher correlations compared to scenarios without scaling. Since temperature scaling only scales the logits without changing the underlying ranking of the predictions, it does not affect the rank correlation.\n\nOur analysis suggests that the default value of 2.0 is a robust choice for calibration, even for datasets where the optimal temperature is not equal to 2.0."], "score": 0.97314453125}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 238419596, "title": "Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition", "year": 2021, "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing", "authors": [{"name": "Qiujia Li", "authorId": "1522347808"}, {"name": "Yu Zhang", "authorId": "2153632494"}, {"name": "David Qiu", "authorId": "150080713"}, {"name": "Yanzhang He", "authorId": "2145999837"}, {"name": "Liangliang Cao", "authorId": "48749954"}, {"name": "P. Woodland", "authorId": "1716393"}], "n_citations": 11}, "snippets": ["Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation", "Using pseudo transcriptions to provide binary targets for training model-based confidence estimators and including additional features from an OOD LM are useful for improving the confidence scores on OOD datasets, even when pretraining uses OOD data. By exposing the CEM or R-EBM to OOD data, the calibration performance is also improved."], "score": 0.87939453125}, {"id": "(Guo et al., 2017)", "paper": {"corpus_id": 28671436, "title": "On Calibration of Modern Neural Networks", "year": 2017, "venue": "International Conference on Machine Learning", "authors": [{"name": "Chuan Guo", "authorId": "144993411"}, {"name": "Geoff Pleiss", "authorId": "10804137"}, {"name": "Yu Sun", "authorId": "2117103358"}, {"name": "Kilian Q. Weinberger", "authorId": "7446832"}], "n_citations": 5869}, "snippets": ["Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions."], "score": 0.0}, {"id": "(Muller et al., 2019)", "paper": {"corpus_id": 174802983, "title": "When Does Label Smoothing Help?", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rafael M\u00fcller", "authorId": "2114054259"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}], "n_citations": 1954}, "snippets": ["The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions."], "score": 0.0}], "table": null}], "cost": 0.5538839999999999}}
