{"clarifying_information": [{"clarifying_question1": "Are you interested in comparing the effectiveness and biases of translation-based multilingual evaluation datasets versus those collected natively in target languages?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in a comparison of the effectiveness and biases between translation-based multilingual evaluation datasets and those that are natively collected in the target languages."}}, {"clarifying_question2": "Do you want to know best practices specifically for ensuring quality and cultural relevance when translating English NLP benchmarks into multiple languages?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in best practices specifically for ensuring both quality and cultural relevance when translating English NLP benchmarks into multiple languages."}}, {"clarifying_question3": "Are you seeking detailed examples of pitfalls and reliability issues\u2014such as translation artifacts, cultural biases, or annotation challenges\u2014that arise when using translation as the primary method for non-English benchmarks?", "clarifying_answer3": {"clarifying_answer": "Yes, I am seeking detailed examples of pitfalls and reliability issues\u2014including translation artifacts, cultural biases, and annotation challenges\u2014that specifically arise when using translation as the main method for constructing non-English evaluation benchmarks."}}], "better_queries_2": {"reformulated1": "How do translation-based multilingual evaluation datasets compare in effectiveness and bias against datasets natively collected in target languages, particularly regarding their impact on the reliability of cross-lingual evaluations?", "reformulated2": "What are the best practices for ensuring both linguistic quality and cultural relevance when translating English NLP evaluation benchmarks into multiple languages for use in non-English settings?", "reformulated3": "What are concrete examples of reliability issues and pitfalls\u2014such as translation artifacts, cultural bias, and annotation transfer errors\u2014that arise when translated English datasets are used as the primary method for creating non-English evaluation benchmarks?"}}
