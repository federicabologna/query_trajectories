{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in methods for detecting whether a given text was generated by a large language model (LLM), such as classifier-based, zero-shot, or watermarking techniques?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in methods for detecting whether a given text was generated by a large language model, including classifier-based, zero-shot, and watermarking techniques."}}, {"clarifying_question2": "Are you seeking information on approaches to determine whether specific pre-training data was included in an LLM's training set, such as membership inference attacks, probing, or divergence-based methods?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in approaches to determine whether specific pre-training data was included in an LLM's training set, including membership inference attacks, probing, divergence-based methods, and related techniques."}}, {"clarifying_question3": "Are you looking for recent research or benchmarks focused on out-of-distribution (OOD) detection for language models, such as identifying inputs outside a model's training distribution?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in recent research and benchmarks specifically focused on out-of-distribution (OOD) detection for language models, including methods for identifying inputs that fall outside a model's training distribution."}}], "better_query": "What are the most recent methods and benchmarks for detecting whether a given text was generated by a large language model, including classifier-based approaches, zero-shot detection, and watermarking techniques?", "better_answer": {"sections": [{"title": "Introduction to AI-Generated Text Detection", "tldr": "The emergence of sophisticated Large Language Models (LLMs) capable of producing human-like text has created an urgent need for reliable detection methods. Current approaches to AI-generated text detection include classifier-based methods, statistical analysis, zero-shot techniques, and watermarking, each with their own strengths and limitations. (11 sources)", "text": "\nThe remarkable advancement of Large Language Models (LLMs) has enabled them to reach human-level proficiency in generating text across numerous professional and academic tasks <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246426909\" paperTitle=\"(Ouyang et al., 2022)\" isShortName></Paper>. While these capabilities bring many benefits, they also raise significant concerns about potential misuse, including phishing, disinformation campaigns, and academic dishonesty, making the detection and authentication of generated text crucial for responsible AI use <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nSeveral approaches have emerged to address the challenge of AI-generated text detection. These include passive detection methods that use text classifiers fine-tuned on pretrained language models like RoBERTa and GPT-2 to distinguish between human and machine-written text <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Statistical discrepancy detection tools like GPTZero and DetectGPT <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper> exploit unique patterns in machine-generated text, with GPTZero using metrics such as perplexity and burstiness, while DetectGPT analyzes the negative curvature regions of a model's log probability function <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nDeep learning-based methods rely on training classifiers using datasets of paired human and AI-generated texts on identical topics, showing strong performance within the training data distribution but struggling with adversarial attacks and human-AI collaboration scenarios <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. More recent approaches include watermarking techniques that embed signals into generated text that remain invisible to humans but are algorithmically detectable <Paper corpusId=\"271161969\" paperTitle=\"(Cava et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of these detection methodologies varies based on factors such as the architecture of the language model, the decoding strategy, the length of the text, and the availability of raw log probabilities from the LLMs <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>. Simple classifiers may be limited, particularly with shorter texts and complex generation strategies, while zero-shot models offer robust detection by exploiting statistical patterns but remain vulnerable to adversarial actions <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\nSome researchers have explored retrieval-based approaches that involve maintaining records of all generated texts and checking against them for matches, though this raises concerns about scale, coordination, and privacy <Paper corpusId=\"273547059\" paperTitle=\"(Dathathri et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235694265\" paperTitle=\"(Clark et al., 2021)\" isShortName></Paper> <Paper corpusId=\"249674779\" paperTitle=\"(Jakesch et al., 2022)\" isShortName></Paper>. Other notable detection tools include GPTZero, DetectGPT, and OpenAI's text classifier <Paper corpusId=\"276574900\" paperTitle=\"(Trivedi et al., 2025)\" isShortName></Paper>.\n\nAn interesting discovery in detection research is that the intrinsic dimensionality of embeddings for human-written texts consistently hovers around 9 for alphabet-based languages and 7 for Chinese, while AI-generated texts typically score approximately 1.5 lower, allowing for a statistical separation that outperforms state-of-the-art detectors in model-agnostic and cross-domain scenarios <Paper corpusId=\"259108779\" paperTitle=\"(Tulchinskii et al., 2023)\" isShortName></Paper>. This property demonstrates the ongoing search for invariant features that can robustly separate natural and AI-generated texts regardless of generation model, sampling method, or text domain.", "citations": [{"id": "(Yang et al., 2023)", "paper": {"corpus_id": 258714683, "title": "Watermarking Text Generated by Black-Box Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xi Yang", "authorId": "2143365293"}, {"name": "Kejiang Chen", "authorId": "8780109"}, {"name": "Weiming Zhang", "authorId": "51027868"}, {"name": "Chang-rui Liu", "authorId": "40997227"}, {"name": "Yuang Qi", "authorId": "2217971466"}, {"name": "Jie Zhang", "authorId": "2155863060"}, {"name": "Han Fang", "authorId": "12464207"}, {"name": "Neng H. Yu", "authorId": "2052212945"}], "n_citations": 62}, "snippets": ["Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty", "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."], "score": 0.97021484375}, {"id": "(Ouyang et al., 2022)", "paper": {"corpus_id": 246426909, "title": "Training language models to follow instructions with human feedback", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Long Ouyang", "authorId": "31793034"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Xu Jiang", "authorId": "2115903168"}, {"name": "Diogo Almeida", "authorId": "2061137049"}, {"name": "Carroll L. Wainwright", "authorId": "2064084601"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Chong Zhang", "authorId": null}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Katarina Slama", "authorId": "2117680841"}, {"name": "Alex Ray", "authorId": "2064770039"}, {"name": "John Schulman", "authorId": "47971768"}, {"name": "Jacob Hilton", "authorId": "2052366271"}, {"name": "Fraser Kelton", "authorId": "2151735262"}, {"name": "Luke E. Miller", "authorId": "2142365973"}, {"name": "Maddie Simens", "authorId": "2151735251"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "P. Welinder", "authorId": "2930640"}, {"name": "P. Christiano", "authorId": "145791315"}, {"name": "Jan Leike", "authorId": "2990741"}, {"name": "Ryan J. Lowe", "authorId": "49407415"}], "n_citations": 13203}, "snippets": ["Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent."], "score": 0.0}, {"id": "(Mitchell et al., 2023)", "paper": {"corpus_id": 256274849, "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Yoonho Lee", "authorId": "2110392124"}, {"name": "Alexander Khazatsky", "authorId": "121873407"}, {"name": "Christopher D. Manning", "authorId": "144783904"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 627}, "snippets": ["In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."], "score": 0.9609375}, {"id": "(Cava et al., 2024)", "paper": {"corpus_id": 271161969, "title": "Is Contrasting All You Need? Contrastive Learning for the Detection and Attribution of AI-generated Text", "year": 2024, "venue": "European Conference on Artificial Intelligence", "authors": [{"name": "Lucio La Cava", "authorId": "2115944163"}, {"name": "Davide Costa", "authorId": "2070008514"}, {"name": "Andrea Tagarelli", "authorId": "2261507500"}], "n_citations": 3}, "snippets": ["The remarkable boost in human-like text generation performances achieved by Large Language Models (LLMs) in recent years has determined a rising challenge in detecting whether and to what extent texts have been generated by humans or machines [21,49,39]. In this context, the \"watermarking\" paradigm rapidly gained attention (Kirchenbauer et al., 2023)(Yoo et al., 2023)28,50], as it allows embedding specific signals into generated texts that remain invisible to humans but are algorithmically detectable. Statistical learning methods also offer advanced solutions for detecting the authorship of texts. These include probabilistic models (Mitchell et al., 2023)1,47,14], log rank information [38], perplexity [44], discourse motifs [24], and other statistical approaches (Gehrmann et al., 2019)(Tulchinskii et al., 2023)[45] More recently, we have witnessed the emergence of deep learning to detect or attribute AI-generated content, which stands as a promising body of research. Researchers have been exploiting LLMs to detect generated text [18,46], using ChatGPT itself as a detector (Bhattacharjee et al., 2023), or combining LLMs with topological aspects [43]."], "score": 0.96337890625}, {"id": "(Kirchenbauer et al., 2023)", "paper": {"corpus_id": 256194179, "title": "A Watermark for Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "John Kirchenbauer", "authorId": "2166053502"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Yuxin Wen", "authorId": "123191916"}, {"name": "Jonathan Katz", "authorId": "143975296"}, {"name": "Ian Miers", "authorId": "2679804"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 507}, "snippets": ["Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."], "score": 0.0}, {"id": "(Mubarak et al., 2023)", "paper": {"corpus_id": 266369103, "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Rami Mubarak", "authorId": "2275319803"}, {"name": "Tariq A. A. Alsboui", "authorId": "3277359"}, {"name": "Omar Alshaikh", "authorId": "2207298991"}, {"name": "Isa Inuwa-Dutse", "authorId": "2276533468"}, {"name": "Saad Khan", "authorId": "143955457"}, {"name": "Si\u00f4n Parkinson", "authorId": "100581778"}], "n_citations": 36}, "snippets": ["In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research."], "score": 0.970703125}, {"id": "(Dathathri et al., 2024)", "paper": {"corpus_id": 273547059, "title": "Scalable watermarking for identifying large language model outputs", "year": 2024, "venue": "The Naturalist", "authors": [{"name": "Sumanth Dathathri", "authorId": "3491117"}, {"name": "Abigail See", "authorId": "2268761249"}, {"name": "Sumedh Ghaisas", "authorId": "145053127"}, {"name": "Po-Sen Huang", "authorId": "2268826600"}, {"name": "Rob McAdam", "authorId": "2327318512"}, {"name": "Johannes Welbl", "authorId": "1851564"}, {"name": "Vandana Bachani", "authorId": "2327316530"}, {"name": "Alex Kaskasoli", "authorId": "2275186627"}, {"name": "Robert Stanforth", "authorId": "49860489"}, {"name": "Tatiana Matejovicova", "authorId": "2327319709"}, {"name": "Jamie Hayes", "authorId": "2270174566"}, {"name": "Nidhi Vyas", "authorId": "2275153245"}, {"name": "Majd Al Merey", "authorId": "2089890458"}, {"name": "Jonah Brown-Cohen", "authorId": "1400348545"}, {"name": "Rudy Bunel", "authorId": "3407947"}, {"name": "Borja Balle", "authorId": "2324784942"}, {"name": "taylan. cemgil", "authorId": "122347685"}, {"name": "Zahra Ahmed", "authorId": "2268758347"}, {"name": "Kitty Stacpoole", "authorId": "1396786528"}, {"name": "Ilia Shumailov", "authorId": "47473421"}, {"name": "Cip Baetu", "authorId": "2324799480"}, {"name": "Sven Gowal", "authorId": "2071666"}, {"name": "D. Hassabis", "authorId": "48987704"}, {"name": "Pushmeet Kohli", "authorId": "143967473"}], "n_citations": 74}, "snippets": ["Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions."], "score": 0.9580078125}, {"id": "(Clark et al., 2021)", "paper": {"corpus_id": 235694265, "title": "All That\u2019s \u2018Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Elizabeth Clark", "authorId": "40684993"}, {"name": "Tal August", "authorId": "50509991"}, {"name": "Sofia Serrano", "authorId": "38618739"}, {"name": "Nikita Haduong", "authorId": "3465456"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 414}, "snippets": ["Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models."], "score": 0.0}, {"id": "(Jakesch et al., 2022)", "paper": {"corpus_id": 249674779, "title": "Human heuristics for AI-generated language are flawed", "year": 2022, "venue": "Proceedings of the National Academy of Sciences of the United States of America", "authors": [{"name": "Maurice Jakesch", "authorId": "113179096"}, {"name": "Jeffrey T. Hancock", "authorId": "1697703"}, {"name": "Mor Naaman", "authorId": "1687465"}], "n_citations": 188}, "snippets": ["Significance Human communication is now rife with language generated by AI. Every day, across the web, chat, email, and social media, AI systems produce billions of messages that could be perceived as created by humans. In this work, we analyze human judgments of self-presentations written by humans and generated by AI systems. We find that people cannot detect AI-generated self-presentations as their judgment is misguided by intuitive but flawed heuristics for AI-generated language. We demonstrate that AI systems can exploit these heuristics to produce text perceived as \"more human than human.\" Our results raise the question of how humanity will adapt to AI-generated text, illustrating the need to reorient the development of AI language systems to ensure that they support rather than undermine human cognition."], "score": 0.0}, {"id": "(Trivedi et al., 2025)", "paper": {"corpus_id": 276574900, "title": "Sarang at DEFACTIFY 4.0: Detecting AI-Generated Text Using Noised Data and an Ensemble of DeBERTa Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Avinash Trivedi", "authorId": "2346209847"}, {"name": "S. Sangeetha", "authorId": "46505742"}], "n_citations": 0}, "snippets": ["To address these risks, various algorithms have been developed to detect AI-generated text, which include watermarking techniques [3], tools like GPTZero [4], DetectGPT [5], and OpenAI's text classifier [6]."], "score": 0.9404296875}, {"id": "(Tulchinskii et al., 2023)", "paper": {"corpus_id": 259108779, "title": "Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Eduard Tulchinskii", "authorId": "2165662874"}, {"name": "Kristian Kuznetsov", "authorId": "2192824975"}, {"name": "Laida Kushnareva", "authorId": "147328740"}, {"name": "D. Cherniavskii", "authorId": "2037913268"}, {"name": "S. Barannikov", "authorId": "101098891"}, {"name": "Irina Piontkovskaya", "authorId": "2219690846"}, {"name": "S. Nikolenko", "authorId": "1742235"}, {"name": "Evgeny Burnaev", "authorId": "51139941"}], "n_citations": 89}, "snippets": ["Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over different text domains and varying proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant for human-written texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings for a given text sample. We show that the average intrinsic dimensionality of fluent texts in a natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\\approx 1.5$ lower, with a clear statistical separation between human-generated and AI-generated distributions. This property allows us to build a score-based artificial text detector. The proposed detector's accuracy is stable over text domains, generator models, and human writer proficiency levels, outperforming SOTA detectors in model-agnostic and cross-domain scenarios by a significant margin."], "score": 0.0}], "table": null}, {"title": "Zero-Shot Detection Methods", "tldr": "Zero-shot detection methods identify AI-generated text without requiring training data specifically for this task. Popular approaches include DetectGPT, which analyzes the curvature of a model's log probability function, and methods that leverage statistical patterns like perplexity, burstiness, and token rank distributions. (11 sources)", "text": "\nZero-shot detection methods have emerged as an important category of AI-generated text detection tools that do not require training on labeled datasets of machine-generated text. These methods are particularly valuable because they can potentially generalize to new models and domains without needing additional training data <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258048481\" paperTitle=\"(Chakraborty et al., 2023)\" isShortName></Paper>.\n\nOne of the most influential zero-shot detection approaches is DetectGPT, which identifies a fundamental property of language models: text generated by LLMs tends to occupy negative curvature regions of the model's log probability function <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>. By comparing the log probability of the original text with perturbed versions, DetectGPT can detect AI-generated content without training a separate classifier. This approach has significantly improved detection performance, achieving 0.95 AUROC for fake news articles generated by GPT-NeoX-20B compared to 0.81 AUROC for previous zero-shot baselines <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>.\n\nWhile effective, DetectGPT requires numerous queries to the source model, making it computationally expensive <Paper corpusId=\"258947640\" paperTitle=\"(Deng et al., 2023)\" isShortName></Paper>. Addressing this limitation, Su et al. introduced DetectLLM, which offers two variants: DetectLLM-LRR (Log Rank Ratio), a fast and efficient method, and DetectLLM-NPR (Negative Probability Ratio), which requires perturbations but achieves higher accuracy with fewer perturbations than DetectGPT <Paper corpusId=\"259129463\" paperTitle=\"(Su et al., 2023)\" isShortName></Paper>.\n\nOther statistical zero-shot methods leverage metrics such as perplexity, burstiness, entropy, and n-gram frequency distributions <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. Tools like GPTZero specifically use perplexity and burstiness to distinguish between human and machine text, with lower values typically indicating machine-generated content <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. The GLTR tool analyzes token rank distributions and entropy to help identify predictable patterns in AI-generated text <Paper corpusId=\"267094724\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>.\n\nZero-shot detection can operate in either white-box or black-box settings. In white-box scenarios, detectors have access to the source model that generated the text, while black-box detection aims to identify AI-generated content without knowing which model produced it <Paper corpusId=\"263831345\" paperTitle=\"(Bao et al., 2023)\" isShortName></Paper>. This distinction is important for practical applications, as most real-world detection scenarios involve black-box settings where the specific generative model is unknown.\n\nA key advantage of zero-shot methods is their ability to adapt to new models without retraining, though they may be less effective than supervised approaches within specific domains <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. However, these methods face challenges with adversarial inputs and highly optimized language models that increasingly mimic human writing patterns <Paper corpusId=\"267027819\" paperTitle=\"(Bethany et al., 2024)\" isShortName></Paper>. As language models continue to evolve, developing more robust zero-shot detection methods remains an active area of research.", "citations": [{"id": "(Lu et al., 2023)", "paper": {"corpus_id": 258762215, "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Ning Lu", "authorId": "2147128045"}, {"name": "Shengcai Liu", "authorId": "2152939552"}, {"name": "Ruidan He", "authorId": "22272507"}, {"name": "Ke Tang", "authorId": "2078692567"}], "n_citations": 53}, "snippets": ["In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."], "score": 0.9609375}, {"id": "(Chakraborty et al., 2023)", "paper": {"corpus_id": 258048481, "title": "On the Possibilities of AI-Generated Text Detection", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Souradip Chakraborty", "authorId": "49081354"}, {"name": "A. S. Bedi", "authorId": "3387859"}, {"name": "Sicheng Zhu", "authorId": "2110018762"}, {"name": "Bang An", "authorId": "49640821"}, {"name": "Dinesh Manocha", "authorId": "2172597446"}, {"name": "Furong Huang", "authorId": "2117426487"}], "n_citations": 127}, "snippets": ["Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification."], "score": 0.9658203125}, {"id": "(Mitchell et al., 2023)", "paper": {"corpus_id": 256274849, "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Yoonho Lee", "authorId": "2110392124"}, {"name": "Alexander Khazatsky", "authorId": "121873407"}, {"name": "Christopher D. Manning", "authorId": "144783904"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 627}, "snippets": ["In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."], "score": 0.9609375}, {"id": "(Deng et al., 2023)", "paper": {"corpus_id": 258947640, "title": "Efficient Detection of LLM-generated Texts with a Bayesian Surrogate Model", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhijie Deng", "authorId": "145114723"}, {"name": "Hongcheng Gao", "authorId": "2162081759"}, {"name": "Yibo Miao", "authorId": "2188993538"}, {"name": "Hao Zhang", "authorId": "46702482"}], "n_citations": 20}, "snippets": ["The detection of machine-generated text, especially from large language models (LLMs), is crucial in preventing serious social problems resulting from their misuse. Some methods train dedicated detectors on specific datasets but fall short in generalizing to unseen test data, while other zero-shot ones often yield suboptimal performance. Although the recent DetectGPT has shown promising detection performance, it suffers from significant inefficiency issues, as detecting a single candidate requires querying the source LLM with hundreds of its perturbations.\n\nSome methods (e.g., OpenAI, 2023b) involve training supervised classifiers, which, yet, suffer from overfitting to the training data and ineffectiveness to generalize to new test data. Zero-shot LLM-generated text detection approaches bypass these issues by leveraging the source LLM to detect its samples (Solaiman et al., 2019;Gehrmann et al., 2019b;Ippolito et al., 2020). They usually proceed by inspecting the average per-token log probability of the candidate text, but the practical detection performance can be unsatisfactory.\n\nDetectGPT (Mitchell et al., 2023) is a recent method that achieves improved zero-shot detection efficacy by exploring the probability curvature of LLMs. It generates multiple perturbations of the candidate text and scores them using the source LLM to define detection statistics. It can detect texts generated by GPT-2 (Radford et al., 2019) and GPT-NeoX-20B (Black et al., 2022). Yet, DetectGPT relies on hundreds of queries to the source LLM to estimate the local probability curvature surrounding one single candidate passage."], "score": 0.9423828125}, {"id": "(Su et al., 2023)", "paper": {"corpus_id": 259129463, "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jinyan Su", "authorId": "2116966710"}, {"name": "Terry Yue Zhuo", "authorId": "2080123731"}, {"name": "Di Wang", "authorId": "2119265743"}, {"name": "Preslav Nakov", "authorId": "2026545715"}], "n_citations": 141}, "snippets": ["Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use."], "score": 0.95849609375}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258967971, "title": "Baselines for Identifying Watermarked Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Leonard Tang", "authorId": "2144484107"}, {"name": "Gavin Uberti", "authorId": "2143997407"}, {"name": "Tom Shlomi", "authorId": "2211332429"}], "n_citations": 3}, "snippets": ["Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."], "score": 0.970703125}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 267094724, "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feng Xiong", "authorId": "2280331364"}, {"name": "Thanet Markchom", "authorId": "52196310"}, {"name": "Ziwei Zheng", "authorId": "2280391458"}, {"name": "Subin Jung", "authorId": "2280666724"}, {"name": "Varun Ojha", "authorId": "2293393169"}, {"name": "Huizhi Liang", "authorId": "2281373610"}], "n_citations": 5}, "snippets": ["In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023)."], "score": 0.97705078125}, {"id": "(Gehrmann et al., 2019)", "paper": {"corpus_id": 182952848, "title": "GLTR: Statistical Detection and Visualization of Generated Text", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Alexander M. Rush", "authorId": "2531268"}], "n_citations": 546}, "snippets": ["The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."], "score": 0.0}, {"id": "(Bao et al., 2023)", "paper": {"corpus_id": 263831345, "title": "Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Guangsheng Bao", "authorId": "1993226927"}, {"name": "Yanbin Zhao", "authorId": "2257125588"}, {"name": "Zhiyang Teng", "authorId": "2272668"}, {"name": "Linyi Yang", "authorId": "2145500840"}, {"name": "Yue Zhang", "authorId": "2256997888"}], "n_citations": 153}, "snippets": ["Our research centers on zero-shot detection of machine-generated text, under the premise that our model has not undergone training using any machine-generated text. This distinguishes our approach from conventional supervised methods, which commonly employ discriminative training strategies to acquire specific syntactic or semantic attributes customized for machine-generated text. In contrast, our zero-shot methodology capitalizes on the inherent capabilities of large language models to identify anomalies that function as markers of machine-generated content.\n\nThe White-box Setting. Conventional zero-shot methodologies often operate under the assumption that the source model responsible for generating machine-generated text is accessible. We refer to this context as the white-box setting, where the primary goal is to distinguish machine-generated texts produced by the source model from those generated by humans. In this white-box setting, our detection decisions are dependent on the source model, but it is not mandatory to possess detailed knowledge of the source model's architecture and parameters. For instance, within the white-box framework, a system like DetectGPT utilizes the OpenAI API to identify text generated by GPT-3, without requiring extensive knowledge of the inner workings of GPT-3.\n\nThe Black-box Setting. In real-world situations, there could be instances where we lack knowledge about the specific source models employed for content generation. This necessitates the development of a versatile detector capable of identifying texts generated by a variety of automated systems. We term this scenario the black-box setting, where the objective is to differentiate between machinegenerated texts produced by diverse, unidentified models and those composed by humans. In this context, the term \"black box\" signifies that we lack access to information about the source model or any details pertaining to it."], "score": 0.95458984375}, {"id": "(Nguyen-Son et al., 2024)", "paper": {"corpus_id": 273901392, "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hoang-Quoc Nguyen-Son", "authorId": "1403588130"}, {"name": "Minh-Son Dao", "authorId": "2260782421"}, {"name": "Koji Zettsu", "authorId": "2238657258"}], "n_citations": 2}, "snippets": ["Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."], "score": 0.95654296875}, {"id": "(Bethany et al., 2024)", "paper": {"corpus_id": 267027819, "title": "Deciphering Textual Authenticity: A Generalized Strategy through the Lens of Large Language Semantics for Detecting Human vs. Machine-Generated Text", "year": 2024, "venue": "USENIX Security Symposium", "authors": [{"name": "Mazal Bethany", "authorId": "2151516184"}, {"name": "Brandon Wherry", "authorId": "2279750613"}, {"name": "Emet Bethany", "authorId": "2279757012"}, {"name": "Nishant Vishwamitra", "authorId": "8842079"}, {"name": "Peyman Najafirad", "authorId": "71756373"}], "n_citations": 6}, "snippets": ["One of the subsets of statistical features leveraged in detecting machine-generated text revolves around term frequencies within text samples. Text authored by humans typically aligns with Zipf's Law, where the most frequent word in humanwritten text has roughly twice the frequency of the second most frequent word, and nearly three times that of the third most frequent word and so on (Nguyen-Son et al., 2018). Another feature-based method for detecting machine-generated text focuses on clarity and coherence. Earlier models like GPT-2 have shown that in longer outputs, there is a higher likelihood that inconsistencies, redundancy, or logical errors will emerge (See et al., 2019). Some works build classifiers on bag-of-words features and TF-IDF (Fagni et al., 2020) to analyze shorter text sequences. Other works such as Fr\u00f6hling et al. (Fr\u00f6hling et al., 2021) combine several features, including lack of syntactic and lexical diversity, repetitiveness, and extract linguistic features such as named entities and part-of-speech distributions to build a neural network classifier on this information. However, a notable limitation of these statistical and feature-based methods is their potential vulnerability to more advanced language models, which continually improve in generating human-like text patterns and linguistic diversity. \n\nZero-shot methods for detecting machine-generated text have garnered attention due to the lack of need to train models on large datasets. Works such as those by Gehrmann et al. (Gehrmann et al., 2019) find that human-written texts exhibit a broader range of word choices, whereas machine-generated texts tend to be more constrained and predictable and calculate entropy on text sequences to help distinguish machine-generated text from human-generated text. Other works used simple logprobability thresholding, where LLM token probabilities are averaged and a threshold is determined [56]. Recent works like DetectGPT [40] take this idea further by introducing the observation that the curvature of a model's log probability function is typically more negative for machine-generated text compared to human-written text."], "score": 0.955078125}], "table": null}, {"title": "Classifier-Based Detection Approaches", "tldr": "Classifier-based detection approaches treat AI-generated text identification as a binary classification task by training models on datasets containing both human and AI-generated texts. These methods range from traditional machine learning algorithms to fine-tuned large language models, showing strong performance within their training distribution but struggling with new generation models and adversarial scenarios. (14 sources)", "text": "\nClassifier-based detectors represent a prominent approach for distinguishing between human and AI-generated texts. Unlike statistical or zero-shot methods, these detectors utilize supervised learning techniques to build discriminative models trained on labeled datasets of human and machine-generated content <Paper corpusId=\"258048481\" paperTitle=\"(Chakraborty et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>.\n\nEarly classifier-based detection methods focused on specific domains such as fake reviews and fake news. Zellers et al. (2019) demonstrated the effectiveness of classifiers in identifying neural fake news, finding that using the same model architecture for both generation and detection yielded the best results <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"168169824\" paperTitle=\"(Zellers et al., 2019)\" isShortName></Paper>. This work established a pattern where the most effective detector for a specific model's output is often the model itself or one with similar architecture.\n\nMore sophisticated approaches have emerged with the advancement of large language models. OpenAI developed a detector by fine-tuning a GPT model on datasets comprising paired human and AI-generated texts covering identical topics <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Similarly, researchers have fine-tuned robust pre-trained language models like RoBERTa to create specialized classifiers. For example, Guo et al. created the Human ChatGPT Comparison Corpus (HC3) to fine-tune a RoBERTa-based classifier for detecting ChatGPT-generated content <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"273482129\" paperTitle=\"(Tao et al., 2024)\" isShortName></Paper>.\n\nThe effectiveness of these detectors varies significantly based on several factors. Training-based classifiers typically perform well when the test data shares a similar distribution with the training data, but their performance deteriorates when faced with out-of-distribution samples or texts from new generation models <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper>. This limitation becomes increasingly problematic as language models continue to evolve rapidly. Some studies have shown that for sufficiently advanced language models, machine-generated text detectors may offer only marginal improvement over random classifiers <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\nTo address these limitations, more recent approaches incorporate adversarial training techniques. The RADAR framework, for instance, jointly trains a paraphraser and a detector in an adversarial setting. The paraphraser attempts to generate realistic content that evades detection, while the detector learns to identify even these adversarially modified texts <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>. This approach has shown significant improvements in robustness, especially when facing paraphrased content.\n\nOther innovations include sentence-level detection methods like SeqXGPT, which uses log-probability lists and self-attention networks to enhance detection at a more granular level <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264128397\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>. This approach is particularly valuable for identifying partially AI-generated or human-edited content.\n\nTraditional machine learning approaches using feature extraction (such as stylometric analysis) are still used in some contexts, with algorithms like logistic regression, SVM, and XGBoost offering the advantage of explainability. However, these methods generally show lower accuracy compared to deep learning approaches that can capture more nuanced aspects of writing style <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267094724\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper>.\n\nDespite their widespread use, classifier-based detectors face significant challenges. They are susceptible to adversarial attacks, lack interpretability, and struggle with human-AI collaboration scenarios where text may be partially generated or edited <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper>. Furthermore, they require substantial data collection and model training costs, which can be prohibitive for some applications <Paper corpusId=\"264426339\" paperTitle=\"(Wang et al._1, 2023)\" isShortName></Paper>. As language models continue to improve and generate increasingly human-like text, the effectiveness of these classifiers is expected to decline unless detection techniques evolve accordingly <Paper corpusId=\"272987651\" paperTitle=\"(Feng et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Chakraborty et al., 2023)", "paper": {"corpus_id": 258048481, "title": "On the Possibilities of AI-Generated Text Detection", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Souradip Chakraborty", "authorId": "49081354"}, {"name": "A. S. Bedi", "authorId": "3387859"}, {"name": "Sicheng Zhu", "authorId": "2110018762"}, {"name": "Bang An", "authorId": "49640821"}, {"name": "Dinesh Manocha", "authorId": "2172597446"}, {"name": "Furong Huang", "authorId": "2117426487"}], "n_citations": 127}, "snippets": ["Traditional approaches. They involve statistical outlier detection methods, which employ statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human and machine-generated texts (Lavergne et al., 2008)Gehrmann et al., 2019). However, with the advent of ChatGPT (OpenAI) (OpenAI, 2023), a new innovative statistical detection methodology, DetectGPT (Mitchell et al., 2023), has been developed. It operates on the principle that text generated by the model tends to lie in the negative curvature areas of the model's log probability. DetectGPT (Mitchell et al., 2023) generates and compares multiple perturbations of model-generated text to determine whether the text is machine-generated or not based on the log probability of the original text and the perturbed versions. DetectGPT significantly outperforms the majority of the existing zero-shot methods for model sample detection with very high AUC scores (note that we use the terms AUROC and AUC interchangeably for presentation convenience). \n\nClassifier-based detectors. In contrast to statistical methods, classifier-based detectors are common in natural language detection paradigms, particularly in fake news and misinformation detection (Schildhauer, 2022;Zou & Ling, 2021). OpenAI has recently fine-tuned a GPT model (OpenAI, 2023) using data from Wikipedia, WebText, and internal human demonstration data to create a web interface for a discrimination task using text generated by 34 language models. This approach combines a classifier-based approach with a human evaluation component to determine whether a given text was machine-generated or not. These recent advancements in the field of detecting AI-generated text have significant implications for detecting and preventing the spread of misinformation and fake news, thereby contributing to the betterment of society (Schildhauer, 2022;Zou & Ling, 2021;Kshetri & Voas, 2022). \n\nWatermark-based identification."], "score": 0.9658203125}, {"id": "(Yang et al., 2023)", "paper": {"corpus_id": 258714683, "title": "Watermarking Text Generated by Black-Box Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xi Yang", "authorId": "2143365293"}, {"name": "Kejiang Chen", "authorId": "8780109"}, {"name": "Weiming Zhang", "authorId": "51027868"}, {"name": "Chang-rui Liu", "authorId": "40997227"}, {"name": "Yuang Qi", "authorId": "2217971466"}, {"name": "Jie Zhang", "authorId": "2155863060"}, {"name": "Han Fang", "authorId": "12464207"}, {"name": "Neng H. Yu", "authorId": "2052212945"}], "n_citations": 62}, "snippets": ["Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty", "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."], "score": 0.97021484375}, {"id": "(Lu et al., 2023)", "paper": {"corpus_id": 258762215, "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Ning Lu", "authorId": "2147128045"}, {"name": "Shengcai Liu", "authorId": "2152939552"}, {"name": "Ruidan He", "authorId": "22272507"}, {"name": "Ke Tang", "authorId": "2078692567"}], "n_citations": 53}, "snippets": ["In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."], "score": 0.9609375}, {"id": "(Zellers et al., 2019)", "paper": {"corpus_id": 168169824, "title": "Defending Against Neural Fake News", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Rowan Zellers", "authorId": "2545335"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Hannah Rashkin", "authorId": "2516777"}, {"name": "Yonatan Bisk", "authorId": "3312309"}, {"name": "Ali Farhadi", "authorId": "143787583"}, {"name": "Franziska Roesner", "authorId": "3268360"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 1030}, "snippets": ["Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news. \nModern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation. \nDeveloping robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news."], "score": 0.0}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258967971, "title": "Baselines for Identifying Watermarked Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Leonard Tang", "authorId": "2144484107"}, {"name": "Gavin Uberti", "authorId": "2143997407"}, {"name": "Tom Shlomi", "authorId": "2211332429"}], "n_citations": 3}, "snippets": ["Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."], "score": 0.970703125}, {"id": "(Tao et al., 2024)", "paper": {"corpus_id": 273482129, "title": "Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhen Tao", "authorId": "2299329349"}, {"name": "Zhiyu Li", "authorId": "2279173209"}, {"name": "Runyu Chen", "authorId": "2506761"}, {"name": "Dinghao Xi", "authorId": "2128365150"}, {"name": "Wei Xu", "authorId": "2301200175"}], "n_citations": 2}, "snippets": ["Existing detection methods are categorized into metricbased and model-based approaches. Metric-based methods quantify linguistic features, such as stylometric analysis and perplexity scoring, to determine text origin. Gehrmann et al. [22] introduced GLTR, which uses metrics like word probability and entropy for detection, while Solaiman et al. [23] proposed a zero-shot detection method leveraging pre-trained models like GPT-2 or GROVER. However, as LLMs advance, these methods have become less effective. To address these limitations, more advanced and sophisticated detection techniques have emerged. Mitchell et al. (Mitchell et al., 2023) presented DetectGPT, a zero-shot method utilizing probability curvature analysis, and Tian et al. [25] developed the Multiscale Positive-Unlabeled (MPU) framework to enhance detection across varying text lengths. While metricbased approaches rely on linguistic analysis, model-based methods use advanced machine learning to classify text as human or AI-generated. These often involve fine-tuning language models or developing specialized architectures. For instance, Guo et al. [26] fine-tuned a RoBERTa model to detect ChatGPT-generated text, while Wang et al. [27] introduced SeqXGPT, a sentence-level method using logprobability lists and self-attention networks for enhanced detection. Liu et al. (Liu et al., 2023) proposed COCO, which improves detection in low-resource settings by combining an entity coherence graph with contrastive learning."], "score": 0.978515625}, {"id": "(Cao, 2025)", "paper": {"corpus_id": 277595847, "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lele Cao", "authorId": "2354295905"}], "n_citations": 0}, "snippets": ["One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."], "score": 0.95458984375}, {"id": "(Hu et al., 2023)", "paper": {"corpus_id": 259501842, "title": "RADAR: Robust AI-Text Detection via Adversarial Learning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xiaomeng Hu", "authorId": "2199229406"}, {"name": "Pin-Yu Chen", "authorId": "2158177948"}, {"name": "Tsung-Yi Ho", "authorId": "2103197703"}], "n_citations": 127}, "snippets": ["Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."], "score": 0.0}, {"id": "(Nguyen-Son et al., 2024)", "paper": {"corpus_id": 273901392, "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hoang-Quoc Nguyen-Son", "authorId": "1403588130"}, {"name": "Minh-Son Dao", "authorId": "2260782421"}, {"name": "Koji Zettsu", "authorId": "2238657258"}], "n_citations": 2}, "snippets": ["Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."], "score": 0.95654296875}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264128397, "title": "SeqXGPT: Sentence-Level AI-Generated Text Detection", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pengyu Wang", "authorId": "2199600227"}, {"name": "Linyang Li", "authorId": "2107897400"}, {"name": "Ke Ren", "authorId": "2258550235"}, {"name": "Botian Jiang", "authorId": "2258680333"}, {"name": "Dong Zhang", "authorId": "2109797247"}, {"name": "Xipeng Qiu", "authorId": "2188058565"}], "n_citations": 57}, "snippets": ["Widely applied large language models (LLMs) can generate human-like content, raising concerns about the abuse of LLMs. Therefore, it is important to build strong AI-generated text (AIGT) detectors. Current works only consider document-level AIGT detection, therefore, in this paper, we first introduce a sentence-level detection challenge by synthesizing a dataset that contains documents that are polished with LLMs, that is, the documents contain sentences written by humans and sentences modified by LLMs. Then we propose \\textbf{Seq}uence \\textbf{X} (Check) \\textbf{GPT}, a novel method that utilizes log probability lists from white-box LLMs as features for sentence-level AIGT detection. These features are composed like \\textit{waves} in speech processing and cannot be studied by LLMs. Therefore, we build SeqXGPT based on convolution and self-attention networks. We test it in both sentence and document-level detection challenges. Experimental results show that previous methods struggle in solving sentence-level AIGT detection, while our method not only significantly surpasses baseline methods in both sentence and document-level detection challenges but also exhibits strong generalization capabilities."], "score": 0.0}, {"id": "(Bitton et al., 2025)", "paper": {"corpus_id": 276773353, "title": "Detecting Stylistic Fingerprints of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yehonatan Bitton", "authorId": "1405506735"}, {"name": "Elad Bitton", "authorId": "2348468222"}, {"name": "Shai Nisan", "authorId": "2348465980"}], "n_citations": 0}, "snippets": ["Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text."], "score": 0.94775390625}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 267094724, "title": "Fine-tuning Large Language Models for Multigenerator, Multidomain, and Multilingual Machine-Generated Text Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Feng Xiong", "authorId": "2280331364"}, {"name": "Thanet Markchom", "authorId": "52196310"}, {"name": "Ziwei Zheng", "authorId": "2280391458"}, {"name": "Subin Jung", "authorId": "2280666724"}, {"name": "Varun Ojha", "authorId": "2293393169"}, {"name": "Huizhi Liang", "authorId": "2281373610"}], "n_citations": 5}, "snippets": ["In the realm of distinguishing texts generated by large language models (LLMs) from those authored by humans, researchers have employed a variety of methods and tools. Broadly, these approaches can be categorized into two main types: black-box and white-box detection methods (Tang et al., 2023). Black-box detection relies on API-level access to LLMs, utilizing textual samples from both human and machine sources to train classification models (Dugan et al., 2020). The study by Guo et al. (2023) integrated existing question-and-answer datasets and leveraged fine-tuning of pre-trained models to investigate the characteristics and similarities between human-generated and AI-generated texts (Guo et al., 2023). \n\nIn the context of white-box detection, Kirchenbauer et al. ( 2023) introduced a novel approach involving the embedding of watermarks in the outputs of large language models to facilitate the detection of AI-generated text (Kirchenbauer et al., 2023). Additionally, a variety of tools and methodologies, including XGBoost, decision trees, and transformer-based models, have been evaluated for their efficacy in detecting texts produced by AI (Zaitsu and Jin, 2023). These techniques incorporate multiple stylistic measurement features to differentiate between AI-generated and human-generated texts (Shijaku and Canhasi, 2023). \n\nSpecific tools and techniques in this domain include the GLTR tool developed by Gehrmann et al. (2019), which analyzes the usage of rare words in texts to distinguish between those generated by the GPT-2 model and human writers (Gehrmann et al., 2019). The DetectGPT method posits that minor rewrites of LLM-generated texts tend to reduce the log probability under the model, a hypothesis that has been explored in depth (Mitchell et al., 2023). Furthermore, intrinsic dimension analysis, including methods like the Persistent Homology Dimension estimator (PHD), has been applied to distinguish between authentic texts and those generated artificially (Tulchinskii et al., 2023)."], "score": 0.97705078125}, {"id": "(Wang et al._1, 2023)", "paper": {"corpus_id": 264426339, "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Rongsheng Wang", "authorId": "2261313297"}, {"name": "Qi Li", "authorId": "2261359496"}, {"name": "Sihong Xie", "authorId": "2261347599"}], "n_citations": 7}, "snippets": ["Recent research has shown promising results in the development of detection methods. The existing detectors are built on the assumption that there is a distributional difference between human-generated texts and AI-generated texts. These differences are typically identified by training classifiers or using statistical information. \n\nClassifier-based detectors. Classifier-based detectors are commonly used in natural language processing detection paradigms, especially in fake news and misinformation detection [3]. Guo et al. [4] proposed the ChatGPT Detector, where they initially constructed a dataset consisting of ChatGPT conversations with human questions and answers, and trained a text detection classifier based on this dataset. The use of these methods requires substantial data collection and incurs the cost of training these classifier models. \n\nStatistical-based detectors. Statistical-based detectors utilize statistical metrics such as entropy, perplexity, and n-gram frequency to differentiate between human-generated and AI-generated texts [1]. \n\nSome other work is based on watermark-based detectors. In previous research, watermarks have been applied in the field of image processing and computer vision to protect copyrighted content and prevent intellectual property theft (Langelaar et al., 2000). \n\nRecently, with the emergence of ChatGPT, the work by Kirchenbauer et al. [7] demonstrated how to incorporate a watermark using only the logarithmic credentials of each step to mark AI-generated texts. While watermark-based detectors are an intriguing area of research, adding watermarks may affect the readability of the texts, and the removal of watermarks is also a challenge we need to address. \n\nHowever, with the emergence of ChatGPT, an innovative statistical detection method called DetectGPT [5] has been developed. Its principle is that text generated by the model typically resides in the negative curvature region of the model's log probability. DetectGPT [5] generates and compares multiple variants of model-generated texts to determine whether the texts are machine-generated based on the log probabilities of the original texts and these variants. \n\nDetectGPT [5] outperforms the vast majority of existing zero-shot methods in terms of model sample detection, achieving very high AUC. It is based on this concept that DetectGPT-SC was proposed."], "score": 0.970703125}, {"id": "(Feng et al., 2024)", "paper": {"corpus_id": 272987651, "title": "A Certified Robust Watermark For Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xianheng Feng", "authorId": "2323563488"}, {"name": "Jian Liu", "authorId": "2279667983"}, {"name": "Kui Ren", "authorId": "2249553637"}, {"name": "Chun Chen", "authorId": "2331522408"}], "n_citations": 0}, "snippets": ["AI Generated Text Identification. While large language models are applied to various scenarios with their increasingly powerful text generation capabilities, such as story generation (Yuan et al., 2022), article translation [59] and code assistance [53], the potential risks they bring [7,(Lee et al., 2022)(Yanagi et al., 2020) have made the identi- Direct detection methods often identify AI-generated text by uncovering distinguishing characteristics between AIgenerated and human-written texts or utilizing an extensive dataset of AI-generated text to train a accurate text classifier for AI-generated content. For example, regarding the first kind of method, Hamed and Wu [1] discovered that the bigram similarity of texts generated by ChatGPT is higher than that of human-written texts and developed a corresponding detection method based on this finding. Mitchell et al. found that AIgenerated texts tend to occupy the negative curvature region of the model's log-probability function. Based on this observation they proposed DetectGPT (Mitchell et al., 2023). Building on Mitchell et al.'s approach, Bao et al. [2] improved the perturbation step in their method, significantly enhancing both its accuracy and speed. However, with the rapid advancement in both the size of model and text generation capabilities of LLMs, the gap between AI-generated texts and human-written texts has been narrowing [16,35,38,(Pack et al., 2024). As a result, methods based on text characteristics are becoming increasingly less effective.For the second type of method, Mindner et al. (Mindner et al., 2023) employed multidimensional text feature extraction approaches to construct a classifier, with the best classifier outperforming GPTZero [14] in terms of F1-score. Chen et al. [6] and Liu et al. [32] utilized the advanced language understanding capabilities of pretrained LLMs [33](Raffel et al., 2019), finetuning them as binary classifiers on various text datasets for AI-generated text detection. While these methods perform well on their respective test datasets, their effectiveness may be limited when applied to texts generated by newly emerging models. Likewise, as the capabilities of large language models continue to advance, their effectiveness remains a question. \n\nWatermarking, as an alternative method for AI-generated text identification, is generally more effective, versatile, and interpretable compared to direct detection."], "score": 0.95361328125}], "table": null}, {"title": "Watermarking Techniques", "tldr": "Watermarking embeds invisible signals in AI-generated text that can be algorithmically detected while remaining imperceptible to humans. These techniques include vocabulary partitioning into \"green/red\" lists, semantic embedding approaches, and rejection sampling methods that offer more robust detection than classifier-based approaches. (16 sources)", "text": "\nWatermarking has emerged as a proactive approach to AI-generated text detection that fundamentally differs from classifier and zero-shot methods. Rather than attempting to identify statistical patterns in already-generated text, watermarking embeds detectable signals directly during the text generation process <Paper corpusId=\"258714683\" paperTitle=\"(Yang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. This technique creates text with embedded patterns that remain invisible to human readers but can be algorithmically identified with high confidence <Paper corpusId=\"268819799\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nThe most influential watermarking framework, introduced by Kirchenbauer et al., operates by partitioning the model's vocabulary into \"greenlist\" and \"redlist\" tokens based on a cryptographic hash of previously generated content <Paper corpusId=\"258987266\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>. During generation, the model softly encourages the selection of \"greenlist\" tokens, creating a statistical pattern that can be detected through hypothesis testing with interpretable p-values <Paper corpusId=\"260351507\" paperTitle=\"(Fernandez et al., 2023)\" isShortName></Paper>. This approach requires no access to the language model's API or parameters for detection and has minimal impact on text quality <Paper corpusId=\"273849981\" paperTitle=\"(Lau et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256194179\" paperTitle=\"(Kirchenbauer et al., 2023)\" isShortName></Paper>.\n\nWatermarking techniques generally fall into two categories: token sampling biasing and token probability biasing <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. The first approach forces models to select only \"greenlist\" tokens during generation, while the second introduces a bias to the probability distribution to softly encourage generating \"greenlist\" tokens while preserving semantic richness <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. Methods like NS-Mark constrain biasing frequency, while DiPMark approximates the biased probability distribution to the original to minimize impact on text quality <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nMore recent watermarking innovations have focused on improving robustness against paraphrasing attacks, which represent a significant challenge to token-level watermarks <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>. SemStamp, for example, implements semantic-level watermarking using locality-sensitive hashing to partition the semantic space of sentences, showing greater resistance to paraphrase attacks than token-level approaches <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>. Similarly, SimMark leverages the similarity of semantic sentence embeddings and rejection sampling to impose detectable patterns that remain robust against paraphrasing <Paper corpusId=\"276116307\" paperTitle=\"(Dabiriaghdam et al., 2025)\" isShortName></Paper>.\n\nOther notable watermarking techniques include DeepTextMark, which applies Word2Vec and sentence encoding for watermark insertion and uses a transformer-based classifier for detection <Paper corpusId=\"258588289\" paperTitle=\"(Munyer et al., 2023)\" isShortName></Paper>, and the Adversarial Watermarking Transformer (AWT), which can embed binary messages into text while minimizing changes to semantics and correctness <Paper corpusId=\"258762215\" paperTitle=\"(Lu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"221516138\" paperTitle=\"(Abdelnabi et al., 2020)\" isShortName></Paper>.\n\nDespite their effectiveness, watermarking techniques face several limitations. First, they can only be implemented by the developers of language models, meaning detection depends on model creators choosing to incorporate watermarks <Paper corpusId=\"261697402\" paperTitle=\"(Anand, 2023)\" isShortName></Paper>. Second, watermarks can potentially be removed through text manipulation techniques like paraphrasing, word insertion/deletion, or sentence reordering <Paper corpusId=\"261697402\" paperTitle=\"(Anand, 2023)\" isShortName></Paper>. Finally, there is an inherent trade-off between watermark robustness and text quality, as stronger watermarks may impact the naturalness and fluidity of the generated content <Paper corpusId=\"273901404\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\nOverall, watermarking represents a promising approach for AI-generated text detection that complements classifier-based and zero-shot methods. As language models continue to improve and generate increasingly human-like text, watermarking provides a more reliable detection mechanism that works regardless of how natural the text appears <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Yang et al., 2023)", "paper": {"corpus_id": 258714683, "title": "Watermarking Text Generated by Black-Box Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xi Yang", "authorId": "2143365293"}, {"name": "Kejiang Chen", "authorId": "8780109"}, {"name": "Weiming Zhang", "authorId": "51027868"}, {"name": "Chang-rui Liu", "authorId": "40997227"}, {"name": "Yuang Qi", "authorId": "2217971466"}, {"name": "Jie Zhang", "authorId": "2155863060"}, {"name": "Han Fang", "authorId": "12464207"}, {"name": "Neng H. Yu", "authorId": "2052212945"}], "n_citations": 62}, "snippets": ["Recent advances in large language models (LLMs) have enabled them to reach human-level proficiency across numerous professional and academic tasks [24,(Ouyang et al., 2022)[33]. One of the most impressive examples is OpenAI's ChatGPT [23], which has demonstrated remarkable prowess in answering questions, composing emails, essays, and even generating code. However, this impressive ability to create human-like text with remarkable efficiency has ignited apprehension regarding the potential abuse of LLMs for malicious purposes [6,(2023)(Liebrenz et al., 2023)(Stokel-Walker, 2022), such as phishing, disinformation campaigns, and academic dishonesty", "Therefore, detecting and authenticating generated text becomes crucial to ensure the responsible and secure use of LLMs.\n\nA prevalent solution is passive detection [1, 10,18,25,28,40], where a text classifier, usually fine-tuned on a pretrained language model like RoBERTa [16] and GPT-2 [30], is adopted to distinguish between generated and human-written text. However, these learning-based methods perform well only when the input data share a similar distribution with the training data, thereby limiting their applicability to specific domains.\n\nStatistical Discrepancy Detection. Several methods distinguish between generated and human-written text by identifying statistical discrepancies between them, as exemplified by two recent tools: GPTZero [40] and DetectGPT [18]. GPTZero uses perplexity and burstiness to tell apart human-written and generated text, as language models tend to produce more predictable and consistent text based on the patterns they learned from training data, resulting in lower perplexity scores for generated text. DetectGPT exploits the negative curvature regions of a model's log probability function to identify generated text by comparing the log probability of unperturbed and perturbed text variations. However, as language models are constantly improving and becoming more sophisticated, these heuristic features struggle to achieve robustness and generalization. Deep Learning-based Detection. Deep learning-based methods rely on gathering human-written and generated samples to train classifiers. Recently, OpenAI fine-tuned a GPT model for this discrimination task using a dataset comprising paired human and AI-generated texts on identical topics [25]. Similarly, Guo et al. [10] fine-tuned a text classifier based on pre-trained autoencoding LLMs (e.g., RoBERTa) by collecting the Human ChatGPT Comparison Corpus (HC3). Deep learning-based methods exhibit strong performance under the training data distribution, but they are susceptible to adversarial attacks, lack interpretability, and struggle to provide reliable judgments in human-AI collaboration scenarios. Watermark-based Detection. Kirchenbauer et al. [12] proposed the watermarking framework for white-box language models. The watermarking operates by randomly selecting a random set of \"greenlist\" words from the model's vocabulary and softly encouraging the use of these \"greenlist\" words by interfering with the sampling process at each generation step. The watermark can be detected by testing the following null hypothesis,  0 : The text sequence is generated with no knowledge of the selection rule of \"greenlist\" words."], "score": 0.97021484375}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258967971, "title": "Baselines for Identifying Watermarked Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Leonard Tang", "authorId": "2144484107"}, {"name": "Gavin Uberti", "authorId": "2143997407"}, {"name": "Tom Shlomi", "authorId": "2211332429"}], "n_citations": 3}, "snippets": ["Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."], "score": 0.970703125}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 268819799, "title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xiang Li", "authorId": "2294777336"}, {"name": "Feng Ruan", "authorId": "2294359739"}, {"name": "Huiyuan Wang", "authorId": "2294380671"}, {"name": "Qi Long", "authorId": "2055685346"}, {"name": "Weijie J. Su", "authorId": "2288099064"}], "n_citations": 22}, "snippets": ["Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart."], "score": 0.96044921875}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 258987266, "title": "Red Teaming Language Model Detectors with Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Zhouxing Shi", "authorId": "2987927"}, {"name": "Yihan Wang", "authorId": "2108927851"}, {"name": "Fan Yin", "authorId": "2065089223"}, {"name": "Xiangning Chen", "authorId": "2143737082"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}, {"name": "Cho-Jui Hsieh", "authorId": "1793529"}], "n_citations": 56}, "snippets": ["Recent detectors for AI-generated text mostly fall into three categories. First, classifier-based detectors are trained with labeled data to distinguish human-written text and AI-generated text. For example, the AI Text Classifier developed by OpenAI (OpenAI, 2023a) is a fine-tuned language model. Second, watermarking methods introduce distinct patterns into AI-generated text, allowing for its identification. Among them, (Kirchenbauer et al., 2023) randomly partition the vocabulary into a greenlist and a redlist during the generation, where the division is based on the hash of the previously generated tokens. The language model only uses words in the greenlists, and thereby the generated text has a different pattern compared to human-written text which does not consider such greenlists and redlists. Third, De-tectGPT (Mitchell et al., 2023) uses the likelihood of the generated text for the detection, as they find that text generated by language models tends to reside in the negative curvature region of the log probability function. Consequently, they define a curvature-based criterion for the detection."], "score": 0.9453125}, {"id": "(Kirchenbauer et al., 2023)", "paper": {"corpus_id": 256194179, "title": "A Watermark for Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "John Kirchenbauer", "authorId": "2166053502"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Yuxin Wen", "authorId": "123191916"}, {"name": "Jonathan Katz", "authorId": "143975296"}, {"name": "Ian Miers", "authorId": "2679804"}, {"name": "T. Goldstein", "authorId": "1962083"}], "n_citations": 507}, "snippets": ["Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security."], "score": 0.0}, {"id": "(Fernandez et al., 2023)", "paper": {"corpus_id": 260351507, "title": "Three Bricks to Consolidate Watermarks for Large Language Models", "year": 2023, "venue": "International Workshop on Information Forensics and Security", "authors": [{"name": "Pierre Fernandez", "authorId": "2147013351"}, {"name": "Antoine Chaffin", "authorId": "2129106958"}, {"name": "Karim Tit", "authorId": "2144093662"}, {"name": "Vivien Chappelier", "authorId": "2686030"}, {"name": "T. Furon", "authorId": "1775704"}], "n_citations": 55}, "snippets": ["Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking."], "score": 0.9736328125}, {"id": "(Lau et al., 2024)", "paper": {"corpus_id": 273849981, "title": "Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection", "year": 2024, "venue": "Natural Language Processing Journal", "authors": [{"name": "Hiu Ting Lau", "authorId": "2329369337"}, {"name": "A. Zubiaga", "authorId": "2805349"}], "n_citations": 1}, "snippets": ["Currently, there are 2 major streams of LLM-generated text detectors: (i) zero-shot classifiers [25,33], which identify LLM-generated text based on the pattern and characteristics of the input, and (ii) watermark detectors [17], which rely on detecting the presence of watermarks which are imprinted into the text during the generation process (Kirchenbauer et al., 2023), and are effective in the cases where the watermarks have been added by the LLM. The detectors then examine the input, classifying it as LLM-generated if the level of watermarking exceeds a set threshold, or as human-generated otherwise."], "score": 0.9638671875}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 273901404, "title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yuxuan Guo", "authorId": "2330513747"}, {"name": "Zhiliang Tian", "authorId": "2260832680"}, {"name": "Yiping Song", "authorId": "2261069334"}, {"name": "Tianlun Liu", "authorId": "2329747280"}, {"name": "Liang Ding", "authorId": "46573238"}, {"name": "Dongsheng Li", "authorId": "2163335292"}], "n_citations": 2}, "snippets": ["Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \"green-red\" list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text.\n\nCurrent watermark methods mainly inject the signature according to \"green-red list\" (Ren et al., 2023): they divide vocabulary into green/red lists, regard words in green lists as the unique signature, and encourage to generate green tokens, which is easy to be recognized. The methods can be divided into two categories: (1) Token sampling biasing-based watermark forces LLMs to select only green tokens during generation. EXP-Edit (Kuditipudi et al., 2023) intervenes in the sampling process of each token. However, forcing LLMs to sample green tokens restricts the semantic richness of LLM-generated text, thus undermining its text quality. (2) To improve the generation quality, researchers further propose token probability biasing-based watermark, which enriches the semantics of watermarked texts by introducing a bias to the probability distribution to softly encourage generating green tokens instead of restricting to select green tokens. Takezawa et al. (2023) proposed NS-Mark to constrain the frequency of biasing. Wu et al. (2023) introduced DiPMark to approximate the biased probability distribution to the original one. These methods mitigate the impact of biasing on text quality and ensure superior text quality."], "score": 0.9638671875}, {"id": "(Cao, 2025)", "paper": {"corpus_id": 277595847, "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lele Cao", "authorId": "2354295905"}], "n_citations": 0}, "snippets": ["One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."], "score": 0.95458984375}, {"id": "(Hou et al., 2024)", "paper": {"corpus_id": 263831179, "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "A. Hou", "authorId": "2257002716"}, {"name": "Jingyu (Jack) Zhang", "authorId": "2167508843"}, {"name": "Tianxing He", "authorId": "2249540815"}, {"name": "Yichen Wang", "authorId": "2125062703"}, {"name": "Yung-Sung Chuang", "authorId": "2257002712"}, {"name": "Hongwei Wang", "authorId": "2257103558"}, {"name": "Lingfeng Shen", "authorId": "2121272448"}, {"name": "Benjamin Van Durme", "authorId": "7536576"}, {"name": "Daniel Khashabi", "authorId": "1783281"}, {"name": "Yulia Tsvetkov", "authorId": "2249583325"}], "n_citations": 66}, "snippets": ["Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \"bigram paraphrase\" attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation."], "score": 0.0}, {"id": "(Dabiriaghdam et al., 2025)", "paper": {"corpus_id": 276116307, "title": "SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Amirhossein Dabiriaghdam", "authorId": "2210267013"}, {"name": "Lele Wang", "authorId": "2294899071"}], "n_citations": 1}, "snippets": ["The rapid proliferation of large language models (LLMs) has created an urgent need for reliable methods to detect whether a text is generated by such models. In this paper, we propose SimMark, a posthoc watermarking algorithm that makes LLMs' outputs traceable without requiring access to the model's internal logits, enabling compatibility with a wide range of LLMs, including API-only models. By leveraging the similarity of semantic sentence embeddings and rejection sampling to impose detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks."], "score": 0.98388671875}, {"id": "(Munyer et al., 2023)", "paper": {"corpus_id": 258588289, "title": "DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Travis J. E. Munyer", "authorId": "2106504469"}, {"name": "Xin Zhong", "authorId": "2113310369"}], "n_citations": 32}, "snippets": ["Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously."], "score": 0.96337890625}, {"id": "(Lu et al., 2023)", "paper": {"corpus_id": 258762215, "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Ning Lu", "authorId": "2147128045"}, {"name": "Shengcai Liu", "authorId": "2152939552"}, {"name": "Ruidan He", "authorId": "22272507"}, {"name": "Ke Tang", "authorId": "2078692567"}], "n_citations": 53}, "snippets": ["In recent years, the research community has developed a wide range of detectors for AI-generated contents. In general, these detectors can be classified into three categories: training-based, statistical, and watermarking methods. Training-based methods treat the detection problem as a binary classification task, where neural networks are trained using AI-generated text and human-written text. Early studies utilized classifiers to identify fake reviews (Hovy, 2016) and fake news (Zellers et al., 2019). More recently, researchers have trained classifiers using text generated by LLMs, such as the GPT-3.5 detector (Guo et al., 2023) and GPT-2 detector (Solaiman et al., 2019). Statistical methods, on the other hand, focus on zero-shot detection without any additional training overhead. These methods seek to distinguish between human-written text and AI-generated text based on the statistical characteristics of text, such as the statistical irregularities in measures like entropy (Lavergne et al., 2008), perplexity (Beresneva, 2016) and token rank (Gehrmann et al., 2019). A recent method, DetectGPT (Mitchell et al., 2023), exploits the phenomenon that AI-generated text tends to lie in the negative curvature regions of log probability of text. The watermarking methods involve modifying the LLM's text generation process to imprint specific patterns on the generated text, such that it can be detected (Abdelnabi et al., 2020)Grinbaum & Adomaitis, 2022;Kirchenbauer et al., 2023)."], "score": 0.9609375}, {"id": "(Abdelnabi et al., 2020)", "paper": {"corpus_id": 221516138, "title": "Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding", "year": 2020, "venue": "IEEE Symposium on Security and Privacy", "authors": [{"name": "Sahar Abdelnabi", "authorId": "1383113350"}, {"name": "Mario Fritz", "authorId": "1739548"}], "n_citations": 152}, "snippets": ["Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks."], "score": 0.0}, {"id": "(Anand, 2023)", "paper": {"corpus_id": 261697402, "title": "Studying the impacts of pre-training using ChatGPT-generated text on downstream tasks", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Sarthak Anand", "authorId": "2313731991"}], "n_citations": 0}, "snippets": ["Watermarking in text generation is a technique that introduces hidden signals into generated text, which can be used to detect whether the text is artificial. In this approach, the language model generates a probability distribution for the next word based on a given sequence of words. The next word is then randomly selected from this distribution. The watermarking technique operates by pseudo-randomly sampling words from the distribution, thereby enabling the detection of text generated by the language model.\n\nIn a recent study conducted by Kirchenbauer et al. [33], the effectiveness of the watermarking technique was evaluated on a multi-billion parameter model from the Open Pre-trained Transformer (OPT) family. The proposed method involved the selection of randomized sets of green tokens (allowed tokens) and red tokens (restricted tokens) during the generation process. The use of green tokens was promoted while sampling, while red tokens were discouraged. This approach facilitated the detection of artificial text by counting the occurrences of red tokens. In a similar direction, Ope-nAI has also expressed intentions to incorporate watermarking into its language models.\n\nAs outlined in a blog post by Scott Aaronson 6 , OpenAI plans to use cryptographic pseudo-random functions to sample words and generate detectors using cryptographic keys. This approach aims to embed watermarks into the language model, allowing for the identification of artificial text.\n\nIn practice, evading watermarking detection in language models can be achieved through various techniques, such as inserting or deleting words randomly, rearranging sentence order, or paraphrasing text. These methods pose challenges to the effectiveness of watermarking as a detection mechanism. Additionally, the responsibility of embedding watermarking lies with the developer of the language model, and only they have the ability to build the corresponding detection models. This limitation means that watermarking can only mitigate the potential harms of language models to a certain extent, as it relies on the actions and choices of the developer."], "score": 0.95166015625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 273228395, "title": "Signal Watermark on Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhenyu Xu", "authorId": "2041278503"}, {"name": "Victor S. Sheng", "authorId": "2278314006"}], "n_citations": 0}, "snippets": ["As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."], "score": 0.97705078125}], "table": null}, {"title": "Notable Detection Tools and Benchmarks", "tldr": "Several detection tools and benchmarks have emerged to identify AI-generated text, ranging from commercial solutions to open-source frameworks. These tools employ various approaches including statistical analysis, supervised learning, and zero-shot detection, with notable examples like DetectGPT, GPTZero, Ghostbuster, and AuthentiGPT demonstrating different strengths and limitations across evaluation datasets. (10 sources)", "text": "\n## Detection Tools\n\n- **DetectGPT**: A zero-shot detection method that leverages the negative curvature regions of a language model's log probability function. It identifies AI-generated text by comparing the log probability of original text with perturbed versions, achieving 0.95 AUROC for fake news articles generated by GPT-NeoX-20B. <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>\n\n- **GPTZero**: Uses perplexity and burstiness metrics to distinguish between human and machine text. Lower perplexity and burstiness typically indicate machine-generated content. This tool is particularly geared toward educators. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266690717\" paperTitle=\"(Caiado et al., 2023)\" isShortName></Paper>\n\n- **Ghostbuster**: A state-of-the-art system that passes documents through weaker language models, searches for feature combinations, and trains a classifier to predict AI-generated text. It achieves 99.0 F1 score when evaluated across domains, outperforming previous models by 5.9 F1. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274581321\" paperTitle=\"(Tufts et al., 2024)\" isShortName></Paper>\n\n- **AuthentiGPT**: An efficient classifier that distinguishes between machine-generated and human-written texts with only one trainable parameter. It works by leveraging a black-box LLM to denoise input text with artificially added noise, then semantically compares the denoised text with the original. <Paper corpusId=\"265157663\" paperTitle=\"(Guo et al., 2023)\" isShortName></Paper>\n\n- **GLTR (Giant Language model Test Room)**: A tool designed to support non-experts in detecting generated text by applying baseline statistical methods that can detect generation artifacts across multiple sampling schemes. It improves human detection rates of fake text from 54% to 72% without prior training. <Paper corpusId=\"182952848\" paperTitle=\"(Gehrmann et al., 2019)\" isShortName></Paper>\n\n- **OpenAI's AI Text Classifier**: A fine-tuned language model developed by OpenAI to distinguish between human and AI-generated text. However, its effectiveness has been questioned as language models become increasingly human-like. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\n- **RADAR**: A framework that jointly trains a paraphraser and a detector in an adversarial setting to create a robust AI-text detector. The paraphraser generates realistic content to evade detection, while the detector learns to identify even adversarially modified texts. <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>\n\n- **Other Commercial Tools**: Several commercial detection tools targeted at educators include Copyleaks AI Content Detector, Crossplag, Originality.ai, Turnitin AI Detection, and ZeroGPT. <Paper corpusId=\"266690717\" paperTitle=\"(Caiado et al., 2023)\" isShortName></Paper>\n\n## Benchmarks and Datasets\n\n- **Ghostbuster Benchmarks**: Three new datasets of human and AI-generated text in student essays, creative writing, and news articles domains, released alongside the Ghostbuster detection system. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper>\n\n- **Human ChatGPT Comparison Corpus (HC3)**: A dataset collected to fine-tune RoBERTa for AI-generated text detection, specifically focused on ChatGPT-generated content. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\n- **Mindner et al. Corpus**: A text corpus covering 10 school topics used to classify human-generated vs. AI-generated texts, with their best systems achieving F1-scores over 96% for basic classification and more than 78% for human/AI-rephrased text classification. \n\n## Comparative Performance\n\n- Zero-shot methods like DetectGPT show strong performance (0.95 AUROC) without requiring training data, but can be computationally expensive as they require numerous queries to the source model. <Paper corpusId=\"256274849\" paperTitle=\"(Mitchell et al., 2023)\" isShortName></Paper>\n\n- Supervised approaches like Ghostbuster demonstrate superior performance (99.0 F1) when evaluated across domains and show better generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1) compared to previous approaches. <Paper corpusId=\"258865787\" paperTitle=\"(Verma et al., 2023)\" isShortName></Paper>\n\n- As language models continue to advance, detection tools based on variance in perplexity between human and AI-generated text are becoming less effective, making watermarking techniques increasingly valuable as a stable and reliable verification method. <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>\n\n- Most current detectors face challenges with out-of-distribution text, adversarial attacks, and human-AI collaboration scenarios where text may be partially generated or edited. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264439054\" paperTitle=\"(Ghosal et al., 2023)\" isShortName></Paper>\n\n- Recent research indicates that for sufficiently advanced language models, machine-generated text detectors may offer only marginal improvement over random classifiers, highlighting the need for continued innovation in detection methodologies. <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>", "citations": [{"id": "(Mitchell et al., 2023)", "paper": {"corpus_id": 256274849, "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "E. Mitchell", "authorId": "49688913"}, {"name": "Yoonho Lee", "authorId": "2110392124"}, {"name": "Alexander Khazatsky", "authorId": "121873407"}, {"name": "Christopher D. Manning", "authorId": "144783904"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 627}, "snippets": ["In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT."], "score": 0.9609375}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258967971, "title": "Baselines for Identifying Watermarked Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Leonard Tang", "authorId": "2144484107"}, {"name": "Gavin Uberti", "authorId": "2143997407"}, {"name": "Tom Shlomi", "authorId": "2211332429"}], "n_citations": 3}, "snippets": ["Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."], "score": 0.970703125}, {"id": "(Caiado et al., 2023)", "paper": {"corpus_id": 266690717, "title": "AI Content Self-Detection for Transformer-based Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Antonio Junior Alves Caiado", "authorId": "2277214778"}, {"name": "Michael Hahsler", "authorId": "2277214594"}], "n_citations": 1}, "snippets": ["Many tools to detect AI-generated text are now offered. Some popular tools geared toward educators are Copyleaks AI Content Detector, Crossplag, GPTZero, Hugging Face OpenAI Detector, Originality.ai, Turnitin AI Detection and ZeroGPT. The list of detectors and their capability is constantly changing following the fast-paced changes seen in the development of large language models.\n\nMost tools are based on detecting artifacts of the text generation process, including word choice, writing style, sentence length, and many more. A report by Open AI [9] lays out three AI content detection strategies, including a simple classifier learned from scratch, a classifier resulting from fine-tuning an existing language model, or using the probabilities assigned by the model to strings. Many existing tools follow the first two approaches. For example, the Hugging Face Open AI detector is a transformer-based classifier that is fine-tuned to detect GPT-2 text. Self-detection introduced in this paper is most closely related to the third approach. However, it does not require access to the model parameters to assess probabilities. It relies on the model itself to perform the detection."], "score": 0.94189453125}, {"id": "(Verma et al., 2023)", "paper": {"corpus_id": 258865787, "title": "Ghostbuster: Detecting Text Ghostwritten by Large Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "V. Verma", "authorId": "2143413953"}, {"name": "Eve Fleisig", "authorId": "1988380666"}, {"name": "Nicholas Tomlin", "authorId": "1391385991"}, {"name": "D. Klein", "authorId": "38666915"}], "n_citations": 60}, "snippets": ["We introduce Ghostbuster, a state-of-the-art system for detecting AI-generated text.Our method works by passing documents through a series of weaker language models, running a structured search over possible combinations of their features, and then training a classifier on the selected features to predict whether documents are AI-generated.Crucially, Ghostbuster does not require access to token probabilities from the target model, making it useful for detecting text generated by black-box or unknown models.In conjunction with our model, we release three new datasets of human- and AI-generated text as detection benchmarks in the domains of student essays, creative writing, and news articles. We compare Ghostbuster to several existing detectors, including DetectGPT and GPTZero, as well as a new RoBERTa baseline. Ghostbuster achieves 99.0 F1 when evaluated across domains, which is 5.9 F1 higher than the best preexisting model. It also outperforms all previous approaches in generalization across writing domains (+7.5 F1), prompting strategies (+2.1 F1), and language models (+4.4 F1)."], "score": 0.9482421875}, {"id": "(Tufts et al., 2024)", "paper": {"corpus_id": 274581321, "title": "A Practical Examination of AI-Generated Text Detectors for Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Brian Tufts", "authorId": "2334353491"}, {"name": "Xuandong Zhao", "authorId": "150345512"}, {"name": "Lei Li", "authorId": "2257344711"}], "n_citations": 2}, "snippets": ["Many methods have been released recently that claim to have a strong ability to detect the difference between AI-generated and human-generated texts. These detectors primarily fall into three categories: trained detectors, zero-shot detectors, and watermarking techniques (Yang et al., 2023)(Ghosal et al., 2023)Tang et al., 2023). Trained detectors utilize datasets of human and AI-generated texts and train a binary classification model to detect the source of a text (Zellers et al., 2019)(Hovy, 2016)(Hu et al., 2023)Tian and Cui, 2023;(Verma et al., 2023). Zero-shot detection utilizes a language model's inherent traits to identify text it generates, without explicit training for detection tasks other than calibrating a threshold for detection in some cases (Gehrmann et al., 2019;Mitchell et al., 2023;Bao et al., 2024;Yang et al., 2023a;Venkatraman et al., 2024)."], "score": 0.94873046875}, {"id": "(Guo et al., 2023)", "paper": {"corpus_id": 265157663, "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Zhen Guo", "authorId": "2266505035"}, {"name": "Shangdi Yu", "authorId": "2266834515"}], "n_citations": 11}, "snippets": ["One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood."], "score": 0.9443359375}, {"id": "(Gehrmann et al., 2019)", "paper": {"corpus_id": 182952848, "title": "GLTR: Statistical Detection and Visualization of Generated Text", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Sebastian Gehrmann", "authorId": "3159346"}, {"name": "Hendrik Strobelt", "authorId": "2879705"}, {"name": "Alexander M. Rush", "authorId": "2531268"}], "n_citations": 546}, "snippets": ["The rapid improvement of language models has raised the specter of abuse of text generation systems. This progress motivates the development of simple methods for detecting generated text that can be used by non-experts. In this work, we introduce GLTR, a tool to support humans in detecting whether a text was generated by a model. GLTR applies a suite of baseline statistical methods that can detect generation artifacts across multiple sampling schemes. In a human-subjects study, we show that the annotation scheme provided by GLTR improves the human detection-rate of fake text from 54% to 72% without any prior training. GLTR is open-source and publicly deployed, and has already been widely used to detect generated outputs."], "score": 0.0}, {"id": "(Hu et al., 2023)", "paper": {"corpus_id": 259501842, "title": "RADAR: Robust AI-Text Detection via Adversarial Learning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xiaomeng Hu", "authorId": "2199229406"}, {"name": "Pin-Yu Chen", "authorId": "2158177948"}, {"name": "Tsung-Yi Ho", "authorId": "2103197703"}], "n_citations": 127}, "snippets": ["Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 273228395, "title": "Signal Watermark on Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhenyu Xu", "authorId": "2041278503"}, {"name": "Victor S. Sheng", "authorId": "2278314006"}], "n_citations": 0}, "snippets": ["As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."], "score": 0.97705078125}, {"id": "(Ghosal et al., 2023)", "paper": {"corpus_id": 264439054, "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Soumya Suvra Ghosal", "authorId": "2655959"}, {"name": "Souradip Chakraborty", "authorId": "49081354"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Furong Huang", "authorId": "2261325066"}, {"name": "Dinesh Manocha", "authorId": "2172597446"}, {"name": "A. S. Bedi", "authorId": "3387859"}], "n_citations": 37}, "snippets": ["Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection. To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection."], "score": 0.0}], "table": null}, {"title": "Limitations and Challenges", "tldr": "AI-generated text detection faces significant challenges including diminishing effectiveness as LLMs become more human-like, vulnerability to adversarial attacks, and difficulties with short texts and mixed human-AI content. Detection methods also struggle with out-of-distribution scenarios, cross-model generalization, and face inherent trade-offs between detection accuracy and computational efficiency. (11 sources)", "text": "\nAs detection methods for AI-generated text continue to evolve, they face several significant limitations and challenges that impact their practical effectiveness and reliability:\n\n## Diminishing Effectiveness with Advanced LLMs\n\nA fundamental challenge for detection is that as language models become more sophisticated, the statistical differences between human and AI-generated text continue to narrow. Sadasivan et al. demonstrated theoretically that for sufficiently advanced language models, machine-generated text detectors offer only marginal improvement over random classifiers <Paper corpusId=\"258967971\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. This suggests an inherent limit to detection approaches that rely on statistical patterns to differentiate between human and machine writing.\n\n## Vulnerability to Adversarial Attacks\n\nMost detection methods are highly vulnerable to adversarial attacks and evasion techniques. Simple modifications like paraphrasing can significantly reduce detection accuracy <Paper corpusId=\"259501842\" paperTitle=\"(Hu et al., 2023)\" isShortName></Paper>. Several studies have documented methods to circumvent popular detection tools like GPTZero, with one study highlighting three distinct approaches to bypass detection <Paper corpusId=\"268357168\" paperTitle=\"(Munyer et al._1, 2023)\" isShortName></Paper>. This vulnerability extends to watermarking techniques as well, with semantic-level watermarks like SemStamp being developed specifically to address vulnerabilities to paraphrase attacks <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"263831179\" paperTitle=\"(Hou et al., 2024)\" isShortName></Paper>.\n\n## Challenges with Short Texts and Mixed Content\n\nDetection methods typically require substantial text length to function effectively. GPTZero, for example, requires a minimum of 250 characters to begin detection <Paper corpusId=\"268357168\" paperTitle=\"(Munyer et al._1, 2023)\" isShortName></Paper>. This limitation is particularly problematic for short-form content like social media posts, reviews, or brief communications. Additionally, detection becomes significantly more difficult with mixed human-AI collaboration, where parts of a text might be human-written and others AI-generated <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper>.\n\n## Out-of-Distribution Performance\n\nMost detector models perform well only within their training distribution but struggle significantly with out-of-distribution texts. Zero-shot approaches are particularly sensitive to out-of-distribution scenarios <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. This limitation is exacerbated by the rapid evolution of language models, making it difficult for detection systems to keep pace with new generation techniques and model architectures.\n\n## Model-Specific Limitations\n\nMany detection approaches are optimized for specific language models or generation techniques, limiting their generalizability. While some tools like Binoculars claim to detect machine text from various LLMs without model-specific modifications <Paper corpusId=\"277595847\" paperTitle=\"(Cao, 2025)\" isShortName></Paper> <Paper corpusId=\"267068867\" paperTitle=\"(Hans et al., 2024)\" isShortName></Paper>, most detectors show reduced effectiveness when faced with unfamiliar models or novel generation strategies <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\n## Computational Resource Requirements\n\nMethods like DetectGPT and RoBERTa-QA, which require multiple text-generation processes and model retraining, consume substantial computational resources and time <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This computational burden makes real-time detection challenging, especially for applications requiring immediate verification of large volumes of content.\n\n## Trade-offs Between Approaches\n\nEach detection approach involves inherent trade-offs. Supervised classifiers require extensive training data but may be more accurate within their training distribution. Zero-shot methods eliminate the need for training data but may be less effective overall. Watermarking provides a more reliable verification method but requires implementation at the model level and can impact text quality <Paper corpusId=\"273228395\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266369103\" paperTitle=\"(Mubarak et al., 2023)\" isShortName></Paper>.\n\n## Implementation Barriers for Watermarking\n\nWhile watermarking techniques offer a promising direction for verification, they can only be implemented by the developers of language models, limiting their universal applicability <Paper corpusId=\"273901392\" paperTitle=\"(Nguyen-Son et al., 2024)\" isShortName></Paper>. Additionally, watermarking faces challenges with factual responses where there are fewer options to control the output and is highly sensitive to even minor edits of the watermarked text <Paper corpusId=\"276773353\" paperTitle=\"(Bitton et al., 2025)\" isShortName></Paper> <Paper corpusId=\"273547059\" paperTitle=\"(Dathathri et al., 2024)\" isShortName></Paper>.\n\nThese challenges underscore the need for continued innovation in detection methodologies, particularly approaches that can remain effective as language models evolve and that can resist various forms of evasion and adversarial manipulation.", "citations": [{"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258967971, "title": "Baselines for Identifying Watermarked Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Leonard Tang", "authorId": "2144484107"}, {"name": "Gavin Uberti", "authorId": "2143997407"}, {"name": "Tom Shlomi", "authorId": "2211332429"}], "n_citations": 3}, "snippets": ["Recent methods such as DetectGPT and GPTZero distinguish between machine-generated and human-written text by analyzing their statistical discrepancies (Tian, 2023;Mitchell et al., 2023). DetectGPT compares the log probability computed by a model on unperturbed text and perturbed variations, leveraging the observation that text sampled from a LLM generally occupy negative curvature regions of the model's log probability function. GPTZero instead uses perplexity and burstiness to distinguish human from machine text, with lower perplexity and burstiness indicating a greater likelihood of machine-generated text. However, these heuristics do not generalize and are often fallible.\n\nDetection by Learning Classifiers Several papers have proposed to train classifiers to distinguish between AI and human generated text. During the initial GPT-2 release, OpenAI trained a RoBERTa classifier to detect GPT-2 generated text with 95% accuracy (Solaiman et al., 2019). More recently, OpenAI fine-tuned a GPT model on a dataset of machine-generated and human texts focusing on the same topic, with a true positive identification rate of 26% (Ope-nAI, 2023). Similarly, Guo et al. (2023) collected the Human ChatGPT Comparison Corpuse (HC3) and fine-tuned RoBERTa for the detection task.\n\nNotably, the capabilities of such classifiers decrease as machine-generated text becomes increasingly human-like. Sadasivan et al. (2023) show theoretically that for sufficiently advanced language models, machine-generated text detectors offer only a marginal improvement over random classifiers. Moreover, such methods are prone to adversarial attacks and are not robust to out-of-distribution text.\n\nWatermarking Large Language Models An alternative to detecting of machine-generated text using statistical discrepancies and learned classifiers is the concept of watermarks. Watermarks are hidden patterns in machinegenerated text that are imperceptible to humans, but algorithmically identifiable as synthetic. Natural language watermarks long predate the development of LLMs, relying on methods such as synonym substitution, as well as syntactic and semantic transformations (Topkara et al., 2005)."], "score": 0.970703125}, {"id": "(Hu et al., 2023)", "paper": {"corpus_id": 259501842, "title": "RADAR: Robust AI-Text Detection via Adversarial Learning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xiaomeng Hu", "authorId": "2199229406"}, {"name": "Pin-Yu Chen", "authorId": "2158177948"}, {"name": "Tsung-Yi Ho", "authorId": "2103197703"}], "n_citations": 127}, "snippets": ["Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusations of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a robust AI-text detector via adversarial learning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic content to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly 2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets, experimental results show that RADAR significantly outperforms existing AI-text detection methods, especially when paraphrasing is in place. We also identify the strong transferability of RADAR from instruction-tuned LLMs to other LLMs, and evaluate the improved capability of RADAR via GPT-3.5-Turbo."], "score": 0.0}, {"id": "(Munyer et al._1, 2023)", "paper": {"corpus_id": 268357168, "title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Travis J. E. Munyer", "authorId": "2106504469"}, {"name": "A. Tanvir", "authorId": "2215112081"}, {"name": "A. Das", "authorId": "2107200061"}, {"name": "Xin Zhong", "authorId": "2257984128"}], "n_citations": 15}, "snippets": ["Recent endeavors have been directed towards developing classifiers aimed at differentiating between LLM-generated text and human-written text.The prevailing approach entails the collection and labeling of LLM-generated and humanwritten texts, followed by the training of a binary classifier through supervised learning.Although the efficacy of these classifiers has yet to be fully established, some preliminary analyses have been reported [8,9].One study [9] elucidated three distinct methods, substantiated with examples, to circumvent the GPTZero [7] classifier detection.Another investigation [8] conducted a direct assessment of GPTZero's accuracy, uncovering inconsistencies in its ability to detect human-written text.Moreover, classifier-based LLM-generated text detectors commonly necessitate a substantial character count to perform detection accurately.For instance, GPTZero [7] required a minimum of 250 characters to initiate detection.Looking ahead, OpenAI is planning a cryptography-based watermarking system for ChatGPTgenerated text detection [15], although no definitive work has been disclosed as of yet.Zero-shot learning-based methods have also demonstrated some advancement.For example, Cer et al. [16] reported an increment in AUROC from 1% to 14% compared to other zero-shot detection strategies across various datasets; however, the accuracy might still fall short in real-world applications concerning text generated by models.\n\nA method has been proposed for detecting LLM-generated texts based on text watermarking (Kirchenbauer et al., 2023), which involves watermarking the text by modifying the LLMs (sensitive tokens are defined and excluded from the output of the LLMs).In contrast, our proposed DeepTextMark does not necessitate access to or modifications of the LLM.Distinct from model-dependent methods, DeepTextMark exhibits a model-independent feature, enabling its application to any text.Moreover, DeepTextMark employs a substantially more compact architecture with about 50 million parameters, whereas the method in [11] necessitates billions of parameters to implement the watermarking process."], "score": 0.958984375}, {"id": "(Cao, 2025)", "paper": {"corpus_id": 277595847, "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lele Cao", "authorId": "2354295905"}], "n_citations": 0}, "snippets": ["One popular approach leverages an external LLM to classify whether a piece of text is AI-generated or not [12]. With carefully crafted prompts, these zero-shot methods can achieve moderate accuracy quickly. However, sensitivity to prompt design and the adversarial gap between the LLM detector and the text generator are common issues [22].\n\n2) Linguistic and statistical signatures: Traditional stylometric features (e.g., function words, syntax complexity, average phrase length) have long been used in authorship attribution [23], [24]. More modern detection focuses on computing perplexity or log-likelihood using reference language models, observing that LLM-generated text tends to show distinctive probability distributions. Additionally, specialized white-box methods can measure rank ordering of tokens if the generating model is partially known [25].\n\n3) Supervised classification (training-based): Labeled corpora of AI versus human text enable fine-tuning of large pre-trained transformers like RoBERTa or T5 to discriminate synthetic text [26], [27]. Researchers improve robustness with adversarial training sets that contain paraphrased or AIgenerated passages shifted in style. Tools like GPTZero [28] and RADAR [29] exemplify advanced supervised detectors. However, assembling high-quality, representative training data remains a challenge, especially as new generator architectures emerge frequently.\n\n4) Watermarking for AI text: Cooperative watermarking modifies text generation at token selection time, embedding an imperceptible pattern in the distribution of words or punctuation [30], [31]. A verifier can detect such patterns after the fact. While promising for major industrial LLMs that adopt the standard, watermarking fails if malicious or open-source models do not embed it, or if paraphrasing disrupts the signal [32], [33]."], "score": 0.95458984375}, {"id": "(Hou et al., 2024)", "paper": {"corpus_id": 263831179, "title": "SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "A. Hou", "authorId": "2257002716"}, {"name": "Jingyu (Jack) Zhang", "authorId": "2167508843"}, {"name": "Tianxing He", "authorId": "2249540815"}, {"name": "Yichen Wang", "authorId": "2125062703"}, {"name": "Yung-Sung Chuang", "authorId": "2257002712"}, {"name": "Hongwei Wang", "authorId": "2257103558"}, {"name": "Lingfeng Shen", "authorId": "2121272448"}, {"name": "Benjamin Van Durme", "authorId": "7536576"}, {"name": "Daniel Khashabi", "authorId": "1783281"}, {"name": "Yulia Tsvetkov", "authorId": "2249583325"}], "n_citations": 66}, "snippets": ["Existing watermarked generation algorithms employ token-level designs and therefore, are vulnerable to paraphrase attacks. To address this issue, we introduce watermarking on the semantic representation of sentences. We propose SemStamp, a robust sentence-level semantic watermarking algorithm that uses locality-sensitive hashing (LSH) to partition the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by a language model, and conducts rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. To test the paraphrastic robustness of watermarking algorithms, we propose a \"bigram paraphrase\" attack that produces paraphrases with small bigram overlap with the original sentence. This attack is shown to be effective against existing token-level watermark algorithms, while posing only minor degradations to SemStamp. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on various paraphrasers and domains, but also better at preserving the quality of generation."], "score": 0.0}, {"id": "(Bitton et al., 2025)", "paper": {"corpus_id": 276773353, "title": "Detecting Stylistic Fingerprints of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yehonatan Bitton", "authorId": "1405506735"}, {"name": "Elad Bitton", "authorId": "2348468222"}, {"name": "Shai Nisan", "authorId": "2348465980"}], "n_citations": 0}, "snippets": ["Classical approaches applied feature extraction on the texts in order to quantify stylistic or syntactic features such as perplexity, burstiness, and word density. These features are then fed into machine learning algorithms such as logistic regression or SVM (which provide an advantage of explainability), or into \"deep\" neural networks such as LSTMs. More modern approaches use pre-trained language models such as BERT, and fine-tune them for the specific classification task. This eliminates the need to manually define a list of features, and overall presents a much higher accuracy compared to feature-based methods, mainly because this approach is able to capture more general elements and nuances of the writing style, and is less dependent on a specific domain or topic. [20](Yadagiri et al., 2024)(Dathathri et al., 2024)(Kwon et al., 2025) Approaches that rely solely on capturing the differences between AI and human texts operate under the assumption that such differences exist; however, in reality, the differences between machine and human text may diminish over time as these systems evolve. A \"watermarking\" approach has recently emerged in an attempt to overcome these shortcomings. \"Watermarks\" can be added to the text during the generation phase (generative watermarking) or after it (edit-based watermarking). (Dathathri et al., 2024) However, this approach has its own limitations, such as lower effectiveness on factual responses (as there are fewer options to control the output), and higher sensitivity to small edits of the watermarked text."], "score": 0.94775390625}, {"id": "(Nguyen-Son et al., 2024)", "paper": {"corpus_id": 273901392, "title": "SimLLM: Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hoang-Quoc Nguyen-Son", "authorId": "1403588130"}, {"name": "Minh-Son Dao", "authorId": "2260782421"}, {"name": "Koji Zettsu", "authorId": "2238657258"}], "n_citations": 2}, "snippets": ["Detection of text generated by large language models uses three main techniques. Firstly, supervised learning methods (Solaiman et al., 2019;(Wang et al., 2023)(Hu et al., 2023)(Wu et al., 2023) train classifiers on datasets of original and generated text, though this requires large volumes of training data. The zero-shot approach (Bhattacharjee and Liu, 2023;(Mitchell et al., 2023) eliminates the need for training but is sensitive to out-of-distribution text. Recent research explores watermarking methodologies (Kirchenbauer et al., 2023) to force models to produce predefined words, aiding detection, but this requires modifying the models, which is impractical for proprietary models like ChatGPT."], "score": 0.95654296875}, {"id": "(Hans et al., 2024)", "paper": {"corpus_id": 267068867, "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Abhimanyu Hans", "authorId": "2280138753"}, {"name": "Avi Schwarzschild", "authorId": "102604362"}, {"name": "Valeriia Cherepanova", "authorId": "1471176143"}, {"name": "Hamid Kazemi", "authorId": "2075283338"}, {"name": "Aniruddha Saha", "authorId": "2056290221"}, {"name": "Micah Goldblum", "authorId": "121592562"}, {"name": "Jonas Geiping", "authorId": "8284185"}, {"name": "Tom Goldstein", "authorId": "2237426712"}], "n_citations": 107}, "snippets": ["Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data."], "score": 0.0}, {"id": "(Mubarak et al., 2023)", "paper": {"corpus_id": 266369103, "title": "A Survey on the Detection and Impacts of Deepfakes in Visual, Audio, and Textual Formats", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Rami Mubarak", "authorId": "2275319803"}, {"name": "Tariq A. A. Alsboui", "authorId": "3277359"}, {"name": "Omar Alshaikh", "authorId": "2207298991"}, {"name": "Isa Inuwa-Dutse", "authorId": "2276533468"}, {"name": "Saad Khan", "authorId": "143955457"}, {"name": "Si\u00f4n Parkinson", "authorId": "100581778"}], "n_citations": 36}, "snippets": ["In the context of the rapid advancement of Large Language Models (LLMs), the ability to produce highly accurate and contextually relevant text is becoming an increasing concern in online security. The sophisticated text generation capabilities of these models can potentially facilitate harmful activities on online platforms, emphasising the importance of detecting AI-generated texts as a mitigation strategy. Several detection methodologies have been developed, each with varying degrees of effectiveness influenced by factors such as the architecture of the language model, the decoding strategy, the length of the text and the availability of raw log probabilities from the LLMs. \n\nSimple classifiers offer a basic, yet sometimes limited, approach, particularly with shorter texts and complex generation strategies. Zero-shot models provide a robust detection strategy by exploiting unique statistical patterns in machine-generated text; however, they are susceptible to adversarial actions or different generation strategies. Finetuned language models represent another approach that provides a more promising route for enhancing the detection accuracy. However, their effectiveness can be influenced by the sampling method and the length of the generated sequences. \n\nWatermarking, a more recent development, has emerged as a unique method for detection. This involves embedding watermarks in the logits of an LLM during text generation, forming a pattern that can potentially identify synthetic text. Despite the promise of watermarking, its vulnerability to spoofing/paraphrasing attacks and evasion techniques presents significant challenges that warrant further research."], "score": 0.970703125}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 273228395, "title": "Signal Watermark on Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zhenyu Xu", "authorId": "2041278503"}, {"name": "Victor S. Sheng", "authorId": "2278314006"}], "n_citations": 0}, "snippets": ["As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources.\n\nNumerous open-source and commercial tools have been developed to detect model-generated text, such as GPT2-Detector [29], RoBERTa-QA [31], DetectGPT [30], GPTZero [32], Writer [33], and others. However, there are three significant limitations to these methods. First, their probability predictions cannot provide conclusive certainty and should only be regarded as a reference. Second, most current approaches rely on methods that differentiate human text from modelgenerated text based on variations in perplexity and burstiness. Third, methods like DetectGPT and RoBERTa-QA, which require multiple text-generation process and model retraining, consume substantial computational resources and time when detecting model-generated text. As LLMs advance, the generated text increasingly resembles human writing, making these methods based on variance in perplexity between human and model-generated text less applicable. Therefore, embedding watermarks in text generated by LLMs presents a faster, more stable, and reliable method of verification.\n\nRecent advancements in watermarking techniques for language models have been crucial in ensuring text authenticity and model protection. Abdelnabi and Fritz [20] pioneered the Adversarial Watermarking Transformer (AWT), marking a significant step in embedding binary messages into text and demonstrating resilience against multiple attacks. He et al. [21] developed CATER, a conditional watermarking framework that secures text generation APIs against imitation attacks while preserving word distribution integrity. Zhao, Li, and Wang [22] introduced Distillation-Resistant Watermarking (DRW), a method effective in protecting NLP models from distillation theft and maintaining accuracy. Kirchenbauer et al. [23] proposed a framework for embedding watermarks in language models that are detectable with minimal impact on text quality. In a similar vein, Zhao, Wang, and Li's GINSEW technique [24] innovatively embeds secret signals in the probability vector of decoding steps, enhancing IP infringement detection. Peng et al. [25] developed EmbMarker for EaaS, embedding watermarks in trigger words for effective copyright verification. Yoo et al. [26] explored robust multi-bit watermarking, focusing on error resistance and enhanced payload. Lastly, Li et al. [27] introduced a novel strategy for embedding watermarks in the weight quantization process of large language models, ensuring model security without compromising operational modes.\n\nThe advancement in language models has prompted the development of various detection tools for identifying model-generated text. Gehrmann [28] developed GLTR, a tool to assist non-experts in detecting texts generated by models. It uses statistical methods to identify generation artifacts and has been shown to improve human detection rates of fake text significantly. GPT2-Detector developed by OpenAI [29] is a RoBERTa base fine-tuned with outputs from the 1.5Bparameter GPT-2 model. Its primary function is to predict if text was generated by a GPT-2 model."], "score": 0.97705078125}, {"id": "(Dathathri et al., 2024)", "paper": {"corpus_id": 273547059, "title": "Scalable watermarking for identifying large language model outputs", "year": 2024, "venue": "The Naturalist", "authors": [{"name": "Sumanth Dathathri", "authorId": "3491117"}, {"name": "Abigail See", "authorId": "2268761249"}, {"name": "Sumedh Ghaisas", "authorId": "145053127"}, {"name": "Po-Sen Huang", "authorId": "2268826600"}, {"name": "Rob McAdam", "authorId": "2327318512"}, {"name": "Johannes Welbl", "authorId": "1851564"}, {"name": "Vandana Bachani", "authorId": "2327316530"}, {"name": "Alex Kaskasoli", "authorId": "2275186627"}, {"name": "Robert Stanforth", "authorId": "49860489"}, {"name": "Tatiana Matejovicova", "authorId": "2327319709"}, {"name": "Jamie Hayes", "authorId": "2270174566"}, {"name": "Nidhi Vyas", "authorId": "2275153245"}, {"name": "Majd Al Merey", "authorId": "2089890458"}, {"name": "Jonah Brown-Cohen", "authorId": "1400348545"}, {"name": "Rudy Bunel", "authorId": "3407947"}, {"name": "Borja Balle", "authorId": "2324784942"}, {"name": "taylan. cemgil", "authorId": "122347685"}, {"name": "Zahra Ahmed", "authorId": "2268758347"}, {"name": "Kitty Stacpoole", "authorId": "1396786528"}, {"name": "Ilia Shumailov", "authorId": "47473421"}, {"name": "Cip Baetu", "authorId": "2324799480"}, {"name": "Sven Gowal", "authorId": "2071666"}, {"name": "D. Hassabis", "authorId": "48987704"}, {"name": "Pushmeet Kohli", "authorId": "143967473"}], "n_citations": 74}, "snippets": ["Multiple strategies have emerged to address this problem. One is a retrieval-based approach, which involves keeping a growing record of all generated texts and checking against it for matches 12. This requires scale and coordination, and raises privacy concerns as it requires accessing and storing all LLM interactions."], "score": 0.9580078125}], "table": null}], "cost": 0.707844}}
