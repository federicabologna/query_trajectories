{"clarifying_information": [{"clarifying_question1": "Are you primarily interested in recent task-agnostic pruning methods for large language models that require no or minimal retraining, such as SparseGPT or Wanda?", "clarifying_answer1": {"clarifying_answer": "Yes, I am primarily interested in recent task-agnostic pruning methods for large language models that require no or minimal retraining, such as SparseGPT or Wanda."}}, {"clarifying_question2": "Do you want a comparison of structured versus unstructured pruning techniques for LLM compression in terms of hardware compatibility and efficiency gains?", "clarifying_answer2": {"clarifying_answer": "Yes, please provide a comparison of structured versus unstructured pruning techniques for large language model (LLM) compression, focusing specifically on hardware compatibility and efficiency gains."}}, {"clarifying_question3": "Are you specifically seeking pruning methods that can be combined effectively with other compression techniques (e.g., quantization, distillation) for maximal efficiency in NLP deployments?", "clarifying_answer3": {"clarifying_answer": "Yes, I am especially interested in pruning methods that can be effectively integrated with other compression techniques such as quantization and knowledge distillation to achieve maximal efficiency for deploying large language models in NLP applications."}}], "better_query": "What are the most recent task-agnostic pruning methods for large language models (LLMs) that require no or minimal retraining, such as SparseGPT or Wanda, and how do they achieve efficiency gains while maintaining performance?", "better_answer": {"sections": [{"title": "Introduction to LLM Pruning", "tldr": "LLM pruning aims to reduce model size and computational requirements while maintaining performance. Recent task-agnostic pruning methods like SparseGPT and Wanda have emerged that can efficiently sparsify large language models without expensive retraining. (14 sources)", "text": "\nLarge Language Models (LLMs) have demonstrated remarkable capabilities across various domains, but their massive size presents significant challenges for deployment, inference, and training <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273811289\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259360395\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. As these models continue to grow in scale, optimizing them becomes increasingly critical for practical applications.\n\nPruning has emerged as a promising approach to address these challenges by reducing the computational complexity and memory usage of LLMs <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. It enables models to support longer sequence lengths while maintaining desirable performance characteristics. Pruning techniques for LLMs can be broadly categorized into two types:\n\n1. **Structured pruning**: Removes entire structured units (like neurons or attention heads)\n2. **Unstructured pruning**: Eliminates individual parameters based on their importance, creating irregular sparse structures <Paper corpusId=\"267412232\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\nTraditional pruning approaches for neural networks often required iterative pruning with retraining <Paper corpusId=\"208267757\" paperTitle=\"(Evci et al., 2019)\" isShortName></Paper>, which becomes impractical for billion-parameter LLMs due to the enormous computational costs and limited access to training data <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper>.\n\nRecent advances in LLM pruning have shifted toward task-agnostic methods that require minimal or no retraining. These approaches aim to preserve the model's multi-task solving capabilities while reducing parameter count <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. Pioneering works in this area include:\n\n- **SparseGPT**: Introduced by Frantar et al., this method can compress LLMs by up to 60% with almost no performance degradation through a one-shot pruning technique that uses a diagonal Hessian approximation to assess weight importance <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>\n\n- **Wanda (Weights and Activations)**: Developed by Sun et al., this approach simplifies SparseGPT by eliminating Hessian approximations and instead uses the product of weight magnitudes and input activations to determine pruning importance, achieving competitive results with significantly lower computational requirements <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277435040\" paperTitle=\"(Mecke et al., 2025)\" isShortName></Paper>\n\nThese innovative pruning methods have opened new possibilities for deploying efficient LLMs in resource-constrained environments, with researchers continuing to build upon these foundations to achieve even better performance-efficiency trade-offs <Paper corpusId=\"263829692\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264128029\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Cunegatti et al., 2024)", "paper": {"corpus_id": 273962638, "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Elia Cunegatti", "authorId": "2162450849"}, {"name": "Leonardo Lucio Custode", "authorId": "2037391480"}, {"name": "Giovanni Iacca", "authorId": "2295670461"}], "n_citations": 0}, "snippets": ["In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023)."], "score": 0.84375}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273811289, "title": "MoE-I\u00b2: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Cheng Yang", "authorId": "2329224758"}, {"name": "Yang Sui", "authorId": "2117517225"}, {"name": "Jinqi Xiao", "authorId": "2196307128"}, {"name": "Lingyi Huang", "authorId": "2152279863"}, {"name": "Yu Gong", "authorId": "2168502148"}, {"name": "Yuanlin Duan", "authorId": "2329727093"}, {"name": "Wenqi Jia", "authorId": "2297818320"}, {"name": "Miao Yin", "authorId": "1471722186"}, {"name": "Yu Cheng", "authorId": "2329746797"}, {"name": "Bo Yuan", "authorId": "2241581494"}], "n_citations": 7}, "snippets": ["Recent advancements in large language models have underscored the need to reduce parameter sizes and latency (Ma et al., 2023). Compression techniques for language models include network pruning (Xu et al., 2021), knowledge distillation (Sun et al., 2019(Sun et al., , 2020)), quantization (Yao et al., 2022), decomposition (Hsu et al., 2022;Yuan et al., 2023;Wang et al., 2024), and methods like early exit (Xin et al., 2020)", "Recent studies on LLMs (Sun et al., 2023;(Ma et al., 2023) focus on pruning linear layers, but these methods often fail to reduce computing costs without specialized hardware or libraries."], "score": 0.82470703125}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 259360395, "title": "A Survey on Evaluation of Large Language Models", "year": 2023, "venue": "ACM Transactions on Intelligent Systems and Technology", "authors": [{"name": "Yu-Chu Chang", "authorId": "2140050490"}, {"name": "Xu Wang", "authorId": "2206577797"}, {"name": "Jindong Wang", "authorId": "1519290245"}, {"name": "Yuan Wu", "authorId": "48608007"}, {"name": "Kaijie Zhu", "authorId": "2543684"}, {"name": "Hao Chen", "authorId": "2051536212"}, {"name": "Linyi Yang", "authorId": "2145500840"}, {"name": "Xiaoyuan Yi", "authorId": "3393196"}, {"name": "Cunxiang Wang", "authorId": "35504092"}, {"name": "Yidong Wang", "authorId": "2108024279"}, {"name": "Weirong Ye", "authorId": "2147205193"}, {"name": "Yue Zhang", "authorId": "2211964951"}, {"name": "Yi Chang", "authorId": "2131636065"}, {"name": "Philip S. Yu", "authorId": "2191036692"}, {"name": "Qian Yang", "authorId": "2158406244"}, {"name": "Xingxu Xie", "authorId": "1576441343"}], "n_citations": 1710}, "snippets": ["Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the \u2018where\u2019 and \u2018how\u2019 questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey"], "score": 0.0}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 267412232, "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models", "year": 2024, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Xindi Wang", "authorId": "2108048327"}, {"name": "Mahsa Salmani", "authorId": "1904419"}, {"name": "Parsa Omidi", "authorId": "2282534833"}, {"name": "Xiangyu Ren", "authorId": "2283447900"}, {"name": "Mehdi Rezagholizadeh", "authorId": "2066076226"}, {"name": "A. Eshaghi", "authorId": "50782111"}], "n_citations": 45}, "snippets": ["Pruning can help optimize the model for deployment and make the model more efficient in terms of computation complexity and memory usage. Accordingly, pruning can be considered as an approach to enable a language model to support longer sequence length, while maintaining the desirable complexity and performance. In general, pruning a model can be categorized into structured and unstructured pruning", "Unstructured pruning involves with pruning individual parameters of a model independently based on their magnitudes or importance, resulting in an irregular sparse structure. Due to the irregularity in the structure and in the memory access patterns, unstructured pruning hinders the efficiency gain that might be achieved through structured pruning, and it requires specialized software and/or hardware for efficient deployment. SparseGPT [Frantar and Alistarh, 2023] compresses LLMs with billions of parameter by as much as 60%, almost without affecting the performance of the models. However, SparseGPT heavily relies on weight updates."], "score": 0.80078125}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]", "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."], "score": 0.87548828125}, {"id": "(Evci et al., 2019)", "paper": {"corpus_id": 208267757, "title": "Rigging the Lottery: Making All Tickets Winners", "year": 2019, "venue": "International Conference on Machine Learning", "authors": [{"name": "Utku Evci", "authorId": "3399348"}, {"name": "Trevor Gale", "authorId": "2066558041"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "P. S. Castro", "authorId": "39163115"}, {"name": "Erich Elsen", "authorId": "152585800"}], "n_citations": 608}, "snippets": ["Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in this http URL."], "score": 0.0}, {"id": "(Ashkboos et al., 2024)", "paper": {"corpus_id": 267301573, "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Saleh Ashkboos", "authorId": "9543395"}, {"name": "Maximilian L. Croci", "authorId": "2008063761"}, {"name": "Marcelo Gennari do Nascimento", "authorId": "2281641743"}, {"name": "Torsten Hoefler", "authorId": "2258547286"}, {"name": "James Hensman", "authorId": "2266803418"}], "n_citations": 184}, "snippets": ["Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"], "score": 0.0}, {"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs", "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."], "score": 0.88037109375}, {"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."], "score": 0.88818359375}, {"id": "(Mecke et al., 2025)", "paper": {"corpus_id": 277435040, "title": "STADE: Standard Deviation as a Pruning Metric", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Diego Coello de Portugal Mecke", "authorId": "2352793764"}, {"name": "Haya Alyoussef", "authorId": "2352792956"}, {"name": "Ilia Koloiarov", "authorId": "2352793615"}, {"name": "Maximilian Stubbemann", "authorId": "2290075048"}, {"name": "Lars Schmidt-Thieme", "authorId": "2280660731"}], "n_citations": 0}, "snippets": ["Recently, Large Language Models (LLMs) have become very widespread and are used to solve a wide variety of tasks. To successfully handle these tasks, LLMs require longer training times and larger model sizes. This makes LLMs ideal candidates for pruning methods that reduce computational demands while maintaining performance. Previous methods require a retraining phase after pruning to maintain the original model's performance. However, state-of-the-art pruning methods, such as Wanda, prune the model without retraining, making the pruning process faster and more efficient."], "score": 0.93017578125}, {"id": "(Yin et al., 2023)", "paper": {"corpus_id": 263829692, "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Lu Yin", "authorId": "2254142682"}, {"name": "You Wu", "authorId": "2325206905"}, {"name": "Zhenyu (Allen) Zhang", "authorId": "2109338656"}, {"name": "Cheng-Yu Hsieh", "authorId": "2256992922"}, {"name": "Yaqing Wang", "authorId": "2257105674"}, {"name": "Yiling Jia", "authorId": "2257230381"}, {"name": "Mykola Pechenizkiy", "authorId": "1691997"}, {"name": "Yi Liang", "authorId": "2260290217"}, {"name": "Zhangyang Wang", "authorId": "2254949434"}, {"name": "Shiwei Liu", "authorId": "2255081092"}], "n_citations": 102}, "snippets": ["Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 264128029, "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yu-xin Zhang", "authorId": "2108078624"}, {"name": "Lirui Zhao", "authorId": "2258678648"}, {"name": "Mingbao Lin", "authorId": "49352079"}, {"name": "Yunyun Sun", "authorId": "2258670567"}, {"name": "Yiwu Yao", "authorId": "2258671504"}, {"name": "Xingjia Han", "authorId": "2258598205"}, {"name": "Jared Tanner", "authorId": "2258549938"}, {"name": "Shiwei Liu", "authorId": "2258718674"}, {"name": "Rongrong Ji", "authorId": "2258551942"}], "n_citations": 43}, "snippets": ["The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268032346, "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2153917002"}, {"name": "Wenqi Shao", "authorId": "2283133523"}, {"name": "Mengzhao Chen", "authorId": "2287768783"}, {"name": "Shitao Tang", "authorId": "2287949030"}, {"name": "Kai-Chuang Zhang", "authorId": "2273778831"}, {"name": "Peng Gao", "authorId": "2269823523"}, {"name": "Fengwei An", "authorId": "2287838451"}, {"name": "Yu Qiao", "authorId": "2256992387"}, {"name": "Ping Luo", "authorId": "2253674868"}], "n_citations": 32}, "snippets": ["Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."], "score": 0.84326171875}], "table": null}, {"title": "SparseGPT: One-Shot Pruning with Sparse Regression", "tldr": "SparseGPT revolutionized LLM pruning by introducing a one-shot approach that treats pruning as a sparse regression problem, enabling up to 60% parameter reduction without retraining. It uses Hessian-based weight importance assessment and optimized weight updates to maintain model performance while being computationally efficient enough to prune even the largest available LLMs. (8 sources)", "text": "\nSparseGPT, introduced by Frantar and Alistarh in 2023, represents a groundbreaking advancement in LLM pruning techniques that eliminates the need for costly retraining <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This method frames pruning as an extensive sparse regression problem and employs an approximate sparse regression solver to identify unimportant weights <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper>. The key innovation lies in its one-shot approach, which allows for significant parameter reduction in a single step rather than through iterative pruning cycles <Paper corpusId=\"271083368\" paperTitle=\"(Rostam et al., 2024)\" isShortName></Paper>.\n\nThe pruning process in SparseGPT follows a sophisticated workflow:\n1. It conducts a thorough model analysis to identify parameters that can be removed without significant impact <Paper corpusId=\"271083368\" paperTitle=\"(Rostam et al., 2024)\" isShortName></Paper>\n2. It uses synchronized second-order Hessian updates to assess weight importance <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>\n3. It solves a layer-wise reconstruction problem to determine weight activations <Paper corpusId=\"273549773\" paperTitle=\"(Bhansali et al., 2024)\" isShortName></Paper>\n4. It performs weight updates to minimize the reconstruction error between dense and sparse weights <Paper corpusId=\"278327238\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>\n\nWhat makes SparseGPT particularly remarkable is its ability to scale to enormous models while maintaining efficiency. It can be executed on the largest available open-source models like OPT-175B and BLOOM-176B in under 4.5 hours <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. The approach achieves up to 60% unstructured sparsity with only negligible increases in perplexity, effectively allowing more than 100 billion weights to be ignored during inference <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268041812\" paperTitle=\"(Bai et al., 2024)\" isShortName></Paper>.\n\nAdditionally, SparseGPT demonstrates versatility across different sparsity patterns. It generalizes to semi-structured patterns (2:4 and 4:8) and is compatible with weight quantization approaches, enabling further compression beyond pruning alone <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. This flexibility makes it adaptable to different hardware acceleration requirements.\n\nThe computational efficiency of SparseGPT comes from its design to handle memory constraints by sequentially loading transformer blocks one at a time instead of loading the entire model <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. It also reduces data requirements by using only a small amount of calibration data, eliminating the need for retraining on massive datasets <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nWhile SparseGPT represents a significant advancement in LLM pruning, it has inspired further refinements such as Wanda, which simplifies the approach by eliminating the computationally expensive Hessian approximations while achieving competitive results <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 260900101, "title": "A Survey on Model Compression for Large Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Xunyu Zhu", "authorId": "2205622982"}, {"name": "Jian Li", "authorId": "153154515"}, {"name": "Yong Liu", "authorId": "2144384857"}, {"name": "Can Ma", "authorId": "2112563365"}, {"name": "Weiping Wang", "authorId": "2154491572"}], "n_citations": 229}, "snippets": ["An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels."], "score": 0.8232421875}, {"id": "(Rostam et al., 2024)", "paper": {"corpus_id": 271083368, "title": "Achieving Peak Performance for Large Language Models: A Systematic Review", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Z. R. K. Rostam", "authorId": "1470666105"}, {"name": "S. Sz\u00e9n\u00e1si", "authorId": "3208184"}, {"name": "G\u00e1bor Kert\u00e9sz", "authorId": "9717627"}], "n_citations": 4}, "snippets": ["One-Shot Pruning: SparseGPT implements its pruning strategy through a streamlined process. First, a thorough model analysis is conducted to pinpoint parameters that can be removed without significant impact. This analysis leverages pruning criteria that assess parameter importance without requiring gradient calculations, saving on computational resources. Finally, SparseGPT employs a single step pruning approach, achieving substantial sparsity (at least 50% for massive models) in a single step. This oneshot approach significantly reduces the time and complexity compared to iterative pruning methods."], "score": 0.8583984375}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264590698, "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Tianyi Chen", "authorId": "2257252148"}, {"name": "Tianyu Ding", "authorId": "2257191230"}, {"name": "Badal Yadav", "authorId": "2262446441"}, {"name": "Ilya Zharkov", "authorId": "15623770"}, {"name": "Luming Liang", "authorId": "46225943"}], "n_citations": 32}, "snippets": ["SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations."], "score": 0.8828125}, {"id": "(Bhansali et al., 2024)", "paper": {"corpus_id": 273549773, "title": "LEGO: Language Model Building Blocks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shrenik Bhansali", "authorId": "2164257071"}, {"name": "Alwin Jin", "authorId": "2327337534"}, {"name": "Tyler Lizzo", "authorId": "2315304043"}, {"name": "Larry Heck", "authorId": "2315302093"}], "n_citations": 0}, "snippets": ["Recently, more nuanced pruning approaches have been discussed in the literature, improving over more traditional methods like magnitude pruning. Specifically, two state-of-the-art pruning methods are widely discussed in the literature-SparseGPT (Frantar et al., 2023) and Wanda (Sun et al., 2023). Whereas traditional magnitude pruning operates by pruning weights with the largest magnitude, these pruning techniques instead track weight activations, and prune weights with the lowest amount of activation. \n\nSparseGPT creates and solves a layer-wise reconstruction problem to determine the weight activations, whereas Wanda takes the product of a weight's magnitude and the norm of its associated input activations."], "score": 0.86962890625}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278327238, "title": "Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chuan Sun", "authorId": "2359207803"}, {"name": "Han Yu", "authorId": "2148706587"}, {"name": "Li-zhen Cui", "authorId": "2313694394"}, {"name": "Xiaoxiao Li", "authorId": "2283747425"}], "n_citations": 3}, "snippets": ["Uniform Pruning. Traditional pruning requires a round of re-training to restore performance, which poses significant challenges for LLMs. Researchers have developed pruning algorithms specifically tailored for LLM compression. For instance, (Ma et al., 2023) investigated structured sparse LLMs by applying Taylor pruning to remove entire weight rows, followed by LoRA fine-tuning [13]. In recent years, the focus has shifted toward unstructured pruning which eliminates the need for fine-tuning. SparseGPT (Frantar et al., 2023) employs the Hessian inverse for pruning, followed by weight updates to reduce reconstruction errors between dense and sparse weights. Wanda (Sun et al., 2023) introduced a criterion that incorporates weight magnitude and input activations to preserve outlier features."], "score": 0.82666015625}, {"id": "(Bai et al., 2024)", "paper": {"corpus_id": 268041812, "title": "SparseLLM: Towards Global Pruning of Pre-trained Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Guangji Bai", "authorId": "7583867"}, {"name": "Yijiang Li", "authorId": "2288037157"}, {"name": "Chen Ling", "authorId": "2284591355"}, {"name": "Kibaek Kim", "authorId": "2288023827"}, {"name": "Liang Zhao", "authorId": "2284637383"}], "n_citations": 11}, "snippets": ["SparseGPT Frantar and Alistarh [2023], an efficient unstructured pruning method for LLMs with hundreds of billions of parameters, achieves parameter reduction of up to 60% with minimal performance loss. Another approach, Wanda Sun et al. [2023], introduces a novel pruning criterion that evaluates weights by considering both magnitude and related input activations."], "score": 0.826171875}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 273501976, "title": "Pruning Foundation Models for High Accuracy without Retraining", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pu Zhao", "authorId": "2241612245"}, {"name": "Fei Sun", "authorId": "2327046378"}, {"name": "Xuan Shen", "authorId": "2007668856"}, {"name": "Pinrui Yu", "authorId": "2241698013"}, {"name": "Zhenglun Kong", "authorId": "32409528"}, {"name": "Yanzhi Wang", "authorId": "2290628977"}, {"name": "Xue Lin", "authorId": "2322988586"}], "n_citations": 13}, "snippets": ["The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT."], "score": 0.826171875}], "table": null}, {"title": "Wanda: Weight and Activation-Based Pruning", "tldr": "Wanda simplifies SparseGPT's approach by using a straightforward metric that multiplies weight magnitudes with input activations to determine pruning importance, requiring no retraining or weight updates. It achieves competitive performance while being computationally more efficient than methods relying on Hessian approximations. (9 sources)", "text": "\nIntroduced by Sun et al. in 2023, Wanda (Pruning by Weights and Activations) represents a significant advancement in task-agnostic LLM pruning by offering a simpler yet effective alternative to SparseGPT <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. While SparseGPT uses computationally expensive Hessian approximations and weight updates, Wanda achieves comparable results through a more straightforward approach that requires no retraining or weight modification <Paper corpusId=\"260900101\" paperTitle=\"(Zhu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"264590698\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nThe core innovation of Wanda lies in its pruning criterion, which combines two key factors:\n1. The magnitude of the weight itself\n2. The norm of the corresponding input activations\n\nBy identifying and pruning weights with the smallest product of these two factors on a per-output basis, Wanda effectively preserves the most important connections in the network <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. This approach is motivated by recent observations of emergent large magnitude features in LLMs, which Wanda specifically aims to preserve <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nOne of Wanda's most significant advantages is its computational efficiency. Unlike SparseGPT, which requires solving a layer-wise reconstruction problem and performing weight updates, Wanda can be executed in a single forward pass through the model <Paper corpusId=\"271064490\" paperTitle=\"(Kolbeinsson et al., 2024)\" isShortName></Paper>. This efficiency makes it particularly suitable for extremely large models where computational resources are limited <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nWanda has been extensively evaluated on models like LLaMA and LLaMA-2 across various language benchmarks, where it significantly outperforms traditional magnitude pruning while performing competitively against methods involving intensive weight updates <Paper corpusId=\"259203115\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>. It can achieve up to 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nThe method's success has established it as a baseline for subsequent pruning approaches, with numerous researchers building upon its foundations <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. For instance, newer methods like RIA (Zhang et al., 2024) extend Wanda by re-evaluating weight importance based on all input and output connections <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>, while DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties to further enhance performance without retraining <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\nDespite its simplicity, Wanda has certain limitations. Some researchers note that its layer-wise approach can result in significant perturbation to the model's output and requires careful hyperparameter tuning of pruning rates, which may adversely affect overall model performance <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This has motivated the development of methods like GBLM-Pruner, which leverages gradients from pretrained LLMs to determine pruning importance <Paper corpusId=\"265050936\" paperTitle=\"(Das et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Sun et al., 2023)", "paper": {"corpus_id": 259203115, "title": "A Simple and Effective Pruning Approach for Large Language Models", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Mingjie Sun", "authorId": "2984183"}, {"name": "Zhuang Liu", "authorId": "2109168016"}, {"name": "Anna Bair", "authorId": "25901845"}, {"name": "J. Z. Kolter", "authorId": "145116464"}], "n_citations": 439}, "snippets": ["In this paper, we introduce a novel, straightforward yet effective pruning method, termed Wanda (Pruning by Weights and activations), designed to induce sparsity in pretrained LLMs. Motivated by the recent observation of emergent large magnitude features in LLMs, our approach prunes weights with the smallest magnitudes multiplied by the corresponding input activations, on a per-output basis. Notably, Wanda requires no retraining or weight update, and the pruned LLM can be used as is. We conduct a thorough evaluation of our method Wanda on LLaMA and LLaMA-2 across various language benchmarks. Wanda significantly outperforms the established baseline of magnitude pruning and performs competitively against recent method involving intensive weight update."], "score": 0.88818359375}, {"id": "(Zhu et al., 2023)", "paper": {"corpus_id": 260900101, "title": "A Survey on Model Compression for Large Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Xunyu Zhu", "authorId": "2205622982"}, {"name": "Jian Li", "authorId": "153154515"}, {"name": "Yong Liu", "authorId": "2144384857"}, {"name": "Can Ma", "authorId": "2112563365"}, {"name": "Weiping Wang", "authorId": "2154491572"}], "n_citations": 229}, "snippets": ["An innovative approach in this domain is SparseGPT (Frantar and Alistarh, 2023), which introduces a one-shot pruning strategy without retraining. SparseGPT frames pruning as an extensive sparse regression problem and solves it using an approximate sparse regression solver. SparseGPT achieves significant unstructured sparsity, even up to over 50% on the largest GPT models like OPT-175B and BLOOM-176B, with minimal increase in perplexity. To reduce the cost about the weight update process required by SparseGPT, Wanda (Sun et al., 2023) achieves model sparsity by pruning weights with the smallest magnitudes multiplied by the norm of the corresponding input activations, without the need for retraining or weight updates. To further minimize pruning-induced errors while upholding the desired overall sparsity level, SAMSP (Shao et al., 2023) utilizes the Hessian matrix as a metric for weight matrix sensitivity evaluation, and dynamically adjusts sparsity allocation based on sensitivity. Furthermore, DSnoT (Zhang et al., 2023) minimizes the reconstruction error between dense and sparse models through iterative weight pruning-and-growing on top of sparse LLMs to enhance LLM performance across various sparsity rates, especially at high sparsity levels."], "score": 0.8232421875}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264590698, "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Tianyi Chen", "authorId": "2257252148"}, {"name": "Tianyu Ding", "authorId": "2257191230"}, {"name": "Badal Yadav", "authorId": "2262446441"}, {"name": "Ilya Zharkov", "authorId": "15623770"}, {"name": "Luming Liang", "authorId": "46225943"}], "n_citations": 32}, "snippets": ["SparseGPT (Frantar et al., 2023)) uses a sophisticated weight update process involving synchronized secondorder Hessian updates, bypassing traditional retraining. In contrast, Wanda (Sun et al., 2023) achieves high sparsity without any retraining, simply by pruning weights with the smallest magnitudes multiplied by their corresponding input activations."], "score": 0.8828125}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]", "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."], "score": 0.87548828125}, {"id": "(Kolbeinsson et al., 2024)", "paper": {"corpus_id": 271064490, "title": "Composable Interventions for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Arinbj\u00f6rn Kolbeinsson", "authorId": "68978244"}, {"name": "Kyle O'Brien", "authorId": "2212970046"}, {"name": "Tianjin Huang", "authorId": "2310637497"}, {"name": "Shanghua Gao", "authorId": "2269765109"}, {"name": "Shiwei Liu", "authorId": "2310512874"}, {"name": "Jonathan Richard Schwarz", "authorId": "2290185444"}, {"name": "Anurag Vaidya", "authorId": "2187496179"}, {"name": "Faisal Mahmood", "authorId": "2310436669"}, {"name": "M. Zitnik", "authorId": "2095762"}, {"name": "Tianlong Chen", "authorId": "2295593785"}, {"name": "Thomas Hartvigsen", "authorId": "32452740"}], "n_citations": 4}, "snippets": ["\u2022 SparseGPT (Frantar & Alistarh, 2023): an efficient one-shot pruning method tailored for large models. It converts the pruning process into solving large-scale sparse regression problems using an approximate solver. This approach enables rapid pruning on a single GPU with minimal accuracy loss, achieving 50-60% on large models. \n\n\u2022 Wanda (Sun et al., 2023): is another popular method for pruning large language models that relies on a pruning metric that combines a weight's magnitude with the norm of its corresponding input activations, determined from a small calibration dataset. The method focuses on selectively pruning weights within individual outputs of linear layers, aiming for high sparsity levels without modifying unpruned weights. Wanda is computationally efficient, executable in a single forward pass."], "score": 0.87060546875}, {"id": "(Ding et al., 2025)", "paper": {"corpus_id": 276774084, "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yizhuo Ding", "authorId": "2237598833"}, {"name": "Xinwei Sun", "authorId": "2244778967"}, {"name": "Yanwei Fu", "authorId": "2244698019"}, {"name": "Guosheng Hu", "authorId": "2349205822"}], "n_citations": 2}, "snippets": ["In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune."], "score": 0.9404296875}, {"id": "(Yi et al., 2025)", "paper": {"corpus_id": 276079889, "title": "Symmetric Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kai Yi", "authorId": "2292143960"}, {"name": "Peter Richt'arik", "authorId": "2342412598"}], "n_citations": 0}, "snippets": ["Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."], "score": 0.85400390625}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268032346, "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2153917002"}, {"name": "Wenqi Shao", "authorId": "2283133523"}, {"name": "Mengzhao Chen", "authorId": "2287768783"}, {"name": "Shitao Tang", "authorId": "2287949030"}, {"name": "Kai-Chuang Zhang", "authorId": "2273778831"}, {"name": "Peng Gao", "authorId": "2269823523"}, {"name": "Fengwei An", "authorId": "2287838451"}, {"name": "Yu Qiao", "authorId": "2256992387"}, {"name": "Ping Luo", "authorId": "2253674868"}], "n_citations": 32}, "snippets": ["Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."], "score": 0.84326171875}, {"id": "(Das et al., 2023)", "paper": {"corpus_id": 265050936, "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Rocktim Jyoti Das", "authorId": "2211732585"}, {"name": "Liqun Ma", "authorId": "2243392466"}, {"name": "Zhiqiang Shen", "authorId": "2243374493"}], "n_citations": 19}, "snippets": ["Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks."], "score": 0.86962890625}], "table": null}, {"title": "Other Recent Task-Agnostic Pruning Methods", "tldr": "Beyond SparseGPT and Wanda, several innovative pruning methods have emerged for LLMs that require no or minimal retraining. These approaches include structured pruning techniques like LLM-Pruner and FLAP, as well as specialized methods targeting layer pruning, expert activation patterns, and depth reduction. (11 sources)", "text": "\n## LLM-Pruner\nLLM-Pruner was the first framework specifically designed for structured pruning of LLMs in a task-agnostic manner. It identifies and removes non-critical coupled structures based on gradient information to preserve the model's functionality while minimizing reliance on the original training dataset. The approach follows three main stages: dependency detection to identify dependent structures, importance estimation using first-order and approximated Hessian information, and a rapid recovery stage to fine-tune the pruned model with limited data. <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269791108\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270411995\" paperTitle=\"(Touheed et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271909626\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>\n\n## FLAP (FLuctuation-based Adaptive Structured Pruning)\nFLAP addresses the challenges of structured pruning for LLMs by introducing a novel pruning metric, adaptive global model compression strategies, and robust compensation mechanisms. This retraining-free framework maintains perplexity and zero-shot performance while providing the benefits of structured compression. <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper>\n\n## SoBP (Structured Optimal Brain Pruning)\nSoBP offers a retraining-free structured pruning method that leverages global first-order information to select pruning structures. It then refines these structures with a local greedy approach and adopts module-wise reconstruction to minimize information loss, making it suitable for practical applications without relying on specialized hardware or extensive computational resources for post-pruning fine-tuning. <Paper corpusId=\"273901152\" paperTitle=\"(Wei et al., 2024)\" isShortName></Paper>\n\n## SEAP (Sparse Expert Activation Pruning)\nInspired by clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and selectively retains task-relevant parameters. This training-free method reduces inference overhead while preserving task performance. At 50% pruning, SEAP outperforms both Wanda and FLAP by over 20%, and at 20% pruning, it experiences only a 2.2% performance drop compared to the dense model. <Paper corpusId=\"276928323\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>\n\n## Layer Collapse (LaCo)\nLaCo focuses on reducing model depth by merging adjacent layers from the topmost layer downward. This approach enables rapid reduction in model size while preserving the model structure. LaCo maintains an average task performance of over 80% at pruning ratios of 25-30%, outperforming other structured pruning methods. <Paper corpusId=\"277622258\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"267751181\" paperTitle=\"(Yang et al._1, 2024)\" isShortName></Paper>\n\n## Depth-2 Pruning with Inference-Aware Criteria\nThis approach identifies a depth-2 pruning structure and develops inference-aware pruning criteria that outperform traditional metrics while eliminating the need for computationally expensive retraining. A two-step reconstruction technique mitigates pruning errors, ensuring superior performance across various datasets and models while significantly reducing computational costs and hardware requirements. <Paper corpusId=\"271533761\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n## Continual Pruning\nThis novel concept addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training entirely. It represents a shift toward more dynamic and adaptive pruning approaches for LLMs. <Paper corpusId=\"269605957\" paperTitle=\"(Malla et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs", "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."], "score": 0.88037109375}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 269791108, "title": "When Large Language Model Meets Optimization", "year": 2024, "venue": "Swarm and Evolutionary Computation", "authors": [{"name": "Sen Huang", "authorId": "2301544444"}, {"name": "Kaixiang Yang", "authorId": "2301490921"}, {"name": "Sheng Qi", "authorId": "2301455526"}, {"name": "Rui Wang", "authorId": "2268020758"}], "n_citations": 12}, "snippets": ["Ma et al. (Ma et al., 2023) propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism."], "score": 0.8603515625}, {"id": "(Touheed et al., 2024)", "paper": {"corpus_id": 270411995, "title": "Applications of Pruning Methods in Natural Language Processing", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Marva Touheed", "authorId": "2305959319"}, {"name": "Urooj Zubair", "authorId": "2305868456"}, {"name": "Dilshad Sabir", "authorId": "17492832"}, {"name": "Ali Hassan", "authorId": "2293111925"}, {"name": "Muhammad Fasih Uddin Butt", "authorId": "2305969817"}, {"name": "Farhan Riaz", "authorId": "1713703"}, {"name": "Wadood Abdul", "authorId": "2305963536"}, {"name": "R. Ayub", "authorId": "119778535"}], "n_citations": 1}, "snippets": ["Ma et al. [29] proposes Large Language Model-Pruner (LLM-Pruner), a task-agnostic compression method, aiming to maintain the multi-task solving and language generation abilities of the original LLM while minimizing reliance on the extensive training dataset.LLM-Pruner adopts structural pruning, removing non-critical coupled structures based on gradient information to preserve most of the LLM's functionality.The pruned models can be efficiently recovered through tuning techniques in a short time and with minimal data.Experimental results on three LLMs, Large Language Model Meta AI (LLaMA), Vicuna, and General Language Model (ChatGLM), demonstrate that the compressed models perform well in zero-shot classification and generation tasks."], "score": 0.8623046875}, {"id": "(Dong et al., 2024)", "paper": {"corpus_id": 271909626, "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yanjie Dong", "authorId": "2278396109"}, {"name": "Xiaoyi Fan", "authorId": "2316525665"}, {"name": "Fangxin Wang", "authorId": "2266804662"}, {"name": "Chengming Li", "authorId": "2278907861"}, {"name": "Victor C. M. Leung", "authorId": "2264958744"}, {"name": "Xiping Hu", "authorId": "1718919"}], "n_citations": 5}, "snippets": ["In order to reduce the training duration, the model pruning technology can be used to remove unnecessary neurons in the LLMs while retaining the versatility of the pruned LLMs. In this vein, X. Ma et al. propose a task-agnostic pruner (named as, the LLM-Pruner) to preserve the capability to handle various task without requiring original training dataset and long duration  of retraining. Since the LLMs contain redundant parameters that have little/no effects on the performance of models, the LLM-Pruner reduces the scale of LLMs based on the gradient information. More specifically, the LLM-Pruner incorporates three main stages: discovery, estimation, and recovery."], "score": 0.9091796875}, {"id": "(An et al., 2023)", "paper": {"corpus_id": 266362404, "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yongqi An", "authorId": "2167834971"}, {"name": "Xu Zhao", "authorId": "2118489444"}, {"name": "Tao Yu", "authorId": "40418746"}, {"name": "Ming Tang", "authorId": "2113727378"}, {"name": "Jinqiao Wang", "authorId": "2241943585"}], "n_citations": 61}, "snippets": ["In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining."], "score": 0.88671875}, {"id": "(Wei et al., 2024)", "paper": {"corpus_id": 273901152, "title": "Structured Optimal Brain Pruning for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiateng Wei", "authorId": "2330242586"}, {"name": "Quan Lu", "authorId": "2329898504"}, {"name": "Ning Jiang", "authorId": "2329738680"}, {"name": "Siqi Li", "authorId": "2258340244"}, {"name": "Jingyang Xiang", "authorId": "2256984205"}, {"name": "Jun Chen", "authorId": "2257200295"}, {"name": "Yong Liu", "authorId": "2257376000"}], "n_citations": 4}, "snippets": ["The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss."], "score": 0.89794921875}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 276928323, "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xun Liang", "authorId": "2268495952"}, {"name": "Hanyu Wang", "authorId": "2284861141"}, {"name": "Huayi Lai", "authorId": "2349573796"}, {"name": "Simin Niu", "authorId": "2268393907"}, {"name": "Shichao Song", "authorId": "2268434524"}, {"name": "Jiawei Yang", "authorId": "2303425635"}, {"name": "Jihao Zhao", "authorId": "2326243408"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Bo Tang", "authorId": "2268400606"}, {"name": "Zhiyu Li", "authorId": "2268429641"}], "n_citations": 0}, "snippets": ["This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model."], "score": 0.853515625}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 277622258, "title": "Entropy-Based Block Pruning for Efficient Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Liangwei Yang", "authorId": "2354128342"}, {"name": "Yuhui Xu", "authorId": "48615640"}, {"name": "Juntao Tan", "authorId": "2286700513"}, {"name": "Doyen Sahoo", "authorId": "36187119"}, {"name": "Silvio Savarese", "authorId": "2238207181"}, {"name": "Caiming Xiong", "authorId": "2256976968"}, {"name": "Huan Wang", "authorId": "2258793468"}, {"name": "Shelby Heinecke", "authorId": "71926704"}], "n_citations": 0}, "snippets": ["Among structural pruning methods, layer pruning is particularly relevant. Laco (Yang et al., 2024) reduces model depth by merging adjacent layers from the topmost layer downward. ShortGPT (Men et al., 2024) prunes unimportant layers based on a cosine similarity criterion. LLMDrop (He et al., 2024) finds that attention layers are more redundant than MLP layers but also relies on cosine similarity for pruning. Different from these approaches, in this paper, we propose a more effective criterion i.e. En-tropy Increase to identify and remove unimportant layers."], "score": 0.810546875}, {"id": "(Yang et al._1, 2024)", "paper": {"corpus_id": 267751181, "title": "LaCo: Large Language Model Pruning via Layer Collapse", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Yifei Yang", "authorId": "2108989265"}, {"name": "Zouying Cao", "authorId": "2253004027"}, {"name": "Hai Zhao", "authorId": "2251173128"}], "n_citations": 63}, "snippets": ["Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the model internal structure. In this paper, we propose a concise layer-wise structured pruner called \\textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\\% at pruning ratios of 25-30\\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the \\textit{LaCo} effectively inherits the parameters of the original model. Additionally, we perform ablation studies on various settings of \\textit{LaCo}. Finally, we discuss our motivation from the perspective of layer-wise similarity and evaluate the performance of the pruned LLMs across various pruning ratios\\footnote{\\url{https://github.com/yangyifei729/LaCo}}."], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271533761, "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jianwei Li", "authorId": "2261150750"}, {"name": "Yijun Dong", "authorId": "2310390224"}, {"name": "Qi Lei", "authorId": "2261081394"}], "n_citations": 6}, "snippets": ["This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."], "score": 0.8359375}, {"id": "(Malla et al., 2024)", "paper": {"corpus_id": 269605957, "title": "COPAL: Continual Pruning in Large Language Generative Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Srikanth Malla", "authorId": "51128743"}, {"name": "Joon Hee Choi", "authorId": "2300246153"}, {"name": "Chiho Choi", "authorId": "2301128189"}], "n_citations": 2}, "snippets": ["To the best of our knowledge, we are the first to introduce the concept of continual pruning that addresses pruning under a continual model adaptation setting, bypassing the requirement for model re-training."], "score": 0.849609375}], "table": null}, {"title": "Efficiency Gains and Performance Maintenance Techniques", "tldr": "Recent task-agnostic pruning methods achieve efficiency gains through diverse techniques that minimize performance degradation. These approaches balance computational efficiency and accuracy maintenance through specialized pruning criteria, module-wise reconstruction, and adaptive sparsity allocation. (11 sources)", "text": "\nPruning LLMs without retraining presents a fundamental challenge: achieving significant efficiency gains while preserving the model's performance capabilities. Modern pruning methods address this challenge through several key techniques:\n\n## Minimizing Model Disruption\n\nEffective pruning methods aim to identify and preserve the most important connections in the network while removing redundant ones. LLM-Pruner accomplishes this through a three-stage approach that detects dependencies between model structures, estimates their importance using both first-order and approximated Hessian information, and implements a rapid recovery stage with limited data <Paper corpusId=\"258823276\" paperTitle=\"(Ma et al., 2023)\" isShortName></Paper>. This careful approach to structural preservation helps maintain model functionality despite significant parameter reduction.\n\n## Reconstruction Techniques\n\nMany pruning methods implement reconstruction strategies to minimize the output difference between the dense and sparse models. SoBP (Structured Optimal Brain Pruning) employs module-wise reconstruction to mitigate information loss, which enables effective pruning without reliance on specialized hardware or extensive post-pruning computations <Paper corpusId=\"273901152\" paperTitle=\"(Wei et al., 2024)\" isShortName></Paper>. Similarly, newer approaches like DSnoT minimize reconstruction error between dense and sparse LLMs by iteratively pruning and regrowing weights based on statistical properties, avoiding traditional backpropagation-based fine-tuning <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"264128029\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n## Adaptive Sparsity Allocation\n\nRather than applying uniform sparsity across all layers, more sophisticated approaches implement adaptive sparsity allocation. FLAP introduces novel pruning metrics, adaptive global model compression strategies, and robust compensation mechanisms to maintain perplexity and zero-shot performance without retraining <Paper corpusId=\"266362404\" paperTitle=\"(An et al., 2023)\" isShortName></Paper>. BESA (Blockwise Parameter-Efficient Sparsity Allocation) allocates layer-specific sparsity in a differentiable manner, reducing performance degradation after pruning <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>.\n\n## Inference-Aware Criteria\n\nA critical advancement in recent pruning approaches is the development of inference-aware pruning criteria. These methods explicitly consider how pruning affects the model's inference capabilities rather than focusing solely on training-time metrics. For example, the depth-2 pruning approach with inference-aware criteria and two-step reconstruction significantly reduces computational costs and hardware requirements while maintaining superior performance across various datasets and models <Paper corpusId=\"271533761\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\n## Leveraging Activation Patterns\n\nUnderstanding emergent activation patterns in LLMs has led to improved pruning strategies. Wanda's success in using the product of weight magnitudes and input activations has inspired methods like RIA (Relative Importance Assessment), which re-evaluates weight importance based on input and output connections <Paper corpusId=\"277043299\" paperTitle=\"(Xu et al., 2025)\" isShortName></Paper>. OWL (Outlier Weighed Layerwise sparsity) incorporates non-uniform layerwise sparsity ratios proportional to the outlier ratio observed within each layer, creating more effective alignment between weight sparsity and activation patterns <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829692\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper>.\n\n## Hardware Compatibility\n\nEfficient pruning methods must also consider hardware compatibility to translate parameter reduction into actual computational speedups. Some recent methods focus on creating sparse patterns that align with specific hardware acceleration capabilities, while others like SliceGPT replace weight matrices with smaller dense matrices to reduce the embedding dimension, enabling models to run on fewer GPUs without additional code optimization <Paper corpusId=\"273962638\" paperTitle=\"(Cunegatti et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267301573\" paperTitle=\"(Ashkboos et al., 2024)\" isShortName></Paper>.\n\nThrough these diverse approaches, modern task-agnostic pruning methods have achieved remarkable efficiency gains while maintaining model performance, making LLMs more accessible for deployment in resource-constrained environments.", "citations": [{"id": "(Ma et al., 2023)", "paper": {"corpus_id": 258823276, "title": "LLM-Pruner: On the Structural Pruning of Large Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Xinyin Ma", "authorId": "15532066"}, {"name": "Gongfan Fang", "authorId": "150110431"}, {"name": "Xinchao Wang", "authorId": "48631088"}], "n_citations": 440}, "snippets": ["We propose a novel framework, LLM-Pruner, for the task-agnostic compression of the large language model. To the best of our knowledge, LLM-Pruner is the first framework designed for structured pruning of LLMs", "Since our goal is to compress LLMs with reduced data dependency and expedited post-training, how to prune model with the minimal disruption to the origin is crucial. To accomplish this, we propose a dependency detection algorithm that identifies all the dependent structures within the model. Once the coupled structure is identified, we employ an efficient importance estimation strategy to select the optimal group for pruning under the task-agnostic setting, where the first-order information and an approximated hessian information is taken into account. Finally, a rapid recovery stage is executed to post-train the pruned model with limited data."], "score": 0.88037109375}, {"id": "(Wei et al., 2024)", "paper": {"corpus_id": 273901152, "title": "Structured Optimal Brain Pruning for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiateng Wei", "authorId": "2330242586"}, {"name": "Quan Lu", "authorId": "2329898504"}, {"name": "Ning Jiang", "authorId": "2329738680"}, {"name": "Siqi Li", "authorId": "2258340244"}, {"name": "Jingyang Xiang", "authorId": "2256984205"}, {"name": "Jun Chen", "authorId": "2257200295"}, {"name": "Yong Liu", "authorId": "2257376000"}], "n_citations": 4}, "snippets": ["The massive parameters and computational demands hinder the widespread application of Large Language Models (LLMs). Network pruning provides a practical solution to this problem. However, existing pruning works for LLMs mainly focus on unstructured pruning or necessitate post-pruning fine-tuning. The former relies on special hardware to accelerate computation, while the latter may need substantial computational resources. In this paper, we introduce a retraining-free structured pruning method called SoBP (Structured Optimal Brain Pruning). It leverages global first-order information to select pruning structures, then refines them with a local greedy approach, and finally adopts module-wise reconstruction to mitigate information loss."], "score": 0.89794921875}, {"id": "(Yi et al., 2025)", "paper": {"corpus_id": 276079889, "title": "Symmetric Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kai Yi", "authorId": "2292143960"}, {"name": "Peter Richt'arik", "authorId": "2342412598"}], "n_citations": 0}, "snippets": ["Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."], "score": 0.85400390625}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 264128029, "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yu-xin Zhang", "authorId": "2108078624"}, {"name": "Lirui Zhao", "authorId": "2258678648"}, {"name": "Mingbao Lin", "authorId": "49352079"}, {"name": "Yunyun Sun", "authorId": "2258670567"}, {"name": "Yiwu Yao", "authorId": "2258671504"}, {"name": "Xingjia Han", "authorId": "2258598205"}, {"name": "Jared Tanner", "authorId": "2258549938"}, {"name": "Shiwei Liu", "authorId": "2258718674"}, {"name": "Rongrong Ji", "authorId": "2258551942"}], "n_citations": 43}, "snippets": ["The ever-increasing large language models (LLMs), though opening a potential path for the upcoming artificial general intelligence, sadly drops a daunting obstacle on the way towards their on-device deployment. As one of the most well-established pre-LLMs approaches in reducing model complexity, network pruning appears to lag behind in the era of LLMs, due mostly to its costly fine-tuning (or re-training) necessity under the massive volumes of model parameter and training data. To close this industry-academia gap, we introduce Dynamic Sparse No Training (DSnoT), a training-free fine-tuning approach that slightly updates sparse LLMs without the expensive backpropagation and any weight updates. Inspired by the Dynamic Sparse Training, DSnoT minimizes the reconstruction error between the dense and sparse LLMs, in the fashion of performing iterative weight pruning-and-growing on top of sparse LLMs. To accomplish this purpose, DSnoT particularly takes into account the anticipated reduction in reconstruction error for pruning and growing, as well as the variance w.r.t. different input data for growing each weight. This practice can be executed efficiently in linear time since its obviates the need of backpropagation for fine-tuning LLMs. Extensive experiments on LLaMA-V1/V2, Vicuna, and OPT across various benchmarks demonstrate the effectiveness of DSnoT in enhancing the performance of sparse LLMs, especially at high sparsity levels. For instance, DSnoT is able to outperform the state-of-the-art Wanda by 26.79 perplexity at 70% sparsity with LLaMA-7B. Our paper offers fresh insights into how to fine-tune sparse LLMs in an efficient training-free manner and open new venues to scale the great potential of sparsity to LLMs. Codes are available at https://github.com/zyxxmu/DSnoT."], "score": 0.0}, {"id": "(An et al., 2023)", "paper": {"corpus_id": 266362404, "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yongqi An", "authorId": "2167834971"}, {"name": "Xu Zhao", "authorId": "2118489444"}, {"name": "Tao Yu", "authorId": "40418746"}, {"name": "Ming Tang", "authorId": "2113727378"}, {"name": "Jinqiao Wang", "authorId": "2241943585"}], "n_citations": 61}, "snippets": ["In this paper, we propose FLAP (FLuctuation-based Adaptive Structured Pruning), a retraining-free structured pruning framework explicitly designed for Large Language Models (LLMs). To address the challenges posed by structured pruning, we introduce a novel structured pruning metric, employ adaptive global model compression strategies, and implement robust compensation mechanisms designed to mitigate potential performance losses. Our empirical results affirm that the structured compression model crafted by FLAP can maintain perplexity and zero-shot performance without any retraining."], "score": 0.88671875}, {"id": "(Cunegatti et al., 2024)", "paper": {"corpus_id": 273962638, "title": "Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Elia Cunegatti", "authorId": "2162450849"}, {"name": "Leonardo Lucio Custode", "authorId": "2037391480"}, {"name": "Giovanni Iacca", "authorId": "2295670461"}], "n_citations": 0}, "snippets": ["In recent times, Large Language Models (LLMs) have shown incredible performance over almost any language task (Wei et al., 2022)(Min et al., 2021)(Chang et al., 2023). However, their performance tends to grow with their sizes (i.e., the number of trainable parameters), which in turn is proportional to the computational burden required to train and then use such models. One way to reduce the computational cost of LLMs is through network pruning, i.e., algorithms that remove parameters while minimizing performance degradation. This approach has been extensively studied on Convolutional Neural Networks (CNNs) [13,25,43,(Evci et al., 2019), but nowadays the focus has shifted towards pre-trained models [41,42,22]. This shift has required a change of paradigm in pruning techniques: in fact, while in CNNs the main paradigm is iterative pruning (with re-training) [13], with pre-trained models (such as LLMs) in most cases it is not possible to fully re-train such models, because (Ashkboos et al., 2024) training data are often not accessible, and (2) full re-training would be anyway too expensive. This calls for \"exploiting\" as much as possible the information contained in a pre-trained model to obtain a performant sparse version of it, using weight's information (Jaiswal et al., 2023), activations (Sun et al., 2023)[39], or reconstruction error (Frantar et al., 2023), without the need for re-training. More recently, a new category of pruning algorithms, which we may call top-up algorithms (i.e., methods that can be applied on top of a given pruning algorithm for LLMs), has emerged, aiming at further improving pruning performance. Such approaches can be divided into two categories: those that minimize the reconstruction error [18](Xu et al., 2024)(Zhang et al., 2023), and those that impose non-uniform sparsity distribution modifying the block-wise sparsity (Yin et al., 2023)."], "score": 0.84375}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268032346, "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2153917002"}, {"name": "Wenqi Shao", "authorId": "2283133523"}, {"name": "Mengzhao Chen", "authorId": "2287768783"}, {"name": "Shitao Tang", "authorId": "2287949030"}, {"name": "Kai-Chuang Zhang", "authorId": "2273778831"}, {"name": "Peng Gao", "authorId": "2269823523"}, {"name": "Fengwei An", "authorId": "2287838451"}, {"name": "Yu Qiao", "authorId": "2256992387"}, {"name": "Ping Luo", "authorId": "2253674868"}], "n_citations": 32}, "snippets": ["Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."], "score": 0.84326171875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 271533761, "title": "Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jianwei Li", "authorId": "2261150750"}, {"name": "Yijun Dong", "authorId": "2310390224"}, {"name": "Qi Lei", "authorId": "2261081394"}], "n_citations": 6}, "snippets": ["This paper introduces a novel approach to pruning large language models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware pruning criteria. These methods surpass traditional metrics and eliminate the need for computationally expensive retraining. Our two-step reconstruction technique further mitigates pruning errors, ensuring superior performance across various datasets and models. Overall, our approach significantly reduces computational costs and hardware requirements, offering an efficient solution for pruning colossal LLMs."], "score": 0.8359375}, {"id": "(Xu et al., 2025)", "paper": {"corpus_id": 277043299, "title": "Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Chi Xu", "authorId": "2350425334"}, {"name": "Gefei Zhang", "authorId": "2350430287"}, {"name": "Yantong Zhu", "authorId": "2350478667"}, {"name": "Luca Benini", "authorId": "2323368873"}, {"name": "Guosheng Hu", "authorId": "2282196819"}, {"name": "Yawei Li", "authorId": "2323432053"}, {"name": "Zhihong Zhang", "authorId": "2350803789"}], "n_citations": 1}, "snippets": ["Recent works have introduced innovative approaches in this domain. SparseGPT (Frantar et al., 2023) formalizes the problem of pruning LLMs by solving a local layer-wise reconstruction problem, where their pruning metric and weight update procedure is inspired from Optimal Brain Surgeon (OBS) (Hassibi et al., 1993). Wanda (Kwon et al., 2022) streamlines the process by simplifying SparseGPT's (Frantar et al., 2023) methodology, and explores the weight magnitude and activations as a criterion for pruning, offering a simple yet effective strategy to achieve high sparsity ratios. RIA [8] also focuses on metrics related to weights and activations but introduces channel permutation (Pool et al., 2021) to maximize the retention of important weights under N:M sparsity. Pruner Zero [9] employs evolutionary algorithms to discover optimal pruning metrics, providing a comprehensive framework for metric exploration."], "score": 0.81787109375}, {"id": "(Yin et al., 2023)", "paper": {"corpus_id": 263829692, "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Lu Yin", "authorId": "2254142682"}, {"name": "You Wu", "authorId": "2325206905"}, {"name": "Zhenyu (Allen) Zhang", "authorId": "2109338656"}, {"name": "Cheng-Yu Hsieh", "authorId": "2256992922"}, {"name": "Yaqing Wang", "authorId": "2257105674"}, {"name": "Yiling Jia", "authorId": "2257230381"}, {"name": "Mykola Pechenizkiy", "authorId": "1691997"}, {"name": "Yi Liang", "authorId": "2260290217"}, {"name": "Zhangyang Wang", "authorId": "2254949434"}, {"name": "Shiwei Liu", "authorId": "2255081092"}], "n_citations": 102}, "snippets": ["Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Inspired by this finding, we introduce a novel LLM pruning methodology that incorporates a tailored set of non-uniform layerwise sparsity ratios, termed as Outlier Weighed Layerwise sparsity (OWL). The sparsity ratio of OWL is proportional to the outlier ratio observed within each layer, facilitating a more effective alignment between layerwise weight sparsity and outlier ratios. Our empirical evaluation, conducted across the LLaMA-V1 family and OPT, spanning various benchmarks, demonstrates the distinct advantages offered by OWL over previous methods. For instance, OWL exhibits a remarkable performance gain, surpassing the state-of-the-art Wanda and SparseGPT by 61.22 and 6.80 perplexity at a high sparsity level of 70%, respectively, while delivering 2.6x end-to-end inference speed-up in the DeepSparse inference engine. Codes are available at https://github.com/luuyin/OWL."], "score": 0.0}, {"id": "(Ashkboos et al., 2024)", "paper": {"corpus_id": 267301573, "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Saleh Ashkboos", "authorId": "9543395"}, {"name": "Maximilian L. Croci", "authorId": "2008063761"}, {"name": "Marcelo Gennari do Nascimento", "authorId": "2281641743"}, {"name": "Torsten Hoefler", "authorId": "2258547286"}, {"name": "James Hensman", "authorId": "2266803418"}], "n_citations": 184}, "snippets": ["Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression"], "score": 0.0}], "table": null}, {"title": "Comparative Performance and Trade-offs", "tldr": "Recent task-agnostic pruning methods exhibit varying performance across different models and sparsity levels, with methods like SparseGPT generally achieving better results at higher computational costs, while Wanda offers a favorable balance of efficiency and performance with simpler implementation. (10 sources)", "text": "\nWhen comparing recent task-agnostic pruning methods for LLMs, clear trade-offs emerge between computational complexity, pruning performance, and implementation difficulty. These trade-offs are critical considerations when selecting appropriate pruning approaches for specific deployment scenarios.\n\n## Performance Across Sparsity Levels\n\nSparseGPT remains a performance leader at high sparsity levels, allowing models to reach up to 60% unstructured sparsity with negligible increases in perplexity <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. However, newer methods like AlphaPruning have demonstrated substantial improvements when applied to existing pruning techniques. For instance, when applied to LLaMA-7B at 70% sparsity, AlphaPruning reduced perplexity by 61.91 for Wanda and 7.76 for SparseGPT, indicating that SparseGPT's initial performance was already stronger <Paper corpusId=\"273350592\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>.\n\nWanda has proven particularly effective at moderate sparsity levels, achieving 60% sparsity on LLaMA-7B with minimal performance degradation across multiple downstream tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. However, specialized methods like SEAP (Sparse Expert Activation Pruning) have surpassed both Wanda and FLAP by over 20% at 50% pruning ratios, experiencing only a 2.2% performance drop compared to the dense model at 20% pruning <Paper corpusId=\"276928323\" paperTitle=\"(Liang et al., 2025)\" isShortName></Paper>.\n\n## Computational Complexity\n\nA significant trade-off exists between pruning performance and computational requirements. SparseGPT's use of diagonal Hessian approximations allows for precise pruning but comes with high computational complexity and substantial hardware resource demands <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. In contrast, Wanda eliminates the need for Hessian approximations by simply multiplying weights with input activations, significantly reducing computational requirements while maintaining competitive performance <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\nThis computational efficiency has made Wanda a popular baseline for subsequent pruning methods. Many newer approaches either use SparseGPT and Wanda as baselines or build upon their foundations <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. For example, RIA extends Wanda by re-evaluating weight importance based on all input and output connections, offering performance improvements at minimal additional computational cost <Paper corpusId=\"276079889\" paperTitle=\"(Yi et al., 2025)\" isShortName></Paper>.\n\n## Implementation Challenges and Hyperparameter Sensitivity\n\nImplementation complexity varies significantly across pruning methods. While SparseGPT's optimization-based approach delivers outstanding performance, it requires sophisticated implementation and careful management of memory constraints <Paper corpusId=\"273501976\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Methods like ADMM build on SparseGPT by incorporating the Alternating Direction Method of Multipliers to restore model performance after pruning through an iterative mask selection process <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266818263\" paperTitle=\"(Boza, 2024)\" isShortName></Paper>.\n\nHyperparameter sensitivity represents another critical trade-off. Layer-wise pruning approaches like Wanda can result in significant perturbation to the model's output and require meticulous tuning of pruning rates, which may adversely affect overall model performance <Paper corpusId=\"268032346\" paperTitle=\"(Xu et al., 2024)\" isShortName></Paper>. This sensitivity has motivated the development of methods like GBLM-Pruner, which leverages gradients from pretrained LLMs to determine pruning importance <Paper corpusId=\"265050936\" paperTitle=\"(Das et al., 2023)\" isShortName></Paper>.\n\n## Model and Task Generalization\n\nThe generalization capability of pruning methods across different model architectures and tasks represents another important consideration. SparseGPT has demonstrated broad applicability, working effectively on models ranging from OPT-175B to BLOOM-176B <Paper corpusId=\"255372747\" paperTitle=\"(Frantar et al., 2023)\" isShortName></Paper>. Similarly, Wanda has been extensively evaluated on LLaMA and LLaMA-2 across various language benchmarks <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nHowever, performance can vary significantly depending on the specific model architecture and task requirements. Recent research has found that while SparseGPT, Wanda, and RIA all prune models using a combination of weights and activations, their different metrics and strategies result in varying effectiveness across different scenarios <Paper corpusId=\"276774084\" paperTitle=\"(Ding et al., 2025)\" isShortName></Paper>.\n\nAs the field continues to evolve, newer methods increasingly aim to combine the strengths of earlier approaches while addressing their limitations. For instance, OWL integrates both Wanda and SparseGPT, proposing a metric to allocate varying pruning rates across different layers <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This trend toward hybrid approaches suggests that the future of task-agnostic LLM pruning likely lies in methods that balance computational efficiency, implementation simplicity, and robust performance across diverse models and tasks.", "citations": [{"id": "(Frantar et al., 2023)", "paper": {"corpus_id": 255372747, "title": "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Elias Frantar", "authorId": "1502248377"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 734}, "snippets": ["We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. We can execute SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, in under 4.5 hours, and can reach 60% unstructured sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches. The code is available at: https://github.com/IST-DASLab/sparsegpt."], "score": 0.0}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 273350592, "title": "AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved Layer-wise Pruning of Large Language Models", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Haiquan Lu", "authorId": "2311995497"}, {"name": "Yefan Zhou", "authorId": "2111405998"}, {"name": "Shiwei Liu", "authorId": "2255081092"}, {"name": "Zhangyang Wang", "authorId": "2284563898"}, {"name": "Michael W. Mahoney", "authorId": "2249392052"}, {"name": "Yaoqing Yang", "authorId": "2249529142"}], "n_citations": 10}, "snippets": ["For example, in the case of LLaMA-7B with a sparsity of 70%, AlphaPruning produces sparse networks with a perplexity of 231.01, significantly outperforming the Magnitude-based pruning baseline of 48419.13. Notably, when applied to Wanda and SparseGPT, two robust LLM pruning methods, AlphaPruning still achieves substantial perplexity reductions, evidenced by a decrease of 61.91 for Wanda and 7.76 for SparseGPT, in the case of LLaMA-7B with a sparsity of 70%."], "score": 0.88037109375}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Unstructured pruning is an optimization technique that achieves model sparsity by evaluating the importance of individual weights. Its flexibility and high compression rates make it a key method for optimizing large language models (LLMs). Unstructured pruning can achieve extremely high compression rates; for instance, Wanda achieves a 60% sparsity rate on LLaMA-7B with minimal performance degradation across multiple downstream tasks (Sun et al., 2023), while Flash-LLM achieves a 70% sparsity rate on OPT-175B, significantly reducing storage requirements with less than 2% performance degradation during inference [28]", "SparseGPT (Frantar et al., 2023), on the other hand, introduces a diagonal Hessian approximation to assess the impact of weights on errors, enabling more precise pruning at the cost of high computational complexity and hardware resource requirements. Wanda (Sun et al., 2023) simplifies the SparseGPT algorithm by eliminating the need for Hessian approximations and instead computes pruning metrics by multiplying weights with input activations. This simplification significantly reduces computational complexity while achieving a balance between high accuracy and efficiency. Following this approach, many subsequent methods use SparseGPT and Wanda as baselines or build upon their foundations. RIA (Zhang et al., 2024) introduces a post-training pruning method that re-evaluates the importance of each weight element based on all input and output connections. ADMM (Boza, 2024) builds on SparseGPT by incorporating the Alternating Direction Method of Multipliers (ADMM) to restore model performance after pruning, using a simple iterative mask selection process for pruning. OWL [32] integrates both Wanda and SparseGPT, proposing the OWL metric to allocate varying pruning rates across different layers."], "score": 0.87548828125}, {"id": "(Liang et al., 2025)", "paper": {"corpus_id": 276928323, "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xun Liang", "authorId": "2268495952"}, {"name": "Hanyu Wang", "authorId": "2284861141"}, {"name": "Huayi Lai", "authorId": "2349573796"}, {"name": "Simin Niu", "authorId": "2268393907"}, {"name": "Shichao Song", "authorId": "2268434524"}, {"name": "Jiawei Yang", "authorId": "2303425635"}, {"name": "Jihao Zhao", "authorId": "2326243408"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Bo Tang", "authorId": "2268400606"}, {"name": "Zhiyu Li", "authorId": "2268429641"}], "n_citations": 0}, "snippets": ["This paper introduces Sparse Expert Activation Pruning (SEAP), a training-free pruning method that selectively retains task-relevant parameters to reduce inference overhead. Inspired by the clustering patterns of hidden states and activations in LLMs, SEAP identifies task-specific expert activation patterns and prunes the model while preserving task performance and enhancing computational efficiency. Experimental results demonstrate that SEAP significantly reduces computational overhead while maintaining competitive accuracy. Notably, at 50% pruning, SEAP surpasses both WandA and FLAP by over 20%, and at 20% pruning, it incurs only a 2.2% performance drop compared to the dense model."], "score": 0.853515625}, {"id": "(Yi et al., 2025)", "paper": {"corpus_id": 276079889, "title": "Symmetric Pruning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Kai Yi", "authorId": "2292143960"}, {"name": "Peter Richt'arik", "authorId": "2342412598"}], "n_citations": 0}, "snippets": ["Wanda (Sun et al., 2023) introduces a pruning metric that incorporates both weight magnitudes and corresponding input activations, achieving perplexity performance comparable to SparseGPT while surpassing simple magnitude-based pruning. The RIA method (Zhang et al., 2024) builds on Wanda by considering relative weight importance, offering performance improvements at minimal additional cost. Moreover, DSnoT (Zhang et al., 2023) proposes pruning and regrowing weights based on statistical properties (e.g., mean and variance) in each pruning row, obviating the need for retraining."], "score": 0.85400390625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 273501976, "title": "Pruning Foundation Models for High Accuracy without Retraining", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pu Zhao", "authorId": "2241612245"}, {"name": "Fei Sun", "authorId": "2327046378"}, {"name": "Xuan Shen", "authorId": "2007668856"}, {"name": "Pinrui Yu", "authorId": "2241698013"}, {"name": "Zhenglun Kong", "authorId": "32409528"}, {"name": "Yanzhi Wang", "authorId": "2290628977"}, {"name": "Xue Lin", "authorId": "2322988586"}], "n_citations": 13}, "snippets": ["The traditional pruning techniques, which finetune or retrain models (Li et al., 2020) on full datasets for many epochs (i.e., pruning-aware training), are too expensive for LLMs in terms of data and GPU resources. Thus, post-training pruning based on well-pre-trained models with reduced resource requirements represents a more reasonable approach for LLMs. Notably, SparseGPT (Frantar et al., 2023) is the representative post-training pruning work with outstanding performance. It reduces memory cost by sequentially loading transformer blocks, one at a time, instead of loading the whole model. Moreover, it reduces the data cost by using only a small amount of calibration data, eliminating the retraining process on massive data. Besides the optimization based SparseGPT, there are some other heuristic posttraining pruning methods such as (Sun et al., 2023;Zhang et al., 2024), achieving accuracy close to SparseGPT."], "score": 0.826171875}, {"id": "(Boza, 2024)", "paper": {"corpus_id": 266818263, "title": "Fast and Effective Weight Update for Pruned Large Language Models", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Vladim\u00edr Boza", "authorId": "2315325819"}], "n_citations": 5}, "snippets": ["Pruning large language models (LLMs) is a challenging task due to their enormous size. The primary difficulty is fine-tuning the model after pruning, which is needed to recover the lost performance caused by dropping weights. Recent approaches have either ignored fine-tuning entirely, focusing on efficient pruning criteria, or attempted layer-wise weight updates, preserving the behavior of each layer. However, even layer-wise weight updates can be costly for LLMs, and previous works have resorted to various approximations. In our paper, we propose a fast and effective weight update algorithm for pruned layers based on the Alternating Direction Method of Multipliers (ADMM). We further extend it with a simple gradual pruning mask selection and achieve state-of-the-art pruning performance across a wide range of LLMs. Code is available at https://github.com/fmfi-compbio/admm-pruning."], "score": 0.0}, {"id": "(Xu et al., 2024)", "paper": {"corpus_id": 268032346, "title": "BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Peng Xu", "authorId": "2153917002"}, {"name": "Wenqi Shao", "authorId": "2283133523"}, {"name": "Mengzhao Chen", "authorId": "2287768783"}, {"name": "Shitao Tang", "authorId": "2287949030"}, {"name": "Kai-Chuang Zhang", "authorId": "2273778831"}, {"name": "Peng Gao", "authorId": "2269823523"}, {"name": "Fengwei An", "authorId": "2287838451"}, {"name": "Yu Qiao", "authorId": "2256992387"}, {"name": "Ping Luo", "authorId": "2253674868"}], "n_citations": 32}, "snippets": ["Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance."], "score": 0.84326171875}, {"id": "(Das et al., 2023)", "paper": {"corpus_id": 265050936, "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Rocktim Jyoti Das", "authorId": "2211732585"}, {"name": "Liqun Ma", "authorId": "2243392466"}, {"name": "Zhiqiang Shen", "authorId": "2243374493"}], "n_citations": 19}, "snippets": ["Large Language Models (LLMs) with billions of parameters are prime targets for network pruning, removing some model weights without hurting performance. Prior approaches such as magnitude pruning, SparseGPT, and Wanda, either concentrated solely on weights or integrated weights with activations for sparsity. However, they overlooked the informative gradients derived from pretrained LLMs. In this paper, we present a novel sparsity-centric pruning method for pretrained LLMs, termed Gradient-based Language Model Pruner (GBLM-Pruner). GBLM-Pruner leverages the first-order term of the Taylor expansion, operating in a training-free manner by harnessing properly normalized gradients from a few calibration samples to determine the pruning metric, and substantially outperforms competitive counterparts like SparseGPT and Wanda in multiple benchmarks."], "score": 0.86962890625}, {"id": "(Ding et al., 2025)", "paper": {"corpus_id": 276774084, "title": "Revisiting Large Language Model Pruning using Neuron Semantic Attribution", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yizhuo Ding", "authorId": "2237598833"}, {"name": "Xinwei Sun", "authorId": "2244778967"}, {"name": "Yanwei Fu", "authorId": "2244698019"}, {"name": "Guosheng Hu", "authorId": "2349205822"}], "n_citations": 2}, "snippets": ["In this study, we employed three state-of-the-art posttraining pruning methods: SparseGPT (Frantar et al., 2023), Wanda (Sun et al., 2023), and RIA (Zhang et al., 2024). These methods are designed to reduce the size of large language models (LLMs) while maintaining their performance across various tasks. Even though all three methods prune the model using a combination of weights W and activations X, they each use different metrics M and strategies to decide which parameters to prune."], "score": 0.9404296875}], "table": null}], "cost": 0.49391700000000005}}
