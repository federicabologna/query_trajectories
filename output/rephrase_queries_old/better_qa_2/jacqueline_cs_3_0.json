{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in scenarios where the teacher and student models are of identical or similar size (size-matched knowledge distillation), and how effective knowledge transfer is in those cases?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in scenarios where the teacher and student models are of identical or similar size (size-matched knowledge distillation), and in understanding how effective knowledge transfer is in those cases."}}, {"clarifying_question2": "Do you want to know more about when and why using a smaller or weaker teacher model\u2014rather than a conventionally larger or more powerful one\u2014can actually benefit the student model during knowledge distillation?", "clarifying_answer2": {"clarifying_answer": "Yes, I would like more detailed insights into when and why using a smaller or weaker teacher model can lead to more effective knowledge distillation, including any empirical findings or theoretical explanations about scenarios where this approach outperforms traditional large-teacher setups."}}, {"clarifying_question3": "Are you looking for evidence or examples from research where non-traditional knowledge distillation methods (such as mutual learning or online distillation approaches) have outperformed or matched the results of classic large-teacher/small-student distillation?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in concrete evidence or research examples where non-traditional knowledge distillation methods\u2014such as mutual learning, online distillation, or size-matched teacher/student configurations\u2014have achieved performance equal to or surpassing standard approaches that use a much larger teacher and a smaller student. Direct comparisons, case studies, or empirical results demonstrating these outcomes would be highly valuable."}}], "better_query": "What does current research reveal about the effectiveness of knowledge distillation when the teacher and student models are of similar or identical size (size-matched distillation), and how does their performance compare to the traditional setup with a much larger teacher model?", "better_answer": {"sections": [{"title": "Introduction to Knowledge Distillation", "tldr": "Knowledge distillation is a technique that transfers knowledge from larger teacher models to smaller student models, enabling more efficient deployment while maintaining comparable performance. This approach has evolved to include various distillation schemes such as offline, online, and self-distillation methods. (7 sources)", "text": "\nKnowledge distillation is a powerful model compression technique that transfers the knowledge learned by large, complex models (teachers) to smaller, simpler models (students). Originally proposed by Hinton et al., this approach creates more efficient models that can be deployed in resource-constrained environments while maintaining competitive performance <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>. The process typically involves training the student model using a weighted combination of the original loss from ground-truth labels and a distillation loss calculated from the teacher model's soft outputs <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nThe effectiveness of knowledge distillation is evident across various applications. For instance, DistilBERT maintains 97% of BERT's performance while reducing the parameter count by 40% <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Similarly, shallow BiLSTM-based models have achieved performance comparable to the ELMo language model with approximately 100 times fewer parameters and 15 times faster inference <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This makes knowledge distillation particularly valuable for deploying models on devices with limited resources, such as mobile phones and embedded devices <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.\n\nSeveral distillation schemes have emerged over time. Offline distillation follows the traditional approach where the teacher is trained first and then guides the student. Online distillation involves simultaneous training of both models, while self-distillation uses the same architecture for both teacher and student <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper>. The SPEQ (Stochastic Precision Ensemble for Quantized DNNs) method exemplifies an innovative self-training paradigm where high-precision teachers and low-bit-width students derived from the same source model are trained collaboratively <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222066738\" paperTitle=\"(Boo et al., 2020)\" isShortName></Paper>.\n\nKnowledge distillation has become especially important in the growing field of Tiny Machine Learning (TinyML), which aims to deploy AI models on microcontrollers and IoT devices with severe memory constraints <Paper corpusId=\"264109973\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. Through system-algorithm co-design approaches, researchers continue to expand the capabilities of these compressed models, making them suitable for an increasingly diverse range of applications.", "citations": [{"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Wu et al., 2022)", "paper": {"corpus_id": 254564348, "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Hai Wu", "authorId": "2187214993"}, {"name": "Ruifei He", "authorId": "2053865518"}, {"name": "Hao Hao Tan", "authorId": "3218666"}, {"name": "Xiaojuan Qi", "authorId": "50844674"}, {"name": "Kaibin Huang", "authorId": "2149775992"}], "n_citations": 2}, "snippets": ["Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)", "SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones."], "score": 0.92041015625}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 270878760, "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "year": 2024, "venue": "ACM Transactions on Intelligent Systems and Technology", "authors": [{"name": "Chuanpeng Yang", "authorId": "2309482604"}, {"name": "Yao Zhu", "authorId": "2290230390"}, {"name": "Wang Lu", "authorId": "2309429610"}, {"name": "Yidong Wang", "authorId": "2293893759"}, {"name": "Qian Chen", "authorId": "2276257486"}, {"name": "Chenlong Gao", "authorId": "26155233"}, {"name": "Bingjie Yan", "authorId": "2276089941"}, {"name": "Yiqiang Chen", "authorId": "2276178270"}], "n_citations": 33}, "snippets": ["Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model."], "score": 0.9228515625}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}, {"id": "(Fuente et al., 2024)", "paper": {"corpus_id": 273963558, "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ra'ul de la Fuente", "authorId": "2330192141"}, {"name": "Luciano Radrig\u00e1n", "authorId": "2047941908"}, {"name": "Anibal S Morales", "authorId": "2330183718"}], "n_citations": 0}, "snippets": ["Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)", "Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models."], "score": 0.92041015625}, {"id": "(Boo et al., 2020)", "paper": {"corpus_id": 222066738, "title": "Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yoonho Boo", "authorId": "3350090"}, {"name": "Sungho Shin", "authorId": "1859825"}, {"name": "Jungwook Choi", "authorId": "2506452"}, {"name": "Wonyong Sung", "authorId": "66936521"}], "n_citations": 30}, "snippets": ["The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks."], "score": 0.0}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 264109973, "title": "Tiny Machine Learning: Progress and Futures [Feature]", "year": 2024, "venue": "IEEE Circuits and Systems Magazine", "authors": [{"name": "Ji Lin", "authorId": "2258439127"}, {"name": "Ligeng Zhu", "authorId": "20515689"}, {"name": "Wei-Ming Chen", "authorId": "2164114675"}, {"name": "Wei-Chen Wang", "authorId": "2258439498"}, {"name": "Song Han", "authorId": "2258448604"}], "n_citations": 55}, "snippets": ["Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today\u2019s \"large\" model might be tomorrow\u2019s \"tiny\" model. The scope of TinyML should evolve and adapt over time."], "score": 0.0}], "table": null}, {"title": "Effectiveness of Size-Matched Distillation", "tldr": "Size-matched distillation, where teacher and student models have identical architectures, has shown surprising effectiveness with students often outperforming their teachers. This phenomenon, known as self-distillation or Born-Again Networks, demonstrates performance improvements that can be further enhanced through multiple rounds of distillation. (8 sources)", "text": "\nContrary to traditional knowledge distillation where smaller students aim to approach the performance of larger teachers, research reveals that size-matched distillation can yield unexpected benefits. Furlanello et al. introduced Born-Again Networks (BANs), demonstrating that multiple rounds of distillation between models with identical architectures can surprisingly improve the student's performance beyond that of the teacher <Paper corpusId=\"245650327\" paperTitle=\"(Boschini et al., 2022)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. Their experiments with BANs based on DenseNets achieved state-of-the-art performance on CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, showing significant advantages from transferring knowledge between identically parameterized models <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nThis counterintuitive finding has been consistently observed across different domains. Gong et al. found that in size-matched distillation, the student model matches or outperforms its teacher, which differs from conventional distillation where students typically remain weaker than their teachers <Paper corpusId=\"247446679\" paperTitle=\"(Gong et al., 2022)\" isShortName></Paper>. Similarly, Team et al. observed large gains from distillation even when model sizes are identical for both student and teacher, with the most significant improvements occurring in the first round of distillation and diminishing returns in subsequent rounds <Paper corpusId=\"255998274\" paperTitle=\"(Team et al., 2023)\" isShortName></Paper>.\n\nSeveral theoretical explanations have been proposed for this phenomenon. The \"multi-view\" hypothesis suggests that self-distillation performs an implicit ensemble of various models <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Empirical evidence also indicates that self-distillation encourages students to find flatter minima, leading to better generalization <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Additionally, distillation may help avoid bad representations learned early by the student model, which is consistent with findings in offline reinforcement learning where additional data is often required to effectively scale models <Paper corpusId=\"255998274\" paperTitle=\"(Team et al., 2023)\" isShortName></Paper>.\n\nThe effectiveness of size-matched distillation extends to ensemble methods as well. Iordache et al. developed a technique that allows student architectures to be identical to those of individual teachers, enabling efficient knowledge transfer between same-sized models <Paper corpusId=\"273662110\" paperTitle=\"(Iordache et al., 2024)\" isShortName></Paper>. Furthermore, Li et al. and Furlanello et al. observed that repeatedly applying self-distillation on the same training data with identical architecture models provides additional performance gains on benchmark datasets <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"14659675\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.", "citations": [{"id": "(Boschini et al., 2022)", "paper": {"corpus_id": 245650327, "title": "Class-Incremental Continual Learning Into the eXtended DER-Verse", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Matteo Boschini", "authorId": "51096265"}, {"name": "Lorenzo Bonicelli", "authorId": "2123319338"}, {"name": "Pietro Buzzega", "authorId": "1429191945"}, {"name": "Angelo Porrello", "authorId": "51119730"}, {"name": "S. Calderara", "authorId": "2175529"}], "n_citations": 141}, "snippets": ["Furlanello et al. (Furlanello et al., 2018) show that multiple rounds of distillation between models with the same architecture (termed self-distillation) can surprisingly improve the performance of the student."], "score": 0.92919921875}, {"id": "(Furlanello et al., 2018)", "paper": {"corpus_id": 4110009, "title": "Born Again Neural Networks", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tommaso Furlanello", "authorId": "2067208583"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "Michael Tschannen", "authorId": "143902495"}, {"name": "L. Itti", "authorId": "7326223"}, {"name": "Anima Anandkumar", "authorId": "2047844"}], "n_citations": 1034}, "snippets": ["Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."], "score": 0.0}, {"id": "(Pareek et al., 2024)", "paper": {"corpus_id": 271039191, "title": "Understanding the Gains from Repeated Self-Distillation", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Divyansh Pareek", "authorId": "51176912"}, {"name": "S. S. Du", "authorId": "2330243848"}, {"name": "Sewoong Oh", "authorId": "2310392746"}], "n_citations": 6}, "snippets": ["Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1)."], "score": 0.9443359375}, {"id": "(Gong et al., 2022)", "paper": {"corpus_id": 247446679, "title": "CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Yuan Gong", "authorId": "145802952"}, {"name": "Sameer Khurana", "authorId": "40570741"}, {"name": "Andrew Rouditchenko", "authorId": "2110769738"}, {"name": "James R. Glass", "authorId": "145898106"}], "n_citations": 29}, "snippets": ["2. For both directions, the student model matches or outperforms its teacher. Usually, in knowledge distillation, the student model gets closer to, but is still weaker than, its teacher model."], "score": 0.927734375}, {"id": "(Team et al., 2023)", "paper": {"corpus_id": 255998274, "title": "Human-Timescale Adaptation in an Open-Ended Task Space", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Adaptive Agent Team", "authorId": "2202022003"}, {"name": "Jakob Bauer", "authorId": "2067935448"}, {"name": "Kate Baumli", "authorId": "1734809439"}, {"name": "Satinder Baveja", "authorId": "70336016"}, {"name": "Feryal M. P. Behbahani", "authorId": "145124447"}, {"name": "Avishkar Bhoopchand", "authorId": "7567594"}, {"name": "N. Bradley-Schmieg", "authorId": "1440707913"}, {"name": "Michael Chang", "authorId": "47235561"}, {"name": "Natalie Clay", "authorId": "2201776471"}, {"name": "Adrian Collister", "authorId": "69041729"}, {"name": "Vibhavari Dasagi", "authorId": "2871171"}, {"name": "Lucy Gonzalez", "authorId": "2202047544"}, {"name": "Karol Gregor", "authorId": "144717963"}, {"name": "Edward Hughes", "authorId": "37591038"}, {"name": "Sheleem Kashem", "authorId": "80976942"}, {"name": "Maria Loks-Thompson", "authorId": "2202022196"}, {"name": "Hannah Openshaw", "authorId": "2085293302"}, {"name": "Jack Parker-Holder", "authorId": "1410302742"}, {"name": "Shreyaan Pathak", "authorId": "1987123565"}, {"name": "Nicolas Perez Nieves", "authorId": "1999879303"}, {"name": "Nemanja Rakicevic", "authorId": "3287144"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Yannick Schroecker", "authorId": "3403061"}, {"name": "Jakub Sygnowski", "authorId": "3407592"}, {"name": "K. Tuyls", "authorId": "2274623"}, {"name": "Sarah York", "authorId": "143981350"}, {"name": "Alexander Zacherl", "authorId": "2156928090"}, {"name": "Lei M. Zhang", "authorId": "2152836492"}], "n_citations": 119}, "snippets": ["Additionally, we find that even when the model size is the same for both student and teacher, we observe large gains from distillation, for a constant total frame budget (Figure 16). We speculate that this is due to bad representations learned early on by the student agent (Cetin et al., 2022;Nikishin et al., 2022), which can be avoided by using distillation. This is also consistent with findings in offline RL, where additional data is often required to effectively scale the model (Reid et al., 2022). The effect is largest for the first round of distillation, with diminishing returns in subsequent rounds of distillation (Figure E.5)."], "score": 0.92041015625}, {"id": "(Lee et al., 2023)", "paper": {"corpus_id": 258309753, "title": "Bayesian Optimization Meets Self-Distillation", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "HyunJae Lee", "authorId": "2279782391"}, {"name": "Heon Song", "authorId": "2157781156"}, {"name": "Hyeonsoo Lee", "authorId": "2279825857"}, {"name": "Gi-hyeon Lee", "authorId": "2215364195"}, {"name": "Suyeong Park", "authorId": "2199808369"}, {"name": "Donggeun Yoo", "authorId": "2283756"}], "n_citations": 2}, "snippets": ["Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization."], "score": 0.94921875}, {"id": "(Iordache et al., 2024)", "paper": {"corpus_id": 273662110, "title": "Multi-Level Feature Distillation of Joint Teachers Trained on Distinct Image Datasets", "year": 2024, "venue": "IEEE Workshop/Winter Conference on Applications of Computer Vision", "authors": [{"name": "Adrian Iordache", "authorId": "2328085900"}, {"name": "B. Alexe", "authorId": "2365442"}, {"name": "R. Ionescu", "authorId": "2249763264"}], "n_citations": 1}, "snippets": ["Our method allows student architectures to be identical to those of the individual teachers."], "score": 0.919921875}, {"id": "(Li et al., 2017)", "paper": {"corpus_id": 14659675, "title": "Learning from Noisy Labels with Distillation", "year": 2017, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Yuncheng Li", "authorId": "2110470764"}, {"name": "Jianchao Yang", "authorId": "1706007"}, {"name": "Yale Song", "authorId": "2317183"}, {"name": "Liangliang Cao", "authorId": "48749954"}, {"name": "Jiebo Luo", "authorId": "33642939"}, {"name": "Li-Jia Li", "authorId": "2040091191"}], "n_citations": 550}, "snippets": ["The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use \"side\" information, including a small clean dataset and label relations in knowledge graph, to \"hedge the risk\" of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains."], "score": 0.0}], "table": null}, {"title": "Performance Comparison with Traditional Distillation", "tldr": "Traditional distillation with larger teachers doesn't always outperform size-matched distillation due to capacity gaps that hinder knowledge transfer. Studies show that excessively large teachers can actually degrade student performance, leading to the development of multi-step approaches using intermediate-sized \"teacher assistants\" to bridge capacity gaps. (12 sources)", "text": "\nWhen comparing size-matched distillation to traditional approaches with significantly larger teacher models, researchers have identified several important trade-offs. Contrary to intuition, increasing the teacher model size doesn't always improve knowledge distillation effectiveness. Cho et al. demonstrated that more accurate, larger teachers don't necessarily make better teachers, raising fundamental questions about the distillation process in such scenarios <Paper corpusId=\"249209742\" paperTitle=\"(Ojha et al., 2022)\" isShortName></Paper> <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. This finding has been corroborated by multiple studies showing that student performance can actually degrade when the capacity gap between teacher and student becomes too large <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nThe capacity gap phenomenon manifests in practical applications as well. Experiments with CNN architectures of varying depths revealed that continuing to increase the teacher network's depth beyond a certain point failed to improve distillation performance and sometimes led to worse results <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>. Similarly, in natural language processing tasks, researchers observed significant performance gaps when distilling from very large models to much smaller ones <Paper corpusId=\"272753230\" paperTitle=\"(Ballout et al., 2024)\" isShortName></Paper>.\n\nTo address these limitations, researchers have developed several approaches. One effective strategy is multi-step knowledge distillation, which employs an intermediate-sized network (termed a \"teacher assistant\") to bridge the gap between large teachers and small students <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. This approach has shown promising results across various datasets and model architectures. Alternative methods involve modifying distillation algorithms or employing multi-stage learning to better handle capacity disparities <Paper corpusId=\"276776153\" paperTitle=\"(Medina et al., 2025)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper> <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper>.\n\nStudent model capacity also affects distillation outcomes. When student models are excessively large relative to the task complexity, they may learn more slowly and become more susceptible to overfitting, resulting in diminished generalization performance <Paper corpusId=\"271270180\" paperTitle=\"(Violos et al., 2024)\" isShortName></Paper>. Conversely, knowledge distillation shows optimal results when teacher models achieve strong performance while maintaining an appropriate size advantage over student networks <Paper corpusId=\"268543692\" paperTitle=\"(Mukherjee et al., 2023)\" isShortName></Paper>. Some researchers have proposed building more powerful teacher models without changing the capacity (width and depth) of the student model to optimize performance transfer <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper>.\n\nIn object detection tasks, comparative studies between one-stage and two-stage knowledge distillation revealed that when no powerful teacher is available, one-stage approaches (similar to size-matched distillation) can achieve comparable or even better performance than two-stage distillation with weaker teachers. However, two-stage distillation with strong teachers still yields the best overall results, suggesting that traditional distillation maintains advantages when appropriate teacher-student pairings are selected <Paper corpusId=\"260378128\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Ojha et al., 2022)", "paper": {"corpus_id": 249209742, "title": "What Knowledge Gets Distilled in Knowledge Distillation?", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Utkarsh Ojha", "authorId": "47284770"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "144756076"}], "n_citations": 19}, "snippets": ["(Cho et al., 2019) showed that performing knowledge distillation with a bigger teacher does not necessarily improve the student's performance over that with a smaller teacher, and thus raised questions about the effectiveness of the distillation procedure in such cases."], "score": 0.923828125}, {"id": "(Cho et al., 2019)", "paper": {"corpus_id": 203642130, "title": "On the Efficacy of Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Jang Hyun Cho", "authorId": "39878282"}, {"name": "B. Hariharan", "authorId": "73710317"}], "n_citations": 619}, "snippets": ["In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."], "score": 0.0}, {"id": "(Gao, 2023)", "paper": {"corpus_id": 258048760, "title": "A Survey on Recent Teacher-student Learning Studies", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Min Gao", "authorId": "2147417425"}], "n_citations": 3}, "snippets": ["When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network."], "score": 0.92431640625}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}, {"id": "(Ballout et al., 2024)", "paper": {"corpus_id": 272753230, "title": "Efficient Knowledge Distillation: Empowering Small Language Models with Teacher Model Insights", "year": 2024, "venue": "International Conference on Applications of Natural Language to Data Bases", "authors": [{"name": "Mohamad Ballout", "authorId": "1491169373"}, {"name": "U. Krumnack", "authorId": "1751765"}, {"name": "Gunther Heidemann", "authorId": "2238205582"}, {"name": "Kai-Uwe K\u00fchnberger", "authorId": "1743582"}], "n_citations": 3}, "snippets": ["Our research demonstrates an effective and straightforward approach for generating rationales from a large model to enhance a smaller model. However, a significant performance gap between the teacher model (the large model) and the student model (the small model) still exists. As illustrated in table 6, the flan-t5-3b, which serves as the teacher model, significantly outperforms the smaller t5-small model across all four datasets we tested."], "score": 0.9609375}, {"id": "(Medina et al., 2025)", "paper": {"corpus_id": 276776153, "title": "Mamba base PKD for efficient knowledge compression", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jos\u00e9 Medina", "authorId": "2348480395"}, {"name": "Amnir Hadachi", "authorId": "3221342"}, {"name": "Paul Honeine", "authorId": "2256850325"}, {"name": "Abdelaziz Bensrhair", "authorId": "2314690317"}], "n_citations": 0}, "snippets": ["However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."], "score": 0.916015625}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 159041406, "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Linfeng Zhang", "authorId": "50081570"}, {"name": "Jiebo Song", "authorId": "122151709"}, {"name": "Anni Gao", "authorId": "94970361"}, {"name": "Jingwei Chen", "authorId": "2144141924"}, {"name": "Chenglong Bao", "authorId": "2060106235"}, {"name": "Kaisheng Ma", "authorId": "2075321204"}], "n_citations": 865}, "snippets": ["Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."], "score": 0.0}, {"id": "(Passban et al., 2020)", "paper": {"corpus_id": 229679667, "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Peyman Passban", "authorId": "5062230"}, {"name": "Yimeng Wu", "authorId": "2000863903"}, {"name": "Mehdi Rezagholizadeh", "authorId": "1924511"}, {"name": "Qun Liu", "authorId": "2115900360"}], "n_citations": 122}, "snippets": ["Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques."], "score": 0.0}, {"id": "(Violos et al., 2024)", "paper": {"corpus_id": 271270180, "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge", "year": 2024, "venue": "European Signal Processing Conference", "authors": [{"name": "John Violos", "authorId": "2573061"}, {"name": "Symeon Papadopoulos", "authorId": "2276779989"}, {"name": "Y. Kompatsiaris", "authorId": "1715604"}], "n_citations": 4}, "snippets": ["When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices."], "score": 0.91455078125}, {"id": "(Mukherjee et al., 2023)", "paper": {"corpus_id": 268543692, "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM", "year": 2023, "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)", "authors": [{"name": "Suvaditya Mukherjee", "authorId": "2293143380"}, {"name": "Dev Vinod Chandan", "authorId": "2292319438"}, {"name": "Shreyas Dongre", "authorId": "2268069324"}], "n_citations": 0}, "snippets": ["Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network."], "score": 0.9306640625}, {"id": "(Yu, 2024)", "paper": {"corpus_id": 271956980, "title": "Bring the Power of Diffusion Model to Defect Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuyi Yu", "authorId": "2317142609"}], "n_citations": 1}, "snippets": ["In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."], "score": 0.927734375}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 260378128, "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Linfeng Zhang", "authorId": "50081570"}, {"name": "Kaisheng Ma", "authorId": "2075321204"}], "n_citations": 27}, "snippets": ["Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model", "For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance."], "score": 0.91552734375}], "table": null}, {"title": "Self-Distillation and Online Distillation Approaches", "tldr": "Self-distillation and online distillation represent important innovations where teacher and student models have identical architectures or are trained simultaneously. These approaches have shown remarkable effectiveness, often enabling students to outperform their identical teachers through mechanisms like knowledge transfer between network layers and collaborative training. (9 sources)", "text": "\nSelf-distillation is a specialized form of knowledge distillation where the teacher and student networks share identical architectures. Born-Again Networks (BANs) demonstrated that when a student is trained to match the output distribution of a teacher with the same architecture, it can surprisingly outperform the teacher <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. This counter-intuitive finding has been consistently observed across multiple studies, with researchers noting that repeatedly applying self-distillation with identical architecture models provides additional performance gains on benchmark datasets <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>.\n\nSeveral theoretical explanations have been proposed for the effectiveness of self-distillation. The \"multi-view\" hypothesis suggests that self-distillation performs an implicit ensemble of various models <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Empirical evidence indicates that self-distillation encourages students to find flatter minima, leading to better generalization <Paper corpusId=\"258309753\" paperTitle=\"(Lee et al., 2023)\" isShortName></Paper>. Self-distillation can also operate within the same network by transferring knowledge from deeper to shallower layers, thus achieving knowledge transfer and training optimization <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>. This approach not only enhances accuracy (by an average of 2.65% across various architectures) but also provides flexibility for depth-wise scalable inference on resource-constrained devices <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nOnline distillation represents another important advancement, where both teacher and student models are updated simultaneously during training <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper>. This approach is particularly suitable when large-capacity, high-performance teacher models are unavailable <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper> <Paper corpusId=\"208526905\" paperTitle=\"(Chen et al., 2019)\" isShortName></Paper>. In online distillation, the aggregated intermediate predictions of multiple student models can serve as targets to train each student model, creating a collaborative learning environment <Paper corpusId=\"270755154\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.\n\nOne particularly effective variant of self-distillation regularizes the \"dark knowledge\" (knowledge about wrong predictions) of a network by forcing it to produce more meaningful and consistent predictions in a class-wise manner <Paper corpusId=\"262084420\" paperTitle=\"(Capogrosso et al., 2023)\" isShortName></Paper> <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>. This approach mitigates overconfident predictions and reduces intra-class variations, significantly improving both generalization ability and calibration performance of modern convolutional neural networks <Paper corpusId=\"214727822\" paperTitle=\"(Yun et al., 2020)\" isShortName></Paper>.\n\nThe empirical success of self-distillation and online distillation challenges traditional interpretations of knowledge distillation. Li et al. and Furlanello et al. observed significant performance gains through distillation even when teacher and student models have identical architectures <Paper corpusId=\"271039191\" paperTitle=\"(Pareek et al., 2024)\" isShortName></Paper> <Paper corpusId=\"14659675\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper> <Paper corpusId=\"4110009\" paperTitle=\"(Furlanello et al., 2018)\" isShortName></Paper>. These findings suggest that distillation benefits extend beyond merely transferring knowledge from larger to smaller models, opening new possibilities for model improvement without increasing computational demands.", "citations": [{"id": "(Furlanello et al., 2018)", "paper": {"corpus_id": 4110009, "title": "Born Again Neural Networks", "year": 2018, "venue": "International Conference on Machine Learning", "authors": [{"name": "Tommaso Furlanello", "authorId": "2067208583"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "Michael Tschannen", "authorId": "143902495"}, {"name": "L. Itti", "authorId": "7326223"}, {"name": "Anima Anandkumar", "authorId": "2047844"}], "n_citations": 1034}, "snippets": ["Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction."], "score": 0.0}, {"id": "(Pareek et al., 2024)", "paper": {"corpus_id": 271039191, "title": "Understanding the Gains from Repeated Self-Distillation", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Divyansh Pareek", "authorId": "51176912"}, {"name": "S. S. Du", "authorId": "2330243848"}, {"name": "Sewoong Oh", "authorId": "2310392746"}], "n_citations": 6}, "snippets": ["Challenging this interpretation, Li et al. [19] and Furlanello et al. [9] empirically observed performance gains through distillation even when the teacher and student are same-sized models. One can set T and S to have the same architecture, and S trained with the objective in Eq. (1) outperforms T. This is referred to as Born-Again Networks (BANs) or Self-Distillation (SD). Furthermore, repeatedly applying self-distillation on the same training data with a student model having the same architecture provides additional gains on benchmark datasets and architectures [9,35]43]. At each step, the student from the previous step acts as the teacher used to train a new student model under the self-distillation loss of Eq. (1)."], "score": 0.9443359375}, {"id": "(Lee et al., 2023)", "paper": {"corpus_id": 258309753, "title": "Bayesian Optimization Meets Self-Distillation", "year": 2023, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "HyunJae Lee", "authorId": "2279782391"}, {"name": "Heon Song", "authorId": "2157781156"}, {"name": "Hyeonsoo Lee", "authorId": "2279825857"}, {"name": "Gi-hyeon Lee", "authorId": "2215364195"}, {"name": "Suyeong Park", "authorId": "2199808369"}, {"name": "Donggeun Yoo", "authorId": "2283756"}], "n_citations": 2}, "snippets": ["Self-Distillation (SD) is a special form of KD where the teacher and student networks have identical architecture. Born-Again Networks (BAN) [16] demonstrated that when training the student to match the output distribution of the teacher with the identical architecture, it could outperform the teacher. Furthermore, they showed that performing multiple rounds of BAN could further improve the performance where the trained student is set to be a new teacher in the following round. The ef-fectiveness of SD has been theoretically explained by the \"multi-view\" hypothesis introduced by Allen-Zhu and Li, who showed that self-distillation performs an implicit ensemble of various models [2]. Empirical evidence from Pham et al. [36] suggests that SD encourages the student to find flatter minima, leading to better generalization."], "score": 0.94921875}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 270755154, "title": "RSMoDM: Multimodal Momentum Distillation Model for Remote Sensing Visual Question Answering", "year": 2024, "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "authors": [{"name": "Pengfei Li", "authorId": "2278383802"}, {"name": "Gang Liu", "authorId": "2192649954"}, {"name": "Jinlong He", "authorId": "2221231272"}, {"name": "Xiangxu Meng", "authorId": "2299725033"}, {"name": "Shenjun Zhong", "authorId": "2053866414"}, {"name": "Xun Chen", "authorId": "2308411177"}], "n_citations": 2}, "snippets": ["Online distillation is an end-to-end training framework where both teacher and student models are updated simultaneously, particularly suitable when largecapacity, high-performance teacher models are unavailable (Chen et al., 2019). Self-distillation is a special form of online distillation, involving the transfer of knowledge within the same network from deeper to shallower layers or from early to later training stages, thus achieving knowledge transfer and training optimization (Zhang et al., 2019)."], "score": 0.92578125}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 159041406, "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Linfeng Zhang", "authorId": "50081570"}, {"name": "Jiebo Song", "authorId": "122151709"}, {"name": "Anni Gao", "authorId": "94970361"}, {"name": "Jingwei Chen", "authorId": "2144141924"}, {"name": "Chenglong Bao", "authorId": "2060106235"}, {"name": "Kaisheng Ma", "authorId": "2075321204"}], "n_citations": 865}, "snippets": ["Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."], "score": 0.0}, {"id": "(Capogrosso et al., 2023)", "paper": {"corpus_id": 262084420, "title": "A Machine Learning-Oriented Survey on Tiny Machine Learning", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Luigi Capogrosso", "authorId": "2135267479"}, {"name": "Federico Cunico", "authorId": "1396330675"}, {"name": "D. Cheng", "authorId": "1780197"}, {"name": "Franco Fummi", "authorId": "2243336023"}, {"name": "Marco Cristani", "authorId": "2238815087"}], "n_citations": 43}, "snippets": ["Online distillation is an end-to-end approach where both the teacher and student models are updated simultaneously, making it suitable when the teacher model is not significantly larger or higher performing (Zhang et al., 2021). Finally, self-distillation is a special case of online distillation where the teacher and student networks have the same architecture (Yun et al., 2020)."], "score": 0.9169921875}, {"id": "(Chen et al., 2019)", "paper": {"corpus_id": 208526905, "title": "Online Knowledge Distillation with Diverse Peers", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Defang Chen", "authorId": "1684692"}, {"name": "Jian-Ping Mei", "authorId": "1798852"}, {"name": "Can Wang", "authorId": "98262982"}, {"name": "Yan Feng", "authorId": "1692947908"}, {"name": "Chun Chen", "authorId": "2588203"}], "n_citations": 302}, "snippets": ["Distillation is an effective knowledge-transfer technique that uses predicted distributions of a powerful teacher model as soft targets to train a less-parameterized student model. A pre-trained high capacity teacher, however, is not always available. Recently proposed online variants use the aggregated intermediate predictions of multiple student models as targets to train each student model. Although group-derived targets give a good recipe for teacher-free distillation, group members are homogenized quickly with simple aggregation functions, leading to early saturated solutions. In this work, we propose Online Knowledge Distillation with Diverse peers (OKDDip), which performs two-level distillation during training with multiple auxiliary peers and one group leader. In the first-level distillation, each auxiliary peer holds an individual set of aggregation weights generated with an attention-based mechanism to derive its own targets from predictions of other auxiliary peers. Learning from distinct target distributions helps to boost peer diversity for effectiveness of group-based distillation. The second-level distillation is performed to transfer the knowledge in the ensemble of auxiliary peers further to the group leader, i.e., the model used for inference. Experimental results show that the proposed framework consistently gives better performance than state-of-the-art approaches without sacrificing training or inference complexity, demonstrating the effectiveness of the proposed two-level distillation framework."], "score": 0.0}, {"id": "(Yun et al., 2020)", "paper": {"corpus_id": 214727822, "title": "Regularizing Class-Wise Predictions via Self-Knowledge Distillation", "year": 2020, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Sukmin Yun", "authorId": "66863443"}, {"name": "Jongjin Park", "authorId": "2109073979"}, {"name": "Kimin Lee", "authorId": "3436470"}, {"name": "Jinwoo Shin", "authorId": "143720148"}], "n_citations": 281}, "snippets": ["Deep neural networks with millions of parameters may suffer from poor generalization due to overfitting. To mitigate the issue, we propose a new regularization method that penalizes the predictive distribution between similar samples. In particular, we distill the predictive distribution between different samples of the same label during training. This results in regularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a single network (i.e., a self-knowledge distillation) by forcing it to produce more meaningful and consistent predictions in a class-wise manner. Consequently, it mitigates overconfident predictions and reduces intra-class variations. Our experimental results on various image classification tasks demonstrate that the simple yet powerful method can significantly improve not only the generalization ability but also the calibration performance of modern convolutional neural networks."], "score": 0.0}, {"id": "(Li et al., 2017)", "paper": {"corpus_id": 14659675, "title": "Learning from Noisy Labels with Distillation", "year": 2017, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Yuncheng Li", "authorId": "2110470764"}, {"name": "Jianchao Yang", "authorId": "1706007"}, {"name": "Yale Song", "authorId": "2317183"}, {"name": "Liangliang Cao", "authorId": "48749954"}, {"name": "Jiebo Luo", "authorId": "33642939"}, {"name": "Li-Jia Li", "authorId": "2040091191"}], "n_citations": 550}, "snippets": ["The ability of learning from noisy labels is very useful in many visual recognition tasks, as a vast amount of data with noisy labels are relatively easy to obtain. Traditionally, label noise has been treated as statistical outliers, and techniques such as importance re-weighting and bootstrapping have been proposed to alleviate the problem. According to our observation, the real-world noisy labels exhibit multimode characteristics as the true labels, rather than behaving like independent random outliers. In this work, we propose a unified distillation framework to use \"side\" information, including a small clean dataset and label relations in knowledge graph, to \"hedge the risk\" of learning from noisy labels. Unlike the traditional approaches evaluated based on simulated label noises, we propose a suite of new benchmark datasets, in Sports, Species and Artifacts domains, to evaluate the task of learning from noisy labels in the practical setting. The empirical study demonstrates the effectiveness of our proposed method in all the domains."], "score": 0.0}], "table": null}, {"title": "Factors Affecting Distillation Effectiveness", "tldr": "Several key factors influence knowledge distillation effectiveness, including the capacity gap between teacher and student models, training dynamics, and alignment strategies. Research shows that excessive disparities in model size can impede knowledge transfer, while adaptive weighting methods and early stopping techniques can enhance distillation performance. (11 sources)", "text": "\nThe effectiveness of knowledge distillation is significantly influenced by the capacity gap between teacher and student models. Contrary to intuition, increasing the teacher model size doesn't always improve distillation outcomes. Gao demonstrated that continuing to increase the depth of teacher networks (from 4 to 10 layers) while maintaining a fixed student architecture (2-layer CNN) failed to improve distillation performance and sometimes made it worse <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper>. This phenomenon occurs because an excessively large gap between teacher and student capacities hinders effective knowledge transfer <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nStudent model capacity also plays a crucial role in distillation outcomes. When student models are too large relative to the task complexity, they tend to learn more slowly and become more susceptible to overfitting, resulting in diminished generalization performance <Paper corpusId=\"271270180\" paperTitle=\"(Violos et al., 2024)\" isShortName></Paper>. This challenge is particularly pronounced in resource-constrained environments such as edge devices, where computational and memory limitations further constrain model design.\n\nResearch suggests that optimal distillation occurs when the teacher exhibits strong performance while maintaining an appropriate size advantage over the student network <Paper corpusId=\"268543692\" paperTitle=\"(Mukherjee et al., 2023)\" isShortName></Paper>. This balanced approach facilitates expedited convergence during training while avoiding the negative effects of excessive capacity gaps. Some researchers have proposed building more powerful teacher models without changing the capacity (width and depth) of the student model to optimize performance transfer <Paper corpusId=\"271956980\" paperTitle=\"(Yu, 2024)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>.\n\nVarious strategies have been developed to address capacity gap challenges. Multi-step knowledge distillation employs intermediate-sized networks (\"teacher assistants\") to bridge the gap between large teachers and small students <Paper corpusId=\"258048760\" paperTitle=\"(Gao, 2023)\" isShortName></Paper> <Paper corpusId=\"212908749\" paperTitle=\"(Mirzadeh et al., 2019)\" isShortName></Paper>. Other approaches modify distillation algorithms or implement multi-stage learning to improve knowledge transfer across capacity disparities <Paper corpusId=\"276776153\" paperTitle=\"(Medina et al., 2025)\" isShortName></Paper> <Paper corpusId=\"229679667\" paperTitle=\"(Passban et al., 2020)\" isShortName></Paper> <Paper corpusId=\"159041406\" paperTitle=\"(Zhang et al., 2019)\" isShortName></Paper>.\n\nAdaptive weighting methods have also proven effective in enhancing knowledge distillation. Confidence-based or similarity-based approaches improve the effectiveness of model distillation by dynamically adjusting the importance of different knowledge components <Paper corpusId=\"277217930\" paperTitle=\"(Casey et al., 2025)\" isShortName></Paper> <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. These techniques align with importance sampling methods in transfer learning, where distillation weights reflect the reliability of each teacher component <Paper corpusId=\"277217930\" paperTitle=\"(Casey et al., 2025)\" isShortName></Paper> <Paper corpusId=\"740063\" paperTitle=\"(Pan et al., 2010)\" isShortName></Paper>.\n\nTraining dynamics significantly impact distillation outcomes as well. Cho et al. demonstrated that stopping the teacher's training early can mitigate negative effects when distilling from larger teachers to smaller students <Paper corpusId=\"203642130\" paperTitle=\"(Cho et al., 2019)\" isShortName></Paper>. This finding suggests that teacher model optimization should be tailored specifically for distillation rather than pursuing maximum standalone performance.", "citations": [{"id": "(Gao, 2023)", "paper": {"corpus_id": 258048760, "title": "A Survey on Recent Teacher-student Learning Studies", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Min Gao", "authorId": "2147417425"}], "n_citations": 3}, "snippets": ["When knowledge distillation is performed, increasing the model size of the teacher network instead makes the knowledge distillation worse. The results are as follows: the student network is a 2-layer CNN, while the teacher network is a 4-10-layer CNN, respectively, and it can be seen from the results that continuing to increase the depth of the teacher network does not necessarily improve the performance of knowledge distillation.\n\nTo further prove that the gap does affect knowledge distillation, the authors conducted a small comparison experiment by fixing the teacher network as a 10-layer CNN and changing the number of layers of the student network, and the results are as follows.\n\nBased on the above observations, it is not difficult to think that a medium-sized network (teacher assistant [16], TA) should be added between the large network and the small network, and the teacher network should be used to distill the TA first, and then the student network should be distilled by the TA. When TA was added, the distillation effect of the student network was significantly better than the direct distillation of the teacher network."], "score": 0.92431640625}, {"id": "(Mirzadeh et al., 2019)", "paper": {"corpus_id": 212908749, "title": "Improved Knowledge Distillation via Teacher Assistant", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Seyed Iman Mirzadeh", "authorId": "145156788"}, {"name": "Mehrdad Farajtabar", "authorId": "1682124"}, {"name": "Ang Li", "authorId": "2112839418"}, {"name": "Nir Levine", "authorId": "153898744"}, {"name": "Akihiro Matsukawa", "authorId": "2063980545"}, {"name": "H. Ghasemzadeh", "authorId": "144600887"}], "n_citations": 1081}, "snippets": ["Despite the fact that deep neural networks are powerful models and achieve appealing results on many tasks, they are too large to be deployed on edge devices like smartphones or embedded sensor nodes. There have been efforts to compress these networks, and a popular method is knowledge distillation, where a large (teacher) pre-trained network is used to train a smaller (student) network. However, in this paper, we show that the student network performance degrades when the gap between student and teacher is large. Given a fixed student network, one cannot employ an arbitrarily large teacher, or in other words, a teacher can effectively transfer its knowledge to students up to a certain size, not smaller. To alleviate this shortcoming, we introduce multi-step knowledge distillation, which employs an intermediate-sized network (teacher assistant) to bridge the gap between the student and the teacher. Moreover, we study the effect of teacher assistant size and extend the framework to multi-step distillation. Theoretical analysis and extensive experiments on CIFAR-10,100 and ImageNet datasets and on CNN and ResNet architectures substantiate the effectiveness of our proposed approach."], "score": 0.0}, {"id": "(Violos et al., 2024)", "paper": {"corpus_id": 271270180, "title": "Towards Optimal Trade-Offs in Knowledge Distillation for CNNs and Vision Transformers at the Edge", "year": 2024, "venue": "European Signal Processing Conference", "authors": [{"name": "John Violos", "authorId": "2573061"}, {"name": "Symeon Papadopoulos", "authorId": "2276779989"}, {"name": "Y. Kompatsiaris", "authorId": "1715604"}], "n_citations": 4}, "snippets": ["When the student model's capacity is excessively large, the expected improvements in distillation efficacy may not materialize. Larger student models tend to exhibit slower learning rates and are more susceptible to overfitting, resulting in diminished generalization performance. Furthermore, the computational and memory resources required for training and deploying larger student models pose significant challenges, particularly in resource-constrained environments such as edge devices."], "score": 0.91455078125}, {"id": "(Mukherjee et al., 2023)", "paper": {"corpus_id": 268543692, "title": "Guiding the Student\u2019s Learning Curve: Augmenting Knowledge Distillation with Insights from GradCAM", "year": 2023, "venue": "2023 International Conference on Modeling, Simulation & Intelligent Computing (MoSICom)", "authors": [{"name": "Suvaditya Mukherjee", "authorId": "2293143380"}, {"name": "Dev Vinod Chandan", "authorId": "2292319438"}, {"name": "Shreyas Dongre", "authorId": "2268069324"}], "n_citations": 0}, "snippets": ["Notably, our findings reveal that this approach facilitates expedited convergence, particularly when the Teacher network exhibits strong performance and a substantial size advantage over the Student network."], "score": 0.9306640625}, {"id": "(Yu, 2024)", "paper": {"corpus_id": 271956980, "title": "Bring the Power of Diffusion Model to Defect Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuyi Yu", "authorId": "2317142609"}], "n_citations": 1}, "snippets": ["In addition, the gap between the capacity of the teacher model and the student model affects the effectiveness of knowledge distillation, so the capacity of the teacher model needs to be controlled (Mirzadeh et al., 2019). We propose to build powerful teacher model without changing the capacity (width and depth) of the student model. This approach allows for optimal performance transfer between teacher and student."], "score": 0.927734375}, {"id": "(Medina et al., 2025)", "paper": {"corpus_id": 276776153, "title": "Mamba base PKD for efficient knowledge compression", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jos\u00e9 Medina", "authorId": "2348480395"}, {"name": "Amnir Hadachi", "authorId": "3221342"}, {"name": "Paul Honeine", "authorId": "2256850325"}, {"name": "Abdelaziz Bensrhair", "authorId": "2314690317"}], "n_citations": 0}, "snippets": ["However, one limitation of traditional KD is the performance drop when there is a large capacity gap between the teacher and student models because the student cannot effectively represent key features of the teacher. This limitation has been addressed by various methods (Passban et al., 2020)16,(Yim et al., 2017)(Zhang et al., 2019) that modify distillation algorithms or employ multi-stage learning."], "score": 0.916015625}, {"id": "(Passban et al., 2020)", "paper": {"corpus_id": 229679667, "title": "ALP-KD: Attention-Based Layer Projection for Knowledge Distillation", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Peyman Passban", "authorId": "5062230"}, {"name": "Yimeng Wu", "authorId": "2000863903"}, {"name": "Mehdi Rezagholizadeh", "authorId": "1924511"}, {"name": "Qun Liu", "authorId": "2115900360"}], "n_citations": 122}, "snippets": ["Knowledge distillation is considered as a training and compression strategy in which two neural networks, namely a teacher and a student, are coupled together during training. The teacher network is supposed to be a trustworthy predictor and the student tries to mimic its predictions. Usually, a student with a lighter architecture is selected so we can achieve compression and yet deliver high-quality results. In such a setting, distillation only happens for final predictions whereas the student could also benefit from teacher\u2019s supervision for internal components.\n\nMotivated by this, we studied the problem of distillation for intermediate layers. Since there might not be a one-to-one alignment between student and teacher layers, existing techniques skip some teacher layers and only distill from a subset of them. This shortcoming directly impacts quality, so we instead propose a combinatorial technique which relies on attention. Our model fuses teacher-side information and takes each layer\u2019s significance into consideration, then it performs distillation between combined teacher layers and those of the student. Using our technique, we distilled a 12-layer BERT (Devlin et al. 2019) into 6-, 4-, and 2-layer counterparts and evaluated them on GLUE tasks (Wang et al. 2018). Experimental results show that our combinatorial approach is able to outperform other existing techniques."], "score": 0.0}, {"id": "(Zhang et al., 2019)", "paper": {"corpus_id": 159041406, "title": "Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Linfeng Zhang", "authorId": "50081570"}, {"name": "Jiebo Song", "authorId": "122151709"}, {"name": "Anni Gao", "authorId": "94970361"}, {"name": "Jingwei Chen", "authorId": "2144141924"}, {"name": "Chenglong Bao", "authorId": "2060106235"}, {"name": "Kaisheng Ma", "authorId": "2075321204"}], "n_citations": 865}, "snippets": ["Convolutional neural networks have been widely deployed in various application scenarios. In order to extend the applications' boundaries to some accuracy-crucial domains, researchers have been investigating approaches to boost accuracy through either deeper or wider network structures, which brings with them the exponential increment of the computational and storage cost, delaying the responding time. In this paper, we propose a general training framework named self distillation, which notably enhances the performance (accuracy) of convolutional neural networks through shrinking the size of the network rather than aggrandizing it. Different from traditional knowledge distillation - a knowledge transformation methodology among networks, which forces student neural networks to approximate the softmax layer outputs of pre-trained teacher neural networks, the proposed self distillation framework distills knowledge within network itself. The networks are firstly divided into several sections. Then the knowledge in the deeper portion of the networks is squeezed into the shallow ones. Experiments further prove the generalization of the proposed self distillation framework: enhancement of accuracy at average level is 2.65%, varying from 0.61% in ResNeXt as minimum to 4.07% in VGG19 as maximum. In addition, it can also provide flexibility of depth-wise scalable inference on resource-limited edge devices. Our codes have been released on github."], "score": 0.0}, {"id": "(Casey et al., 2025)", "paper": {"corpus_id": 277217930, "title": "Distributed Collaborative Learning with Representative Knowledge Sharing", "year": 2025, "venue": "Mathematics", "authors": [{"name": "Joseph Casey", "authorId": "2342413409"}, {"name": "Qianjiao Chen", "authorId": "2351733702"}, {"name": "Mengchen Fan", "authorId": "2300177011"}, {"name": "Baocheng Geng", "authorId": "2285910606"}, {"name": "Roman Shterenberg", "authorId": "2342412176"}, {"name": "Zhong Chen", "authorId": "2344089016"}, {"name": "Keren Li", "authorId": "2340963079"}], "n_citations": 1}, "snippets": ["Recent advances have demonstrated the importance of adaptive weighting in knowledge transfer, where confidence-based or similarity-based methods improve the effectiveness of model distillation [14,(Cho et al., 2019). This aligns with importance sampling techniques in transfer learning and domain adaptation, where distillation weights reflect the reliability of each teacher node [16,(Pan et al., 2010). Prior works [2]18] emphasize that knowledge transfer effectiveness depends on teacher-student alignment, motivating our approach of using Energy Coefficients for adaptive distillation weighting."], "score": 0.9248046875}, {"id": "(Cho et al., 2019)", "paper": {"corpus_id": 203642130, "title": "On the Efficacy of Knowledge Distillation", "year": 2019, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Jang Hyun Cho", "authorId": "39878282"}, {"name": "B. Hariharan", "authorId": "73710317"}], "n_citations": 619}, "snippets": ["In this paper, we present a thorough evaluation of the efficacy of knowledge distillation and its dependence on student and teacher architectures. Starting with the observation that more accurate teachers often don\u2019t make good teachers, we attempt to tease apart the factors that affect knowledge distillation performance. We find crucially that larger models do not often make better teachers. We show that this is a consequence of mismatched capacity, and that small students are unable to mimic large teachers. We find typical ways of circumventing this (such as performing a sequence of knowledge distillation steps) to be ineffective. Finally, we show that this effect can be mitigated by stopping the teacher\u2019s training early. Our results generalize across datasets and models."], "score": 0.0}, {"id": "(Pan et al., 2010)", "paper": {"corpus_id": 740063, "title": "A Survey on Transfer Learning", "year": 2010, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Sinno Jialin Pan", "authorId": "1746914"}, {"name": "Qiang Yang", "authorId": "152290618"}], "n_citations": 21010}, "snippets": ["A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research."], "score": 0.0}], "table": null}, {"title": "Practical Applications and Implementation Methods", "tldr": "Knowledge distillation has been successfully implemented across various domains, from computer vision to natural language processing, using multiple approaches including homomorphic, feature-level, and module-level distillation techniques. These methods have enabled the deployment of efficient models on resource-constrained devices while maintaining competitive performance. (12 sources)", "text": "\n- **Logit-level distillation**: The most common implementation method transfers knowledge using soft outputs from teacher models. This approach combines original loss (using ground-truth labels) with distillation loss (calculated from teacher's soft outputs), enabling effective knowledge transfer between models <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"7200347\" paperTitle=\"(Hinton et al., 2015)\" isShortName></Paper>.\n\n- **Feature-level distillation**: Methods like MiniViT transfer knowledge at the feature level by combining weights from consecutive Transformer blocks for cross-layer weight sharing. This approach introduces transformations to enhance learning while reducing model size <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248177918\" paperTitle=\"(Zhang et al., 2022)\" isShortName></Paper>.\n\n- **Module-level distillation**: Techniques like m2mKD separate teacher modules from pre-trained unified models and combine student modules with modular models. This approach uses a shared meta-model for composition, enabling student modules to effectively mimic teacher module behavior <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper>.\n\n- **Patch-level distillation**: DeiT-Tiny employs patch-level distillation where a small student model is trained to match the pre-trained teacher model's patch structure. This method optimizes with decomposed manifold matching loss to reduce computational costs <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247230104\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\n- **One-stage vs. two-stage distillation**: In object detection, one-stage knowledge distillation can achieve comparable or better performance than two-stage distillation when powerful teachers aren't available. However, two-stage distillation with strong teachers generally yields the best overall performance <Paper corpusId=\"260378128\" paperTitle=\"(Zhang et al., 2023)\" isShortName></Paper>.\n\n- **Self-training paradigms**: SPEQ introduces a novel approach where high-precision teachers and low-bit-width students derived from the same full precision source model are trained collaboratively. This method utilizes a relatively higher precision network's output as soft labels for training lower precision ones <Paper corpusId=\"254564348\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper> <Paper corpusId=\"222066738\" paperTitle=\"(Boo et al., 2020)\" isShortName></Paper>.\n\n- **Distillation for Transformers**: TinyViT applies distillation during pre-training, storing logits from large teacher models on hardware to achieve memory and computational efficiency when transferring knowledge to smaller student Transformers <Paper corpusId=\"270870796\" paperTitle=\"(Mai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"250920355\" paperTitle=\"(Wu et al._1, 2022)\" isShortName></Paper>.\n\n- **Natural language processing applications**: Shallow BiLSTM-based models have achieved performance comparable to the ELMo language model with approximately 100 times fewer parameters and 15 times faster inference. Similarly, DistilBERT maintains 97% of BERT's performance while reducing parameter count by 40% <Paper corpusId=\"270878760\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>.\n\n- **Tiny Machine Learning**: Knowledge distillation has become essential for deploying AI on microcontrollers and IoT devices with severe memory constraints. System-algorithm co-design approaches enable squeezing deep learning models into billions of resource-constrained devices <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264109973\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>.\n\n- **Distillation schemes**: Implementation approaches include offline distillation (teacher trained first, then guides student), online distillation (both models train simultaneously), and self-distillation (teacher and student share architecture). These methods create lightweight models that can maintain or even surpass larger models' performance <Paper corpusId=\"273963558\" paperTitle=\"(Fuente et al., 2024)\" isShortName></Paper> <Paper corpusId=\"219559263\" paperTitle=\"(Gou et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Wu et al., 2022)", "paper": {"corpus_id": 254564348, "title": "Vertical Layering of Quantized Neural Networks for Heterogeneous Inference", "year": 2022, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Hai Wu", "authorId": "2187214993"}, {"name": "Ruifei He", "authorId": "2053865518"}, {"name": "Hao Hao Tan", "authorId": "3218666"}, {"name": "Xiaojuan Qi", "authorId": "50844674"}, {"name": "Kaibin Huang", "authorId": "2149775992"}], "n_citations": 2}, "snippets": ["Knowledge distillation transfers the knowledge learned by large models (i.e., teacher) to a compact one (i.e., student) via a weighted combination of original loss using ground-truth labels and distillation loss calculated with the soft output of a teacher model (Hinton et al., 2015)", "SPEQ (Boo et al., 2020) introduces a novel self-training paradigm where the high-precision teacher and the low-bit-width student from the same full precision source model are trained collaboratively. In our training process, where different bit-width networks are enclosed, we design a self-knowledge transfer that utilizes a relatively higher precision network's output as soft labels for the cooperative training of the lower precision ones."], "score": 0.92041015625}, {"id": "(Hinton et al., 2015)", "paper": {"corpus_id": 7200347, "title": "Distilling the Knowledge in a Neural Network", "year": 2015, "venue": "arXiv.org", "authors": [{"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "J. Dean", "authorId": "49959210"}], "n_citations": 19742}, "snippets": ["A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."], "score": 0.0}, {"id": "(Mai et al., 2024)", "paper": {"corpus_id": 270870796, "title": "From Efficient Multimodal Models to World Models: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xinji Mai", "authorId": "2276937443"}, {"name": "Zeng Tao", "authorId": "2261831274"}, {"name": "Junxiong Lin", "authorId": "2261891655"}, {"name": "Haoran Wang", "authorId": "2276807843"}, {"name": "Yang Chang", "authorId": "2276969811"}, {"name": "Yanlan Kang", "authorId": "2212014366"}, {"name": "Yan Wang", "authorId": "2276879376"}, {"name": "Wenqiang Zhang", "authorId": "2276819302"}], "n_citations": 5}, "snippets": ["Homomorphic KD means the student and teacher models have similar or identical structures.In this approach, the student model learns by mimicking the teacher model's outputs (e.g., logits, feature layer outputs).Common homomorphic KD methods include logit-level distillation, feature-level distillation, and module-level distillation.For instance, TinyViT (Wu et al., 2022) applies distillation during pre-training, storing logits from a large teacher model on hardware to achieve memory and computational efficiency when transferring knowledge to a smaller student Transformer.DeiT-Tiny (Wang et al., 2022) adopts patchlevel distillation, training a small student model to match the pre-trained teacher model's patch structure, then optimizing with decomposed manifold matching loss to reduce computational costs.Module-level methods like m2mKD [48] separate the teacher module from a pre-trained unified model, combining student modules with modular models, and using a shared meta-model for composition, enabling student modules to mimic teacher module behavior.Feature-level distillation methods like MiniViT (Zhang et al., 2022) combine weights from consecutive Transformer blocks for cross-layer weight sharing, introducing transformations to enhance learning."], "score": 0.92626953125}, {"id": "(Zhang et al., 2022)", "paper": {"corpus_id": 248177918, "title": "MiniViT: Compressing Vision Transformers with Weight Multiplexing", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Jinnian Zhang", "authorId": "3431885"}, {"name": "Houwen Peng", "authorId": "2484788"}, {"name": "Kan Wu", "authorId": "2115597973"}, {"name": "Mengchen Liu", "authorId": "2152968847"}, {"name": "Bin Xiao", "authorId": "2054421528"}, {"name": "Jianlong Fu", "authorId": "3247966"}, {"name": "Lu Yuan", "authorId": "2150687325"}], "n_citations": 124}, "snippets": ["Vision Transformer (ViT) models have recently drawn much attention in computer vision due to their high model capability. However, ViT models suffer from huge number of parameters, restricting their applicability on devices with limited memory. To alleviate this problem, we propose MiniViT, a new compression framework, which achieves parameter reduction in vision transformers while retaining the same performance. The central idea of MiniViT is to multiplex the weights of consecutive transformer blocks. More specifically, we make the weights shared across layers, while imposing a transformation on the weights to increase diversity. Weight distillation over self-attention is also applied to transfer knowledge from large-scale ViT models to weight-multiplexed compact models. Comprehensive experiments demonstrate the efficacy of MiniViT, showing that it can reduce the size of the pre-trained Swin-B transformer by 48%, while achieving an increase of 1.0% in Top-1 accuracy on ImageNet. Moreover, using a single-layer of parameters, MiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters, without seriously compromising the performance. Finally, we verify the transferability of MiniViT by reporting its performance on downstream benchmarks. Code and models are available at here."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 247230104, "title": "Towards efficient vision transformer inference: a first study of transformers on mobile devices", "year": 2022, "venue": "Workshop on Mobile Computing Systems and Applications", "authors": [{"name": "Xudong Wang", "authorId": "2144671295"}, {"name": "Li Zhang", "authorId": "2152831498"}, {"name": "Yang Wang", "authorId": "2157274867"}, {"name": "Mao Yang", "authorId": "51169628"}], "n_citations": 44}, "snippets": ["Convolution neural networks (CNNs) have long been dominating the model choice in on-device intelligent mobile applications. Recently, we are witnessing the fast development of vision transformers, which are notable for the use of the self-attention mechanism, have demonstrated the superiority in accuracy over CNNs. However, vision transformers are with expensive computation costs, and their inference efficiency on resource-constrained mobile devices are still unclear by now. This brings a lot of uncertainty for on-device intelligence to benefit from the vision transformers. In this work, we carry out the first empirical study to investigate the possibility of efficiently deploying vision transformers on mobile devices. Our twofold study (i) profiles the representative vision transformers to understand the inference performance on commercial mobile devices and the behind reasons; and (ii) study multi-dimensional DNN acceleration approaches to achieve minimal latency. Results show that it is too expensive for vision transformer inference on mobile devices. Its inference is 1.58x-41x slower than CNNs. By removing the redundant Attention heads and FFN layers, DeiT-Tiny saves 23.2\\% latency with negligible 0.75\\% accuracy loss. Our study provides 7 insightful findings for future efficient vision transformer optimization and design."], "score": 0.0}, {"id": "(Zhang et al., 2023)", "paper": {"corpus_id": 260378128, "title": "Structured Knowledge Distillation for Accurate and Efficient Object Detection", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Linfeng Zhang", "authorId": "50081570"}, {"name": "Kaisheng Ma", "authorId": "2075321204"}], "n_citations": 27}, "snippets": ["Traditional knowldistillation follows a two-stage training pipeline, where large teacher model is initially trained and then distilled to a lightweight student model", "For two-stage knowledge distillation experiments, we explore the following two schemes. \n\nr Strong Teacher Scheme: distilling knowledge from a pre- trained Cascade Mask RCNN teacher with ResNetX101-DCN backbone, which achieves 47.3 AP. \n\nr Weak Teacher Scheme: distilling knowledge from a pre- trained teacher which has the identical architecture with the student (i.e., Faster RCNN with ResNet50 backbone) and achieves 38.4 AP. Experimental results are shown in Table X. It is observed that one-stage knowledge distillation leads to 2.3 AP improvements over the baseline, which is 0.2 higher than two-stage knowledge distillation with a weak teacher, but still 0.8 AP lower than the two-stage knowledge distillation with a strong teacher. This observation indicates that one-stage knowledge distillation can achieve comparable and even better performance than two-stage knowledge distillation when no powerful teacher is available. Moreover, two-stage knowledge distillation can make more use of the knowledge from a powerful teacher and achieve better performance."], "score": 0.91552734375}, {"id": "(Boo et al., 2020)", "paper": {"corpus_id": 222066738, "title": "Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks", "year": 2020, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yoonho Boo", "authorId": "3350090"}, {"name": "Sungho Shin", "authorId": "1859825"}, {"name": "Jungwook Choi", "authorId": "2506452"}, {"name": "Wonyong Sung", "authorId": "66936521"}], "n_citations": 30}, "snippets": ["The quantization of deep neural networks (QDNNs) has been actively studied for deployment in edge devices. Recent studies employ the knowledge distillation (KD) method to improve the performance of quantized networks. In this study, we propose stochastic precision ensemble training for QDNNs (SPEQ). SPEQ is a knowledge distillation training scheme; however, the teacher is formed by sharing the model parameters of the student network. We obtain the soft labels of the teacher by randomly changing the bit precision of the activation stochastically at each layer of the forward-pass computation. The student model is trained with these soft labels to reduce the activation quantization noise. The cosine similarity loss is employed, instead of the KL-divergence, for KD training. As the teacher model changes continuously by random bit-precision assignment, it exploits the effect of stochastic ensemble KD. SPEQ outperforms the existing quantization training methods in various tasks, such as image classification, question-answering, and transfer learning without the need for cumbersome teacher networks."], "score": 0.0}, {"id": "(Wu et al._1, 2022)", "paper": {"corpus_id": 250920355, "title": "TinyViT: Fast Pretraining Distillation for Small Vision Transformers", "year": 2022, "venue": "European Conference on Computer Vision", "authors": [{"name": "Kan Wu", "authorId": "2115597973"}, {"name": "Jinnian Zhang", "authorId": "2108041613"}, {"name": "Houwen Peng", "authorId": "2484788"}, {"name": "Mengchen Liu", "authorId": "2152968847"}, {"name": "Bin Xiao", "authorId": "2054421528"}, {"name": "Jianlong Fu", "authorId": "3247966"}, {"name": "Lu Yuan", "authorId": "2150687325"}], "n_citations": 265}, "snippets": ["Vision transformer (ViT) recently has drawn great attention in computer vision due to its remarkable model capability. However, most prevailing ViT models suffer from huge number of parameters, restricting their applicability on devices with limited resources. To alleviate this issue, we propose TinyViT, a new family of tiny and efficient small vision transformers pretrained on large-scale datasets with our proposed fast distillation framework. The central idea is to transfer knowledge from large pretrained models to small ones, while enabling small models to get the dividends of massive pretraining data. More specifically, we apply distillation during pretraining for knowledge transfer. The logits of large teacher models are sparsified and stored in disk in advance to save the memory cost and computation overheads. The tiny student transformers are automatically scaled down from a large pretrained model with computation and parameter constraints. Comprehensive experiments demonstrate the efficacy of TinyViT. It achieves a top-1 accuracy of 84.8% on ImageNet-1k with only 21M parameters, being comparable to Swin-B pretrained on ImageNet-21k while using 4.2 times fewer parameters. Moreover, increasing image resolutions, TinyViT can reach 86.5% accuracy, being slightly better than Swin-L while using only 11% parameters. Last but not the least, we demonstrate a good transfer ability of TinyViT on various downstream tasks. Code and models are available at https://github.com/microsoft/Cream/tree/main/TinyViT."], "score": 0.0}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 270878760, "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application", "year": 2024, "venue": "ACM Transactions on Intelligent Systems and Technology", "authors": [{"name": "Chuanpeng Yang", "authorId": "2309482604"}, {"name": "Yao Zhu", "authorId": "2290230390"}, {"name": "Wang Lu", "authorId": "2309429610"}, {"name": "Yidong Wang", "authorId": "2293893759"}, {"name": "Qian Chen", "authorId": "2276257486"}, {"name": "Chenlong Gao", "authorId": "26155233"}, {"name": "Bingjie Yan", "authorId": "2276089941"}, {"name": "Yiqiang Chen", "authorId": "2276178270"}], "n_citations": 33}, "snippets": ["Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance.\n\nExperimental results show that the shallow BiLSTM-based model achieves performance comparable to the ELMo language model [104], but with approximately 100 times fewer parameters and a 15-fold increase in inference speed.\n\nCompared to BERT, DistilBERT maintains 97% of the performance while reducing the number of parameters by 40%.\n\nPKD method outperformed standard knowledge distillation methods. It achieved superior performance and better generalization, significantly enhancing training efficiency and reducing storage requirements while maintaining accuracy comparable to the original large-scale model."], "score": 0.9228515625}, {"id": "(Fuente et al., 2024)", "paper": {"corpus_id": 273963558, "title": "Enhancing Predictive Maintenance in Mining Mobile Machinery through a TinyML-enabled Hierarchical Inference Network", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ra'ul de la Fuente", "authorId": "2330192141"}, {"name": "Luciano Radrig\u00e1n", "authorId": "2047941908"}, {"name": "Anibal S Morales", "authorId": "2330183718"}], "n_citations": 0}, "snippets": ["Knowledge distillation transfers knowledge from a large, complex model (the teacher) to a smaller, simpler model (the student) (Lin et al., 2024)", "Distillation schemes include offline distillation, where the teacher is trained first and then guides the student; online distillation, where both models train simultaneously; and self-distillation, where the teacher and student share the same architecture (Ray, 2021), (Gou et al., 2020). This method creates lightweight models that can maintain or even surpass the performance of larger models."], "score": 0.92041015625}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 264109973, "title": "Tiny Machine Learning: Progress and Futures [Feature]", "year": 2024, "venue": "IEEE Circuits and Systems Magazine", "authors": [{"name": "Ji Lin", "authorId": "2258439127"}, {"name": "Ligeng Zhu", "authorId": "20515689"}, {"name": "Wei-Ming Chen", "authorId": "2164114675"}, {"name": "Wei-Chen Wang", "authorId": "2258439498"}, {"name": "Song Han", "authorId": "2258448604"}], "n_citations": 55}, "snippets": ["Tiny machine learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of applications and enable ubiquitous intelligence. However, TinyML is challenging due to the hardware constraints: the tiny memory resource is difficult hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today\u2019s \"large\" model might be tomorrow\u2019s \"tiny\" model. The scope of TinyML should evolve and adapt over time."], "score": 0.0}, {"id": "(Gou et al., 2020)", "paper": {"corpus_id": 219559263, "title": "Knowledge Distillation: A Survey", "year": 2020, "venue": "International Journal of Computer Vision", "authors": [{"name": "Jianping Gou", "authorId": "38978232"}, {"name": "B. Yu", "authorId": "2425630"}, {"name": "S. Maybank", "authorId": "144555237"}, {"name": "D. Tao", "authorId": "143719920"}], "n_citations": 2984}, "snippets": ["In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher\u2013student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded."], "score": 0.0}], "table": null}], "cost": 0.390858}}
