{"clarifying_information": [{"clarifying_question1": "Are you most interested in the sequential stages and phases of how language models acquire different linguistic capabilities during training, such as the transition from syntax to semantics?", "clarifying_answer1": {"clarifying_answer": "Yes, I am primarily interested in the sequential stages and phases by which language models acquire linguistic capabilities during training, particularly the progression from learning syntactic to semantic knowledge and how these transitions are manifested and measured."}}, {"clarifying_question2": "Do you want to focus on how specific architectural differences (e.g., Transformers vs. LSTMs) affect the learning dynamics and emergence of capabilities in language models?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in how specific architectural differences, such as between Transformers and LSTMs, affect the learning dynamics and the emergence of capabilities in language models."}}, {"clarifying_question3": "Are you seeking information about specific metrics and methods (such as token-level loss analysis, phase transition detection, or scaling laws) used to study and quantify training dynamics in language models?", "clarifying_answer3": {"clarifying_answer": "Yes, I am particularly interested in the specific metrics and methods\u2014such as token-level loss analysis, phase transition detection, and scaling laws\u2014used to study and quantify the training dynamics in language models."}}], "better_query": "Analyze the sequential stages by which language models acquire linguistic capabilities during training, especially the progression from syntactic to semantic knowledge, and describe how these transitions are manifested and measured.", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Language models undergo pre-training on vast text corpora to learn fundamental linguistic capabilities through tasks like next token prediction. This self-supervised process enables models to acquire both syntactic structures and semantic relationships that form the foundation for their language understanding abilities. (4 sources)", "text": "\nLarge language models (LLMs) develop their linguistic capabilities through a structured training process that begins with extensive pre-training on diverse text corpora. This critical initial phase involves exposing the model to massive amounts of unlabeled text data, enabling it to learn the underlying statistical properties of natural language <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. During pre-training, models typically learn through self-supervised objectives such as predicting the next token in a sequence, where the model is conditioned on previous tokens (w\u2081:\u1d62\u208b\u2081) and trained to predict the subsequent token (w\u1d62) <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>.\n\nThis unsupervised learning approach allows models to acquire fundamental linguistic patterns without task-specific guidance <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper>. The training techniques commonly employed include masked language modeling (MLM), where models predict missing words from context, and autoregressive language modeling, where models predict the next word in a sequence <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"277065732\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>. Through these methods, models develop the ability to capture syntactic structures, semantic relationships, and contextual dependencies between words and phrases <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277065732\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nThe outcome of this pre-training process is what's commonly referred to as a \"base\" model, which possesses a robust, transferable understanding of linguistic structures <Paper corpusId=\"270688478\" paperTitle=\"(Cuconasu et al., 2024)\" isShortName></Paper>. This foundation enables the model to generalize effectively across different contexts and serves as the basis for further specialized training or fine-tuning for specific applications <Paper corpusId=\"260775882\" paperTitle=\"(Krishnan, 2023)\" isShortName></Paper> <Paper corpusId=\"272910877\" paperTitle=\"(Liang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Krishnan, 2023)", "paper": {"corpus_id": 260775882, "title": "Exploring Machine Learning and Transformer-based Approaches for Deceptive Text Classification: A Comparative Analysis", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Anusuya Krishnan", "authorId": "2217728327"}], "n_citations": 1}, "snippets": ["The training process typically involves two main steps: pre-training and fine-tuning. In the pretraining step, the model is trained on a massive corpus of unlabeled text. This unsupervised pre-training enables the model to learn the statistical properties of the language, capture syntactic and semantic relationships, and develop a contextual understanding of words. \n\nDuring pre-training, the model is trained on tasks like masked language modeling (MLM) or autoregressive language modeling (e.g., predicting the next word in a sequence). By predicting missing or masked words, the model learns to encode contextual information and generate coherent representations for various language patterns."], "score": 0.89892578125}, {"id": "(Cuconasu et al., 2024)", "paper": {"corpus_id": 270688478, "title": "A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Florin Cuconasu", "authorId": "2281641042"}, {"name": "Giovanni Trappolini", "authorId": "120709579"}, {"name": "Nicola Tonellotto", "authorId": "2281641625"}, {"name": "Fabrizio Silvestri", "authorId": "2260653973"}], "n_citations": 2}, "snippets": ["Pre-training for large language models (Radford et al., 2018) involves an extensive unsupervised learning phase, where the model is exposed to a large corpus of text data to learn the underlying statistical properties of natural language.This process employs the next token prediction task, where the model is conditioned on a sequence of tokens w 1:i\u22121 and trained to predict the subsequent token w i , i.e., p(y) = n i p \u03b8 (w i |w 1:i\u22121 ).By iteratively processing vast and diverse textual datasets, the model learns linguistic patterns, including syntactic structures, semantic relationships, and contextual dependencies.This process delivers what is commonly called a \"base\" model.This first model is usually improved with further specialized training, as we will see now."], "score": 0.87890625}, {"id": "(Liang et al., 2024)", "paper": {"corpus_id": 272910877, "title": "Data Proportion Detection for Optimized Data Management for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hao Liang", "authorId": "2303856806"}, {"name": "Keshi Zhao", "authorId": "2320126993"}, {"name": "Yajie Yang", "authorId": "2323000103"}, {"name": "Bin Cui", "authorId": "2260817839"}, {"name": "Guosheng Dong", "authorId": "2242124536"}, {"name": "Zenan Zhou", "authorId": "2183768304"}, {"name": "Wentao Zhang", "authorId": "2309265357"}], "n_citations": 0}, "snippets": ["The pre-training stage is a critical phase in the development of large language models (LLMs), where the model learns fundamental linguistic patterns from large-scale corpora. During this stage, the model is exposed to vast amounts of text data from various domains, enabling it to acquire a broad understanding of language. The primary objective is for the model to capture syntactic and semantic relationships between words and phrases, thereby developing a foundational knowledge that can be applied to a range of downstream tasks. This stage typically involves unsupervised learning, where the model attempts to predict tokens in a sequence without task-specific guidance, allowing it to generalize effectively across different contexts."], "score": 0.95556640625}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277065732, "title": "A Survey on Federated Fine-tuning of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yebo Wu", "authorId": "2297831289"}, {"name": "Chunlin Tian", "authorId": "2297767120"}, {"name": "Jingguang Li", "authorId": "2350520035"}, {"name": "He Sun", "authorId": "2350521999"}, {"name": "Kahou Tam", "authorId": "2220304337"}, {"name": "Li Li", "authorId": "2260286654"}, {"name": "Chengzhong Xu", "authorId": "2362594432"}], "n_citations": 5}, "snippets": ["During pre-training, the model learns fundamental linguistic properties, including syntactic structures, semantic representations, and contextual understanding, in a self-supervised or unsupervised manner. This process typically lever-  ages techniques such as masked language modeling (MLM) (Sinha et al., 2021) or autoregressive language modeling (Yang et al., 2019), where the model predicts missing or subsequent tokens based on surrounding context (Naveed et al., 2023). By processing large-scale datasets, the model develops a robust, transferable understanding of linguistic structures, enabling it to capture complex patterns, syntactic dependencies, and semantic relationships across diverse contexts."], "score": 0.8994140625}], "table": null}, {"title": "General Developmental Trajectory of Linguistic Capabilities", "tldr": "Language models follow a consistent developmental trajectory, acquiring syntactic knowledge early in training before progressing to more complex semantic and reasoning capabilities. This sequential pattern shows remarkable consistency across different models and languages, with early stages focused on structural patterns and later stages dedicated to building more abstract conceptual knowledge. (6 sources)", "text": "\nResearch has revealed that language models exhibit distinct developmental stages during pre-training that follow a surprisingly consistent trajectory. Multiple studies have identified a clear progression in which models first acquire syntactic capabilities before advancing to more complex semantic understanding. Nikoulina et al. found that models \"follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training\" <Paper corpusId=\"232092403\" paperTitle=\"(Nikoulina et al., 2021)\" isShortName></Paper>. This pattern appears to be universal across different model architectures and training approaches.\n\nThe acquisition of linguistic knowledge occurs rapidly in early training phases. Muller-Eberstein et al. observed that \"syntactic knowledge is acquired rapidly after 0.5% of full training\" while \"continued performance improvements primarily stem from the acquisition of open-domain knowledge\" <Paper corpusId=\"264451507\" paperTitle=\"(Muller-Eberstein et al., 2023)\" isShortName></Paper>. Similarly, Liu et al. found that \"linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired\" <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. This evidence suggests a clear hierarchy in the difficulty and training time required for different types of linguistic capabilities.\n\nThe developmental trajectory appears to be highly predictable. Choshen et al. discovered that \"NLMs exhibit consistent 'developmental' stages\" and that \"the learning trajectory approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired\" <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. They also identified \"phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them\" <Paper corpusId=\"237491997\" paperTitle=\"(Choshen et al., 2021)\" isShortName></Paper>. This indicates that certain linguistic phenomena are learned together, possibly because they rely on similar underlying principles.\n\nIn multilingual contexts, this developmental sequence remains consistent but with additional cross-lingual dimensions. Blevins et al. found that \"the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics\" <Paper corpusId=\"249017999\" paperTitle=\"(Blevins et al., 2022)\" isShortName></Paper>. However, they noted that \"while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process\" <Paper corpusId=\"249017999\" paperTitle=\"(Blevins et al., 2022)\" isShortName></Paper>, suggesting that cross-lingual capabilities develop more gradually throughout training.\n\nAt a more granular level, language models appear to progress from local to more abstract representations. Inaba et al. observed that models \"initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations\" and that \"LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings\" <Paper corpusId=\"276902722\" paperTitle=\"(Inaba et al., 2025)\" isShortName></Paper>. This progression from concrete to abstract understanding mirrors the development of increasingly sophisticated linguistic capabilities throughout the training process.", "citations": [{"id": "(Nikoulina et al., 2021)", "paper": {"corpus_id": 232092403, "title": "The Rediscovery Hypothesis: Language Models Need to Meet Linguistics", "year": 2021, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Vassilina Nikoulina", "authorId": "2841761"}, {"name": "Maxat Tezekbayev", "authorId": "1471888023"}, {"name": "Nuradil Kozhakhmet", "authorId": "2051805778"}, {"name": "Madina Babazhanova", "authorId": "2051799158"}, {"name": "Matthias Gall\u00e9", "authorId": "2907260"}, {"name": "Z. Assylbekov", "authorId": "2842549"}], "n_citations": 8}, "snippets": ["We have analyzed how the amount of linguistic knowledge evolves during the pretraining process. Our analysis of intermediate pretraining checkpoints suggests that models follow NLP pipeline discovery during the pretraining process: it reaches good performance on syntactic tasks at the beginning of the training and keeps improving on semantic tasks further in training."], "score": 0.8671875}, {"id": "(Muller-Eberstein et al., 2023)", "paper": {"corpus_id": 264451507, "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Max M\u00fcller-Eberstein", "authorId": "1416353805"}, {"name": "Rob van der Goot", "authorId": "3449407"}, {"name": "Barbara Plank", "authorId": "2256991041"}, {"name": "Ivan Titov", "authorId": "2261492323"}], "n_citations": 10}, "snippets": ["We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization."], "score": 0.962890625}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."], "score": 0.92724609375}, {"id": "(Choshen et al., 2021)", "paper": {"corpus_id": 237491997, "title": "The Grammar-Learning Trajectories of Neural Language Models", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Guy Hacohen", "authorId": "94064232"}, {"name": "D. Weinshall", "authorId": "1789171"}, {"name": "Omri Abend", "authorId": "2769805"}], "n_citations": 29}, "snippets": ["Results suggest that NLMs exhibit consistent \"developmental\" stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them."], "score": 0.9677734375}, {"id": "(Blevins et al., 2022)", "paper": {"corpus_id": 249017999, "title": "Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Terra Blevins", "authorId": "3443287"}, {"name": "Hila Gonen", "authorId": "1821892"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 31}, "snippets": ["Our analysis uncovers several insights into multilingual knowledge acquisition. First, while the model acquires most in-language linguistic information early on, cross-lingual transfer is learned across the entire pretraining process. Second, the order in which the model acquires linguistic information for each language is generally consistent with monolingual models: lower-level syntax is learned prior to higher-level syntax and then semantics."], "score": 0.92333984375}, {"id": "(Inaba et al., 2025)", "paper": {"corpus_id": 276902722, "title": "How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tatsuro Inaba", "authorId": "2342412237"}, {"name": "Kentaro Inui", "authorId": "2261744024"}, {"name": "Yusuke Miyao", "authorId": "2302320083"}, {"name": "Yohei Oseki", "authorId": "50856622"}, {"name": "Benjamin Heinzerling", "authorId": "2266692"}, {"name": "Yu Takagi", "authorId": "2310230516"}], "n_citations": 1}, "snippets": ["Our experiments yield two key findings: (1) LLMs first learn knowledge within individual languages before acquiring cross-lingual mappings ( \u00a74.3), and (2) they initially capture fine-grained, token-level knowledge before progressing to more abstract, conceptual representations ( \u00a74.4) These findings offer new insights into the internal mechanisms that underlie the emergence of LLMs' generalization abilities."], "score": 0.98046875}], "table": null}, {"title": "Early-Stage Acquisition: Syntax and Structural Knowledge", "tldr": "During early training, language models rapidly acquire syntactic knowledge through a distinct phase transition marked by the emergence of Syntactic Attention Structure. This early acquisition of structural patterns precedes semantic understanding, with models first learning n-gram patterns before developing more complex linguistic capabilities. (8 sources)", "text": "\nLanguage models demonstrate a remarkably consistent pattern in their earliest learning stages, with syntactic knowledge emerging as a foundational capability. Multiple studies have observed that syntax is acquired very early in the training process, with Muller-Eberstein et al. finding that \"syntactic knowledge is acquired rapidly after 0.5% of full training\" <Paper corpusId=\"264451507\" paperTitle=\"(Muller-Eberstein et al., 2023)\" isShortName></Paper>. This finding aligns with broader observations that language models consistently learn \"linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of syntax is not a gradual process but occurs during a dramatic phase transition in training. Chen et al. identified \"a brief window in pretraining when models abruptly acquire SAS , concurrent with a steep drop in loss\" <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This \"structure onset\" represents a breakthrough moment when transformer attention heads begin to specialize in tracking specific syntactic relationships. Following this transition, researchers can even \"reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>.\n\nThe organization of syntax in language models follows a hierarchical pattern that mirrors traditional linguistic theory. Lower layers of the model architecture tend to capture basic syntactic patterns <Paper corpusId=\"276725505\" paperTitle=\"(Orlicki, 2025)\" isShortName></Paper>, with information flowing in a way that resembles \"the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>. This finding is consistent with earlier work by Saphra et al., who demonstrated that \"different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early\" <Paper corpusId=\"53633715\" paperTitle=\"(Saphra et al., 2018)\" isShortName></Paper>.\n\nAt the most granular level, language models initially learn through n-gram patterns before developing more sophisticated linguistic capabilities. Chang et al. describe this process as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. This progression from simple token-level statistics to structured syntactic knowledge forms the foundation upon which more complex semantic understanding is later built.\n\nThe rapid acquisition of syntax appears to be a prerequisite for subsequent linguistic development. Chen et al. observed that the \"internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior\" <Paper corpusId=\"261822542\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This suggests that syntactic knowledge serves as a critical scaffold that enables the model to develop more advanced capabilities in later training stages, establishing a clear developmental sequence from syntax to semantics.", "citations": [{"id": "(Muller-Eberstein et al., 2023)", "paper": {"corpus_id": 264451507, "title": "Subspace Chronicles: How Linguistic Information Emerges, Shifts and Interacts during Language Model Training", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Max M\u00fcller-Eberstein", "authorId": "1416353805"}, {"name": "Rob van der Goot", "authorId": "3449407"}, {"name": "Barbara Plank", "authorId": "2256991041"}, {"name": "Ivan Titov", "authorId": "2261492323"}], "n_citations": 10}, "snippets": ["We identify critical learning phases across tasks and time, during which subspaces emerge, share information, and later disentangle to specialize. Across these phases, syntactic knowledge is acquired rapidly after 0.5% of full training. Continued performance improvements primarily stem from the acquisition of open-domain knowledge, while semantics and reasoning tasks benefit from later boosts to long-range contextualization and higher specialization."], "score": 0.962890625}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.92138671875}, {"id": "(Evanson et al., 2023)", "paper": {"corpus_id": 259089351, "title": "Language acquisition: do children and language models follow similar learning stages?", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Linnea Evanson", "authorId": "2219624467"}, {"name": "Yair Lakretz", "authorId": "3051598"}, {"name": "J. King", "authorId": "28972230"}], "n_citations": 29}, "snippets": ["During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."], "score": 0.94970703125}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261822542, "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Angelica Chen", "authorId": "13336152"}, {"name": "Ravid Schwartz-Ziv", "authorId": "2240524527"}, {"name": "Kyunghyun Cho", "authorId": "1979489"}, {"name": "Matthew L. Leavitt", "authorId": "2240527814"}, {"name": "Naomi Saphra", "authorId": "2362960"}], "n_citations": 74}, "snippets": ["We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities.\n\nMost work on scaling laws (Kaplan et al., 2020) presents test loss as a quantity that homogeneously responds to the scale of training, declining by a power law relative to the size of the corpus. In the MLM setting, we instead identify a precipitous drop in the loss curve of BERT Base (Fig. 1(a)), consistently spanning 20K-30K timesteps of training across various random seeds. We now show how this rapid learning stage can be interpreted as the composition of two distinct phase transitions.\n\nThe MLM loss drop occurs alongside the acquisition of grammatical capabilities in two consecutive stages, each distinguished by breaks as defined by Eq. ( 2). The first stage aligns with the formation of SAS-we call this break in implicit parse UAS the structure onset. As seen in Fig. 1(b), the UAS spikes at a consistent time during each run, in tandem with abrupt improvements in MLM loss (Fig. 1(a)) and finetuning metrics (Fig. 2(b)). Immediately following the spike, UAS plateaus, but the loss continues to drop precipitously before leveling off. The second part of this loss drop is associated with a break in the observed grammatical capabilities of the model, as measured by accuracy on BLiMP (Fig. 1(c)). We call the BLiMP break the capabilities onset.\n\nBy observing these phase transitions, we can see that the internal representation of grammar, in the form of syntactic attention, precipitates the external observation of grammatical behavior, in the form of correct language modeling judgements on linguistically challenging examples. This is not only a single breakthrough during training, but a sequence of breakthroughs that appear to be dependent on each other."], "score": 0.93505859375}, {"id": "(McGrath et al., 2024)", "paper": {"corpus_id": 272661905, "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?", "year": 2024, "venue": "Current Directions in Psychological Science", "authors": [{"name": "Sam Whitman McGrath", "authorId": "2290681639"}, {"name": "Jacob Russin", "authorId": "2321226194"}, {"name": "Ellie Pavlick", "authorId": "2260118854"}, {"name": "Roman Feiman", "authorId": "48522350"}], "n_citations": 9}, "snippets": ["For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable."], "score": 0.86181640625}, {"id": "(Orlicki, 2025)", "paper": {"corpus_id": 276725505, "title": "Beyond Words: A Latent Memory Approach to Internal Reasoning in LLMs", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "J. I. Orlicki", "authorId": "20441545"}], "n_citations": 0}, "snippets": ["A mathematical framework for transformer circuits [et al., 2021] further elucidates how information is organized and processed across layers, with lower layers capturing syntactic patterns and higher layers encoding complex semantics."], "score": 0.890625}, {"id": "(Saphra et al., 2018)", "paper": {"corpus_id": 53633715, "title": "Language Models Learn POS First", "year": 2018, "venue": "BlackboxNLP@EMNLP", "authors": [{"name": "Naomi Saphra", "authorId": "2362960"}, {"name": "Adam Lopez", "authorId": "144871732"}], "n_citations": 12}, "snippets": ["Rather than treating the training process itself as a black box, we investigate how representations of linguistic structure are learned over time. In particular, we demonstrate that different aspects of linguistic structure are learned at different rates, with part of speech tagging acquired early and global topic information learned continuously."], "score": 0.96142578125}, {"id": "(Chang et al., 2021)", "paper": {"corpus_id": 238408423, "title": "Word Acquisition in Neural Language Models", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 40}, "snippets": ["We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)", "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."], "score": 0.8984375}], "table": null}, {"title": "Mid-to-Late Stage Acquisition: Semantics and Complex Knowledge", "tldr": "After establishing syntactic foundations, language models progress to acquire semantic knowledge and more complex capabilities during mid-to-late training stages, with world knowledge and reasoning abilities emerging later and less consistently. This developmental pattern shows higher model layers specializing in semantic processing, though recent research suggests this relationship becomes more complex after fine-tuning. (7 sources)", "text": "\nFollowing the early acquisition of syntactic knowledge, language models undergo a gradual transition to developing more advanced semantic capabilities and complex knowledge. This progression from syntax to semantics follows a consistent pattern where \"world knowledge and reasoning are learned later and less stably\" compared to syntactic rules <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. The development of these more sophisticated capabilities requires substantially more training data and time than the initial acquisition of syntactic knowledge.\n\nThe architecture of language models reflects this developmental progression through a layer-wise specialization. Research has consistently shown that \"lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing\" <Paper corpusId=\"264590507\" paperTitle=\"(Starace et al., 2023)\" isShortName></Paper>. This hierarchical organization mirrors \"the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity\" <Paper corpusId=\"272661905\" paperTitle=\"(McGrath et al., 2024)\" isShortName></Paper>. This structured progression suggests that syntactic knowledge serves as a foundation upon which semantic understanding is built.\n\nThe transition from syntax to semantics is not abrupt but follows a gradual refinement process. Chang et al. characterize this progression as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. During this refinement phase, models evolve from simple statistical pattern recognition to developing more abstract conceptual representations that enable deeper semantic understanding. This progression allows models to eventually \"converge on more nuanced predictions\" beyond simple token frequencies <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nRecent research has challenged some conventional assumptions about how semantic knowledge is distributed across model layers after fine-tuning. While pre-trained models show increasing semantic capabilities in higher layers, Wu et al. found that in fine-tuned models, \"the proportion of captured semantic knowledge initially grows then drops aggressively\" <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This suggests that instruction fine-tuning may reorganize how semantic knowledge is represented within the model architecture, contradicting \"conventional belief that higher layers capture more semantic knowledge\" <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>.\n\nThe acquisition of semantic knowledge appears to benefit from establishing abstract relations between syntactic and semantic properties. Gauthier et al. suggest that \"by explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time\" <Paper corpusId=\"44093987\" paperTitle=\"(Gauthier et al., 2018)\" isShortName></Paper>. This indicates that the early acquisition of syntactic knowledge creates a framework that facilitates more efficient semantic learning in later training stages.\n\nThe importance of pre-training for semantic knowledge acquisition has been highlighted in recent studies examining the relative contributions of different training phases. Zhou et al. demonstrated that \"almost all knowledge of LLMs is acquired during pre-training,\" with subsequent fine-tuning primarily teaching models interaction styles and formats rather than introducing new knowledge <Paper corpusId=\"261530292\" paperTitle=\"(Zhao et al., 2023)\" isShortName></Paper>. This finding underscores the critical role of the pre-training phase in establishing the semantic knowledge base upon which more specialized capabilities are later built.", "citations": [{"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.92138671875}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."], "score": 0.92724609375}, {"id": "(Starace et al., 2023)", "paper": {"corpus_id": 264590507, "title": "Probing LLMs for Joint Encoding of Linguistic Categories", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Giulio Starace", "authorId": "2168285763"}, {"name": "Konstantinos Papakostas", "authorId": "2262444926"}, {"name": "Rochelle Choenni", "authorId": "2067163164"}, {"name": "Apostolos Panagiotopoulos", "authorId": "31713235"}, {"name": "Matteo Rosati", "authorId": "2168109752"}, {"name": "Alina Leidinger", "authorId": "2262444923"}, {"name": "Ekaterina Shutova", "authorId": "2262445370"}], "n_citations": 7}, "snippets": ["Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing."], "score": 0.90576171875}, {"id": "(McGrath et al., 2024)", "paper": {"corpus_id": 272661905, "title": "How Can Deep Neural Networks Inform Theory in Psychological Science?", "year": 2024, "venue": "Current Directions in Psychological Science", "authors": [{"name": "Sam Whitman McGrath", "authorId": "2290681639"}, {"name": "Jacob Russin", "authorId": "2321226194"}, {"name": "Ellie Pavlick", "authorId": "2260118854"}, {"name": "Roman Feiman", "authorId": "48522350"}], "n_citations": 9}, "snippets": ["For example, the ability of large language models (LLMs) to generate long strings of grammatical text (Linzen & Baroni, 2021) spurred research on correspondences between aspects of LLMs' internal states and specific constructs in linguistic theory, such as parts of speech and syntax trees (Manning et al., 2020;Tenney et al., 2019). The overwhelming consensus from such work is that LLMs' internal states are neither unstructured nor inscrutable but exhibit geometric regularities that can be aligned with these linguistic constructs. For example, the way that sentence information is organized across the layers of these models reflects the traditional language-processing pipeline, with earlier layers representing parts of speech and parsing grammatical relations and later layers encoding semantic roles (who did what to whom) and tracking when different terms refer to the same entity (Tenney et al., 2019). It is even possible to reconstruct the syntactic parse trees postulated by traditional linguistic theory from models' internal representations of particular sentences (Manning et al., 2020). \n\nFurther work has suggested not only that these higher level descriptions are useful for interpreting model components but also that they play a causal role in the models' grammatical behavior. For instance, Chen et al. (2024) found that reconstructing syntactic parse trees first becomes possible during a specific window of training in which models show a sudden increase in the grammatical capacities that these same representations should enable."], "score": 0.86181640625}, {"id": "(Chang et al., 2021)", "paper": {"corpus_id": 238408423, "title": "Word Acquisition in Neural Language Models", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 40}, "snippets": ["We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)", "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."], "score": 0.8984375}, {"id": "(Zhao et al., 2023)", "paper": {"corpus_id": 261530292, "title": "Explainability for Large Language Models: A Survey", "year": 2023, "venue": "ACM Transactions on Intelligent Systems and Technology", "authors": [{"name": "Haiyan Zhao", "authorId": "2237987232"}, {"name": "Hanjie Chen", "authorId": "7315244"}, {"name": "F. Yang", "authorId": "145338224"}, {"name": "Ninghao Liu", "authorId": "47717322"}, {"name": "Huiqi Deng", "authorId": "13689700"}, {"name": "Hengyi Cai", "authorId": "22561596"}, {"name": "Shuaiqiang Wang", "authorId": "2237948548"}, {"name": "Dawei Yin", "authorId": "2136400100"}, {"name": "Mengnan Du", "authorId": "2237804196"}], "n_citations": 461}, "snippets": ["A recent study by Zhou et al. (2023) investigated the relative importance of pre-training versus instruction fine-tuning for language models. In the experiment, the authors used only 1,000 carefully selected instructions to tune the LLaMA-65B model, without reinforcement learning, and achieved performance comparable to GPT-4. The researchers hypothesized that alignment may be a simpler process where the model learns interaction styles and formats, while almost all knowledge of LLMs is acquired during pre-training. The experimental findings demonstrated the power of pre-training and its relative importance over large-scale finetuning and reinforcement learning approaches", "Wu et al. (2023c) looked into the role of instruction fine-tuning by examining instruction following and concept-level knowledge evolution. The result shows that instruction fine-tuned models can better distinguish instruction and context, and follow users' instructions well. Besides, they can focus more on middle and tail of input prompts than pretrained models. And fine-tuned models adjust concepts toward downstream user-oriented tasks explicitly but the linguistic distributions remain the same. Contradict to conventional belief that higher layers capture more semantic knowledge, the proportion of captured semantic knowledge initially grows then drops aggressively in fine-tuned models."], "score": 0.90478515625}, {"id": "(Gauthier et al., 2018)", "paper": {"corpus_id": 44093987, "title": "Word learning and the acquisition of syntactic-semantic overhypotheses", "year": 2018, "venue": "Annual Meeting of the Cognitive Science Society", "authors": [{"name": "Jon Gauthier", "authorId": "24339276"}, {"name": "R. Levy", "authorId": "143643017"}, {"name": "J. Tenenbaum", "authorId": "1763295"}], "n_citations": 10}, "snippets": ["By explicitly tracking abstract relations between the syntactic and semantic properties of words at the level of the lexicon, a language learner can become more efficient at acquiring word meanings over time. Our computational model demonstrates how such abstract relations can be induced from very little data, and learned in an efficient and incremental fashion."], "score": 0.85498046875}], "table": null}, {"title": "Measurement and Evaluation Methods", "tldr": "Researchers employ a diverse range of evaluation methods to track language models' acquisition of linguistic capabilities, including probing classifiers, acceptability judgments, and fine-tuning on downstream tasks. These measurement approaches reveal that different linguistic capabilities emerge at varying rates during training, with syntactic features being reliably encoded after exposure to relatively small amounts of text. (4 sources)", "text": "\nMeasuring the emergence of linguistic capabilities in language models requires specialized evaluation techniques that can capture both the timing and quality of acquired knowledge. Researchers have developed a rich toolkit of assessment methods that track different aspects of language learning throughout the training process. Zhang et al. identify five primary evaluation approaches: \"classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This multi-faceted evaluation strategy enables researchers to comprehensively track the development of different linguistic capabilities across training iterations.\n\nProbing tasks have emerged as a particularly valuable method for measuring the acquisition of specific linguistic knowledge. By plotting \"probing performance across iterations,\" researchers can create detailed learning curves that reveal when and how quickly different capabilities emerge <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. These probing experiments have demonstrated that models require surprisingly little data to acquire basic linguistic structure, with language models needing \"only about 10M to 100M words to learn to reliably encode most syntactic and semantic features\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. In contrast, the same studies show that \"a much larger quantity of data\" is required for models to develop commonsense knowledge and master downstream NLU tasks <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>.\n\nThe assessment of grammatical competence can take either binary or gradient approaches, depending on the research question. Pannitto et al. note that researchers can either \"test whether the network is able or not to handle some linguistic phenomenon\" or adopt a gradient perspective to examine \"how and why some linguistic aspect becomes more and more salient to the network during training\" <Paper corpusId=\"256627634\" paperTitle=\"(Pannitto et al., 2023)\" isShortName></Paper>. This latter approach aligns with continuous assessment methodologies that track incremental improvements in model capabilities throughout the training process.\n\nMore recently, evaluation frameworks inspired by human language acquisition have been developed to provide structured assessment of language model capabilities. Yang et al. propose \"a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This developmental approach draws on \"methods from linguistic research\" to evaluate model performance across a progression of increasingly complex language tasks <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Interestingly, their findings suggest that while newer models show improved overall performance, \"their developmental trajectory does not strictly follow the path of human language acquisition\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>, highlighting important differences between artificial and human language learning processes.", "citations": [{"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 226289695, "title": "When Do You Need Billions of Words of Pretraining Data?", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yian Zhang", "authorId": "9227100"}, {"name": "Alex Warstadt", "authorId": "46236380"}, {"name": "Haau-Sing Li", "authorId": "1993902967"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 141}, "snippets": ["First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks."], "score": 0.91552734375}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."], "score": 0.92724609375}, {"id": "(Pannitto et al., 2023)", "paper": {"corpus_id": 256627634, "title": "CALaMo: a Constructionist Assessment of Language Models", "year": 2023, "venue": "CXGSNLP", "authors": [{"name": "Ludovica Pannitto", "authorId": "22233530"}, {"name": "Aur\u00e9lie Herbelot", "authorId": "3352951"}], "n_citations": 2}, "snippets": ["Regarding stability, depending on the view that is taken on the continuity hypothesis, we can see NLM's grammatical competence either as a binary or as a gradient property. In the first case, we test whether the network is able or not to handle some linguistic phenomenon, while in the second case, as advocated by CALaMo, we are interested in seeing how and why some linguistic aspect becomes more and more salient to the network during training."], "score": 0.89501953125}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273403825, "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qiyuan Yang", "authorId": "2326447675"}, {"name": "Pengda Wang", "authorId": "2294722814"}, {"name": "Luke D. Plonsky", "authorId": "2326296255"}, {"name": "Frederick L. Oswald", "authorId": "2250843586"}, {"name": "Hanjie Chen", "authorId": "7315244"}], "n_citations": 2}, "snippets": ["We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition."], "score": 0.91357421875}], "table": null}, {"title": "Factors Influencing Acquisition Patterns", "tldr": "Language model training dynamics are influenced by multiple factors including data volume, token frequency, and architectural choices, with most models requiring 10-100M words to acquire basic syntactic features. Environmental influences further differentiate artificial language learning from human acquisition, with LLMs heavily relying on distributional patterns while lacking the embodied experiences that shape human language development. (6 sources)", "text": "\nThe acquisition of linguistic capabilities in language models is shaped by several key factors that influence both the rate and pattern of learning. One of the most significant factors is the volume of training data, with research showing distinct thresholds for different types of knowledge acquisition. Zhang et al. found that language models \"require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features,\" but need \"a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks\" <Paper corpusId=\"226289695\" paperTitle=\"(Zhang et al., 2020)\" isShortName></Paper>. This finding indicates that basic linguistic structures can be learned with relatively modest amounts of data, while more complex knowledge requires substantially more extensive training.\n\nToken frequency and n-gram probability play crucial roles in shaping learning patterns throughout the training process. Chang et al. demonstrated that \"learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts,\" with high-probability n-grams being \"less likely to be 'forgotten'\" during training <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper>. This observation suggests that the statistical properties of the training corpus significantly influence which linguistic features are acquired more quickly and retained more reliably. Their analysis further indicates that language model learning might be characterized as \"early n-gram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"233289478\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>.\n\nThe training environment and learning context also substantially impact acquisition patterns, highlighting important differences between artificial and human language learning. While language models can develop \"sensitivity to certain linguistic phenomena\" through training, their learning process differs fundamentally from human language acquisition <Paper corpusId=\"267627053\" paperTitle=\"(Qin et al., 2024)\" isShortName></Paper>. Unlike children, who acquire language through interactive, embodied experiences, language models rely heavily on distributional patterns in text. This distinction is further emphasized in research by Mahowald et al., who highlight \"the gap between formal and functional linguistic competencies in LLMs,\" where formal competencies involve syntactic and semantic correctness, while functional competencies require applying language in practical, goal-oriented contexts <Paper corpusId=\"271916426\" paperTitle=\"(Neveditsin et al., 2024)\" isShortName></Paper>.\n\nThe architectural design of models influences their acquisition patterns, though research suggests these patterns remain relatively consistent across different model types. Studies comparing language models' learning trajectories to those of children have found that \"similarly to children, the language models tend to learn linguistic skills in a systematic order\" <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>. However, these studies also reveal that \"some - but not all - learning stages are shared between children and these language models,\" pointing to fundamental differences in how humans and algorithms process language <Paper corpusId=\"261277016\" paperTitle=\"(Chang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259089351\" paperTitle=\"(Evanson et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Zhang et al., 2020)", "paper": {"corpus_id": 226289695, "title": "When Do You Need Billions of Words of Pretraining Data?", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yian Zhang", "authorId": "9227100"}, {"name": "Alex Warstadt", "authorId": "46236380"}, {"name": "Haau-Sing Li", "authorId": "1993902967"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 141}, "snippets": ["First, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks."], "score": 0.91552734375}, {"id": "(Chang et al., 2023)", "paper": {"corpus_id": 261277016, "title": "Characterizing Learning Curves During Language Model Pre-Training: Learning, Forgetting, and Stability", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "Z. Tu", "authorId": "144035504"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 13}, "snippets": ["Previous work has studied the pre-training dynamics of language models (Saphra, 0). Choshen et al. (2022) and (Evanson et al., 2023) find that language models learn linguistic generalizations in similar stages regardless of model architecture, initialization, and data-shuffling. In masked language models, syntactic rules are learned early, but world knowledge and reasoning are learned later and less stably (Chiang et al., 2020)(Liu et al., 2021)", "N-gram learning and refinement. As a further step towards understanding fine-grained learning patterns in language models, our work investigates whether simple statistical regularities can explain learning patterns such as the sudden loss changes discussed above. We demonstrate that learning curves are more stable and converge faster for frequent tokens, n-gram probable tokens, and frequent contexts ( \u00a76.2). High probability n-grams in particular are less likely to be \"forgotten\", suggesting that evolving model generalizations throughout pre-training have larger effects on low-probability n-grams. Combined with findings that language models roughly follow n-gram learning early in pre-training and only later produce longform coherent text ( \u00a74; (Chang et al., 2021), language model learning might be characterized as early ngram learning, then gradual refinement of the tail n-gram probabilities based on longer contexts and more nuanced linguistic capabilities (e.g. world knowledge and reasoning; (Liu et al., 2021)."], "score": 0.92138671875}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 233289478, "title": "Probing Across Time: What Does RoBERTa Know and When?", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Leo Z. Liu", "authorId": null}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Jungo Kasai", "authorId": "11348687"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 87}, "snippets": ["We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired."], "score": 0.92724609375}, {"id": "(Qin et al., 2024)", "paper": {"corpus_id": 267627053, "title": "A systematic investigation of learnability from single child linguistic input", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yulu Qin", "authorId": "2283881086"}, {"name": "Wentao Wang", "authorId": "2108329007"}, {"name": "B. Lake", "authorId": "2373318"}], "n_citations": 5}, "snippets": ["Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset."], "score": 0.86962890625}, {"id": "(Neveditsin et al., 2024)", "paper": {"corpus_id": 271916426, "title": "Clinical insights: A comprehensive review of language models in medicine", "year": 2024, "venue": "PLOS Digital Health", "authors": [{"name": "Nikita Neveditsin", "authorId": "147692873"}, {"name": "Pawan Lingras", "authorId": "2171126372"}, {"name": "V. Mago", "authorId": "2103123095"}], "n_citations": 5}, "snippets": ["Nevertheless, challenges remain, as highlighted by Mahowald et al. (Mahowald et al., 2024), who emphasize the gap between formal and functional linguistic competencies in LLMs. Formal linguistic competencies refer to a model's ability to understand and generate syntactically and semantically correct language, enabling tasks like sentence completion, grammar correction, or summarization. In contrast, functional competencies involve applying language in practical, goal-oriented contexts, such as interpreting patient symptoms to suggest a diagnosis or deriving actionable steps from clinical guidelines."], "score": 0.8720703125}, {"id": "(Evanson et al., 2023)", "paper": {"corpus_id": 259089351, "title": "Language acquisition: do children and language models follow similar learning stages?", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Linnea Evanson", "authorId": "2219624467"}, {"name": "Yair Lakretz", "authorId": "3051598"}, {"name": "J. King", "authorId": "28972230"}], "n_citations": 29}, "snippets": ["During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language."], "score": 0.94970703125}], "table": null}, {"title": "Connections to Human Language Acquisition", "tldr": "Comparing language models to human acquisition reveals both similarities and differences, with models showing systematic learning progressions that partially overlap with child development. While LLMs rely primarily on distributional learning from text, they lack the embodied, interactive experiences that shape human language acquisition, resulting in different developmental trajectories despite some shared learning patterns. (2 sources)", "text": "\nResearch comparing language model training with human language acquisition has revealed intriguing parallels and important distinctions between these processes. Chang et al. conducted an innovative study that extracted \"learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory,\" a tool commonly used to assess vocabulary development in children <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. Their findings suggest that neural language models follow a developmental trajectory that shares some characteristics with human language acquisition, particularly in how they transition from simple to more complex prediction mechanisms.\n\nThe developmental sequence observed in language models shows a progression from relying on statistical frequencies to more sophisticated linguistic understanding. Chang et al. found that models \"predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions\" <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>. This progression mirrors aspects of child language development, where early word learning is influenced by frequency and later transitions to more complex linguistic patterns. These parallels \"shed light on the role of distributional learning mechanisms in children,\" suggesting that some statistical learning principles may be shared between human and artificial language learners <Paper corpusId=\"238408423\" paperTitle=\"(Chang et al., 2021)\" isShortName></Paper>.\n\nDespite these similarities, recent research indicates significant divergences between human and artificial language acquisition trajectories. Yang et al. proposed \"a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning\" based on classical language development theories <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. Their evaluation of language models using linguistic research methods revealed that \"although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition\" <Paper corpusId=\"273403825\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. This finding highlights that while language models may acquire certain linguistic capabilities in a systematic order similar to children, the specific sequence and mechanisms differ in important ways.\n\nThese differences likely stem from the fundamentally different learning environments and mechanisms available to humans versus language models. While models rely exclusively on text-based distributional learning, humans benefit from multimodal, interactive, and socially grounded experiences that provide additional cues for language acquisition <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. The absence of embodied experience and social interaction in language model training represents a critical distinction that shapes their developmental trajectory in ways that diverge from human language acquisition patterns.", "citations": [{"id": "(Chang et al., 2021)", "paper": {"corpus_id": 238408423, "title": "Word Acquisition in Neural Language Models", "year": 2021, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Tyler A. Chang", "authorId": "2087001989"}, {"name": "B. Bergen", "authorId": "24316216"}], "n_citations": 40}, "snippets": ["We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007)", "Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models."], "score": 0.8984375}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 273403825, "title": "From Babbling to Fluency: Evaluating the Evolution of Language Models in Terms of Human Language Acquisition", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qiyuan Yang", "authorId": "2326447675"}, {"name": "Pengda Wang", "authorId": "2294722814"}, {"name": "Luke D. Plonsky", "authorId": "2326296255"}, {"name": "Frederick L. Oswald", "authorId": "2250843586"}, {"name": "Hanjie Chen", "authorId": "7315244"}], "n_citations": 2}, "snippets": ["We examine the language capabilities of language models (LMs) from the critical perspective of human language acquisition. Building on classical language development theories, we propose a three-stage framework to assess the abilities of LMs, ranging from preliminary word understanding to complex grammar and complex logical reasoning. Using this framework, we evaluate the generative capacities of LMs using methods from linguistic research. Results indicate that although recent LMs outperform earlier models in overall performance, their developmental trajectory does not strictly follow the path of human language acquisition."], "score": 0.91357421875}], "table": null}], "cost": 0.38015699999999997}}
