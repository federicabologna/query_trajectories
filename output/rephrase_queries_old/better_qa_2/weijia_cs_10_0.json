{"clarifying_information": [{"clarifying_question1": "Are you interested in recent neural or classical extractive approaches to multi-document summarization, or both?", "clarifying_answer1": {"clarifying_answer": "I am interested in both recent neural and classical extractive approaches to multi-document summarization."}}, {"clarifying_question2": "Would you like references on graph-based, hierarchical, or entity-aware models specifically within multi-document summarization techniques?", "clarifying_answer2": {"clarifying_answer": "Graph-based models would be most helpful, particularly recent developments combining graph representation and neural models in multi-document summarization."}}, {"clarifying_question3": "Are you seeking information on datasets and evaluation metrics for multi-document summarization, or mainly methodological advancements?", "clarifying_answer3": {"clarifying_answer": "I am mainly interested in methodological advancements in multi-document summarization."}}], "better_query": "Can you provide references on graph-based approaches in multi-document summarization, including both classical extractive methods (like LexRank) and recent neural models that incorporate graph representations?", "better_answer": {"sections": [{"title": "Introduction/Background to Graph-Based Summarization", "tldr": "Graph-based approaches to multi-document summarization represent documents as graphs where nodes are textual units and edges represent relationships between them. These methods excel at capturing document structure and identifying important information through various centrality measures. (LLM Memory)", "text": "\nGraph-based approaches have emerged as a powerful paradigm in multi-document summarization, offering an intuitive and effective way to model the relationships between textual units. In graph-based summarization, documents are represented as graphs where nodes typically correspond to sentences, paragraphs, or other textual units, while edges represent relationships between these units such as semantic similarity, lexical overlap, or discourse connections. This representation allows summarization systems to capture the structure and organization of information across multiple documents. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe fundamental insight behind graph-based summarization is that important information tends to be central within the document graph structure. By analyzing the connectivity patterns, these methods can identify salient content without requiring extensive linguistic or domain knowledge. The core process typically involves three steps: constructing a graph representation of the documents, computing importance scores for nodes using various centrality measures, and selecting the most central nodes to form a summary. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nGraph-based approaches offer several advantages for multi-document summarization. They can effectively handle redundancy across documents by identifying similar content through graph connections. They also provide a natural way to capture both local and global document structure, helping to maintain coherence in the generated summaries. Additionally, graph representations are flexible and can incorporate various types of linguistic information, from simple lexical overlap to complex semantic relationships. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe evolution of graph-based summarization has closely followed broader trends in natural language processing, moving from purely statistical approaches to neural network-based methods that can learn rich representations from data. This progression has maintained the graph structure as a core component while enhancing how nodes and edges are represented and processed. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Classical Graph-Based Approaches", "tldr": "Classical graph-based approaches like LexRank and TextRank revolutionized multi-document summarization by using centrality measures to identify important sentences in document graphs. These methods employ various algorithms such as PageRank, HITS, and eigenvector centrality to rank sentences based on their connectivity patterns. (13 sources)", "text": "\nGraph-based approaches emerged as a significant paradigm in multi-document summarization in the early 2000s, with LexRank <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> and TextRank being the pioneering algorithms in this domain. These methods represent documents as graphs where sentences serve as nodes and the relationships between sentences, typically based on similarity measures, form the edges <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>.\n\nLexRank, introduced by Erkan and Radev, applies eigenvector centrality to identify prominent sentences within a document graph. In this approach, a connectivity matrix based on intra-sentence cosine similarity serves as the adjacency matrix for the graph representation <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The algorithm ranked first in several Document Understanding Conference (DUC) 2004 evaluation tasks, demonstrating its effectiveness in multi-document summarization.\n\nTextRank, another influential graph-based method, operates similarly to LexRank but constructs graphs based on content overlap between sentences <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. Both LexRank and TextRank derive from Google's PageRank algorithm, adapting its core principles to the task of sentence ranking <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper> <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nVarious centrality measures have been employed in these graph-based approaches to determine sentence importance. Degree centrality, which counts the number of connections a node has (normalized for graph size), and eigenvector centrality, which considers both direct connections and the connectedness of neighboring nodes, are among the most common metrics <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. Several studies have shown that degree centrality often performs best for most summarization tasks <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper> <Paper corpusId=\"13664986\" paperTitle=\"(Ozgur et al., 2008)\" isShortName></Paper>.\n\nIn addition to PageRank-based methods, other graph algorithms like HITS (Hyperlink-Induced Topic Search) have been adapted for summarization. The HITS algorithm determines the authority and hub values of nodes in the graph, with applications in identifying important sentences <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper> <Paper corpusId=\"2100529\" paperTitle=\"(Mihalcea, 2004)\" isShortName></Paper>.\n\nResearchers have continued to refine these classical graph-based approaches. Wan and Yang developed a method that assigns different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper> <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>. Other improvements include incorporating document-level information into the graph structure <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper> and applying graph-based ranking with clustering techniques, as demonstrated in CollabSum <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nTopic-sensitive variations have also been developed, such as topic-sensitive LexRank <Paper corpusId=\"10274362\" paperTitle=\"(Otterbacher et al., 2005)\" isShortName></Paper>, which introduces a query bias in transition probabilities to assign higher scores to sentences similar to a query. This adaptation makes the approach suitable for query-focused summarization tasks <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>.\n\nThe success of these classical graph-based methods lies in their ability to capture the structural and semantic relationships between sentences across multiple documents, effectively identifying the most central and important content without requiring extensive linguistic knowledge or domain-specific information <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper>.", "citations": [{"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."], "score": 0.87255859375}, {"id": "(Zhang et al., 2013)", "paper": {"corpus_id": 10274390, "title": "Clustering cliques for graph-based summarization of the biomedical research literature", "year": 2013, "venue": "BMC Bioinformatics", "authors": [{"name": "Han Zhang", "authorId": "48213346"}, {"name": "M. Fiszman", "authorId": "1719666"}, {"name": "Dongwook Shin", "authorId": "50877008"}, {"name": "Bartlomiej Wilkowski", "authorId": "2982296"}, {"name": "Thomas C. Rindflesch", "authorId": "1693891"}], "n_citations": 28}, "snippets": ["Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."], "score": 0.876953125}, {"id": "(Mohamed et al., 2019)", "paper": {"corpus_id": 145993667, "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis", "year": 2019, "venue": "Information Processing & Management", "authors": [{"name": "Muhidin A. Mohamed", "authorId": "47302320"}, {"name": "M. Oussalah", "authorId": "1714030"}], "n_citations": 101}, "snippets": ["In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."], "score": 0.94140625}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.9169921875}, {"id": "(Sipos et al., 2011)", "paper": {"corpus_id": 16689087, "title": "Large-Margin Learning of Submodular Summarization Methods", "year": 2011, "venue": "arXiv.org", "authors": [{"name": "Ruben Sipos", "authorId": "2195688"}, {"name": "Pannagadatta K. Shivaswamy", "authorId": "2248631"}, {"name": "T. Joachims", "authorId": "1680188"}], "n_citations": 6}, "snippets": ["A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities."], "score": 0.91796875}, {"id": "(Ozgur et al., 2008)", "paper": {"corpus_id": 13664986, "title": "Identifying gene-disease associations using centrality on a literature mined gene-interaction network", "year": 2008, "venue": "Intelligent Systems in Molecular Biology", "authors": [{"name": "Arzucan \u00d6zg\u00fcr", "authorId": "1702629"}, {"name": "T. Vu", "authorId": "2042777"}, {"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 372}, "snippets": ["Motivation: Understanding the role of genetics in diseases is one of the most important aims of the biological sciences. The completion of the Human Genome Project has led to a rapid increase in the number of publications in this area. However, the coverage of curated databases that provide information manually extracted from the literature is limited. Another challenge is that determining disease-related genes requires laborious experiments. Therefore, predicting good candidate genes before experimental analysis will save time and effort. We introduce an automatic approach based on text mining and network analysis to predict gene-disease associations. We collected an initial set of known disease-related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines. Our hypothesis is that the central genes in this disease-specific network are likely to be related to the disease. We used the degree, eigenvector, betweenness and closeness centrality metrics to rank the genes in the network. Results: The proposed approach can be used to extract known and to infer unknown gene-disease associations. We evaluated the approach for prostate cancer. Eigenvector and degree centrality achieved high accuracy. A total of 95% of the top 20 genes ranked by these methods are confirmed to be related to prostate cancer. On the other hand, betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study. Availability: A web-based system for browsing the disease-specific gene-interaction networks is available at: http://gin.ncibi.org Contact: radev@umich.edu"], "score": 0.0}, {"id": "(Mridha et al., 2021)", "paper": {"corpus_id": 244510269, "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges", "year": 2021, "venue": "IEEE Access", "authors": [{"name": "M. F. Mridha", "authorId": "2348260813"}, {"name": "Aklima Akter Lima", "authorId": "2149534558"}, {"name": "Prof. Dr. Kamruddin Nur", "authorId": "2086569"}, {"name": "S. Das", "authorId": "2152905283"}, {"name": "M. Hasan", "authorId": "2663201"}, {"name": "Muhammad Mohsin Kabir", "authorId": "2055992775"}], "n_citations": 88}, "snippets": ["The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document."], "score": 0.9140625}, {"id": "(Mihalcea, 2004)", "paper": {"corpus_id": 2100529, "title": "Graph-based Ranking Algorithms for Sentence Extraction, Applied to Text Summarization", "year": 2004, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Rada Mihalcea", "authorId": "145557251"}], "n_citations": 520}, "snippets": ["This paper presents an innovative unsupervised method for automatic sentence extraction using graph-based ranking algorithms. We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks."], "score": 0.0}, {"id": "(Wan et al., 2006)", "paper": {"corpus_id": 5457260, "title": "Improved Affinity Graph Based Multi-Document Summarization", "year": 2006, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}, {"name": "Jianwu Yang", "authorId": "1743923"}], "n_citations": 109}, "snippets": ["Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization."], "score": 0.9384765625}, {"id": "(Kumar et al., 2016)", "paper": {"corpus_id": 33696521, "title": "A Review on Automatic Text Summarization Approaches", "year": 2016, "venue": "Journal of Computer Science", "authors": [{"name": "Y. J. Kumar", "authorId": "1734844"}, {"name": "O. Goh", "authorId": "2445567"}, {"name": "H. Basiron", "authorId": "2136088"}, {"name": "Ngo Hea Choon", "authorId": "3388053"}, {"name": "Puspalata C. Suppiah", "authorId": "21154901"}], "n_citations": 72}, "snippets": ["This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."], "score": 0.90771484375}, {"id": "(Otterbacher et al., 2005)", "paper": {"corpus_id": 10274362, "title": "Using Random Walks for Question-focused Sentence Retrieval", "year": 2005, "venue": "Human Language Technology - The Baltic Perspectiv", "authors": [{"name": "Jahna Otterbacher", "authorId": "2216583"}, {"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 179}, "snippets": ["We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."], "score": 0.0}, {"id": "(Lierde et al., 2019)", "paper": {"corpus_id": 182712867, "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization", "year": 2019, "venue": "Information Sciences", "authors": [{"name": "H. V. Lierde", "authorId": "29364954"}, {"name": "T. Chow", "authorId": "144134805"}], "n_citations": 26}, "snippets": ["The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously."], "score": 0.90625}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."], "score": 0.9326171875}], "table": null}, {"title": "Graph Construction and Representation Methods", "tldr": "Graph construction in multi-document summarization involves creating nodes from textual units and edges from various relationships between them. Different representation approaches range from simple lexical similarity graphs to complex semantic networks incorporating discourse relationships, entity information, and hierarchical structures. (24 sources)", "text": "\nGraph-based approaches to multi-document summarization rely on effective graph construction and representation methods to model the relationships between textual units. The way these graphs are constructed significantly impacts the performance of summarization systems. In most implementations, sentences or paragraphs serve as nodes, while edges represent the relationships between these textual units <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. However, the specific methods for constructing these graphs vary considerably across different approaches.\n\nThe most common approach is to construct similarity-based graphs, where edges represent lexical or semantic similarity between sentences. LexRank and TextRank, for example, build connectivity matrices based on intra-sentence cosine similarity to form graph representations <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. These similarity measures are often computed using term frequency-inverse document frequency (TF-IDF) weights to determine the importance of words in establishing connections <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nBeyond simple similarity graphs, researchers have developed more sophisticated graph representations that incorporate various linguistic features. These include sentence position <Paper corpusId=\"11989149\" paperTitle=\"(Katragadda et al., 2009)\" isShortName></Paper>, term frequency <Paper corpusId=\"5519987\" paperTitle=\"(Lin et al., 1997)\" isShortName></Paper>, topic signatures <Paper corpusId=\"8598694\" paperTitle=\"(Lin et al., 2000)\" isShortName></Paper>, lexical chains, and syntactic patterns <Paper corpusId=\"503868\" paperTitle=\"(Haggag, 2013)\" isShortName></Paper> <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. Such features enrich the graph representation by capturing more complex relationships between textual units.\n\nDiscourse-based graphs have emerged as another significant approach, where edges represent discourse relations between sentences or documents. These graphs capture the logical and rhetorical structure of the text, helping to generate more coherent summaries <Paper corpusId=\"219036690\" paperTitle=\"(Xu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. The G-Flow and DISCOBERT models, for instance, utilize discourse graphs to produce concise and informative summaries <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nSeveral researchers have proposed multi-level or heterogeneous graph representations that incorporate different types of nodes and edges. Canhasi and Kononenko developed a three-layer graph representation consisting of terms, sentences, and document vertices, linked via term-sentence and sentence-document connections <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. This approach enables the model to capture relationships at different granularity levels. Similarly, Wang et al. proposed a heterogeneous graph-based neural network that includes semantic nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"257219819\" paperTitle=\"(Zesheng et al., 2023)\" isShortName></Paper>.\n\nConcept graphs have emerged as an alternative to traditional sentence-based representations. Instead of representing sentences directly, these graphs model the concepts and their relationships within the documents <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. Plaza et al. adapted this approach for biomedical summarization by modeling concept graphs specific to the domain <Paper corpusId=\"5674504\" paperTitle=\"(Plaza et al., 2008)\" isShortName></Paper>.\n\nMore recent approaches have incorporated entity and event information into graph representations. Zhou et al. presented an entity-aware model that constructs a knowledge graph with text units and entities as nodes, utilizing coreference resolution tools to extract semantic entities <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. Li et al. proposed using event relation graphs to represent documents for multi-document summarization <Paper corpusId=\"196170254\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nThe evolution of graph representation methods has moved toward incorporating more semantic information and utilizing neural networks to learn graph representations. For instance, Pasunuru et al. developed a graph-enhanced approach that leverages knowledge graphs constructed using natural language processing tools to capture multi-level semantic information within documents <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. These neural graph-based models can learn rich representations that capture complex relationships between textual units <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nAbstract Meaning Representation (AMR) has also been used to construct semantic graphs for summarization. This approach provides a structured representation of sentences that captures their meaning independent of surface form <Paper corpusId=\"5001921\" paperTitle=\"(Liu et al., 2018)\" isShortName></Paper> <Paper corpusId=\"49210924\" paperTitle=\"(Liao et al., 2018)\" isShortName></Paper> <Paper corpusId=\"51867906\" paperTitle=\"(Dohare et al., 2018)\" isShortName></Paper> <Paper corpusId=\"52111191\" paperTitle=\"(Hardy et al., 2018)\" isShortName></Paper> <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nThe diversity of graph construction and representation methods reflects the ongoing exploration of how best to model document structures and relationships for effective summarization. Each approach offers different advantages in capturing various aspects of textual relationships, from simple lexical connections to complex semantic structures <Paper corpusId=\"273784877\" paperTitle=\"(Ragazzi et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Zhang et al., 2013)", "paper": {"corpus_id": 10274390, "title": "Clustering cliques for graph-based summarization of the biomedical research literature", "year": 2013, "venue": "BMC Bioinformatics", "authors": [{"name": "Han Zhang", "authorId": "48213346"}, {"name": "M. Fiszman", "authorId": "1719666"}, {"name": "Dongwook Shin", "authorId": "50877008"}, {"name": "Bartlomiej Wilkowski", "authorId": "2982296"}, {"name": "Thomas C. Rindflesch", "authorId": "1693891"}], "n_citations": 28}, "snippets": ["Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."], "score": 0.876953125}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."], "score": 0.87255859375}, {"id": "(Han et al., 2016)", "paper": {"corpus_id": 13237013, "title": "Text Summarization Using FrameNet-Based Semantic Graph Model", "year": 2016, "venue": "Scientific Programming", "authors": [{"name": "Xu Han", "authorId": "2110981656"}, {"name": "Tao Lv", "authorId": "143750477"}, {"name": "Zhirui Hu", "authorId": "48430746"}, {"name": "Xinyan Wang", "authorId": "2115554077"}, {"name": "Cong Wang", "authorId": "50097174"}], "n_citations": 25}, "snippets": ["Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms."], "score": 0.9091796875}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 170079112, "title": "Hierarchical Transformers for Multi-Document Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 298}, "snippets": ["The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links."], "score": 0.9169921875}, {"id": "(Katragadda et al., 2009)", "paper": {"corpus_id": 11989149, "title": "Sentence Position revisited: A robust light-weight Update Summarization \u2018baseline\u2019 Algorithm", "year": 2009, "venue": "", "authors": [{"name": "Rahul Katragadda", "authorId": "1958707"}, {"name": "Prasad Pingali", "authorId": "1941849"}, {"name": "Vasudeva Varma", "authorId": "1704709"}], "n_citations": 42}, "snippets": ["In this paper, we describe a sentence position based summarizer that is built based on a sentence position policy, created from the evaluation testbed of recent summarization tasks at Document Understanding Conferences (DUC). We show that the summarizer thus built is able to outperform most systems participating in task focused summarization evaluations at Text Analysis Conferences (TAC) 2008. Our experiments also show that such a method would perform better at producing short summaries (upto 100 words) than longer summaries. Further, we discuss the baselines traditionally used for summarization evaluation and suggest the revival of an old baseline to suit the current summarization task at TAC: the Update Summarization task."], "score": 0.0}, {"id": "(Lin et al., 1997)", "paper": {"corpus_id": 5519987, "title": "Identifying Topics by Position", "year": 1997, "venue": "Applied Natural Language Processing Conference", "authors": [{"name": "Chin-Yew Lin", "authorId": "1781574"}, {"name": "E. Hovy", "authorId": "144547315"}], "n_citations": 316}, "snippets": ["This paper addresses the problem of identifying likely topics of texts by their position in the text. It describes the automated training and evaluation of an Optimal Position Policy, a method of locating the likely positions of topic-bearing sentences based on genre-specific regularities of discourse structure. This method can be used in applications such as information retrieval, routing, and text summarization."], "score": 0.0}, {"id": "(Lin et al., 2000)", "paper": {"corpus_id": 8598694, "title": "The Automated Acquisition of Topic Signatures for Text Summarization", "year": 2000, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Chin-Yew Lin", "authorId": "1781574"}, {"name": "E. Hovy", "authorId": "144547315"}], "n_citations": 548}, "snippets": ["In order to produce a good summary, one has to identify the most relevant portions of a given text. We describe in this paper a method for automatically training topic signatures-sets of related words, with associated weights, organized around head topics and illustrate with signatures we created with 6,194 TREC collection texts over 4 selected topics. We describe the possible integration of topic signatures with outologies and its evaluaton on an automated text summarization system."], "score": 0.0}, {"id": "(Haggag, 2013)", "paper": {"corpus_id": 503868, "title": "Semantic Text Summarization Based on Syntactic Patterns", "year": 2013, "venue": "International Journal of Information Retrieval Research", "authors": [{"name": "M. Haggag", "authorId": "8442172"}], "n_citations": 6}, "snippets": ["Text summarization is machine based generation of a shortened version of a text. The summary should be a non-redundant extract from the original text. Most researches of text summarization use sentence extraction instead of abstraction to produce a summary. Extraction is depending mainly on sentences that already contained in the original input, which makes it more accurate and more concise. When all input articles are surrounding a particular event, extracting similar sentences would result in producing a highly repetitive summary. In this paper, a novel model for text summarization is proposed based on removing the non-effective sentences in producing an extract from the text. The model utilizes semantic analysis by evaluating sentences similarity. This similarity is provided by evaluating individual words similarity as well as syntactic relationships between neighboring words. These relationships addressed throughout the model as syntactic patterns. Word senses and the correlating part of speech for the word within context are provided in the semantic processing of matched patterns. The introduction of syntactic patterns knowledge supports text reduction by mapping the matched patterns into summarized ones. In addition, syntactic patterns make use of sentence relatedness evaluation in defining which sentences to keep and which to drop. Experiments proved that the model presented throughout the paper is well performing in results evaluation of compression rate, accuracy, recall and other human criteria like correctness, novelty, fluency and usefulness."], "score": 0.0}, {"id": "(Xu et al., 2019)", "paper": {"corpus_id": 219036690, "title": "Discourse-Aware Neural Extractive Text Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jiacheng Xu", "authorId": "34837371"}, {"name": "Zhe Gan", "authorId": "144702900"}, {"name": "Yu Cheng", "authorId": "145215470"}, {"name": "Jingjing Liu", "authorId": "46700348"}], "n_citations": 282}, "snippets": ["Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization."], "score": 0.93017578125}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 239768911, "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Moye Chen", "authorId": "2108267116"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 22}, "snippets": ["The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS", ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."], "score": 0.9423828125}, {"id": "(Mohamed et al., 2019)", "paper": {"corpus_id": 145993667, "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis", "year": 2019, "venue": "Information Processing & Management", "authors": [{"name": "Muhidin A. Mohamed", "authorId": "47302320"}, {"name": "M. Oussalah", "authorId": "1714030"}], "n_citations": 101}, "snippets": ["In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."], "score": 0.94140625}, {"id": "(Zesheng et al., 2023)", "paper": {"corpus_id": 257219819, "title": "Topic-Selective Graph Network for Topic-Focused Summarization", "year": 2023, "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "authors": [{"name": "Shi Zesheng", "authorId": "1764406"}, {"name": "Yucheng Zhou", "authorId": "2110348767"}], "n_citations": 4}, "snippets": ["In text summarization, Wang et al. [28] propose a heterogeneous graph-based neural network for extracting summaries, which contains semantic nodes of different granularity levels except sentences. These extra nodes act as \"intermediaries\" between sentences and enrich cross-sentence relations. The introduction of document nodes allows the graph structure to be flexibly extended from a single document setup to multiple documents. Another work [8] proposes a multiplex graph summary (Multi-GraS) model based on multiplex graph convolutional networks that can be used to extract text summaries. This model not only considers Various types of intersentential relations (such as semantic similarity and natural connection), and intra-sentential relations (such as semantic and syntactic relations between words) are also modeled."], "score": 0.89453125}, {"id": "(Plaza et al., 2008)", "paper": {"corpus_id": 5674504, "title": "Concept-Graph Based Biomedical Automatic Summarization Using Ontologies", "year": 2008, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Laura Plaza", "authorId": "145377713"}, {"name": "Alberto D\u00edaz", "authorId": "102639698"}, {"name": "Pablo Gerv\u00e1s", "authorId": "1798564"}], "n_citations": 53}, "snippets": ["One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified."], "score": 0.0}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."], "score": 0.97119140625}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP", "Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.88330078125}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 196170254, "title": "Abstractive Multi-Document Summarization Based on Semantic Link Network", "year": 2021, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "H. Zhuge", "authorId": "143632630"}], "n_citations": 30}, "snippets": ["The key to realize advanced document summarization is semantic representation of documents. This paper investigates the role of Semantic Link Network in representing and understanding documents for multi-document summarization. It proposes a novel abstractive multi-document summarization framework by first transforming documents into a Semantic Link Network of concepts and events and then transforming the Semantic Link Network into the summary of the documents based on the selection of important concepts and events while keeping semantics coherence. Experiments on benchmark datasets show that the proposed summarization approach significantly outperforms relevant state-of-the-art baselines and the Semantic Link Network plays an important role in representing and understanding documents."], "score": 0.0}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."], "score": 0.9326171875}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."], "score": 0.962890625}, {"id": "(Ribeiro, 2022)", "paper": {"corpus_id": 264614849, "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Leonardo F. R. Ribeiro", "authorId": "10430740"}], "n_citations": 60}, "snippets": ["A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization."], "score": 0.91015625}, {"id": "(Liu et al., 2018)", "paper": {"corpus_id": 5001921, "title": "Toward Abstractive Summarization Using Semantic Representations", "year": 2018, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Fei Liu", "authorId": "144544919"}, {"name": "Jeffrey Flanigan", "authorId": "144683841"}, {"name": "Sam Thomson", "authorId": "38094552"}, {"name": "N. Sadeh", "authorId": "2464164"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 302}, "snippets": ["We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-tograph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-totext generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization"], "score": 0.0}, {"id": "(Liao et al., 2018)", "paper": {"corpus_id": 49210924, "title": "Abstract Meaning Representation for Multi-Document Summarization", "year": 2018, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Kexin Liao", "authorId": "49792730"}, {"name": "Logan Lebanoff", "authorId": "50827114"}, {"name": "Fei Liu", "authorId": "144544919"}], "n_citations": 105}, "snippets": ["Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research."], "score": 0.0}, {"id": "(Dohare et al., 2018)", "paper": {"corpus_id": 51867906, "title": "Unsupervised Semantic Abstractive Summarization", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shibhansh Dohare", "authorId": "13295612"}, {"name": "Vivek Gupta", "authorId": "46346053"}, {"name": "H. Karnick", "authorId": "2376013"}], "n_citations": 19}, "snippets": ["Automatic abstractive summary generation remains a significant open problem for natural language processing. In this work, we develop a novel pipeline for Semantic Abstractive Summarization (SAS). SAS, as introduced by Liu et. al. (2015) first generates an AMR graph of an input story, through which it extracts a summary graph and finally, creates summary sentences from this summary graph. Compared to earlier approaches, we develop a more comprehensive method to generate the story AMR graph using state-of-the-art co-reference resolution and Meta Nodes. Which we then use in a novel unsupervised algorithm based on how humans summarize a piece of text to extract the summary sub-graph. Our algorithm outperforms the state of the art SAS method by 1.7% F1 score in node prediction."], "score": 0.0}, {"id": "(Hardy et al., 2018)", "paper": {"corpus_id": 52111191, "title": "Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation", "year": 2018, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hardy Hardy", "authorId": "46441201"}, {"name": "Andreas Vlachos", "authorId": "2064056928"}], "n_citations": 69}, "snippets": ["Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset."], "score": 0.0}, {"id": "(Ragazzi et al., 2025)", "paper": {"corpus_id": 273784877, "title": "Cross-Document Distillation via Graph-Based Summarization of Extracted Essential Knowledge", "year": 2025, "venue": "IEEE Transactions on Audio, Speech, and Language Processing", "authors": [{"name": "Luca Ragazzi", "authorId": "134327204"}, {"name": "Gianluca Moro", "authorId": "2237646991"}, {"name": "Lorenzo Valgimigli", "authorId": "2132084411"}, {"name": "Riccardo Fiorani", "authorId": "2328891652"}], "n_citations": 0}, "snippets": ["Despite the proficiency exhibited by language models in text summarization, challenges persist in capturing and aggregating salient information dispersed across a cluster of lengthy sources. To accommodate more input, existing solutions prioritize sparse attention mechanisms, relying on sequence truncation without incorporating graph-based modeling of multiple semantic units to locate essential facets."], "score": 0.900390625}], "table": null}, {"title": "Centrality and Ranking Algorithms", "tldr": "Graph-based summarization systems rely on centrality measures to identify important sentences, with PageRank, HITS, and eigenvector centrality being the most prominent algorithms. Various adaptations of these algorithms have been developed for different summarization tasks, including threshold-based approaches, query-focused variations, and directional edge weighting schemes. (15 sources)", "text": "\nAt the core of graph-based summarization lies the concept of centrality - a measure that determines the importance of nodes within a graph structure. Various centrality measures have been employed to identify salient sentences in document graphs, with degree centrality and eigenvector centrality being among the most common <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>. Degree centrality simply counts the number of connections a node has (normalized for graph size), while eigenvector centrality considers both direct connections and the connectedness of neighboring nodes <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper> <Paper corpusId=\"13664986\" paperTitle=\"(Ozgur et al., 2008)\" isShortName></Paper>. Several studies have shown that degree centrality often performs best for most summarization tasks <Paper corpusId=\"10274390\" paperTitle=\"(Zhang et al., 2013)\" isShortName></Paper>.\n\nThe most widely used graph ranking algorithms in summarization are adaptations of web search algorithms. PageRank serves as the foundation for LexRank and TextRank, where the algorithm is modified to work with sentence similarity instead of hyperlinks <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"232404911\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper>. These algorithms determine the relevance of a vertex based on global information iteratively extracted from the entire graph <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper>. In LexRank, a connectivity matrix based on intra-sentence cosine similarity serves as the adjacency matrix for the graph representation <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"8437725\" paperTitle=\"(Parveen et al., 2014)\" isShortName></Paper>.\n\nLexRank has been implemented in several variations, including LexRank with threshold and continuous LexRank, with studies showing that the threshold method often outperforms other degree-based techniques <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. In addition to PageRank-based methods, other graph algorithms like HITS (Hyperlink-Induced Topic Search) have been adapted for summarization. The HITS algorithm determines the authority and hub values of nodes in the graph to identify important sentences <Paper corpusId=\"244510269\" paperTitle=\"(Mridha et al., 2021)\" isShortName></Paper> <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper>.\n\nRecent advancements in centrality-based ranking algorithms have led to more sophisticated approaches. PacSum constructs graphs with directed edges, considering that the centrality of two nodes should be influenced by their relative position in the document <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"182952892\" paperTitle=\"(Zheng et al., 2019)\" isShortName></Paper>. DASG selects sentences based on both similarities and relative distances among neighboring sentences, incorporating a graph edge weighting scheme that uses coefficients calculated by relative distance <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"235792514\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. FAR modifies the centrality model by applying a facet-aware ranking approach to filter out insignificant sentences <Paper corpusId=\"264817506\" paperTitle=\"(Mao et al., 2023)\" isShortName></Paper> <Paper corpusId=\"236477774\" paperTitle=\"(Liang et al., 2021)\" isShortName></Paper>.\n\nFor query-focused summarization, adaptations of centrality algorithms have been developed. Topic-sensitive LexRank introduces a query bias in transition probabilities, assigning higher scores to sentences similar to a query <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper> <Paper corpusId=\"10274362\" paperTitle=\"(Otterbacher et al., 2005)\" isShortName></Paper>. Similarly, manifold ranking approaches have been proposed where scores are propagated across a graph including both sentences and the query as vertices <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>.\n\nTo address redundancy in summaries, specialized ranking algorithms have been developed. DivRank, for example, tends to select dissimilar sentences to ensure diversity in the summary <Paper corpusId=\"182712867\" paperTitle=\"(Lierde et al., 2019)\" isShortName></Paper>. Other approaches combine clustering with graph-based ranking, such as CollabSum, which first clusters documents and then applies graph-based algorithms that incorporate both intra-document and inter-document sentence similarities <Paper corpusId=\"16689087\" paperTitle=\"(Sipos et al., 2011)\" isShortName></Paper> <Paper corpusId=\"219690773\" paperTitle=\"(Khan et al., 2020)\" isShortName></Paper>.\n\nSome recent approaches have moved beyond traditional centrality measures to incorporate document-level information. The DsR model uses a document-sensitive graph-based ranking approach that considers the influence of the entire document set at the sentence level <Paper corpusId=\"13237013\" paperTitle=\"(Han et al., 2016)\" isShortName></Paper>. Other improvements include assigning different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"219690773\" paperTitle=\"(Khan et al., 2020)\" isShortName></Paper>.\n\nThe evolution of centrality-based ranking algorithms in graph-based summarization reflects ongoing efforts to better capture the complex relationships between textual units and identify the most salient content for inclusion in summaries. Each approach offers different advantages in modeling document structure and relationships, from simple connectivity measures to sophisticated ranking schemes that account for position, direction, and document-level context.", "citations": [{"id": "(Zhang et al., 2013)", "paper": {"corpus_id": 10274390, "title": "Clustering cliques for graph-based summarization of the biomedical research literature", "year": 2013, "venue": "BMC Bioinformatics", "authors": [{"name": "Han Zhang", "authorId": "48213346"}, {"name": "M. Fiszman", "authorId": "1719666"}, {"name": "Dongwook Shin", "authorId": "50877008"}, {"name": "Bartlomiej Wilkowski", "authorId": "2982296"}, {"name": "Thomas C. Rindflesch", "authorId": "1693891"}], "n_citations": 28}, "snippets": ["Recently, graph structures have been used to represent source content to be summarized. Often, terms or sentences are represented as nodes and relations between them as arcs; however, abstractive representations are also used in graph-based analysis. Graph theory-based metrics have been proposed to identify salient information. Two commonly used metrics are degree centrality and eigenvector centrality, and both are based on connectedness. Degree centrality is determined by the connecting arcs a node has, normalized for the size of the graph, while eigenvector centrality is computed based on the connections a node has along with the connectedness of neighboring nodes. Several studies (e.g. (Erkan et al., 2004)(Zhang et al., 2007)(\u00d6zg\u00fcr et al., 2008)) have shown that degree centrality, when compared to other connectedness metrics, performs best for most tasks. LexRank (Erkan et al., 2004) and TextRank [21] have applied connectedness metrics to generate multidocument summaries. In LexRank, for example, nodes represent sentences and arcs similarity between them. Node connectedness is used to identify prominent sentences as a summary."], "score": 0.876953125}, {"id": "(Ozgur et al., 2008)", "paper": {"corpus_id": 13664986, "title": "Identifying gene-disease associations using centrality on a literature mined gene-interaction network", "year": 2008, "venue": "Intelligent Systems in Molecular Biology", "authors": [{"name": "Arzucan \u00d6zg\u00fcr", "authorId": "1702629"}, {"name": "T. Vu", "authorId": "2042777"}, {"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 372}, "snippets": ["Motivation: Understanding the role of genetics in diseases is one of the most important aims of the biological sciences. The completion of the Human Genome Project has led to a rapid increase in the number of publications in this area. However, the coverage of curated databases that provide information manually extracted from the literature is limited. Another challenge is that determining disease-related genes requires laborious experiments. Therefore, predicting good candidate genes before experimental analysis will save time and effort. We introduce an automatic approach based on text mining and network analysis to predict gene-disease associations. We collected an initial set of known disease-related genes and built an interaction network by automatic literature mining based on dependency parsing and support vector machines. Our hypothesis is that the central genes in this disease-specific network are likely to be related to the disease. We used the degree, eigenvector, betweenness and closeness centrality metrics to rank the genes in the network. Results: The proposed approach can be used to extract known and to infer unknown gene-disease associations. We evaluated the approach for prostate cancer. Eigenvector and degree centrality achieved high accuracy. A total of 95% of the top 20 genes ranked by these methods are confirmed to be related to prostate cancer. On the other hand, betweenness and closeness centrality predicted more genes whose relation to the disease is currently unknown and are candidates for experimental study. Availability: A web-based system for browsing the disease-specific gene-interaction networks is available at: http://gin.ncibi.org Contact: radev@umich.edu"], "score": 0.0}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."], "score": 0.87255859375}, {"id": "(Zhang et al., 2021)", "paper": {"corpus_id": 232404911, "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Haopeng Zhang", "authorId": "2135688409"}, {"name": "Jiawei Zhang", "authorId": "1718428"}], "n_citations": 0}, "snippets": ["Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization."], "score": 0.9052734375}, {"id": "(Mridha et al., 2021)", "paper": {"corpus_id": 244510269, "title": "A Survey of Automatic Text Summarization: Progress, Process and Challenges", "year": 2021, "venue": "IEEE Access", "authors": [{"name": "M. F. Mridha", "authorId": "2348260813"}, {"name": "Aklima Akter Lima", "authorId": "2149534558"}, {"name": "Prof. Dr. Kamruddin Nur", "authorId": "2086569"}, {"name": "S. Das", "authorId": "2152905283"}, {"name": "M. Hasan", "authorId": "2663201"}, {"name": "Muhammad Mohsin Kabir", "authorId": "2055992775"}], "n_citations": 88}, "snippets": ["The graph-based method can be applied to both extractive and abstractive text summarization. This approach is an unsupervised learning method that rates the required sentences or terms using a graph. The purpose of the graphical process is to extract the most relevant sentences from a single text (Moratanch et al., 2017). \n\nGraph-based ranking algorithms determine the relevance of a vertex in a graph based on global information iteratively extracted from the entire chart. When it comes to text summarization, specific graph-based techniques are applied. 1) LexRank: LexRank is a probabilistic graph-based technique for calculating sentence significance based on the notion of eigenvector centrality in a graph representation of phrases for natural language processing. It is a connectivity matrix based on intra-sentence cosine similarity that is utilized as the adjacency matrix in a sentence graph representation (Erkan et al., 2004). (Otterbacher et al., 2009), (Wu et al., 2015), (Verma et al., 2016), (Li et al., 2016), (Samuel et al., 2016) proposed the LexRank algorithm for graph-based text summarization task. 2) Hyperlink-Induced Topic Search (HITS): Hyperlinkinduced topic search is a link analysis algorithm that determines the authority and hub values. The results for the search query are retrieved, and then the computation is performed only on this set of results. A hub value is the total of the scaled authority values of the pages it points to, and an authority value is the sum of the scaled authority values of the pages it points to (Litvak et al., 2008). Some articles focused on the HITS ranking algorithm for graphbased text summarization tasks, as can be seen in (Thakkar et al., 2010), (Mihalcea, 2004), (Bhandari et al., 2008), and (Zhang et al., 2005). 3) PageRank: The PageRank algorithm utilizes the inbound links of specified pages to measure their significance or quality to rank the search results. PageRank links more weight based on the importance of the page from which it originates [267]. Some articles proposed the PageRank algorithm for summarization (Elbarougy et al., 2020), (Mallick et al., 2018), (Wang et al., 2007), (Chongsuntornsri et al., 2006), (Sornil et al., 2006). 4) TextRank: TextRank is an unsupervised method for automatic text summarization to extract the most important keywords from a document."], "score": 0.9140625}, {"id": "(Parveen et al., 2014)", "paper": {"corpus_id": 8437725, "title": "Multi-document Summarization Using Bipartite Graphs", "year": 2014, "venue": "TextGraphs@EMNLP", "authors": [{"name": "Daraksha Parveen", "authorId": "2567321"}, {"name": "M. Strube", "authorId": "31380436"}], "n_citations": 16}, "snippets": ["A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan et al., 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by (Gong et al., 2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent semantic analysis."], "score": 0.90283203125}, {"id": "(Sipos et al., 2011)", "paper": {"corpus_id": 16689087, "title": "Large-Margin Learning of Submodular Summarization Methods", "year": 2011, "venue": "arXiv.org", "authors": [{"name": "Ruben Sipos", "authorId": "2195688"}, {"name": "Pannagadatta K. Shivaswamy", "authorId": "2248631"}, {"name": "T. Joachims", "authorId": "1680188"}], "n_citations": 6}, "snippets": ["A popular stohastic graph-based summarization method is LexRank (Erkan et al., 2004). It computes sentence importance based on the concept of eigenvector centrality in a graph of sentence similarities. Similarly, TextRank [16] is also graph based ranking system for identification of important sentences in a document by using sentence similarity and PageRank [17]. Sentence extraction can also be implemented using other graph based scoring approaches (Mihalcea, 2004) such as HITS (Kleinberg, 1999) and positional power functions. \n\nGraph based methods can also be paired with clustering such as in CollabSum (Wan et al., 2007). This approach first uses clustering to obtain document clusters and then uses graph based algorithm for sentence selection which includes inter and intra-document sentence similarities."], "score": 0.91796875}, {"id": "(Mao et al., 2023)", "paper": {"corpus_id": 264817506, "title": "Bipartite Graph Pre-training for Unsupervised Extractive Summarization with Graph Convolutional Auto-Encoders", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Qianren Mao", "authorId": "67081502"}, {"name": "Shaobo Zhao", "authorId": "2263933839"}, {"name": "Jiarui Li", "authorId": "2264274905"}, {"name": "Xiaolei Gu", "authorId": "2263823705"}, {"name": "Shizhu He", "authorId": "2263839211"}, {"name": "Bo Li", "authorId": "2264733691"}, {"name": "Jianxin Li", "authorId": "2263794896"}], "n_citations": 2}, "snippets": ["There are several simple unsupervised summarization extraction frameworks, including TextRank (Mihalcea et al., 2004) and LexRank (Erkan et al., 2004), as well as more robust graph-based ranking methods such as Pac-Sum (Zheng et al., 2019), FAR (Liang et al., 2021), DASG (Liu et al., 2021). Graph-based ranking methods take sentence representations as input, using the algorithm of graph-based sentence centrality ranking for sentence selection. We now introduce extractive summarization backbones. \n\n\u2022 TextRank and LexRank utilize PageRank to calculate node centrality based on a Markov chain model recursively. \n\n\u2022 PacSum (Zheng and Lapata, 2019) constructs graphs with directed edges. The rationale behind this approach is that the centrality of two nodes is influenced by their relative position in the document, as illustrated by Equation 15. \n\n\u2022 DASG (Liu et al., 2021) selects sentences for summarization based on the similarities and relative distances among neighbouring sentences. It incorporates a graph edge weighting scheme to Equation 15, using a coefficient that maps a pair of sentence indices to a value calculated by their relative distance. \n\n\u2022 FAR (Liang et al., 2021) modifies Equation 15by applying a facet-aware centrality-based ranking model to filter out insignificant sentences."], "score": 0.916015625}, {"id": "(Zheng et al., 2019)", "paper": {"corpus_id": 182952892, "title": "Sentence Centrality Revisited for Unsupervised Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Hao Zheng", "authorId": "2115239329"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 171}, "snippets": ["Single document summarization has enjoyed renewed interest in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. In this paper we develop an unsupervised approach arguing that it is unrealistic to expect large-scale and high-quality training data to be available or created for different types of summaries, domains, or languages. We revisit a popular graph-based ranking algorithm and modify how node (aka sentence) centrality is computed in two ways: (a) we employ BERT, a state-of-the-art neural representation learning model to better capture sentential meaning and (b) we build graphs with directed edges arguing that the contribution of any two nodes to their respective centrality is influenced by their relative position in a document. Experimental results on three news summarization datasets representative of different languages and writing styles show that our approach outperforms strong baselines by a wide margin."], "score": 0.0}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 235792514, "title": "Unsupervised Extractive Text Summarization with Distance-Augmented Sentence Graphs", "year": 2021, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Jingzhou Liu", "authorId": null}, {"name": "Dominic J. D. Hughes", "authorId": "5677927"}, {"name": "Yiming Yang", "authorId": "46286308"}], "n_citations": 29}, "snippets": ["Supervised summarization has made significant improvements in recent years by leveraging cutting-edge deep learning technologies. However, the true success of supervised methods relies on the availability of large quantity of human-generated summaries of documents, which is highly costly and difficult to obtain in general. This paper proposes an unsupervised approach to extractive text summarization, which uses an automatically constructed sentence graph from each document to select salient sentences for summarization based on both the similarities and relative distances in the neighborhood of each sentences. We further generalize our approach from single-document summarization to a multi-document setting, by aggregating document-level graphs via proximity-based cross-document edges. In our experiments on benchmark datasets, the proposed approach achieved competitive or better results than previous state-of-the-art unsupervised extractive summarization methods in both single-document and multi-document settings, and the performance is competitive to strong supervised baselines."], "score": 0.0}, {"id": "(Liang et al., 2021)", "paper": {"corpus_id": 236477774, "title": "Improving Unsupervised Extractive Summarization with Facet-Aware Modeling", "year": 2021, "venue": "Findings", "authors": [{"name": "Xinnian Liang", "authorId": "120436437"}, {"name": "Shuangzhi Wu", "authorId": "2362902"}, {"name": "Mu Li", "authorId": "2112143809"}, {"name": "Zhoujun Li", "authorId": "1707275"}], "n_citations": 44}, "snippets": ["Unsupervised extractive summarization aims to extract salient sentences from documents without labeled corpus. Existing methods are mostly graph-based by computing sentence centrality. These methods usually tend to select sentences within the same facet, however, which often leads to the facet bias problem especially when the document has multiple facets (i.e. long-document and multi-documents). To address this problem, we proposed a novel facet-aware centrality-based ranking model. We let the model pay more attention to different facets by introducing a sentence-document weight. The weight is added to the sentence centrality score. We evaluate our method on a wide range of summarization tasks that include 8 representative benchmark datasets. Experimental re-sults show that our method consistently out-performs strong baselines especially in long-and multi-document scenarios and even performs comparably to some supervised models. Extensive analyses con\ufb01rm that the performance gains come from alleviating the facet bias problem."], "score": 0.0}, {"id": "(Lierde et al., 2019)", "paper": {"corpus_id": 182712867, "title": "Learning with fuzzy hypergraphs: A topical approach to query-oriented text summarization", "year": 2019, "venue": "Information Sciences", "authors": [{"name": "H. V. Lierde", "authorId": "29364954"}, {"name": "T. Chow", "authorId": "144134805"}], "n_citations": 26}, "snippets": ["The earliest graph-based summarizer, called LexRank (Erkan et al., 2004), defines edges as term co-occurrence relationships between sentences. Then, PageRank algorithm is applied to compute relevance scores of sentences. Adapting this idea for the task of query-focused summarization, topic sensitive LexRank (Otterbacher et al., 2005) introduces a query bias in probabilities of transition, which results in higher scores for sentences that are similar to the query. Sim-ilarly, (Wan, 2013) proposes a manifold ranking algorithm in which scores are popagated accross a graph including both sentences and the query as vertices. To remove redundancies in summaries, (Mei et al., 2010) proposes a new node ranking algorithm called DivRank, which tends to select dissimilar sentences. While early graph-based algorithms only involved sentences, a bipartite graph model is proposed in (Wan et al., 2008), involving both sentences and terms as vertices and it applies HITS algorithm to score sentences. (Wei et al., 2010) combines this idea with a PageRank-like method to score sentences, terms and documents simultaneously."], "score": 0.90625}, {"id": "(Otterbacher et al., 2005)", "paper": {"corpus_id": 10274362, "title": "Using Random Walks for Question-focused Sentence Retrieval", "year": 2005, "venue": "Human Language Technology - The Baltic Perspectiv", "authors": [{"name": "Jahna Otterbacher", "authorId": "2216583"}, {"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 179}, "snippets": ["We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. \"How many victims have been found?\") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline."], "score": 0.0}, {"id": "(Khan et al., 2020)", "paper": {"corpus_id": 219690773, "title": "Movie Review Summarization Using Supervised Learning and Graph-Based Ranking Algorithm", "year": 2020, "venue": "Computational Intelligence and Neuroscience", "authors": [{"name": "Atif Khan", "authorId": "1561421122"}, {"name": "Muhammad Adnan Gul", "authorId": "1750930091"}, {"name": "M. Zareei", "authorId": "3060705"}, {"name": "R. Rajesh", "authorId": "9820762"}, {"name": "Asim Zeb", "authorId": "1411333558"}, {"name": "Muhammad Naeem", "authorId": "2057127260"}, {"name": "Yousaf Saeed", "authorId": "1411821449"}, {"name": "N. Salim", "authorId": "1680372"}], "n_citations": 16}, "snippets": ["In recent years, various graph-based methods have attracted more attention and effectively attempted for text summarization. These methods utilize PageRank algorithm [39] and its variants to give rank/score to graph nodes, which represent sentences or passages. The authors in [40] proposed a connectivity graph, which assumes that nodes only carry significant information if they are connected to many other nodes. The authors in [41] introduced a Lex-PageRank approach that is based on eigenvector centrality, which constructs a sentence connectivity matrix and utilizes similar algorithm like PageRank to find the significant sentences for summary. A similar algorithm to PageRank was also proposed in [42] which finds salient sentences for summary generation.\n\nThe authors in [24] presented a graph-based approach, which integrates surface features with text content and investigates subtopic features in multiple documents to incorporate them into the graph-based ranking algorithm. A multidocument summarization approach based on affinity graph [43] exploits similar algorithm to PageRank and computes sentence scores in the affinity graph based on information richness. The authors in [44] demonstrated a document-sensitive graph model for multidocument generic summarization and highlighted the impact of global document set information at sentence level. A weighted graph model for generic multidocument summarization introduced in [45] combines sentence ranking and sentence clustering methods. The authors in [46] presented a graphbased method for multidocument summarization of Vietnamese documents and employed traditional PageRank algorithm to rank the important sentences. The authors in [47] demonstrated an event graph-based approach for multidocument extractive summarization."], "score": 0.91845703125}, {"id": "(Han et al., 2016)", "paper": {"corpus_id": 13237013, "title": "Text Summarization Using FrameNet-Based Semantic Graph Model", "year": 2016, "venue": "Scientific Programming", "authors": [{"name": "Xu Han", "authorId": "2110981656"}, {"name": "Tao Lv", "authorId": "143750477"}, {"name": "Zhirui Hu", "authorId": "48430746"}, {"name": "Xinyan Wang", "authorId": "2115554077"}, {"name": "Cong Wang", "authorId": "50097174"}], "n_citations": 25}, "snippets": ["Another approach of unsupervised extract-based summarization (Wang et al., 2008)(Li, 2005) uses graph-based model. TextRank (Park et al., 2007) and LexRank (Dhillon, 2001) are first two graph-based models applied in text summarization, which use the PageRank-like algorithms to mark sentences. Then, other researchers have integrated the statistical and linguistic features to drive the sentence selection process, for example, the sentence position (Katragadda et al., 2009), term frequency (Lin et al., 1997), topic signature (Lin et al., 2000), lexical chains (Barzilay et al., 1997), and syntactic patterns (Baralis et al., 2013)(Haggag, 2013). Ko and Seo (Ko et al., 2008) composed two sentences nearby into a bigram. Those bigrams were supposed to be context information. First, they extracted the bigrams by using the sentence extraction model. Then they used another extraction module to extract sentences from them. The ClusterCMRW and ClusterHITS models calculated the sentences scores by considering the clusterlevel information in the graph-based ranking algorithm. Canhasi and Kononenko (Canhasi et al., 2013) improve matrix decomposition by employing the archetypal analysis for generic multidocument summarization. While coming to the document set, there must be more consideration about the document-level influence. But it did not consider the relationship between words and sentences. The DsR model (Wei et al., 2010) achieved it by using document-sensitive graph-based ranking model. But this method did not get a satisfied result. Yin et al. improved the summarization quality by adding extra information which came from the query-extraction scenario. Goyal et al. (Goyal et al., 2013) take Bernoulli model of randomness to index weights of sentences taking the context into consideration. The method proposed in [28] decomposed sentences by semantic role analysis, but while building the model, it did not use graphbased algorithms."], "score": 0.9091796875}], "table": null}, {"title": "Recent Neural Graph-Based Models", "tldr": "Neural graph-based models have revolutionized multi-document summarization by integrating graph representations with neural networks. These approaches leverage both traditional graph structures and neural architectures to better capture document relationships, with innovations in graph construction, node representations, and attention mechanisms driving significant performance improvements. (16 sources)", "text": "\nThe integration of neural networks with graph-based representations has marked a significant evolution in multi-document summarization approaches. This fusion combines the structural advantages of graphs with the representation power of deep neural networks, enabling more sophisticated modeling of cross-document relationships and content importance.\n\nA pioneering contribution in this domain was made by Yasunaga et al., who proposed a neural multi-document summarization system incorporating sentence relation graphs. Their approach employs Graph Convolutional Networks (GCNs) on relation graphs, with sentence embeddings from Recurrent Neural Networks as node features. Through layer-wise propagation, the GCN generates high-level sentence features for salience estimation, demonstrating advantages over traditional graph-based extractive approaches <Paper corpusId=\"6532096\" paperTitle=\"(Yasunaga et al., 2017)\" isShortName></Paper>.\n\nBuilding on this foundation, subsequent neural graph-based models have explored diverse architectures and graph representations. Liu et al. highlighted the importance of cross-document links in identifying salient information and eliminating redundancy. Their approach utilized graph convolutional networks over relation graphs to recursively compute the importance of passages based on the entire document collection <Paper corpusId=\"170079112\" paperTitle=\"(Liu et al., 2019)\" isShortName></Paper>.\n\nThe construction of knowledge graphs has emerged as a prominent strategy in neural graph-based summarization. Zhou et al. presented an entity-aware model (EMSum) that constructs a knowledge graph with text units and entities as nodes, using co-reference resolution tools to extract semantic entities <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>. Similarly, Li et al. proposed a graph-informed self-attention mechanism that effectively combines graph structures with abstractive multi-document summarization <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nPasunuru et al. developed an efficient graph-enhanced approach (BART-Long-Graph) that leverages knowledge graphs constructed using natural language processing tools to capture multi-level semantic information within documents. Their approach achieved significant improvements on benchmark datasets, demonstrating that graph encodings lead to summaries that are more abstractive, informative, and factually consistent <Paper corpusId=\"235097309\" paperTitle=\"(Pasunuru et al., 2021)\" isShortName></Paper> <Paper corpusId=\"269762702\" paperTitle=\"(Qu, 2024)\" isShortName></Paper>.\n\nHeterogeneous graph structures have been introduced to capture different types of information in documents. Wang et al. proposed a heterogeneous graph-based neural network (HDSG) that includes semantic nodes of different granularity levels to enrich cross-sentence relations <Paper corpusId=\"216552978\" paperTitle=\"(Wang et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. This approach addresses the limitations of traditional models that only consider sentence-to-sentence relationships.\n\nRecent advances have also explored the integration of pre-trained language models with graph-based approaches. Wu et al. presented BASS, a framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases across context and conveys rich relations between phrases <Paper corpusId=\"235187330\" paperTitle=\"(Wu et al., 2021)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>.\n\nGraph-based neural approaches have demonstrated versatility in handling various summarization scenarios. Chen et al. formulated multi-document summarization as a sub-graph selection problem (SgSum), selecting a salient sub-graph from a relation graph instead of individual sentences <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. This approach captures both intra and cross-document relations while producing more coherent summaries.\n\nThe incorporation of Abstract Meaning Representation (AMR) into neural graph-based models has further enhanced summarization performance. Ribeiro and others have shown that AMR structures benefit both document representation and summary generation, with the potential to improve controllability in summarization <Paper corpusId=\"264614849\" paperTitle=\"(Ribeiro, 2022)\" isShortName></Paper>.\n\nNeural graph-based models continue to evolve with specialized applications for scientific literature summarization. Chen et al. developed a target-aware related work generator that models relationships between reference papers and target papers with target-centered attention mechanisms <Paper corpusId=\"249097545\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Similarly, Wang et al. presented KGSum, a model centered on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThese neural graph-based models have consistently demonstrated superior performance compared to traditional graph-based methods like LexRank and TextRank, as well as other neural approaches without graph structures <Paper corpusId=\"232404911\" paperTitle=\"(Zhang et al., 2021)\" isShortName></Paper> <Paper corpusId=\"219036690\" paperTitle=\"(Xu et al., 2019)\" isShortName></Paper> <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. The combination of structured graph representations with the learning capabilities of neural networks has proven particularly effective for capturing complex document relationships and generating coherent, informative summaries from multiple sources.", "citations": [{"id": "(Yasunaga et al., 2017)", "paper": {"corpus_id": 6532096, "title": "Graph-based Neural Multi-Document Summarization", "year": 2017, "venue": "Conference on Computational Natural Language Learning", "authors": [{"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Rui Zhang", "authorId": "144142360"}, {"name": "Kshitijh Meelu", "authorId": "19266011"}, {"name": "Ayush Pareek", "authorId": "18081101"}, {"name": "K. Srinivasan", "authorId": "2093939303"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 258}, "snippets": ["We propose a neural multi-document summarization system that incorporates sentence relation graphs. We employ a Graph Convolutional Network (GCN) on the relation graphs, with sentence embeddings obtained from Recurrent Neural Networks as input node features. Through multiple layer-wise propagation, the GCN generates high-level hidden sentence features for salience estimation. We then use a greedy heuristic to extract salient sentences that avoid redundancy. In our experiments on DUC 2004, we consider three types of sentence relation graphs and demonstrate the advantage of combining sentence relations in graphs with the representation power of deep neural networks. Our model improves upon other traditional graph-based extractive approaches and the vanilla GRU sequence model with no graph, and it achieves competitive results against other state-of-the-art multi-document summarization systems."], "score": 0.92041015625}, {"id": "(Liu et al., 2019)", "paper": {"corpus_id": 170079112, "title": "Hierarchical Transformers for Multi-Document Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yang Liu", "authorId": "39798499"}, {"name": "Mirella Lapata", "authorId": "1747893"}], "n_citations": 298}, "snippets": ["The realization that cross-document links are important in isolating salient information, eliminating redundancy, and creating overall coherent summaries, has led to the widespread adoption of graph-based models for multi-document summarization (Erkan and Radev, 2004;Christensen et al., 2013;Wan, 2008;Parveen and Strube, 2014). Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them.\n\nMost previous multi-document summarization methods are extractive operating over graph-based representations of sentences or passages. Approaches vary depending on how edge weights are computed e.g., based on cosine similarity with tf-idf weights for words (Erkan and Radev, 2004) or on discourse relations (Christensen et al., 2013), and the specific algorithm adopted for ranking text units for inclusion in the final summary. Several variants of the PageRank algorithm have been adopted in the literature (Erkan and Radev, 2004) in order to compute the importance or salience of a passage recursively based on the entire graph. More recently, Yasunaga et al. (2017) propose a neural version of this framework, where salience is estimated using features extracted from sentence embeddings and graph convolutional networks (Kipf and Welling, 2017) applied over the relation graph representing cross-document links."], "score": 0.9169921875}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."], "score": 0.97119140625}, {"id": "(Qu, 2024)", "paper": {"corpus_id": 269762702, "title": "Leveraging Knowledge-aware Methodologies for Multi-document Summarization", "year": 2024, "venue": "The Web Conference", "authors": [{"name": "Yutong Qu", "authorId": "2163451228"}], "n_citations": 0}, "snippets": ["Zhou et al. (Zhou et al., 2021) presented an entity-aware model for abstractive multi-document summarization, called EMSum, augmenting the classical Transformer-based encoder with a knowledge graph consisting of text units and entities as nodes while utilizing Graph Attention Networks (GAT).Relying on this design, EMSum allows to capture the cross-document information and identify relative information among documents, significantly benefiting the multi-document summarization task.Specifically, the utilized knowledge graph is constructed by extracted semantic entities by the co-reference resolution tool from AllenNLP", "Pasunuru et al. (Pasunuru et al., 2021) presented an efficient graph-enhanced approach denoted as BART-Long-Graph for the multi-document summarization task that achieved remarkable results on benchmark multi-document summarization datasets, Multi-News (Fabbri et al., 2019) and DUC-2004.This summarizer is based on the pre-trained BART Seq2Seq Transformer-based model (Lewis et al., 2019) with an integration of a Longformer, containing both the local and global attention mechanisms, for encoding long texts.Additionally, it leveraged a knowledge graph by linearizing and encoding the graphical information within a separate graph encoder.To construct the semantic knowledge graph, Pasunuru et al. (Pasunuru et al., 2021) utilized AllenNLP at the document level and OpenIE at the sentence level to capture the multi-level semantic information within documents, with more informativeness and factually consistent features."], "score": 0.88330078125}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."], "score": 0.962890625}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 239768911, "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Moye Chen", "authorId": "2108267116"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 22}, "snippets": ["The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS", ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."], "score": 0.9423828125}, {"id": "(Pasunuru et al., 2021)", "paper": {"corpus_id": 235097309, "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ramakanth Pasunuru", "authorId": "10721120"}, {"name": "Mengwen Liu", "authorId": "2940333"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "Sujith Ravi", "authorId": "120209444"}, {"name": "Markus Dreyer", "authorId": "40262269"}], "n_citations": 73}, "snippets": ["Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani et al., 1997)(Radev et al., 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures -perhaps influenced by the link structure of the WWW itself. (Mani et al., 1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and (Radev et al., 1998) summarized multiple documents by mapping them to abstract template representations, then generating text from the templates. \n\nIn the early 2000s, datasets from the Document Understanding Conference (DUC), which included human-written summaries for multi-document clusters, sparked increased research interest. In LexRank, (Erkan et al., 2004) extracted the most salient sentences from a multi-document cluster by constructing a graph representing pairwise sentence similarities and running a PageRank algorithm on the graph. Subsequent approaches followed the same paradigm while improving diversity of the extracted sentences (Wan and Yang, 2006) or adding document-level information into the graph (Wan, 2008). (Dasgupta et al., 2013) incorporated dependency graph features into their sentence relation graphs. (Baralis et al., 2013) built graphs over sets of terms, rather than sentences. (Li et al., 2016) built a graph over event mentions and their relationships, in order to summarize news events using sentence extraction techniques. (Liu et al., 2018) and (Liao et al., 2018) leveraged AMR formalism to convert source text into AMR graphs and then generate a summary using these graphs. \n\nMore recently, the introduction of larger datasets for MDS has enabled researchers to train neural models for multi-document summarization."], "score": 0.9326171875}, {"id": "(Wang et al., 2020)", "paper": {"corpus_id": 216552978, "title": "Heterogeneous Graph Neural Networks for Extractive Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Danqing Wang", "authorId": "49371126"}, {"name": "Pengfei Liu", "authorId": "144118452"}, {"name": "Y. Zheng", "authorId": "3337238"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Xuanjing Huang", "authorId": "1790227"}], "n_citations": 282}, "snippets": ["As a crucial step in extractive document summarization, learning cross-sentence relations has been explored by a plethora of approaches. An intuitive way is to put them in the graph-based neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (HETERSUMGRAPH), which contains semantic nodes of different granularity levels apart from sentences. These additional nodes act as the intermediary between sentences and enrich the cross-sentence relations. Besides, our graph structure is flexible in natural extension from a single-document setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 252185277, "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Kunyuan Pang", "authorId": "2116489"}, {"name": "Liangliang He", "authorId": "50670961"}, {"name": "Dong Li", "authorId": "2108821455"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "38972135"}], "n_citations": 15}, "snippets": ["HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."], "score": 0.88427734375}, {"id": "(Wu et al., 2021)", "paper": {"corpus_id": 235187330, "title": "BASS: Boosting Abstractive Summarization with Unified Semantic Graph", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wenhao Wu", "authorId": "2139644141"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Ziqiang Cao", "authorId": "2314396"}, {"name": "Sujian Li", "authorId": "48831399"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 45}, "snippets": ["Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."], "score": 0.0}, {"id": "(Ribeiro, 2022)", "paper": {"corpus_id": 264614849, "title": "FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Leonardo F. R. Ribeiro", "authorId": "10430740"}], "n_citations": 60}, "snippets": ["A growing body of work focuses on using graph-based representations for improving summarization. Whereas different approaches encode graphs into neural models for multi-document summarization (Fan et al., 2019;Li et al., 2020;Pasunuru et al., 2021;Chen et al., 2021), AMR structures have been shown to benefit both document representation and summary generation (Liu et al., 2015;Liao et al., 2018;Hardy and Vlachos, 2018;Dohare et al., 2018) and have the potential of improving controllability in summarization."], "score": 0.91015625}, {"id": "(Chen et al., 2022)", "paper": {"corpus_id": 249097545, "title": "Target-aware Abstractive Related Work Generation with Contrastive Learning", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Xiuying Chen", "authorId": "46772896"}, {"name": "Hind Alamro", "authorId": "1753416672"}, {"name": "Li Mingzhe", "authorId": "9255404"}, {"name": "Shen Gao", "authorId": "2112311595"}, {"name": "Rui Yan", "authorId": "2055864368"}, {"name": "Xin Gao", "authorId": "2118502950"}, {"name": "Xiangliang Zhang", "authorId": "2928371"}], "n_citations": 29}, "snippets": ["Our extractive baselines include: LexRank (Erkan et al., 2004): a graph-based extractive model that computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences", "EMS (Zhou et al., 2021): an entity-aware model for abstractive multi-document summarization with BERT encoder. It augments the classical Transformer based encoder-decoder framework with a graph consisting of paragraph nodes and entity nodes", "RRG (Chen et al., 2021): an abstractive related work generator based on reference papers. It augments the RNN encoder with a relation-aware graph between multiple references. The relation graph and the document representation are refined iteratively."], "score": 0.88720703125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269157041, "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Dong Li", "authorId": "2214849368"}, {"name": "Kehan Long", "authorId": "2186557380"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "2257363292"}], "n_citations": 2}, "snippets": ["According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template."], "score": 0.8935546875}, {"id": "(Zhang et al., 2021)", "paper": {"corpus_id": 232404911, "title": "Centrality Meets Centroid: A Graph-based Approach for Unsupervised Document Summarization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Haopeng Zhang", "authorId": "2135688409"}, {"name": "Jiawei Zhang", "authorId": "1718428"}], "n_citations": 0}, "snippets": ["Unsupervised graph summarization methods rely on graph connectivity (centrality) or node representations (centroid) (Radev et al., 2004;(Rossiello et al., 2017) to score and rank sentences. Popular centrality-based methods include TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and PACSUM (Zheng and Lapata, 2019). Details of centrality-based and centroid-based summarization will be discussed in Section 3. \n\nResearchers also explore supervised graph neural network (GNN) on the task (Yasunaga et al., 2017;Xu et al., 2019;Wang et al., 2020). Our paper follows this line of works on developing novel unsupervised graph models for single document summarization."], "score": 0.9052734375}, {"id": "(Xu et al., 2019)", "paper": {"corpus_id": 219036690, "title": "Discourse-Aware Neural Extractive Text Summarization", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jiacheng Xu", "authorId": "34837371"}, {"name": "Zhe Gan", "authorId": "144702900"}, {"name": "Yu Cheng", "authorId": "145215470"}, {"name": "Jingjing Liu", "authorId": "46700348"}], "n_citations": 282}, "snippets": ["Graph-based Summarization Graph approach has been explored in text summarization over decades. LexRank introduced a stochastic graphbased method for computing relative importance of textual units (Erkan et al., 2004). (Yasunaga et al., 2017) employed a GCN on the relation graphs with sentence embeddings obtained from RNN. (Tan et al., 2017) also proposed graphbased attention in abstractive summarization model. Fernandes et al. (2018) developed a framework to reason long-distance relationships for text summarization."], "score": 0.93017578125}, {"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."], "score": 0.87255859375}], "table": null}, {"title": "Applications and Effectiveness of Graph-Based Approaches", "tldr": "Graph-based approaches have been successfully applied across various summarization domains including news, scientific literature, and biomedical text, consistently outperforming traditional extractive methods. Their effectiveness stems from the ability to capture complex document relationships, reduce redundancy, and improve summary coherence, with research confirming their superiority in both traditional and neural implementations. (17 sources)", "text": "\nGraph-based approaches to multi-document summarization have demonstrated remarkable effectiveness across various domains and applications. Early graph-based methods like LexRank and TextRank revolutionized the field by showing significant improvements over previous approaches, with LexRank ranking first in several Document Understanding Conference (DUC) 2004 evaluation tasks <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper>. This success established graph-based methods as a standard approach for multi-document summarization, particularly in news domains where capturing cross-document relationships is crucial.\n\nThe effectiveness of graph-based summarization extends to specialized domains as well. In biomedical text summarization, concept graph modeling has proven particularly successful, with approaches like those proposed by Plaza et al. adapting graph-based methods to model biomedical-specific concept graphs <Paper corpusId=\"5674504\" paperTitle=\"(Plaza et al., 2008)\" isShortName></Paper> <Paper corpusId=\"145993667\" paperTitle=\"(Mohamed et al., 2019)\" isShortName></Paper>. These domain-specific adaptations demonstrate the flexibility of graph-based approaches in handling specialized terminology and relationships.\n\nGraph-based approaches have consistently outperformed other extractive methods in comparative evaluations. Multiple studies have confirmed that degree-based methods, including LexRank with threshold, outperform both centroid-based methods and other systems participating in standardized evaluations <Paper corpusId=\"506350\" paperTitle=\"(Erkan et al., 2004)\" isShortName></Paper> <Paper corpusId=\"59599960\" paperTitle=\"(Lierde et al._1, 2019)\" isShortName></Paper>. This superiority stems from their ability to capture the structural and semantic relationships between sentences across multiple documents, effectively identifying the most central and important content.\n\nThe effectiveness of graph-based methods has been further enhanced through various improvements to the basic framework. Wan and Yang developed a more sophisticated approach that assigns different weights to intra-document and inter-document links, giving higher priority to sentences with significant inter-document connections <Paper corpusId=\"5457260\" paperTitle=\"(Wan et al., 2006)\" isShortName></Paper> <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>. Other researchers have explored incorporating document-level information into the graph structure, considering the influence of global information from document clusters on local sentence evaluation <Paper corpusId=\"33696521\" paperTitle=\"(Kumar et al., 2016)\" isShortName></Paper>.\n\nWith the integration of neural networks, graph-based approaches have reached new levels of effectiveness. Neural graph-based models have consistently demonstrated superior performance compared to traditional graph-based methods like LexRank and TextRank, as well as other neural approaches without graph structures <Paper corpusId=\"235187219\" paperTitle=\"(Hickmann et al., 2021)\" isShortName></Paper>. The GraphSum model, for example, showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank <Paper corpusId=\"235187219\" paperTitle=\"(Hickmann et al., 2021)\" isShortName></Paper>.\n\nRecent studies have confirmed that graphs provide significant benefits to multi-document summarization by helping generate more informative, concise, and coherent summaries <Paper corpusId=\"218718706\" paperTitle=\"(Li et al., 2020)\" isShortName></Paper> <Paper corpusId=\"259262067\" paperTitle=\"(Nguyen et al., 2023)\" isShortName></Paper>. The ability of graphs to capture relations between textual units has proven especially valuable for detecting salient information from multiple documents and generating overall coherent summaries <Paper corpusId=\"236478143\" paperTitle=\"(Zhou et al., 2021)\" isShortName></Paper>.\n\nThe evolution of graph-based summarization techniques has followed the broader progression of summarization approaches. Sankar et al. identified four primary approaches in multi-document summarization: graph ranking-based extractive methods (like TextRank and LexRank), syntax and structure-based compression methods, paraphrasing-based approaches, and neural sequence-to-sequence abstractive methods <Paper corpusId=\"248571519\" paperTitle=\"(Sankar et al., 2022)\" isShortName></Paper>. Graph-based approaches have remained relevant throughout this evolution, with researchers continuing to find ways to integrate graph structures with newer neural architectures.\n\nRecent research has emphasized the complementary strengths of graph-based extractive methods and neural sequence-to-sequence architectures. Graph-based extractive methods excel at identifying relationships between sentences or documents and extracting salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>. This recognition has led to the development of hybrid approaches that leverage both paradigms, with abstractive models based on graphs gaining significant attention <Paper corpusId=\"250374734\" paperTitle=\"(Chen et al._1, 2022)\" isShortName></Paper>.\n\nIn the domain of scientific literature summarization, graph-based approaches have been particularly effective. Wang et al. developed KGSum, a model centered on knowledge graphs during both encoding and decoding processes for multi-document scientific summarization, achieving state-of-the-art performance <Paper corpusId=\"252185277\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper> <Paper corpusId=\"273963834\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>. Other researchers have explored hierarchical graph-based methods for related work generation in scientific papers <Paper corpusId=\"269157041\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe widespread adoption of graph-based summarization methods across various applications and domains demonstrates their versatility and effectiveness. From news summarization to scientific literature and biomedical texts, graph-based approaches have consistently proven their ability to capture document relationships, reduce redundancy, and improve summary coherence <Paper corpusId=\"264146402\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"239768911\" paperTitle=\"(Chen et al., 2021)\" isShortName></Paper>. This versatility, combined with the integration of neural techniques, ensures that graph-based approaches will continue to play a central role in multi-document summarization research and applications.", "citations": [{"id": "(Erkan et al., 2004)", "paper": {"corpus_id": 506350, "title": "LexRank: Graph-based Lexical Centrality as Salience in Text Summarization", "year": 2004, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "G\u00fcnes Erkan", "authorId": "2158159"}, {"name": "Dragomir R. Radev", "authorId": "9215251"}], "n_citations": 3097}, "snippets": ["We introduce a stochastic graph-based method for computing relative importance of textual units for Natural Language Processing. We test the technique on the problem of Text Summarization (TS). Extractive TS relies on the concept of sentence salience to identify the most important sentences in a document or set of documents. Salience is typically defined in terms of the presence of particular important words or in terms of similarity to a centroid pseudo-sentence. We consider a new approach, LexRank, for computing sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences."], "score": 0.87255859375}, {"id": "(Plaza et al., 2008)", "paper": {"corpus_id": 5674504, "title": "Concept-Graph Based Biomedical Automatic Summarization Using Ontologies", "year": 2008, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Laura Plaza", "authorId": "145377713"}, {"name": "Alberto D\u00edaz", "authorId": "102639698"}, {"name": "Pablo Gerv\u00e1s", "authorId": "1798564"}], "n_citations": 53}, "snippets": ["One of the main problems in research on automatic summarization is the inaccurate semantic interpretation of the source. Using specific domain knowledge can considerably alleviate the problem. In this paper, we introduce an ontology-based extractive method for summarization. It is based on mapping the text to concepts and representing the document and its sentences as graphs. We have applied our approach to summarize biomedical literature, taking advantages of free resources as UMLS. Preliminary empirical results are presented and pending problems are identified."], "score": 0.0}, {"id": "(Mohamed et al., 2019)", "paper": {"corpus_id": 145993667, "title": "SRL-ESA-TextSum: A text summarization approach based on semantic role labeling and explicit semantic analysis", "year": 2019, "venue": "Information Processing & Management", "authors": [{"name": "Muhidin A. Mohamed", "authorId": "47302320"}, {"name": "M. Oussalah", "authorId": "1714030"}], "n_citations": 101}, "snippets": ["In addition, graph-based representations are some of the most prevalent text analysis methods and have shown their effectiveness for text summarization (Erkan et al., 2004)15,(Canhasi, 2011)(Wei et al., 2010)(Wan, 2010)21,(Azadani et al., 2018)[23]. The conventional way of graph-based summarization uses document sentences as vertices, known as sentence-based document graphs. Erkan and Radev (Erkan et al., 2004) proposed one of the most popular sentence-based graph representations for summarisation. Their LexRank algorithm is based on the eigenvector centrality concept. Similarly, Mihalcea and Tarau [15] presented TextRank, another graph-based ranking method constructed using content overlap. Both LexRank and TextRank are derivatives of the seminal PageRank algorithm [16]. Some graph-based approaches have cross-linked different levels of text granularities particularly tailored for multi-document summarization (Canhasi, 2011)(Wei et al., 2010)(Wan, 2010). In this way, Canhasi and Kononenko [21] used three-layer graph representation consisting of terms, sentences, and document vertices, and linked them via term-sentence and sentence-document links on top of the conventional sentence similarity graphs. In addition, the works of Wei et al. (Wei et al., 2010) and Wan (Wan, 2010) considered the influence of global information from the document clusters on local sentence evaluation. Contrary to the traditional way of representing source text units, concept graphs have also been emerging as alternative graph representation of the source texts (Plaza et al., 2008)25,(Lloret et al., 2011)(Azadani et al., 2018). Of this, Azadani et al. (Azadani et al., 2018) and Plaza et al. (Plaza et al., 2008) have both adapted such a method by modelling a biomedical summarization algorithm on concept graphs. Also, Zhuge [26] proposed a multi-dimensional summarization methodology to summarize various objects (including texts, pictures and videos) from multiple dimensions. The effectiveness of this methodology has been evaluated for text summarization [27]. Concept graph modelling proved some success particularly in domain-specific areas such as biomedical and news summarizations [28]."], "score": 0.94140625}, {"id": "(Lierde et al._1, 2019)", "paper": {"corpus_id": 59599960, "title": "Query-oriented text summarization based on hypergraph transversals", "year": 2019, "venue": "Information Processing & Management", "authors": [{"name": "H. V. Lierde", "authorId": "29364954"}, {"name": "T. Chow", "authorId": "144134805"}], "n_citations": 55}, "snippets": ["Extractive summarizers differ in the method used to identify relevant sentences, which leads to a classification of models as either feature-based or graph-based approaches", "Since feature-based approaches generally require datasets with labelled sentences which are hard to produce [12], unsupervised graph-based methods have attracted growing interest in recent years.\n\nGraph-based summarizers represent the sentences of a corpus as the nodes of a graph with the edges modelling relationships of similarity between the sentences [1]. Then, graph-based algorithms are applied to identify relevant sentences. The models generally differ in the type of relationship captured by the graph or in the sentence selection approach. Most graph-based models define the edges connecting sentences based on the co-occurrence of terms in pairs of sentences [1,3,4]. Then, important sentences are identified either based on node ranking algorithms, or using a global optimization approach. Methods based on node ranking compute individual relevance scores for the sentences and build summaries with highly scored sentences. The earliest such summarizer, LexRank [1], applies the PageRank algorithm to compute sentence scores."], "score": 0.87890625}, {"id": "(Wan et al., 2006)", "paper": {"corpus_id": 5457260, "title": "Improved Affinity Graph Based Multi-Document Summarization", "year": 2006, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Xiaojun Wan", "authorId": "145078589"}, {"name": "Jianwu Yang", "authorId": "1743923"}], "n_citations": 109}, "snippets": ["Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani et al., 1997) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization."], "score": 0.9384765625}, {"id": "(Kumar et al., 2016)", "paper": {"corpus_id": 33696521, "title": "A Review on Automatic Text Summarization Approaches", "year": 2016, "venue": "Journal of Computer Science", "authors": [{"name": "Y. J. Kumar", "authorId": "1734844"}, {"name": "O. Goh", "authorId": "2445567"}, {"name": "H. Basiron", "authorId": "2136088"}, {"name": "Ngo Hea Choon", "authorId": "3388053"}, {"name": "Puspalata C. Suppiah", "authorId": "21154901"}], "n_citations": 72}, "snippets": ["This approach differs from the cluster based approach where sentences are ranked based on its closeness to cluster centroid. Two well-known graph based ranking algorithms is the HITS algorithm (Kleinberg, 1999) and the Google's PageRank (Brin and Page, 2012). Both methods have been traditionally used in Web-link analysis and social networks. Lexrank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004) are two successful graph-based ranking systems that implement these algorithms.\n\nFurther studies have been carried to make improvement through modification in the ranking algorithm. Wan and Yang (2006) assigned different weights to intra-document links and inter-document links. \n\nThey gave more priority to sentence with high interdocument links. In later work by Hariharan and Srinivasan (2009), they approached the graph based method differently i.e., by discounting the already selected sentence by removing it from further consideration when they rank the remaining sentences in the document. \n\nApart from sentence level information, Wan (2008) and Wei et al. (2010) devised a document-sensitive graph model to explore document impact on the graphbased summarization, by incorporating both the document-level information and the sentence-to-document relationship in the graph-based ranking process."], "score": 0.90771484375}, {"id": "(Hickmann et al., 2021)", "paper": {"corpus_id": 235187219, "title": "Analysis of GraphSum\u2019s Attention Weights to Improve the Explainability of Multi-Document Summarization", "year": 2021, "venue": "International Conference on Information Integration and Web-based Applications & Services", "authors": [{"name": "M. Hickmann", "authorId": "1938939"}, {"name": "Fabian Wurzberger", "authorId": "2105164214"}, {"name": "Megi Hoxhalli", "authorId": "2105165105"}, {"name": "Arne Lochner", "authorId": "2105164598"}, {"name": "Jessica T\u00f6llich", "authorId": "2025314313"}, {"name": "A. Scherp", "authorId": "1753135"}], "n_citations": 1}, "snippets": ["Graph-based approaches on graph neural networks and specifically using knowledge graphs have gained popularity [8,12,20,21] while recent models combine graph neural networks with a transformer architecture [8,12].\n\nFor extractive MDS, a graph-based representation can help to detect salient vertices, so that they are included in the generated summary to cover all important information. In abstractive MDS, salient vertices can guide the summarization process via attention mechanisms of the transformer architecture.\n\nDifferent textual units have been used for graph-based text summarization in the past, as discourse relations [8,20], sentences [3,21], or paragraphs [12].\n\nThe GraphSum model showed substantial improvements compared to strong MDS baselines such as LEAD-3 and LexRank [5]."], "score": 0.96337890625}, {"id": "(Li et al., 2020)", "paper": {"corpus_id": 218718706, "title": "Leveraging Graph to Improve Abstractive Multi-Document Summarization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "48624966"}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}, {"name": "Junping Du", "authorId": "2117218629"}], "n_citations": 136}, "snippets": ["Graphs that capture relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models.\n\nMost previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009;Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences."], "score": 0.962890625}, {"id": "(Nguyen et al., 2023)", "paper": {"corpus_id": 259262067, "title": "Vietnamese multi-document summary using subgraph selection approach - VLSP 2022 AbMuSu Shared Task", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Huu-Thin Nguyen", "authorId": "2220690681"}, {"name": "Tam Doan Thanh", "authorId": "2101316144"}, {"name": "Cam-Van Thi Nguyen", "authorId": "2220630783"}], "n_citations": 0}, "snippets": ["Graphs capturing relationships between textual units are of great benefit to MDS, which can help create more concise, informative, and coherent summaries from multiple sources documents. Furthermore, graphs can be easily constructed by representing sentences or paragraph as graph nodes and edges. LexRank (Erkan et al., 2004)) computes sentence importance based on a lexical similarity graph of sentences. Graph representations of documents such as discourse graph based on discourse relations (Christensen et al., 2013)", "In this paper, we approach by switching back to the graph-based extractive problem, specifically, extracting summaries for multi-document by selecting sub-graphs based on the constructed graph. Inspired by study (Chen et al., 2021), we exploit more edge weights representing the relationship of sentences in the same document and train and improve the model for Vietnamese dataset."], "score": 0.90478515625}, {"id": "(Zhou et al., 2021)", "paper": {"corpus_id": 236478143, "title": "Entity-Aware Abstractive Multi-Document Summarization", "year": 2021, "venue": "Findings", "authors": [{"name": "Hao Zhou", "authorId": null}, {"name": "Weidong Ren", "authorId": "2053308860"}, {"name": "Gongshen Liu", "authorId": "150112803"}, {"name": "Bo Su", "authorId": "153253583"}, {"name": "Wei Lu", "authorId": "143844110"}], "n_citations": 28}, "snippets": ["Graph-based methods have long been utilized for extractive summarization. Text units on graphs are ranked and selected as the most salient ones to be included in the summary. LexRank (Erkan et al., 2004) computes sentence salience based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Wan, 2008) further incorporate the document-level information and the sentence-to-document relationship into the graph-based ranking process. Christensen et al. (2013) build multi-document graphs to approximate the discourse relations across sentences based on indicators including discourse cues, deverbal nouns, co-reference and more. \n\nFor recent methods based on graph neural networks, (Tan et al., 2017) propose a graph-based attention mechanism to identify salient sentences. (Yasunaga et al., 2017) construct an approximate discourse graph based on discourse markers and entity links, then apply graph convolutional networks over the relation graph. (Fan et al., 2019) construct a local knowledge graph, which is then linearized into a structured input sequence so that models can encode within the sequence-to-sequence setting. (huang et al., 2020) further design a graph encoder, which improves upon graph attention networks, to maintain the global context and local entities complementing each other. (Li et al., 2020) utilize homogeneous graphs to capture cross-document relations and guide the summary generation process. However, (Wang et al., 2020) are the first to introduce different granularity levels of text nodes to construct heterogeneous graphs for extractive summarization."], "score": 0.97119140625}, {"id": "(Sankar et al., 2022)", "paper": {"corpus_id": 248571519, "title": "ACM - Attribute Conditioning for Abstractive Multi Document Summarization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Aiswarya Sankar", "authorId": "2064325789"}, {"name": "Ankit R. Chadha", "authorId": "145934595"}], "n_citations": 0}, "snippets": ["Multi document summarization has evolved through four primary approaches since the task was first introduced. The first set of approaches focused on graph ranking based extractive methods through TextRank (Mihalcea et al., 2004), LexRank (Erkan et al., 2004) and others. These approaches came before syntax and structure based compression methods which aimed to tackle issues of information redundancy and paraphrasing between multiple documents. Compression-based methods as shown in (Li et al., 2014) and paraphrasing based were improved upon with the advent of neural seq2seq based abstractive methods in 2017. This allowed multi document summarization to further improve upon the work done with single document abstractive summarization through approaches such as pointer generator-maximal marignal relevance (Lebanoff et al., 2018), T-DMCA (Liu et al., 2018) the paper that also introduced the foundational WikiSum dataset and HierMMR (Fabbri et al., 2019) that introduced MultiNews. These approaches aimed to tackle information compression through maximal marginal relevance scores across documents and through attention based mechanisms. Improvements upon those baseline models include further leveraging graph based approaches to pre-synthesize dependencies between the articles prior to multi document summarization as tackled in (Li et al., 2020). Further work needs to be done to further exploit these graphical representations as (Li et al., 2020) essentially works to establish baselines with tf-idf, cosine similarity and a graphical representation first described in (Christensen et al., 2013)."], "score": 0.9169921875}, {"id": "(Chen et al._1, 2022)", "paper": {"corpus_id": 250374734, "title": "Two-phase Multi-document Event Summarization on Core Event Graphs", "year": 2022, "venue": "Journal of Artificial Intelligence Research", "authors": [{"name": "Zengjian Chen", "authorId": "48354529"}, {"name": "Jin Xu", "authorId": "2116315442"}, {"name": "M. Liao", "authorId": "145865588"}, {"name": "Tong Xue", "authorId": "2138967543"}, {"name": "Kun He", "authorId": "2190820519"}], "n_citations": 2}, "snippets": ["Existing multi-document summarization methods mainly focus on sentence-level summarization and can be categorized into extractive and abstractive methods. Most extractive methods are operated over graph-based representations of sentences or passages with edge weights computed by tf-idf (Erkan et al., 2004), discourse relations (Christensen et al., 2013) or sentence embeddings (Yasunaga et al., 2017b) and then a specific algorithm is further adopted for ranking text units for inclusion in the final summary. More recently, some extractive summarization works also utilize graph convolutional networks for salient sentences estimation (Kipf et al., 2016) and sentence ordering (Yin et al., 2019)", "Intuitively, graph-based extractive methods are suitable to identify relationship of different sentences or documents and extract salient information, while neural sequence-to-sequence architectures are effective in abstraction and content rewriting. Hence, abstractive models based on graphs gain much attention (Yasunaga et al., 2017b;Li et al., 2020)."], "score": 0.95556640625}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 252185277, "title": "Multi-Document Scientific Summarization from a Knowledge Graph-Centric View", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Kunyuan Pang", "authorId": "2116489"}, {"name": "Liangliang He", "authorId": "50670961"}, {"name": "Dong Li", "authorId": "2108821455"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "38972135"}], "n_citations": 15}, "snippets": ["HDSG (Wang et al., 2020) is a heterogeneous graph-based extractive model with semantic nodes of different granularity. HiMAP (Fabbri et al., 2019) expands the pointergenerator network (See et al., 2017) into a hierarchical network and integrates an MMR module. HierSumm (Liu and Lapata, 2019a) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. MGSum (Jin et al., 2020) is a multi-granularity interaction network for abstractive multi-document summarization. We also consider evaluating on single document summarization models by concatenating multiple papers into a long sequence. GraphSum (Li et al., 2020) is a neural multi-document summarization model that leverages well-known graphs to produce abstractive summaries. We use TF-IDF graph as the input graph. PEGASUS (Zhang et al., 2020) is a sequence-to-sequence model with gapsentences generation as a pre-training objective tailored for abstractive summarization. Pointer-Generator (See et al., 2017) is an RNN based model with an attention mechanism and allows the system to copy words from the source via pointing for abstractive summarization. BertABS (Liu and Lapata, 2019b) uses a pretrained BERT (Devlin et al., 2019) as the encoder for abstractive summarization. We also report the performance of BertABS with an encoder (SciBertABS) pretrained on scientific articles."], "score": 0.88427734375}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 273963834, "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation", "year": 2025, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Zhi Zhang", "authorId": "2116765029"}, {"name": "Yan Liu", "authorId": "2290114013"}, {"name": "Sheng-hua Zhong", "authorId": "2240541436"}, {"name": "Gong Chen", "authorId": "2265358574"}, {"name": "Yu Yang", "authorId": "2283200843"}, {"name": "Jiannong Cao", "authorId": "2312268442"}], "n_citations": 0}, "snippets": ["In this regard, LexRank (Erkan et al., 2004) and TextRank (Mihalcea and Tarau 2004) first introduced graphs to extractive text summarization in 2004. They compute sentence importance using a graph representation of sentences to extract salient textual units from documents as summarization. (Zou, 2020), Wang et al. (Wang et al. 2020) propose to extract salient textual units from documents as summarization using a heterogeneous graph consisting of semantic nodes at several granularity levels of documents. In 2022, Wang et al. (Wang et al., 2022)) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template to achieve state-of-the-art performance."], "score": 0.89453125}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 269157041, "title": "Disentangling Instructive Information from Ranked Multiple Candidates for Multi-Document Scientific Summarization", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Pancheng Wang", "authorId": "2073437"}, {"name": "Shasha Li", "authorId": "2145340498"}, {"name": "Dong Li", "authorId": "2214849368"}, {"name": "Kehan Long", "authorId": "2186557380"}, {"name": "Jintao Tang", "authorId": "1762106"}, {"name": "Ting Wang", "authorId": "2257363292"}], "n_citations": 2}, "snippets": ["According to the paradigm adopted, previous studies can be categorized into graph-based (Chen et al., 2022)(Wang et al., 2022), flat-based (Moro et al., 2022)(Shi et al., 2023), and hierarchical-based methods (Chen et al., 2021)[30].Concretely, graph-based methods construct external graphs (e.g., entity-relation graph, AMR, TF-IDF) to assist document representation and cross-document relation modeling.In this regard, Chen et al. (Chen et al., 2022) leverage graphs to encode source articles, constructing keyphrase graphs and using a multi-level contrastive learning strategy to improve the accuracy of generated summaries.\n\nWang et al. (Wang et al., 2022) incorporate knowledge graphs into document encoding and decoding, generating the summary from a knowledge graph template."], "score": 0.8935546875}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 264146402, "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review", "year": 2023, "venue": "Discret. Math. Algorithms Appl.", "authors": [{"name": "Guanghua Wang", "authorId": "2258949345"}, {"name": "Weili Wu", "authorId": "2258731583"}], "n_citations": 4}, "snippets": ["Traditional approaches usually employ extractive techniques both in the context of single-document and multi-document summarization. Graph-based methods can be applied to both single-document and multi-document summarization by representing the relationships between sentences in one document or several documents as a graph, with sentences as nodes and the edges as the similarity between the sentences. The systems (Erkan et al., 2004)(Lin et al., 2009)(Parveen et al., 2015)(Thakkar et al., 2010) then use algorithms like PageRank, HITS, or LexRank to identify the most important sentences in the graph, which are then extracted and combined to form the summary."], "score": 0.88330078125}, {"id": "(Chen et al., 2021)", "paper": {"corpus_id": 239768911, "title": "SgSum:Transforming Multi-document Summarization into Sub-graph Selection", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Moye Chen", "authorId": "2108267116"}, {"name": "Wei Li", "authorId": "48624966"}, {"name": "Jiachen Liu", "authorId": null}, {"name": "Xinyan Xiao", "authorId": "2107521158"}, {"name": "Hua Wu", "authorId": "40354707"}, {"name": "Haifeng Wang", "authorId": "144270731"}], "n_citations": 22}, "snippets": ["The graph structure is effective to model relations between sentences which is an essential point to select interrelated summary-worthy sentences in extractive summarization. (Erkan et al., 2004) utilize a similarity graph to construct an unsupervised summarization methods called LexRank. G-Flow (Christensen et al., 2013) and DISCOBERT (Xu et al., 2019) both use discourse graphs to generate concise and informative summaries. (Li et al., 2016) and (Li et al., 2021) propose to utilize event relation graph to represent documents for MDS", ".Most previous graph extractive MDS approaches aim to extract salient textual units from documents based on graph structure representations of sentences. (Erkan et al., 2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. (Christensen et al., 2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences. More recently, (Yasunaga et al., 2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. (Li et al., 2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. (Wu et al., 2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases."], "score": 0.9423828125}], "table": null}], "cost": 0.707268}}
