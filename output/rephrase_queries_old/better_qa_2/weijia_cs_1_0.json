{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in the technical methods for merging or integrating specialized language models into a single unified system, such as parameter-space merging or mixture-of-experts architectures?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in the technical methods for merging or integrating specialized language models into a single unified system, such as parameter-space merging, mixture-of-experts architectures, dynamic gating, or related approaches."}}, {"clarifying_question2": "Do you want to compare the empirical performance (accuracy, efficiency, generalization) of combined specialist systems versus large general-purpose language models, or are you more concerned with theoretical potential?", "clarifying_answer2": {"clarifying_answer": "I want to compare the empirical performance (accuracy, efficiency, generalization) of combined specialist systems versus large general-purpose language models."}}, {"clarifying_question3": "Are you seeking information about the practical challenges and solutions (like task interference or knowledge conflicts) when combining specialized models, and how these are addressed in recent literature?", "clarifying_answer3": {"clarifying_answer": "Yes, I am seeking information about the practical challenges\u2014such as task interference and knowledge conflicts\u2014encountered when combining specialized language models, and how recent research proposes to address those issues."}}], "better_query": "What are the current technical methods (such as parameter-space merging, mixture-of-experts architectures, dynamic gating) for merging or integrating multiple specialized language models into a single unified system, and how do these approaches address practical challenges like task interference and knowledge conflicts, as discussed in recent literature?", "better_answer": {"sections": [{"title": "Introduction to Model Merging and Integration", "tldr": "Model merging and integration techniques combine multiple specialized language models into unified systems that leverage their collective strengths. These approaches aim to create more capable AI systems that can handle diverse tasks without the computational costs of running multiple separate models. (LLM Memory)", "text": "\nModel merging and integration represent an emerging paradigm in language model development that seeks to combine the capabilities of multiple specialized models into unified systems. As language models continue to grow in size and specificity, researchers have recognized that different models often excel at different tasks or possess complementary knowledge. Rather than maintaining numerous separate models or training increasingly massive general-purpose models, merging techniques aim to create integrated systems that combine the strengths of multiple specialized models. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nAt its core, model merging addresses a fundamental challenge in AI development: the tension between specialized expertise and broad capabilities. Specialized models often perform exceptionally well on narrow tasks but fail to generalize, while general models may have broad capabilities but lack depth in specific domains. Integration techniques provide a middle path by allowing multiple specialized capabilities to coexist within a unified framework. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe field has developed several distinct approaches to model merging, including parameter-space merging (where model weights are directly combined), mixture-of-experts architectures (where specialized sub-models handle different inputs), and dynamic routing mechanisms (where inputs are directed to appropriate processing pathways). These techniques vary in their implementation complexity, computational requirements, and effectiveness across different applications. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent research demonstrates that properly integrated models can not only maintain the capabilities of their constituent parts but sometimes exhibit emergent abilities that weren't present in any individual model. This synergistic effect makes model merging particularly attractive as a path toward more capable AI systems without the computational and data requirements of training increasingly large models from scratch. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Parameter-Space Merging Methods", "tldr": "Parameter-space merging techniques directly combine weights from multiple models to create unified systems without requiring additional training. These methods range from simple weight averaging to sophisticated approaches that address parameter conflicts and redundancy. (8 sources)", "text": "\nParameter-space merging represents a direct approach to model integration that operates by combining the weights of multiple specialized models in weight space. These techniques enable the creation of unified models without the computational expense of retraining, making them increasingly popular for integrating specialized language models.\n\nThe simplest form of parameter-space merging is weight averaging, where parameters from different models are directly averaged. Model Soups <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper> demonstrated that averaging weights of models fine-tuned with different hyperparameter configurations can significantly improve accuracy and robustness without increasing inference costs. This approach works particularly well when models are trained on the same task but with different configurations.\n\nFor merging models trained on different tasks, Task Arithmetic <Paper corpusId=\"254408495\" paperTitle=\"(Ilharco et al., 2022)\" isShortName></Paper> introduced the concept of \"task vectors\" - directions in parameter space that represent the knowledge gained during fine-tuning for specific tasks. By subtracting the weights of a pre-trained model from its fine-tuned version, task vectors can be combined through operations like addition to create models capable of performing multiple tasks simultaneously.\n\nHowever, naively combining parameters often leads to interference between tasks. TIES-Merging <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> addresses this challenge through a three-step process: (1) resetting parameters that changed minimally during fine-tuning, (2) resolving sign conflicts between parameters, and (3) merging only parameters that align with the agreed-upon sign. This method significantly reduces interference when combining multiple task-specific models.\n\nBuilding on this concept, DARE <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper> introduced a technique that randomly drops delta parameters (differences between fine-tuned and pre-trained parameters) and rescales the remaining ones to approximate original embeddings. This approach effectively eliminates redundancy while preserving model capabilities, allowing even large language models to acquire new capabilities by assimilating parameters from other models without retraining.\n\nMore advanced approaches conceptualize merging as operating in \"task parameter subspaces\" - dimensions in parameter space that are important for specific tasks <Paper corpusId=\"266053657\" paperTitle=\"(Tam et al., 2023)\" isShortName></Paper>. Fisher Merging <Paper corpusId=\"244345933\" paperTitle=\"(Matena et al., 2021)\" isShortName></Paper> uses the Fisher information matrix to weight parameters according to their importance for specific tasks, providing a theoretically grounded approach to merging.\n\nRecent developments have introduced sensitivity-guided merging approaches that recognize not all parameters contribute equally to model performance. Sens-Merging performs parameter sensitivity analysis within individual tasks and task sensitivity analysis across different tasks to prioritize parameters that significantly impact performance <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. Similarly, LoRS-Merging combines low-rank and sparse pruning techniques to retain essential structures while eliminating redundant parameters <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nParameter-space merging techniques offer a resource-efficient alternative to training increasingly large models, allowing existing specialized models to be combined into unified systems with broader capabilities. These methods continue to evolve, with increasingly sophisticated approaches addressing challenges such as parameter redundancy, interference between tasks, and model architecture differences.", "citations": [{"id": "(Wortsman et al., 2022)", "paper": {"corpus_id": 247362886, "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "S. Gadre", "authorId": "1387466862"}, {"name": "R. Roelofs", "authorId": "40458654"}, {"name": "Raphael Gontijo-Lopes", "authorId": "2158366935"}, {"name": "Ari S. Morcos", "authorId": "4690624"}, {"name": "Hongseok Namkoong", "authorId": "40281109"}, {"name": "Ali Farhadi", "authorId": "143787583"}, {"name": "Y. Carmon", "authorId": "2444742"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}], "n_citations": 1011}, "snippets": ["The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."], "score": 0.0}, {"id": "(Ilharco et al., 2022)", "paper": {"corpus_id": 254408495, "title": "Editing Models with Task Arithmetic", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "Marco Tulio Ribeiro", "authorId": "78846919"}, {"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Suchin Gururangan", "authorId": "40895369"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}, {"name": "Ali Farhadi", "authorId": "143787583"}], "n_citations": 520}, "snippets": ["Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \\textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performance on multiple tasks at once. Finally, when tasks are linked by an analogy relationship of the form ``A is to B as C is to D\", combining task vectors from three of the tasks can improve performance on the fourth, even when no data from the fourth task is used for training. Overall, our experiments with several models, modalities and tasks show that task arithmetic is a simple, efficient and effective way of editing models."], "score": 0.0}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"], "score": 0.0}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 265034087, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Le Yu", "authorId": "2265527327"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Haiyang Yu", "authorId": "46493167"}, {"name": "Fei Huang", "authorId": "2257407873"}, {"name": "Yongbin Li", "authorId": "1527090216"}], "n_citations": 335}, "snippets": ["In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."], "score": 0.0}, {"id": "(Tam et al., 2023)", "paper": {"corpus_id": 266053657, "title": "Merging by Matching Models in Task Parameter Subspaces", "year": 2023, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Mohit Bansal", "authorId": "2253762115"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 12}, "snippets": ["In our work, we show how several recent merging methods can be viewed as finding a single model that matches task-specific models in their respective \"task parameter subspaces\". We define a task parameter subspace as the subspace implicitly used by a given merging method that aims to correspond to the important dimensions in parameter space for the task."], "score": 0.6708984375}, {"id": "(Matena et al., 2021)", "paper": {"corpus_id": 244345933, "title": "Merging Models with Fisher-Weighted Averaging", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Colin Raffel", "authorId": "2402716"}], "n_citations": 402}, "snippets": ["Averaging the parameters of models that have the same architecture and initialization can provide a means of combining their respective capabilities. In this paper, we take the perspective that this\"merging\"operation can be seen as choosing parameters that approximately maximize the joint likelihood of the posteriors of the models' parameters. Computing a simple average of the models' parameters therefore corresponds to making an isotropic Gaussian approximation to their posteriors. We develop an alternative merging procedure based on the Laplace approximation where we approximate each model's posterior as a Gaussian distribution whose precision matrix corresponds to its Fisher information. We first show that our\"Fisher merging\"technique provides a performance boost in settings where simple parameter averaging is currently used -- specifically, robust fine-tuning and model ensembling. Then, we compare merging to standard gradient-based transfer learning and demonstrate that merging enables a fundamentally different method for transferring capabilities across models. Specifically, we show that Fisher merging is competitive with gradient-based transfer learning approaches (while being significantly cheaper) in intermediate-task training and domain-adaptive pre-training. We also show that our merging procedure makes it possible to combine models in previously unexplored ways. We release our code to facilitate future research into methods for merging models."], "score": 0.0}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276422064, "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 3}, "snippets": ["In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."], "score": 0.68359375}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276575632, "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qiuming Zhao", "authorId": "2241702793"}, {"name": "Guangzhi Sun", "authorId": "2107310187"}, {"name": "Chao Zhang", "authorId": "2256775692"}, {"name": "Mingxing Xu", "authorId": "2241950375"}, {"name": "Thomas Fang Zheng", "authorId": "2241350908"}], "n_citations": 1}, "snippets": ["We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility."], "score": 0.68603515625}], "table": null}, {"title": "Mixture-of-Experts (MoE) Architectures", "tldr": "Mixture-of-Experts (MoE) architectures integrate multiple specialized models by using routing mechanisms to dynamically direct inputs to appropriate expert components. This approach enables efficient scaling of model capacity while maintaining computational efficiency through conditional computation, where only relevant expert pathways are activated for each input. (13 sources)", "text": "\nMixture-of-Experts (MoE) architectures represent a sophisticated approach to model integration that dynamically combines specialized components based on input characteristics. Unlike parameter-space merging methods that create static unified models, MoE architectures maintain distinct \"expert\" networks that are selectively activated through trainable routing mechanisms.\n\nAt their core, MoE models employ conditional computation, where only certain weights are activated per input as determined by a gating mechanism. This approach offers a significant advantage: substantial increases in model parameters while keeping computational costs relatively constant <Paper corpusId=\"248227728\" paperTitle=\"(Gupta et al., 2022)\" isShortName></Paper>. The seminal work by Shazeer et al. introduced sparsely-gated MoE layers for language modeling and machine translation, demonstrating dramatic improvements in model capacity with only minor computational efficiency losses <Paper corpusId=\"12462234\" paperTitle=\"(Shazeer et al., 2017)\" isShortName></Paper>.\n\nMoE architectures address a fundamental challenge in model merging: parameter interference. Rather than seeking a static optimal solution within the original parameter space, MoE models dynamically integrate shared and task-specific knowledge based on each input <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This approach provides a more flexible solution that can adapt to specific needs of each instance, effectively mitigating interference between parameters from different models <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nThe architecture typically consists of expert networks (specialized components) and router networks (gating mechanisms). During inference, the router determines which experts should process each input, activating only the most relevant subset of the model. This selective activation dramatically improves efficiency compared to ensemble methods that run multiple complete models in parallel <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>.\n\nRecent innovations in MoE architectures have produced several notable variants:\n- Weight-Ensembling Mixture of Experts (WEMoE) transforms only critical modules into MoE structures while statically merging non-critical ones, enabling more efficient integration <Paper corpusId=\"273662099\" paperTitle=\"(Shen et al., 2024)\" isShortName></Paper>\n- Multi-gate Mixture-of-Experts (MMoE) improves upon basic MoE by utilizing multiple gating networks to enable task-specific expert selection, better capturing inter-task relationships <Paper corpusId=\"50770252\" paperTitle=\"(Ma et al., 2018)\" isShortName></Paper>\n- Branch-Train-MiX (BTX) creates multiple expert models from a pre-trained seed model, trains them on different data domains, and then merges their feedforward sublayers into a single MoE module <Paper corpusId=\"277510128\" paperTitle=\"(Shan et al., 2025)\" isShortName></Paper>\n\nFor merging expert models with divergent architectures or specialized capabilities, more sophisticated approaches have emerged. These include techniques to mitigate parameter interference, routing heuristics to reduce fine-tuning needs, and methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. Some approaches employ knowledge fusion mechanisms that adaptively adjust the intensity of information exchange between experts while protecting task-specific knowledge through parameter isolation <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nExpert pruning and merging represent two distinct approaches to MoE compression. Pruning methods like MoE-Pruner implement inter-expert pruning and intra-expert weight sparsification, while merging methods like EEP consolidate multiple experts into fewer, more compact representations through weighted summation of expert weights <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>.\n\nFor adaptation to new tasks while preventing catastrophic forgetting, methods like MINGLE employ parameter-efficient, low-rank experts with null-space constrained gating, which restricts updates to subspaces orthogonal to prior task representations <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nDespite their advantages, MoE architectures face challenges similar to those in multi-task learning, including gradient conflicts and inefficiencies in expert selection <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"210839011\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper>. These issues reflect the fundamental difficulty of balancing shared and task-specific knowledge within a unified architecture.", "citations": [{"id": "(Gupta et al., 2022)", "paper": {"corpus_id": 248227728, "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Shashank Gupta", "authorId": "2152953535"}, {"name": "Subhabrata Mukherjee", "authorId": "2153292652"}, {"name": "K. Subudhi", "authorId": "2043231778"}, {"name": "Eduardo Gonzalez", "authorId": "2162804727"}, {"name": "Damien Jose", "authorId": "144430856"}, {"name": "A. Awadallah", "authorId": "2072795428"}, {"name": "Jianfeng Gao", "authorId": "48441311"}], "n_citations": 50}, "snippets": ["Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant."], "score": 0.70751953125}, {"id": "(Shazeer et al., 2017)", "paper": {"corpus_id": 12462234, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "year": 2017, "venue": "International Conference on Learning Representations", "authors": [{"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}, {"name": "Krzysztof Maziarz", "authorId": "2275364713"}, {"name": "Andy Davis", "authorId": "36347083"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "J. Dean", "authorId": "48448318"}], "n_citations": 2690}, "snippets": ["The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."], "score": 0.0}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 267365047, "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "A. Tang", "authorId": "2178366354"}, {"name": "Li Shen", "authorId": "2248152216"}, {"name": "Yong Luo", "authorId": "2279402395"}, {"name": "Nan Yin", "authorId": "2237424891"}, {"name": "Lefei Zhang", "authorId": "2282189838"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 54}, "snippets": ["Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."], "score": 0.7197265625}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 271957310, "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yifei He", "authorId": "2182670937"}, {"name": "Yuzheng Hu", "authorId": "2317078449"}, {"name": "Yong Lin", "authorId": "2292270783"}, {"name": "Tong Zhang", "authorId": "2306841244"}, {"name": "Han Zhao", "authorId": "2283183420"}], "n_citations": 25}, "snippets": ["More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem."], "score": 0.67431640625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 272832307, "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zesen Zhao", "authorId": "2322611014"}, {"name": "Shuowei Jin", "authorId": "2244738638"}, {"name": "Z. M. Mao", "authorId": "2321511953"}], "n_citations": 5}, "snippets": ["Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models."], "score": 0.73388671875}, {"id": "(Shen et al., 2024)", "paper": {"corpus_id": 273662099, "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Li Shen", "authorId": "2327007623"}, {"name": "A. Tang", "authorId": "2178366354"}, {"name": "Enneng Yang", "authorId": "151497321"}, {"name": "Guibing Guo", "authorId": "2237427680"}, {"name": "Yong Luo", "authorId": "2279402395"}, {"name": "Lefei Zhang", "authorId": "2282189838"}, {"name": "Xiaochun Cao", "authorId": "2316150631"}, {"name": "Bo Du", "authorId": "2212029373"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 9}, "snippets": ["In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach."], "score": 0.83935546875}, {"id": "(Ma et al., 2018)", "paper": {"corpus_id": 50770252, "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts", "year": 2018, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Jiaqi W. Ma", "authorId": "47793019"}, {"name": "Zhe Zhao", "authorId": "48634137"}, {"name": "Xinyang Yi", "authorId": "2838461"}, {"name": "Jilin Chen", "authorId": "2144168512"}, {"name": "Lichan Hong", "authorId": "2217278"}, {"name": "Ed H. Chi", "authorId": "2226805"}], "n_citations": 1146}, "snippets": ["Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google."], "score": 0.0}, {"id": "(Shan et al., 2025)", "paper": {"corpus_id": 277510128, "title": "Cognitive Memory in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lianlei Shan", "authorId": "2350351496"}, {"name": "Shixian Luo", "authorId": "2355000238"}, {"name": "Zezhou Zhu", "authorId": "2353569735"}, {"name": "Yu Yuan", "authorId": "2354002282"}, {"name": "Yong Wu", "authorId": "2354309916"}], "n_citations": 3}, "snippets": ["Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing."], "score": 0.7177734375}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 276095183, "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yuhang Zhou", "authorId": "2266789873"}, {"name": "Giannis Karamanolakis", "authorId": "8458211"}, {"name": "Victor Soto", "authorId": "2302332301"}, {"name": "Anna Rumshisky", "authorId": "1681193"}, {"name": "Mayank Kulkarni", "authorId": "2302332615"}, {"name": "Furong Huang", "authorId": "2257407889"}, {"name": "Wei Ai", "authorId": "2218202090"}, {"name": "Jianhua Lu", "authorId": "2302633316"}], "n_citations": 3}, "snippets": ["The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."], "score": 0.859375}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276938164, "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoda Yang", "authorId": "2308224151"}, {"name": "JunYu Lu", "authorId": "2350336954"}, {"name": "Hongshun Qiu", "authorId": "2220747584"}, {"name": "Sijing Li", "authorId": "2350180388"}, {"name": "Hao Li", "authorId": "2349632427"}, {"name": "Shengpeng Ji", "authorId": "72890649"}, {"name": "Xudong Tang", "authorId": "2349737557"}, {"name": "Jiayang Xu", "authorId": "2349670795"}, {"name": "Jiaqi Duan", "authorId": "2329894630"}, {"name": "Ziyue Jiang", "authorId": "2112347676"}, {"name": "Cong Lin", "authorId": "2349737916"}, {"name": "Sihang Cai", "authorId": "2328348412"}, {"name": "Zejian Xie", "authorId": "2266912737"}, {"name": "Zhuoyang Song", "authorId": "2352067468"}, {"name": "Songxin Zhang", "authorId": "2266803682"}], "n_citations": 0}, "snippets": ["To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."], "score": 0.73486328125}, {"id": "(Gu et al., 2025)", "paper": {"corpus_id": 276575054, "title": "Delta Decompression for MoE-based LLMs Compression", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hao Gu", "authorId": "2347571185"}, {"name": "Wei Li", "authorId": "2331681523"}, {"name": "Lujun Li", "authorId": "2331723310"}, {"name": "Qi Zhu", "authorId": "2313367125"}, {"name": "Mark Lee", "authorId": "2331702843"}, {"name": "Shengjie Sun", "authorId": "2331691577"}, {"name": "Wei Xue", "authorId": "2239201089"}, {"name": "Yi-Ting Guo", "authorId": "2118270918"}], "n_citations": 3}, "snippets": ["To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations."], "score": 0.72314453125}, {"id": "(Qiu et al., 2025)", "paper": {"corpus_id": 278739786, "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Zihuan Qiu", "authorId": "2150449851"}, {"name": "Yi Xu", "authorId": "2321686264"}, {"name": "Chiyuan He", "authorId": "2190031555"}, {"name": "Fanman Meng", "authorId": "1706784"}, {"name": "Linfeng Xu", "authorId": "47775696"}, {"name": "Qingbo Wu", "authorId": "144816629"}, {"name": "Hongliang Li", "authorId": "2300984960"}], "n_citations": 0}, "snippets": ["MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."], "score": 0.82666015625}, {"id": "(Yu et al., 2020)", "paper": {"corpus_id": 210839011, "title": "Gradient Surgery for Multi-Task Learning", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tianhe Yu", "authorId": "10909315"}, {"name": "Saurabh Kumar", "authorId": "2121434953"}, {"name": "Abhishek Gupta", "authorId": "2129458064"}, {"name": "S. Levine", "authorId": "1736651"}, {"name": "Karol Hausman", "authorId": "1944801"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 1228}, "snippets": ["While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."], "score": 0.0}], "table": null}, {"title": "Dynamic Gating and Routing Mechanisms", "tldr": "Dynamic gating and routing mechanisms direct inputs to the most appropriate components within merged model architectures, enabling efficient and context-dependent processing. These approaches range from trainable gating networks in Mixture-of-Experts architectures to sophisticated routing algorithms that optimize for performance, cost, or computational efficiency. (14 sources)", "text": "\nDynamic gating and routing mechanisms serve as the decision-making components within integrated model architectures, determining which specialized pathways should process each input. Unlike static parameter-merging approaches, these mechanisms provide flexibility by activating different parts of the model based on input characteristics, allowing for more efficient computation and better handling of diverse tasks.\n\nAt the core of many gating approaches is the concept of conditional computation, where only specific model weights are activated for each input, as governed by a trainable gating mechanism. This design provides the significant advantage of increasing model parameters while keeping computational costs relatively constant <Paper corpusId=\"248227728\" paperTitle=\"(Gupta et al., 2022)\" isShortName></Paper> <Paper corpusId=\"220265858\" paperTitle=\"(Lepikhin et al., 2020)\" isShortName></Paper>. The foundational work by Shazeer et al. introduced sparsely-gated Mixture-of-Experts layers that dramatically increased model capacity with minimal computational overhead by selectively activating only relevant experts for each input <Paper corpusId=\"12462234\" paperTitle=\"(Shazeer et al., 2017)\" isShortName></Paper>.\n\nMore recent developments have refined these gating mechanisms for specialized applications. Weight-Ensembling Mixture of Experts (WEMoE) transforms only critical model modules into MoE structures while statically merging non-critical ones, providing a more balanced approach to model integration <Paper corpusId=\"273662099\" paperTitle=\"(Shen et al., 2024)\" isShortName></Paper>. Similarly, Multi-gate Mixture-of-Experts (MMoE) improves upon basic MoE by implementing multiple gating networks that enable task-specific expert selection, better capturing inter-task relationships <Paper corpusId=\"50770252\" paperTitle=\"(Ma et al., 2018)\" isShortName></Paper> <Paper corpusId=\"251302720\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>.\n\nBranch-Train-MiX (BTX) represents another innovative approach, where multiple expert models are created from a pre-trained seed model, trained on different data domains, and then merged into a single MoE architecture. Within each Transformer layer, a router network dynamically selects which expert's feedforward sublayer should process each token <Paper corpusId=\"277510128\" paperTitle=\"(Shan et al., 2025)\" isShortName></Paper>.\n\nFor addressing catastrophic forgetting in continual learning scenarios, MINGLE employs parameter-efficient, low-rank experts with null-space constrained gating, which restricts updates to subspaces orthogonal to prior task representations. This approach effectively preserves model behavior on past tasks while allowing adaptation to new ones <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nDynamic knowledge fusion MoE architectures further enhance collaboration between experts through adaptive knowledge-sharing mechanisms, which adjust the intensity of information exchange while protecting task-specific knowledge through parameter isolation <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. This approach directly addresses the challenge of mitigating interference between parameters from different models, which can substantially deteriorate performance <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nRouter-based methods provide another dimension to dynamic processing by directing queries to the most suitable models based on factors such as task complexity, model performance, and system load <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Systems like Route LLM, PolyRouter, and Intelligent Router for LLM Workloads optimize resource utilization by matching queries to the most capable models while balancing performance with cost <Paper corpusId=\"269303119\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270764307\" paperTitle=\"(Ong et al., 2024)\" isShortName></Paper>. These approaches can significantly reduce costs\u2014by over 2 times in some cases\u2014without compromising response quality.\n\nWhile dynamic routing approaches offer compelling advantages, they face practical challenges including increased complexity in designing effective routing algorithms and managing real-time coordination among multiple models <Paper corpusId=\"272832307\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>. Additionally, task-specific routing methods often introduce substantial storage overhead by requiring the preservation of all task vectors to maintain performance <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.", "citations": [{"id": "(Gupta et al., 2022)", "paper": {"corpus_id": 248227728, "title": "Sparsely Activated Mixture-of-Experts are Robust Multi-Task Learners", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Shashank Gupta", "authorId": "2152953535"}, {"name": "Subhabrata Mukherjee", "authorId": "2153292652"}, {"name": "K. Subudhi", "authorId": "2043231778"}, {"name": "Eduardo Gonzalez", "authorId": "2162804727"}, {"name": "Damien Jose", "authorId": "144430856"}, {"name": "A. Awadallah", "authorId": "2072795428"}, {"name": "Jianfeng Gao", "authorId": "48441311"}], "n_citations": 50}, "snippets": ["Mixture-of-Experts (MoE) framework (Shazeer et al., 2017;Fedus et al., 2021;(Lepikhin et al., 2020) provides a way to model this mechanism. Such architectures are designed to support conditional computation in which only certain weights of the network are activated per input as governed by a gating mechanism. This sparse design has an additional advantage of providing additional capacity in terms of model parameters while keeping overall computational cost constant."], "score": 0.70751953125}, {"id": "(Lepikhin et al., 2020)", "paper": {"corpus_id": 220265858, "title": "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding", "year": 2020, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dmitry Lepikhin", "authorId": "150077954"}, {"name": "HyoukJoong Lee", "authorId": "34946720"}, {"name": "Yuanzhong Xu", "authorId": "2145139570"}, {"name": "Dehao Chen", "authorId": "7167328"}, {"name": "Orhan Firat", "authorId": "2345617"}, {"name": "Yanping Huang", "authorId": "2145438541"}, {"name": "M. Krikun", "authorId": "2048712"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Z. Chen", "authorId": "2545358"}], "n_citations": 1191}, "snippets": ["Neural network scaling has been critical for improving the model quality in many real-world machine learning applications with vast amounts of training data and compute. Although this trend of scaling is affirmed to be a sure-fire approach for better model quality, there are challenges on the path such as the computation cost, ease of programming, and efficient implementation on parallel devices. GShard is a module composed of a set of lightweight annotation APIs and an extension to the XLA compiler. It provides an elegant way to express a wide range of parallel computation patterns with minimal changes to the existing model code. GShard enabled us to scale up multilingual neural machine translation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600 billion parameters using automatic sharding. We demonstrate that such a giant model can efficiently be trained on 2048 TPU v3 accelerators in 4 days to achieve far superior quality for translation from 100 languages to English compared to the prior art."], "score": 0.0}, {"id": "(Shazeer et al., 2017)", "paper": {"corpus_id": 12462234, "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "year": 2017, "venue": "International Conference on Learning Representations", "authors": [{"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}, {"name": "Krzysztof Maziarz", "authorId": "2275364713"}, {"name": "Andy Davis", "authorId": "36347083"}, {"name": "Quoc V. Le", "authorId": "2827616"}, {"name": "Geoffrey E. Hinton", "authorId": "1695689"}, {"name": "J. Dean", "authorId": "48448318"}], "n_citations": 2690}, "snippets": ["The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost."], "score": 0.0}, {"id": "(Shen et al., 2024)", "paper": {"corpus_id": 273662099, "title": "Efficient and Effective Weight-Ensembling Mixture of Experts for Multi-Task Model Merging", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Li Shen", "authorId": "2327007623"}, {"name": "A. Tang", "authorId": "2178366354"}, {"name": "Enneng Yang", "authorId": "151497321"}, {"name": "Guibing Guo", "authorId": "2237427680"}, {"name": "Yong Luo", "authorId": "2279402395"}, {"name": "Lefei Zhang", "authorId": "2282189838"}, {"name": "Xiaochun Cao", "authorId": "2316150631"}, {"name": "Bo Du", "authorId": "2212029373"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 9}, "snippets": ["In this paper, we propose a Weight-Ensembling Mixture of Experts (WEMoE) method for multi-task model merging. Specifically, we first identify critical (or sensitive) modules by analyzing parameter variations in core modules of Transformer-based models before and after finetuning. Then, our WEMoE statically merges non-critical modules while transforming critical modules into a mixture-of-experts (MoE) structure. During inference, expert modules in the MoE are dynamically merged based on input samples, enabling a more flexible and adaptive merging approach."], "score": 0.83935546875}, {"id": "(Ma et al., 2018)", "paper": {"corpus_id": 50770252, "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts", "year": 2018, "venue": "Knowledge Discovery and Data Mining", "authors": [{"name": "Jiaqi W. Ma", "authorId": "47793019"}, {"name": "Zhe Zhao", "authorId": "48634137"}, {"name": "Xinyang Yi", "authorId": "2838461"}, {"name": "Jilin Chen", "authorId": "2144168512"}, {"name": "Lichan Hong", "authorId": "2217278"}, {"name": "Ed H. Chi", "authorId": "2226805"}], "n_citations": 1146}, "snippets": ["Neural-based multi-task learning has been successfully used in many real-world large-scale applications such as recommendation systems. For example, in movie recommendations, beyond providing users movies which they tend to purchase and watch, the system might also optimize for users liking the movies afterwards. With multi-task learning, we aim to build a single model that learns these multiple goals and tasks simultaneously. However, the prediction quality of commonly used multi-task models is often sensitive to the relationships between tasks. It is therefore important to study the modeling tradeoffs between task-specific objectives and inter-task relationships. In this work, we propose a novel multi-task learning approach, Multi-gate Mixture-of-Experts (MMoE), which explicitly learns to model task relationships from data. We adapt the Mixture-of-Experts (MoE) structure to multi-task learning by sharing the expert submodels across all tasks, while also having a gating network trained to optimize each task. To validate our approach on data with different levels of task relatedness, we first apply it to a synthetic dataset where we control the task relatedness. We show that the proposed approach performs better than baseline methods when the tasks are less related. We also show that the MMoE structure results in an additional trainability benefit, depending on different levels of randomness in the training data and model initialization. Furthermore, we demonstrate the performance improvements by MMoE on real tasks including a binary classification benchmark, and a large-scale content recommendation system at Google."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 251302720, "title": "Multi-Task Learning with Calibrated Mixture of Insightful Experts", "year": 2022, "venue": "IEEE International Conference on Data Engineering", "authors": [{"name": "Sinan Wang", "authorId": "2143243378"}, {"name": "Yumeng Li", "authorId": "2111175981"}, {"name": "Hongyan Li", "authorId": "2115263944"}, {"name": "Tanchao Zhu", "authorId": "2072834393"}, {"name": "Zhao Li", "authorId": null}, {"name": "Wenwu Ou", "authorId": "10336865"}], "n_citations": 12}, "snippets": ["Multi-task learning has been established as an important machine learning framework for leveraging shared knowledge among multiple different but related tasks, with the generalization performance of models enhanced. As a promising learning paradigm, multi-task learning has been widely adopted by various real-world applications, such as recommendation systems. Multi-gate Mixture-of-Experts (MMoE), a well-received multi-task learning method in industry, based on the classic and inspiring Mixture-of-Experts (MoE) structure, explicitly models task relationships and learns task-specific functionalities, generating significant improvements. However, in our applications, negative transfer, which confuses considerable existing multi-task learning methods, is still observed to happen to MMoE. In this paper, an in-depth empirical investigation into negative transfer is launched. And it reveals that, incompetent experts, which play fundamental roles under the learning framework of MoE, are the key technique bottleneck. To tackle this dilemma, we propose the Calibrated Mixture of Insightful Experts (CMoIE), with three novel modules (Conflict Resolution, Expert Communication, and Mixture Calibration), customed for multi-task learning. Hence a group of insightful experts are constructed with enhanced diversity, communication and specialization. To validate the proposed method CMoIE, experiments are conducted on three public datasets and one real-world click-through-rate prediction dataset we construct based on traffic logs collected from a large-scale online product recommendation system. Our approach yields best performance across all of these benchmarks, demonstrating the superiority of it."], "score": 0.0}, {"id": "(Shan et al., 2025)", "paper": {"corpus_id": 277510128, "title": "Cognitive Memory in Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Lianlei Shan", "authorId": "2350351496"}, {"name": "Shixian Luo", "authorId": "2355000238"}, {"name": "Zezhou Zhu", "authorId": "2353569735"}, {"name": "Yu Yuan", "authorId": "2354002282"}, {"name": "Yong Wu", "authorId": "2354309916"}], "n_citations": 3}, "snippets": ["Sukhbaatar et al. [2024] introduces an innovative model training method called Branch-Train-MiX (BTX), which aims to efficiently integrate multiple expert large language models (LLMs) into a single mixture-of-experts (MoE) model. This method combines the strengths of the Branch-Train-Merge (BTM) approach and the MoE architecture while mitigating their respective drawbacks. \n\nThe BTX method consists of three main steps. First, during the Branch and Train phase, multiple copies (referred to as expert models) are created from a pre-trained seed model and trained independently on different data subsets, each corresponding to a specific knowledge domain such as mathematics, programming, or Wikipedia. This training process is parallel and asynchronous, reducing communication costs and increasing training throughput. Next, in the MiX phase, the feedforward sublayers of these expert models are merged into a single MoE module to form a unified MoE model. Within each Transformer layer, a router network is used to select which expert's feedforward sublayer should be applied to each token. The weights of the self-attention sublayers and other modules are combined through simple averaging. Finally, in the MoE Finetuning phase, the merged model is further fine-tuned on the entire training dataset, allowing the router network to learn how to route tokens dynamically between different experts during testing."], "score": 0.7177734375}, {"id": "(Qiu et al., 2025)", "paper": {"corpus_id": 278739786, "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Zihuan Qiu", "authorId": "2150449851"}, {"name": "Yi Xu", "authorId": "2321686264"}, {"name": "Chiyuan He", "authorId": "2190031555"}, {"name": "Fanman Meng", "authorId": "1706784"}, {"name": "Linfeng Xu", "authorId": "47775696"}, {"name": "Qingbo Wu", "authorId": "144816629"}, {"name": "Hongliang Li", "authorId": "2300984960"}], "n_citations": 0}, "snippets": ["MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."], "score": 0.82666015625}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276938164, "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoda Yang", "authorId": "2308224151"}, {"name": "JunYu Lu", "authorId": "2350336954"}, {"name": "Hongshun Qiu", "authorId": "2220747584"}, {"name": "Sijing Li", "authorId": "2350180388"}, {"name": "Hao Li", "authorId": "2349632427"}, {"name": "Shengpeng Ji", "authorId": "72890649"}, {"name": "Xudong Tang", "authorId": "2349737557"}, {"name": "Jiayang Xu", "authorId": "2349670795"}, {"name": "Jiaqi Duan", "authorId": "2329894630"}, {"name": "Ziyue Jiang", "authorId": "2112347676"}, {"name": "Cong Lin", "authorId": "2349737916"}, {"name": "Sihang Cai", "authorId": "2328348412"}, {"name": "Zejian Xie", "authorId": "2266912737"}, {"name": "Zhuoyang Song", "authorId": "2352067468"}, {"name": "Songxin Zhang", "authorId": "2266803682"}], "n_citations": 0}, "snippets": ["To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."], "score": 0.73486328125}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 267365047, "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "A. Tang", "authorId": "2178366354"}, {"name": "Li Shen", "authorId": "2248152216"}, {"name": "Yong Luo", "authorId": "2279402395"}, {"name": "Nan Yin", "authorId": "2237424891"}, {"name": "Lefei Zhang", "authorId": "2282189838"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 54}, "snippets": ["Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."], "score": 0.7197265625}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 272832307, "title": "Eagle: Efficient Training-Free Router for Multi-LLM Inference", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zesen Zhao", "authorId": "2322611014"}, {"name": "Shuowei Jin", "authorId": "2244738638"}, {"name": "Z. M. Mao", "authorId": "2321511953"}], "n_citations": 5}, "snippets": ["Mixture-of-Experts (MoE) and Ensemble Learning are two pivotal techniques for optimizing multi-LLM systems by leveraging multiple models to improve both performance and efficiency. Ensemble Learning, seen in systems like LLM Blender [12] and Blending Is All You Need [16], combines outputs from multiple models to enhance accuracy and robustness, albeit often at the cost of increased computational overhead. In contrast, MoE [19] activates only a subset of experts for each task, reducing computational demands by using only the most relevant models. While both approaches aim to boost LLM performance through the use of multiple models, MoE emphasizes scalability and resource efficiency, whereas Ensemble Learning focuses on robustness by combining model outputs. Nonetheless, challenges such as increased complexity in ensemble methods and potential inefficiencies in expert selection for MoE remain. \n\nRouter-based methods, including Route LLM [17], PolyRouter [21], hybrid LLM [7], and Intelligent Router for LLM Workloads [11], strive to enhance efficiency by dynamically routing queries to the most suitable model. These methods intelligently allocate tasks based on factors like task complexity, model performance, and system load, minimizing unnecessary computation and optimizing resource utilization. Route LLM focuses on matching queries to the most capable model, PolyRouter balances performance with cost, hybrid LLM tries to predict query complexity and route to most suitable models rather than singleton superior LLM, and Intelligent Router applies workload-aware scheduling to maximize throughput under heavy loads. While these approaches improve efficiency, they often introduce complexity in designing effective routing algorithms and managing real-time coordination among multiple models."], "score": 0.73388671875}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 269303119, "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dujian Ding", "authorId": "123966440"}, {"name": "Ankur Mallick", "authorId": "2297849625"}, {"name": "Chi Wang", "authorId": "2298452007"}, {"name": "Robert Sim", "authorId": "2253669181"}, {"name": "Subhabrata Mukherjee", "authorId": "2153292652"}, {"name": "Victor R\u00fchle", "authorId": "3898805"}, {"name": "L. Lakshmanan", "authorId": "1708593"}, {"name": "A. Awadallah", "authorId": "2072795428"}], "n_citations": 106}, "snippets": ["Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality."], "score": 0.0}, {"id": "(Ong et al., 2024)", "paper": {"corpus_id": 270764307, "title": "RouteLLM: Learning to Route LLMs with Preference Data", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isaac Ong", "authorId": "2199840167"}, {"name": "Amjad Almahairi", "authorId": "2634674"}, {"name": "Vincent Wu", "authorId": "2308466924"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Tianhao Wu", "authorId": "2251528079"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "M. W. Kadous", "authorId": "1793159"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 104}, "snippets": ["Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs."], "score": 0.0}, {"id": "(Liu et al._1, 2025)", "paper": {"corpus_id": 276409347, "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Zehua Liu", "authorId": "2333317068"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 2}, "snippets": ["Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments."], "score": 0.69580078125}], "table": null}, {"title": "Addressing Task Interference and Knowledge Conflicts", "tldr": "Task interference and knowledge conflicts arise when merging specialized models, leading to performance degradation in the unified system. Various techniques have been developed to mitigate these issues, including parameter sensitivity analysis, sign conflict resolution, and knowledge separation approaches that dynamically balance task-specific and shared information. (14 sources)", "text": "\nModel merging techniques face a fundamental challenge: parameter interference between models trained on different tasks, which can substantially deteriorate performance in the unified model <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. This interference manifests in two primary forms: redundant parameter values and disagreement on parameter signs across models <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nTo address these challenges, researchers have developed increasingly sophisticated approaches. TIES-Merging implements a three-step process: (1) resetting parameters that changed minimally during fine-tuning, (2) resolving sign conflicts through majority voting, and (3) merging only parameters that align with the final agreed-upon sign <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This approach has shown significant improvements across diverse settings, modalities, and model architectures.\n\nAnother effective technique, DARE, addresses parameter redundancy by randomly dropping delta parameters (differences between fine-tuned and pre-trained parameters) and rescaling the remaining ones to approximate original embeddings <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>. This approach can eliminate up to 90-99% of delta parameters without significant performance loss, making it particularly effective for merging large language models.\n\nSens-Merging introduces sensitivity-guided coefficient adjustment that operates at two levels: within individual tasks (identifying critical layers through parameter sensitivity analysis) and across different tasks (prioritizing models that enhance others' performance) <Paper corpusId=\"276422064\" paperTitle=\"(Liu et al., 2025)\" isShortName></Paper>. This fine-grained control over parameter importance allows for more effective layer-wise merging <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nSome approaches address task interference by conceptualizing knowledge in specialized models as consisting of both shared (generalizable) and task-specific components. Weight-Ensembling Mixture of Experts (WEMoE) mitigates interference by upscaling certain model components into MoE modules that can dynamically integrate these knowledge types based on input <Paper corpusId=\"267365047\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>. Similarly, Twin-Merging extracts and modularizes different knowledge types, then uses a router to dynamically integrate them based on input characteristics <Paper corpusId=\"277322544\" paperTitle=\"(Wu et al., 2025)\" isShortName></Paper>.\n\nRecent research has also drawn parallels between model merging challenges and those faced in multi-task learning (MTL), where gradient conflicts between tasks have been extensively studied <Paper corpusId=\"271957310\" paperTitle=\"(He et al., 2024)\" isShortName></Paper> <Paper corpusId=\"210839011\" paperTitle=\"(Yu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"239998731\" paperTitle=\"(Liu et al., 2021)\" isShortName></Paper>. Techniques developed for MTL, such as gradient surgery, task weighting, and modularization, have informed approaches to model merging <Paper corpusId=\"275119334\" paperTitle=\"(Jung et al., 2024)\" isShortName></Paper>.\n\nThe newest generation of merging techniques includes Conflict-Aware Task Merging (CAT Merging), which selectively trims conflict-prone components from task vectors using parameter-specific strategies such as projection for linear weights and masking for scaling parameters <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. Additionally, Representation Surgery inserts lightweight task-specific modules to realign the merged model's internal representations with those of individual models, enhancing overall performance in multitask scenarios <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\nAs model merging techniques continue to evolve, they increasingly focus on selectively integrating knowledge while preserving task-specific capabilities, moving beyond simple averaging or linear combinations toward more sophisticated approaches that explicitly address parameter conflicts <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271064761\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Tang et al., 2024)", "paper": {"corpus_id": 267365047, "title": "Merging Multi-Task Models via Weight-Ensembling Mixture of Experts", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "A. Tang", "authorId": "2178366354"}, {"name": "Li Shen", "authorId": "2248152216"}, {"name": "Yong Luo", "authorId": "2279402395"}, {"name": "Nan Yin", "authorId": "2237424891"}, {"name": "Lefei Zhang", "authorId": "2282189838"}, {"name": "D. Tao", "authorId": "2255502438"}], "n_citations": 54}, "snippets": ["Merging various task-specific Transformer-based models trained on different tasks into a single unified model can execute all the tasks concurrently. Previous methods, exemplified by task arithmetic, have been proven to be both effective and scalable. Existing methods have primarily focused on seeking a static optimal solution within the original model parameter space. A notable challenge is mitigating the interference between parameters of different models, which can substantially deteriorate performance. In this paper, we propose to merge most of the parameters while upscaling the MLP of the Transformer layers to a weight-ensembling mixture of experts (MoE) module, which can dynamically integrate shared and task-specific knowledge based on the input, thereby providing a more flexible solution that can adapt to the specific needs of each instance. Our key insight is that by identifying and separating shared knowledge and task-specific knowledge, and then dynamically integrating them, we can mitigate the parameter interference problem to a great extent."], "score": 0.7197265625}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 270702345, "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhenyi Lu", "authorId": "2262512474"}, {"name": "Chenghao Fan", "authorId": "2277238906"}, {"name": "Wei Wei", "authorId": "2284721764"}, {"name": "Xiaoye Qu", "authorId": "2262446609"}, {"name": "Dangyang Chen", "authorId": "2182623368"}, {"name": "Yu Cheng", "authorId": "2284687448"}], "n_citations": 63}, "snippets": ["However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models."], "score": 0.732421875}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"], "score": 0.0}, {"id": "(Liu et al., 2025)", "paper": {"corpus_id": 276422064, "title": "Sens-Merging: Sensitivity-Guided Parameter Balancing for Merging Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 3}, "snippets": ["In the context of model merging, task vectors (Ilharco et al., 2023a) have emerged as a powerful component for representing task-specific capabilities. These vectors, defined as the differences between parameter values before and after fine-tuning, enable effective integration of specialized knowledge from different models. While task vector-based merging methods (Yadav et al., 2023;Yu et al., 2024) have shown promising results, their reliance on uniform coefficients for each task and parameter limits their potential effectiveness. This uniformity implies that every task and every parameter is treated with equal importance during the merging process. Consequently, it overlooks the fact that parameters within each layer demonstrate varying levels of importance for specific tasks, and parameters from different tasks contribute distinctly during the merging process.\n\nTo address these challenges, we propose Sens-Merging, a sensitivity-guided merging coefficient adjustment method that functions as a plug-andplay enhancement to existing task vector-based merging techniques. Our method operates at two levels: within individual tasks and across different tasks, allowing for fine-grained control over parameter importance. Within each task-specific model, we perform parameter sensitivity analysis to highlight critical layers that significantly impact performance. Concurrently, across different tasks, we conduct task sensitivity analysis to prioritize models that enhance the performance of others."], "score": 0.68359375}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 265034087, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Le Yu", "authorId": "2265527327"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Haiyang Yu", "authorId": "46493167"}, {"name": "Fei Huang", "authorId": "2257407873"}, {"name": "Yongbin Li", "authorId": "1527090216"}], "n_citations": 335}, "snippets": ["In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."], "score": 0.0}, {"id": "(Wu et al., 2025)", "paper": {"corpus_id": 277322544, "title": "Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Han Wu", "authorId": "2346255376"}, {"name": "Yuxuan Yao", "authorId": "2345985527"}, {"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Zehua Liu", "authorId": "2333317068"}, {"name": "Xiaojin Fu", "authorId": "2221337060"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Xing Li", "authorId": "2344902525"}, {"name": "Hui-Ling Zhen", "authorId": "2267558779"}, {"name": "Tao Zhong", "authorId": "2332348570"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}], "n_citations": 14}, "snippets": ["Model merging seeks to integrate multiple fine-tuned (FT) models, derived from a pre-trained (PT) model \u03b8 0 , into a unified model that consolidates knowledge from diverse sources. Given K FT models to be merged, denoted as \u03b8 1 , . . . , \u03b8 K , the goal is to produce a single model \u03b8 M that inherits the capabilities of the individual models.\n\nAverage Merging Average merging (Wortsman et al., 2022) is a simple and effective method to enhance overall performance by performing an arithmetic average of the model weights. It reduces variance by smoothing random errors, especially when base models are diverse and exhibit low bias. However, its effectiveness depends on the quality and diversity of the base models; high bias across models limits its improvement potential.\n\nTask Arithmetic (TA) In most existing task-vector-based approaches, the base model \u03b8 0 is essential for computing task vectors (Ilharco et al., 2023), which generally encapsulate the knowledge acquired during fine-tuning. A task vector is defined as the parameter shift between an FT model and its corresponding base model, expressed as \u03b4 k = \u03b8 k \u2212 \u03b8 0 . The merged model \u03b8 M is then obtained by aggregating the task vectors into the base model, as \n\n, where \u03bb k represents the weight coefficient, which can either be manually set as a constant or determined through optimization.\n\nTies Merging TIES Merging (Yadav et al., 2023) is an efficient method for integrating parameters from multiple FT models, addressing redundancy and conflicts. Its key steps include: (1) pruning parameters, retaining significant deviations from pre-trained weights; (2) resolving conflicts via majority voting or alignment; and (3) weighted aggregation of significant parameters to form the final model. This approach reduces noise and enhances generalization, particularly for integrating fine-tuned large language models (LLMs) across related tasks.\n\nDARE (Yu et al., 2024a) DARE Merging is a lightweight approach, whose core steps include: (1) randomly dropping redundant parameters (e.g., those with minimal gradient changes) to reduce noise; (2) adjusting the direction of retained parameters to resolve conflicts between models; and (3) performing weighted integration of key parameters to preserve essential knowledge.\n\nTwin-Merging Performance gaps between merged and fine-tuned models stem from conflicts among models and diverse testing data. Twin-Merging (Lu et al., 2024) resolves this by categorizing expert knowledge into generalizable shared knowledge and task-specific knowledge. Through compression and difference extraction, this knowledge is modularized. A router then dynamically integrates shared and task-specific knowledge based on input, similar to the Mixture of Experts approach, allowing for flexible adjustments.\n\nSens-Merging Sens-Merging (Liu et al., 2025b) focuses on the varying importance of parameters within and across tasks during model merging. It operates at two levels: (1) within individual tasks, where parameter sensitivity analysis identifies critical layers impacting performance, and (2) across tasks, where task sensitivity analysis prioritizes models that enhance others' performance. By combining these analyses, Sens-Merging derives merging coefficients for fine-grained parameter control, enabling effective layer-wise merging. It also serves as a plug-and-play enhancement to task vector-based merging, improving flexibility and performance."], "score": 0.72021484375}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 271957310, "title": "Localize-and-Stitch: Efficient Model Merging via Sparse Task Arithmetic", "year": 2024, "venue": "Trans. Mach. Learn. Res.", "authors": [{"name": "Yifei He", "authorId": "2182670937"}, {"name": "Yuzheng Hu", "authorId": "2317078449"}, {"name": "Yong Lin", "authorId": "2292270783"}, {"name": "Tong Zhang", "authorId": "2306841244"}, {"name": "Han Zhao", "authorId": "2283183420"}], "n_citations": 25}, "snippets": ["More recently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms to direct inputs to task-specific networks. In this work, we primarily focus on merging specialized models into a single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu et al., 2020)(Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when merged together, and our method provides an effective solution to this problem."], "score": 0.67431640625}, {"id": "(Yu et al., 2020)", "paper": {"corpus_id": 210839011, "title": "Gradient Surgery for Multi-Task Learning", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tianhe Yu", "authorId": "10909315"}, {"name": "Saurabh Kumar", "authorId": "2121434953"}, {"name": "Abhishek Gupta", "authorId": "2129458064"}, {"name": "S. Levine", "authorId": "1736651"}, {"name": "Karol Hausman", "authorId": "1944801"}, {"name": "Chelsea Finn", "authorId": "46881670"}], "n_citations": 1228}, "snippets": ["While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance."], "score": 0.0}, {"id": "(Liu et al., 2021)", "paper": {"corpus_id": 239998731, "title": "Conflict-Averse Gradient Descent for Multi-task Learning", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Bo Liu", "authorId": "1720831208"}, {"name": "Xingchao Liu", "authorId": "46521757"}, {"name": "Xiaojie Jin", "authorId": "2103483"}, {"name": "P. Stone", "authorId": "144848112"}, {"name": "Qiang Liu", "authorId": "47362268"}], "n_citations": 318}, "snippets": ["The goal of multi-task learning is to enable more efficient learning than single task learning by sharing model structures for a diverse set of tasks. A standard multi-task learning objective is to minimize the average loss across all tasks. While straightforward, using this objective often results in much worse final performance for each task than learning them independently. A major challenge in optimizing a multi-task model is the conflicting gradients, where gradients of different task objectives are not well aligned so that following the average gradient direction can be detrimental to specific tasks' performance. Previous work has proposed several heuristics to manipulate the task gradients for mitigating this problem. But most of them lack convergence guarantee and/or could converge to any Pareto-stationary point. In this paper, we introduce Conflict-Averse Gradient descent (CAGrad) which minimizes the average loss function, while leveraging the worst local improvement of individual tasks to regularize the algorithm trajectory. CAGrad balances the objectives automatically and still provably converges to a minimum over the average loss. It includes the regular gradient descent (GD) and the multiple gradient descent algorithm (MGDA) in the multi-objective optimization (MOO) literature as special cases. On a series of challenging multi-task supervised learning and reinforcement learning tasks, CAGrad achieves improved performance over prior state-of-the-art multi-objective gradient manipulation methods."], "score": 0.0}, {"id": "(Jung et al., 2024)", "paper": {"corpus_id": 275119334, "title": "Why Train Everything? Tint a Single Layer for Multi-task Model Merging", "year": 2024, "venue": "", "authors": [{"name": "Aecheon Jung", "authorId": "2305817448"}, {"name": "Seunghwan Lee", "authorId": "2261794770"}, {"name": "Dongyoon Han", "authorId": "2338395404"}, {"name": "Sungeun Hong", "authorId": "2261902726"}], "n_citations": 1}, "snippets": ["Multi-task Learning (MTL) enables models to perform multiple tasks simultaneously by leveraging shared knowledge (Caruana, 1997)(Vandenhende et al., 2020)(Zhang et al., 2020). However, MTL faces challenges like task interference and negative transfer. To address these issues, modularization techniques introduce task-specific modules or pathways, preserving unique task information and reducing interference (Ma et al., 2018)(Misra et al., 2016)(Rosenbaum et al., 2017). Gradient-based methods balance tasks through normalization and align gradient directions to minimize conflicts (Chen et al., 2017)(Yu et al., 2020). Task weighting approaches dynamically assign weights to balance the learning process and prevent any single task from dominating (Guo et al., 2018)(Hu et al., 2023)(Kendall et al., 2017)(Liu et al., 2018)(Sener et al., 2018). Knowledge distillation further enhances MTL by transferring insights from specialized models to a unified framework (Ghiasi et al., 2021)(Jacob et al., 2023)[70]. Despite these advancements, traditional MTL methods often require extensive labeled data and significant computational resources. This highlights the need for more efficient and scalable approaches such as model merging, which can consolidate independently fine-tuned models into a unified framework."], "score": 0.658203125}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278501405, "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 2}, "snippets": ["Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers."], "score": 0.71630859375}, {"id": "(Coleman et al., 2025)", "paper": {"corpus_id": 277940324, "title": "Parameter-Efficient Continual Fine-Tuning: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Eric Nuertey Coleman", "authorId": "2244619371"}, {"name": "Luigi Quarantiello", "authorId": "2223689402"}, {"name": "Ziyue Liu", "authorId": "2356575441"}, {"name": "Qinwen Yang", "authorId": "2356596632"}, {"name": "Samrat Mukherjee", "authorId": "2356499006"}, {"name": "J. Hurtado", "authorId": "2064859104"}, {"name": "Vincenzo Lomonaco", "authorId": "2285835294"}], "n_citations": 0}, "snippets": ["Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss", "For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios."], "score": 0.69091796875}, {"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements."], "score": 0.83935546875}, {"id": "(Jin et al., 2024)", "paper": {"corpus_id": 271064761, "title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Ruochen Jin", "authorId": "2310437466"}, {"name": "Bojian Hou", "authorId": "2248244711"}, {"name": "Jiancong Xiao", "authorId": "2327839292"}, {"name": "Weijie J. Su", "authorId": "2311908363"}, {"name": "Li Shen", "authorId": "2248152254"}], "n_citations": 5}, "snippets": ["During Merging Methods. In the context of multi-task learning (MTL), model merging can be effectively achieved by employing various strategies to resolve task conflicts and perform parameter merging operations. Traditional methods often involve averaging or combining weights from multiple models to create a unified system, as demonstrated in prior works (Garipov et al., 2018)(Ilharco et al., 2022)(Wortsman et al., 2022). However, these basic merging techniques frequently underperform, particularly when tasks interfere with one another. Advanced methods have been developed to address this challenge by incorporating weighted-based strategies that assign different importance levels to task vectors during merging (Matena & Raffel, 2021;Ainsworth et al., 2023;Stoica et al., 2023;Yang et al., 2023)."], "score": 0.6806640625}], "table": null}, {"title": "Advanced Integration Techniques", "tldr": "Advanced model integration techniques go beyond basic merging methods by introducing sophisticated approaches like Twin-Merging, CAT Merging, and Representation Surgery that address specialized challenges in combining diverse models. These techniques enable more effective knowledge transfer while minimizing conflicts through strategies such as dynamic knowledge fusion, conflict-aware trimming, and orthogonal parameter updates. (8 sources)", "text": "\nBuilding on traditional parameter-space merging and MoE architectures, researchers have developed increasingly sophisticated techniques to address the complex challenges of integrating diverse specialized models. These advanced approaches move beyond simple averaging or linear combinations toward more nuanced methods that directly target the core challenges of model integration.\n\nTwin-Merging represents one such innovation, recognizing that expert knowledge can be categorized into generalizable shared knowledge and task-specific knowledge. This approach uses compression and difference extraction to modularize these knowledge types, then employs a router to dynamically integrate them based on input characteristics, creating a flexible system similar to MoE architectures <Paper corpusId=\"276937513\" paperTitle=\"(Ruan et al., 2025)\" isShortName></Paper>.\n\nFor models with divergent architectures or highly specialized capabilities, newer techniques have emerged to facilitate effective integration. These include specialized strategies to mitigate parameter interference, innovative routing heuristics to reduce fine-tuning requirements, and novel methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nRecent advances in adaptive knowledge integration include dynamic knowledge fusion MoE architectures, which enhance collaboration between experts through adaptive knowledge-sharing mechanisms. These systems dynamically adjust the intensity of information exchange between experts while protecting task-specific knowledge through parameter isolation, allowing for more effective knowledge transfer while preserving specialized capabilities <Paper corpusId=\"276938164\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nConflict-Aware Task Merging (CAT Merging) addresses knowledge conflicts by selectively trimming conflict-prone components from task vectors. This training-free framework introduces parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers, effectively preserving model capabilities while reducing interference <Paper corpusId=\"278501405\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nFor continual learning scenarios where catastrophic forgetting is a concern, MINGLE employs parameter-efficient, low-rank experts with Null-Space Constrained Gating. This innovative approach restricts gating updates to subspaces orthogonal to prior task representations, suppressing activations on old task inputs and preserving model behavior on past tasks. MINGLE further incorporates an Adaptive Relaxation Strategy that dynamically adjusts constraint strength based on interference signals captured during adaptation <Paper corpusId=\"278739786\" paperTitle=\"(Qiu et al., 2025)\" isShortName></Paper>.\n\nLoRS-Merging (low-rank and sparse model merging) represents another advancement, combining low-rank and sparse pruning techniques to retain essential structures while eliminating redundant parameters. This approach effectively mitigates language and task interference while enhancing model extensibility, making it particularly suitable for integrating models trained across different languages or tasks <Paper corpusId=\"276575632\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nFrankenMoEs exemplifies the trend toward combining model merging with MoE architectures, initializing MoE MLP layers using weights from task-specific models. Recent extensions of this approach leverage features from specialized models for gate initialization and merge self-attention sublayers within transformer architectures, enabling more effective knowledge transfer across diverse domains <Paper corpusId=\"271947337\" paperTitle=\"(Pourreza et al., 2024)\" isShortName></Paper>.\n\nAs model merging techniques continue to evolve, they increasingly incorporate elements from multiple paradigms, blending parameter-space manipulation, expert routing, and specialized knowledge protection mechanisms. These hybrid approaches offer promising directions for creating unified models that effectively leverage the strengths of specialized components while addressing the core challenges of integration <Paper corpusId=\"276422131\" paperTitle=\"(Rousset et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Ruan et al., 2025)", "paper": {"corpus_id": 276937513, "title": "From Task-Specific Models to Unified Systems: A Review of Model Merging Approaches", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wei Ruan", "authorId": "2321405806"}, {"name": "Tianze Yang", "authorId": "2263682353"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Tianming Liu", "authorId": "2349736445"}, {"name": "Jin Lu", "authorId": "2331910055"}], "n_citations": 0}, "snippets": ["Model merging techniques have evolved from simple linear interpolation methods or weight averaging to more sophisticated approaches [Yang et al., 2024a;Sung et al., 2023]. These include weight interference suppression, parameter freezing, and decoupling parameters for old and new tasks, allowing for the fine-tuning of specific model aspects while preserving core functionalities. There is also growing interest in integrating merging methods with Mixture of Experts (MoE) frameworks, where specialized \"experts\" are dynamically engaged based on the task requirements."], "score": 0.83935546875}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 276095183, "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yuhang Zhou", "authorId": "2266789873"}, {"name": "Giannis Karamanolakis", "authorId": "8458211"}, {"name": "Victor Soto", "authorId": "2302332301"}, {"name": "Anna Rumshisky", "authorId": "1681193"}, {"name": "Mayank Kulkarni", "authorId": "2302332615"}, {"name": "Furong Huang", "authorId": "2257407889"}, {"name": "Wei Ai", "authorId": "2218202090"}, {"name": "Jianhua Lu", "authorId": "2302633316"}], "n_citations": 3}, "snippets": ["The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."], "score": 0.859375}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 276938164, "title": "Astrea: A MOE-based Visual Understanding Model with Progressive Alignment", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaoda Yang", "authorId": "2308224151"}, {"name": "JunYu Lu", "authorId": "2350336954"}, {"name": "Hongshun Qiu", "authorId": "2220747584"}, {"name": "Sijing Li", "authorId": "2350180388"}, {"name": "Hao Li", "authorId": "2349632427"}, {"name": "Shengpeng Ji", "authorId": "72890649"}, {"name": "Xudong Tang", "authorId": "2349737557"}, {"name": "Jiayang Xu", "authorId": "2349670795"}, {"name": "Jiaqi Duan", "authorId": "2329894630"}, {"name": "Ziyue Jiang", "authorId": "2112347676"}, {"name": "Cong Lin", "authorId": "2349737916"}, {"name": "Sihang Cai", "authorId": "2328348412"}, {"name": "Zejian Xie", "authorId": "2266912737"}, {"name": "Zhuoyang Song", "authorId": "2352067468"}, {"name": "Songxin Zhang", "authorId": "2266803682"}], "n_citations": 0}, "snippets": ["To address these challenges, we propose a dynamic knowledge fusion Mixture of Expert (MoE) architecture. The core innovation lies in enhancing the collaborative capabilities between different experts through adaptive knowledge-sharing and task isolation mechanisms.\n\nSpecifically, we design a coarse-to-fine pre-alignment strategy in the upstream training, and introduce a dynamic knowledge fusion module during downstream training to adaptively adjust the intensity of information exchange between experts, and protect task-specific knowledge through parameter isolation mechanisms."], "score": 0.73486328125}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 278501405, "title": "CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Wenju Sun", "authorId": "35640834"}, {"name": "Qingyong Li", "authorId": "2262408434"}, {"name": "Yangli-ao Geng", "authorId": "8010931"}, {"name": "Boyang Li", "authorId": "2342563128"}], "n_citations": 2}, "snippets": ["Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers."], "score": 0.71630859375}, {"id": "(Qiu et al., 2025)", "paper": {"corpus_id": 278739786, "title": "MINGLE: Mixtures of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging", "year": 2025, "venue": "", "authors": [{"name": "Zihuan Qiu", "authorId": "2150449851"}, {"name": "Yi Xu", "authorId": "2321686264"}, {"name": "Chiyuan He", "authorId": "2190031555"}, {"name": "Fanman Meng", "authorId": "1706784"}, {"name": "Linfeng Xu", "authorId": "47775696"}, {"name": "Qingbo Wu", "authorId": "144816629"}, {"name": "Hongliang Li", "authorId": "2300984960"}], "n_citations": 0}, "snippets": ["MINGLE employs a mixture-of-experts architecture composed of parameter-efficient, low-rank experts, enabling efficient adaptation and improving robustness to distribution shifts. To mitigate catastrophic forgetting, we propose Null-Space Constrained Gating, which restricts gating updates to subspaces orthogonal to prior task representations. This suppresses activations on old task inputs and preserves model behavior on past tasks. To further balance stability and adaptability, we design an Adaptive Relaxation Strategy, which dynamically adjusts the constraint strength based on interference signals captured during test-time adaptation."], "score": 0.82666015625}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276575632, "title": "Low-Rank and Sparse Model Merging for Multi-Lingual Speech Recognition and Translation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Qiuming Zhao", "authorId": "2241702793"}, {"name": "Guangzhi Sun", "authorId": "2107310187"}, {"name": "Chao Zhang", "authorId": "2256775692"}, {"name": "Mingxing Xu", "authorId": "2241950375"}, {"name": "Thomas Fang Zheng", "authorId": "2241350908"}], "n_citations": 1}, "snippets": ["We propose LoRS-Merging (low-rank and sparse model merging), a novel technique designed to efficiently integrate models trained on different languages or tasks while preserving performance and reducing computational overhead. LoRS-Merging combines low-rank and sparse pruning to retain essential structures while eliminating redundant parameters, mitigating language and task interference, and enhancing extensibility."], "score": 0.68603515625}, {"id": "(Pourreza et al., 2024)", "paper": {"corpus_id": 271947337, "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mohammadreza Pourreza", "authorId": "2192284724"}, {"name": "Ruoxi Sun", "authorId": "2068169921"}, {"name": "Hailong Li", "authorId": "2316892037"}, {"name": "Lesly Miculicich", "authorId": "71436125"}, {"name": "Tomas Pfister", "authorId": "2264567300"}, {"name": "Sercan \u00d6. Arik", "authorId": "2676352"}], "n_citations": 9}, "snippets": ["Model merging (Goddard et al., 2024). Initial approaches to model merging, such as Task Arithmetic (Ilharco et al., 2022), involve calculating task-specific vectors by determining the weight differences between the fine-tuned model and its base counterpart. These vectors are then linearly combined and reintegrated with the original base model. Subsequent methodologies like DARE, TIES, and Model BreadCrumbs (Davari & Belilovsky, 2023;(Yadav et al., 2023)(Yu et al., 2023) have aimed to minimize interference among task-specific models through techniques such as sparsification, sign consensus algorithms, and the exclusion of extreme values", "More recently, the integration of model merging with Mixture of Experts (MoE) architectures has been explored. This method, termed FrankenMoEs, initializes MoE MLP layers using weights from task-specific models (Goddard, 2024;Tang et al., 2024). Our work extends these efforts by specifically leveraging features from dialect-specific models for gate initialization and merging self-attention sublayers within transformer architectures."], "score": 0.71240234375}, {"id": "(Rousset et al., 2025)", "paper": {"corpus_id": 276422131, "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Thibault Rousset", "authorId": "2345924598"}, {"name": "Taisei Kakibuchi", "authorId": "2326795128"}, {"name": "Yusuke Sasaki", "authorId": "2346085934"}, {"name": "Yoshihide Nomura", "authorId": "2345925100"}], "n_citations": 1}, "snippets": ["Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance."], "score": 0.81884765625}], "table": null}, {"title": "Practical Challenges and Solutions", "tldr": "Model merging and integration approaches face significant practical challenges including parameter interference, architectural incompatibility, and computational complexity. Various solutions have emerged to address these issues, ranging from interference reduction techniques and automated merging approaches to efficient routing mechanisms that balance performance with resource constraints. (16 sources)", "text": "\nDespite their theoretical promise, model merging and integration techniques face numerous practical challenges that can limit their effectiveness in real-world applications. One of the most significant challenges is parameter interference between models trained on different domains, which often sacrifices task-specific performance and creates a substantial gap compared to individual expert models <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>. This interference stems primarily from redundant parameter values and disagreements on parameter signs across models, which can substantially deteriorate performance in the unified system <Paper corpusId=\"270702345\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper>.\n\nArchitectural compatibility presents another major challenge, particularly when attempting to merge models with divergent architectures or specialized capabilities. This challenge has spurred research into new techniques for mitigating parameter interference, developing innovative routing heuristics to reduce fine-tuning requirements, and creating novel methods for merging experts with different architectures <Paper corpusId=\"276095183\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>.\n\nThe computational complexity of merging approaches varies significantly, creating a spectrum of trade-offs between performance and resource efficiency. Model merging techniques can be broadly categorized as manual and automated, with further distinctions between data-free and data-informed methods <Paper corpusId=\"273323680\" paperTitle=\"(Gauthier-Caron et al., 2024)\" isShortName></Paper>. Manual, data-free methods like Model Soups <Paper corpusId=\"247362886\" paperTitle=\"(Wortsman et al., 2022)\" isShortName></Paper> and TIES-Merging <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> focus on merging model parameters directly without relying on data, making them computationally efficient but requiring manual tuning that can limit scalability. In contrast, automated, data-informed methods utilize representative data to optimize parameter adjustments, reducing the need for manual tuning but typically demanding more computational resources <Paper corpusId=\"273323680\" paperTitle=\"(Gauthier-Caron et al., 2024)\" isShortName></Paper>.\n\nFor MoE architectures, compression approaches have emerged to address efficiency concerns, broadly categorized into expert pruning and expert merging methods <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>. Expert pruning approaches implement inter-expert pruning and intra-expert weight sparsification, achieving significant parameter reduction but often resulting in substantial performance degradation due to irreversible loss of expert knowledge. Expert merging methods, conversely, consolidate multiple experts into fewer, more compact representations through weighted summation of expert weights, preserving more information but introducing new challenges when experts possess distinct, complementary specializations <Paper corpusId=\"276575054\" paperTitle=\"(Gu et al., 2025)\" isShortName></Paper>.\n\nMemory consumption presents another practical challenge, particularly for approaches that maintain multiple models. Ensemble methods enhance performance and robustness by combining predictions from multiple models but require all models to remain active during inference, leading to substantial computational and memory costs <Paper corpusId=\"276813020\" paperTitle=\"(Yang et al._1, 2025)\" isShortName></Paper> <Paper corpusId=\"259075564\" paperTitle=\"(Jiang et al., 2023)\" isShortName></Paper>. Similarly, task-specific routing methods introduce substantial storage overhead by necessitating the preservation of all task vectors to ensure task relevance and performance <Paper corpusId=\"276409347\" paperTitle=\"(Liu et al._1, 2025)\" isShortName></Paper>.\n\nTo address these challenges, numerous solutions have been developed:\n\n1. For parameter interference, TIES-Merging introduced a three-step process: resetting parameters that changed minimally during fine-tuning, resolving sign conflicts through majority voting, and merging only parameters that align with the agreed-upon sign <Paper corpusId=\"259064039\" paperTitle=\"(Yadav et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\n2. DARE addresses parameter redundancy by randomly dropping delta parameters and rescaling the remaining ones, effectively eliminating 90-99% of delta parameters without significant performance loss <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper> <Paper corpusId=\"265034087\" paperTitle=\"(Yu et al., 2023)\" isShortName></Paper>.\n\n3. For balancing performance with computational efficiency, router-based approaches like Route LLM select the most appropriate models based on query difficulty and desired quality levels, reducing costs by over 2 times in some cases without compromising response quality <Paper corpusId=\"270764307\" paperTitle=\"(Ong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269303119\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>.\n\n4. Representation Surgery addresses representation bias in multitask scenarios by inserting lightweight, task-specific modules that realign the merged model's internal representations with those of individual models, enhancing overall performance <Paper corpusId=\"277940324\" paperTitle=\"(Coleman et al., 2025)\" isShortName></Paper>.\n\n5. For continual learning scenarios, model merging approaches integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods <Paper corpusId=\"276580914\" paperTitle=\"(Feng et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271915471\" paperTitle=\"(Dou et al., 2024)\" isShortName></Paper>.\n\nEffective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency <Paper corpusId=\"276422131\" paperTitle=\"(Rousset et al., 2025)\" isShortName></Paper>. As research progresses, the focus continues to shift toward developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance, offering promising directions for creating unified models that effectively leverage the strengths of specialized components.", "citations": [{"id": "(Lu et al., 2024)", "paper": {"corpus_id": 270702345, "title": "Twin-Merging: Dynamic Integration of Modular Expertise in Model Merging", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhenyi Lu", "authorId": "2262512474"}, {"name": "Chenghao Fan", "authorId": "2277238906"}, {"name": "Wei Wei", "authorId": "2284721764"}, {"name": "Xiaoye Qu", "authorId": "2262446609"}, {"name": "Dangyang Chen", "authorId": "2182623368"}, {"name": "Yu Cheng", "authorId": "2284687448"}], "n_citations": 63}, "snippets": ["However, merging models from different domains often sacrifices specific task performance, leading to a large performance gap compared to the individual expert (Jiang et al., 2023)(Yadav et al., 2023). Two major causes prevent the existing merging methods from reaching the theoretical upper-bound performance of individual experts: (1) Interference between models."], "score": 0.732421875}, {"id": "(Yadav et al., 2023)", "paper": {"corpus_id": 259064039, "title": "TIES-Merging: Resolving Interference When Merging Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Prateek Yadav", "authorId": "46841632"}, {"name": "Derek Tam", "authorId": "1390031652"}, {"name": "Leshem Choshen", "authorId": "41019330"}, {"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 317}, "snippets": ["Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To address this, we propose our method, TRIM, ELECT SIGN&MERGE (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms several existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, and highlight the importance of resolving sign interference. Our code is available at https://github.com/prateeky2806/ties-merging"], "score": 0.0}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 276095183, "title": "MergeME: Model Merging Techniques for Homogeneous and Heterogeneous MoEs", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yuhang Zhou", "authorId": "2266789873"}, {"name": "Giannis Karamanolakis", "authorId": "8458211"}, {"name": "Victor Soto", "authorId": "2302332301"}, {"name": "Anna Rumshisky", "authorId": "1681193"}, {"name": "Mayank Kulkarni", "authorId": "2302332615"}, {"name": "Furong Huang", "authorId": "2257407889"}, {"name": "Wei Ai", "authorId": "2218202090"}, {"name": "Jianhua Lu", "authorId": "2302633316"}], "n_citations": 3}, "snippets": ["The recent success of specialized Large Language Models (LLMs) in domains such as mathematical reasoning and coding has led to growing interest in methods for merging these expert LLMs into a unified Mixture-of-Experts (MoE) model, with the goal of enhancing performance in each domain while retaining effectiveness on general tasks. However, the effective merging of expert models remains an open challenge, especially for models with highly divergent weight parameters or different architectures. State-of-the-art MoE merging methods only work with homogeneous model architectures and rely on simple unweighted averaging to merge expert layers, which does not address parameter interference and requires extensive fine-tuning of the merged MoE to restore performance. To address these limitations, this paper introduces new MoE merging techniques, including strategies to mitigate parameter interference, routing heuristics to reduce the need for MoE fine-tuning, and a novel method for merging experts with different architectures."], "score": 0.859375}, {"id": "(Gauthier-Caron et al., 2024)", "paper": {"corpus_id": 273323680, "title": "Merging in a Bottle: Differentiable Adaptive Merging (DAM) and the Path from Averaging to Automation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Thomas Gauthier-Caron", "authorId": "2325729401"}, {"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Elliot Stein", "authorId": "2325730893"}, {"name": "Malikeh Ehghaghi", "authorId": "2175482685"}, {"name": "Charles Goddard", "authorId": "2292260669"}, {"name": "Mark McQuade", "authorId": "2292260070"}, {"name": "Jacob Solawetz", "authorId": "2047397646"}, {"name": "Maxime Labonne", "authorId": "2325729518"}], "n_citations": 2}, "snippets": ["Model merging techniques can be divided into two primary categories: manual and automated, and further distinguished by whether they are datafree or data-informed. Manual, data-free methods such as Model Soups (Wortsman et al., 2022), Trim, Elect, Sign, & Merge (TIES-Merging) (Yadav et al., 2023) or Spherical Linear intERPolation (SLERP)2 focus on merging model parameters directly without any reliance on data, making them computationally efficient but requiring manual tuning, which can limit scalability. \n\nAutomated, data-informed methods like AdaMerging (Yang et al., 2023) and evolutionary model merging (Akiba et al., 2024) utilize representative data to inform and optimize parameter adjustments. This approach supports fine-grained control, such as per-layer or per-feature adjustments, reducing the need for manual tuning and improving performance on complex tasks. However, these automated methods typically demand more computational resources and may be impractical in scale."], "score": 0.6640625}, {"id": "(Wortsman et al., 2022)", "paper": {"corpus_id": 247362886, "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time", "year": 2022, "venue": "International Conference on Machine Learning", "authors": [{"name": "Mitchell Wortsman", "authorId": "52193502"}, {"name": "Gabriel Ilharco", "authorId": "1387994137"}, {"name": "S. Gadre", "authorId": "1387466862"}, {"name": "R. Roelofs", "authorId": "40458654"}, {"name": "Raphael Gontijo-Lopes", "authorId": "2158366935"}, {"name": "Ari S. Morcos", "authorId": "4690624"}, {"name": "Hongseok Namkoong", "authorId": "40281109"}, {"name": "Ali Farhadi", "authorId": "143787583"}, {"name": "Y. Carmon", "authorId": "2444742"}, {"name": "Simon Kornblith", "authorId": "40464924"}, {"name": "Ludwig Schmidt", "authorId": "152772922"}], "n_citations": 1011}, "snippets": ["The conventional recipe for maximizing model accuracy is to (1) train multiple models with various hyperparameters and (2) pick the individual model which performs best on a held-out validation set, discarding the remainder. In this paper, we revisit the second step of this procedure in the context of fine-tuning large pre-trained models, where fine-tuned models often appear to lie in a single low error basin. We show that averaging the weights of multiple models fine-tuned with different hyperparameter configurations often improves accuracy and robustness. Unlike a conventional ensemble, we may average many models without incurring any additional inference or memory costs -- we call the results\"model soups.\"When fine-tuning large pre-trained models such as CLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides significant improvements over the best model in a hyperparameter sweep on ImageNet. The resulting ViT-G model, which attains 90.94% top-1 accuracy on ImageNet, achieved a new state of the art. Furthermore, we show that the model soup approach extends to multiple image classification and natural language processing tasks, improves out-of-distribution performance, and improves zero-shot performance on new downstream tasks. Finally, we analytically relate the performance similarity of weight-averaging and logit-ensembling to flatness of the loss and confidence of the predictions, and validate this relation empirically. Code is available at https://github.com/mlfoundations/model-soups."], "score": 0.0}, {"id": "(Gu et al., 2025)", "paper": {"corpus_id": 276575054, "title": "Delta Decompression for MoE-based LLMs Compression", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hao Gu", "authorId": "2347571185"}, {"name": "Wei Li", "authorId": "2331681523"}, {"name": "Lujun Li", "authorId": "2331723310"}, {"name": "Qi Zhu", "authorId": "2313367125"}, {"name": "Mark Lee", "authorId": "2331702843"}, {"name": "Shengjie Sun", "authorId": "2331691577"}, {"name": "Wei Xue", "authorId": "2239201089"}, {"name": "Yi-Ting Guo", "authorId": "2118270918"}], "n_citations": 3}, "snippets": ["To address these challenges, MoE compression methods have recently gained significant attention. As illustrated in Table 1, current approaches broadly categorized into expert pruning and expert merging methods.\n\n(1) Expert pruning approaches, represented by MoE-Pruner (Xie et al., 2024), NAEE (Lu et al., 2024a), and MoE-I 2 (Yang et al., 2024), implement inter-expert pruning and intra-expert weight sparsification. While these approaches achieve significant parameter reduction, they often result in substantial performance degradation due to the irreversible loss of expert knowledge. The direct removal of expert weights compromises the model's specialized capabilities, frequently necessitating additional fine-tuning to partially recover performance.\n\n(2) Expert merging methods, on the other hand, aim to consolidate multiple experts into fewer, more compact representations. Methods like EEP (Liu et al., 2024a), MC-SMoE (Li et al., 2023a), and HC-SMoE (Chen et al., 2024) develop various weighting schemes for weighted summation of different experts' weights. While these approaches preserve more information than direct pruning, it introduces new challenges. The merging process assumes significant overlap in expert functionalities, but in practice, experts often possess distinct, complementary specializations."], "score": 0.72314453125}, {"id": "(Yang et al._1, 2025)", "paper": {"corpus_id": 276813020, "title": "FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyi Yang", "authorId": "2287297309"}, {"name": "Fanqi Wan", "authorId": "2217614543"}, {"name": "Longguang Zhong", "authorId": "2286975236"}, {"name": "Canbin Huang", "authorId": "2258677979"}, {"name": "Guosheng Liang", "authorId": "2348918304"}, {"name": "Xiaojun Quan", "authorId": "2258552983"}], "n_citations": 3}, "snippets": ["Various strategies have been developed to achieve this, each with unique trade-offs. Ensemble methods (Jiang et al., 2023)(Wang et al., 2024) enhance performance and robustness by combining predictions from multiple models. However, they require all models to remain active during inference, leading to substantial computational and memory costs. LLM routing (Ding et al., 2024)Hu et al., 2024;(Ong et al., 2024) offers a more efficient alternative: a router selects the most appropriate LLM to handle each query. While this balances effectiveness and efficiency, it requires training a new router for each task, limiting its generalization to unseen tasks. Model merging (Wortsman et al., 2022) integrates models with identical architectures into a unified parameter set, improving robustness and generalization but limiting applicability to homogeneous model families. Explicit model fusion (EMF) methods (Wan et al., 2024a;b) use knowledge distillation to transfer knowledge from multiple source models to a single target model, often through probabilistic distribution matrices. While adaptable to different model structures and sizes, EMF faces challenges like vocabulary alignment and distribution merging, which can complicate the fusion process and introduce errors."], "score": 0.77587890625}, {"id": "(Jiang et al., 2023)", "paper": {"corpus_id": 259075564, "title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Dongfu Jiang", "authorId": "2197076899"}, {"name": "Xiang Ren", "authorId": "1384550891"}, {"name": "Bill Yuchen Lin", "authorId": "51583409"}], "n_citations": 333}, "snippets": ["We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly outperform individual LLMs and baseline methods across various metrics, establishing a substantial performance gap."], "score": 0.0}, {"id": "(Liu et al._1, 2025)", "paper": {"corpus_id": 276409347, "title": "1bit-Merging: Dynamic Quantized Merging for Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Shuqi Liu", "authorId": "2305720492"}, {"name": "Han Wu", "authorId": "2346255376"}, {"name": "Bowei He", "authorId": "2276605422"}, {"name": "Zehua Liu", "authorId": "2333317068"}, {"name": "Xiongwei Han", "authorId": "2148635550"}, {"name": "Mingxuan Yuan", "authorId": "2347282055"}, {"name": "Linqi Song", "authorId": "2257556686"}], "n_citations": 2}, "snippets": ["Recent advances in parameter-space model merging (Wortsman et al., 2022;Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) provide an efficient alternative -by directly operating on model parameters, these methods preserve data privacy and eliminate the need for expensive retraining.\n\nTraditional model merging approaches (Ilharco et al., 2023;Yadav et al., 2023;Yu et al., 2024b) typically combine the parameters of multiple finetuned models, or expert models, into a single static model without additional training, thereby enabling efficient multi-task functionality. However, merging models from different domains often sacrifices task-specific performance, resulting in a noticeable gap compared to individual expert models. In contrast, merging with task-specific routing (Muqeeth et al., 2024;Lu et al., 2024) dynamically prioritizes relevant task vectors based on input data, effectively maintaining accuracy by isolating taskspecific parameters. However, this routing-based merging strategy introduces substantial storage overhead, as it necessitates the preservation of all task vectors to ensure task relevance and performance. Thus, despite their ability to uphold model accuracy, task-specific routing methods face severe storage challenges, limiting their scalability and practicality in resource-constrained environments."], "score": 0.69580078125}, {"id": "(Coleman et al., 2025)", "paper": {"corpus_id": 277940324, "title": "Parameter-Efficient Continual Fine-Tuning: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Eric Nuertey Coleman", "authorId": "2244619371"}, {"name": "Luigi Quarantiello", "authorId": "2223689402"}, {"name": "Ziyue Liu", "authorId": "2356575441"}, {"name": "Qinwen Yang", "authorId": "2356596632"}, {"name": "Samrat Mukherjee", "authorId": "2356499006"}, {"name": "J. Hurtado", "authorId": "2064859104"}, {"name": "Vincenzo Lomonaco", "authorId": "2285835294"}], "n_citations": 0}, "snippets": ["Model merging presents an exciting opportunity in CL, by combining multiple expert models, each specialized in different aspects of a task, we can create a system that not only mitigates issues like catastrophic forgetting but also benefits from the diverse strengths of each model. This becomes particularly important in dynamic, evolving domains, as it allows the model to expand its knowledge over time without forgetting what it has previously learned. However, a common problem that these model merging solutions, such as Task Arithmetic [55], often encounter is parameter interference, which leads to significant performance degradation when these expert models are merged. Some works such as TIES-MERGING [148] and DARE [152] have led to significant improvements in model merging. [148] addresses interference by resetting parameters that have only changed minimally, resolving sign conflicts, and merging only those parameters that align with the final agreed-upon sign. [152], on the other hand, eliminates redundant delta parameters by randomly dropping them and rescaling the remaining ones, which has shown tremendous effectiveness in sparsifying and merging multiple expert models without significant performance loss", "For instance, Mag-Max [99] introduces a framework that merges task-specific models using sequential fine-tuning combined with a maximum magnitude weight selection strategy. This approach integrates new information effectively and preserves the integrity of earlier learning to help tackle CT. In contrast, Representation Surgery for Multitask Model Learning [149] addresses a different challenge. Here, the focus is on mitigating the representation bias that emerges when merging models trained on disparate tasks. By inserting a lightweight, task-specific module-dubbed \"Surgery\"-the method realigns the merged model's internal representations with those of the individual models, thereby enhancing overall performance in multitask scenarios."], "score": 0.69091796875}, {"id": "(Yu et al., 2023)", "paper": {"corpus_id": 265034087, "title": "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Le Yu", "authorId": "2265527327"}, {"name": "Yu Bowen", "authorId": "48613402"}, {"name": "Haiyang Yu", "authorId": "46493167"}, {"name": "Fei Huang", "authorId": "2257407873"}, {"name": "Yongbin Li", "authorId": "1527090216"}], "n_citations": 335}, "snippets": ["In this paper, we unveil that Language Models (LMs) can acquire new capabilities by assimilating parameters from homologous models without retraining or GPUs. We first introduce DARE to set most delta parameters (i.e., the disparity between fine-tuned and pre-trained parameters) to zeros without affecting the abilities of Supervised Fine-Tuning (SFT) LMs, which randomly Drops delta parameters with a ratio $p$ And REscales the remaining ones by $1 / (1 - p)$ to approximate the original embeddings. Then, we use DARE as a versatile plug-in to sparsify delta parameters of multiple SFT homologous models for mitigating parameter interference and merge them into a single model by parameter fusing. We experiment with encoder- and decoder-based LMs, showing that: (1) SFT delta parameter value ranges are typically small (within 0.002) with extreme redundancy, and DARE can effortlessly eliminate 90% or even 99% of them; (2) DARE can merge multiple task-specific LMs into one LM with diverse capabilities. Notably, this phenomenon is more pronounced in large-scale LMs, where the merged LM reveals the potential to surpass the performance of any source LM, providing a new discovery. We also utilize DARE to create a merged LM that ranks first among models with 7 billion parameters on the Open LLM Leaderboard."], "score": 0.0}, {"id": "(Ong et al., 2024)", "paper": {"corpus_id": 270764307, "title": "RouteLLM: Learning to Route LLMs with Preference Data", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isaac Ong", "authorId": "2199840167"}, {"name": "Amjad Almahairi", "authorId": "2634674"}, {"name": "Vincent Wu", "authorId": "2308466924"}, {"name": "Wei-Lin Chiang", "authorId": "2537924"}, {"name": "Tianhao Wu", "authorId": "2251528079"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "M. W. Kadous", "authorId": "1793159"}, {"name": "Ion Stoica", "authorId": "2055174324"}], "n_citations": 104}, "snippets": ["Large language models (LLMs) exhibit impressive capabilities across a wide range of tasks, yet the choice of which model to use often involves a trade-off between performance and cost. More powerful models, though effective, come with higher expenses, while less capable models are more cost-effective. To address this dilemma, we propose several efficient router models that dynamically select between a stronger and a weaker LLM during inference, aiming to optimize the balance between cost and response quality. We develop a training framework for these routers leveraging human preference data and data augmentation techniques to enhance performance. Our evaluation on widely-recognized benchmarks shows that our approach significantly reduces costs-by over 2 times in certain cases-without compromising the quality of responses. Interestingly, our router models also demonstrate significant transfer learning capabilities, maintaining their performance even when the strong and weak models are changed at test time. This highlights the potential of these routers to provide a cost-effective yet high-performance solution for deploying LLMs."], "score": 0.0}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 269303119, "title": "Hybrid LLM: Cost-Efficient and Quality-Aware Query Routing", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Dujian Ding", "authorId": "123966440"}, {"name": "Ankur Mallick", "authorId": "2297849625"}, {"name": "Chi Wang", "authorId": "2298452007"}, {"name": "Robert Sim", "authorId": "2253669181"}, {"name": "Subhabrata Mukherjee", "authorId": "2153292652"}, {"name": "Victor R\u00fchle", "authorId": "3898805"}, {"name": "L. Lakshmanan", "authorId": "1708593"}, {"name": "A. Awadallah", "authorId": "2072795428"}], "n_citations": 106}, "snippets": ["Large language models (LLMs) excel in most NLP tasks but also require expensive cloud servers for deployment due to their size, while smaller models that can be deployed on lower cost (e.g., edge) devices, tend to lag behind in terms of response quality. Therefore in this work we propose a hybrid inference approach which combines their respective strengths to save cost and maintain quality. Our approach uses a router that assigns queries to the small or large model based on the predicted query difficulty and the desired quality level. The desired quality level can be tuned dynamically at test time to seamlessly trade quality for cost as per the scenario requirements. In experiments our approach allows us to make up to 40% fewer calls to the large model, with no drop in response quality."], "score": 0.0}, {"id": "(Feng et al., 2025)", "paper": {"corpus_id": 276580914, "title": "Recurrent Knowledge Identification and Fusion for Language Model Continual Learning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yujie Feng", "authorId": "2261661979"}, {"name": "Xujia Wang", "authorId": "2347170204"}, {"name": "Zexin Lu", "authorId": "2220034673"}, {"name": "Shenghong Fu", "authorId": "2347197165"}, {"name": "Guangyuan Shi", "authorId": "144218801"}, {"name": "Yongxin Xu", "authorId": "2215472732"}, {"name": "Yasha Wang", "authorId": "2253831765"}, {"name": "Philip S. Yu", "authorId": "2348327959"}, {"name": "Xu Chu", "authorId": "2315809919"}, {"name": "Xiao-Ming Wu", "authorId": "2261456593"}], "n_citations": 1}, "snippets": ["Recently, model mixture-based methods have emerged as a mainstream approach for CL in LLMs (Chen et al., 2023)Wu et al., 2024a;Rype\u015b\u0107 et al., 2024). By leveraging parameter-efficient finetuning (PEFT) techniques, which reduce the computational burden, these methods can be broadly classified into two categories: model ensemble and model merging. Model ensemble methods assign a dedicated PEFT block to each task, capturing task-specific knowledge, which is then stored in a pool and dynamically selected during inference (Zhu et al., 2024)Wang et al., 2024c). While effective, these methods require storing all task-specific models, leading to high memory consumption that grows with the number of tasks, which limits their scalability for long task sequences. \n\nAnother line of research focuses on model merging approaches (Dou et al., 2024)Wan et al., 2024;Yadav et al., 2024a), which integrate new task knowledge after training into the historical model, maintaining a single unified model and reducing memory costs compared to model ensemble methods. Consequently, our work primarily focuses on model merging approaches. However, determining which parameters to merge and how to merge remains an open challenge (Qin et al., 2024)."], "score": 0.6591796875}, {"id": "(Dou et al., 2024)", "paper": {"corpus_id": 271915471, "title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shihan Dou", "authorId": "2042683163"}, {"name": "Enyu Zhou", "authorId": "2240446306"}, {"name": "Yan Liu", "authorId": "2275033850"}, {"name": "Songyang Gao", "authorId": "2181306462"}, {"name": "Wei Shen", "authorId": "2248291262"}, {"name": "Limao Xiong", "authorId": "2222630539"}, {"name": "Yuhao Zhou", "authorId": "2212175381"}, {"name": "Xiao Wang", "authorId": "2118451107"}, {"name": "Zhiheng Xi", "authorId": "2218237934"}, {"name": "Xiaoran Fan", "authorId": "2241140630"}, {"name": "Shiliang Pu", "authorId": "2274941422"}, {"name": "Jiang Zhu", "authorId": "2277702055"}, {"name": "Rui Zheng", "authorId": "2058585152"}, {"name": "Tao Gui", "authorId": "2067331064"}, {"name": "Qi Zhang", "authorId": "2256972399"}, {"name": "Xuanjing Huang", "authorId": "2257129989"}], "n_citations": 55}, "snippets": ["Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novel framework that introduces several low-rank adapters (LoRA) and integrates them by us-ing a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of Lo-RAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, Lo-RAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/ Ablustrund/LoRAMoE ."], "score": 0.0}, {"id": "(Rousset et al., 2025)", "paper": {"corpus_id": 276422131, "title": "Merging Language and Domain Specific Models: The Impact on Technical Vocabulary Acquisition", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Thibault Rousset", "authorId": "2345924598"}, {"name": "Taisei Kakibuchi", "authorId": "2326795128"}, {"name": "Yusuke Sasaki", "authorId": "2346085934"}, {"name": "Yoshihide Nomura", "authorId": "2345925100"}], "n_citations": 1}, "snippets": ["Model merging methods, widely used across diverse fields within NLP, are increasingly employed for LLM domain adaptation [3]. This approach involves combining the strengths of multiple models -often a general-purpose LLM with one or more domain-specific models -to enhance performance in a targeted domain. The aim is to leverage the broad knowledge base of the general LLM while incorporating the specialized expertise of the domain models, creating a hybrid system that surpasses the capabilities of its individual components. However, effective model merging requires careful consideration of model compatibility, potential knowledge interference, and computational efficiency. Ongoing research focuses on developing optimal merging strategies and addressing the complexities of integrating diverse knowledge sources without compromising overall model performance."], "score": 0.81884765625}], "table": null}], "cost": 0.640215}}
