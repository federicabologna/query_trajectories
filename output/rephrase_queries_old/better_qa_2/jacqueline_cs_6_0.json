{"clarifying_information": [{"clarifying_question1": "Are you interested in specific types of social bias in large language models (e.g., gender, racial, religious, or disability bias), or are you looking for a general overview of all social biases present in these models?", "clarifying_answer1": {"clarifying_answer": "I am interested in a general overview of all social biases present in large language models, not limited to specific types."}}, {"clarifying_question2": "Do you want a detailed comparison of recent post-training mitigation techniques (such as fine-tuning, prompt-based methods, or model editing) and their effectiveness in reducing specific stereotypes or biases in language models?", "clarifying_answer2": {"clarifying_answer": "Yes, please provide a detailed comparison of recent post-training mitigation techniques, such as fine-tuning, prompt-based methods, and model editing, especially with respect to their effectiveness in reducing specific stereotypes or biases in language models."}}, {"clarifying_question3": "Are you seeking information primarily about the real-world impacts and harms caused by social bias in large language models, or about the technical challenges and limitations faced in mitigating these biases?", "clarifying_answer3": {"clarifying_answer": "I am seeking information primarily about the real-world impacts and harms caused by social bias in large language models."}}], "better_query": "Provide a comprehensive overview of the various forms of social bias (e.g., gender, racial, religious, disability, and stigma-related) present in large language models, including how these biases originate from training data and manifest across downstream applications.", "better_answer": {"sections": [{"title": "Introduction and Background on Social Bias in LLMs", "tldr": "Large Language Models (LLMs) exhibit various forms of social bias that originate from their training data and can perpetuate or amplify societal prejudices and stereotypes. These biases have significant implications for fairness and can lead to harmful outcomes for marginalized groups across numerous applications. (19 sources)", "text": "\nSocial bias in Large Language Models (LLMs) has emerged as a critical concern as these powerful systems become increasingly integrated into various aspects of society. LLMs, such as BERT, GPT-3, RoBERTa, and BLOOM, are trained on vast amounts of text data collected from the internet and other sources, inadvertently inheriting and sometimes amplifying the biases present in these datasets <Paper corpusId=\"248366292\" paperTitle=\"(Kadan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. These biases reflect historical inequalities, prejudices, and stereotypes encoded in human-generated texts <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nAt its core, social bias in LLMs can be defined as \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" that manifest through language <Paper corpusId=\"265212726\" paperTitle=\"(Manerba et al., 2023)\" isShortName></Paper> <Paper corpusId=\"258688053\" paperTitle=\"(Navigli et al., 2023)\" isShortName></Paper>. Research has consistently demonstrated that language models trained on large corpora of human-generated text encode human-like social biases <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. These biases are particularly concerning because LLMs are deployed in numerous downstream applications, where they can propagate biases and produce unfair outcomes for specific groups <Paper corpusId=\"259095603\" paperTitle=\"(Thakur et al., 2023)\" isShortName></Paper>.\n\nThe sources of bias in LLMs are multifaceted, including dataset bias, embedding bias, and label bias <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>. Dataset bias stems from the unbalanced and skewed nature of training data, which often overrepresents hegemonic views while underrepresenting marginalized perspectives <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper> <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>. Internet access is not evenly distributed across society, and content filtering processes may disproportionately remove language used by marginalized groups <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. Additionally, the scale of LLM training datasets makes them impossible to thoroughly document, further complicating bias mitigation efforts <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nSocial biases in LLMs can result in two primary forms of harm: representational and allocational <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper>. Representational harms occur when models portray certain groups negatively or fail to represent them adequately, while allocational harms involve denying opportunities or resources to specific groups <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>. These harms can have far-reaching consequences in critical domains such as healthcare, employment, and law enforcement <Paper corpusId=\"271745282\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper> <Paper corpusId=\"121125604\" paperTitle=\"(Basta et al., 2019)\" isShortName></Paper>.\n\nVarious types of social bias have been documented in LLMs, including gender, racial, religious, and disability-related biases <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper>. Research has shown that models like GPT-2 and GPT-3 exhibit biases against various demographic groups, and these biases can be exacerbated in dialogue generation models trained on unchecked user-generated content from social media <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. More recently, researchers have begun to examine social stigmas related to disabilities, mental illness, and socioeconomic status, which affect a substantial portion of the population <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246654106\" paperTitle=\"(Meisel et al., 2022)\" isShortName></Paper>.\n\nAlarmingly, LLMs don't merely reflect the biases present in their training data but can amplify them <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper> <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. For example, studies have found that LLMs are significantly more likely to associate Muslims with violence compared to other religious groups <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>, and they consistently associate professions with stereotypical genders at rates far exceeding real-world statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>.\n\nThe issue of bias in LLMs has practical implications, as evidenced by real-world incidents. For instance, Amazon discontinued an AI recruiting tool after discovering it systematically downgraded resumes containing the term \"women's\" <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>, and Google suspended Gemini's image generation feature following reports of inaccuracies and biases <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>. These examples underscore the urgent need to address social bias in LLMs as they become increasingly integrated into systems that impact individuals' lives.\n\nAs research in this area continues to evolve, understanding the various forms, origins, and impacts of social bias in LLMs remains crucial for developing more fair and equitable AI systems <Paper corpusId=\"268553687\" paperTitle=\"(Cai et al., 2024)\" isShortName></Paper> <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Kadan et al., 2022)", "paper": {"corpus_id": 248366292, "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Anoop Kadan", "authorId": "144485480"}, {"name": "Manjary P.Gangan", "authorId": "3390280"}, {"name": "Deepak P", "authorId": "145791688"}, {"name": "L. LajishV.", "authorId": "151092338"}], "n_citations": 10}, "snippets": ["The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm."], "score": 0.88671875}, {"id": "(Kolisko et al., 2023)", "paper": {"corpus_id": 259716055, "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Skylar Kolisko", "authorId": "2222666109"}, {"name": "Carolyn Jane Anderson", "authorId": "144901955"}], "n_citations": 11}, "snippets": ["There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3"], "score": 0.826171875}, {"id": "(Wald et al., 2023)", "paper": {"corpus_id": 259075978, "title": "Exposing Bias in Online Communities through Large-Scale Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Celine Wald", "authorId": "2219550842"}, {"name": "Lukas Pfahler", "authorId": "32421394"}], "n_citations": 6}, "snippets": ["Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021)."], "score": 0.8486328125}, {"id": "(Manerba et al., 2023)", "paper": {"corpus_id": 265212726, "title": "Social Bias Probing: Fairness Benchmarking for Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Marta Marchiori Manerba", "authorId": "2121386115"}, {"name": "Karolina Sta\u0144czak", "authorId": "82563120"}, {"name": "Riccardo Guidotti", "authorId": "2257013371"}, {"name": "Isabelle Augenstein", "authorId": "1736067"}], "n_citations": 20}, "snippets": ["Social bias can be defined as the manifestation through language of \"prejudices, stereotypes, and discriminatory attitudes against certain groups of people\" (Navigli et al., 2023). These biases are featured in training datasets and are carried over into downstream applications, resulting in, for instance, classification errors concerning specific minorities and the generation of harmful content when models are prompted with sensitive identities (Cui et al., 2024;Gallegos et al., 2023)."], "score": 0.89697265625}, {"id": "(Navigli et al., 2023)", "paper": {"corpus_id": 258688053, "title": "Biases in Large Language Models: Origins, Inventory, and Discussion", "year": 2023, "venue": "ACM Journal of Data and Information Quality", "authors": [{"name": "Roberto Navigli", "authorId": "1733928"}, {"name": "Simone Conia", "authorId": "1396456007"}, {"name": "Bj\u00f6rn Ross", "authorId": "26234127"}], "n_citations": 283}, "snippets": ["In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias."], "score": 0.0}, {"id": "(Mei et al., 2023)", "paper": {"corpus_id": 259129801, "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "year": 2023, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Katelyn Mei", "authorId": "2189183000"}, {"name": "Sonia Fereidooni", "authorId": "2196943720"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 51}, "snippets": ["The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks", "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37]."], "score": 0.89599609375}, {"id": "(Caliskan et al., 2016)", "paper": {"corpus_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "year": 2016, "venue": "Science", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "J. Bryson", "authorId": "145315445"}, {"name": "Arvind Narayanan", "authorId": "47735253"}], "n_citations": 2673}, "snippets": ["Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."], "score": 0.0}, {"id": "(Thakur et al., 2023)", "paper": {"corpus_id": 259095603, "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Himanshu Thakur", "authorId": "2221493995"}, {"name": "Atishay Jain", "authorId": "1819271266"}, {"name": "Praneetha Vaddamanu", "authorId": "2127734657"}, {"name": "Paul Pu Liang", "authorId": "28130078"}, {"name": "Louis-philippe Morency", "authorId": "49933077"}], "n_citations": 37}, "snippets": ["Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people."], "score": 0.830078125}, {"id": "(Chu et al., 2024)", "paper": {"corpus_id": 268856702, "title": "Fairness in Large Language Models: A Taxonomic Survey", "year": 2024, "venue": "SIGKDD Explorations", "authors": [{"name": "Zhibo Chu", "authorId": "2283849596"}, {"name": "Zichong Wang", "authorId": "2206777360"}, {"name": "Wenbin Zhang", "authorId": "2243674300"}], "n_citations": 41}, "snippets": ["Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model."], "score": 0.81201171875}, {"id": "(Bender et al., 2021)", "paper": {"corpus_id": 262580630, "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c", "year": 2021, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Emily M. Bender", "authorId": "2471699"}, {"name": "Timnit Gebru", "authorId": "2076288"}, {"name": "Angelina McMillan-Major", "authorId": "1584940075"}, {"name": "Shmargaret Shmitchell", "authorId": "2246048745"}], "n_citations": 4657}, "snippets": ["The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."], "score": 0.0}, {"id": "(Doll et al., 2024)", "paper": {"corpus_id": 274117151, "title": "Evaluating Gender Bias in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Michael D\u00f6ll", "authorId": "2338278194"}, {"name": "Markus D\u00f6hring", "authorId": "2338277213"}, {"name": "Andreas M\u00fcller", "authorId": "2338675374"}], "n_citations": 1}, "snippets": ["LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."], "score": 0.8681640625}, {"id": "(Kumar et al., 2024)", "paper": {"corpus_id": 271745282, "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shachi H. Kumar", "authorId": "2109680564"}, {"name": "Saurav Sahay", "authorId": "38531701"}, {"name": "Sahisnu Mazumder", "authorId": "2066417452"}, {"name": "Eda Okur", "authorId": "3442103"}, {"name": "R. Manuvinakurike", "authorId": "2175808"}, {"name": "Nicole Beckage", "authorId": "2275457041"}, {"name": "Hsuan Su", "authorId": "2087042748"}, {"name": "Hung-yi Lee", "authorId": "2259815964"}, {"name": "L. Nachman", "authorId": "1896095"}], "n_citations": 18}, "snippets": ["Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023)."], "score": 0.80908203125}, {"id": "(Basta et al., 2019)", "paper": {"corpus_id": 121125604, "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings", "year": 2019, "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing", "authors": [{"name": "Christine Basta", "authorId": "73312674"}, {"name": "M. Costa-juss\u00e0", "authorId": "1398996347"}, {"name": "Noe Casas", "authorId": "1794731"}], "n_citations": 193}, "snippets": ["Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."], "score": 0.0}, {"id": "(Meisel et al., 2022)", "paper": {"corpus_id": 246654106, "title": "Education-based stigma and discrimination among young adults not in 4-year college", "year": 2022, "venue": "BMC Psychology", "authors": [{"name": "Matthew K. Meisel", "authorId": "3118881"}, {"name": "Michelle Haikalis", "authorId": "4609930"}, {"name": "S. Colby", "authorId": "4240161"}, {"name": "N. Barnett", "authorId": "143796191"}], "n_citations": 10}, "snippets": ["Background Lower levels of education are strongly associated with negative health outcomes. The current study examined the degree to which those without a history of 4-year college attendance experience social stigmatization of their educational status and if these experiences are associated with mental health symptoms. Methods Data was obtained from 488 emerging adults who never attended 4-year college using Qualtrics Panels. Results 79.4% of participants agreed to one of the six statements that not attending 4-year college is stigmatized, and 71.8% endorsed experiencing at least one form of discrimination. Higher levels of education-related stigma and more frequent experiences of education-related discrimination was associated with greater past-month anxiety and depression symptoms. Conclusions These findings could serve to increase awareness regarding the unique and significant discrimination faced by young adults who do not attend 4-year college and identify specific areas of intervention that can help these young adults cope with the effects of stigma and discrimination."], "score": 0.0}, {"id": "(Kotek et al., 2023)", "paper": {"corpus_id": 261276445, "title": "Gender bias and stereotypes in Large Language Models", "year": 2023, "venue": "International Conference on Climate Informatics", "authors": [{"name": "Hadas Kotek", "authorId": "3365389"}, {"name": "Rikker Dockum", "authorId": "90166394"}, {"name": "David Q. Sun", "authorId": "32100412"}], "n_citations": 236}, "snippets": ["Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."], "score": 0.0}, {"id": "(Abid et al., 2021)", "paper": {"corpus_id": 231603388, "title": "Persistent Anti-Muslim Bias in Large Language Models", "year": 2021, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Abubakar Abid", "authorId": "144948925"}, {"name": "Maheen Farooqi", "authorId": "77751476"}, {"name": "James Y. Zou", "authorId": "145085305"}], "n_citations": 555}, "snippets": ["It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."], "score": 0.0}, {"id": "(Mirza et al., 2024)", "paper": {"corpus_id": 272826949, "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vishal Mirza", "authorId": "2322445184"}, {"name": "Rahul Kulkarni", "authorId": "2322445481"}, {"name": "Aakanksha Jadhav", "authorId": "2322445728"}], "n_citations": 2}, "snippets": ["Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process."], "score": 0.82080078125}, {"id": "(Cai et al., 2024)", "paper": {"corpus_id": 268553687, "title": "Locating and Mitigating Gender Bias in Large Language Models", "year": 2024, "venue": "International Conference on Intelligent Computing", "authors": [{"name": "Yuchen Cai", "authorId": "2292681106"}, {"name": "Ding Cao", "authorId": "2237226610"}, {"name": "Rongxi Guo", "authorId": "2237261799"}, {"name": "Yaqin Wen", "authorId": "2293239741"}, {"name": "Guiquan Liu", "authorId": "2237403462"}, {"name": "Enhong Chen", "authorId": "2292390890"}], "n_citations": 5}, "snippets": ["Bias refers to the existence of consistent inaccuracies, misattributions, or erroneous perceptions leading to a preference for specific groups or concepts, the reinforcement of stereotypes, or the formation of flawed conclusions derived from established patterns, it now profoundly impact users and society at large, underscoring the urgent need for a comprehensive examination and mitigation of these issues [9]26]."], "score": 0.81640625}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 274130807, "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yufei Guo", "authorId": "2331613833"}, {"name": "Muzhe Guo", "authorId": "2331371661"}, {"name": "Juntao Su", "authorId": "2273060735"}, {"name": "Zhou Yang", "authorId": "2331371753"}, {"name": "Mengqiu Zhu", "authorId": "2331357933"}, {"name": "Hongfei Li", "authorId": "2331365682"}, {"name": "Mengyang Qiu", "authorId": "1455277182"}, {"name": "Shuo Shuo Liu", "authorId": "2331377869"}], "n_citations": 22}, "snippets": ["The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below."], "score": 0.9150390625}], "table": null}, {"title": "Types of Social Bias in LLMs", "tldr": "Large Language Models (LLMs) exhibit multiple forms of social bias related to gender, race, religion, disability, sexual orientation, age, nationality, physical appearance, and socioeconomic status. These biases manifest in various ways and can compound for individuals with intersectional identities, potentially causing both representational and allocational harms. (20 sources)", "text": "\n## Gender Bias\nGender bias remains one of the most extensively studied forms of bias in LLMs. Research has consistently shown that models associate professions and attributes with gender in stereotypical ways that often exceed real-world statistics <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper> <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>. For example, models tend to associate women with family and domestic roles while linking men with careers and leadership positions <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. Even contextual word embeddings like BERT and GPT-2 encode gender biases, though studies suggest these biases may be less pronounced than other forms of social bias <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper> <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Racial Bias\nRacial bias in LLMs manifests through associations between racial identities and stereotypical attributes or negative sentiments. Research has found that racial bias is encoded strongly in contextual word models, potentially even more strongly than gender bias <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. These biases can appear in downstream tasks such as sentiment analysis and toxicity prediction, where text mentioning certain racial groups receives more negative scores <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. Studies using tools like the Contextualized Embedding Association Test (CEAT) have confirmed that all tested English-language models contain biased representations related to race <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\n## Religious Bias\nReligious bias represents another significant dimension of social bias in LLMs. Research has revealed alarming associations, such as LLMs being significantly more likely to associate Muslims with violence compared to other religious groups <Paper corpusId=\"231603388\" paperTitle=\"(Abid et al., 2021)\" isShortName></Paper>. Muslim identity labels consistently receive more negative sentiment scores across various models <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. This bias appears to be more resistant to context than gender bias and persists even with increases in model size and training data diversity <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"266521434\" paperTitle=\"(Oketunji et al., 2023)\" isShortName></Paper>.\n\n## Disability-Related Bias\nResearch on disability-related bias has emerged more recently but shows concerning patterns. Models exhibit undesirable biases toward mentions of disability in tasks such as toxicity prediction and sentiment analysis <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Neural embeddings that form the foundation of most NLP pipelines contain biases toward disability mentions, with certain topics like gun violence, homelessness, and drug addiction being over-represented in texts discussing mental illness <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Disability labels consistently receive more negative sentiment scores across models, with only a few exceptions like \"with Down Syndrome\" <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>.\n\n## Intersectional Bias\nA growing body of research focuses on intersectional bias, where individuals at the intersection of multiple marginalized identities face unique biases. Studies have found that intersectional identities often suffer from stronger bias than their constituent minority identities alone <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. For instance, African American females and Mexican American females are associated with unique emergent biases that don't overlap with the biases of their constituent minority identities <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. Research has demonstrated that race appears to have a larger effect on intersectional identities than gender <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. Bias can manifest in unpredictable ways at category intersections, such as when combining gender, religion, and disability labels <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Other Forms of Social Bias\nBeyond these major categories, researchers are increasingly examining biases related to sexual orientation, age, nationality, physical appearance, and socioeconomic status <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266521434\" paperTitle=\"(Oketunji et al., 2023)\" isShortName></Paper> <Paper corpusId=\"270560549\" paperTitle=\"(Das et al., 2024)\" isShortName></Paper>. Social stigmas, which often relate to diseases, disabilities, mental illness, and socioeconomic status, affect a substantial portion of the population and are becoming a focus of bias research <Paper corpusId=\"259129801\" paperTitle=\"(Mei et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246654106\" paperTitle=\"(Meisel et al., 2022)\" isShortName></Paper>.\n\nRecent research has also expanded to examine biases in multimodal large language models (MLLMs), which can inherit severe social biases from their training datasets related to attributes like race and gender <Paper corpusId=\"271859735\" paperTitle=\"(Cheng et al., 2024)\" isShortName></Paper>. As the scope of bias research broadens, researchers are developing more comprehensive approaches to address multiple bias dimensions simultaneously rather than tackling them independently <Paper corpusId=\"267627928\" paperTitle=\"(Jain et al., 2024)\" isShortName></Paper>.\n\nThe various forms of social bias in LLMs can result in two primary forms of harm: representational and allocational <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>. Representational harms occur when models portray certain groups negatively or fail to represent them adequately, while allocational harms involve denying opportunities or resources to specific groups, such as in recruitment algorithms that may disadvantage certain demographic groups <Paper corpusId=\"259716055\" paperTitle=\"(Kolisko et al., 2023)\" isShortName></Paper> <Paper corpusId=\"259203321\" paperTitle=\"(Nemani et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Caliskan et al., 2022)", "paper": {"corpus_id": 249431834, "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics", "year": 2022, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "Pimparkar Parth Ajay", "authorId": "2168463879"}, {"name": "Tessa E. S. Charlesworth", "authorId": "66649568"}, {"name": "R. Wolfe", "authorId": "100508901"}, {"name": "M. Banaji", "authorId": "1968771"}], "n_citations": 51}, "snippets": ["Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)", "When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7])."], "score": 0.89306640625}, {"id": "(Bolukbasi et al., 2016)", "paper": {"corpus_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "year": 2016, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tolga Bolukbasi", "authorId": "2843215"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}, {"name": "James Y. Zou", "authorId": "145085305"}, {"name": "Venkatesh Saligrama", "authorId": "1699322"}, {"name": "A. Kalai", "authorId": "2186481"}], "n_citations": 3153}, "snippets": ["The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."], "score": 0.0}, {"id": "(Caliskan et al., 2016)", "paper": {"corpus_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "year": 2016, "venue": "Science", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "J. Bryson", "authorId": "145315445"}, {"name": "Arvind Narayanan", "authorId": "47735253"}], "n_citations": 2673}, "snippets": ["Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."], "score": 0.0}, {"id": "(Tan et al., 2019)", "paper": {"corpus_id": 202781363, "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Y. Tan", "authorId": "144787248"}, {"name": "Elisa Celis", "authorId": "47777961"}], "n_citations": 229}, "snippets": ["In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities", "we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias", "we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender."], "score": 0.83154296875}, {"id": "(Magee et al., 2021)", "paper": {"corpus_id": 236034024, "title": "Intersectional Bias in Causal Language Models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "L. Magee", "authorId": "2733075"}, {"name": "Lida Ghahremanlou", "authorId": "9557084"}, {"name": "K. Soldati\u0107", "authorId": "13714096"}, {"name": "S. Robertson", "authorId": "97868921"}], "n_citations": 33}, "snippets": ["To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label."], "score": 0.8779296875}, {"id": "(Guo et al., 2020)", "paper": {"corpus_id": 219530686, "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases", "year": 2020, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "W. Guo", "authorId": "28735341"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 243}, "snippets": ["We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models."], "score": 0.876953125}, {"id": "(Abid et al., 2021)", "paper": {"corpus_id": 231603388, "title": "Persistent Anti-Muslim Bias in Large Language Models", "year": 2021, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Abubakar Abid", "authorId": "144948925"}, {"name": "Maheen Farooqi", "authorId": "77751476"}, {"name": "James Y. Zou", "authorId": "145085305"}], "n_citations": 555}, "snippets": ["It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups."], "score": 0.0}, {"id": "(Oketunji et al., 2023)", "paper": {"corpus_id": 266521434, "title": "Large Language Model (LLM) Bias Index - LLMBI", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Abiodun Finbarrs Oketunji", "authorId": "2274104512"}, {"name": "Muhammad Anas", "authorId": "2276205421"}, {"name": "Deepthi Saina", "authorId": "2276202526"}], "n_citations": 12}, "snippets": ["The formulation of the Large Language Model Bias Index (LLMBI) necessitates a sweeping understanding of the multifaceted nature of bias in Large Language Models (LLMs). These biases, spanning across various dimensions such as gender, religion, race, age, nationality, disability, sexual orientation, physical appearance, and socioeconomic status, can signicantly impact the outputs of LLMs, leading to skewed and potentially harmful outcomes. \n\nGender bias in LLMs often manifests in stereotypical representations and unequal treatment of dierent genders. This bias can perpetuate gender stereotypes and discrimination, aecting the fairness of AI applications in areas like recruitment and social media (Zhao et al., 2017) (Zhao et al., 2017). Religion bias in LLMs can lead to the marginalisation of certain religious groups, fostering an environment of intolerance and misunderstanding (Aroyo et al., 2015) (Aroyo et al., 2015). \n\nRacial bias is another pressing concern, as LLMs can perpetuate racial stereotypes and prejudices. This bias can have severe implications in legal, healthcare, and nancial systems, where it can lead to discriminatory practices (Sweeney, 2013). Age bias in LLMs can result in underrepresenting or misrepresenting specic age groups, impacting sectors like healthcare and marketing (Levy, 2003) (Levy, 2003). \n\nNationality bias in LLMs can lead to the preferential treatment of certain nationalities over others, aecting global perceptions and international relations (Liao et al., 2018) (Liao et al., 2020). Disability bias in LLMs can result in the exclusion or misrepresentation of people with disabilities, impacting accessibility and inclusivity (Trewin et al., 2019) [14]. \n\nSexual orientation bias in LLMs can perpetuate harmful stereotypes and discrimination against LGBTQ+ individuals, aecting mental health and social acceptance (Costanza-Chock, 2018) (Costanza-Chock, 2018). Physical appearance bias in LLMs can lead to body shaming and unrealistic beauty standards, impacting mental health and self-esteem (Yee et al., 2007) (Yee et al., 2007). \n\nLastly, socioeconomic status bias in LLMs can reinforce class divisions and economic inequalities, aecting opportunities and social mobility (O'Neil, 2016) [17]."], "score": 0.81884765625}, {"id": "(Hutchinson et al., 2020)", "paper": {"corpus_id": 218487466, "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily L. Denton", "authorId": "40081727"}, {"name": "Kellie Webster", "authorId": "20825661"}, {"name": "Yu Zhong", "authorId": "2112887022"}, {"name": "Stephen Denuyl", "authorId": "1667883461"}], "n_citations": 313}, "snippets": ["Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."], "score": 0.81103515625}, {"id": "(Venkit, 2023)", "paper": {"corpus_id": 261214814, "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens", "year": 2023, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Pranav Narayanan Venkit", "authorId": "2053812167"}], "n_citations": 8}, "snippets": ["In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."], "score": 0.8291015625}, {"id": "(Sharma et al., 2023)", "paper": {"corpus_id": 258170403, "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Swapnil Sharma", "authorId": "2214583125"}, {"name": "Nikita Anand", "authorId": "2214521713"}, {"name": "V. KranthiKiranG.", "authorId": "1415341297"}, {"name": "Alind Jain", "authorId": "2214565507"}], "n_citations": 0}, "snippets": ["Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."], "score": 0.8125}, {"id": "(Das et al., 2024)", "paper": {"corpus_id": 270560549, "title": "Investigating Annotator Bias in Large Language Models for Hate Speech Detection", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Amit Das", "authorId": "2221319016"}, {"name": "Zheng Zhang", "authorId": "2264121393"}, {"name": "Fatemeh Jamshidi", "authorId": "2307011198"}, {"name": "Vinija Jain", "authorId": "2212131028"}, {"name": "Aman Chadha", "authorId": "2275226689"}, {"name": "Nilanjana Raychawdhary", "authorId": "2181140446"}, {"name": "Mary Sandage", "authorId": "2291126998"}, {"name": "Lauramarie Pope", "authorId": "2290014871"}, {"name": "Gerry V. Dozier", "authorId": "38328224"}, {"name": "Cheryl Seals", "authorId": "2283201612"}], "n_citations": 2}, "snippets": ["In our research, we specifically examine four types of biases: gender, race, disability, and religion. Gender bias refers to the prejudiced treatment based on an individual's sex or gender identity. Race bias involves discriminatory actions or attitudes towards individuals based on their racial or ethnic background. Disability bias encompasses unfair treatment of people with physical or mental impairments. Religion bias involves prejudices and discriminatory behaviors directed at individuals based on their religious beliefs or practices. Our study aims to analyze the prevalence and impact of these biases in various contexts."], "score": 0.94921875}, {"id": "(Mei et al., 2023)", "paper": {"corpus_id": 259129801, "title": "Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks", "year": 2023, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Katelyn Mei", "authorId": "2189183000"}, {"name": "Sonia Fereidooni", "authorId": "2196943720"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 51}, "snippets": ["The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks", "Caliskan et al. [7] demonstrate that word embeddings and language models (LMs) trained on a large amount of human-generated texts encode human-like social biases. Social biases encoded in these models are also reflected in their downstream tasks such as machine translation, sentiment classification, and natural language generation [1,[22][23][24]. As the downstream tasks of language models are rapidly deployed for real-world applications, the presence of social biases in these models reinforces social stereotypes, discrimination, and inequalities. Despite enormous efforts in bias evaluation of LMs, prior work extensively focuses on biases related to gender, race, and ethnicity [5,22,24,50,52]. Social stigmas, also an element of social biases, are stigmatized conditions that often relate to diseases, disabilities, mental illness, socioeconomic status, etc [36]. Considering all stigmatized conditions, social stigmas affect a substantial amount of people. In the United States, approximately 26 percent of adults experience a disability, with up to one in four individuals being affected. In 2021, there were around 57.8M adults that experienced mental illness, which was around 22% of the population in the United States. Social stigmas prevent individuals from social activities and access to education, healthcare, and career opportunities, negatively influencing their psychological well-being and life outcomes [14,[31][32][33][37]."], "score": 0.89599609375}, {"id": "(Meisel et al., 2022)", "paper": {"corpus_id": 246654106, "title": "Education-based stigma and discrimination among young adults not in 4-year college", "year": 2022, "venue": "BMC Psychology", "authors": [{"name": "Matthew K. Meisel", "authorId": "3118881"}, {"name": "Michelle Haikalis", "authorId": "4609930"}, {"name": "S. Colby", "authorId": "4240161"}, {"name": "N. Barnett", "authorId": "143796191"}], "n_citations": 10}, "snippets": ["Background Lower levels of education are strongly associated with negative health outcomes. The current study examined the degree to which those without a history of 4-year college attendance experience social stigmatization of their educational status and if these experiences are associated with mental health symptoms. Methods Data was obtained from 488 emerging adults who never attended 4-year college using Qualtrics Panels. Results 79.4% of participants agreed to one of the six statements that not attending 4-year college is stigmatized, and 71.8% endorsed experiencing at least one form of discrimination. Higher levels of education-related stigma and more frequent experiences of education-related discrimination was associated with greater past-month anxiety and depression symptoms. Conclusions These findings could serve to increase awareness regarding the unique and significant discrimination faced by young adults who do not attend 4-year college and identify specific areas of intervention that can help these young adults cope with the effects of stigma and discrimination."], "score": 0.0}, {"id": "(Cheng et al., 2024)", "paper": {"corpus_id": 271859735, "title": "Social Debiasing for Fair Multi-modal LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Harry Cheng", "authorId": "2149241557"}, {"name": "Yangyang Guo", "authorId": "1390575046"}, {"name": "Qingpei Guo", "authorId": "2273322768"}, {"name": "Ming Yang", "authorId": "2249834712"}, {"name": "Tian Gan", "authorId": "2247906706"}, {"name": "Liqiang Nie", "authorId": "2284688853"}], "n_citations": 2}, "snippets": ["Multi-modal Large Language Models (MLLMs) have advanced significantly, offering powerful vision-language understanding capabilities. However, these models often inherit severe social biases from their training datasets, leading to unfair predictions based on attributes like race and gender."], "score": 0.884765625}, {"id": "(Jain et al., 2024)", "paper": {"corpus_id": 267627928, "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Prachi Jain", "authorId": "3094662"}, {"name": "Ashutosh Sathe", "authorId": "2266397701"}, {"name": "Varun Gumma", "authorId": "2140408530"}, {"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "2256989615"}], "n_citations": 1}, "snippets": ["Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion."], "score": 0.83544921875}, {"id": "(Doll et al., 2024)", "paper": {"corpus_id": 274117151, "title": "Evaluating Gender Bias in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Michael D\u00f6ll", "authorId": "2338278194"}, {"name": "Markus D\u00f6hring", "authorId": "2338277213"}, {"name": "Andreas M\u00fcller", "authorId": "2338675374"}], "n_citations": 1}, "snippets": ["LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."], "score": 0.8681640625}, {"id": "(Suresh et al., 2019)", "paper": {"corpus_id": 235436386, "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle", "year": 2019, "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "authors": [{"name": "Harini Suresh", "authorId": "46537606"}, {"name": "J. Guttag", "authorId": "1724429"}], "n_citations": 442}, "snippets": ["As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."], "score": 0.0}, {"id": "(Kolisko et al., 2023)", "paper": {"corpus_id": 259716055, "title": "Exploring Social Biases of Large Language Models in a College Artificial Intelligence Course", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Skylar Kolisko", "authorId": "2222666109"}, {"name": "Carolyn Jane Anderson", "authorId": "144901955"}], "n_citations": 11}, "snippets": ["There is a growing body of work documenting social biases in large language models. We use the term large language model (LLM) to refer to text generation neural network models trained on massive amounts of text. Popular examples include BERT (Devlin et al. 2019), GPT-3 (Brown et al. 2020), RoBERTa (Liu et al., 2019)BLOOM (Big-Science 2022). These models exhibit powerful text generation capabilies, but have also been shown to pick up biases from their training data. \n\nSocial bias in LLMs is concerning because it may result in harm to the targeted social groups. Potential harms can be representational, portraying some groups negatively or failing to represent them at all, or allocational, denying certain groups opportunities or resources (Barocas et al. 2017). \n\nMuch existing work focuses on diagnosing representational harms with bias probe tasks: tasks that measure whether a model's predictions differ between two (or more) groups of interest. A number of probe tasks have been proposed: (Rudinger et al., 2017); (Sheng et al., 2019); Bordia and Bowman (2019); (Lee et al., 2019); (Liu et al., 2019)(Liu et al., 2019); (Nadeem et al., 2020)(Sheng et al., 2021) and others. Most of these focus on gender stereotypes. 2 A smaller number explore other aspects of identity, such as religion (Abid, Farooqi, and Zou 2021) and race. 3"], "score": 0.826171875}, {"id": "(Nemani et al., 2023)", "paper": {"corpus_id": 259203321, "title": "Gender Bias in Transformer Models: A comprehensive survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Praneeth Nemani", "authorId": "2156841973"}, {"name": "Yericherla Deepak Joel", "authorId": "2220288860"}, {"name": "Pallavi Vijay", "authorId": "41153415"}, {"name": "Farhana Ferdousi Liza", "authorId": "3445542"}], "n_citations": 3}, "snippets": ["Gender bias in artificial intelligence (AI) has emerged as a pressing concern with profound implications for individuals' lives. Researchers have identified three primary categories of bias: Denigration (A1), Stereotyping (A2), and Under-representation (A3). Denigration involves the usage of racial, ethnic, or religious slurs, which can often be observed as a prevalent method of cyberbullying. This type of bias manifests as derogatory language aimed at demeaning specific groups. Stereotyping, on the other hand, refers to individuals' cognitive representation of a particular social group. In NLP tasks, stereotyping can manifest through word embeddings and large-scale language models, where certain biases are learned and perpetuated in the resulting representations. Under-representation pertains to the absence of identifiable group members from representative bodies and well-being indicators in proportion to their population's size.\n\nStudies have demonstrated that word embeddings can detect racial and gender biases, even in well-labeled and balanced datasets. These biases can arise due to societal prejudices reflected in the training data, leading to biased predictions and outputs from NLP models. Contextual representations learned by large pre-trained language models, such as BERT and GPT, are also susceptible to biases. These models are typically trained on vast amounts of internet text, including societal biases in online content. Consequently, these models can replicate and perpetuate biases, often mirroring societal biases.\n\nFurthermore, machine learning models tend to amplify the behaviors and patterns they are exposed to, including biases present in the training data. Studies such as [30], [31] have explored the compounding effect of bias in newer models, highlighting the phenomenon known as bias overamplification. This refers to the tendency of machine learning models to disproportionately amplify and reinforce biases rather than mitigate them."], "score": 0.83984375}], "table": null}, {"title": "Origins of Bias in LLMs", "tldr": "Social bias in Large Language Models stems primarily from unbalanced training data that reflects historical societal prejudices and stereotypes. These biases originate from multiple sources including historical text corpora, internet content with skewed representations, and technical choices in model development. (20 sources)", "text": "\n## Training Data as the Primary Source of Bias\n\nThe most significant source of bias in Large Language Models (LLMs) lies in the training data itself. LLMs are trained on vast corpora of human-produced text that inherently contain societal prejudices, stereotypes, and historical biases <Paper corpusId=\"248006238\" paperTitle=\"(Camara et al., 2022)\" isShortName></Paper> <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. These biases get encoded into word embeddings and other representations within the models, creating what researchers call \"recoverable and accurate imprints of our historic biases\" <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper>. The statistical patterns in language that LLMs learn during training inadvertently capture and sometimes amplify societal biases present in the data <Paper corpusId=\"248366292\" paperTitle=\"(Kadan et al., 2022)\" isShortName></Paper>.\n\nResearch has demonstrated that when datasets reflect the thoughts, feelings, and actions of a community, AI models inevitably incorporate the associations and representations of that community <Paper corpusId=\"249431834\" paperTitle=\"(Caliskan et al., 2022)\" isShortName></Paper>. For example, a seminal study by Caliskan et al. showed that pre-trained word embeddings replicated ten major biases widely found among human participants, including biases associating European American names with positive concepts and African American names with negative concepts <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>.\n\n## Unbalanced Data Representation\n\nThe training data for LLMs often features unbalanced representations of different demographic groups. Internet content, which forms a substantial portion of training data, is not created equally by all segments of society <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper>. Internet access is unevenly distributed globally, leading to an overrepresentation of hegemonic views while underrepresenting marginalized perspectives <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>. Additionally, content filtering processes may disproportionately remove language used by marginalized groups, further skewing the training data <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>.\n\nThis selection bias creates significant disparities in how different demographic groups are represented in the training data. For instance, researchers have noted that datasets used for facial recognition contain predominantly light-skinned individuals, which can bias analyses toward underrepresented groups <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"201666566\" paperTitle=\"(Mehrabi et al., 2019)\" isShortName></Paper>.\n\n## Types of Bias Sources in LLMs\n\nResearchers have identified several specific sources of bias in LLMs:\n\n1. **Dataset Bias**: This stems from the unbalanced and skewed nature of training data, which often contains explicit discriminatory language or implicit stereotypes <Paper corpusId=\"260438662\" paperTitle=\"(Razaa et al., 2023)\" isShortName></Paper>. If datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and sometimes amplify these biases <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\n2. **Embedding Bias**: Word embeddings and other semantic representations capture relationships between words that reflect stereotypical associations, such as linking certain professions with specific genders <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"1704893\" paperTitle=\"(Bolukbasi et al., 2016)\" isShortName></Paper>. These biased embeddings serve as a fundamental component in LLMs and can inadvertently introduce semantic bias into downstream applications <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper>.\n\n3. **Label Bias**: In the context of supervised fine-tuning or instruction tuning, biases can be introduced through the subjective judgments of human annotators who provide labels or annotations <Paper corpusId=\"268856702\" paperTitle=\"(Chu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"196211238\" paperTitle=\"(Sap et al., 2019)\" isShortName></Paper>. When annotators inject their personal beliefs or stereotypes into the labeling process, they inadvertently introduce bias into the model.\n\n4. **Contextual Bias**: This arises from the context in which language models are used, which can influence the way biases manifest <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. The context can significantly impact applications such as recommender systems, hiring processes, and sentiment analysis <Paper corpusId=\"258426630\" paperTitle=\"(Fraiwan et al., 2023)\" isShortName></Paper>.\n\n5. **User Interaction Bias**: The responses generated by language models are influenced by the input they receive from users. If users consistently ask biased or prejudiced questions, the model may learn and replicate these biases in its responses <Paper corpusId=\"258426630\" paperTitle=\"(Fraiwan et al., 2023)\" isShortName></Paper>.\n\n## Amplification of Bias\n\nAn alarming aspect of bias in LLMs is that they don't merely reflect the biases present in their training data but can sometimes amplify them <Paper corpusId=\"259075978\" paperTitle=\"(Wald et al., 2023)\" isShortName></Paper>. Recent research suggests that LLMs can exaggerate existing stereotypes beyond what is reflected in real-world statistics or human perceptions <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. For example, a study found that LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person's gender, amplifying the bias beyond what is reflected in actual job statistics <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>.\n\n## Intrinsic vs. Extrinsic Bias\n\nResearchers distinguish between intrinsic and extrinsic bias in LLMs. Intrinsic bias refers to biases inherently within the internal representations or outputs of a trained or pre-trained LLM, independent of any specific downstream tasks <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>. These biases are typically encoded during the training phase when the LLM learns from large-scale corpora. Extrinsic bias, on the other hand, refers to biases that manifest during the model's performance on specific downstream tasks after training or fine-tuning <Paper corpusId=\"274130807\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n## Real-World Consequences\n\nThe biases present in LLMs have led to documented real-world harms. For instance, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261276445\" paperTitle=\"(Kotek et al., 2023)\" isShortName></Paper>. More recently, Google suspended Gemini's image generation feature following reports of inaccuracies and potential biases <Paper corpusId=\"272826949\" paperTitle=\"(Mirza et al., 2024)\" isShortName></Paper>.\n\nThese biases can lead to both allocative and representational harms <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalized groups, such as in recruitment algorithms. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and further entrenching societal biases <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>.\n\nThe problem of bias in LLMs continues to be a significant challenge, requiring comprehensive approaches to detection and mitigation to prevent the perpetuation and amplification of societal prejudices and stereotypes <Paper corpusId=\"274437724\" paperTitle=\"(Meijer et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Camara et al., 2022)", "paper": {"corpus_id": 248006238, "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic", "year": 2022, "venue": "LTEDI", "authors": [{"name": "Antonio Camara", "authorId": "2077593669"}, {"name": "Nina Taneja", "authorId": "70655120"}, {"name": "Tamjeed Azad", "authorId": "1571776558"}, {"name": "Emily Allaway", "authorId": "46208659"}, {"name": "R. Zemel", "authorId": "1804104"}], "n_citations": 21}, "snippets": ["The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users."], "score": 0.87255859375}, {"id": "(Caliskan et al., 2016)", "paper": {"corpus_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "year": 2016, "venue": "Science", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "J. Bryson", "authorId": "145315445"}, {"name": "Arvind Narayanan", "authorId": "47735253"}], "n_citations": 2673}, "snippets": ["Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."], "score": 0.0}, {"id": "(Caliskan et al., 2022)", "paper": {"corpus_id": 249431834, "title": "Gender Bias in Word Embeddings: A Comprehensive Analysis of Frequency, Syntax, and Semantics", "year": 2022, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "Pimparkar Parth Ajay", "authorId": "2168463879"}, {"name": "Tessa E. S. Charlesworth", "authorId": "66649568"}, {"name": "R. Wolfe", "authorId": "100508901"}, {"name": "M. Banaji", "authorId": "1968771"}], "n_citations": 51}, "snippets": ["Word embeddings are numeric representations of meaning derived from word co-occurrence statistics in corpora of human-produced texts. The statistical regularities in language corpora encode well-known social biases into word embeddings (e.g., the word vector for family is closer to the vector women than to men)", "When datasets that reflect the thoughts, feelings, and actions of a community are used to train artificial intelligence (AI) models, the models inevitably incorporate the associations and representations of the community [11-13, 15, 17, 26, 40, 51, 54].\n\nIn one of the first studies systematically assessing social biases in word embeddings, Caliskan et al. [12] showed that pretrained GloVe embeddings replicated ten major biases widely found among human participants including biases associating young-good/elderly-bad, European American-good/African American-bad, and women-family/men-career [28]. Such group-based biases can manifest in NLP applications (a recent example was seen in Amazon's automated resume screening algorithm that preferentially hired men over women [7])."], "score": 0.89306640625}, {"id": "(Kadan et al., 2022)", "paper": {"corpus_id": 248366292, "title": "Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Anoop Kadan", "authorId": "144485480"}, {"name": "Manjary P.Gangan", "authorId": "3390280"}, {"name": "Deepak P", "authorId": "145791688"}, {"name": "L. LajishV.", "authorId": "151092338"}], "n_citations": 10}, "snippets": ["The remarkable progress in Natural Language Processing (NLP) brought about by deep learning, particularly with the recent advent of large pre-trained neural language models, is brought into scrutiny as several studies began to discuss and report potential biases in NLP applications. Bias in NLP is found to originate from latent historical biases encoded by humans into textual data which gets perpetuated or even amplified by NLP algorithm."], "score": 0.88671875}, {"id": "(Grieve et al., 2024)", "paper": {"corpus_id": 271161894, "title": "The sociolinguistic foundations of language modeling", "year": 2024, "venue": "Frontiers Artif. Intell.", "authors": [{"name": "Jack Grieve", "authorId": "2280139407"}, {"name": "Sara Bartl", "authorId": "2311115214"}, {"name": "Matteo Fuoli", "authorId": "2268357014"}, {"name": "Jason Grafmiller", "authorId": "69370030"}, {"name": "Weihang Huang", "authorId": "2280176696"}, {"name": "A. Jawerbaum", "authorId": "2304488914"}, {"name": "Akira Murakami", "authorId": "2280137448"}, {"name": "Marcus Perlman", "authorId": "2311114471"}, {"name": "Dana Roemling", "authorId": "2298903710"}, {"name": "Bodo Winter", "authorId": "2320760943"}], "n_citations": 10}, "snippets": ["NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67]."], "score": 0.85107421875}, {"id": "(Bender et al., 2021)", "paper": {"corpus_id": 262580630, "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c", "year": 2021, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Emily M. Bender", "authorId": "2471699"}, {"name": "Timnit Gebru", "authorId": "2076288"}, {"name": "Angelina McMillan-Major", "authorId": "1584940075"}, {"name": "Shmargaret Shmitchell", "authorId": "2246048745"}], "n_citations": 4657}, "snippets": ["The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."], "score": 0.0}, {"id": "(Wald et al., 2023)", "paper": {"corpus_id": 259075978, "title": "Exposing Bias in Online Communities through Large-Scale Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Celine Wald", "authorId": "2219550842"}, {"name": "Lukas Pfahler", "authorId": "32421394"}], "n_citations": 6}, "snippets": ["Research has shown that language models are prone to exhibit societal biases, reflecting the nature of the underlying training data written by humans. For this work, we focus on representational bias, which in the context of language models, occurs when there is an imbalance in the portrayal of different groups in LM-generated text (Blodgett et al. 2020;Liang et al. 2021). This can arise from stereotypes that cultivate negative generalisations about particular social groups and language that is used to degrade these groups. Ultimately, this may lead to certain groups being depicted less favourably than others. Bender et al. argue that even in large models with large datasets, diversity and unbiasedness are not guaranteed (Bender et al. 2021). Datasets are too big to be thoroughly documented, while marginalised social groups are more likely to be filtered out due to the nature of the filtering process. Phrases or dialects used by these groups are disproportionally categorised as hateful, resulting in their exclusion from training datasets (Sap et al. 2019;Bender et al. 2021). Hegemonic views are over-represented as Internet access is not evenly distributed, and datasets are often curated using male-dominated websites. Large language generation models such as GPT-2 (Radford et al. 2019) and GPT-3 (Brown et al. 2020) have been shown to exhibit various kinds of representational bias, including racial, gender, and religious bias (Sheng et al. 2019;Abid, Farooqi, and Zou 2021;Sheng et al. 2021). They do not only absorb the biases present in their source material but even tend to amplify them. Dialogue generation models, in particular, are often trained on social media datasets that contain unchecked user-generated content that is prone to toxicity. These models have an immediate user impact and can thus play a more significant role in propagating harmful biases directly (Sheng et al. 2021)."], "score": 0.8486328125}, {"id": "(Chu et al., 2024)", "paper": {"corpus_id": 268856702, "title": "Fairness in Large Language Models: A Taxonomic Survey", "year": 2024, "venue": "SIGKDD Explorations", "authors": [{"name": "Zhibo Chu", "authorId": "2283849596"}, {"name": "Zichong Wang", "authorId": "2206777360"}, {"name": "Wenbin Zhang", "authorId": "2243674300"}], "n_citations": 41}, "snippets": ["Language modeling bias, often defined as \"bias that results in harm to various social groups\" [70], presents itself in various forms, encompassing the association of specific stereotypes with groups, the devaluation of certain groups, the underrepresentation of particular social groups, and the unequal allocation of resources among groups [44].Here, three primary sources contributing to bias in LLMs are introduced: i) Training data bias.The training data used to develop LLMs is not free from historical biases, which inevitably influence the behavior of these models.For instance, if the training data includes the statement \"all programmers are male and all nurses are female, \" the model is likely to learn and perpetuate these occupational and gender biases in its outputs, reflecting a narrow and biased view of societal roles (Bolukbasi et al., 2016)(Caliskan et al., 2016).Additionally, a significant disparity in the training data could also lead to biased outcomes [161].For example, Buolamwini and Gebru (Buolamwini et al., 2018) highlighted significant disparities in datasets like IJB-A and Adience, where predominantly light-skinned individuals make up 79.6% and 86.2% of the data, respectively, thereby biasing analyses toward underrepresented dark-skinned groups (Mehrabi et al., 2019).\n\nii) Embedding bias.Embeddings serve as a fundamental component in LLMs, offering a rich source of semantic information by capturing the nuances of language.However, these embeddings may unintentionally introduce biases, as demonstrated by the clustering of certain professions, such as nurses near words associated with femininity and doctors near words associated with masculinity.This phenomenon inadvertently introduces semantic bias into downstream models, impacting their performance and fairness [9]63].The presence of such biases underscores the importance of critically examining and mitigating bias in embeddings to ensure the equitable and unbiased functioning of LLMs across various applications and domains.\n\niii) Label bias.In instruction tuning scenarios, biases can arise from the subjective judgments of human annotators who provide labels or annotations for training data (Sap et al., 2019).This occurs when annotators inject their personal beliefs, perspectives, or stereotypes into the labeling process, inadvertently introducing bias into the model."], "score": 0.81201171875}, {"id": "(Mehrabi et al., 2019)", "paper": {"corpus_id": 201666566, "title": "A Survey on Bias and Fairness in Machine Learning", "year": 2019, "venue": "ACM Computing Surveys", "authors": [{"name": "Ninareh Mehrabi", "authorId": "51997673"}, {"name": "Fred Morstatter", "authorId": "2775559"}, {"name": "N. Saxena", "authorId": "51884035"}, {"name": "Kristina Lerman", "authorId": "1782658"}, {"name": "A. Galstyan", "authorId": "143728483"}], "n_citations": 4373}, "snippets": ["With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields."], "score": 0.0}, {"id": "(Razaa et al., 2023)", "paper": {"corpus_id": 260438662, "title": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text", "year": 2023, "venue": "Expert systems with applications", "authors": [{"name": "Shaina Razaa", "authorId": "2226784526"}, {"name": "Muskan Garg", "authorId": "3410824"}, {"name": "Deepak John Reji", "authorId": "2155769046"}, {"name": "S. Bashir", "authorId": "98911272"}, {"name": "Chen Ding", "authorId": "2053158730"}], "n_citations": 48}, "snippets": ["The recent surge in Natural Language Processing (NLP) applications, encompassing fields from recommendation systems to social justice and employment screening, has sparked a critical concern -the emergence of bias within these systems [1]. Instances of racial and gender bias have been increasingly reported [2], indicating an urgent need for scrutiny. These biases often originate from the training data used in NLP models, and a majority of these large datasets harbor inherent biases.\n\nBias in text data is a pervasive and deeply-rooted issue. The bias in data often stems from cognitive predispositions that influences our dialogues, views, and understanding of information [3]. This bias can be explicit which are often seen in discriminatory language targeting certain racial or ethnic groups [4], as in social media. Implicit bias [5], on the other hand, subtly perpetuates prejudice through unintentional language use but is equally harmful.\n\nThe necessity for unbiased, trustworthy text data has grown across sectors like healthcare [6], social media [4,7], and recruitment [8]. This data is essential for training NLP models for various downstream tasks, like formulating healthcare diagnoses and treatment plans, handling discriminatory language on social media, and promoting fair recruitment practices. Figure 1 illustrates the complexities of biases in text data in various domains, including job hiring, social media, and healthcare. These biases are primarily conveyed through lexical choices [9] and demand sophisticated detection methods, motivating this research."], "score": 0.83154296875}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis."], "score": 0.88720703125}, {"id": "(Bolukbasi et al., 2016)", "paper": {"corpus_id": 1704893, "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings", "year": 2016, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tolga Bolukbasi", "authorId": "2843215"}, {"name": "Kai-Wei Chang", "authorId": "2782886"}, {"name": "James Y. Zou", "authorId": "145085305"}, {"name": "Venkatesh Saligrama", "authorId": "1699322"}, {"name": "A. Kalai", "authorId": "2186481"}], "n_citations": 3153}, "snippets": ["The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias."], "score": 0.0}, {"id": "(Sap et al., 2019)", "paper": {"corpus_id": 196211238, "title": "The Risk of Racial Bias in Hate Speech Detection", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Maarten Sap", "authorId": "2729164"}, {"name": "Dallas Card", "authorId": "35540755"}, {"name": "Saadia Gabriel", "authorId": "119902504"}, {"name": "Yejin Choi", "authorId": "1699545"}, {"name": "Noah A. Smith", "authorId": "144365875"}], "n_citations": 848}, "snippets": ["We investigate how annotators\u2019 insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet\u2019s dialect they are significantly less likely to label the tweet as offensive."], "score": 0.0}, {"id": "(Fraiwan et al., 2023)", "paper": {"corpus_id": 258426630, "title": "A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Mohammad Fraiwan", "authorId": "145874705"}, {"name": "Natheer Khasawneh", "authorId": "47343240"}], "n_citations": 48}, "snippets": ["Language models can exhibit bias if the training data used to create them is biased. As Schramowski et al. (Schramowski et al., 2021) pointed out, large pre-trained models that try to mimic natural languages, may end up repeating the same unfairness and prejudices. This can lead to discriminatory or inaccurate analyses and recommendations. Moreover, this may lead to public outcry (i.e., political, social, and legal) against the commercial applications. These biases manifests themselves in several ways, as follows: \n\n\u2022 Training data bias: Language models are typically trained on large datasets of human language. If these datasets are biased in some way (e.g., based on race, gender, socioeconomic status, etc.), then the model may learn and replicate these biases in its responses. For example, if the training data is biased towards a particular gender, then the model may generate responses that are more favorable towards that gender. \n\n\u2022 User interaction bias: The responses generated by Chatbots are based on the input they receive from users. If users consistently ask biased or prejudiced questions, then the model may learn and replicate these biases in its responses. For example, if users frequently ask questions that are discriminatory towards a particular group, then the model may generate responses that perpetuate these biases. \n\n\u2022 Algorithmic bias: The algorithms used to train and operate language models and Chatbots may also introduce biases. For example, if the model is trained to optimize for a particular metric (e.g., accuracy, engagement, etc.), then it may prioritize generating responses that optimize for that metric, even if those responses are biased in some way. \n\n\u2022 Contextual bias: Chatbots generate responses based on the context they receive from users. If the context is biased in some way (e.g., based on the user's location, language, etc.), then the model may generate biased responses. For example, if a user is asking questions about a particular culture or religion, and the model is not trained on that culture or religion, it may generate biased responses due to its lack of knowledge."], "score": 0.83544921875}, {"id": "(Kotek et al., 2023)", "paper": {"corpus_id": 261276445, "title": "Gender bias and stereotypes in Large Language Models", "year": 2023, "venue": "International Conference on Climate Informatics", "authors": [{"name": "Hadas Kotek", "authorId": "3365389"}, {"name": "Rikker Dockum", "authorId": "90166394"}, {"name": "David Q. Sun", "authorId": "32100412"}], "n_citations": 236}, "snippets": ["Large Language Models (LLMs) have made substantial progress in the past several months, shattering state-of-the-art benchmarks in many domains. This paper investigates LLMs\u2019 behavior with respect to gender stereotypes, a known issue for prior models. We use a simple paradigm to test the presence of gender bias, building on but differing from WinoBias, a commonly used gender bias dataset, which is likely to be included in the training data of current LLMs. We test four recently published LLMs and demonstrate that they express biased assumptions about men and women\u2019s occupations. Our contributions in this paper are as follows: (a) LLMs are 3-6 times more likely to choose an occupation that stereotypically aligns with a person\u2019s gender; (b) these choices align with people\u2019s perceptions better than with the ground truth as reflected in official job statistics; (c) LLMs in fact amplify the bias beyond what is reflected in perceptions or the ground truth; (d) LLMs ignore crucial ambiguities in sentence structure 95% of the time in our study items, but when explicitly prompted, they recognize the ambiguity; (e) LLMs provide explanations for their choices that are factually inaccurate and likely obscure the true reason behind their predictions. That is, they provide rationalizations of their biased behavior. This highlights a key property of these models: LLMs are trained on imbalanced datasets; as such, even with the recent successes of reinforcement learning with human feedback, they tend to reflect those imbalances back at us. As with other types of societal biases, we suggest that LLMs must be carefully tested to ensure that they treat minoritized individuals and communities equitably."], "score": 0.0}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 274130807, "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yufei Guo", "authorId": "2331613833"}, {"name": "Muzhe Guo", "authorId": "2331371661"}, {"name": "Juntao Su", "authorId": "2273060735"}, {"name": "Zhou Yang", "authorId": "2331371753"}, {"name": "Mengqiu Zhu", "authorId": "2331357933"}, {"name": "Hongfei Li", "authorId": "2331365682"}, {"name": "Mengyang Qiu", "authorId": "1455277182"}, {"name": "Shuo Shuo Liu", "authorId": "2331377869"}], "n_citations": 22}, "snippets": ["The bias presented in Large Language Models (LLMs) can be broadly categorized into intrinsic bias and extrinsic bias based on the different stages at which the biases manifest within the model's lifecycle and the type of bias being measured (Doan et al., 2024). Intrinsic bias refers to biases that are inherently within the internal representations or outputs of a trained or pre-trained LLM and are independent of any specific downstream tasks. Extrinsic bias refers to the biases that manifest during the model's performance on specific downstream tasks after training or fine-turing. Intrinsic biases are typically encoded during the training or pre-training phase when the LLM learns from large-scale corpora. For instance, if a pre-trained LLM consistently associates certain professions with specific genders (e.g., assuming all doctors to be male and assuming all nurses to be female) or racial stereotypes (May et al., 2019), this bias would be considered intrinsic bias. This type of bias is tied to the way the model has encoded relationships and patterns in its internal representations.\n\nIntrinsic biases are primarily introduced during the training/pre-training stages where models learn patterns and representations from vast corpora. These biases can be traced to several aspects of data and model design, which will be outlined below."], "score": 0.9150390625}, {"id": "(Mirza et al., 2024)", "paper": {"corpus_id": 272826949, "title": "Evaluating Gender, Racial, and Age Biases in Large Language Models: A Comparative Analysis of Occupational and Crime Scenarios", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Vishal Mirza", "authorId": "2322445184"}, {"name": "Rahul Kulkarni", "authorId": "2322445481"}, {"name": "Aakanksha Jadhav", "authorId": "2322445728"}], "n_citations": 2}, "snippets": ["Bias in LLMs manifests in multiple forms, including racial, gender, and cultural stereotypes, often perpetuating systemic inequalities. These biases have tangible consequences; for instance, in 2018, Amazon discontinued an AI-driven recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" reflecting an inherent bias in the training data that favored male candidates [1]. More recently, in early 2024, Google suspended Gemini's imagegeneration feature following reports of inaccuracies and potential biases, further highlighting the challenges associated with mitigating bias in generative AI systems.\n\nThe sources of bias in LLMs are multifaceted, stemming from a) inherent biases in the training data, b) biases introduced by model architecture, and c) the influence of human evaluators during the debiasing process."], "score": 0.82080078125}, {"id": "(Doll et al., 2024)", "paper": {"corpus_id": 274117151, "title": "Evaluating Gender Bias in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Michael D\u00f6ll", "authorId": "2338278194"}, {"name": "Markus D\u00f6hring", "authorId": "2338277213"}, {"name": "Andreas M\u00fcller", "authorId": "2338675374"}], "n_citations": 1}, "snippets": ["LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."], "score": 0.8681640625}, {"id": "(Suresh et al., 2019)", "paper": {"corpus_id": 235436386, "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle", "year": 2019, "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "authors": [{"name": "Harini Suresh", "authorId": "46537606"}, {"name": "J. Guttag", "authorId": "1724429"}], "n_citations": 442}, "snippets": ["As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."], "score": 0.0}, {"id": "(Meijer et al., 2024)", "paper": {"corpus_id": 274437724, "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mijntje Meijer", "authorId": "2333364989"}, {"name": "Hadi Mohammadi", "authorId": "2261402979"}, {"name": "Ayoub Bagheri", "authorId": "2261403495"}], "n_citations": 1}, "snippets": ["An important concern regarding LLMs is whether and in which areas these models may exhibit bias, such as gender, racial and cultural bias.\n\nLarge language models are sensitive to embedded bias due to the way they are trained, as they pick up the societal and cultural biases present in the training data. Due to the fact that LLMs are trained on large amounts of data, the models may be (partially) trained on data that reflects embedded societal and cultural prejudices, such as news articles or social media posts (Karpouzis, 2024;Mishra et al., 2024). Consequently, if a language model is trained on data that consistently portrays certain cultural groups negatively or inaccurately, it may adopt and replicate those biased views. Thus, as applications based on LLM outputs become more prevalent, the potential risk of perpetuating cultural bias present in these models increases as well."], "score": 0.8125}], "table": null}, {"title": "Manifestations of Bias Across Applications", "tldr": "Social bias in LLMs manifests across numerous real-world applications including hiring, content moderation, healthcare, lending, speech recognition, and sentiment analysis, causing both allocational and representational harms to marginalized groups. These biases often disproportionately impact individuals at the intersection of multiple marginalized identities, creating systemic disadvantages in critical domains. (23 sources)", "text": "\n## Hiring and Recruitment\nBias in LLMs has particularly concerning implications in employment contexts. AI-driven hiring tools that utilize language models may exhibit unfair treatment toward applicants from underrepresented groups or those with non-traditional backgrounds <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper> <Paper corpusId=\"195316573\" paperTitle=\"(Raghavan et al., 2019)\" isShortName></Paper>. These biases can perpetuate existing inequalities in the job market by limiting opportunities for affected individuals and reducing workforce diversity. For example, Amazon discontinued an AI recruiting tool after discovering it systematically downgraded resumes containing the term \"women's,\" demonstrating the real-world consequences of gender bias in automated hiring systems <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"58006082\" paperTitle=\"(De-Arteaga et al., 2019)\" isShortName></Paper>. Automatic resume filtering systems have been found to exhibit gender bias, assigning programmer jobs to men and homemaker jobs to women at disproportionate rates <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220040093\" paperTitle=\"(Deshpande et al., 2020)\" isShortName></Paper>.\n\n## Content Moderation\nSocial bias significantly impacts AI-powered content moderation systems used to manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper>. Research has demonstrated that content moderation models exhibit undesirable biases toward mentions of disability in toxicity prediction tasks <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. These biases can result in the over-policing of language used by marginalized communities, further silencing already underrepresented voices.\n\n## Healthcare\nBias in LLMs used for healthcare applications can have life-threatening consequences. Studies have shown that healthcare systems can exhibit racial bias, with models judging Black patients with the same risk level to be sicker than white patients <Paper corpusId=\"261049466\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204881868\" paperTitle=\"(Obermeyer et al., 2019)\" isShortName></Paper>. When LLMs are deployed in clinical settings for tasks such as medical record analysis, symptom assessment, or treatment recommendations, these biases can lead to disparities in care quality and outcomes for marginalized groups.\n\n## Financial Services\nIn the financial sector, institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper> <Paper corpusId=\"51734899\" paperTitle=\"(Ustun et al., 2018)\" isShortName></Paper>. These biases in lending applications can perpetuate financial inequality and limit economic opportunities for already disadvantaged populations.\n\n## Speech and Emotion Recognition\nBias also manifests in speech-related applications powered by LLMs. Research has identified biases in Automatic Speech Recognition (ASR), Speech Translation, Facial Emotion Recognition, and Automatic Speaker Verification systems <Paper corpusId=\"270357379\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"222005741\" paperTitle=\"(Fenu et al., 2020)\" isShortName></Paper>. These biases can result in higher error rates for certain demographic groups, creating accessibility barriers. For example, speech recognition systems often perform worse for speakers with accents, disabilities, or non-standard dialects, limiting the utility of these technologies for diverse populations <Paper corpusId=\"253581416\" paperTitle=\"(Ngueajio et al., 2022)\" isShortName></Paper>.\n\n## Sentiment Analysis and Toxicity Detection\nLanguage models deployed for sentiment analysis and toxicity detection frequently exhibit bias against marginalized groups. Research has uncovered significant biases in popular sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>. All of these models show statistically significant explicit bias against disability mentions <Paper corpusId=\"218487466\" paperTitle=\"(Hutchinson et al., 2020)\" isShortName></Paper>. Similarly, studies have found that racial and religious identity labels consistently receive more negative sentiment scores across various models <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\n## Quality-of-Service and Stereotyping Harms\nThe manifestation of bias in LLMs across applications generally results in two primary types of harms: quality-of-service harms and stereotyping harms <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258041203\" paperTitle=\"(Ferrara, 2023)\" isShortName></Paper>. Quality-of-service harms occur when model performance varies depending on the user's social background, often resulting from underrepresentation of certain groups in training data <Paper corpusId=\"271161894\" paperTitle=\"(Grieve et al., 2024)\" isShortName></Paper> <Paper corpusId=\"209461005\" paperTitle=\"(Shah et al., 2019)\" isShortName></Paper>. Stereotyping harms, on the other hand, involve the perpetuation and amplification of harmful stereotypes about marginalized groups <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper>.\n\n## Intersectional Impact\nThe harmful effects of bias in applications are often compounded for individuals with intersectional identities. For example, a Black woman may face greater biases in hiring algorithms than either Black men or white women due to the intersection of racial and gender biases <Paper corpusId=\"248006238\" paperTitle=\"(Camara et al., 2022)\" isShortName></Paper>. These intersectional impacts highlight the complex nature of bias in LLMs and the need for comprehensive approaches to addressing it.\n\n## Perpetuation of Systemic Discrimination\nWhen deployed at scale, biased LLMs can perpetuate and amplify existing systemic discrimination. The consequences can range from reinforcing harmful stereotypes to causing material harm through the denial of opportunities, resources, or services <Paper corpusId=\"265609662\" paperTitle=\"(Tanyel et al., 2023)\" isShortName></Paper> <Paper corpusId=\"236950797\" paperTitle=\"(Kirk et al., 2021)\" isShortName></Paper>. As LLMs become increasingly integrated into critical systems that impact individuals' lives, addressing these biases becomes an urgent ethical imperative <Paper corpusId=\"271745282\" paperTitle=\"(Kumar et al., 2024)\" isShortName></Paper> <Paper corpusId=\"121125604\" paperTitle=\"(Basta et al., 2019)\" isShortName></Paper>.\n\nThe varied manifestations of bias across applications highlight the need for comprehensive bias detection and mitigation strategies that address both allocational and representational harms <Paper corpusId=\"274117151\" paperTitle=\"(Doll et al., 2024)\" isShortName></Paper> <Paper corpusId=\"235436386\" paperTitle=\"(Suresh et al., 2019)\" isShortName></Paper>. Without intervention, these biases threaten to replicate and amplify existing social inequalities in increasingly automated decision-making systems <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Ferrara, 2023)", "paper": {"corpus_id": 258041203, "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models", "year": 2023, "venue": "First Monday", "authors": [{"name": "Emilio Ferrara", "authorId": "48898287"}], "n_citations": 261}, "snippets": ["Bias in widely-adopted AI models, including ChatGPT and other generative language models, can have farreaching consequences that extend beyond the immediate context of their applications. When these models absorb and propagate biases, including those present in their training data, they may inadvertently reinforce stereotypes, marginalize certain groups, and lead to unfair treatment across various domains. Some examples of how biased AI models can adversely impact different areas include:\n\n\u2022 Hiring: AI-driven hiring tools that use biased models may exhibit unfair treatment towards applicants from underrepresented groups or those with non-traditional backgrounds. This could lead to the perpetuation of existing inequalities in the job market, limiting opportunities for affected individuals and reducing diversity in the workforce [102,(Raghavan et al., 2019). Large language models can be used to automate the screening of job applicants, such as by analyzing resumes and cover letters. Since these models are trained on vast amounts of text data, they may have internalized biases present in the data, such as gender or racial biases. As a result, they could unintentionally favor certain applicants or disqualify others based on factors unrelated to their qualifications, reinforcing existing inequalities in the job market.\n\n\u2022 Lending: Financial institutions increasingly rely on AI models for credit scoring and lending decisions. Biased models may unfairly penalize certain groups, such as minority communities or individuals with lower socioeconomic status, by assigning them lower credit scores or denying them access to loans and financial services based on biased assumptions (Citron et al., 2014)105,(Ustun et al., 2018). In lending, large language models can be used to assess creditworthiness or predict loan default risk, e.g., based on automated analysis of application or support documents.\n\nIf the data used to train these models contain historical biases or discriminatory lending practices, the models may learn to replicate these patterns. Consequently, they could deny loans to certain demographics or offer unfavorable terms based on factors like race, gender, or socioeconomic status, perpetuating financial inequality [107].\n\n\u2022 Content moderation: AI-powered content moderation systems help manage and filter user-generated content on social media platforms and other online communities. If these systems are trained on biased data, they may disproportionately censor or suppress the voices of certain groups"], "score": 0.845703125}, {"id": "(Raghavan et al., 2019)", "paper": {"corpus_id": 195316573, "title": "Mitigating bias in algorithmic hiring: evaluating claims and practices", "year": 2019, "venue": "FAT*", "authors": [{"name": "Manish Raghavan", "authorId": "38009222"}, {"name": "Solon Barocas", "authorId": "2881033"}, {"name": "J. Kleinberg", "authorId": "3371403"}, {"name": "K. Levy", "authorId": "144463523"}], "n_citations": 525}, "snippets": ["There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law."], "score": 0.0}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 261049466, "title": "A Survey on Fairness in Large Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yingji Li", "authorId": "2116202874"}, {"name": "Mengnan Du", "authorId": "3432460"}, {"name": "Rui Song", "authorId": "145401373"}, {"name": "Xin Wang", "authorId": "2153687737"}, {"name": "Y. Wang", "authorId": "49416173"}], "n_citations": 70}, "snippets": ["However, numerous studies have verified that LLMs capture human-like social biases in unprocessed training data (Garg et al., 2017)(Sun et al., 2019). These social biases can be encoded in the embeddings and carried over to decisions in downstream tasks, compromising the fairness of LLMs. Unfair LLM systems make discriminatory, stereotypic and demeaning decisions against vulnerable or marginalized demographics, causing undesirable social impacts and potential harms (Blodgett et al., 2020)(Kumar et al., 2022). For example, GPT-3 is found to associate males with higher levels of education and greater occupational competence, when asked GPT-3 that \"What is the gender of the doctor?\" and \"What is the gender of the nurse?\", its preferred outputs are \"A: Doctor is a masculine noun;\" and \"It's female.\", respectively. In real-world applications, the automatic resume filtering systems can be gender-biased, which tend to assign programmer jobs to men and homemaker jobs to women (De-Arteaga et al., 2019)(Deshpande et al., 2020)[10], and the US healthcare system can be racial biased, which judges black patients with the same risk level to be sicker than white patients (Obermeyer et al., 2019)."], "score": 0.81005859375}, {"id": "(De-Arteaga et al., 2019)", "paper": {"corpus_id": 58006082, "title": "Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting", "year": 2019, "venue": "FAT", "authors": [{"name": "Maria De-Arteaga", "authorId": "1406443102"}, {"name": "Alexey Romanov", "authorId": "145020546"}, {"name": "Hanna M. Wallach", "authorId": "1831395"}, {"name": "J. Chayes", "authorId": "1695997"}, {"name": "C. Borgs", "authorId": "1721812"}, {"name": "Alexandra Chouldechova", "authorId": "2359380329"}, {"name": "S. Geyik", "authorId": "6748971"}, {"name": "K. Kenthapadi", "authorId": "1769861"}, {"name": "A. Kalai", "authorId": "2186481"}], "n_citations": 460}, "snippets": ["We present a large-scale study of gender bias in occupation classification, a task where the use of machine learning may lead to negative outcomes on peoples' lives. We analyze the potential allocation harms that can result from semantic representation bias. To do so, we study the impact on occupation classification of including explicit gender indicators---such as first names and pronouns---in different semantic representations of online biographies. Additionally, we quantify the bias that remains when these indicators are \"scrubbed,\" and describe proxy behavior that occurs in the absence of explicit gender indicators. As we demonstrate, differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances."], "score": 0.0}, {"id": "(Deshpande et al., 2020)", "paper": {"corpus_id": 220040093, "title": "Mitigating Demographic Bias in AI-based Resume Filtering", "year": 2020, "venue": "User Modeling, Adaptation, and Personalization", "authors": [{"name": "K. Deshpande", "authorId": "3106764"}, {"name": "Shimei Pan", "authorId": "2239443126"}, {"name": "James R. Foulds", "authorId": "40289577"}], "n_citations": 65}, "snippets": ["With increasing diversity in the labor market as well as the work force, employers receive resumes from an increasingly diverse population. However, studies and field experiments have confirmed the presence of bias in the labor market based on gender, race, and ethnicity. Many employers use automated resume screening to filter the many possible matches. Depending on how the automated screening algorithm is trained it can potentially exhibit bias towards a particular population by favoring certain socio-linguistic characteristics. The resume writing style and socio-linguistics are a potential source of bias as they correlate with protected characteristics such as ethnicity. A biased dataset is often translated into biased AI algorithms and de-biasing algorithms are being contemplated. In this work, we study the effects of socio-linguistic bias on resume to job description matching algorithms. We develop a simple technique, called fair-tf-idf, to match resumes with job descriptions in a fair way by mitigating the socio-linguistic bias."], "score": 0.0}, {"id": "(Hutchinson et al., 2020)", "paper": {"corpus_id": 218487466, "title": "Social Biases in NLP Models as Barriers for Persons with Disabilities", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ben Hutchinson", "authorId": "2044655623"}, {"name": "Vinodkumar Prabhakaran", "authorId": "3331141"}, {"name": "Emily L. Denton", "authorId": "40081727"}, {"name": "Kellie Webster", "authorId": "20825661"}, {"name": "Yu Zhong", "authorId": "2112887022"}, {"name": "Stephen Denuyl", "authorId": "1667883461"}], "n_citations": 313}, "snippets": ["Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness."], "score": 0.81103515625}, {"id": "(Obermeyer et al., 2019)", "paper": {"corpus_id": 204881868, "title": "Dissecting racial bias in an algorithm used to manage the health of populations", "year": 2019, "venue": "Science", "authors": [{"name": "Z. Obermeyer", "authorId": "3797258"}, {"name": "Brian W. Powers", "authorId": "34593484"}, {"name": "C. Vogeli", "authorId": "4744063"}, {"name": "S. Mullainathan", "authorId": "2062143"}], "n_citations": 3628}, "snippets": ["Racial bias in health algorithms The U.S. health care system uses commercial algorithms to guide health decisions. Obermeyer et al. find evidence of racial bias in one widely used algorithm, such that Black patients assigned the same level of risk by the algorithm are sicker than White patients (see the Perspective by Benjamin). The authors estimated that this racial bias reduces the number of Black patients identified for extra care by more than half. Bias occurs because the algorithm uses health costs as a proxy for health needs. Less money is spent on Black patients who have the same level of need, and the algorithm thus falsely concludes that Black patients are healthier than equally sick White patients. Reformulating the algorithm so that it no longer uses costs as a proxy for needs eliminates the racial bias in predicting who needs extra care. Science, this issue p. 447; see also p. 421 A health algorithm that uses health costs as a proxy for health needs leads to racial bias against Black patients. Health systems rely on commercial prediction algorithms to identify and help patients with complex health needs. We show that a widely used algorithm, typical of this industry-wide approach and affecting millions of patients, exhibits significant racial bias: At a given risk score, Black patients are considerably sicker than White patients, as evidenced by signs of uncontrolled illnesses. Remedying this disparity would increase the percentage of Black patients receiving additional help from 17.7 to 46.5%. The bias arises because the algorithm predicts health care costs rather than illness, but unequal access to care means that we spend less money caring for Black patients than for White patients. Thus, despite health care cost appearing to be an effective proxy for health by some measures of predictive accuracy, large racial biases arise. We suggest that the choice of convenient, seemingly effective proxies for ground truth can be an important source of algorithmic bias in many contexts."], "score": 0.0}, {"id": "(Ustun et al., 2018)", "paper": {"corpus_id": 51734899, "title": "Actionable Recourse in Linear Classification", "year": 2018, "venue": "FAT", "authors": [{"name": "Berk Ustun", "authorId": "3072590"}, {"name": "Alexander Spangher", "authorId": "51444076"}, {"name": "Yang Liu", "authorId": "2152797134"}], "n_citations": 550}, "snippets": ["Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making."], "score": 0.0}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 270357379, "title": "Emo-bias: A Large Scale Evaluation of Social Bias on Speech Emotion Recognition", "year": 2024, "venue": "Interspeech", "authors": [{"name": "Yi-Cheng Lin", "authorId": "2284761645"}, {"name": "Haibin Wu", "authorId": "2243260548"}, {"name": "Huang-Cheng Chou", "authorId": "35725460"}, {"name": "Chi-Chun Lee", "authorId": "2208036805"}, {"name": "Hung-yi Lee", "authorId": "2305556858"}], "n_citations": 9}, "snippets": ["While extensive research has addressed bias in various machine learning domains, such as Automatic Speech Recognition (ASR) (Ngueajio et al., 2022)2,3], Speech Translation (Savoldi et al., 2022)(Gaido et al., 2021), Facial Emotion Recognition [6], and Automatic Speaker Verification (ASV) (Fenu et al., 2020), limited attention has been paid to social bias within SER systems."], "score": 0.8056640625}, {"id": "(Fenu et al., 2020)", "paper": {"corpus_id": 222005741, "title": "Exploring Algorithmic Fairness in Deep Speaker Verification", "year": 2020, "venue": "Communication Systems and Applications", "authors": [{"name": "G. Fenu", "authorId": "40433308"}, {"name": "Hicham Lafhouli", "authorId": "1975012884"}, {"name": "Mirko Marras", "authorId": "28922901"}], "n_citations": 21}, "snippets": ["To allow individuals to complete voice-based tasks (e.g., send messages or make payments), modern automated systems are required to match the speaker\u2019s voice to a unique digital identity representation for verification. Despite the increasing accuracy achieved so far, it still remains under-explored how the decisions made by such systems may be influenced by the inherent characteristics of the individual under consideration. In this paper, we investigate how state-of-the-art speaker verification models are susceptible to unfairness towards legally-protected classes of individuals, characterized by a common sensitive attribute (i.e., gender, age, language). To this end, we first arranged a voice dataset, with the aim of including and identifying various demographic classes. Then, we conducted a performance analysis at different levels, from equal error rates to verification score distributions. Experiments show that individuals belonging to certain demographic groups systematically experience higher error rates, highlighting the need of fairer speaker recognition models and, by extension, of proper evaluation frameworks."], "score": 0.0}, {"id": "(Ngueajio et al., 2022)", "paper": {"corpus_id": 253581416, "title": "Hey ASR System! Why Aren't You More Inclusive? Automatic Speech Recognition Systems' Bias and Proposed Bias Mitigation Techniques. A Literature Review", "year": 2022, "venue": "Interacci\u00f3n", "authors": [{"name": "Mikel K. Ngueajio", "authorId": "2183790481"}, {"name": "Gloria J. Washington", "authorId": "2065226591"}], "n_citations": 38}, "snippets": ["Speech is the fundamental means of communication between humans. The advent of AI and sophisticated speech technologies have led to the rapid proliferation of human-to-computer-based interactions, fueled primarily by Automatic Speech Recognition (ASR) systems. ASR systems normally take human speech in the form of audio and convert it into words, but for some users, it cannot decode the speech, and any output text is filled with errors that are incomprehensible to the human reader. These systems do not work equally for everyone and actually hinder the productivity of some users. In this paper, we present research that addresses ASR biases against gender, race, and the sick and disabled, while exploring studies that propose ASR debiasing techniques for mitigating these discriminations. We also discuss techniques for designing a more accessible and inclusive ASR technology. For each approach surveyed, we also provide a summary of the investigation and methods applied, the ASR systems and corpora used, and the research findings, and highlight their strengths and/or weaknesses. Finally, we propose future opportunities for Natural Language Processing researchers to explore in the next level creation of ASR technologies."], "score": 0.0}, {"id": "(Venkit, 2023)", "paper": {"corpus_id": 261214814, "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens", "year": 2023, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Pranav Narayanan Venkit", "authorId": "2053812167"}], "n_citations": 8}, "snippets": ["In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."], "score": 0.8291015625}, {"id": "(Sharma et al., 2023)", "paper": {"corpus_id": 258170403, "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Swapnil Sharma", "authorId": "2214583125"}, {"name": "Nikita Anand", "authorId": "2214521713"}, {"name": "V. KranthiKiranG.", "authorId": "1415341297"}, {"name": "Alind Jain", "authorId": "2214565507"}], "n_citations": 0}, "snippets": ["Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."], "score": 0.8125}, {"id": "(Grieve et al., 2024)", "paper": {"corpus_id": 271161894, "title": "The sociolinguistic foundations of language modeling", "year": 2024, "venue": "Frontiers Artif. Intell.", "authors": [{"name": "Jack Grieve", "authorId": "2280139407"}, {"name": "Sara Bartl", "authorId": "2311115214"}, {"name": "Matteo Fuoli", "authorId": "2268357014"}, {"name": "Jason Grafmiller", "authorId": "69370030"}, {"name": "Weihang Huang", "authorId": "2280176696"}, {"name": "A. Jawerbaum", "authorId": "2304488914"}, {"name": "Akira Murakami", "authorId": "2280137448"}, {"name": "Marcus Perlman", "authorId": "2311114471"}, {"name": "Dana Roemling", "authorId": "2298903710"}, {"name": "Bodo Winter", "authorId": "2320760943"}], "n_citations": 10}, "snippets": ["NLP systems generally suffer from social bias: their real-world application leads to outcomes that unfairly disadvantage or harm specific social groups [58,59,60,61]. Social bias can be introduced at various points during the development and deployment of NLP systems [62], but given the unsupervised nature of language modeling, training corpora are a key source of social bias in LLMs [10,63]. While bias in NLP systems can harm people in various ways [59], in this section, we primarily focus on two common harmful outcomes of social bias. These two types of harms are most commonly discussed in terms of quality-of-service harms and stereotyping harms (e.g., [64,65,60,66]), although many different systems have been proposed for classifying biases and harms in NLP, which define these terms in somewhat different ways, along with many additional and often overlapping categories [59]. Both of these types of harms are especially relevant to LLMs, and crucially, we believe both can be better understood and addressed in language modeling by adopting a sociolinguistic perspective (see Figure 3). First, social bias can be characterized by poor system performance for certain social groups that are interacting with LLMs and applications based on language models: token prediction will be more or less accurate depending on the social origins of the language inputted into the system. For example, ChatGPT might have difficulty correctly understanding prompts written by people from certain social groups due to their use of non-standard or socially restricted language patterns. This type of bias leads to what is known as quality-of-service harms, where the performance of these systems varies depending on the social background of the user [64,60]. These types of quality-of-service harms can often be the product of selection bias, as they result from how training data is selected from across the society whose language is being modeled [58]: in general, if language data from certain social groups is under-represented in the training data for a language model, we should expect that NLP applications based on that model will process language structures produced by these groups less accurately and consequently exhibit poorer performance for these groups [59]67]."], "score": 0.85107421875}, {"id": "(Shah et al., 2019)", "paper": {"corpus_id": 209461005, "title": "Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Deven Santosh Shah", "authorId": "1994268"}, {"name": "H. A. Schwartz", "authorId": "145035129"}, {"name": "Dirk Hovy", "authorId": "2022288"}], "n_citations": 260}, "snippets": ["An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks."], "score": 0.0}, {"id": "(Doll et al., 2024)", "paper": {"corpus_id": 274117151, "title": "Evaluating Gender Bias in Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Michael D\u00f6ll", "authorId": "2338278194"}, {"name": "Markus D\u00f6hring", "authorId": "2338277213"}, {"name": "Andreas M\u00fcller", "authorId": "2338675374"}], "n_citations": 1}, "snippets": ["LLMs learn from vast amounts of text collected from the Internet and other sources. This training data often contains unbalanced representations and stereotypes that are unintentionally embedded in the models [3].\n\nBias in LLMs can harm both individuals and society, with consequences including allocative and representational harms. Allocative harms occur when resources or opportunities are distributed unfairly, often to the detriment of already marginalised groups. For example, recruitment algorithms may be skewed to disadvantage certain demographic groups. Representational harm occurs when groups are misrepresented or stereotyped, reinforcing harmful narratives and stigmas and further entrenching negative prejudices and societal biases [4]. Language models show a propensity for bias in a variety of areas, including but not limited to gender, age, sexual orientation, physical appearance, disability, nationality, ethnicity, socio-economic status, religion and culture. These biases have the potential to have significant consequences, particularly in contexts where they affect critical areas such as justice, healthcare or employment [5]."], "score": 0.8681640625}, {"id": "(Camara et al., 2022)", "paper": {"corpus_id": 248006238, "title": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment Analysis Systems in English, Spanish, and Arabic", "year": 2022, "venue": "LTEDI", "authors": [{"name": "Antonio Camara", "authorId": "2077593669"}, {"name": "Nina Taneja", "authorId": "70655120"}, {"name": "Tamjeed Azad", "authorId": "1571776558"}, {"name": "Emily Allaway", "authorId": "46208659"}, {"name": "R. Zemel", "authorId": "1804104"}], "n_citations": 21}, "snippets": ["The presence and impact of harmful social biases in machine learning and natural language processing systems is pervasive and well-documented in popular word embedding methods (Caliskan et al., 2016)(Garg et al., 2017)(Bolukbasi et al., 2016) due to large amounts of humanproduced training data that includes historical social biases. Notably, (Caliskan et al., 2016) demonstrate such biases by introducing the Word Embedding Association Test (WEAT) which measures how similar socially sensitive sets of words (e.g., racial or gendered names) are to attributive sets of words (e.g., pleasant or unpleasant words) in the semantic space encoded by word embeddings. While (Bolukbasi et al., 2016); Manzini et al. (2019) introduce methods for \"debiasing\" word embeddings in order to create more equitable semantic representations for usage in downstream tasks, Gonen and Goldberg (2019) argue that such methods are merely cosmetic since social biases are still evident in the semantic space after the application of such methods. Moreover, these \"debiasing\" techniques focus on a particular social cleavage such as gender or race (i.e., unisectional cleavages). In contrast, our work considers both unisectional and intersectional social biases.\n\nRecent studies have also begun to focus on social biases in transformer-based language models (Kurita et al., 2019)(Bender et al., 2021). In particular, (Bender et al., 2021) discusses how increasingly large transformer-based language model in practice regurgitate their training data, resulting in such models perpetuating social biases and harming users."], "score": 0.87255859375}, {"id": "(Tanyel et al., 2023)", "paper": {"corpus_id": 265609662, "title": "Developing Linguistic Patterns to Mitigate Inherent Human Bias in Offensive Language Detection", "year": 2023, "venue": "Turkish J. Electr. Eng. Comput. Sci.", "authors": [{"name": "T. Tanyel", "authorId": "2189148265"}, {"name": "Besher Alkurdi", "authorId": "2189146673"}, {"name": "S. Ayvaz", "authorId": "2460907"}], "n_citations": 0}, "snippets": ["One of the more insidious ways in which discrimination can manifest is through natural language models, which are computational models that use statistical algorithms to understand and generate human language. These models have become ubiquitous in our daily lives, powering everything from virtual assistants to search engines and chatbots. However, as with any human-created system, natural language models are susceptible to biases and discrimination that can be embedded in the data they are trained on [21].\n\nThe problem of discrimination in language models arises because these models are developed based on huge amounts of text data retrieved from the internet, which contains a plethora of societal biases and prejudices. For instance, language models might be trained on text data that contains racist or sexist language, which can lead to the model associating certain words or phrases with negative stereotypes. As a result, when the model is used to generate text or respond to queries, it may produce outputs that perpetuate harmful stereotypes and discriminatory practices.\n\nFurthermore, language models can also be discriminatory in their output if the training data is not diverse enough to represent different demographics or if the model is biased towards a particular dialect or language variety. This can result in language models that are more accurate for certain groups than for others, which can perpetuate existing power imbalances and inequalities.\n\nThe consequences of discriminatory language models can be significant, ranging from perpetuating harmful stereotypes to reinforcing systemic discrimination against marginalized groups [22]."], "score": 0.8603515625}, {"id": "(Kirk et al., 2021)", "paper": {"corpus_id": 236950797, "title": "Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models", "year": 2021, "venue": "Neural Information Processing Systems", "authors": [{"name": "Hannah Rose Kirk", "authorId": "90729626"}, {"name": "Yennie Jun", "authorId": "1664016559"}, {"name": "Haider Iqbal", "authorId": "2146192816"}, {"name": "Elias Benussi", "authorId": "133876374"}, {"name": "Filippo Volpin", "authorId": "2048000297"}, {"name": "F. Dreyer", "authorId": "102485516"}, {"name": "Aleksandar Shtedritski", "authorId": "2048000614"}, {"name": "Yuki M. Asano", "authorId": "47792365"}], "n_citations": 192}, "snippets": ["The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied `out-of-the-box' for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reflect or correct for existing inequalities."], "score": 0.0}, {"id": "(Kumar et al., 2024)", "paper": {"corpus_id": 271745282, "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shachi H. Kumar", "authorId": "2109680564"}, {"name": "Saurav Sahay", "authorId": "38531701"}, {"name": "Sahisnu Mazumder", "authorId": "2066417452"}, {"name": "Eda Okur", "authorId": "3442103"}, {"name": "R. Manuvinakurike", "authorId": "2175808"}, {"name": "Nicole Beckage", "authorId": "2275457041"}, {"name": "Hsuan Su", "authorId": "2087042748"}, {"name": "Hung-yi Lee", "authorId": "2259815964"}, {"name": "L. Nachman", "authorId": "1896095"}], "n_citations": 18}, "snippets": ["Biases in these models-such as gender, racial, ethnic, and ideological biases-stem from training data that reflect societal prejudices. Consequently, language models can perpetuate and even amplify these biases, leading to outputs that reinforce stereotypes and disadvantage certain groups (Basta et al., 2019). The implications are profound, affecting areas, like hiring, law enforcement, and daily AI interactions (Schwartz et al., 2022;Ferrara, 2023)."], "score": 0.80908203125}, {"id": "(Basta et al., 2019)", "paper": {"corpus_id": 121125604, "title": "Evaluating the Underlying Gender Bias in Contextualized Word Embeddings", "year": 2019, "venue": "Proceedings of the First Workshop on Gender Bias in Natural Language Processing", "authors": [{"name": "Christine Basta", "authorId": "73312674"}, {"name": "M. Costa-juss\u00e0", "authorId": "1398996347"}, {"name": "Noe Casas", "authorId": "1794731"}], "n_citations": 193}, "snippets": ["Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased."], "score": 0.0}, {"id": "(Suresh et al., 2019)", "paper": {"corpus_id": 235436386, "title": "A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle", "year": 2019, "venue": "Conference on Equity and Access in Algorithms, Mechanisms, and Optimization", "authors": [{"name": "Harini Suresh", "authorId": "46537606"}, {"name": "J. Guttag", "authorId": "1724429"}], "n_citations": 442}, "snippets": ["As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."], "score": 0.0}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Bias and fairness, if not adequately addressed, pose serious societal implications in the form of biased language generation and its impact on some segments of society. Basis can creep into LLMs from several sources discussed below. The first source of bias, dataset bias, stems from the datasets that were used to train the LLMs. If the datasets contain biases related to race, gender, religion, or socioeconomic status, the models inherit and amplify them.\n\nUnderrepresentation or misrepresentation of certain groups in the training data can lead to representation bias and biased language generation. The LLM developers should have checks and balances to ensure that all perspectives are adequately represented in the datasets. Otherwise, the model will produce inaccurate or skewed output for underrepresented groups. If the training data contain stereotypes, models amplify stereotyping and perpetuate prejudices. Fairness across demographics is a complex challenge but essential for advancing LLMs.\n\nCentextual bias stems from the context in which the language models are used. This poses severe and negative implications in applications such as recommender systems, employee hiring and promotions, clustering, and sentiment analysis."], "score": 0.88720703125}], "table": null}, {"title": "Measurement and Detection of Bias in LLMs", "tldr": "Researchers have developed various metrics and frameworks to measure and detect bias in LLMs, including association tests, prompt-based methods, and automated tools that can identify biases across multiple social dimensions. These approaches have revealed that bias exists at significant levels across different social categories with intersectional biases often showing stronger effects than unisectional biases. (11 sources)", "text": "\n## Bias Measurement Techniques\n\nResearchers have developed several key methodologies to detect and quantify social bias in LLMs. One of the most influential approaches has been association tests, which measure how closely certain social groups are associated with particular attributes in the embedding space. The Word Embedding Association Test (WEAT) pioneered this approach by demonstrating that word embeddings replicate human-like social biases <Paper corpusId=\"23163324\" paperTitle=\"(Caliskan et al., 2016)\" isShortName></Paper>. Building on this foundation, the Contextualized Embedding Association Test (CEAT) was introduced to assess bias in contextual word embeddings, providing information on how bias magnitude varies across different contexts <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nSentiment analysis has emerged as another important technique for measuring bias. Researchers use sentiment scores to quantify how positively or negatively models respond to different identity terms. For instance, studies have used prompt-based methods that combine social categories (e.g., gender, religion, and disability) to generate model outputs that are then analyzed for sentiment bias <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>. These approaches have revealed that while gender bias may be relatively minor and context-dependent, religious and disability biases tend to be more pronounced and resistant to context <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Specialized Datasets and Benchmarks\n\nTo facilitate systematic bias evaluation, researchers have developed specialized datasets and benchmarks. Notable examples include BITS (Bias Identification Test in Sentiment) corpus, which was created to quantify explicit sociodemographic bias in sentiment analysis and toxicity detection models <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>. Other benchmark datasets like StereoSet measure biases related to gender, occupation, race, and religion, while CrowS-Pairs additionally covers sexual orientation, age, nationality, disability, and physical appearance <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>.\n\nThese benchmarks have enabled consistent evaluation across different models and revealed that even state-of-the-art language models exhibit significant biases. Studies applying these benchmarks have found that popular sentiment analysis tools (TextBlob, VADER, Google Cloud Natural Language API) and toxicity analysis models (Toxic-BERT) all show statistically significant explicit bias against disability mentions <Paper corpusId=\"261214814\" paperTitle=\"(Venkit, 2023)\" isShortName></Paper>.\n\n## Intersectional Bias Detection\n\nA significant advancement in bias measurement has been the development of methods to detect intersectional bias\u2014bias affecting individuals with multiple marginalized identities. The Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD) methods have been developed to automatically identify both intersectional biases and emergent intersectional biases from word embeddings <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nThese methods have revealed important findings about the nature of intersectional bias. Research has shown that intersectional group members are often strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. For example, IBD achieved accuracy rates of 81.6% and 82.7% when detecting intersectional biases affecting African American females and Mexican American females, respectively\u2014far exceeding random identification rates <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nStudies have also found that the effect of race on intersectional identities appears to be larger than the effect of gender <Paper corpusId=\"202781363\" paperTitle=\"(Tan et al., 2019)\" isShortName></Paper>. This highlights the complexity of bias in LLMs and the need for measurement approaches that can capture multiple dimensions of bias simultaneously, rather than addressing them independently <Paper corpusId=\"250390904\" paperTitle=\"(Lalor et al., 2022)\" isShortName></Paper> <Paper corpusId=\"267627928\" paperTitle=\"(Jain et al., 2024)\" isShortName></Paper>.\n\n## Bias Across Different Models and Architectures\n\nComparative analysis of bias across different LLM architectures has yielded important insights. Research has shown that all tested models trained on English corpora contain biased representations, though the magnitude varies <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>. When comparing models, studies have found that GPT-2 contains the smallest magnitude of overall bias, followed by GPT, BERT, and then ELMo, suggesting a negative correlation between bias magnitude and contextualization levels <Paper corpusId=\"219530686\" paperTitle=\"(Guo et al., 2020)\" isShortName></Paper>.\n\nHowever, certain biases appear resistant to improvements in model architecture. Research examining models with varying parameter counts (124 million to ~2.7 billion) found that while gender bias might be minor and heavily context-dependent, religious and disability biases remain strongly evident and resistant to increases in model size and training data diversity <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\n## Challenges in Bias Detection\n\nDespite progress in bias measurement, researchers face significant challenges. The multidimensional nature of bias makes comprehensive evaluation difficult, as most prior work has emphasized bias along a single dimension such as gender or race <Paper corpusId=\"250390904\" paperTitle=\"(Lalor et al., 2022)\" isShortName></Paper>. Additionally, biases can manifest in unpredictable ways at category intersections, making them resistant to techniques that target single categories <Paper corpusId=\"236034024\" paperTitle=\"(Magee et al., 2021)\" isShortName></Paper>.\n\nAnother challenge is that models trained on unmoderated and unfiltered internet data inherit biases that reflect those in society <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper>. These biases can be harmful to targeted social groups when the models are deployed in applications affecting millions of people <Paper corpusId=\"258170403\" paperTitle=\"(Sharma et al., 2023)\" isShortName></Paper> <Paper corpusId=\"262580630\" paperTitle=\"(Bender et al., 2021)\" isShortName></Paper>.\n\nThe field continues to evolve, with researchers emphasizing the importance of expanding bias detection beyond traditional categories. Recent work has proposed flexible methods like the \"bias score\" for estimating gender bias in sentence embeddings, designed to be adaptable to different types of social biases (e.g., ethnic, religious) and various sentence encoders <Paper corpusId=\"258174286\" paperTitle=\"(Dolci et al., 2023)\" isShortName></Paper>. These approaches highlight the growing recognition that analyzing bias is an inherently normative process requiring careful consideration of what kinds of system behaviors are harmful, to whom, and why <Paper corpusId=\"218971825\" paperTitle=\"(Blodgett et al., 2020)\" isShortName></Paper>.", "citations": [{"id": "(Caliskan et al., 2016)", "paper": {"corpus_id": 23163324, "title": "Semantics derived automatically from language corpora contain human-like biases", "year": 2016, "venue": "Science", "authors": [{"name": "Aylin Caliskan", "authorId": "144537437"}, {"name": "J. Bryson", "authorId": "145315445"}, {"name": "Arvind Narayanan", "authorId": "47735253"}], "n_citations": 2673}, "snippets": ["Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology."], "score": 0.0}, {"id": "(Guo et al., 2020)", "paper": {"corpus_id": 219530686, "title": "Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases", "year": 2020, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "W. Guo", "authorId": "28735341"}, {"name": "Aylin Caliskan", "authorId": "144537437"}], "n_citations": 243}, "snippets": ["We introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models."], "score": 0.876953125}, {"id": "(Magee et al., 2021)", "paper": {"corpus_id": 236034024, "title": "Intersectional Bias in Causal Language Models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "L. Magee", "authorId": "2733075"}, {"name": "Lida Ghahremanlou", "authorId": "9557084"}, {"name": "K. Soldati\u0107", "authorId": "13714096"}, {"name": "S. Robertson", "authorId": "97868921"}], "n_citations": 33}, "snippets": ["To examine whether intersectional bias can be observed in language generation, we examine \\emph{GPT-2} and \\emph{GPT-NEO} models, ranging in size from 124 million to ~2.7 billion parameters. We conduct an experiment combining up to three social categories - gender, religion and disability - into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the \\emph{GPT} family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories.\n\nWe conclude that (1) bias exists at significant levels across different social categories (gender, religion and disability); (2) it can manifest, in unpredictable ways, at category intersections; and (3) it is resistant to increases in model size and training data diversity. Specifically, while gender bias is minor and heavily conditioned on prompt context, religion and disability bias is strongly evident and resistant to context, with Muslim and all disability labels other than 'with Down Syndrome' scoring worse than no label."], "score": 0.8779296875}, {"id": "(Venkit, 2023)", "paper": {"corpus_id": 261214814, "title": "Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens", "year": 2023, "venue": "AAAI/ACM Conference on AI, Ethics, and Society", "authors": [{"name": "Pranav Narayanan Venkit", "authorId": "2053812167"}], "n_citations": 8}, "snippets": ["In my study [9], I delve into sentiment analysis and toxicity detection models to identify explicit bias against race, gender, and people with disabilities (PWDs). Through statistical exploration of conversations on social media platforms such as Twitter and Reddit, I gain insights into how disability bias permeates real-world social settings. To quantify explicit sociodemographic bias in sentiment analysis and toxicity analysis models, I create the Bias Identification Test in Sentiment (BITS) corpus1. Applying BITS, I uncover significant biases in popular AIaaS sentiment analysis tools, including TextBlob, VADER, and Google Cloud Natural Language API, as well as toxicity analysis models like Toxic-BERT. Remarkably, all of these models exhibit statistically significant explicit bias against disability, underscoring the need for comprehensive understanding and mitigation of biases affecting such groups.\n\nExpanding on this, my next work [8] delves into the realm of implicit bias in NLP models. While some models may not overtly exhibit bias, they can unintentionally perpetuate harmful stereotypes [4]. To measure and identify implicit bias in commonly used embedding and large language models, I propose a methodology to measure social biases in various NLP architectures. Focusing on people with disabilities (PWD) as a group with complex social dynamics, I analyze various word embedding-based and transformer-based LLMs, revealing significant biases against PWDs in all tested models. These findings expose how models trained on extensive corpora tend to favor ableist language, underscoring the urgency of detecting and addressing implicit bias.\n\nTo explore this aspect, I investigate how LLMs represent nationality and their impact on societal stereotypes [6]. By examining LLM-generated stories for various nationalities, I establish a correlation between sentiment and the population of internet users in a country. The study reveals the unintentional implicit and explicit nationality biases exhibited by GPT-2, with nations having lower internet representation and economic status generating negative sentiment stories and employing a greater number of negative adjectives."], "score": 0.8291015625}, {"id": "(Sharma et al., 2023)", "paper": {"corpus_id": 258170403, "title": "Evaluation of Social Biases in Recent Large Pre-Trained Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Swapnil Sharma", "authorId": "2214583125"}, {"name": "Nikita Anand", "authorId": "2214521713"}, {"name": "V. KranthiKiranG.", "authorId": "1415341297"}, {"name": "Alind Jain", "authorId": "2214565507"}], "n_citations": 0}, "snippets": ["Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups.\n\nGiven the popularity of large pre-trained language models and their widespread use in the community, it is pertinent to evaluate their inherent biases. These biases come about because these models (for example, BERT (Devlin et al., 2019) and BERT based models, ALBERT (Lan et al., 2020), RoBERTa (Liu et al., 2019)) are trained on large sets of often freely available and unmoderated text data from sources such as the internet. \n\nDue to this, gender, race, religious and other social biases that we see in the real world are often translated into the models. Since these models are deployed in applications that are used by a large number of people, this bias is harmful. For example, these models are commonly used in content moderation tasks on social media platforms online. If the model is unfairly biased against certain social groups-minorities, marginalized people and those sections of society that are discriminated against, then they would be adversely affected by the application.\n\nStereoSet accounts for four different biases that are based on gender, occupation, race and religion. In addition to these, CrowS-Pairs also has sexual orientation, age, nationality, disability and physical appearance."], "score": 0.8125}, {"id": "(Tan et al., 2019)", "paper": {"corpus_id": 202781363, "title": "Assessing Social and Intersectional Biases in Contextualized Word Representations", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Y. Tan", "authorId": "144787248"}, {"name": "Elisa Celis", "authorId": "47777961"}], "n_citations": 229}, "snippets": ["In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities", "we provide evidence for how racial bias is encoded strongly in contextual word models, potentially even more so than gender bias", "we introduce a method of comparison that anchors at the most or least privileged group to show that intersectional identities suffer from such bias as well, and more so than their constituent minority identities. In we show that the effect of race on intersectional identities seems to be larger than the effect of gender."], "score": 0.83154296875}, {"id": "(Lalor et al., 2022)", "paper": {"corpus_id": 250390904, "title": "Benchmarking Intersectional Biases in NLP", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "John P. Lalor", "authorId": "9051130"}, {"name": "Yi Yang", "authorId": "2143685866"}, {"name": "Kendall Smith", "authorId": "2152595932"}, {"name": "Nicole Forsgren", "authorId": "3047212"}, {"name": "A. Abbasi", "authorId": "144849629"}], "n_citations": 48}, "snippets": ["There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race."], "score": 0.86376953125}, {"id": "(Jain et al., 2024)", "paper": {"corpus_id": 267627928, "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models", "year": 2024, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Prachi Jain", "authorId": "3094662"}, {"name": "Ashutosh Sathe", "authorId": "2266397701"}, {"name": "Varun Gumma", "authorId": "2140408530"}, {"name": "Kabir Ahuja", "authorId": "52154863"}, {"name": "Sunayana Sitaram", "authorId": "2256989615"}], "n_citations": 1}, "snippets": ["Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion."], "score": 0.83544921875}, {"id": "(Bender et al., 2021)", "paper": {"corpus_id": 262580630, "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? \ud83e\udd9c", "year": 2021, "venue": "Conference on Fairness, Accountability and Transparency", "authors": [{"name": "Emily M. Bender", "authorId": "2471699"}, {"name": "Timnit Gebru", "authorId": "2076288"}, {"name": "Angelina McMillan-Major", "authorId": "1584940075"}, {"name": "Shmargaret Shmitchell", "authorId": "2246048745"}], "n_citations": 4657}, "snippets": ["The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models."], "score": 0.0}, {"id": "(Dolci et al., 2023)", "paper": {"corpus_id": 258174286, "title": "Improving Gender-Related Fairness in Sentence Encoders: A Semantics-Based Approach", "year": 2023, "venue": "Data Science and Engineering", "authors": [{"name": "Tommaso Dolci", "authorId": "2182555493"}, {"name": "Fabio Azzalini", "authorId": "25919018"}, {"name": "M. Tanelli", "authorId": "1709652"}], "n_citations": 10}, "snippets": ["Therefore, it is of the utmost importance to expand research on how sentence embedding encoders internalise the semantics of natural languages. An important step towards this direction is to define metrics that are able to reflect and quantify social bias in sentence encoders. Furthermore, studying and limiting the causes and consequences of bias in language models is an extremely important task [4]6].\n\nThis work expands research on social bias in embeddingbased models, focusing specifically on gender bias in sentence representations. First, we propose a method to estimate gender bias in sentence embeddings, highlighting the correlation between bias and stereotypical concepts in the sentence. Our solution, named bias score, is highly flexible and designed to be easily adapted to both different kinds of social biases (e.g. ethnic, religious) and various sentence encoders."], "score": 0.82568359375}, {"id": "(Blodgett et al., 2020)", "paper": {"corpus_id": 218971825, "title": "Language (Technology) is Power: A Critical Survey of \u201cBias\u201d in NLP", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Su Lin Blodgett", "authorId": "3422038"}, {"name": "Solon Barocas", "authorId": "2881033"}, {"name": "Hal Daum'e", "authorId": "2065041692"}, {"name": "Hanna M. Wallach", "authorId": "1831395"}], "n_citations": 1249}, "snippets": ["We survey 146 papers analyzing \"bias\" in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing \"bias\" is an inherently normative process. We further find that these papers\u2019 proposed quantitative techniques for measuring or mitigating \"bias\" are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing \"bias\" in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of \"bias\"---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u2014and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities."], "score": 0.0}], "table": null}], "cost": 0.70278}}
