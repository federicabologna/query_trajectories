{"clarifying_information": [{"clarifying_question1": "Are you primarily interested in understanding how sequence parallelism interacts with other efficiency-focused methods (like sparse attention or memory-efficient algorithms) for extending context length in Transformer models?", "clarifying_answer1": {"clarifying_answer": "Yes, I am primarily interested in how sequence parallelism interacts or can be combined with other efficiency-focused methods (such as sparse attention or memory-efficient algorithms) to extend the context length of Transformer models. Specifically, I want to understand the synergies, trade-offs, and possible integration strategies among these techniques for practical long-context training and inference."}}, {"clarifying_question2": "Do you want a detailed comparison of sequence parallelism versus memory/recurrence-based approaches (such as Transformer-XL, Compressive Transformer, or RWKV) for scaling Transformers to longer contexts?", "clarifying_answer2": {"clarifying_answer": "Yes, I would like a detailed comparison between sequence parallelism techniques and memory/recurrence-based approaches (like Transformer-XL, Compressive Transformer, RWKV) for extending the context length of Transformer models."}}, {"clarifying_question3": "Are you seeking guidance on how to combine different long-context modeling techniques (for example, position encoding extensions, sequence parallelism, and retrieval-based methods) in practical large-scale model training pipelines?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in practical recommendations or guidelines for combining multiple long-context modeling techniques\u2014including position encoding extensions, sequence parallelism, and retrieval-based methods\u2014within large-scale transformer model training and deployment pipelines."}}], "better_query": "How does sequence parallelism interact and integrate with other efficiency-focused methods, such as sparse attention and memory-efficient algorithms, for extending Transformer model context length in practice? What are the synergies and trade-offs among these approaches?", "better_answer": {"sections": [{"title": "Introduction", "tldr": "Modern Transformer models face significant computational challenges when scaling to longer context lengths, requiring innovative efficiency techniques. Sequence parallelism works alongside other methods like sparse attention and memory-efficient algorithms to overcome these limitations, each offering distinct benefits and trade-offs. (LLM Memory)", "text": "\nThe extension of context length in Transformer models has become a critical frontier in natural language processing, enabling tasks that require understanding and generating longer sequences of text. However, this extension comes with substantial computational challenges, particularly in terms of memory usage and processing time. The standard Transformer architecture, with its quadratic complexity in self-attention mechanisms relative to sequence length, quickly becomes prohibitively expensive as context length increases. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nSequence parallelism has emerged as one of several complementary approaches to address these challenges. It involves distributing the sequence dimension across multiple processing units, allowing parallel computation on different segments of the input sequence. This technique works alongside other efficiency-focused methods, creating a rich ecosystem of approaches that can be combined to optimize Transformer performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nUnderstanding how sequence parallelism integrates with approaches like sparse attention patterns (which reduce the quadratic complexity by attending to only a subset of tokens) and memory-efficient algorithms (which optimize how activations and gradients are stored and computed) is crucial for practitioners. These methods can be deployed individually or in combination, with each configuration offering different advantages in terms of throughput, memory usage, training stability, and implementation complexity. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Sequence Parallelism Fundamentals", "tldr": "Sequence parallelism addresses the memory constraints of long-sequence Transformer models by splitting input sequences across multiple devices. This technique distributes activation memory along the sequence dimension while maintaining full model parameters on each device, requiring specialized communication patterns like Ring Self-Attention to complete attention calculations. (7 sources)", "text": "\nSequence parallelism (SP) emerged as a distributed computing approach to overcome memory limitations when processing long sequences in Transformer models. Unlike algorithmic solutions that modify the attention mechanism, SP tackles the problem from a systems perspective by distributing computations across multiple devices <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. The core idea is conceptually straightforward: the input sequence is partitioned along the sequence dimension, with each device processing only a sub-sequence while maintaining a full copy of the model parameters <Paper corpusId=\"240070340\" paperTitle=\"(Li et al._1, 2021)\" isShortName></Paper>.\n\nThe primary innovation of sequence parallelism lies in how it handles the interdependent nature of self-attention computations. Since attention requires interactions between all tokens in a sequence, SP implements specialized communication mechanisms like the Ring Self-Attention (RSA) module, where partial query, key, and value embeddings are exchanged among devices to complete the attention calculation <Paper corpusId=\"240070340\" paperTitle=\"(Li et al._1, 2021)\" isShortName></Paper>. This approach enables the processing of significantly longer sequences by dividing the activation memory requirements by the number of parallel devices (s_sp) <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nIn practice, sequence parallelism can be applied to different components of the Transformer architecture. DeepSpeed Ulysses, for example, applies SP to Linear, Normalization, and Dropout layers, with all-to-all communication for query, key, and value tensors to enable efficient multi-head attention computation <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. When combined with tensor parallelism and selective activation recomputation, SP can reduce activation memory by up to 5x while eliminating over 90% of the computation overhead traditionally associated with activation recomputation <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\nIt's important to note that different implementations of sequence parallelism vary in scope. The original implementation in Megatron-LM focused primarily on parallelizing the activations of Dropout and LayerNorm layers <Paper corpusId=\"273963185\" paperTitle=\"(Fujii et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>, while more recent approaches like Context Parallelism extend this to partition all activations along the sequence dimension <Paper corpusId=\"273963185\" paperTitle=\"(Fujii et al., 2024)\" isShortName></Paper>. Compared to tensor parallelism (TP), sequence parallelism offers superior communication efficiency but requires each device to store the entire model parameters, which can be memory intensive for very large models <Paper corpusId=\"273707418\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nWhile originally developed for training, sequence parallelism also presents unique challenges in inference scenarios, particularly for LLM serving systems. These include the need for lower latency with smaller batch sizes, managing the KV cache during autoregressive decoding, and optimizing performance with large fused kernels <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2021)", "paper": {"corpus_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Fuzhao Xue", "authorId": "2144332771"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 102}, "snippets": ["To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences", "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."], "score": 0.93017578125}, {"id": "(Li et al._1, 2021)", "paper": {"corpus_id": 240070340, "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training", "year": 2021, "venue": "International Conference on Parallel Processing", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Zhengda Bian", "authorId": "1996240097"}, {"name": "Hongxin Liu", "authorId": "2110176595"}, {"name": "Boxiang Wang", "authorId": "49292014"}, {"name": "Haichen Huang", "authorId": "2146286212"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Chuan-Qing Wang", "authorId": "2118397214"}, {"name": "Fan Cui", "authorId": "2211340476"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 148}, "snippets": ["Sequence parallelism [21] is proposed to enable long-sequence modeling by breaking the memory wall brought by the large sequence dimension. In sequence parallelism, the model is replicated across devices just like data parallelism. The input data is split along the sequence dimension and each device only keeps a sub-sequence. The self-attention module is replaced with the Ring Self-Attention module such that the partial query, key, and value embeddings are exchanged among devices to complete the self-attention calculation."], "score": 0.80712890625}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267027729, "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qiaoling Chen", "authorId": "2189935212"}, {"name": "Diandian Gu", "authorId": "1393328013"}, {"name": "Guoteng Wang", "authorId": "2263696698"}, {"name": "Xun Chen", "authorId": "2293554318"}, {"name": "Yingtong Xiong", "authorId": "2279899091"}, {"name": "Ting Huang", "authorId": "2279784429"}, {"name": "Qi Hu", "authorId": "2150570711"}, {"name": "Xin Jin", "authorId": "2279869979"}, {"name": "Yonggang Wen", "authorId": "2114783855"}, {"name": "Tianwei Zhang", "authorId": "2146333441"}, {"name": "Peng Sun", "authorId": "2075416290"}], "n_citations": 8}, "snippets": ["Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp", "DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp ."], "score": 0.68310546875}, {"id": "(Zeng et al., 2024)", "paper": {"corpus_id": 269922123, "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pai Zeng", "authorId": "2302327872"}, {"name": "Zhenyu Ning", "authorId": "2302322712"}, {"name": "Jieru Zhao", "authorId": "2302350607"}, {"name": "Weihao Cui", "authorId": "1492129615"}, {"name": "Mengwei Xu", "authorId": "2239403838"}, {"name": "Liwei Guo", "authorId": "2286749413"}, {"name": "XuSheng Chen", "authorId": "2280208638"}, {"name": "Yizhou Shan", "authorId": "2280137293"}], "n_citations": 4}, "snippets": ["\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization."], "score": 0.6708984375}, {"id": "(Korthikanti et al., 2022)", "paper": {"corpus_id": 248693351, "title": "Reducing Activation Recomputation in Large Transformer Models", "year": 2022, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "V. Korthikanti", "authorId": "3111334"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Sangkug Lym", "authorId": "3448779"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "M. Andersch", "authorId": "47304697"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 275}, "snippets": ["Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."], "score": 0.0}, {"id": "(Fujii et al., 2024)", "paper": {"corpus_id": 273963185, "title": "Accelerating Large Language Model Training with 4D Parallelism and Memory Consumption Estimator", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kazuki Fujii", "authorId": "2298889885"}, {"name": "Kohei Watanabe", "authorId": "2330230064"}, {"name": "Rio Yokota", "authorId": "2294362068"}], "n_citations": 2}, "snippets": ["Unlike Sequence Parallelism (Korthikanti et al., 2022), which only parallelizes the activations of Dropout and LayerNorm, Context Parallelism enables partitioning of the model's network inputs and all activations along the sequence dimension."], "score": 0.5078125}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 273707418, "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference", "year": 2024, "venue": "", "authors": [{"name": "Jiarui Fang", "authorId": "2303233521"}, {"name": "Jinzhe Pan", "authorId": "2329908310"}, {"name": "Jiannan Wang", "authorId": "2302815939"}, {"name": "Aoyu Li", "authorId": "2303334242"}, {"name": "Xibo Sun", "authorId": "2328615626"}], "n_citations": 6}, "snippets": ["In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive."], "score": 0.64892578125}], "table": null}, {"title": "Integration with Sparse Attention Methods", "tldr": "Sequence parallelism can be combined with sparse attention mechanisms to further extend context length capabilities, with implementations like DeepSpeed Ulysses demonstrating compatibility with both dense and sparse attention patterns. This integration offers multiplicative benefits in handling extremely long sequences, though optimizing sparse attention implementations within distributed frameworks remains an active research challenge. (8 sources)", "text": "\nSequence parallelism and sparse attention represent two complementary approaches to address the computational challenges of long-context Transformer models. While sequence parallelism tackles the problem from a distributed systems perspective by partitioning sequences across devices, sparse attention methods approach it algorithmically by reducing the quadratic complexity of attention calculations to linear complexity <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nThe integration of these methods creates powerful synergies. As noted by Li et al., \"when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences\" because the memory requirements are reduced by both the sparse pattern and the sequence partitioning factor <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. DeepSpeed Ulysses exemplifies this integration by supporting both dense and sparse attention patterns while maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nPerformance comparisons demonstrate the benefits of this integration. DeepSpeed sequence parallelism with sparse attention can achieve more than 2x throughput compared to Megatron-LM implementations and scale to 4x longer sequence lengths when leveraging ZeRO-3 memory optimizations <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. However, researchers have identified that current implementations still face bottlenecks: \"DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases\" <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nRecent research has focused on specialized sequence parallelism approaches for different attention patterns. For linear attention models, which naturally offer advantages like linear-time training and constant-memory inference, methods such as LASP and LASP-2 optimize communication patterns specifically for the right-product-first property of linear attention <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276259019\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>. LASP-2 replaces ring-style communication with an all-gather collective operation, improving both communication and computation parallelism for linear attention implementations <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>.\n\nThe landscape of sequence parallelism implementations continues to evolve with various approaches for distributing self-attention computation. DeepSpeed-Ulysses partitions input data along the sequence dimension and uses all-to-all collective communication for attention computation, while other methods like LSS (Long Sequence Segmentation) divide sequences into segments distributed among GPUs with fused communication strategies <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. Ring attention represents yet another variant that organizes tokens in a ring structure, balancing computational efficiency with the ability to capture local and semi-distant relationships <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\nImportantly, techniques that exploit sparsity are orthogonal to sequence parallelism methods, offering the potential to significantly scale sequence lengths with a fixed number of nodes <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. This complementary nature allows researchers to combine sparse attention patterns, which reduce complexity to O(n) in certain cases, with distributed computation strategies to extend the applicability of Transformers even in resource-constrained environments <Paper corpusId=\"277621915\" paperTitle=\"(Kottapalli et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2021)", "paper": {"corpus_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Fuzhao Xue", "authorId": "2144332771"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 102}, "snippets": ["To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences", "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."], "score": 0.93017578125}, {"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."], "score": 0.423095703125}, {"id": "(Jacobs et al., 2023)", "paper": {"corpus_id": 262826014, "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "S. A. Jacobs", "authorId": "1683824"}, {"name": "Masahiro Tanaka", "authorId": "2118817779"}, {"name": "Chengming Zhang", "authorId": "2248958850"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "L. Song", "authorId": "2145202945"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 120}, "snippets": ["Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]", "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM", "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."], "score": 0.87060546875}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 276885221, "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Weigao Sun", "authorId": "2346291295"}, {"name": "Disen Lan", "authorId": "2344833625"}, {"name": "Tong Zhu", "authorId": "2349575238"}, {"name": "Xiaoye Qu", "authorId": "2265753258"}, {"name": "Yu Cheng", "authorId": "2344895705"}], "n_citations": 4}, "snippets": ["The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication."], "score": 0.75830078125}, {"id": "(Sun et al._1, 2025)", "paper": {"corpus_id": 276259019, "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Weigao Sun", "authorId": "2225238340"}, {"name": "Disen Lan", "authorId": "2344833625"}, {"name": "Yiran Zhong", "authorId": "2266275708"}, {"name": "Xiaoye Qu", "authorId": "2265753258"}, {"name": "Yu Cheng", "authorId": "2344895705"}], "n_citations": 4}, "snippets": ["Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."], "score": 0.767578125}, {"id": "(Tsaris et al., 2024)", "paper": {"corpus_id": 270062939, "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "A. Tsaris", "authorId": "3395528"}, {"name": "Chengming Zhang", "authorId": "2303676556"}, {"name": "Xiao Wang", "authorId": "2265563075"}, {"name": "Junqi Yin", "authorId": "2244593486"}, {"name": "Siyan Liu", "authorId": "2257073765"}, {"name": "M. Ashfaq", "authorId": "9259766"}, {"name": "Ming Fan", "authorId": "2180823649"}, {"name": "Jong-Youl Choi", "authorId": "2298006038"}, {"name": "M. Wahib", "authorId": "1700692"}, {"name": "Dan Lu", "authorId": "2258305751"}, {"name": "Prasanna Balaprakash", "authorId": "2138151793"}, {"name": "Feiyi Wang", "authorId": "2244140922"}], "n_citations": 1}, "snippets": ["Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."], "score": 0.8359375}, {"id": "(Tomczak et al., 2025)", "paper": {"corpus_id": 276106883, "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nathaniel Tomczak", "authorId": "2276535724"}, {"name": "S. Kuppannagari", "authorId": "2873546"}], "n_citations": 0}, "snippets": ["Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes."], "score": 0.81494140625}, {"id": "(Kottapalli et al., 2025)", "paper": {"corpus_id": 277621915, "title": "Foundation Models for Time Series: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Siva Rama Krishna Kottapalli", "authorId": "2354177739"}, {"name": "Karthik Hubli", "authorId": "2354177337"}, {"name": "Sandeep Chandrashekhara", "authorId": "2354178763"}, {"name": "Garima Jain", "authorId": "2354142186"}, {"name": "Sunayana Hubli", "authorId": "2354177341"}, {"name": "Gayathri Botla", "authorId": "2354179172"}, {"name": "Ramesh Doddaiah", "authorId": "2354179043"}], "n_citations": 0}, "snippets": ["To address this, recent innovations such as sparse attention and linear Transformers have been proposed, reducing the complexity to O(n) [71], [72] in certain cases. These advancements extend the applicability of Transformers to resource-constrained environments."], "score": 0.552734375}], "table": null}, {"title": "Compatibility with Memory-Efficient Algorithms", "tldr": "Sequence parallelism can be effectively integrated with memory-efficient algorithms like FlashAttention to create multiplicative benefits for processing longer sequences. This compatibility requires specialized implementations that optimize communication patterns while preserving the memory benefits of both approaches. (6 sources)", "text": "\nMemory-efficient attention mechanisms have emerged as critical components for scaling Transformer models to longer sequences. These algorithms, exemplified by FlashAttention, reduce peak memory usage by avoiding materialization of large intermediate states like the attention softmax matrix <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. The integration of these memory-efficient approaches with sequence parallelism presents both significant opportunities and technical challenges.\n\nDeepSpeed-Ulysses has demonstrated compatibility with state-of-the-art memory-efficient attention implementations such as FlashAttention v2, achieving both distributed processing and algorithmic efficiency simultaneously <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. This integration enables multiplicative memory savings: sequence parallelism reduces memory requirements by distributing across devices, while memory-efficient attention algorithms minimize the memory footprint within each device. When combined with ZeRO-3 optimizations, this approach enables scaling to sequence lengths 4x longer than alternative implementations like Megatron-LM <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\nHowever, not all sequence parallelism implementations properly integrate with memory-efficient attention algorithms. As Liu et al. note, many sequence parallelism approaches \"ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This limitation has motivated the development of specialized implementations like DISTFLASHATTN, which optimizes for both memory-efficient attention and causal language modeling while maintaining distributed processing capabilities <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nThe communication patterns employed in sequence parallelism significantly impact compatibility with memory-efficient algorithms. DeepSpeed-Ulysses uses an all-to-all collective communication approach for attention computation when segmenting sequences, which helps avoid communication overhead that would otherwise increase with sequence length <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. In contrast, Ring Attention employs blockwise self-attention to split long sequences across devices and overlaps communication of key-value blocks to improve efficiency <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"265551519\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>.\n\nRecent innovations continue to enhance the compatibility between sequence parallelism and memory-efficient algorithms. For example, Bian et al. have developed sequence parallelism implementations that are \"fully compatible with all state-of-the-art attention optimizations\" <Paper corpusId=\"276585307\" paperTitle=\"(Bian et al., 2025)\" isShortName></Paper>. This suggests a trend toward frameworks that seamlessly integrate these complementary approaches, maximizing both memory efficiency and computational parallelism.\n\nThe compatibility between sequence parallelism and memory-efficient algorithms represents a critical advantage in scaling Transformer models to extreme sequence lengths. By combining the distributed processing capabilities of sequence parallelism with the algorithmic efficiencies of approaches like FlashAttention, researchers can extend context lengths beyond what would be possible with either approach in isolation.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 263671659, "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training", "year": 2023, "venue": "", "authors": [{"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "Rulin Shao", "authorId": "2254264970"}, {"name": "Anze Xie", "authorId": "2254221083"}, {"name": "Eric P. Xing", "authorId": "2243336934"}, {"name": "Xuezhe Ma", "authorId": "2378954"}, {"name": "Ion Stoica", "authorId": "2055174324"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "Hao Zhang", "authorId": "2257340589"}], "n_citations": 22}, "snippets": ["Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."], "score": 0.64111328125}, {"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.0}, {"id": "(Jacobs et al., 2023)", "paper": {"corpus_id": 262826014, "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "S. A. Jacobs", "authorId": "1683824"}, {"name": "Masahiro Tanaka", "authorId": "2118817779"}, {"name": "Chengming Zhang", "authorId": "2248958850"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "L. Song", "authorId": "2145202945"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 120}, "snippets": ["Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]", "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM", "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."], "score": 0.87060546875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263828829, "title": "Scaling Laws of RoPE-based Extrapolation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xiaoran Liu", "authorId": "2257094943"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Shuo Zhang", "authorId": "2257086624"}, {"name": "Chen An", "authorId": "2064164220"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Dahua Lin", "authorId": "2258618409"}], "n_citations": 89}, "snippets": ["The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."], "score": 0.56689453125}, {"id": "(Lv et al., 2023)", "paper": {"corpus_id": 265551519, "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kai Lv", "authorId": "2055634356"}, {"name": "Shuo Zhang", "authorId": "2257086624"}, {"name": "Tianle Gu", "authorId": "2269141855"}, {"name": "Shuhao Xing", "authorId": "2269141152"}, {"name": "Jiawei Hong", "authorId": "2269567728"}, {"name": "Keyu Chen", "authorId": "2269551684"}, {"name": "Xiaoran Liu", "authorId": "2257094943"}, {"name": "Yuqing Yang", "authorId": "2145435513"}, {"name": "Honglin Guo", "authorId": "2269420111"}, {"name": "Tengxiao Liu", "authorId": "2136108329"}, {"name": "Yu Sun", "authorId": "2269176065"}, {"name": "Qipeng Guo", "authorId": "3187768"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Xipeng Qiu", "authorId": "2256661980"}], "n_citations": 7}, "snippets": ["Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie."], "score": 0.0}, {"id": "(Bian et al., 2025)", "paper": {"corpus_id": 276585307, "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Yiming Bian", "authorId": "2248690560"}, {"name": "Arun Somani", "authorId": "35088844"}], "n_citations": 0}, "snippets": ["More importantly, it is fully compatible with all state-of-the-art attention optimizations."], "score": 0.83056640625}], "table": null}, {"title": "Notable Implementation Frameworks", "tldr": "Several major frameworks have implemented sequence parallelism for long-sequence Transformer models, each with distinct approaches to communication patterns and integration with other parallelism strategies. These implementations vary in their compatibility with memory-efficient algorithms and attention mechanisms, reflecting different trade-offs between communication efficiency, memory usage, and computational performance. (7 sources)", "text": "\n- **DeepSpeed-Ulysses**: A comprehensive framework that supports both dense and sparse attention patterns while maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. It partitions input data along the sequence dimension and uses all-to-all collective communication for attention computation, avoiding communication overhead that would otherwise increase with sequence length <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. DeepSpeed-Ulysses can achieve more than 2x throughput compared to Megatron-LM implementations, and when leveraging ZeRO-3 memory optimizations, it can scale to 4x longer sequence lengths <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>.\n\n- **Megatron-SP**: Adopts a sequence parallelism technique that integrates tightly with tensor parallelism, partitioning sequences along the sequence dimension and using all-gather and reduce-scatter collectives to aggregate the query, key, value (QKV) projections for attention computation <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Unlike other implementations, Megatron-SP only applies sequence parallelism during Dropout and Layernorm operations, reducing activation redundancy <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. When combined with tensor parallelism and selective activation recomputation, it can reduce activation memory by 5x while eliminating over 90% of the computation overhead traditionally associated with activation recomputation <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\n- **Ring Attention**: Organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. This approach enhances Transformer scalability by distributing long sequences across multiple devices and overlapping the communication of key-value pairs with the computation of blockwise attention <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. Ring Attention offers a balance between computational efficiency and the ability to capture local and semi-distant relationships, though it may still have limitations for tasks requiring long-range dependencies <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\n- **Long Sequence Segmentation (LSS)**: Divides long sequences into segments distributed among GPUs, with each GPU computing partial self-attention for its segment. LSS introduces a fused communication strategy specifically designed to minimize communication overhead <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\n- **ColossalAI-SP**: One of the first frameworks to propose segmentation and parallelism for the sequence dimension, complementing tensor parallelism for the hidden dimension and pipeline parallelism for model depth <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\n- **Context Parallelism (CP)**: Often implemented as an extension of sequence parallelism, particularly for ultra-long sequences (e.g., exceeding 200k tokens). CP can be configured as an outer parallelism layer surrounding inner TP-SP implementations. During non-attention computations, each layer processes a fraction of the input sequence, while attention computations use all-to-all communication to transpose the sequence, enabling each rank to process attention with full context length but only a fraction of the attention heads <Paper corpusId=\"278714643\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.\n\n- **Dynamic Sequence Parallelism (DSP)**: Enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Jacobs et al., 2023)", "paper": {"corpus_id": 262826014, "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "S. A. Jacobs", "authorId": "1683824"}, {"name": "Masahiro Tanaka", "authorId": "2118817779"}, {"name": "Chengming Zhang", "authorId": "2248958850"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "L. Song", "authorId": "2145202945"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 120}, "snippets": ["Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]", "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM", "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."], "score": 0.87060546875}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 272310078, "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinghan Yao", "authorId": "2266686678"}, {"name": "Sam Ade Jacobs", "authorId": "2297768912"}, {"name": "Masahiro Tanaka", "authorId": "2226706029"}, {"name": "Olatunji Ruwase", "authorId": "2537545"}, {"name": "A. Shafi", "authorId": "1685408"}, {"name": "H. Subramoni", "authorId": "1802958"}, {"name": "Dhabaleswar K. Panda", "authorId": "2222521323"}], "n_citations": 2}, "snippets": ["Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices", "Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices", "DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments."], "score": 0.81103515625}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263828829, "title": "Scaling Laws of RoPE-based Extrapolation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xiaoran Liu", "authorId": "2257094943"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Shuo Zhang", "authorId": "2257086624"}, {"name": "Chen An", "authorId": "2064164220"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Dahua Lin", "authorId": "2258618409"}], "n_citations": 89}, "snippets": ["The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."], "score": 0.56689453125}, {"id": "(Korthikanti et al., 2022)", "paper": {"corpus_id": 248693351, "title": "Reducing Activation Recomputation in Large Transformer Models", "year": 2022, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "V. Korthikanti", "authorId": "3111334"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Sangkug Lym", "authorId": "3448779"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "M. Andersch", "authorId": "47304697"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 275}, "snippets": ["Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."], "score": 0.0}, {"id": "(Tsaris et al., 2024)", "paper": {"corpus_id": 270062939, "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "A. Tsaris", "authorId": "3395528"}, {"name": "Chengming Zhang", "authorId": "2303676556"}, {"name": "Xiao Wang", "authorId": "2265563075"}, {"name": "Junqi Yin", "authorId": "2244593486"}, {"name": "Siyan Liu", "authorId": "2257073765"}, {"name": "M. Ashfaq", "authorId": "9259766"}, {"name": "Ming Fan", "authorId": "2180823649"}, {"name": "Jong-Youl Choi", "authorId": "2298006038"}, {"name": "M. Wahib", "authorId": "1700692"}, {"name": "Dan Lu", "authorId": "2258305751"}, {"name": "Prasanna Balaprakash", "authorId": "2138151793"}, {"name": "Feiyi Wang", "authorId": "2244140922"}], "n_citations": 1}, "snippets": ["Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."], "score": 0.8359375}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Fuzhao Xue", "authorId": "2144332771"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 102}, "snippets": ["To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences", "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."], "score": 0.93017578125}, {"id": "(Shi et al., 2025)", "paper": {"corpus_id": 278714643, "title": "Aquarius: A Family of Industry-Level Video Generation Models for Marketing Scenarios", "year": 2025, "venue": "", "authors": [{"name": "Huafeng Shi", "authorId": "2362191870"}, {"name": "Jianzhong Liang", "authorId": "2362279072"}, {"name": "Rongchang Xie", "authorId": "2332574955"}, {"name": "Xian Wu", "authorId": "2357119154"}, {"name": "Cheng Chen", "authorId": "2362091271"}, {"name": "Chang Liu", "authorId": "2356750113"}], "n_citations": 0}, "snippets": ["Innermost TP-SP. Among 8 ranks within a node, we adopt Tensor Parallelism(TP) (Shoeybi et al., 2019) and Sequence Parallelism(SP) (Korthikanti et al., 2022) to partition the model states and activations. In each Transformer block, we employ a dedicated MLP (a Linear layer followed by a SiLU) to incorporate conditioning embedding for Adaptive Layer Normalization (AdaLN), aiming to enhance the model's capacity to capture conditional information. However, this design incurs substantial memory overhead: these dedicated Linear collectively increase the model's total parameter count by over 3B and introduce redundant model states exceeding 20GB per rank. To address this issue, we partition these Linear layers in a column parallel fashion with their computation and parameters parallelized across devices. Specifically, during the forward pass, all-gather reconstructs the complete Linear output, while reduce-scatter synchronizes gradients across the sequence-parallel group before the Linear in the backward pass. \n\nMid-Optional CP. While TP-SP supports training with moderately long contexts, Context Parallelism (CP) (Jacobs et al., 2023) is optionally enabled only for ultra-long sequences (exceeds 200k tokens). \n\nFor non-attention computations, each layer processes 1/CP of the input sequence. Before and after attention computations, all-to-all communication is applied to transpose the sequence, enabling each rank to process attention with full context length but only 1/CP of the attention heads. Since CP incurs lower communication overhead than TP-SP, we configure TP-SP as inner parallelism and CP as outer parallelism."], "score": 0.65576171875}], "table": null}, {"title": "Technical Challenges and Solutions", "tldr": "Sequence parallelism implementations face significant challenges including communication overhead, compatibility with memory-efficient algorithms, and balancing computation with communication. Recent advances have addressed these issues through optimized communication patterns, blockwise attention computation, and specialized algorithms for causal language modeling. (9 sources)", "text": "\nImplementing sequence parallelism for Transformer models introduces several technical challenges that researchers have worked to overcome. One of the primary obstacles is the substantial communication overhead required for distributed attention computation, which can significantly impact performance. As Liu et al. note, \"sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation,\" limiting scalability for training and inference in large-context scenarios <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nDifferent approaches to managing this communication overhead have emerged. Ring Self-Attention (RSA) was an early attempt to reduce communication costs compared to standard sequence parallelism by using ring-style communication patterns <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. However, this approach still struggles with overlapping communication and computation due to \"constraints of arithmetic intensity\" <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. More recent implementations like Ring Attention have improved upon this by using blockwise self-attention to split long sequences across devices while overlapping the communication of key-value blocks <Paper corpusId=\"265551519\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>, enhancing scalability through better communication-computation balance.\n\nAnother significant challenge is achieving compatibility between sequence parallelism and memory-efficient attention algorithms like FlashAttention <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>. Many sequence parallelism implementations \"ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>, limiting their ability to fully optimize memory usage. This has led to specialized solutions like DISTFLASHATTN, which is specifically designed to be compatible with both memory-efficient attention and causal language modeling <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nFor LLM serving systems, sequence parallelism introduces unique challenges compared to training scenarios. These include: \"(1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance\" <Paper corpusId=\"269922123\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>. These considerations necessitate different optimization strategies for inference compared to training.\n\nVarious communication patterns have been explored to address these challenges. DeepSpeed-Ulysses employs \"an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs\" <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. This approach avoids communication overhead that would otherwise increase with sequence length. In contrast, Megatron-SP takes a more targeted approach, applying \"sequence parallelism only during Dropout and Layernorm operations, thereby reducing activation redundancy\" <Paper corpusId=\"263828829\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>.\n\nThe Long Sequence Segmentation (LSS) approach tackles communication overhead by introducing \"a fused communication strategy to minimize the communication overhead\" when dividing long sequences into segments distributed among GPUs <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>. Additionally, Dynamic Sequence Parallelism (DSP) provides flexibility by \"dynamically switching the parallelism dimension according to the current computation stage\" for multi-dimensional transformers <Paper corpusId=\"270062939\" paperTitle=\"(Tsaris et al., 2024)\" isShortName></Paper>.\n\nThese technical solutions demonstrate the ongoing evolution of sequence parallelism to address the computational and memory challenges of processing increasingly long sequences in Transformer models. By optimizing communication patterns, improving compatibility with memory-efficient algorithms, and developing specialized approaches for different use cases, researchers continue to push the boundaries of what's possible with long-context Transformer models.", "citations": [{"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 263608461, "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hao Liu", "authorId": "2256317240"}, {"name": "Matei Zaharia", "authorId": "2253469012"}, {"name": "Pieter Abbeel", "authorId": "2253464956"}], "n_citations": 258}, "snippets": ["Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios."], "score": 0.646484375}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Fuzhao Xue", "authorId": "2144332771"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 102}, "snippets": ["To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences", "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."], "score": 0.93017578125}, {"id": "(Lv et al., 2023)", "paper": {"corpus_id": 265551519, "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kai Lv", "authorId": "2055634356"}, {"name": "Shuo Zhang", "authorId": "2257086624"}, {"name": "Tianle Gu", "authorId": "2269141855"}, {"name": "Shuhao Xing", "authorId": "2269141152"}, {"name": "Jiawei Hong", "authorId": "2269567728"}, {"name": "Keyu Chen", "authorId": "2269551684"}, {"name": "Xiaoran Liu", "authorId": "2257094943"}, {"name": "Yuqing Yang", "authorId": "2145435513"}, {"name": "Honglin Guo", "authorId": "2269420111"}, {"name": "Tengxiao Liu", "authorId": "2136108329"}, {"name": "Yu Sun", "authorId": "2269176065"}, {"name": "Qipeng Guo", "authorId": "3187768"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Xipeng Qiu", "authorId": "2256661980"}], "n_citations": 7}, "snippets": ["Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie."], "score": 0.0}, {"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.0}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 263828829, "title": "Scaling Laws of RoPE-based Extrapolation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xiaoran Liu", "authorId": "2257094943"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Shuo Zhang", "authorId": "2257086624"}, {"name": "Chen An", "authorId": "2064164220"}, {"name": "Xipeng Qiu", "authorId": "1767521"}, {"name": "Dahua Lin", "authorId": "2258618409"}], "n_citations": 89}, "snippets": ["The second challenge is parallelism for sequence length, that is, Sequence Parallelism (SP) (Li et al., 2021). Since the memory and calculation of a single device are limited, ColossalAI-SP (Li et al., 2021) first proposes the segmentation and parallelism for the sequence dimension in addition to tensor parallelism for the hidden dimension and pipeline parallelism for model depth. On this basis, Ring Attention (Lv et al., 2023) uses blockwise self-attention to split long sequences into different devices and overlap the communication of key-value blocks. Besides, LightSeq (Li et al., 2023b) further improves the efficiency of long sequence modeling through load balancing for causal language modelings and a re-materialization-aware checkpointing strategy. Although the above sequence parallelism can achieve infinitely long sequence modeling, they ignore the compatibility with the existing efficient self-attention mechanisms such as FlashAttention (Dao et al., 2022)Dao, 2023) to achieve input with almost infinity. On the contrary, Megatron-SP (Korthikanti et al., 2022) only uses sequence parallelism during Dropout and Layernorm operations, thereby reducing activation redundancy. In addition, DeepSpeed-Ulysses (Jacobs et al., 2023) uses an alternative all-to-all collective communication gathering weight for attention computation when segmenting the sequence, avoiding communication overhead that originally increases with length."], "score": 0.56689453125}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 263671659, "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training", "year": 2023, "venue": "", "authors": [{"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "Rulin Shao", "authorId": "2254264970"}, {"name": "Anze Xie", "authorId": "2254221083"}, {"name": "Eric P. Xing", "authorId": "2243336934"}, {"name": "Xuezhe Ma", "authorId": "2378954"}, {"name": "Ion Stoica", "authorId": "2055174324"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "Hao Zhang", "authorId": "2257340589"}], "n_citations": 22}, "snippets": ["Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."], "score": 0.64111328125}, {"id": "(Zeng et al., 2024)", "paper": {"corpus_id": 269922123, "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pai Zeng", "authorId": "2302327872"}, {"name": "Zhenyu Ning", "authorId": "2302322712"}, {"name": "Jieru Zhao", "authorId": "2302350607"}, {"name": "Weihao Cui", "authorId": "1492129615"}, {"name": "Mengwei Xu", "authorId": "2239403838"}, {"name": "Liwei Guo", "authorId": "2286749413"}, {"name": "XuSheng Chen", "authorId": "2280208638"}, {"name": "Yizhou Shan", "authorId": "2280137293"}], "n_citations": 4}, "snippets": ["\u2022 SP was first introduced by Li et al. [97] and has been widely used in distributed LLM training frameworks such as Megatron (Korthikanti et al., 2022) and Deepspeed [99].In the context of LLM serving systems, new challenges emerge: (1) LLM serving is usually latency-sensitive and thus requires much smaller batch sizes than LLM training; (2) LLM serving has an autoregressive decode phase, where the sequence length is only one, but it requires large memory for KV cache storage; (3) LLM serving usually relies on large fused kernels for improving performance.While the feed-forward network (FFN) computations for each token in a sequence are linearly independent, the computations for attention are not.Consequently, substantial data exchange is involved when computing distributed attention using SP, thereby opening significant space for performance optimization."], "score": 0.6708984375}, {"id": "(Tsaris et al., 2024)", "paper": {"corpus_id": 270062939, "title": "Sequence Length Scaling in Vision Transformers for Scientific Images on Frontier", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "A. Tsaris", "authorId": "3395528"}, {"name": "Chengming Zhang", "authorId": "2303676556"}, {"name": "Xiao Wang", "authorId": "2265563075"}, {"name": "Junqi Yin", "authorId": "2244593486"}, {"name": "Siyan Liu", "authorId": "2257073765"}, {"name": "M. Ashfaq", "authorId": "9259766"}, {"name": "Ming Fan", "authorId": "2180823649"}, {"name": "Jong-Youl Choi", "authorId": "2298006038"}, {"name": "M. Wahib", "authorId": "1700692"}, {"name": "Dan Lu", "authorId": "2258305751"}, {"name": "Prasanna Balaprakash", "authorId": "2138151793"}, {"name": "Feiyi Wang", "authorId": "2244140922"}], "n_citations": 1}, "snippets": ["Sequence parallelism, which distributes the self-attention computation in transformer models across the sequence dimension, has been an active area of research, with many developed methods employing various approaches.DeepSpeed-Ulysses [15]introduces a suite of system optimizations specifically designed for the efficient training of extreme long sequence transformer models.DeepSpeed-Ulysses partitions the input data along the sequence dimension, which allows the system to handle much longer sequences.For attention computation, it employs an all-to-all collective communication to ensure that each GPU receives a complete sequence, but only for a non-overlapping subset of the attention heads, allowing for parallel computation of attention across GPUs.LSS [16] divides a long sequence into segments distributed among GPUs, with each GPU computing a partial selfattention for its segment.It also introduces a fused communication strategy to minimize the communication overhead.Dynamic Sequence Parallelism (DSP) [22] enables efficient sequence parallelism for multi-dimensional transformers by dynamically switching the parallelism dimension according to the current computation stage.Ring attention [23], [24] is another variant that organizes tokens in a ring structure, allowing each token to attend to a fixed number of adjacent tokens in the sequence.This method offers a compromise between dense and sparse attention, balancing computational efficiency with the ability to capture local and semi-distant relationships.However, ring attention may still fall short in tasks where long-range dependencies are crucial."], "score": 0.8359375}, {"id": "(Korthikanti et al., 2022)", "paper": {"corpus_id": 248693351, "title": "Reducing Activation Recomputation in Large Transformer Models", "year": 2022, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "V. Korthikanti", "authorId": "3111334"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Sangkug Lym", "authorId": "3448779"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "M. Andersch", "authorId": "47304697"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 275}, "snippets": ["Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."], "score": 0.0}], "table": null}, {"title": "Synergies Among Approaches", "tldr": "The combination of sequence parallelism with other efficiency techniques creates multiplicative benefits for extending Transformer context lengths. These approaches complement each other in orthogonal ways, with sequence parallelism addressing distributed computing challenges while sparse attention and memory-efficient algorithms tackle algorithmic complexity. (9 sources)", "text": "\nThe integration of sequence parallelism with other efficiency techniques creates powerful synergies that enable Transformer models to process significantly longer contexts than would be possible with any single approach. Sequence parallelism works orthogonally to other parallelism strategies, allowing for straightforward combination with tensor parallelism, data parallelism, and other distributed training techniques <Paper corpusId=\"258987968\" paperTitle=\"(Liu et al._2, 2023)\" isShortName></Paper>. This orthogonality means that the benefits of sequence parallelism multiply with those of other approaches rather than merely adding to them.\n\nWhen sequence parallelism is combined with sparse attention patterns, the results are particularly impressive. DeepSpeed-Ulysses demonstrates that sequence parallelism works effectively with both dense and sparse attention implementations, maintaining compatibility with efficient attention implementations like FlashAttention v2 <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. The performance benefits are substantial, with DeepSpeed's sequence parallelism showing more than 2x throughput compared to Megatron-LM when using sparse attention patterns <Paper corpusId=\"262826014\" paperTitle=\"(Jacobs et al., 2023)\" isShortName></Paper>. This combination addresses both the memory requirements and computational complexity of long-sequence processing simultaneously.\n\nTechniques that exploit sparsity in attention patterns are particularly complementary to sequence parallelism, with the potential to \"significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes\" <Paper corpusId=\"276106883\" paperTitle=\"(Tomczak et al., 2025)\" isShortName></Paper>. Sparse attention methods like those proposed by Zaheer et al. reduce memory footprints by computing only a subset of the attention score, though this approach may impact model performance <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220831004\" paperTitle=\"(Zaheer et al., 2020)\" isShortName></Paper>.\n\nRecent innovations have focused on optimizing the compatibility between sequence parallelism and memory-efficient algorithms. Some implementations are now \"fully compatible with all state-of-the-art attention optimizations\" <Paper corpusId=\"276585307\" paperTitle=\"(Bian et al., 2025)\" isShortName></Paper>, ensuring that the memory efficiency benefits of algorithms like FlashAttention can be fully realized within a distributed sequence parallelism framework. FlashAttention itself uses tiling to reduce memory reads/writes between GPU high bandwidth memory and on-chip SRAM, complementing the distributed memory approach of sequence parallelism <Paper corpusId=\"263671659\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"249151871\" paperTitle=\"(Dao et al., 2022)\" isShortName></Paper>.\n\nFor linear attention models, which naturally provide linear-time training and constant-memory inference advantages, specialized sequence parallelism implementations like LASP and LASP-2 have been developed <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper>. These implementations leverage the unique \"right-product-first\" property of linear attention to create efficient communication patterns. LASP-2, in particular, replaces ring-style communication with an all-gather collective operation, improving both communication and computation parallelism for linear attention implementations <Paper corpusId=\"276885221\" paperTitle=\"(Sun et al., 2025)\" isShortName></Paper> <Paper corpusId=\"276259019\" paperTitle=\"(Sun et al._1, 2025)\" isShortName></Paper>.\n\nThe synergistic integration of sequence parallelism with other efficiency techniques has enabled researchers to push the boundaries of what's possible with Transformer models, creating systems capable of processing extremely long sequences that would be infeasible with any single approach alone. This combination of distributed computing strategies with algorithmic optimizations represents a powerful framework for addressing the computational challenges of next-generation language models.", "citations": [{"id": "(Liu et al._2, 2023)", "paper": {"corpus_id": 258987968, "title": "Blockwise Parallel Transformer for Large Context Models", "year": 2023, "venue": "", "authors": [{"name": "Hao Liu", "authorId": "2143856672"}, {"name": "P. Abbeel", "authorId": "1689992"}], "n_citations": 11}, "snippets": ["Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49,16,55,27,59,31,(Rasley et al., 2020). Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism."], "score": 0.74169921875}, {"id": "(Jacobs et al., 2023)", "paper": {"corpus_id": 262826014, "title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "S. A. Jacobs", "authorId": "1683824"}, {"name": "Masahiro Tanaka", "authorId": "2118817779"}, {"name": "Chengming Zhang", "authorId": "2248958850"}, {"name": "Minjia Zhang", "authorId": "67016465"}, {"name": "L. Song", "authorId": "2145202945"}, {"name": "Samyam Rajbhandari", "authorId": "32817044"}, {"name": "Yuxiong He", "authorId": "2145020341"}], "n_citations": 120}, "snippets": ["Fully general and implementation agnostic attention: DeepSpeed sequence parallelism (Ulysses) supports dense as well as sparse attention, and it works with efficient attention implementations such as FlashAttention v2 [Dao, 2023]", "We observe similar trends with sparse attention as dense attention experiments. We observe more than 2x throughput performance of DeepSpeed sequence parallelism compared to Megatron-LM. For memory saving, DeepSpeed sequence parallelism leveraging ZeRO-3 scales to 4x longer sequence lengths than Megatron-LM", "DeepSpeed sequence parallelism outperforms Megatron-LM for sequence length that can be run with both. In fact, the current DeepSpeed throughput is bottlenecked by the local sparse attention implementation, and as a result DeepSpeed throughput decreases as the sequence length increases. We expect this gap in performance between DeepSpeed and Megatron-LM to increase further for larger sequence lengths as we improve the performance of the local sparse attention implementation in future."], "score": 0.87060546875}, {"id": "(Tomczak et al., 2025)", "paper": {"corpus_id": 276106883, "title": "Longer Attention Span: Increasing Transformer Context Length with Sparse Graph Processing Techniques", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nathaniel Tomczak", "authorId": "2276535724"}, {"name": "S. Kuppannagari", "authorId": "2873546"}], "n_citations": 0}, "snippets": ["Our technique, that exploits sparsity, is orthogonal to these methods and has the potential to significantly scale the sequence length that can be achieved using these techniques for a fixed number of nodes."], "score": 0.81494140625}, {"id": "(Li et al., 2023)", "paper": {"corpus_id": 263671659, "title": "DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training", "year": 2023, "venue": "", "authors": [{"name": "Dacheng Li", "authorId": "2117961435"}, {"name": "Rulin Shao", "authorId": "2254264970"}, {"name": "Anze Xie", "authorId": "2254221083"}, {"name": "Eric P. Xing", "authorId": "2243336934"}, {"name": "Xuezhe Ma", "authorId": "2378954"}, {"name": "Ion Stoica", "authorId": "2055174324"}, {"name": "Joseph Gonzalez", "authorId": "2254681613"}, {"name": "Hao Zhang", "authorId": "2257340589"}], "n_citations": 22}, "snippets": ["Memory-efficient attention. (Dao et al., 2022) and Lefaudeux et al. (2022) propose to use an online normalizer (Milakov & Gimelshein, 2018) to compute the attention in a blockwise and memory-efficient way. It reduces peak memory usage by not materializing large intermediate states, e.g. the attention softmax matrix. In addition, research on sparse attention computes only a sparse subset of the attention score, which also reduces the memory footprints yet may lead to inferior performance (Beltagy et al., 2020;Sun et al., 2022;(Zaheer et al., 2020). In this work, we limit our scope to exact attention. \n\nSequence parallelism and ring attention Ring Self-Attention (Li et al., 2021) is among the first to parallelize Transformers in the sequence dimension. However, its distributed attention design is not optimized for causal language modeling and incompatible with memory-efficient attention, which are crucial for long-context LLM training. Ring Attention (Liu et al., 2023) proposes to compute distributed attention in a memory-efficient blockwise pattern. However, it is also not optimized for causal language modeling, leading to 2\u00d7 extra computation. DISTFLASHATTN optimizes for both memory-efficient attention and causal language modeling."], "score": 0.64111328125}, {"id": "(Zaheer et al., 2020)", "paper": {"corpus_id": 220831004, "title": "Big Bird: Transformers for Longer Sequences", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "M. Zaheer", "authorId": "1771307"}, {"name": "Guru Guruganesh", "authorId": "1947314"}, {"name": "Kumar Avinava Dubey", "authorId": "89890133"}, {"name": "J. Ainslie", "authorId": "1643737606"}, {"name": "Chris Alberti", "authorId": "114577307"}, {"name": "Santiago Onta\u00f1\u00f3n", "authorId": "1722671"}, {"name": "Philip Pham", "authorId": "38552691"}, {"name": "Anirudh Ravula", "authorId": "101210026"}, {"name": "Qifan Wang", "authorId": "145196279"}, {"name": "Li Yang", "authorId": "113906155"}, {"name": "Amr Ahmed", "authorId": "143629707"}], "n_citations": 2103}, "snippets": ["Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data."], "score": 0.423095703125}, {"id": "(Bian et al., 2025)", "paper": {"corpus_id": 276585307, "title": "CQS-Attention: Scaling Up the Standard Attention Computation for Infinitely Long Sequences", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "Yiming Bian", "authorId": "2248690560"}, {"name": "Arun Somani", "authorId": "35088844"}], "n_citations": 0}, "snippets": ["More importantly, it is fully compatible with all state-of-the-art attention optimizations."], "score": 0.83056640625}, {"id": "(Dao et al., 2022)", "paper": {"corpus_id": 249151871, "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tri Dao", "authorId": "24593911"}, {"name": "Daniel Y. Fu", "authorId": "49577833"}, {"name": "Stefano Ermon", "authorId": "2490652"}, {"name": "A. Rudra", "authorId": "1755572"}, {"name": "Christopher R'e", "authorId": "2061444681"}], "n_citations": 2285}, "snippets": ["Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy)."], "score": 0.0}, {"id": "(Sun et al., 2025)", "paper": {"corpus_id": 276885221, "title": "Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Weigao Sun", "authorId": "2346291295"}, {"name": "Disen Lan", "authorId": "2344833625"}, {"name": "Tong Zhu", "authorId": "2349575238"}, {"name": "Xiaoye Qu", "authorId": "2265753258"}, {"name": "Yu Cheng", "authorId": "2344895705"}], "n_citations": 4}, "snippets": ["The existing methods, LASP (Sun et al., 2025) and its improved version LASP-2 (Sun et al., 2025), are designed specifically to leverage the right-productfirst property of linear attention techniques for efficient sequence parallelism (SP). LASP employs a point-to-point ring-style communication pattern, facilitating the exchange of incremental memory states across devices. This communication pattern is particularly effective for managing dependencies while minimizing the data transferred between devices, enhancing the scalability of SP. LASP-2 further refines this approach by replacing the ring-style communication with an all-gather collective communication operation, streamlining the entire communication process. This modification not only simplifies the communication structure but also improves the parallelism of computation and communication."], "score": 0.75830078125}, {"id": "(Sun et al._1, 2025)", "paper": {"corpus_id": 276259019, "title": "LASP-2: Rethinking Sequence Parallelism for Linear Attention and Its Hybrid", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Weigao Sun", "authorId": "2225238340"}, {"name": "Disen Lan", "authorId": "2344833625"}, {"name": "Yiran Zhong", "authorId": "2266275708"}, {"name": "Xiaoye Qu", "authorId": "2265753258"}, {"name": "Yu Cheng", "authorId": "2344895705"}], "n_citations": 4}, "snippets": ["Linear sequence modeling approaches, such as linear attention, provide advantages like linear-time training and constant-memory inference over sequence lengths. However, existing sequence parallelism (SP) methods are either not optimized for the right-product-first feature of linear attention or use a ring-style communication strategy, which results in lower computation parallelism, limits their scalability for longer sequences in distributed systems. In this paper, we introduce LASP-2, a new SP method to enhance both communication and computation parallelism when training linear attention transformer models with very-long input sequences. Compared to previous work LASP, LASP-2 rethinks the minimal communication requirement for SP on linear attention layers, reorganizes the whole communication-computation workflow of LASP. In this way, only one single AllGather collective communication is needed on intermediate memory states, whose sizes are independent of the sequence length, leading to significant improvements of both communication and computation parallelism, as well as their overlap. Additionally, we extend LASP-2 to LASP-2H by applying similar communication redesign to standard attention modules, offering an efficient SP solution for hybrid models that blend linear and standard attention layers. Our evaluation on a Linear-Llama3 model, a variant of Llama3 with linear attention replacing standard attention, demonstrates the effectiveness of LASP-2 and LASP-2H. Specifically, LASP-2 achieves training speed improvements of 15.2% over LASP and 36.6% over Ring Attention, with a sequence length of 2048K across 64 GPUs. The Code is released as a part of: https://github.com/OpenSparseLLMs/Linear-MoE."], "score": 0.767578125}], "table": null}, {"title": "Trade-offs and Limitations", "tldr": "While sequence parallelism offers significant benefits for extending Transformer context lengths, it comes with notable trade-offs including substantial communication overhead, practical implementation challenges in large-scale environments, and memory limitations when storing full model parameters on each device. (6 sources)", "text": "\nSequence parallelism, despite its advantages for processing long sequences, introduces significant communication overhead that represents one of its primary limitations. Liu et al. note that \"sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation,\" which severely restricts its applicability for training and inference in large-context scenarios <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. Earlier approaches like Ring Self-Attention (RSA) attempted to mitigate this issue by using ring-style communication to reduce costs compared to standard sequence parallelism, but these methods still struggle with effectively overlapping communication and computation due to \"constraints of arithmetic intensity\" <Paper corpusId=\"263608461\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper> <Paper corpusId=\"246017095\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nAnother significant limitation stems from implementation complexity and practical deployment challenges. DeepSpeed Ulysses, while achieving impressive performance through partitioning along the sequence dimension and using all-to-all collective communication, \"may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments\" <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. These implementation challenges can limit the practical adoption of sequence parallelism approaches in production environments.\n\nMemory utilization presents another trade-off when considering sequence parallelism relative to other parallel computation approaches. While sequence parallelism demonstrates \"superior communication efficiency compared to TP (tensor parallelism), each device stores the entire model parameters, which can be memory intensive\" <Paper corpusId=\"273707418\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This requirement means that sequence parallelism may not be suitable for extremely large models where parameter storage becomes a bottleneck, even if activation memory is effectively distributed.\n\nImplementation design choices can also create constraints. DeepSpeed Ulysses, for example, applies \"sequence parallelism on Linear, Norm, and Dropout layers\" with \"an all-to-all communication... applied to the output of Linear qkv\" to enable efficient computation across GPUs <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, this approach imposes a constraint \"that the number of attention heads D should not exceed s_sp\" (the sequence parallelism factor), limiting flexibility in model architecture design <Paper corpusId=\"267027729\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nCommunication patterns represent another key trade-off area among different sequence parallelism implementations. Megatron-SP's approach tightly integrates with tensor parallelism and uses all-gather and reduce-scatter collectives, but its \"communication volume... increases linearly with the sequence length regardless of the number of compute devices\" <Paper corpusId=\"272310078\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"248693351\" paperTitle=\"(Korthikanti et al., 2022)\" isShortName></Paper>. This scaling behavior contrasts with approaches like DeepSpeed Ulysses that maintain constant communication volume as sequence length increases, highlighting the different trade-offs in communication efficiency among implementation approaches.", "citations": [{"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 263608461, "title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Hao Liu", "authorId": "2256317240"}, {"name": "Matei Zaharia", "authorId": "2253469012"}, {"name": "Pieter Abbeel", "authorId": "2253464956"}], "n_citations": 258}, "snippets": ["Tensor parallelism can only reduce parts of activations memory and sequence parallelism introduces a significant communication overhead that cannot be fully overlapped with computation. Prior work has studied sharding along sequence and attention heads, and gathering sequences via an optimized all-to-all topology, achieving reduced communication [17]. However, this method is restricted by the number of attention heads and requires gathering the full sequence on each device. In comparison, our approach fully overlaps communication with blockwise computation, enhancing its scalability. Prior work extends sequence parallelism for computing self-attention using a ring topology (Li et al., 2021), which reduces the communication cost compared to standard sequence parallelism. However, overlapping communication with computation remains challenging due to the constraints of arithmetic intensity. The communication overheads render this approach infeasible for training and inference in largecontext scenarios."], "score": 0.646484375}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 246017095, "title": "Sequence Parallelism: Long Sequence Training from System Perspective", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shenggui Li", "authorId": "2153703322"}, {"name": "Fuzhao Xue", "authorId": "2144332771"}, {"name": "Yongbin Li", "authorId": "2180329661"}, {"name": "Yang You", "authorId": "144259229"}], "n_citations": 102}, "snippets": ["To model long sequences, existing works mainly focus on sparse attention (e.g., [Zaheer et al., 2020]) with linear instead of quadratic space complexity. In this paper, we aim to solve the long sequence modeling problem from the distributed system perspective. Compared with sparse attention, we devote ourselves to designing and implementing a system instead of a deep learning algorithm to train attentionbased models with longer sequences", "Our RSA focuses on full self-attention in this paper. According to Table 2, when we use sparse attention with linear memory usage, theoretically, our SP is expected to handle infinitely long sequences, because three terms of memory usage include L/N."], "score": 0.93017578125}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 272310078, "title": "Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jinghan Yao", "authorId": "2266686678"}, {"name": "Sam Ade Jacobs", "authorId": "2297768912"}, {"name": "Masahiro Tanaka", "authorId": "2226706029"}, {"name": "Olatunji Ruwase", "authorId": "2537545"}, {"name": "A. Shafi", "authorId": "1685408"}, {"name": "H. Subramoni", "authorId": "1802958"}, {"name": "Dhabaleswar K. Panda", "authorId": "2222521323"}], "n_citations": 2}, "snippets": ["Megatron-SP (Korthikanti et al., 2022)) adopts a sequence parallelism technique which is tightly integrated with its tensor parallelism. In this approach, sequences are partitioned along the sequence dimension, and all-gather and reduce-scatter collectives are employed to aggregate the QKV (query, key, value) projections for attention computation. The communication complexity analysis indicates that, in contrast to our approach, the communication volume in Megatron-SP's sequence parallelism increases linearly with the sequence length regardless of the number of compute devices", "Ring Attention (Liu et al., 2023) enhances Transformer's scalability by distributing long sequences across multiple devices. This innovative approach overlaps the communication of key-value pairs with the computation of blockwise attention, effectively increasing the feasible sequence length proportionally to the number of available devices", "DeepSpeed Ulysses (Jacobs et al., 2023) tackles the challenges of sequence parallelism by partitioning input data along the sequence dimension and utilizing an efficient allto-all collective communication strategy for attention computations. Although this method maintains a constant communication volume regardless of the increase in sequence lengths and device counts, achieving significant speedups and scalability, it may still encounter practical hurdles in deployment related to large-scale clusters and the optimization of communication patterns across diverse computing environments."], "score": 0.81103515625}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 273707418, "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference", "year": 2024, "venue": "", "authors": [{"name": "Jiarui Fang", "authorId": "2303233521"}, {"name": "Jinzhe Pan", "authorId": "2329908310"}, {"name": "Jiannan Wang", "authorId": "2302815939"}, {"name": "Aoyu Li", "authorId": "2303334242"}, {"name": "Xibo Sun", "authorId": "2328615626"}], "n_citations": 6}, "snippets": ["In contrast, SP partitions the input image across multiple devices, utilizing methods such as Ulysses or Ring to communicate the Attention input and output tensors. SP demonstrates su-perior communication efficiency compared to TP, but each device stores the entire model parameters, which can be memory intensive."], "score": 0.64892578125}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267027729, "title": "InternEvo: Efficient Long-sequence Large Language Model Training via Hybrid Parallelism and Redundant Sharding", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qiaoling Chen", "authorId": "2189935212"}, {"name": "Diandian Gu", "authorId": "1393328013"}, {"name": "Guoteng Wang", "authorId": "2263696698"}, {"name": "Xun Chen", "authorId": "2293554318"}, {"name": "Yingtong Xiong", "authorId": "2279899091"}, {"name": "Ting Huang", "authorId": "2279784429"}, {"name": "Qi Hu", "authorId": "2150570711"}, {"name": "Xin Jin", "authorId": "2279869979"}, {"name": "Yonggang Wen", "authorId": "2114783855"}, {"name": "Tianwei Zhang", "authorId": "2146333441"}, {"name": "Peng Sun", "authorId": "2075416290"}], "n_citations": 8}, "snippets": ["Processing long sequences is crucial for supporting important applications such as dealing with longer histories in chat applications. To this end, sequence parallelism (SP) has emerged as a technique aimed at alleviating activation memory footprints during the training of Transformers. In SP, the input tensor of each Transformer layer is divided along the sequence dimension, allowing for parallel computation across multiple GPUs. This segmentation, in conjunction with activation recomputation, results in a substantial reduction in activation memory requirements by a factor of s sp", "DeepSpeed Ulysses utilizes sequence parallelism on Linear, Norm, and Dropout layers, as shown in Figure 2 (c). An all-to-all communi- cation is applied to the output of Linear qkv with a shape of B \u00d7 3 \u00d7 S/s sp \u00d7 H. This enables each GPU to receive the complete sequence of Q, K, and V (each with a shape of B \u00d7 S \u00d7 H/s sp ) for a subset of attention heads. Subsequently, FlashAttention is employed for highly efficient MHA computation with a constraint that the number of attention heads D should not exceed s sp ."], "score": 0.68310546875}, {"id": "(Korthikanti et al., 2022)", "paper": {"corpus_id": 248693351, "title": "Reducing Activation Recomputation in Large Transformer Models", "year": 2022, "venue": "Conference on Machine Learning and Systems", "authors": [{"name": "V. Korthikanti", "authorId": "3111334"}, {"name": "J. Casper", "authorId": "48991386"}, {"name": "Sangkug Lym", "authorId": "3448779"}, {"name": "Lawrence C. McAfee", "authorId": "20957879"}, {"name": "M. Andersch", "authorId": "47304697"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 275}, "snippets": ["Training large transformer models is one of the most important computational challenges of modern AI. In this paper, we show how to significantly accelerate training of large transformer models by reducing activation recomputation. Activation recomputation is commonly used to work around memory capacity constraints. Rather than storing activations for backpropagation, they are traditionally recomputed, which saves memory but adds redundant compute. In this work, we show most of this redundant compute is unnecessary because we can reduce memory consumption sufficiently without it. We present two novel yet very simple techniques: sequence parallelism and selective activation recomputation. In conjunction with tensor parallelism, these techniques almost eliminate the need to recompute activations. We evaluate our approach on language models up to one trillion parameters in scale and show that our method reduces activation memory by 5x, while reducing execution time overhead from activation recomputation by over 90%. For example, when training a 530B parameter GPT-3 style model on 2240 NVIDIA A100 GPUs, we achieve a Model Flops Utilization of 54.2%, which is 29% faster than the 42.1% we achieve using recomputation. Our implementation will be available in both Megatron-LM and NeMo-Megatron."], "score": 0.0}], "table": null}], "cost": 0.417546}}
