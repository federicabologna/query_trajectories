{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in recent autoregressive language model architectures designed for dense passage retrieval, such as FiD-Light or Neural Corpus Indexer, and how their retrieval paradigms differ from or improve upon dual-encoder bi-directional models like DPR or ColBERT?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in recent autoregressive language model architectures for dense passage retrieval (e.g., FiD-Light, Neural Corpus Indexer) and how their retrieval paradigms and performance compare with or improve upon dual-encoder bi-directional models like DPR or ColBERT."}}, {"clarifying_question2": "Is your primary focus on quantifiable performance metrics (e.g., Recall@1, R-Precision, zero-shot generalization) comparing autoregressive models to bi-directional encoders on standard benchmarks, or are you also interested in the qualitative trade-offs such as computational efficiency and inference speed?", "clarifying_answer2": {"clarifying_answer": "My primary focus is on quantifiable performance metrics (such as Recall@1, R-Precision, and zero-shot generalization) for comparing autoregressive models to bi-directional encoders on standard benchmarks. However, I am also interested in qualitative trade-offs like computational efficiency and inference speed as secondary considerations."}}, {"clarifying_question3": "Do you want a detailed survey of the latest innovations in dense passage retrieval\u2014like masked auto-encoder pre-training, topic-aware sampling, and retrieval plug-ins\u2014in both autoregressive and bi-directional settings, or should the comparison be limited to head-to-head performance on core retrieval tasks?", "clarifying_answer3": {"clarifying_answer": "Please provide a detailed survey of the latest innovations in dense passage retrieval in both autoregressive and bi-directional settings, including methods like masked auto-encoder pre-training, topic-aware sampling, retrieval plug-ins, as well as head-to-head performance comparisons on core retrieval tasks."}}], "better_queries_2": {"reformulated1": "Please provide a detailed survey of the most recent autoregressive language model architectures for dense passage retrieval (such as FiD-Light and Neural Corpus Indexer), including how their retrieval paradigms and innovations compare with or improve upon dual-encoder bi-directional models like DPR and ColBERT, focusing on both core quantifiable performance metrics and qualitative trade-offs.", "reformulated2": "What are the latest advances and innovations, such as masked auto-encoder pre-training, topic-aware sampling, and retrieval plug-ins, in both autoregressive and bi-directional models for dense passage retrieval, and how do these approaches compare in head-to-head performance on standard benchmarks (e.g., Recall@1, R-Precision, zero-shot generalization)?", "reformulated3": "Survey the current state-of-the-art innovations and techniques in dense passage retrieval across both autoregressive and bi-directional encoder approaches, detailing new methods and training strategies (e.g., contextual masking, listwise autoregressive reranking, STAR and ADORE algorithms), and compare their retrieval performance and efficiency on core information retrieval tasks."}}
