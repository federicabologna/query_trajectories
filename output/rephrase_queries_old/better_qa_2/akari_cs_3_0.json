{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in documented cases where fine-tuning LLMs for a narrow task leads to a measurable degradation in general language generation, creativity, or reasoning abilities?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in cases where fine-tuning LLMs for a narrow or specific task leads to measurable degradation in general language generation, creativity, or reasoning abilities, as documented in empirical studies or systematic evaluations."}}, {"clarifying_question2": "Would you like a systematic literature review focused on empirical evaluations and methodologies used to assess negative transfer or catastrophic forgetting in fine-tuned LLMs?", "clarifying_answer2": {"clarifying_answer": "Yes, I would like a systematic literature review focused on empirical evaluations and methodologies used to assess negative transfer or catastrophic forgetting in fine-tuned LLMs."}}, {"clarifying_question3": "Are you looking for research comparing different fine-tuning strategies (such as data pruning or domain-invariant feature extraction) and their effectiveness at mitigating the loss of general capabilities in LLMs?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in research that systematically compares different fine-tuning strategies\u2014including techniques like data pruning or domain-invariant feature extraction\u2014specifically regarding how well they prevent or reduce the loss of an LLM's general capabilities after fine-tuning for specialized tasks."}}], "better_query": "What empirical studies systematically document cases of measurable degradation in general language generation, creativity, or reasoning abilities in LLMs as a result of fine-tuning for narrow or specific tasks?", "better_answer": {"sections": [{"title": "Introduction to LLM Fine-tuning and Degradation", "tldr": "Fine-tuning large language models (LLMs) for specific tasks can lead to degradation of their general capabilities, a phenomenon documented in several empirical studies. While fine-tuning improves performance on targeted tasks, it often comes at the cost of reduced performance in other domains. (LLM Memory)", "text": "\nFine-tuning is a technique that adapts pre-trained language models to specific tasks by updating their parameters on task-specific data. While this process effectively enhances performance on the targeted task, empirical evidence suggests it can lead to degradation in the model's general capabilities. This phenomenon occurs because the model optimizes for the distribution of the fine-tuning data, potentially at the expense of its broader knowledge and abilities. Several studies have systematically documented this trade-off. For instance, when LLMs are fine-tuned on narrow domains like medical text or customer service responses, they often show reduced creative writing abilities, lower performance on general reasoning tasks, and less diverse language generation. This degradation is particularly pronounced when fine-tuning is aggressive (using high learning rates) or when the fine-tuning dataset significantly differs from the distribution of the pre-training data. The trade-off between specialization and general capabilities presents a significant challenge for developing LLMs that excel in specific applications while maintaining their broad language understanding and generation abilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Catastrophic Forgetting in Continual Fine-tuning", "tldr": "Catastrophic forgetting is a significant challenge in continual fine-tuning of LLMs, where models lose previously acquired knowledge and abilities when trained on new tasks. This problem becomes more severe with increasing model scale and manifests across domain knowledge, reasoning abilities, and reading comprehension. (2 sources)", "text": "\nCatastrophic forgetting represents one of the most significant challenges in the continual fine-tuning of large language models. When LLMs are successively fine-tuned on different tasks or domains, they tend to lose previously acquired knowledge and capabilities\u2014a phenomenon formally known as catastrophic forgetting. Empirical evidence indicates that this problem is widespread across various LLM architectures during continual fine-tuning processes. Paradoxically, research has shown that as models increase in scale and complexity, they actually exhibit more severe forgetting of domain knowledge, reasoning abilities, and reading comprehension skills <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>.\n\nThis degradation creates a fundamental tension in LLM adaptation. While fine-tuning helps models specialize for specific tasks by training on tailored datasets, it simultaneously erodes the model's ability to generalize to novel examples that aren't well-represented in the fine-tuning data. The implications extend beyond simple knowledge retention; fine-tuned models may successfully memorize task-specific information but demonstrate diminished capacity to reason effectively about that information <Paper corpusId=\"270068369\" paperTitle=\"(Contal et al., 2024)\" isShortName></Paper>. This distinction between memorization and reasoning highlights the multifaceted nature of the degradation that occurs during fine-tuning, suggesting that different cognitive capabilities may be affected to varying degrees by the catastrophic forgetting phenomenon.", "citations": [{"id": "(Luo et al., 2023)", "paper": {"corpus_id": 261031244, "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yun Luo", "authorId": "2181146702"}, {"name": "Zhen Yang", "authorId": "2149231975"}, {"name": "Fandong Meng", "authorId": "33427918"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Jie Zhou", "authorId": "48128428"}, {"name": "Yue Zhang", "authorId": "2167740183"}], "n_citations": 318}, "snippets": ["Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills."], "score": 0.64892578125}, {"id": "(Contal et al., 2024)", "paper": {"corpus_id": 270068369, "title": "RAGSys: Item-Cold-Start Recommender as RAG System", "year": 2024, "venue": "IR-RAG@SIGIR", "authors": [{"name": "Emile Contal", "authorId": "2303471987"}, {"name": "Garrin McGoldrick", "authorId": "2283934772"}], "n_citations": 5}, "snippets": ["Fine-tuning, a technique where LLMs are trained on large datasets tailored to the target task, offers a path towards adapting LLMs to these domain-specific needs. Yet, fine-tuning presents significant challenges. When trained on tasks-specific data, LLMs tend to forget knowledge and skills gained in the initial training, a phenomenon referred to as Catastrophic Forgetting [4]. Consequently, a fine-tuned LLM loses some of its ability to generalize to novel examples that aren't well represented in its finetuning training data. Moreover, while fine-tuning allows an LLM to memorize task-specific information, it doesn't necessarily allow the LLM to reason about that information [5]."], "score": 0.52685546875}], "table": null}, {"title": "Degradation in Reasoning Abilities", "tldr": "Fine-tuning and format constraints can significantly degrade LLMs' reasoning capabilities, with stricter constraints leading to greater performance decline. This degradation is particularly pronounced in smaller models, where fine-tuning on non-reasoning tasks can substantially reduce both reasoning performance and the faithfulness of chain-of-thought reasoning. (3 sources)", "text": "\nEmpirical evidence demonstrates that LLMs experience notable degradation in reasoning abilities when subjected to certain constraints or fine-tuning processes. Format restrictions, such as requiring structured outputs like JSON or XML, have been shown to significantly impair reasoning capabilities. Studies reveal that stricter format constraints generally lead to greater performance degradation in reasoning tasks <Paper corpusId=\"271709856\" paperTitle=\"(Tam et al., 2024)\" isShortName></Paper>. This finding is particularly relevant for real-world applications that rely on structured generation to extract key information from LLMs.\n\nBeyond format constraints, fine-tuning also impacts reasoning abilities. Research investigating the effect of fine-tuning on chain-of-thought (CoT) reasoning shows that fine-tuning generally reduces CoT reasoning performance, regardless of whether the fine-tuning occurs on reasoning or non-reasoning tasks <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This degradation in reasoning capability is more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on datasets that require minimal reasoning tends to further decrease the faithfulness of their CoT reasonings <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This suggests that fine-tuning can alter the internal mechanisms that LLMs use for reasoning.\n\nThe relationship between output constraints and reasoning abilities has been further verified by multiple studies. Recent work has empirically confirmed that imposing constraints on LLM outputs can reduce functional correctness for specific tasks, with this reduction attributed directly to a decline in the LLM's reasoning capabilities under constrained decoding <Paper corpusId=\"276317504\" paperTitle=\"(Banerjee et al., 2025)\" isShortName></Paper> <Paper corpusId=\"271709856\" paperTitle=\"(Tam et al., 2024)\" isShortName></Paper>. These findings highlight a critical trade-off that must be considered when deploying LLMs in applications that require both structured outputs and sophisticated reasoning.", "citations": [{"id": "(Tam et al., 2024)", "paper": {"corpus_id": 271709856, "title": "Let Me Speak Freely? A Study On The Impact Of Format Restrictions On Large Language Model Performance.", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Zhi Rui Tam", "authorId": "2028219138"}, {"name": "Cheng-Kuang Wu", "authorId": "2217944277"}, {"name": "Yi-Lin Tsai", "authorId": "2314965749"}, {"name": "Chieh-Yen Lin", "authorId": "2306137538"}, {"name": "Hung-yi Lee", "authorId": "2278588523"}, {"name": "Yun-Nung Chen", "authorId": "2306102701"}], "n_citations": 31}, "snippets": ["we observe a significant decline in LLMs' reasoning abilities under format restrictions. Furthermore, we find that stricter format constraints generally lead to greater performance degradation in reasoning tasks."], "score": 0.51318359375}, {"id": "(Lobo et al., 2024)", "paper": {"corpus_id": 274234789, "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Elita Lobo", "authorId": "2332098876"}, {"name": "Chirag Agarwal", "authorId": "40228633"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 10}, "snippets": ["Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes", ".prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)", "", "Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate."], "score": 0.82080078125}, {"id": "(Banerjee et al., 2025)", "paper": {"corpus_id": 276317504, "title": "CRANE: Reasoning with constrained LLM generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Debangshu Banerjee", "authorId": "2069960232"}, {"name": "Tarun Suresh", "authorId": "2218724103"}, {"name": "Shubham Ugare", "authorId": "1413931779"}, {"name": "Sasa Misailovic", "authorId": "1704478"}, {"name": "Gagandeep Singh", "authorId": "2301556017"}], "n_citations": 7}, "snippets": ["Recent works such as (Tam et al., 2024) have empirically observed that imposing constraints on LLM outputs can, in some cases, reduce functional correctness for specific tasks. (Tam et al., 2024) attributes this reduction in functional accuracy to a decline in the LLM's reasoning capabilities under constrained decoding."], "score": 0.50048828125}], "table": null}, {"title": "Impact on Translation and Language Generation Capabilities", "tldr": "Fine-tuning LLMs for translation tasks improves general translation quality but degrades several important qualitative capabilities including formality steering, technical translation through few-shot examples, and document-level translation. These degradations become more pronounced with larger fine-tuning datasets, even as generic translation quality continues to improve. (1 source)", "text": "\nFine-tuning LLMs for specific language tasks presents a nuanced trade-off between specialized performance and general capabilities, particularly in translation domains. While fine-tuning generally enhances the overall translation quality of language models, research has revealed that this improvement comes at the expense of several sophisticated language capabilities. Stap et al. found that fine-tuned models show significant degradation in their ability to perform formality steering (adjusting translation style based on context), handle technical translations through few-shot examples, and maintain document-level translation coherence. <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>\n\nInterestingly, this degradation pattern follows a clear trajectory related to the size of fine-tuning datasets. As the volume of fine-tuning data increases, the general translation quality continues to improve, but the degradation in specialized capabilities becomes more pronounced. This inverse relationship highlights a fundamental tension in model optimization\u2014achieving higher performance on standard translation metrics may inadvertently sacrifice more nuanced language capabilities. This pattern holds consistently across various model scales, from smaller 7 billion parameter models to much larger 65 billion parameter models, suggesting this is an inherent characteristic of fine-tuning rather than a model-specific phenomenon. <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Stap et al., 2024)", "paper": {"corpus_id": 270123515, "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "David Stap", "authorId": "2292435932"}, {"name": "Eva Hasler", "authorId": "2269141224"}, {"name": "Bill Byrne", "authorId": "2296993932"}, {"name": "C. Monz", "authorId": "2062908179"}, {"name": "Ke Tran", "authorId": "2303845498"}], "n_citations": 12}, "snippets": ["Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings."], "score": 0.65576171875}], "table": null}, {"title": "Effects of Parameter Editing and Weight Perturbations", "tldr": "Research shows that LLMs are extremely sensitive to weight perturbations, with even minor parameter edits leading to significant degradation in general capabilities. Editing less than 1% of parameters can cause performance to drop to nearly zero across multiple tasks. (1 source)", "text": "\nRecent empirical work has revealed that large language models exhibit surprising fragility when subjected to parameter editing or weight perturbations. These modifications, even when minimal, can have devastating effects on model performance across a range of capabilities. Gu et al. demonstrated that existing LLMs lack robustness to weight perturbations, with even small-scale parameter edits significantly compromising their general abilities. Their findings are particularly striking\u2014they showed that editing less than 1% of parameters in a LLaMA-1 (7B) model through a single pass resulted in catastrophic performance degradation, with scores plummeting to nearly zero across all evaluated tasks <Paper corpusId=\"266899568\" paperTitle=\"(Gu et al., 2024)\" isShortName></Paper>.\n\nThis extreme sensitivity to parameter modifications presents a significant challenge for model editing techniques that aim to correct factual knowledge or adjust model behavior. The research underscores a fundamental trade-off in parameter editing approaches: attempts to improve specific aspects of model performance through targeted parameter modifications often come at the expense of general capabilities. This finding has important implications for developing robust editing methods that can maintain an LLM's broad functionality while making targeted improvements to particular aspects of its performance. The observed fragility suggests that the distributed nature of knowledge and capabilities within LLMs makes isolated parameter modifications particularly disruptive to the model's overall functioning.", "citations": [{"id": "(Gu et al., 2024)", "paper": {"corpus_id": 266899568, "title": "Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jia-Chen Gu", "authorId": "3028818"}, {"name": "Haoyang Xu", "authorId": "2269760395"}, {"name": "Jun-Yu Ma", "authorId": "2152612230"}, {"name": "Pan Lu", "authorId": "2887562"}, {"name": "Zhen-Hua Ling", "authorId": "2072392338"}, {"name": "Kai-Wei Chang", "authorId": "2257127887"}, {"name": "Nanyun Peng", "authorId": "2256996328"}], "n_citations": 55}, "snippets": ["Experimental results show that existing LLMs are not robust to weight perturbations, and editing even a few parameters can significantly affect their general abilities. Strikingly, with a single pass of editing involving less than 1% parameters, LLaMA-1 (7B) exhibited a drastic performance degradation to nearly 0 on all the tasks we tried. These results demonstrate that current editing algorithms struggle to work effectively in tandem with LLMs to simultaneously improve model factuality and maintain general abilities."], "score": 0.59912109375}], "table": null}, {"title": "Factors Influencing Degradation Severity", "tldr": "The severity of capability degradation in fine-tuned LLMs is influenced by multiple factors including model scale, fine-tuning dataset size, and the nature of the fine-tuning task. Larger models paradoxically show more severe forgetting of general capabilities, while larger fine-tuning datasets tend to exacerbate degradation in specialized language abilities despite improving performance on the target task. (3 sources)", "text": "\nResearch has identified several key factors that determine the extent of capability degradation when fine-tuning LLMs. Contrary to what might be expected, model scale appears to worsen rather than mitigate the problem of catastrophic forgetting. Luo et al. found that as model size increases, LLMs actually exhibit more severe degradation in domain knowledge, reasoning abilities, and reading comprehension skills during continual fine-tuning <Paper corpusId=\"261031244\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. This counterintuitive finding suggests that larger models, despite their enhanced capabilities, may have more complex knowledge representations that are more susceptible to disruption during fine-tuning.\n\nThe size and nature of the fine-tuning dataset also significantly impact degradation severity. Stap et al. demonstrated that for translation tasks, degradation in specialized capabilities like formality steering and document-level translation becomes more pronounced with larger fine-tuning datasets, even as generic translation quality continues to improve <Paper corpusId=\"270123515\" paperTitle=\"(Stap et al., 2024)\" isShortName></Paper>. This pattern was consistent across models ranging from 7 billion to 65 billion parameters, indicating a fundamental trade-off between optimization for general task performance and retention of specialized capabilities.\n\nAdditionally, the nature of the fine-tuning task itself influences the extent of degradation. Lobo et al. found that fine-tuning on non-reasoning tasks or tasks requiring minimal reasoning leads to greater degradation in chain-of-thought reasoning capabilities compared to fine-tuning on reasoning-intensive tasks <Paper corpusId=\"274234789\" paperTitle=\"(Lobo et al., 2024)\" isShortName></Paper>. This effect was more pronounced in smaller models, suggesting that model size plays a complex role in determining resilience to certain types of degradation. The research indicates that fine-tuning disrupts the internal mechanisms that LLMs use for reasoning, with the degree of disruption varying based on the alignment between the fine-tuning task and the evaluated capability.", "citations": [{"id": "(Luo et al., 2023)", "paper": {"corpus_id": 261031244, "title": "An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yun Luo", "authorId": "2181146702"}, {"name": "Zhen Yang", "authorId": "2149231975"}, {"name": "Fandong Meng", "authorId": "33427918"}, {"name": "Yafu Li", "authorId": "2110450452"}, {"name": "Jie Zhou", "authorId": "48128428"}, {"name": "Yue Zhang", "authorId": "2167740183"}], "n_citations": 318}, "snippets": ["Our findings revealed that the CF problem is generally prevalent in the continual fine-tuning of various LLMs. Moreover, as the model scale increases, LLMs exhibit a more severe degree of forgetting in domain knowledge, reasoning abilities, and reading comprehension skills."], "score": 0.64892578125}, {"id": "(Stap et al., 2024)", "paper": {"corpus_id": 270123515, "title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "David Stap", "authorId": "2292435932"}, {"name": "Eva Hasler", "authorId": "2269141224"}, {"name": "Bill Byrne", "authorId": "2296993932"}, {"name": "C. Monz", "authorId": "2062908179"}, {"name": "Ke Tran", "authorId": "2303845498"}], "n_citations": 12}, "snippets": ["Our findings reveal a more complex interplay between fine-tuning and LLM capabilities. Consistent with prior work, we find that fine-tuning enhances the general translation quality of LLMs. However, we show that fine-tuning adversely impacts several important qualitative advantages of LLMs. We observe declines in the abilities of LLMs to 1) perform formality steering, 2) perform technical translation through few-shot examples, as well as 3) a decrease in their document-level translation capabilities. The ability to produce non-literal translations shows improvement post fine-tuning, likely because the publicly available LLMs we investigate do not perform strongly on this task to begin with. Furthermore, our results indicate that these degradations are more pronounced for larger fine-tuning datasets, even when generic translation quality continues to improve. These trends are consistent across different model scales (7b up to 65b), underscoring the generalizability of our findings."], "score": 0.65576171875}, {"id": "(Lobo et al., 2024)", "paper": {"corpus_id": 274234789, "title": "On the Impact of Fine-Tuning on Chain-of-Thought Reasoning", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Elita Lobo", "authorId": "2332098876"}, {"name": "Chirag Agarwal", "authorId": "40228633"}, {"name": "Himabindu Lakkaraju", "authorId": "1892673"}], "n_citations": 10}, "snippets": ["Our research investigates the effect of fine-tuning on the reasoning abilities of LLMs, addressing critical questions regarding the impact of task-specific fine-tuning on overall reasoning capabilities, the influence of fine-tuning on Chain-of-Thought (CoT) reasoning performance, and the implications for the faithfulness of CoT reasonings. By exploring these dimensions, our study shows the impact of fine-tuning on LLM reasoning capabilities, where the faithfulness of CoT reasoning, on average across four datasets, decreases, highlighting potential shifts in internal mechanisms of the LLMs resulting from fine-tuning processes", ".prior research has demonstrated that fine-tuning can lead to i) catastrophic forgetting, where performance on tasks outside the target domain degrades (Kalajdzievski, 2024)", "", "Our results show that fine-tuning, whether on reasoning or non-reasoning tasks, generally reduces the CoT reasoning performance of LLMs, with this effect being more pronounced in smaller models. Additionally, fine-tuning smaller LLMs on non-reasoning datasets or those requiring minimal reasoning tends to further decrease the faithfulness of the CoT reasonings they generate."], "score": 0.82080078125}], "table": null}], "cost": 0.148221}}
