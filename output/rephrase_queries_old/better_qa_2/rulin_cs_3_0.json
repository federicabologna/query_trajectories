{"clarifying_information": [{"clarifying_question1": "Are you interested in a detailed timeline of the major LLaVA model releases (e.g., LLaVA 1.5, LLaVA-NeXT, LLaVA-OneVision) and the specific improvements introduced with each iteration?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in a detailed timeline of the major LLaVA model releases and the specific improvements introduced with each iteration."}}, {"clarifying_question2": "Would you like more information about specialized LLaVA variants such as TinyLLaVA or LLaVA-Med, particularly regarding their intended use cases or technical adaptations?", "clarifying_answer2": {"clarifying_answer": "Yes, I would appreciate more information specifically on the technical adaptations and intended use cases for specialized LLaVA variants like TinyLLaVA and LLaVA-Med."}}, {"clarifying_question3": "Are you mainly seeking insight into the key technical innovations (like AnyRes or expanded instruction datasets) that have driven performance improvements across the LLaVA series?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in the key technical innovations, such as AnyRes and expanded instruction datasets, that have driven performance improvements across the LLaVA series over time."}}], "better_query": "Provide a detailed timeline of the major LLaVA model releases (e.g., LLaVA 1.5, LLaVA-NeXT, LLaVA-OneVision) and describe the specific improvements and innovations introduced with each iteration.", "better_answer": {"sections": [{"title": "Introduction to LLaVA Models", "tldr": "LLaVA (Large Language and Vision Assistant) models represent a significant advancement in multimodal AI by combining language models with visual understanding capabilities. These models use instruction tuning to enable flexible processing of both visual and textual information across various tasks. (3 sources)", "text": "\nLLaVA (Large Language and Vision Assistant) emerged as a pioneering approach to multimodal AI, connecting vision encoders with large language models (LLMs) to enable comprehensive understanding of both visual and textual information <Paper corpusId=\"258179774\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. The core innovation of LLaVA lies in its joint training methodology, which enhances the synergy between images and text, making it widely applicable across diverse multimodal tasks <Paper corpusId=\"277510513\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper>. \n\nWhat distinguishes LLaVA from previous approaches is its use of instruction tuning, which gives the model exceptional flexibility in handling various visual-language tasks. This instruction-based approach allows LLaVA to dynamically adjust its execution strategy based on natural language prompts, enabling it to perform tasks ranging from visual question answering (VQA) to image captioning and complex visual reasoning <Paper corpusId=\"277510513\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper>. By combining a visual encoder like CLIP with powerful LLMs, LLaVA was the first model to effectively understand multimodal instructions and take actions accordingly <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.\n\nSince its introduction, the LLaVA family has expanded significantly, with each iteration bringing important improvements. LLaVA-1.5 enhanced the model by encoding different types of data into vectors of the same dimension, enabling it to handle more modalities. Later versions like LLaVA-NeXT focused on processing video data, while LLaVA-OneVision proposed a unified model capable of simultaneously handling multiple modalities including single images, multiple images, videos, and audio <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2023)", "paper": {"corpus_id": 258179774, "title": "Visual Instruction Tuning", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2109737569"}, {"name": "Qingyang Wu", "authorId": "31060482"}, {"name": "Yong Jae Lee", "authorId": "144756076"}], "n_citations": 4921}, "snippets": ["Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available."], "score": 0.52099609375}, {"id": "(Han et al., 2025)", "paper": {"corpus_id": 277510513, "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xiaofeng Han", "authorId": "2353440123"}, {"name": "Shunpeng Chen", "authorId": "2282098650"}, {"name": "Zenghuang Fu", "authorId": "2354260566"}, {"name": "Zhe Feng", "authorId": "2353852983"}, {"name": "Lue Fan", "authorId": "2354113244"}, {"name": "Dong An", "authorId": "2353389657"}, {"name": "Changwei Wang", "authorId": "1490749570"}, {"name": "Li Guo", "authorId": "2263619001"}, {"name": "Weiliang Meng", "authorId": "35965884"}, {"name": "Xiaopeng Zhang", "authorId": "2125434570"}, {"name": "Rongtao Xu", "authorId": "2106213769"}, {"name": "Shibiao Xu", "authorId": "2261685321"}], "n_citations": 4}, "snippets": ["The core innovation of these models lies in joint training, enhancing the synergy between images and text, making them more widely applicable in multimodal tasks. MiniGPT-4 [205] and LLaVA (Liu et al., 2023) further refine this by aligning visuallanguage representations, improving the accuracy and flexibility of image-to-text generation. LLaVA (Liu et al., 2023) (Language and Vision Assistant) is an innovative visual-language model that employs instruction tuning, giving the model a high degree of flexibility in handling various tasks. LLaVA (Liu et al., 2023)'s ability to adjust to different tasks using natural language instructions enables it to not only respond rapidly to traditional visuallanguage tasks like VQA and image captioning, but also to adjust its execution strategy dynamically for new tasks. Through instruction tuning, LLaVA (Liu et al., 2023) can generate precise image descriptions or answer visual-related questions based on different task demands. This flexible task-switching capability makes LLaVA [211] particularly advantageous in complex environments, excelling in tasks such as cross-modal and visual reasoning."], "score": 0.6083984375}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.51123046875}], "table": null}, {"title": "Original LLaVA (2023)", "tldr": "The original LLaVA model introduced in 2023 combined CLIP-ViT-L/14 as a visual encoder with Vicuna-13B as its language model, creating an efficient multimodal AI system capable of visual reasoning and understanding. It employed a two-stage training approach with pre-training on filtered image-text pairs followed by instruction fine-tuning. (4 sources)", "text": "\nThe first LLaVA (Large Language and Vision Assistant) model, released in 2023, represented a significant step forward in multimodal AI by seamlessly integrating vision and language capabilities. The architecture elegantly combined a pre-trained large language model, specifically Vicuna-13B, with a pre-trained visual encoder, CLIP-ViT-L/14, to create a comprehensive multimodal system <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>.\n\nThe developers implemented a strategic two-stage training process for LLaVA. The initial pre-training phase utilized a carefully filtered dataset of 595,000 image-text pairs derived from the CC3M dataset. This filtering involved selecting noun phrases based on their frequency to ensure diverse concept representation. The model was pre-trained for one epoch with a learning rate of 2e-3 and a batch size of 128. Following pre-training, LLaVA underwent fine-tuning on a specialized dataset called LLaVA-Instruct-158K for three epochs, using a reduced learning rate of 2e-5 and a batch size of 32 <Paper corpusId=\"276776838\" paperTitle=\"(Kalpelbe et al., 2025)\" isShortName></Paper>.\n\nEven in its first release, LLaVA demonstrated exceptional multimodal conversational abilities, often showing behavior comparable to GPT-4V when interpreting new images and following novel instructions. The model processed images at 224x224 resolution and used a linear projection layer to connect its visual and language components <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This initial version established the foundation for the subsequent iterations that would further enhance the model's capabilities across diverse tasks and modalities.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 261049617, "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yanda Li", "authorId": "2232930725"}, {"name": "Chi Zhang", "authorId": "144876211"}, {"name": "Gang Yu", "authorId": "2116565951"}, {"name": "Zhibin Wang", "authorId": "2051262469"}, {"name": "Bin Fu", "authorId": "2107058893"}, {"name": "Guosheng Lin", "authorId": "2604251"}, {"name": "Chunhua Shen", "authorId": "12459603"}, {"name": "Ling Chen", "authorId": "2232790293"}, {"name": "Yunchao Wei", "authorId": "49020088"}], "n_citations": 31}, "snippets": ["In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model", "It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models", "Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)", "It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent."], "score": 0.64892578125}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Kalpelbe et al., 2025)", "paper": {"corpus_id": 276776838, "title": "Vision Language Models in Medicine", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Beria Chingnabe Kalpelbe", "authorId": "2348482300"}, {"name": "Angel Gabriel Adaambiik", "authorId": "2348482178"}, {"name": "Wei Peng", "authorId": "2348517139"}], "n_citations": 2}, "snippets": ["The idea behind LLaVa [16] is to develop an efficient foundation language model termed \"LLaVA\" (Language and Vision Assistant), which integrates both language understanding and visual comprehension. The primary objective is to enhance the capabilities of language models in multimodal contexts, enabling them to better interpret and generate text based on visual inputs. This involves not only improving performance in traditional language tasks but also expanding functionality to include complex visual reasoning and description tasks. \n\nThe authors employed a two-step training process. Initially, they pre-trained the LLaVA model on a filtered dataset comprising 595,000 image-text pairs, sourced from the CC3M dataset. This filtering process involved selecting noun-phrases based on their frequency to ensure diverse representation across concepts in the dataset. The model was pre-trained for one epoch using a learning rate of 2e-3 and a batch size of 128. Following pre-training, the model underwent finetuning on a specialized dataset, LLaVA-Instruct-158K, for three epochs with a reduced learning rate of 2e-5 and a batch size of 32. Various optimization techniques, such as using the Adam optimizer with no weight decay and enabling BF16 and TF32, were implemented to balance speed and precision during training."], "score": 0.69677734375}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.8369140625}], "table": null}, {"title": "LLaVA-1.5", "tldr": "LLaVA-1.5, released in 2023, introduced significant architectural improvements including a two-layer MLP connector replacing the linear projection layer, increased image resolution from 224\u00d7224 to 336\u00d7336, and incorporated more academic task-oriented instruction data. These enhancements resulted in substantial performance gains across multiple benchmarks while maintaining training efficiency. (5 sources)", "text": "\nBuilding upon the original LLaVA model, LLaVA-1.5 introduced several key architectural and data improvements that substantially enhanced its multimodal capabilities. One of the most significant changes was replacing the simple linear projection layer with a more sophisticated two-layer MLP (Multilayer Perceptron) to connect the vision and language components <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>. This modification allowed for better integration between the visual encoder and language model, improving the model's ability to process multimodal information.\n\nLLaVA-1.5 also increased the input image resolution from 224\u00d7224 to 336\u00d7336 pixels, allowing the model to capture more detailed visual information <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. Additionally, the model upgraded its language foundation by adopting Vicuna-1.5-13B, which provided enhanced language understanding capabilities <Paper corpusId=\"261049617\" paperTitle=\"(Li et al., 2023)\" isShortName></Paper>.\n\nAnother critical improvement in LLaVA-1.5 was the incorporation of more comprehensive and academic task-oriented visual question-answering data with response formatting prompts <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This expanded dataset enabled the model to perform better on specialized academic tasks while maintaining its general-purpose capabilities. The data-efficient approach allowed LLaVA-1.5 to achieve impressive results with just 1.2 million publicly available training examples, and the entire training process could be completed in approximately one day on a single 8-A100 node <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>.\n\nLLaVA-1.5 maintained the core architecture of using CLIP as the visual encoder (specifically CLIP-ViT-L) and Vicuna as the text decoder while enhancing the connector between them <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This model was made available in two sizes, 7B and 13B parameters, providing options for different computational constraints <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>.\n\nThe culmination of these improvements resulted in LLaVA-1.5 achieving state-of-the-art performance across 11 benchmarks at the time of its release <Paper corpusId=\"263672058\" paperTitle=\"(Liu et al._1, 2023)\" isShortName></Paper>. The model's enhanced capabilities for encoding different types of data into vectors of the same dimension also laid the groundwork for handling a wider range of modalities in future iterations <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This significant leap in performance established LLaVA-1.5 as a cornerstone in multimodal AI research, with its simple design, low tuning cost, and outstanding performance making it widely adopted in the field <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Li et al., 2023)", "paper": {"corpus_id": 261049617, "title": "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yanda Li", "authorId": "2232930725"}, {"name": "Chi Zhang", "authorId": "144876211"}, {"name": "Gang Yu", "authorId": "2116565951"}, {"name": "Zhibin Wang", "authorId": "2051262469"}, {"name": "Bin Fu", "authorId": "2107058893"}, {"name": "Guosheng Lin", "authorId": "2604251"}, {"name": "Chunhua Shen", "authorId": "12459603"}, {"name": "Ling Chen", "authorId": "2232790293"}, {"name": "Yunchao Wei", "authorId": "49020088"}], "n_citations": 31}, "snippets": ["In order to gauge the effectiveness of our data generation strategy, we have elected to utilize the open-sourced LLaVA [20,21] model as our multimodal LLM model", "It should be noted that our pipeline is model-agnostic, meaning the datasets generated via our approach can be employed for training a variety of other models", "Architecture. The architecture of the LLaVA model elegantly combines a pre-trained LLM, specifically the Vicuna-13B [5], and a pre-trained visual encoder, known as CLIP-ViT-L/14 (Radford et al., 2021)", "It's worth mentioning that in LLaVA-1.5 [20], the linear projection layer has been substituted with a twolayer MLP. Additionally, the LLM model has been replaced with Vicuna-1.5-13B, and the input image size has been increased from 224x224 to 336x336, thereby elevating the model's multimodal capabilities to a greater extent."], "score": 0.64892578125}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."], "score": 0.78173828125}, {"id": "(Wu et al., 2023)", "paper": {"corpus_id": 266374969, "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jinge Wu", "authorId": "2182815661"}, {"name": "Yunsoo Kim", "authorId": "2275715280"}, {"name": "Eva C. Keller", "authorId": "2275354702"}, {"name": "Jamie Chow", "authorId": "2275129334"}, {"name": "Adam P. Levine", "authorId": "2275239607"}, {"name": "N. Pontikos", "authorId": "48573031"}, {"name": "Zina Ibrahim", "authorId": "2275359351"}, {"name": "Paul Taylor", "authorId": "2257359718"}, {"name": "Michelle C. Williams", "authorId": "2276324181"}, {"name": "Honghan Wu", "authorId": "2241781737"}], "n_citations": 4}, "snippets": ["LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models."], "score": 0.56103515625}, {"id": "(Liu et al._1, 2023)", "paper": {"corpus_id": 263672058, "title": "Improved Baselines with Visual Instruction Tuning", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Haotian Liu", "authorId": "2143856368"}, {"name": "Chunyuan Li", "authorId": "2243126534"}, {"name": "Yuheng Li", "authorId": "1527091339"}, {"name": "Yong Jae Lee", "authorId": "2256122200"}], "n_citations": 2824}, "snippets": ["LLaVA-1.5. We denote this final model with all the modifications as LLaVA-1.5 (the last two rows in Table 2), which achieves an impressive performance that significantly outperforms the original LLaVA (Liu et al., 2023)."], "score": 0.53369140625}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.51123046875}], "table": null}, {"title": "LLaVA-NeXT/LLaVA-1.6", "tldr": "LLaVA-NeXT (also known as LLaVA-1.6) introduced a dynamic high-resolution design called AnyRes that adaptively selects the best aspect ratio for input images, enabling up to 4\u00d7 higher resolution processing. Released in early 2024, it significantly improved reasoning, OCR capabilities, and world knowledge while maintaining data efficiency comparable to LLaVA-1.5. (4 sources)", "text": "\nBuilding on the success of LLaVA-1.5, LLaVA-NeXT (also referred to as LLaVA-1.6) was introduced in early 2024 with substantial improvements in visual understanding capabilities <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. While LLaVA-1.5 increased resolution from 224\u00d7224 to a fixed 336\u00d7336, LLaVA-NeXT took a more sophisticated approach by implementing a dynamic high-resolution design called AnyRes <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe most innovative aspect of LLaVA-NeXT is its ability to adaptively select the optimal aspect ratio based on the input image's resolution, allowing for resolution increases of up to 4\u00d7 compared to previous models <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This was achieved through a clever processing technique: rather than modifying the visual encoder, high-resolution images are split into multiple sub-images of the same size as the original input resolution. These sub-images are individually encoded and then concatenated before being fed into the language model <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nLLaVA-NeXT demonstrated significantly improved reasoning capabilities, OCR (optical character recognition) performance, and world knowledge compared to its predecessors while maintaining data efficiency comparable to LLaVA-1.5 <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. The model also provided enhanced visual conversation capabilities, making it more effective at interpreting and discussing visual content <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>.\n\nShortly after its release, LLaVA-NeXT was extended to handle video data with the introduction of LLaVA-NeXT-Video, which demonstrated strong performance in zero-shot video tasks <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. This video capability built on another innovation from the broader LLaVA ecosystem, Video-LLaVA, which unified the representations of images and videos through alignment before projection <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThe advancement of LLaVA-NeXT was made possible by three key innovations: the AnyRes technique for processing high-resolution images, expansion of high-quality instruction datasets, and integration of the most advanced open-source LLMs available at the time <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>. These improvements collectively enhanced the model's capabilities across diverse tasks and set the stage for even more comprehensive multimodal models in the future.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."], "score": 0.78173828125}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.8369140625}, {"id": "(Elgendy et al., 2024)", "paper": {"corpus_id": 273638057, "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hosam Elgendy", "authorId": "2305201677"}, {"name": "Ahmed Sharshar", "authorId": "2265656556"}, {"name": "Ahmed Aboeitta", "authorId": "2292596478"}, {"name": "Yasser Ashraf", "authorId": "2292603520"}, {"name": "Mohsen Guizani", "authorId": "2327863134"}], "n_citations": 3}, "snippets": ["In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks."], "score": 0.6220703125}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.51123046875}], "table": null}, {"title": "LLaVA-OneVision", "tldr": "LLaVA-OneVision extended the LLaVA family's capabilities by creating a unified model that can simultaneously handle multiple visual modalities including single images, multiple images, videos, and audio. This advancement addressed previous performance limitations across diverse visual scenarios while maintaining the core strengths of the LLaVA architecture. (3 sources)", "text": "\nLLaVA-OneVision, introduced by Li et al. in 2024, represents a significant expansion of the LLaVA model family by creating a unified architecture capable of handling multiple modalities simultaneously <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>. While earlier LLaVA models were primarily focused on single image processing, with LLaVA-NeXT extending capabilities to video data, LLaVA-OneVision broadened the scope considerably by enabling concurrent processing of single images, multiple images, videos, and even audio inputs within a single model framework <Paper corpusId=\"275405668\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe primary motivation behind LLaVA-OneVision was to address the performance limitations previous models faced when managing diverse visual scenarios simultaneously <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. This unified approach allowed the model to maintain consistent performance across different input types without the need for separate specialized models for each modality. LLaVA-OneVision built upon the architectural innovations introduced in LLaVA-NeXT, including the AnyRes technique for high-resolution image processing, while further expanding the high-quality instruction datasets to encompass multi-modal training examples <Paper corpusId=\"277452239\" paperTitle=\"(Riggi et al., 2025)\" isShortName></Paper>.\n\nThe development of LLaVA-OneVision represents part of a broader trend in the LLaVA ecosystem toward more versatile and comprehensive multimodal capabilities. Other variants like LLaVA-Interactive expanded functionality with features such as image chatting, segmentation, and generation capabilities, while MoE-LLaVA explored sparse LVLM architecture based on Mixture of Experts to address performance degradation in multimodal sparse learning <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>. Together, these advancements have significantly enhanced the practical utility of LLaVA models across increasingly diverse application scenarios.", "citations": [{"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 275405668, "title": "LLaVA-Octopus: Unlocking Instruction-Driven Adaptive Projector Fusion for Video Understanding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jia-Xin Zhao", "authorId": "2313875208"}, {"name": "Boyuan Sun", "authorId": "2342467513"}, {"name": "Xiang Chen", "authorId": "2339423925"}, {"name": "Xihan Wei", "authorId": "2339268195"}, {"name": "Qibin Hou", "authorId": "2339266488"}], "n_citations": 5}, "snippets": ["LLaVA [39] was the first to combine the powerful capabilities of LLMs with visual encoders like CLIP, enabling it to understand multimodal instructions and take actions accordingly, thus achieving comprehensive understanding and processing of visual and linguistic inputs. LLaVA1.5 [38] encodes different types of data into vectors of the same dimension, allowing for the handling of more modalities. LLaVA-Next [28,91] focuses more on processing video data, while LLaVA-OneVision [29] proposes a unified model capable of handling single images, multiple images, videos, audio, and other modalities simultaneously."], "score": 0.51123046875}, {"id": "(Riggi et al., 2025)", "paper": {"corpus_id": 277452239, "title": "Evaluating small vision-language models as AI assistants for radio astronomical source analysis tasks", "year": 2025, "venue": "", "authors": [{"name": "S. Riggi", "authorId": "2292400830"}, {"name": "T. Cecconello", "authorId": "2042077694"}, {"name": "A. Pilzer", "authorId": "2352941747"}, {"name": "S. Palazzo", "authorId": "2352939581"}, {"name": "N. Gupta", "authorId": "2299008238"}, {"name": "A. Hopkins", "authorId": "2298907506"}, {"name": "C. Trigilio", "authorId": "2258840598"}, {"name": "G. Umana", "authorId": "2349648144"}], "n_citations": 1}, "snippets": ["Since the first release, the model demonstrated exceptional multimodal conversational skills, often displaying behavior comparable to GPT-4V when tasked with interpreting novel images and following new instructions for the first time. Following releases (LLaVA 1.5, Liu et al. 2024a) greatly enhanced model capabilities by integrating a larger set of academic-focused instructional data, achieving state-of-the-art results on numerous benchmarks while utilizing a highly dataefficient strategy. Recent advancements in the LLaVA series, including models like LLaVA-NeXT (Liu et al., 2024b) and LLaVA-OneVision (Li et al., 2024), have significantly broadened the scope of input modalities they can handle, supporting both single or multiple images as well as video content. These improvements were driven by three key innovations: the AnyRes technique for processing high-resolution images, the expansion of high-quality instruction datasets, and the integration of the most advanced open-source LLMs available at the time, further enhancing model capabilities across diverse tasks."], "score": 0.8369140625}, {"id": "(Dai et al., 2024)", "paper": {"corpus_id": 273821149, "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Dawei Dai", "authorId": "2082462168"}, {"name": "Xu Long", "authorId": "2329189750"}, {"name": "Yutang Li", "authorId": "2136494548"}, {"name": "Yuanhui Zhang", "authorId": "2310835404"}, {"name": "Shuy Xia", "authorId": "2147222435"}], "n_citations": 2}, "snippets": ["Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers."], "score": 0.8046875}], "table": null}, {"title": "Video Variants of LLaVA", "tldr": "Building on image-based capabilities, the LLaVA model family expanded to handle video data through specialized variants. Notable developments include Video-LLaVA, which unified image and video representations through alignment before projection, and LLaVA-NeXT-Video, which leveraged the advanced reasoning capabilities of LLaVA-NeXT for zero-shot video understanding. (2 sources)", "text": "\nAs the LLaVA family of models demonstrated impressive capabilities in processing static images, researchers turned their attention to extending these capabilities to video understanding. Video-LLaVA marked an important advancement in this direction by successfully unifying the representations of images and videos through alignment before projection <Paper corpusId=\"274437586\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This approach enabled the model to effectively transition from static image comprehension to temporal visual understanding, bridging a significant gap in multimodal AI capabilities.\n\nFollowing the release of LLaVA-NeXT (LLaVA-1.6) with its enhanced reasoning and world knowledge capabilities, a specialized video variant called LLaVA-NeXT-Video was introduced in early 2024 <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. This model maintained the data efficiency comparable to LLaVA-1.5 while delivering enhanced visual conversation capabilities specifically tailored for video content <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. LLaVA-NeXT-Video quickly demonstrated strong performance in zero-shot video tasks, showing the model's ability to understand and reason about temporal visual information without explicit training on specific video scenarios <Paper corpusId=\"273638057\" paperTitle=\"(Elgendy et al., 2024)\" isShortName></Paper>. \n\nThe progression from image-only models to video-capable variants represents a natural evolution in the LLaVA model family's development, creating a foundation for more comprehensive multimodal understanding that would later be consolidated in unified approaches like LLaVA-OneVision.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 274437586, "title": "Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs", "year": 2024, "venue": "", "authors": [{"name": "Qizhe Zhang", "authorId": "2118051520"}, {"name": "Aosong Cheng", "authorId": "2292408664"}, {"name": "Ming Lu", "authorId": "2331417542"}, {"name": "Renrui Zhang", "authorId": "2275104296"}, {"name": "Zhiyong Zhuo", "authorId": "2333364107"}, {"name": "Jiajun Cao", "authorId": "2268711797"}, {"name": "Shaobo Guo", "authorId": "2333442704"}, {"name": "Qi She", "authorId": "2331326229"}, {"name": "Shanghang Zhang", "authorId": "2332857566"}], "n_citations": 10}, "snippets": ["LLaVA-1.5 (Liu et al., 2023). LLaVA is one of the most widely used open-source vision-language models, and its simple design, low tuning cost, and outstanding performance make it a cornerstone in the field of multi-modal models. Specifically, LLaVA employs a pre-trained CLIP as the visual encoder and Vicuna as the text decoder. A simple linear projector connects the two modules, enabling the LLM to accept visual tokens of CLIP as input. Meanwhile, visual instruction tuning allows the model to handle vision-language tasks. Compared to the original LLaVA, LLaVA-1.5 increases the input image resolution from 224 to 336 and incorporates more instruction tuning data, resulting in a significant performance improvement. LLaVA-NeXT [35]. Also known as LLaVA-1.6, LLaVA-NeXT builds upon LLaVA-1.5 by further increasing the input image resolution, achieving improvements in reasoning, OCR, and world knowledge. Unlike the fixed resolution increase in LLaVA-1.5, LLaVA-NeXT employs a dynamic high-resolution design. Specifically, the model can select the best aspect ratio based on the resolution of the input image, increasing the resolution by up to 4\u00d7. Without altering the visual encoder, high-resolution images are split into several sub-images of the same size as the original image. These sub-images are individually encoded and concatenated before being fed into the LLM. Video-LLaVA [32]. On the basis of image understanding, Video-LLaVA extends this capability to video comprehension. It unifies representations of images and videos through alignment before projection."], "score": 0.78173828125}, {"id": "(Elgendy et al., 2024)", "paper": {"corpus_id": 273638057, "title": "GeoLLaVA: Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hosam Elgendy", "authorId": "2305201677"}, {"name": "Ahmed Sharshar", "authorId": "2265656556"}, {"name": "Ahmed Aboeitta", "authorId": "2292596478"}, {"name": "Yasser Ashraf", "authorId": "2292603520"}, {"name": "Mohsen Guizani", "authorId": "2327863134"}], "n_citations": 3}, "snippets": ["In our work, we focus on fine-tuning two distinct models that have demonstrated a robust understanding of temporal data through video processing within the VLM framework for question-answering and captioning. The first model, LLaVA-NeXT [20], was introduced in early 2024, offering improved reasoning and world knowledge compared to other large models. It exhibits data efficiency comparable to SOTA models such as LLaVA-1.5 (Liu et al., 2023), while delivering higher image resolution and enhanced visual conversation capabilities. Shortly after the release of LLaVA-NeXT, a video variant was introduced, named LLaVA-NeXT-Video, which has demonstrated strong performance in zero-shot video tasks."], "score": 0.6220703125}], "table": null}, {"title": "Other LLaVA Variants", "tldr": "The LLaVA model family has expanded to include numerous specialized variants addressing different use cases and technical challenges. These include domain-specific models like LLaVA-Med for biomedical applications, architecture innovations like MoE-LLaVA using Mixture of Experts, and enhanced interaction capabilities as seen in LLaVA-Interactive. (2 sources)", "text": "\n- **LLaVA-Med**: A specialized biomedical variant initialized with LLaVA-0 and further trained on biomedical figure-caption pairs from PubMed Central, adapting the model's capabilities to healthcare and medical imaging applications. <Paper corpusId=\"266374969\" paperTitle=\"(Wu et al., 2023)\" isShortName></Paper>\n\n- **MoE-LLaVA**: Developed as a sparse LVLM (Large Vision-Language Model) architecture based on Mixture of Experts (MoE), this variant specifically addresses performance degradation issues in multimodal sparse learning, offering improved efficiency. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\n- **LLaVA-Interactive**: This variant significantly expanded the original LLaVA functionalities by incorporating interactive features such as image chatting, segmentation, and generation and editing capabilities, serving as a comprehensive demonstration platform for multimodal AI interaction. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>\n\n- **MG-LLaVA** (Multi-Granularity LLaVA): Enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, enabling the model to handle features at various resolutions and object centers for more comprehensive visual understanding. <Paper corpusId=\"273821149\" paperTitle=\"(Dai et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Wu et al., 2023)", "paper": {"corpus_id": 266374969, "title": "Exploring Multimodal Large Language Models for Radiology Report Error-checking", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jinge Wu", "authorId": "2182815661"}, {"name": "Yunsoo Kim", "authorId": "2275715280"}, {"name": "Eva C. Keller", "authorId": "2275354702"}, {"name": "Jamie Chow", "authorId": "2275129334"}, {"name": "Adam P. Levine", "authorId": "2275239607"}, {"name": "N. Pontikos", "authorId": "48573031"}, {"name": "Zina Ibrahim", "authorId": "2275359351"}, {"name": "Paul Taylor", "authorId": "2257359718"}, {"name": "Michelle C. Williams", "authorId": "2276324181"}, {"name": "Honghan Wu", "authorId": "2241781737"}], "n_citations": 4}, "snippets": ["LLaVA-0: LLaVA-Med is initialized with LLaVA-0 and then continuously trained with a comprehensive dataset of biomedical figure-caption pairs sourced from PubMed Central.\n\nLLaVA-1.5: LLaVA-1.5 is a general domain VLM that uses the LLaMA2 model, which has a significant improvement in language understanding when compared with LLaMA, as the backbone LLM (18,20). There are two significant improvements besides the change of the backbone LLM. Firstly, the addition of an MLP vision-language connector enhanced the system's capabilities. Secondly, the integration of academic task-oriented data further enhanced its performance and effectiveness. LLaVA-1.5 is available in 2 model sizes, 7B and 13B models, and we used both models."], "score": 0.56103515625}, {"id": "(Dai et al., 2024)", "paper": {"corpus_id": 273821149, "title": "HumanVLM: Foundation for Human-Scene Vision-Language Model", "year": 2024, "venue": "Information Fusion", "authors": [{"name": "Dawei Dai", "authorId": "2082462168"}, {"name": "Xu Long", "authorId": "2329189750"}, {"name": "Yutang Li", "authorId": "2136494548"}, {"name": "Yuanhui Zhang", "authorId": "2310835404"}, {"name": "Shuy Xia", "authorId": "2147222435"}], "n_citations": 2}, "snippets": ["Liu at el. introduced an end-to-end trained large vision-language assistant (LLaVA [51]) on instruction-following data for general purpose visual and language understanding, which gained widespread attention upon release. Subsequent research has further enhanced LLaVA's performance. For instance, LLaVA-OneVision [52] addressed performance limitations in managing single images, multiple images, and videos simultaneously across diverse visual scenarios. LLaVA-Interactive [53] serves as a comprehensive demonstration platform, incorporating features such as image chatting, segmentation, and generation and editing capabilities, significantly expanding LLaVA's original functionalities. MoE-LLaVA [54], a sparse LVLM architecture based on Mixture of Experts (MoE), was developed to tackle performance degradation in multimodal sparse learning. MG-LLaVA [55] enhanced the model's visual processing capabilities by introducing multi-granularity visual streams, allowing it to handle features at various resolutions and object centers."], "score": 0.8046875}], "table": null}], "cost": 0.284565}}
