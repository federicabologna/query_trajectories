{"clarifying_information": [{"clarifying_question1": "Are you interested in specific machine unlearning methods for language models that modify model parameters directly (such as gradient ascent or selective unlearning), or are you also looking for techniques that function without parameter updates, like in-context unlearning?", "clarifying_answer1": {"clarifying_answer": "I am interested in both types of methods: those that directly modify model parameters (e.g., gradient ascent, selective unlearning) and those that do not require parameter updates, such as in-context unlearning. Please discuss and compare both approaches as they apply to language models."}}, {"clarifying_question2": "Do you want to focus on unlearning approaches that target instance-level data (e.g., individual facts or training samples), or are you more interested in more advanced methods such as entity-level or concept-level unlearning that remove entire categories of knowledge?", "clarifying_answer2": {"clarifying_answer": "I am more interested in advanced methods such as entity-level or concept-level unlearning that enable removal of entire categories of knowledge, rather than just instance-level unlearning of individual facts or training samples."}}, {"clarifying_question3": "Are you seeking information about the effectiveness and evaluation of knowledge removal (e.g., metrics for measuring forgetting and retention rates) in these methods, or are you primarily interested in the practical applications and use cases (such as privacy compliance, harmful content mitigation, or copyright removal) of machine unlearning in language models?", "clarifying_answer3": {"clarifying_answer": "I am primarily interested in the effectiveness and evaluation of knowledge removal in machine unlearning methods for language models, including relevant metrics to assess forgetting and retention rates."}}], "better_query": "What are the leading parameter-based and parameter-free machine unlearning methods for entity-level or concept-level knowledge removal in language models, and how is their effectiveness evaluated in terms of forgetting and retention rates?", "better_answer": {"sections": [{"title": "Introduction to Machine Unlearning in Language Models", "tldr": "Machine unlearning has emerged as a key solution for implementing the \"right to be forgotten\" in large language models (LLMs). This approach aims to selectively remove specific knowledge from trained models while preserving overall performance, with evaluation typically focusing on both forget quality and model utility. (2 sources)", "text": "\nMachine unlearning in large language models (LLMs) refers to the process of selectively removing or \"forgetting\" specific information from trained models without requiring complete retraining. This concept has gained significant attention as a practical implementation of the \"right to be forgotten\" principle in AI systems <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>. The framework for machine unlearning in pre-trained LLMs encompasses various methods that can efficiently eliminate undesired knowledge while maintaining the model's overall capabilities.\n\nWhen implementing unlearning for LLMs, the goal is typically to remove specific factual knowledge represented as question-answer pairs (x_i, y_i), where x_i denotes the question and y_i represents the corresponding answer. Given a dataset D containing such pairs and a model P_\u03b8 trained on this dataset, the objective of unlearning is to ensure the model completely forgets the knowledge contained in a designated \"forget set\" while retaining other capabilities <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of unlearning methods is generally evaluated along two critical dimensions: model utility, which measures the general capabilities that remain intact after unlearning, and forget quality, which quantifies how successfully the targeted knowledge has been removed <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. A significant advantage of machine unlearning approaches is their computational efficiency\u2014research has demonstrated that these methods can be over 10^5 times more computationally efficient than complete retraining of models <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper>.\n\nOne prominent technique in the unlearning arsenal is gradient ascent, which essentially reverses the optimization process on the designated forget set. This approach, often combined with traditional gradient descent on remaining data, has shown promising results in improving hyperparameter robustness during the unlearning process <Paper corpusId=\"267897394\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Yao et al., 2024)", "paper": {"corpus_id": 267897394, "title": "Machine Unlearning of Pre-trained Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jin Yao", "authorId": "2287524199"}, {"name": "Eli Chien", "authorId": "2286336292"}, {"name": "Minxin Du", "authorId": "2359683064"}, {"name": "Xinyao Niu", "authorId": "2284224531"}, {"name": "Tianhao Wang", "authorId": "2239503716"}, {"name": "Zezhou Cheng", "authorId": "2332680558"}, {"name": "Xiang Yue", "authorId": "2284224134"}], "n_citations": 51}, "snippets": ["This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276812969, "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenyu Wang", "authorId": "2348951919"}, {"name": "Mengqi Zhang", "authorId": "48985110"}, {"name": "Xiaotian Ye", "authorId": "2286432237"}, {"name": "Zhaochun Ren", "authorId": "2260895127"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Pengjie Ren", "authorId": "1749477"}], "n_citations": 3}, "snippets": ["Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set", "Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set."], "score": 0.72705078125}], "table": null}, {"title": "Parameter-Based Unlearning Methods", "tldr": "Parameter-based unlearning methods modify model weights directly to remove unwanted knowledge, with gradient-based approaches like Gradient Ascent being the most common. These methods typically balance three components: a forget loss to remove target knowledge, a retain loss to preserve general capabilities, and sometimes custom loss terms for specific requirements. (13 sources)", "text": "\nParameter-based unlearning methods in large language models (LLMs) involve direct modification of model parameters to selectively remove specific knowledge. These approaches can be broadly categorized into several types based on their underlying mechanisms.\n\nGradient-based methods represent the most common parameter-based approach to unlearning. The fundamental technique in this category is Gradient Ascent (GA), which essentially reverses the optimization process on targeted data by maximizing the loss on information to be forgotten <Paper corpusId=\"252693065\" paperTitle=\"(Jang et al., 2022)\" isShortName></Paper>. This approach effectively reduces the model's ability to generate correct responses for the forget set <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. However, while GA can successfully reduce the probability of generating ground truth answers, it often damages the model's overall capabilities, resulting in lower forget quality and diminished model utility <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nBuilding upon simple GA, several enhanced gradient-based methods have been developed. Gradient Difference (GD) incorporates learning through gradient descent on a retain set to remedy excessive damage to the model <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. Another variant is GA with KL-divergence regularization (UKL), which applies constraints to maintain model performance on non-target data <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>. Wang et al. proposed Reverse KL-Divergence-based Knowledge Distillation (RKLD) for unlearning personal information, demonstrating the importance of balancing forget quality with model utility <Paper corpusId=\"270440333\" paperTitle=\"(Zagardo, 2024)\" isShortName></Paper>.\n\nThe general framework for parameter-based unlearning methods typically involves a modified loss function with three key components <Paper corpusId=\"273350971\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>:\n\n1. A forget loss (L_FG) that encourages the model to forget undesired data by increasing the loss on data to be forgotten\n2. A retain loss (L_RT) that ensures the model maintains its performance on unaffected data\n3. A custom loss (L_Custom) that may include regularization terms or specific constraints to control parameter updates\n\nMore specialized approaches include Selective Knowledge negation Unlearning (SKU), which employs a two-stage process: first acquiring harmful knowledge and then negating it <Paper corpusId=\"274823032\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This method effectively balances removing harmful information while preserving utility on normal prompts.\n\nAnother notable parameter-based approach is ROME (Rank-One Model Editing), which identifies and individually manipulates specific layers and neurons responsible for factual predictions <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Though originally designed for factual associations, ROME shows potential for broader unlearning applications.\n\nChen and Yang introduced a structural approach to unlearning by creating dedicated unlearning layers trained using a selective student-teacher objective <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>. This method enables the forgetting of specific information through specialized layers that can eventually be fused into a single layer, offering a more computationally efficient alternative to full model fine-tuning <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>.\n\nParameter optimization methods generally demonstrate superior effectiveness compared to in-context unlearning approaches, which rely on modifying input prompts rather than model parameters <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. However, most parameter-based methods still face challenges in balancing effective knowledge removal with preservation of general model capabilities.", "citations": [{"id": "(Jang et al., 2022)", "paper": {"corpus_id": 252693065, "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Joel Jang", "authorId": "2000091730"}, {"name": "Dongkeun Yoon", "authorId": "29830817"}, {"name": "Sohee Yang", "authorId": "16110760"}, {"name": "Sungmin Cha", "authorId": "34352481"}, {"name": "Moontae Lee", "authorId": "3056520"}, {"name": "Lajanugen Logeswaran", "authorId": "2876316"}, {"name": "Minjoon Seo", "authorId": "4418074"}], "n_citations": 239}, "snippets": ["Pretrained Language Models (LMs) memorize a vast amount of knowledge during initial pretraining, including information that may violate the privacy of personal lives and identities. Previous work addressing privacy issues for LMs has mostly focused on data preprocessing and differential privacy methods, both requiring re-training the underlying LM. We propose knowledge unlearning as an alternative method to reduce privacy risks for LMs post hoc. We show that simply performing gradient ascent on target token sequences is effective at forgetting them with little to no degradation of general language modeling performances for larger-sized LMs. We also find that sequential unlearning is better than trying to unlearn all the data at once and that unlearning is highly dependent on which kind of data (domain) is forgotten. By showing comparisons with previous methods known to mitigate privacy risks for LMs, we show that our approach can give a stronger empirical privacy guarantee in scenarios where the data vulnerable to extraction attacks are known a priori while being much more efficient and robust."], "score": 0.0}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270703237, "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Yangfan Ye", "authorId": "2216505879"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 6}, "snippets": ["In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task", "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model", "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."], "score": 0.8076171875}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 247627962, "title": "Continual Learning and Private Unlearning", "year": 2022, "venue": "CoLLAs", "authors": [{"name": "B. Liu", "authorId": "145306564"}, {"name": "Qian Liu", "authorId": "2155193246"}, {"name": "P. Stone", "authorId": "144848112"}], "n_citations": 62}, "snippets": ["As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."], "score": 0.0}, {"id": "(Qiu et al., 2024)", "paper": {"corpus_id": 270703035, "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective", "year": 2024, "venue": "", "authors": [{"name": "Xinchi Qiu", "authorId": "1702997626"}, {"name": "William F. Shen", "authorId": "2302373311"}, {"name": "Yihong Chen", "authorId": "2308073429"}, {"name": "Nicola Cancedda", "authorId": "2313189467"}, {"name": "Pontus Stenetorp", "authorId": "1918552"}, {"name": "N. Lane", "authorId": "2298756346"}], "n_citations": 3}, "snippets": ["We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)", "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."], "score": 0.76513671875}, {"id": "(Zagardo, 2024)", "paper": {"corpus_id": 270440333, "title": "A More Practical Approach to Machine Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "David Zagardo", "authorId": "2309006422"}], "n_citations": 0}, "snippets": ["Gradient-based unlearning methods involve reversing the influence of data points by applying gradients computed during training.Bourtoule et al. (2021) formalize the concept of machine unlearning and propose several practical algorithms for removing the influence of data points from trained models [1].(Neel et al., 2020) present Descent-to-Delete, a gradient-based method for machine unlearning that effectively undoes the impact of specific data points on the model's parameters (Neel et al., 2020).Wang et al. (2024) propose a novel Reverse KL-Divergence-based Knowledge Distillation (RKLD) method for unlearning personal information in large language models, demonstrating the importance of balancing forget quality with model utility [14]."], "score": 0.76123046875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 273350971, "title": "LLM Unlearning via Loss Adjustment with Only Forget Data", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yaxuan Wang", "authorId": "2306067819"}, {"name": "Jiaheng Wei", "authorId": "2306500340"}, {"name": "Chris Liu", "authorId": "2271515779"}, {"name": "Jinlong Pang", "authorId": "2284760719"}, {"name": "Quan Liu", "authorId": "2326243943"}, {"name": "Ankit Shah", "authorId": "2316588330"}, {"name": "Yujia Bao", "authorId": "2306754738"}, {"name": "Yang Liu", "authorId": "2306028548"}, {"name": "Wei Wei", "authorId": "2306480290"}], "n_citations": 20}, "snippets": ["The mainstream class of existing LLM unlearning methods involves fine-tuning the original LLM against an unlearning objective function. Although the exact designs vary, the general type of loss adjustment in LLM unlearning can be characterized as follows: \n\nThe modified loss function comprises three main components: \n\n\u2022 L FG (Forget Loss): Encourages the model to \"forget\" the undesired data or patterns. This typically involves increasing the loss on the data to be forgotten, effectively making the model perform worse on those specific examples. The goal is to reduce the model's reliance on these data points, thereby minimizing their influence on future predictions. \n\n\u2022 L RT (Retain Loss): Ensures that the model maintains its overall performance and general knowledge on unaffected data. It typically involves using the original loss function from training or a modified version that focuses on the data the model is meant to retain. This term prevents the unlearning process from degrading the model's overall capabilities beyond the scope of the specific unlearning objective. \n\n\u2022 L Custom (Custom Loss): Allows for additional flexibility and customization in the unlearning process. It may include regularization terms to control the magnitude of parameter updates or specific constraints to enforce certain unlearning behaviors. This component enables researchers to tailor the unlearning process to specific requirements or incorporate domain-specific knowledge."], "score": 0.7685546875}, {"id": "(Zuo et al., 2024)", "paper": {"corpus_id": 274823032, "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuhan Zuo", "authorId": "2005445750"}, {"name": "Minghao Wang", "authorId": "2005212347"}, {"name": "Tianqing Zhu", "authorId": "2185053609"}, {"name": "Shui Yu", "authorId": "2304458654"}, {"name": "Wanlei Zhou", "authorId": "2134555583"}], "n_citations": 2}, "snippets": ["Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts."], "score": 0.72802734375}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267681958, "title": "Towards Safer Large Language Models through Machine Unlearning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zheyuan Liu", "authorId": "2122087252"}, {"name": "Guangyao Dou", "authorId": "2174956825"}, {"name": "Zhaoxuan Tan", "authorId": "2093186816"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Meng Jiang", "authorId": "2275403324"}], "n_citations": 87}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."], "score": 0.0}, {"id": "(Du et al., 2024)", "paper": {"corpus_id": 274982612, "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hao Du", "authorId": "2315112779"}, {"name": "Shang Liu", "authorId": "2275191329"}, {"name": "Lele Zheng", "authorId": "1865220753"}, {"name": "Yang Cao", "authorId": "2336916642"}, {"name": "Atsuyoshi Nakamura", "authorId": "2336874775"}, {"name": "Lei Chen", "authorId": "2336870835"}], "n_citations": 5}, "snippets": ["Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."], "score": 0.71142578125}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264828972, "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiaao Chen", "authorId": "47739850"}, {"name": "Diyi Yang", "authorId": "2263629011"}], "n_citations": 162}, "snippets": ["Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines."], "score": 0.0}, {"id": "(Cha et al., 2024)", "paper": {"corpus_id": 271860124, "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Sungmin Cha", "authorId": "34352481"}, {"name": "Sungjun Cho", "authorId": "2149157242"}, {"name": "Dasol Hwang", "authorId": "1474356736"}, {"name": "Moontae Lee", "authorId": "2313692227"}], "n_citations": 2}, "snippets": ["Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs."], "score": 0.74462890625}, {"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271212701, "title": "On Large Language Model Continual Unlearning", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chongyang Gao", "authorId": "2311833838"}, {"name": "Lixu Wang", "authorId": "2108631414"}, {"name": "Chenkai Weng", "authorId": "2148353350"}, {"name": "Xiao Wang", "authorId": "2276121035"}, {"name": "Qi Zhu", "authorId": "2275773112"}], "n_citations": 5}, "snippets": ["Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."], "score": 0.728515625}], "table": null}, {"title": "Parameter-Free Unlearning Methods", "tldr": "Parameter-free unlearning methods modify model outputs without changing weights, with in-context unlearning being the primary approach that uses prompt engineering to guide models away from unwanted knowledge. While these methods are less effective than parameter-based approaches, they offer advantages in simplicity and can be applied to black-box models where direct parameter access is unavailable. (5 sources)", "text": "\nParameter-free unlearning methods represent an alternative approach to removing unwanted knowledge from language models without directly modifying model parameters. These techniques are particularly valuable when access to model weights is limited or when computational resources for retraining are constrained.\n\nThe most prominent parameter-free approach is in-context unlearning, which treats the model as a black box and modifies its output results using external knowledge <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper>. This method works by supplying flipped labels or contradictory information during inference, effectively guiding the model to avoid generating specific knowledge while maintaining overall performance <Paper corpusId=\"274436499\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. By altering the prompt structure rather than the model itself, in-context unlearning provides a lightweight solution to selective knowledge removal.\n\nAnother parameter-free approach is Quantized Reward Konditioning (Quark), which employs a reinforcement learning framework to control and reduce undesirable behaviors in text generation <Paper corpusId=\"274436499\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper> <Paper corpusId=\"249152301\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>. Quark works by collecting samples, sorting them into quantiles based on reward signals that quantify unwanted properties, and then conditioning the model's outputs on high-reward tokens during generation, all without changing the underlying model parameters.\n\nWhile parameter-free methods offer advantages in simplicity and applicability to black-box models, they generally demonstrate lower effectiveness compared to parameter optimization approaches <Paper corpusId=\"271212701\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This effectiveness gap exists because parameter-free methods cannot directly modify the internal representations that store factual knowledge within the model.\n\nSome hybrid approaches combine aspects of both parameter-based and parameter-free methods. For instance, localization-informed unlearning first identifies model units (such as specific layers or neurons) closely related to the target knowledge, and then applies focused modifications to those components <Paper corpusId=\"271064299\" paperTitle=\"(Shi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. This targeted approach allows for more precise knowledge removal while minimizing disruption to overall model capabilities.\n\nDespite their limitations in unlearning effectiveness, parameter-free methods remain valuable in situations where direct parameter access is unavailable or when quick, deployment-ready solutions are needed without the computational overhead of model fine-tuning.", "citations": [{"id": "(Shi et al., 2024)", "paper": {"corpus_id": 271064299, "title": "MUSE: Machine Unlearning Six-Way Evaluation for Language Models", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Weijia Shi", "authorId": "2286638403"}, {"name": "Jaechan Lee", "authorId": "2261353791"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Sadhika Malladi", "authorId": "49288855"}, {"name": "Jieyu Zhao", "authorId": "2266698166"}, {"name": "Ari Holtzman", "authorId": "2309248199"}, {"name": "Daogao Liu", "authorId": "2261780806"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Noah A. Smith", "authorId": "2309424274"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}], "n_citations": 84}, "snippets": ["Other notable non-training-based unlearning methods include localization-informed unlearning (Meng et al., 2022)Wu et al., 2023;Wei et al., 2024a), which involves identifying model units (e.g., layers, neurons) closely related to the unlearning data or tasks and then locally editing and modifying the units.\n\nIn-context unlearning (Pawelczyk et al., 2023) offers another approach, treating the model as a black box and modifying its output results using external knowledge."], "score": 0.7763671875}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 274436499, "title": "Unified Parameter-Efficient Unlearning for LLMs", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chenlu Ding", "authorId": "2313929104"}, {"name": "Jiancan Wu", "authorId": "1491035012"}, {"name": "Yancheng Yuan", "authorId": "2263441815"}, {"name": "Jinda Lu", "authorId": "2315426691"}, {"name": "Kai Zhang", "authorId": "2333421125"}, {"name": "Alex Su", "authorId": "2333357625"}, {"name": "Xiang Wang", "authorId": "2259678005"}, {"name": "Xiangnan He", "authorId": "2240825631"}], "n_citations": 8}, "snippets": ["In-context unlearning, proposed by Pawelczyk et al. (2023), allows the selective removal of data points by supplying flipped labels during inference, effectively maintaining performance while unlearning specific information. Additionally, Quark by (Lu et al., 2022) employs a reinforcement learning framework to control and reduce undesirable behaviors, enhancing text generation without extensive retraining. \n\nChen & Yang (2023) introduce a lightweight unlearning method that integrates unlearning layers into transformer architectures, facilitating efficient data removal. Knowledge Unlearning by (Jang et al., 2022) demonstrates that targeted gradient ascent can effectively forget sensitive information, surpassing traditional methods in performance retention. The technique proposed by Eldan & Russinovich (2023) facilitates the removal of specific facts related to the Harry Potter series while preserving the model's overall performance. \n\nOther approaches, such as the Partitioned Gradient Update (PGU) method by (Yu et al., 2023), aim to reduce social biases effectively. Collectively, these studies underline the significance of unlearning in LLMs, paving the way for safer, more responsible AI applications."], "score": 0.81640625}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 249152301, "title": "Quark: Controllable Text Generation with Reinforced Unlearning", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ximing Lu", "authorId": "50085131"}, {"name": "S. Welleck", "authorId": "2129663"}, {"name": "Liwei Jiang", "authorId": "2112504145"}, {"name": "Jack Hessel", "authorId": "2689239"}, {"name": "Lianhui Qin", "authorId": "3444092"}, {"name": "Peter West", "authorId": "119659229"}, {"name": "Prithviraj Ammanabrolu", "authorId": "19179135"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 219}, "snippets": ["Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives."], "score": 0.0}, {"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271212701, "title": "On Large Language Model Continual Unlearning", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chongyang Gao", "authorId": "2311833838"}, {"name": "Lixu Wang", "authorId": "2108631414"}, {"name": "Chenkai Weng", "authorId": "2148353350"}, {"name": "Xiao Wang", "authorId": "2276121035"}, {"name": "Qi Zhu", "authorId": "2275773112"}], "n_citations": 5}, "snippets": ["Current methods for LLM unlearning can be primarily categorized into parameter optimization (Chen & Yang, 2023;Eldan & Russinovich, 2023;Jia et al., 2024;Zhang et al., 2024;(Meng et al., 2022)Li et al., 2024), and in-context unlearning (Thaker et al., 2024;Pawelczyk et al., 2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective typically being to maximize the task loss on the unlearning data or to minimize the random label loss. Some methods identify the related parameters and then make appropriate modifications. Incontext learning-based methods modify the LLM input prompts to make the LLM refuse to output content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization is typically much more effective than in-context learning."], "score": 0.728515625}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}], "table": null}, {"title": "Entity-Level and Concept-Level Unlearning Approaches", "tldr": "Entity-level and concept-level unlearning approaches target the removal of specific knowledge entities or broader conceptual information from language models. These specialized techniques employ various strategies including subspace identification, representation manipulation, and discriminative methods to achieve targeted forgetting while minimizing disruption to related knowledge. (10 sources)", "text": "\nEntity-level and concept-level unlearning approaches represent more granular and targeted techniques for removing specific knowledge from language models. Unlike general unlearning methods, these approaches focus on eliminating particular entities (such as personal information) or broader concepts (like fictional universes) while preserving related knowledge.\n\nFor entity-level unlearning, researchers have systematically evaluated various algorithms by using knowledge-probing methods to identify entity-related information as forget sets. Studies show that existing unlearning algorithms often struggle to generalize effectively to entity-level unlearning tasks <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>. When comparing different approaches, gradient ascent methods can successfully minimize the probability of generating ground truth answers about target entities but often damage the model's overall capabilities. Variants that incorporate gradient descent on a retain set (like Gradient Difference) prove more effective than KL-divergence restrictions in mitigating this collateral damage <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nFor concept-level unlearning, several innovative approaches have emerged. The Erasure of Language Memory (ELM) method leverages the model's introspective capabilities, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>. This approach differs from other methods like Representation Misdirection for Unlearning (RMU), WhoIsHarryPotter (WHP), and Representation Noising (RepNoise), which employ various techniques to disrupt internal representations associated with target concepts <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>.\n\nThe UNLEARN algorithm represents another advancement in targeted knowledge removal, using subspace techniques to identify the vector spaces spanned by particular knowledge and applying discrimination methods to separate these subspaces from those containing similar information. This approach achieves impressive results\u201496% forgetting on target tasks while maintaining performance on other tasks within 2.5% of the original model. When dealing with similar tasks, it still achieves nearly 80% forgetting while preserving performance on related tasks within 10% of the original model <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>.\n\nSome approaches focus on manipulating internal model representations. The ROME (Rank-One Model Editing) method identifies and individually manipulates specific layers and neurons responsible for factual predictions <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"255825985\" paperTitle=\"(Meng et al., 2022)\" isShortName></Paper>. Originally designed for factual associations, ROME shows potential for broader unlearning applications. Similarly, Chen and Yang's structural approach creates dedicated unlearning layers trained using a selective student-teacher objective, which can eventually be fused into a single layer for efficient forgetting of specific information <Paper corpusId=\"274982612\" paperTitle=\"(Du et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264828972\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper>.\n\nFor balancing effective forgetting with knowledge preservation, the Erasing Space Concept (ESC) method restricts important subspaces for forgetting knowledge by eliminating relevant activations in the feature space. Its enhanced version, ESC with Training (ESC-T), employs a learnable mask to better manage the trade-off between forgetting target knowledge and preserving other information <Paper corpusId=\"277510371\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nComparative evaluations reveal that many existing unlearning methods struggle with the challenge of mode collapse, where removing targeted knowledge significantly damages overall model capabilities. This is particularly evident in direct tuning methods like Gradient Ascent (GA) and Negative Preference Optimization (NPO) <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>. Although some variants attempt to address this by applying gradient ascent on the forget set and gradient descent on the retain set, they still underperform compared to more sophisticated approaches <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270703237, "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Yangfan Ye", "authorId": "2216505879"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 6}, "snippets": ["In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task", "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model", "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."], "score": 0.8076171875}, {"id": "(Gandikota et al., 2024)", "paper": {"corpus_id": 273098800, "title": "Erasing Conceptual Knowledge from Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rohit Gandikota", "authorId": "52017367"}, {"name": "Sheridan Feucht", "authorId": "2140009998"}, {"name": "Samuel Marks", "authorId": "2225941937"}, {"name": "David Bau", "authorId": "2284996653"}], "n_citations": 11}, "snippets": ["In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals."], "score": 0.89111328125}, {"id": "(Lizzo et al., 2024)", "paper": {"corpus_id": 271769107, "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Tyler Lizzo", "authorId": "2315304043"}, {"name": "Larry Heck", "authorId": "2315302093"}], "n_citations": 1}, "snippets": ["This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations", "Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks."], "score": 0.86083984375}, {"id": "(Du et al., 2024)", "paper": {"corpus_id": 274982612, "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Hao Du", "authorId": "2315112779"}, {"name": "Shang Liu", "authorId": "2275191329"}, {"name": "Lele Zheng", "authorId": "1865220753"}, {"name": "Yang Cao", "authorId": "2336916642"}, {"name": "Atsuyoshi Nakamura", "authorId": "2336874775"}, {"name": "Lei Chen", "authorId": "2336870835"}], "n_citations": 5}, "snippets": ["Meng et al. (Meng et al., 2022) introduced ROME (Rank-One Model Editing), a method that identifies and individually manipulates layers and neurons responsible for factual predictions. While originally designed for factual associations, ROME shows potential for broader applications to other types of data. Jang et al. (Jang et al., 2022) proposed a method for unlearning specific information in language models by maximizing the training loss on target sequences, achieving unlearning with minimal performance degradation. Eldan and Russinovich [9] highlighted challenges in scenarios like making LLaMA2-7B forget specific topics, such as Harry Potter. Their solution involved replacing specific terms with generic equivalents and training a reinforced model to reduce token likelihoods, requiring numerous gradient descent steps. Chen and Yang (Chen et al., 2023) approached unlearning from a structural perspective. They created unlearning layers and trained them using a selective student-teacher objective. Different unlearning layers were used to forget specific information and could eventually be fused into a single layer."], "score": 0.71142578125}, {"id": "(Meng et al., 2022)", "paper": {"corpus_id": 255825985, "title": "Locating and Editing Factual Associations in GPT", "year": 2022, "venue": "Neural Information Processing Systems", "authors": [{"name": "Kevin Meng", "authorId": "153615419"}, {"name": "David Bau", "authorId": "144159726"}, {"name": "A. Andonian", "authorId": "50112310"}, {"name": "Yonatan Belinkov", "authorId": "2083259"}], "n_citations": 1387}, "snippets": ["We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/"], "score": 0.0}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 264828972, "title": "Unlearn What You Want to Forget: Efficient Unlearning for LLMs", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jiaao Chen", "authorId": "47739850"}, {"name": "Diyi Yang", "authorId": "2263629011"}], "n_citations": 162}, "snippets": ["Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines."], "score": 0.0}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 277510371, "title": "ESC: Erasing Space Concept for Knowledge Deletion", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tae-Young Lee", "authorId": "2301763757"}, {"name": "Sundong Park", "authorId": "2264950149"}, {"name": "Minwoo Jeon", "authorId": "2211098085"}, {"name": "Hyoseok Hwang", "authorId": "2073600754"}, {"name": "Gyeong-Moon Park", "authorId": "3144955"}], "n_citations": 0}, "snippets": ["To address these issues, we introduce a novel concept of Knowledge Deletion (KD), an advanced task that considers both concerns, and provides an appropriate metric, named Knowledge Retention score (KR), for assessing knowledge retention in feature space. To achieve this, we propose a novel training-free erasing approach named Erasing Space Concept (ESC), which restricts the important subspace for the forgetting knowledge by eliminating the relevant activations in the feature. In addition, we suggest ESC with Training (ESC-T), which uses a learnable mask to better balance the trade-off between forgetting and preserving knowledge in KD."], "score": 0.74169921875}, {"id": "(Dong et al., 2024)", "paper": {"corpus_id": 267681754, "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yijiang River Dong", "authorId": "2284461064"}, {"name": "Hongzhou Lin", "authorId": "2325112252"}, {"name": "Mikhail Belkin", "authorId": "2284217404"}, {"name": "Ramon Huerta", "authorId": "2284217279"}, {"name": "Ivan Vuli'c", "authorId": "2267339029"}], "n_citations": 8}, "snippets": ["The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages", "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation", "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."], "score": 0.77294921875}, {"id": "(Maini et al., 2024)", "paper": {"corpus_id": 266933371, "title": "TOFU: A Task of Fictitious Unlearning for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pratyush Maini", "authorId": "153742303"}, {"name": "Zhili Feng", "authorId": "2261439316"}, {"name": "Avi Schwarzschild", "authorId": "102604362"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "J. Kolter", "authorId": "2242257227"}], "n_citations": 194}, "snippets": ["Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."], "score": 0.0}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 269009619, "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruiqi Zhang", "authorId": "2295968232"}, {"name": "Licong Lin", "authorId": "2257668289"}, {"name": "Yu Bai", "authorId": "2257420186"}, {"name": "Song Mei", "authorId": "2257346113"}], "n_citations": 193}, "snippets": ["Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."], "score": 0.0}], "table": null}, {"title": "Evaluation Metrics and Performance Assessment", "tldr": "Evaluation of machine unlearning methods typically focuses on two key dimensions: forget quality (how effectively targeted knowledge is removed) and model utility (how well general capabilities are preserved). Researchers use specialized metrics like Deviation Score to measure the balance between these dimensions, while also emphasizing the need to monitor for residual knowledge that could be adversarially exploited. (8 sources)", "text": "\nEvaluating the effectiveness of machine unlearning methods requires robust assessment frameworks that can measure both the removal of targeted knowledge and the preservation of overall model capabilities. The evaluation process typically divides data into two distinct sets: a \"Forget\" set containing knowledge to be removed, and a \"Retain\" set containing knowledge that should be preserved <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nThe primary evaluation dimensions for unlearning methods are forget quality and model utility <Paper corpusId=\"276812969\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper>. Forget quality measures how effectively the model removes targeted knowledge, often assessed through question-answering tasks where questions related to the Forget set should receive low accuracy or probability scores. Conversely, model utility evaluates whether the unlearning procedure negatively impacts unrelated topics, typically measured by performance on the Retain set <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nTo comprehensively assess the balance between forgetting and retention, researchers have developed specialized metrics. One such metric is the Deviation Score (DS), calculated as DS = 100 \u00d7 (ROUGE1_forget)\u00b2 + (1 \u2212 ROUGE1_retain)\u00b2, which measures the Euclidean distance between the model's current state and the ideal state of perfect forgetting with complete utility preservation. A lower DS indicates more effective unlearning <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>.\n\nSystematic evaluations of unlearning methods reveal significant performance variations across different approaches. Gradient Ascent methods can effectively minimize the probability of generating correct answers for targeted knowledge but often damage overall model capabilities, resulting in lower forget quality and diminished utility. Methods that incorporate gradient descent on a retain set (like Gradient Difference) typically outperform those using KL-divergence restrictions in mitigating collateral damage to the model <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>.\n\nCurrent evaluation protocols primarily rely on behavioral tests, which may not fully capture the presence of residual knowledge within model parameters. This limitation is particularly concerning as any remaining traces of supposedly forgotten information could potentially be exploited through adversarial methods to recover the erased data post-unlearning <Paper corpusId=\"270560986\" paperTitle=\"(Hong et al., 2024)\" isShortName></Paper>. This highlights the need for more comprehensive evaluation frameworks that can detect subtle forms of knowledge retention.\n\nWhen comparing unlearning methods, direct tuning approaches like Gradient Ascent (GA) and Negative Preference Optimization (NPO) often suffer from mode collapse, where the model's overall capabilities deteriorate significantly. While some variants attempt to address this by applying gradient ascent on the Forget set and gradient descent on the Retain set, they still typically underperform compared to more sophisticated approaches <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>.", "citations": [{"id": "(Dong et al., 2024)", "paper": {"corpus_id": 267681754, "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yijiang River Dong", "authorId": "2284461064"}, {"name": "Hongzhou Lin", "authorId": "2325112252"}, {"name": "Mikhail Belkin", "authorId": "2284217404"}, {"name": "Ramon Huerta", "authorId": "2284217279"}, {"name": "Ivan Vuli'c", "authorId": "2267339029"}], "n_citations": 8}, "snippets": ["The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages", "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation", "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."], "score": 0.77294921875}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 276812969, "title": "UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Wenyu Wang", "authorId": "2348951919"}, {"name": "Mengqi Zhang", "authorId": "48985110"}, {"name": "Xiaotian Ye", "authorId": "2286432237"}, {"name": "Zhaochun Ren", "authorId": "2260895127"}, {"name": "Zhumin Chen", "authorId": "1721165"}, {"name": "Pengjie Ren", "authorId": "1749477"}], "n_citations": 3}, "snippets": ["Unlearning LLM unlearning strives to eliminate undesired data without significantly compromising the overall performance of large language models. We represent question-answer pairs derived from specific factual knowledge k i as (x i , y i ), where x i denotes the question and y i represents the corresponding answer. Given a dataset D = {(x i , y i )} n i=1 containing n question-answer pairs, let P \u03b8 be a model trained on D. The goal of LLM unlearning is to ensure that P \u03b8 completely forgets the knowledge contained in the target forget set", "Evaluation of LLM unlearning effectiveness is typically assessed along two key dimensions (Maini et al., 2024): model utility, which measure the general capabilities of the unlearned model, and forget quality, which quantifies the extent to which the targeted knowledge has been successfully removed. Gradient ascent is an important method for LLM unlearning, designed to reverse the optimization process on a designated forget set."], "score": 0.72705078125}, {"id": "(Maini et al., 2024)", "paper": {"corpus_id": 266933371, "title": "TOFU: A Task of Fictitious Unlearning for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pratyush Maini", "authorId": "153742303"}, {"name": "Zhili Feng", "authorId": "2261439316"}, {"name": "Avi Schwarzschild", "authorId": "102604362"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "J. Kolter", "authorId": "2242257227"}], "n_citations": 194}, "snippets": ["Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."], "score": 0.0}, {"id": "(Qiu et al., 2024)", "paper": {"corpus_id": 270703035, "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective", "year": 2024, "venue": "", "authors": [{"name": "Xinchi Qiu", "authorId": "1702997626"}, {"name": "William F. Shen", "authorId": "2302373311"}, {"name": "Yihong Chen", "authorId": "2308073429"}, {"name": "Nicola Cancedda", "authorId": "2313189467"}, {"name": "Pontus Stenetorp", "authorId": "1918552"}, {"name": "N. Lane", "authorId": "2298756346"}], "n_citations": 3}, "snippets": ["We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)", "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."], "score": 0.76513671875}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270703237, "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Yangfan Ye", "authorId": "2216505879"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 6}, "snippets": ["In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task", "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model", "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."], "score": 0.8076171875}, {"id": "(Hong et al., 2024)", "paper": {"corpus_id": 270560986, "title": "Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yihuai Hong", "authorId": "2306946364"}, {"name": "Lei Yu", "authorId": "2306950395"}, {"name": "Shauli Ravfogel", "authorId": "2143278592"}, {"name": "Haiqin Yang", "authorId": "2307764517"}, {"name": "Mor Geva", "authorId": "22245981"}], "n_citations": 24}, "snippets": ["Current protocols to evaluate unlearning methods largely rely on behavioral tests, without monitoring the presence of unlearned knowledge within the model's parameters. This residual knowledge can be adversarially exploited to recover the erased information post-unlearning."], "score": 0.7431640625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 269009619, "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruiqi Zhang", "authorId": "2295968232"}, {"name": "Licong Lin", "authorId": "2257668289"}, {"name": "Yu Bai", "authorId": "2257420186"}, {"name": "Song Mei", "authorId": "2257346113"}], "n_citations": 193}, "snippets": ["Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."], "score": 0.0}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 247627962, "title": "Continual Learning and Private Unlearning", "year": 2022, "venue": "CoLLAs", "authors": [{"name": "B. Liu", "authorId": "145306564"}, {"name": "Qian Liu", "authorId": "2155193246"}, {"name": "P. Stone", "authorId": "144848112"}], "n_citations": 62}, "snippets": ["As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."], "score": 0.0}], "table": null}, {"title": "Comparative Effectiveness of Unlearning Methods", "tldr": "Parameter-based approaches generally outperform parameter-free methods for machine unlearning, with UNLEARN and ELM showing the best balance between forgetting target knowledge and preserving model utility. Most methods face trade-offs between effective forgetting and maintaining performance, with gradient-based approaches like Gradient Ascent often causing significant model degradation. (12 sources)", "text": "\nHere is a comparative assessment of major unlearning methods based on their effectiveness:\n\n* **UNLEARN Algorithm**: Achieves 96% forgetting on target tasks while maintaining performance on other tasks within 2.5% of the original model. When handling similar tasks (a significant challenge for most methods), it still manages nearly 80% forgetting while preserving performance on related tasks within 10% of the original model. Its discriminative capabilities significantly outperform other approaches. <Paper corpusId=\"271769107\" paperTitle=\"(Lizzo et al., 2024)\" isShortName></Paper>\n\n* **Erasure of Language Memory (ELM)**: Demonstrates superior concept-level unlearning by leveraging the model's ability to evaluate its own knowledge. It outperforms competing methods like Representation Misdirection for Unlearning (RMU), WhoIsHarryPotter (WHP), and Representation Noising (RepNoise) across multiple evaluation dimensions. <Paper corpusId=\"273098800\" paperTitle=\"(Gandikota et al., 2024)\" isShortName></Paper>\n\n* **Selective Knowledge Negation Unlearning (SKU)**: Effectively balances harmful content removal and normal prompt performance through a two-stage process of first acquiring harmful knowledge and then negating it. This approach has shown particular promise in maintaining model utility while removing targeted content. <Paper corpusId=\"274823032\" paperTitle=\"(Zuo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267681958\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>\n\n* **Gradient Difference (GD)**: Outperforms KL-divergence regularization approaches by learning through gradient descent on a retain set, which helps mitigate excessive damage to the model. It shows better balance between forgetting and retention than simple Gradient Ascent. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247627962\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>\n\n* **Negative Preference Optimization with Gradient Difference (NPO-GD)**: Stands out as one of the strongest approaches, particularly excelling in instance-level unlearning tasks under ideal conditions. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper>\n\n* **Gradient Ascent (GA)**: While effective at minimizing the probability of generating ground truth answers for target knowledge, GA significantly harms the model's overall capabilities, resulting in lower forget quality and diminished model utility. It suffers from unstable optimization due to the unbounded nature of its objective loss. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper>\n\n* **Preference Optimization methods**: These approaches (including Direct Preference Optimization and Negative Preference Optimization) can significantly reduce ROUGE scores for target set answers, but the original answers often maintain high generation probability and accuracy. This suggests they perform unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. <Paper corpusId=\"270703237\" paperTitle=\"(Ma et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270703035\" paperTitle=\"(Qiu et al., 2024)\" isShortName></Paper>\n\n* **Knowledge Distillation-based methods**: Approaches like Wang et al.'s method transfer knowledge selectively to a secondary model but incur substantial computational costs due to reliance on this secondary model. <Paper corpusId=\"271860124\" paperTitle=\"(Cha et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258615571\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>\n\n* **Direct tuning methods**: Approaches like Gradient Ascent (GA) and Negative Preference Optimization (NPO) often suffer from mode collapse, placing them near the origin on evaluation charts. Some variants attempt to correct this by applying gradient ascent on the forget set and gradient descent on the retain set, but they still underperform compared to more sophisticated approaches. <Paper corpusId=\"267681754\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266933371\" paperTitle=\"(Maini et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269009619\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Lizzo et al., 2024)", "paper": {"corpus_id": 271769107, "title": "UNLEARN Efficient Removal of Knowledge in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Tyler Lizzo", "authorId": "2315304043"}, {"name": "Larry Heck", "authorId": "2315302093"}], "n_citations": 1}, "snippets": ["This paper introduces UNLEARN, a novel algorithm that can forget or unlearn knowledge within an LLM without adversely affecting related knowledge. UNLEARN leverages subspace techniques to identify the subspaces spanned by particular knowledge (tasks) and discrimination methods to separate that subspace from subspaces of similar tasks. This allows the algorithm to prevent performance degradation when there are similar tasks, a common issue with traditional methods and of particular importance to data privacy regulations", "Compared to state-of-the-art methods like Gradient Ascent, UNLEARN offers substantial advantages in terms of generality, efficiency, and precision. UNLEARN achieves 96% forgetting on the task of interest while maintaining performance on other tasks within 2.5% of the original model. When similar tasks are considered, UNLEARN achieves nearly 80% forgetting on the task of interest while preserving performance on the similar task within 10% of the original model. The discriminative ability of UNLEARN far outpaces that of the existing state-of-the-art, ensuring targeted unlearning without compromising the performance on related tasks."], "score": 0.86083984375}, {"id": "(Gandikota et al., 2024)", "paper": {"corpus_id": 273098800, "title": "Erasing Conceptual Knowledge from Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Rohit Gandikota", "authorId": "52017367"}, {"name": "Sheridan Feucht", "authorId": "2140009998"}, {"name": "Samuel Marks", "authorId": "2225941937"}, {"name": "David Bau", "authorId": "2284996653"}], "n_citations": 11}, "snippets": ["In this work, we propose Erasure of Language Memory (ELM), an approach for concept-level unlearning built on the principle of matching the distribution defined by an introspective classifier. Our key insight is that effective unlearning should leverage the model's ability to evaluate its own knowledge, using the model itself as a classifier to identify and reduce the likelihood of generating content related to undesired concepts.\n\nPrior approaches to unlearning broadly fall into three categories: (1) retraining on filtered data (2) reversed-gradientbased methods that attempt to \"un-train\" specific knowledge, and (3) representation manipulation approaches that disrupt internal activations for targeted content.\n\nOur work is most directly comparable to three state-of-the-art techniques: Representation Misdirection for Unlearning (RMU) (Li et al., 2024), WhoIsHarryPotter (WHP) (Eldan & Russinovich, 2023), and Representation Noising (RepNoise) (Rosati et al., 2024). RMU fine-tunes models to align internal activations with random scaled vectors when processing targeted concepts. WHP (Eldan & Russinovich, 2023) employs a two-stage approach, first training a reinforced model for the concept being erased and then training an unlearned model to behave differently on the reinforced logits. RepNoise (Rosati et al., 2024) focuses on removing information about harmful representations across all layers of the LLM, by doing gradient ascent along with some representation noising (training internal representations to match Gaussian noise). While all these methods successfully reduce model performance on erased knowledge, our measurements reveal that these previous approaches fall short in one or more of the three erasing goals."], "score": 0.89111328125}, {"id": "(Zuo et al., 2024)", "paper": {"corpus_id": 274823032, "title": "Large Language Model Federated Learning with Blockchain and Unlearning for Cross-Organizational Collaboration", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Xuhan Zuo", "authorId": "2005445750"}, {"name": "Minghao Wang", "authorId": "2005212347"}, {"name": "Tianqing Zhu", "authorId": "2185053609"}, {"name": "Shui Yu", "authorId": "2304458654"}, {"name": "Wanlei Zhou", "authorId": "2134555583"}], "n_citations": 2}, "snippets": ["Liu et al. [15] introduce Selective Knowledge Negation Unlearning (SKU), a novel unlearning framework designed to eliminate harmful knowledge while preserving the utility of LLMs on normal prompts. The SKU framework involves a two-stage process: a harmful knowledge acquisition stage followed by a knowledge negation stage. The study demonstrates that SKU effectively balances the trade-off between unlearning harmful content and maintaining model performance on nonharmful prompts.\n\nChen et al. [16] propose an effective unlearning framework with an unlearning layer specifically designed for both classification and generation tasks. Their approach focuses on the efficient removal of unwanted knowledge from LLMs, emphasizing the importance of computational efficiency and scalability in the unlearning process.\n\nYao et al. [17] pioneer the concept of large language model unlearning, defining the goal of unlearning in LLMs as the ability to produce non-harmful outputs when faced with harmful prompts. They employ a Gradient Ascent (GA) based method to remove harmful content, though this often results in degraded performance on normal prompts."], "score": 0.72802734375}, {"id": "(Cha et al., 2024)", "paper": {"corpus_id": 271860124, "title": "Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Sungmin Cha", "authorId": "34352481"}, {"name": "Sungjun Cho", "authorId": "2149157242"}, {"name": "Dasol Hwang", "authorId": "1474356736"}, {"name": "Moontae Lee", "authorId": "2313692227"}], "n_citations": 2}, "snippets": ["Several novel approaches have been proposed for approximate unlearning: Jang et al. (2023) introduced a simple method that finetunes LLMs using Gradient Ascent (GA) on data requested for deletion and also proposed n-gram-based metrics to evaluate its effectiveness. Wang et al. (2023) and Liu et al. (2024) proposed knowledge distillation-based methods that selectively transfer knowledge to a secondary model for unlearning. However, both approaches face significant challenges: GA suffers from unstable optimization due to unbounded nature of its objective loss, while distillation-based methods incur substantial computational costs from relying on a secondary model. Above all, these approaches share a critical drawback: the high computational cost of full fine-tuning all parameters within the LLMs."], "score": 0.74462890625}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 267681958, "title": "Towards Safer Large Language Models through Machine Unlearning", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zheyuan Liu", "authorId": "2122087252"}, {"name": "Guangyao Dou", "authorId": "2174956825"}, {"name": "Zhaoxuan Tan", "authorId": "2093186816"}, {"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Meng Jiang", "authorId": "2275403324"}], "n_citations": 87}, "snippets": ["The rapid advancement of Large Language Models (LLMs) has demonstrated their vast potential across various domains, attributed to their extensive pretraining knowledge and exceptional generalizability. However, LLMs often encounter challenges in generating harmful content when faced with problematic prompts. To address this problem, existing work attempted to implement a gradient ascent based approach to prevent LLMs from producing harmful output. While these methods can be effective, they frequently impact the model utility in responding to normal prompts. To address this gap, we introduce Selective Knowledge negation Unlearning (SKU), a novel unlearning framework for LLMs, designed to eliminate harmful knowledge while preserving utility on normal prompts. Specifically, SKU is consisted of two stages: harmful knowledge acquisition stage and knowledge negation stage. The first stage aims to identify and acquire harmful knowledge within the model, whereas the second is dedicated to remove this knowledge. SKU selectively isolates and removes harmful knowledge in model parameters, ensuring the model's performance remains robust on normal prompts. Our experiments conducted across various LLM architectures demonstrate that SKU identifies a good balance point between removing harmful information and preserving utility."], "score": 0.0}, {"id": "(Ma et al., 2024)", "paper": {"corpus_id": 270703237, "title": "Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis", "year": 2024, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Weitao Ma", "authorId": "2265878959"}, {"name": "Xiaocheng Feng", "authorId": "2674998"}, {"name": "Weihong Zhong", "authorId": "2208739098"}, {"name": "Lei Huang", "authorId": "2265930173"}, {"name": "Yangfan Ye", "authorId": "2216505879"}, {"name": "Bing Qin", "authorId": "2257004102"}], "n_citations": 6}, "snippets": ["In this work, we systematically evaluate trending unlearning algorithms on this task. Specifically, within the framework, we introduce an accessible knowledge-probing method to identify entity-related knowledge as the forget sets, and then compare the performance of the unlearning algorithm across different forget sets. Our experiments reveal that existing unlearning algorithms can not generalize well to the entity-level unlearning task", "we select five representative algorithms for evaluation. Among them is Gradient Ascent (Grad. Ascent), which reduces the likelihood of the answers to achieve unlearning. Other methods introduce additional constraints on a retain set, which contains knowledge that should be preserved to minimize the damage to the model", "Comparing the performance of different unlearning algorithms reveals several insights critical for enhancing entity-level unlearning: 1. The Grad. Ascent method can effectively minimize the probability of the ground truth answer but harms the model's ability, resulting in the lower forget quality and diminished model utility. Comparing its two variants, the KL. Min. and Grad. Diff. methods, learning through gradient descent on the retain set proves more effective than the KL restriction in remedying the excessive damage to the model. 2. Although the Pref. Opt. method significantly reduces the ROUGE of the target set answers and achieves relatively high forget quality on both two target models, the original answers still maintain a high generation probability and accuracy. This suggests that the method performs unlearning by increasing the likelihood of refusal answers rather than truly forgetting the target entity. 3. The NPO-GD method stands out as one of the strongest approaches in unlearning, particularly excelling in instance-level unlearning tasks under ideal conditions."], "score": 0.8076171875}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 247627962, "title": "Continual Learning and Private Unlearning", "year": 2022, "venue": "CoLLAs", "authors": [{"name": "B. Liu", "authorId": "145306564"}, {"name": "Qian Liu", "authorId": "2155193246"}, {"name": "P. Stone", "authorId": "144848112"}], "n_citations": 62}, "snippets": ["As intelligent agents become autonomous over longer periods of time, they may eventually become lifelong counterparts to specific people. If so, it may be common for a user to want the agent to master a task temporarily but later on to forget the task due to privacy concerns. However enabling an agent to \\emph{forget privately} what the user specified without degrading the rest of the learned knowledge is a challenging problem. With the aim of addressing this challenge, this paper formalizes this continual learning and private unlearning (CLPU) problem. The paper further introduces a straightforward but exactly private solution, CLPU-DER++, as the first step towards solving the CLPU problem, along with a set of carefully designed benchmark problems to evaluate the effectiveness of the proposed solution. The code is available at https://github.com/Cranial-XIX/Continual-Learning-Private-Unlearning."], "score": 0.0}, {"id": "(Qiu et al., 2024)", "paper": {"corpus_id": 270703035, "title": "How Data Inter-connectivity Shapes LLMs Unlearning: A Structural Unlearning Perspective", "year": 2024, "venue": "", "authors": [{"name": "Xinchi Qiu", "authorId": "1702997626"}, {"name": "William F. Shen", "authorId": "2302373311"}, {"name": "Yihong Chen", "authorId": "2308073429"}, {"name": "Nicola Cancedda", "authorId": "2313189467"}, {"name": "Pontus Stenetorp", "authorId": "1918552"}, {"name": "N. Lane", "authorId": "2298756346"}], "n_citations": 3}, "snippets": ["We experiment with three gradient-based methods: Gradient Ascent (GA) (Jang et al., 2022;Yao et al., 2023), Gradient Difference (GD) (Liu et al., 2022) and GA with KL-divergence regularization (UKL), as well as two preference optimization(PO)-based methods: Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Negative Preference Optimization (NPO) (Zhang et al., 2024)", "We measure unlearning effectiveness through two key aspects: forget efficacy, which captures how much the model's outputs diverge from the forget set, and model utility, which reflects the preserved performance on data outside the forget set. Since these objectives are equally critical, we use Deviation Score (DS) = 100 \u00d7 ROUGE12 forget + (1 \u2212 ROUGE1 retain ) 2 to measure the Euclidean distances of forget efficacy and model utility to their respective ideal state. A lower DS indicates more effective unlearning, signifying a closer approach to the optimal state -where the model outputs no information from the forget set while maintaining full accuracy on the retained data."], "score": 0.76513671875}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 258615571, "title": "KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Lingzhi Wang", "authorId": "2187207650"}, {"name": "Tong Chen", "authorId": "1490931831"}, {"name": "Wei Yuan", "authorId": "2106755543"}, {"name": "Xingshan Zeng", "authorId": "46180553"}, {"name": "Kam-Fai Wong", "authorId": "1784988"}, {"name": "Hongzhi Yin", "authorId": "2416851"}], "n_citations": 78}, "snippets": ["Recent legislation of the \"right to be forgotten\" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on large-scale datasets show that KGA yields comprehensive improvements over baselines, where extensive analyses further validate the effectiveness of KGA and provide insight into unlearning for NLP tasks."], "score": 0.0}, {"id": "(Dong et al., 2024)", "paper": {"corpus_id": 267681754, "title": "UNDIAL: Self-Distillation with Adjusted Logits for Robust Unlearning in Large Language Models", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Yijiang River Dong", "authorId": "2284461064"}, {"name": "Hongzhou Lin", "authorId": "2325112252"}, {"name": "Mikhail Belkin", "authorId": "2284217404"}, {"name": "Ramon Huerta", "authorId": "2284217279"}, {"name": "Ivan Vuli'c", "authorId": "2267339029"}], "n_citations": 8}, "snippets": ["The original work separates the data into two sets: Forget and Retain. In other words, we aim to unlearn a set of BBC News passages from the base model while retaining knowledge on other BBC News passages", "For evaluation, question answering is conducted with respect to the News coming from the two sets. The questions related to the Forget set will test knowledge memorization, which we want to keep low. In contrast, the questions targeting the Retain set will test whether the unlearning procedure impacts unrelated topics, referred to as utility preservation", "Direct tuning methods like GA and NPO suffer from mode collapse, placing them near the origin. Some variants of those methods attempt to correct this by applying gradient ascent on the Forget set and gradient descent on the Retain set to balance the trade-off (Zhang et al., 2024)(Maini et al., 2024). While these adjustments help reduce mode collapse, they still underperform relative to UNDIAL."], "score": 0.77294921875}, {"id": "(Maini et al., 2024)", "paper": {"corpus_id": 266933371, "title": "TOFU: A Task of Fictitious Unlearning for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Pratyush Maini", "authorId": "153742303"}, {"name": "Zhili Feng", "authorId": "2261439316"}, {"name": "Avi Schwarzschild", "authorId": "102604362"}, {"name": "Zachary Chase Lipton", "authorId": "32219137"}, {"name": "J. Kolter", "authorId": "2242257227"}], "n_citations": 194}, "snippets": ["Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."], "score": 0.0}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 269009619, "title": "Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruiqi Zhang", "authorId": "2295968232"}, {"name": "Licong Lin", "authorId": "2257668289"}, {"name": "Yu Bai", "authorId": "2257420186"}, {"name": "Song Mei", "authorId": "2257346113"}], "n_citations": 193}, "snippets": ["Large Language Models (LLMs) often memorize sensitive, private, or copyrighted data during pre-training. LLM unlearning aims to eliminate the influence of undesirable data from the pre-trained model while preserving the model's utilities on other tasks. Several practical methods have recently been proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss of undesirable data. However, on certain unlearning tasks, these methods either fail to effectively unlearn the target data or suffer from catastrophic collapse -- a drastic degradation of the model's utilities. In this paper, we propose Negative Preference Optimization (NPO), a simple alignment-inspired method that could efficiently and effectively unlearn a target dataset. We theoretically show that the progression toward catastrophic collapse by minimizing the NPO loss is exponentially slower than GA. Through experiments on synthetic data and the benchmark TOFU dataset, we demonstrate that NPO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model's utilities. We also observe that NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the first to achieve reasonable unlearning results in forgetting 50% (or more) of the training data, whereas existing methods already struggle with forgetting 10% of training data."], "score": 0.0}], "table": null}], "cost": 0.381957}}
