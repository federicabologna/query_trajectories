{"clarifying_information": [{"clarifying_question1": "Are you primarily interested in the differences in attention mechanisms (i.e., causal vs. non-causal/bidirectional) among encoder-only, decoder-only, and encoder-decoder language models?", "clarifying_answer1": {"clarifying_answer": "Yes, my primary interest is in understanding the differences in attention mechanisms\u2014specifically, causal (unidirectional) versus non-causal (bidirectional) attention\u2014across encoder-only, decoder-only, and encoder-decoder language model architectures."}}, {"clarifying_question2": "Do you want a comparison of how these architectures impact specific NLP tasks, such as text generation versus text understanding or sequence-to-sequence tasks?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in a comparison of how encoder-only, decoder-only (causal/non-causal), and encoder-decoder architectures impact specific NLP tasks like text generation, text understanding, and sequence-to-sequence applications. Please include examples of which architectures are best suited for each kind of task and explain why."}}, {"clarifying_question3": "Are you seeking information about the implications of using (non-)causal attention in decoder-only models, such as prefix LMs or hybrid models, and how they compare to classical encoder-decoder architectures?", "clarifying_answer3": {"clarifying_answer": "Yes, I am seeking information specifically about the implications of using causal vs. non-causal attention in decoder-only models (such as prefix LMs, hybrid models), and how these approaches compare\u2014architecturally and functionally\u2014to classical encoder-decoder models that have both dedicated encoder and decoder components. I'm interested in the differences in information flow, expressivity, parameter usage, task suitability, and any performance or theoretical trade-offs between these architectures and masking patterns."}}], "better_query": "How do causal (unidirectional) and non-causal (bidirectional) attention mechanisms differ in encoder-only, decoder-only (including prefix/hybrid LMs), and encoder-decoder language models, particularly in terms of information flow and their suitability for common NLP tasks like text generation, understanding, and sequence-to-sequence applications?", "better_answer": {"sections": [{"title": "Introduction", "tldr": "Attention mechanisms enable language models to process contextual information, with causal attention looking only at past tokens and non-causal attention accessing the entire sequence. Different model architectures implement these attention mechanisms in distinct ways, affecting their capabilities and suitable applications. (LLM Memory)", "text": "\nAttention mechanisms are fundamental components of modern language models that determine how these models process and utilize contextual information. The distinction between causal (unidirectional) and non-causal (bidirectional) attention represents one of the most important architectural choices in natural language processing systems. Causal attention restricts a model to only attend to previous tokens in a sequence, creating a left-to-right information flow that mimics how humans generate text. In contrast, non-causal attention allows a model to access the entire sequence simultaneously, providing a comprehensive view of context from both directions. This fundamental difference in information access significantly impacts how language models function across different architectures\u2014encoder-only, decoder-only, and encoder-decoder frameworks\u2014each optimized for specific tasks in the NLP landscape. Understanding these differences is crucial for selecting the appropriate model architecture for tasks ranging from text generation and summarization to comprehension and translation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Causal vs. Non-Causal Attention Mechanisms", "tldr": "Causal attention restricts information flow to previous tokens only, enabling autoregressive text generation while non-causal attention allows bidirectional information exchange between all tokens, facilitating comprehensive understanding of context. (11 sources)", "text": "\nCausal (unidirectional) and non-causal (bidirectional) attention mechanisms represent fundamentally different approaches to processing sequence information in language models. In causal attention, each token can only attend to itself and preceding tokens, creating a left-to-right information flow where \"subsequent tokens can 'see' preceding tokens, but preceding tokens cannot 'see' subsequent tokens\" <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This unidirectional design is essential for autoregressive text generation, as it \"ensures that during text aggressive generation, historical token information is not leaked\" <Paper corpusId=\"272423598\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nThe causal mechanism is particularly valuable for sequence generation tasks because it \"preserves the correct temporal order\" <Paper corpusId=\"273877976\" paperTitle=\"(Karagodin et al., 2024)\" isShortName></Paper>. During inference, this allows the model to generate text one token at a time, using only previously generated tokens to inform the next prediction. This process is computationally efficient as \"each token is produced in a forward pass without needing to revisit previous steps\" <Paper corpusId=\"273877976\" paperTitle=\"(Karagodin et al., 2024)\" isShortName></Paper>.\n\nIn contrast, non-causal (bidirectional) attention allows each token to attend to all other tokens in the sequence, regardless of position. As described by Lin et al., \"both subsequent and preceding tokens can assimilate information from each other\" <Paper corpusId=\"267759571\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper>. This provides the model with a \"robust information encoding capability\" <Paper corpusId=\"267759571\" paperTitle=\"(Lin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>, allowing it to develop a comprehensive understanding of the entire sequence. BERT exemplifies this approach, as it is \"designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers\" <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n\nThe architectural choice between causal and non-causal attention creates a fundamental trade-off. Bidirectional models are generally more powerful for understanding tasks because \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than a left-to-right model\" <Paper corpusId=\"248377166\" paperTitle=\"(Chaffin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. However, bidirectional models lack the autoregressive capability necessary for efficient text generation. When a new token is added to the sequence in a bidirectional model, \"every hidden states need to be re-computed\" <Paper corpusId=\"248377166\" paperTitle=\"(Chaffin et al., 2022)\" isShortName></Paper> <Paper corpusId=\"13756489\" paperTitle=\"(Vaswani et al., 2017)\" isShortName></Paper>, making them less suitable for generative tasks.\n\nConverting between these attention types is not trivial. As Lv et al. note, \"Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask\" <Paper corpusId=\"265150001\" paperTitle=\"(Lv et al., 2023)\" isShortName></Paper>. This is because doing so would introduce positional information patterns that the model was never exposed to during training.\n\nHybrid approaches have emerged to leverage the benefits of both attention types. PrefixLM, introduced in T5, represents one such approach that \"utilizes causal masking with a designated 'prefix' section\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. In this design, the prefix is processed bidirectionally while the remaining sequence uses causal attention, \"combining the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nOther hybrid approaches include the context-span method described by Khosla et al., where \"context tokens attend to all other context tokens\" bidirectionally, while \"span tokens are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>. This configuration allows the model to operate in multiple modes: \"fully causal/unidirectional for open-ended text generation tasks, fully bidirectional representation learning tasks, or a combination of causal and bidirectional for text infilling\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>.\n\nThe choice between causal and non-causal attention mechanisms ultimately depends on the intended application. Causal attention is the foundation of \"decoder-only transformers are the most effective task-agnostic NLG systems\" <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>, while non-causal attention excels in understanding tasks and specialized generation scenarios like \"speech recognition, style transfer and grammar correction, textual data augmentation, and dialog systems\" <Paper corpusId=\"276423946\" paperTitle=\"(Busto-Castineira et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 272423598, "title": "TC-LLaVA: Rethinking the Transfer from Image to Video Understanding with Temporal Considerations", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingze Gao", "authorId": "2319805585"}, {"name": "Jingyu Liu", "authorId": "2302790279"}, {"name": "Mingda Li", "authorId": "2302785092"}, {"name": "Jiangtao Xie", "authorId": "2319964588"}, {"name": "Qingbin Liu", "authorId": "2258682951"}, {"name": "Bo Zhao", "authorId": "2304448412"}, {"name": "Xi Chen", "authorId": "2302990371"}, {"name": "Hui Xiong", "authorId": "2319814814"}], "n_citations": 2}, "snippets": ["In causal language models like the GPT [1] and LLama [36] series, causal attention masks are employed to ensure that during text aggressive generation, historical token information is not leaked; that is, subsequent tokens can \"see\" preceding tokens, but preceding tokens cannot \"see\" subsequent tokens. This design is uniformly applied in such generative models to maintain the unidirectional flow of information, which is crucial for generating coherent and contextually appropriate text."], "score": 0.6767578125}, {"id": "(Karagodin et al., 2024)", "paper": {"corpus_id": 273877976, "title": "Clustering in Causal Attention Masking", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Nikita Karagodin", "authorId": "2329559804"}, {"name": "Yury Polyanskiy", "authorId": "2300093577"}, {"name": "Philippe Rigollet", "authorId": "2275197338"}], "n_citations": 7}, "snippets": ["Causal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output."], "score": 0.60400390625}, {"id": "(Lin et al., 2024)", "paper": {"corpus_id": 267759571, "title": "Scaling Laws Behind Code Understanding Model", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiayi Lin", "authorId": "2211900186"}, {"name": "Hande Dong", "authorId": "2260535450"}, {"name": "Yutao Xie", "authorId": "2260831632"}, {"name": "Lei Zhang", "authorId": "2261093743"}], "n_citations": 2}, "snippets": ["\u2022 Full attention (Devlin et al., 2019)(Feng et al., 2020). This bidirectional attention operates between every pair of tokens in the sequence. Notably, both subsequent and preceding tokens can assimilate information from each other. The robust information encoding capability afforded by full attention designates the model as a transformer-encoder, specialized for tasks involving understanding. \u2022 Masking attention (Brown et al., 2020)[3]. In this directional attention model, interactions occur exclusively from left to right within the token sequence. Consequently, only the subsequent token can integrate information from preceding tokens, creating a unidirectional flow. Masking attention aligns with standard causal language modeling and is denoted as a transformerdecoder, tailored for tasks centered around generation."], "score": 0.5048828125}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Chaffin et al., 2022)", "paper": {"corpus_id": 248377166, "title": "Which Discriminator for Cooperative Text Generation?", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Antoine Chaffin", "authorId": "2129106958"}, {"name": "Thomas Scialom", "authorId": "90745780"}, {"name": "S. Lamprier", "authorId": "1782552"}, {"name": "Jacopo Staiano", "authorId": "1767493"}, {"name": "Benjamin Piwowarski", "authorId": "1703777"}, {"name": "Ewa Kijak", "authorId": "1801242"}, {"name": "V. Claveau", "authorId": "1735666"}], "n_citations": 4}, "snippets": ["By default, attention layers as defined in (Vaswani et al., 2017) are bidirectional: every token can attend to tokens at every position. When it comes to discrimination, models based on such bidirectional attention are commonly used since \"intuitively, it is reasonable to believe that a deep bidirectional model is strictly more powerful than [", "] a left-to-right model\" (Devlin et al., 2019). However, while it brings some capacity to the model, it also makes it non auto-regressive: when a token is added at the end of a sequence, every hidden states need to be re-computed. \n\nOne way to train a transformer based LM for text generation is to use unidirectional attention masks [29]. In this unidirectional setting, any extra token added at the end of a sequence does not change the already calculated hidden states, since previous tokens do not attend to it. Thus, starting from an already classified sequence  1: \u22121 , classifying  1: only requires to compute  attention scores, rather than the whole set of  2 scores per self-attention layer, as it would be required in the bidirectional setting."], "score": 0.5556640625}, {"id": "(Vaswani et al., 2017)", "paper": {"corpus_id": 13756489, "title": "Attention is All you Need", "year": 2017, "venue": "Neural Information Processing Systems", "authors": [{"name": "Ashish Vaswani", "authorId": "40348417"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Niki Parmar", "authorId": "3877127"}, {"name": "Jakob Uszkoreit", "authorId": "39328010"}, {"name": "Llion Jones", "authorId": "145024664"}, {"name": "Aidan N. Gomez", "authorId": "19177000"}, {"name": "Lukasz Kaiser", "authorId": "40527594"}, {"name": "I. Polosukhin", "authorId": "3443442"}], "n_citations": 132444}, "snippets": ["The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."], "score": 0.0}, {"id": "(Lv et al., 2023)", "paper": {"corpus_id": 265150001, "title": "An Analysis and Mitigation of the Reversal Curse", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Ang Lv", "authorId": "66261602"}, {"name": "Kaiyi Zhang", "authorId": "2266486204"}, {"name": "Shufang Xie", "authorId": "1889683"}, {"name": "Quan Tu", "authorId": "2071635049"}, {"name": "Yuhan Chen", "authorId": "2266420263"}, {"name": "Ji-Rong Wen", "authorId": "2263887786"}, {"name": "Rui Yan", "authorId": "2172312251"}], "n_citations": 21}, "snippets": ["Converting a unidirectional causal attention mechanism in a causal language model into a bidirectional one is non-trivial. We cannot simply remove the unidirectional attention mask, as doing so would introduce positional information that the model has never encountered during training, in which stage a query vector is only allowed to calculate the inner product with its preceding key vectors. This is evident in Eq.4: the relative position n \u2212 m is always non-positive during training but is positive when q m needs to attend to k >m."], "score": 0.5771484375}, {"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."], "score": 0.82861328125}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Khosla et al., 2025)", "paper": {"corpus_id": 275544523, "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Savya Khosla", "authorId": "2056070459"}, {"name": "Aditi Tiwari", "authorId": "2218199485"}, {"name": "Kushal Kafle", "authorId": "33315685"}, {"name": "Simon Jenni", "authorId": "2297849207"}, {"name": "Handong Zhao", "authorId": "2341050386"}, {"name": "John P. Collomosse", "authorId": "2288642908"}, {"name": "Jing Shi", "authorId": "2288445861"}], "n_citations": 1}, "snippets": ["Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)", "During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling."], "score": 0.76513671875}, {"id": "(Busto-Castineira et al., 2025)", "paper": {"corpus_id": 276423946, "title": "Optimal word order for non-causal text generation with Large Language Models: The Spanish case", "year": 2025, "venue": "Pattern Recognition Letters", "authors": [{"name": "Andrea Busto-Casti\u00f1eira", "authorId": "2222734467"}, {"name": "Silvia Garc\u00eda-M\u00e9ndez", "authorId": "1405165681"}, {"name": "Francisco de Arriba-P\u00e9rez", "authorId": "2326130687"}, {"name": "F. Gonz\u00e1lez-Casta\u00f1o", "authorId": "1395988865"}], "n_citations": 0}, "snippets": ["Transformers are unsupervised learners thanks to their selfattention mechanism [48], which controls the impact of the context on the model's output. The original transformer architecture is composed of an encoder and a decoder. While the encoder's attention is bidirectional, the decoder has a masked multi-head attention block that masks non-causal context and a bidirectional multi-head attention block that receives noncausal information from the encoder.\n\nAlthough the encoder-decoder architecture is widely used in some nlp applications like machine translation [21,29], other transformer-based models only use one of these two components. By excluding the encoder, we eliminate all non-causal contextual dependencies, thus using only the masked attention of the decoder. Currently, decoder-only transformers are the most effective task-agnostic nlg systems.\n\nWhile open domain nlg is mainly causal, there are a few noncausal nlg solutions. Most non-causal nlg systems are focused on particular tasks such as speech recognition [5,50], style transfer and grammar correction [20], textual data augmentation [33], and dialog systems [56,51].\n\nNon-causal language models can also be trained for masked Language Modeling (mlm) [57]. mlm is an nlg task consisting of predicting masked words within a sentence. Some generative systems use bidirectional transformers trained on this task to recursively generate and fill masked tokens [38]. As these can be filled in any location within the text, these models can produce text in a non-causal way."], "score": 0.6884765625}], "table": null}, {"title": "Encoder-Only Models", "tldr": "Encoder-only models like BERT use bidirectional attention to understand context from both directions simultaneously, making them excellent for comprehension tasks like classification, question answering, and natural language inference, but less efficient for text generation. (10 sources)", "text": "\nEncoder-only models represent a major architectural approach in the landscape of language models, with BERT (Bidirectional Encoder Representations from Transformers) being the most prominent example. These models are distinguished by their use of bidirectional self-attention mechanisms, which allow each token to attend to all other tokens in the sequence regardless of position <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>. This bidirectional processing enables the models to \"capture contextual information bidirectionally, fostering a comprehensive understanding of language semantics\" <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper>.\n\nThe architecture of encoder-only models is based on the Transformer encoder, which \"allows it to attend to all tokens in the sequence, both before and after each word\" <Paper corpusId=\"274305758\" paperTitle=\"(Yang et al., 2024)\" isShortName></Paper>. During processing, the input is tokenized, and some tokens may be masked. These tokens are then \"fed into Transformer blocks with self-attention to obtain contextualized output embeddings\" <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper>. The resulting embeddings can be processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads.\n\nThe bidirectional nature of encoder-only models makes them particularly well-suited for \"natural language understanding (NLU) tasks including sentence similarity, natural language inference, and question answering\" <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3432876\" paperTitle=\"(Williams et al., 2017)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper> <Paper corpusId=\"11816014\" paperTitle=\"(Rajpurkar et al., 2016)\" isShortName></Paper>. These models \"excel in applications such as classification and question answering\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>, where the ability to process context from both directions simultaneously provides significant advantages for understanding the meaning and relationships between elements in text.\n\nHowever, encoder-only models face limitations when it comes to text generation tasks. While they can theoretically generate text autoregressively, this process is inefficient because \"each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>. Unlike decoder-only models that can use a key-value (KV) cache to efficiently generate multiple tokens during inference, \"BERT's bidirectional design prevents such optimization,\" making it \"impractical for token-by-token decoding\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nThe training objective for encoder-only models like BERT typically involves masked language modeling (MLM), \"where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context\" <Paper corpusId=\"278202626\" paperTitle=\"(Hallee et al., 2025)\" isShortName></Paper>. This approach helps the model develop a deep understanding of language semantics and contextual relationships, further enhancing its capabilities for comprehension tasks.\n\nIn summary, encoder-only models are primarily designed for \"discriminative tasks\" <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper> that require a thorough understanding of text, leveraging their bidirectional attention mechanisms to build rich contextual representations. While highly effective for understanding tasks, they are less suitable for the efficient generation of text when compared to their decoder-only counterparts.", "citations": [{"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Cascella et al., 2024)", "paper": {"corpus_id": 267701011, "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives", "year": 2024, "venue": "J. Medical Syst.", "authors": [{"name": "M. Cascella", "authorId": "2265474928"}, {"name": "Federico Semeraro", "authorId": "2265474397"}, {"name": "J. Montomoli", "authorId": "4239573"}, {"name": "Valentina Bellini", "authorId": "8663537"}, {"name": "Ornella Piazza", "authorId": "2239310513"}, {"name": "E. Bignami", "authorId": "2198023579"}], "n_citations": 83}, "snippets": ["Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial."], "score": 0.51123046875}, {"id": "(Yang et al., 2024)", "paper": {"corpus_id": 274305758, "title": "Can bidirectional encoder become the ultimate winner for downstream applications of foundation models?", "year": 2024, "venue": "2024 2nd International Conference on Foundation and Large Language Models (FLLM)", "authors": [{"name": "Lewen Yang", "authorId": "2332727631"}, {"name": "Xuanyu Zhou", "authorId": "2332813122"}, {"name": "Juao Fan", "authorId": "2332600501"}, {"name": "Xinyi Xie", "authorId": "2332733783"}, {"name": "Shengxin Zhu", "authorId": "2332541087"}], "n_citations": 0}, "snippets": ["GPT is a one-way or causal language model that predicts the next word in a sequence by considering only the preceding words. In contrast, BERT is a bidirectional language model, which processes both forward and backward context simultaneously to improve feature extraction and representation capacity. Both models are based on the Transformer (Vaswani et al., 2017) architecture, which consists of two main components: the encoder and decoder. Transformer introduces a self-attention mechanism. For each position in the sequence, the self-attention mechanism computes the query, the key and value vectors, and the attention weight, and uses the attention weight to weigh the sum of the value vectors to get the final output representation. In the Transformer encoder architecture, each encoding layer's multi-head self-attention sublayer will interact with each position in the input sequence with all other positions in the sequence, so that the selfattention layer when encoding a word will consider the entire sentence's words when encoding a word. However, in the decoder, the self-attention layer uses a mask matrix so that each position in the sequence can only see the sequence before it and the positions behind it will be hidden. GPT employs the Transformer decoder, which uses masked self-attention to only attend to past tokens in the sequence, making it suitable for tasks like text generation. On the other hand, BERT utilizes the Transformer encoder, which allows it to attend to all tokens in the sequence, both before and after each word, making it ideal for tasks that require deep understanding of context, such as question answering and text classification."], "score": 0.701171875}, {"id": "(Posada et al., 2024)", "paper": {"corpus_id": 270703043, "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Andrea Posada", "authorId": "2308036602"}, {"name": "D. Rueckert", "authorId": "2091163163"}, {"name": "Felix Meissen", "authorId": "2126502908"}, {"name": "Philip M\u00fcller", "authorId": "1399796227"}], "n_citations": 0}, "snippets": ["Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively."], "score": 0.5234375}, {"id": "(Park et al., 2024)", "paper": {"corpus_id": 267312283, "title": "A Comprehensive Survey of Compression Algorithms for Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Seungcheol Park", "authorId": "2108418112"}, {"name": "Jaehyeon Choi", "authorId": "2281832669"}, {"name": "Sojin Lee", "authorId": "2281792832"}, {"name": "U. Kang", "authorId": "2281746333"}], "n_citations": 16}, "snippets": ["Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration."], "score": 0.52587890625}, {"id": "(Williams et al., 2017)", "paper": {"corpus_id": 3432876, "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "year": 2017, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Adina Williams", "authorId": "81840293"}, {"name": "Nikita Nangia", "authorId": "10666396"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 4497}, "snippets": ["This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."], "score": 0.0}, {"id": "(Rajpurkar et al., 2018)", "paper": {"corpus_id": 47018994, "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pranav Rajpurkar", "authorId": "2706258"}, {"name": "Robin Jia", "authorId": "3422908"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 2854}, "snippets": ["Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."], "score": 0.0}, {"id": "(Rajpurkar et al., 2016)", "paper": {"corpus_id": 11816014, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "year": 2016, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pranav Rajpurkar", "authorId": "2706258"}, {"name": "Jian Zhang", "authorId": "2151810148"}, {"name": "Konstantin Lopyrev", "authorId": "2787620"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 8174}, "snippets": ["We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"], "score": 0.0}, {"id": "(Katz et al., 2024)", "paper": {"corpus_id": 274992300, "title": "Segment-Based Attention Masking for GPTs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shahar Katz", "authorId": "121254633"}, {"name": "Liran Ringel", "authorId": "2186740854"}, {"name": "Yaniv Romano", "authorId": "2335566528"}, {"name": "Lior Wolf", "authorId": "2284763723"}], "n_citations": 1}, "snippets": ["Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."], "score": 0.61474609375}, {"id": "(Hallee et al., 2025)", "paper": {"corpus_id": 278202626, "title": "Contrastive learning and mixture of experts enables precise vector embeddings in biological databases", "year": 2025, "venue": "Scientific Reports", "authors": [{"name": "Logan Hallee", "authorId": "2143308913"}, {"name": "Rohan Kapur", "authorId": "2281747985"}, {"name": "Arjun Patel", "authorId": "2281850189"}, {"name": "Jason P. Gleghorn", "authorId": "4561626"}, {"name": "Bohdan B. Khomtchouk", "authorId": "2281747963"}], "n_citations": 1}, "snippets": ["GPT (Generative Pretrained Transformer) models, such as OpenAI's GPT series (GPT-3, GPT-4, etc.), are designed for generative tasks and use transformer decoders [36][37][38] . They employ causal (unidirectional) attention, meaning each token attends only to previous tokens in the sequence, enabling autoregressive generation during inference. This allows them to predict the next word in a sequence without direct access to future words. \n\nIn contrast, BERT models utilize transformer encoders with bidirectional attention, meaning they can attend to all tokens within an input simultaneously. This structure enables them to capture additional contextual dependencies, making them well-suited for tasks like text classification and sentence similarity 39 . Unlike GPT models, BERT is trained using a masked language modeling (MLM) objective, where some tokens are randomly hidden, requiring the model to predict them based on the surrounding context."], "score": 0.70068359375}], "table": null}, {"title": "Decoder-Only Models", "tldr": "Decoder-only models use either causal attention (allowing tokens to only attend to previous tokens) or prefix decoder attention (combining bidirectional attention for prefixes with causal attention for generation), making them particularly effective for text generation tasks while maintaining computational efficiency during inference. (10 sources)", "text": "\nDecoder-only models have emerged as the dominant architecture for modern large language models, with two main variants: causal decoder and prefix decoder architectures <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. The distinction between these variants lies primarily in their attention mechanisms, which determine how information flows through the model.\n\nThe causal decoder architecture, exemplified by the GPT series of models, employs a unidirectional attention mechanism that restricts each token to attend only to itself and preceding tokens <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218971783\" paperTitle=\"(Brown et al., 2020)\" isShortName></Paper>. This design ensures an autoregressive generation process with a strictly left-to-right information flow <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper>. The unidirectional constraint is implemented through a specific attention mask that prevents tokens from accessing information from future positions <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This architecture has become the standard for most modern large language models, including PaLM, LLaMA, OPT, and BLOOM <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>.\n\nThe prefix decoder architecture represents a hybrid approach that combines elements of both causal and non-causal attention. In this design, bidirectional attention is applied to the prefix tokens (allowing them to attend to both preceding and succeeding tokens), while unidirectional attention is maintained for the generated tokens <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"266755678\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. This arrangement enables \"flexible and controlled generation, conditioned on both the prefix and the generated tokens\" <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Notable models using this architecture include U-PaLM and GLM-130B <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"277349741\" paperTitle=\"(Nie et al., 2025)\" isShortName></Paper>.\n\nThe popularity of decoder-only models for text generation stems from their efficiency in autoregressive processing. Unlike encoder-only models that would need to recompute all hidden states when generating new tokens, decoder-only models can use a key-value (KV) cache for efficient token-by-token generation <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>. This makes them \"highly efficient for token-by-token generation\" during inference <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n\nHowever, the causal attention mechanism in standard decoder-only models does create limitations for certain applications. The restricted information flow means that \"the representation of each word is restricted to depend solely on the words that came before,\" which can be suboptimal for tasks requiring rich contextual understanding <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>. This limitation has led some researchers to suggest that \"the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations\" <Paper corpusId=\"269009682\" paperTitle=\"(BehnamGhader et al., 2024)\" isShortName></Paper>.\n\nDespite these limitations, decoder-only architectures\u2014particularly causal decoders\u2014have become the dominant choice for large language models focused on text generation <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>. Their ability to efficiently generate coherent text in an autoregressive manner makes them well-suited for a wide range of applications, while prefix decoders offer a compromise that provides some bidirectional context understanding while maintaining generation capabilities <Paper corpusId=\"270702559\" paperTitle=\"(Yin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Hommel et al., 2021)", "paper": {"corpus_id": 242033930, "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation", "year": 2021, "venue": "Psychometrika", "authors": [{"name": "Bj\u00f6rn E. Hommel", "authorId": "2136597350"}, {"name": "Franz-Josef M. Wollang", "authorId": "2138754315"}, {"name": "Veronika Kotova", "authorId": "2137277509"}, {"name": "H. Zacher", "authorId": "50999175"}, {"name": "S. Schmukle", "authorId": "5278036"}], "n_citations": 21}, "snippets": ["One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation."], "score": 0.52392578125}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 261064777, "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "year": 2023, "venue": "Empirical Software Engineering", "authors": [{"name": "Zibin Zheng", "authorId": "2148256392"}, {"name": "Kai-Chun Ning", "authorId": "2115304"}, {"name": "Jiachi Chen", "authorId": "2254800142"}, {"name": "Yanlin Wang", "authorId": "2214155529"}, {"name": "Wenqing Chen", "authorId": "2274095496"}, {"name": "Lianghong Guo", "authorId": "2217902484"}, {"name": "Weicheng Wang", "authorId": "2233023641"}], "n_citations": 76}, "snippets": ["This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture", "Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a)."], "score": 0.52880859375}, {"id": "(Brown et al., 2020)", "paper": {"corpus_id": 218971783, "title": "Language Models are Few-Shot Learners", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tom B. Brown", "authorId": "31035595"}, {"name": "Benjamin Mann", "authorId": "2056658938"}, {"name": "Nick Ryder", "authorId": "39849748"}, {"name": "Melanie Subbiah", "authorId": "2065894334"}, {"name": "J. Kaplan", "authorId": "152724169"}, {"name": "Prafulla Dhariwal", "authorId": "6515819"}, {"name": "Arvind Neelakantan", "authorId": "2072676"}, {"name": "Pranav Shyam", "authorId": "67311962"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Ariel Herbert-Voss", "authorId": "1404060687"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "T. Henighan", "authorId": "103143311"}, {"name": "R. Child", "authorId": "48422824"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Daniel M. Ziegler", "authorId": "2052152920"}, {"name": "Jeff Wu", "authorId": "49387725"}, {"name": "Clemens Winter", "authorId": "2059411355"}, {"name": "Christopher Hesse", "authorId": "144239765"}, {"name": "Mark Chen", "authorId": "2108828435"}, {"name": "Eric Sigler", "authorId": "2064673055"}, {"name": "Ma-teusz Litwin", "authorId": "1380985420"}, {"name": "Scott Gray", "authorId": "145565184"}, {"name": "Benjamin Chess", "authorId": "1490681878"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Christopher Berner", "authorId": "133740015"}, {"name": "Sam McCandlish", "authorId": "52238703"}, {"name": "Alec Radford", "authorId": "38909097"}, {"name": "I. Sutskever", "authorId": "1701686"}, {"name": "Dario Amodei", "authorId": "2698777"}], "n_citations": 42437}, "snippets": ["Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general."], "score": 0.0}, {"id": "(Yin et al., 2024)", "paper": {"corpus_id": 270702559, "title": "CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kai Yin", "authorId": "2265383225"}, {"name": "Chengkai Liu", "authorId": "2308073678"}, {"name": "Ali Mostafavi", "authorId": "2258714985"}, {"name": "Xia Hu", "authorId": "2308068627"}], "n_citations": 12}, "snippets": ["The causal decoder architecture employs a unidirectional attention mask, ensuring that each input token can only attend to preceding tokens and itself.Both input and output tokens are processed identically through the decoder (Zhao et al., 2023).\n\nThe prefix decoder architecture, also known as the non-causal decoder, modifies the masking mechanism used in causal decoders (Zhang et al., 2020).This adjustment allows for bidirectional attention over the prefix tokens while restricting attention to unidirectional generated tokens (Dong et al., 2019).Similar to the encoder-decoder architecture, prefix decoders can bidirectionally encode the prefix sequence and autoregressively predict the output tokens sequentially (Zhao et al., 2023).The same parameters are shared during both encoding and decoding processes.\n\nThe vanilla Transformer model is constructed on the encoder-decoder architecture (Vaswani et al., 2017), comprising two stacks of Transformer blocks designated as the encoder and decoder, respectively.The encoder utilizes stacked multi-head self-attention layers to encode the input sequence into its latent representations.Meanwhile, the decoder employs cross-attention mechanisms on these representations, and autoregressive generates the target sequence."], "score": 0.6611328125}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 266755678, "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference", "year": 2024, "venue": "Neurocomputing", "authors": [{"name": "Yi-Hsueh Liu", "authorId": "2116426849"}, {"name": "Haoyang He", "authorId": "2155082967"}, {"name": "Tianle Han", "authorId": "2184719751"}, {"name": "Xu Zhang", "authorId": "2273584640"}, {"name": "Mengyuan Liu", "authorId": "2210636248"}, {"name": "Jiaming Tian", "authorId": "2257433902"}, {"name": "Yutong Zhang", "authorId": "2257095790"}, {"name": "Jiaqi Wang", "authorId": "2110238778"}, {"name": "Xiaohui Gao", "authorId": "2277869261"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Shaochen Xu", "authorId": "2211904452"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Xin Zhang", "authorId": "2257586495"}, {"name": "Shu Zhang", "authorId": "2277750447"}, {"name": "Xintao Hu", "authorId": "1742535"}, {"name": "Tuo Zhang", "authorId": "49104946"}, {"name": "Ning Qiang", "authorId": "2251076040"}, {"name": "Tianming Liu", "authorId": "2254792886"}, {"name": "Bao Ge", "authorId": "2257302793"}], "n_citations": 74}, "snippets": ["The Causal Decoder Architecture: In the Causal Decoder architecture, each token in the model input sequence can only attend to past input tokens and itself during the decoding process. It achieves unidirectional attention to the input sequence by using a specific mask as shown in Figure 1. In fact, different architectures are mainly implemented by configuring different mask matrices.\n\nThe Prefix Decoder Architecture: The Prefix Decoder architecture combines the advantages of both the Encoderdecoder and Causal Decoder architectures. It leverages its unique mask configurations, as illustrated in Figure 1, enabling bidirectional attention for tokens in the prefix while maintaining unidirectional attention for generating subsequent tokens [54]. This design allows for the autoregressive generation of the output sequence with the flexibility to attend bi-directionally to the prefix tokens."], "score": 0.732421875}, {"id": "(Nie et al., 2025)", "paper": {"corpus_id": 277349741, "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tong Nie", "authorId": "94168461"}, {"name": "Jiangming Sun", "authorId": "2028643500"}, {"name": "Wei Ma", "authorId": "2277421553"}], "n_citations": 4}, "snippets": ["\u2022 Causal decoder. As a representative decoder-only architecture, causal decoder models introduce the unidirectional attention mask to ensure that each input token can only attend to the past tokens and itself. This mechanism makes them suitable for text generation tasks. Prominent examples are the GPTseries models (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020). \n\n\u2022 Non-causal decoder. Another kind of decoder-only architecture is the non-casual structure. This architecture performs bidirectional attention on prefix tokens and unidirectional attention only on generated tokens. One representative prefix decoder LLMs is GLM (Zeng et al., 2022)."], "score": 0.63818359375}, {"id": "(Katz et al., 2024)", "paper": {"corpus_id": 274992300, "title": "Segment-Based Attention Masking for GPTs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shahar Katz", "authorId": "121254633"}, {"name": "Liran Ringel", "authorId": "2186740854"}, {"name": "Yaniv Romano", "authorId": "2335566528"}, {"name": "Lior Wolf", "authorId": "2284763723"}], "n_citations": 1}, "snippets": ["Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."], "score": 0.61474609375}, {"id": "(Kopiczko et al., 2024)", "paper": {"corpus_id": 269981935, "title": "Bitune: Bidirectional Instruction-Tuning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "D. J. Kopiczko", "authorId": "2258964424"}, {"name": "Tijmen Blankevoort", "authorId": "83133279"}, {"name": "Yuki Markus Asano", "authorId": "2258963625"}], "n_citations": 3}, "snippets": ["But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling."], "score": 0.76611328125}, {"id": "(BehnamGhader et al., 2024)", "paper": {"corpus_id": 269009682, "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Parishad BehnamGhader", "authorId": "2101317786"}, {"name": "Vaibhav Adlakha", "authorId": "1666183192"}, {"name": "Marius Mosbach", "authorId": "2269460274"}, {"name": "Dzmitry Bahdanau", "authorId": "3335364"}, {"name": "Nicolas Chapados", "authorId": "2748188"}, {"name": "Siva Reddy", "authorId": "145732771"}], "n_citations": 241}, "snippets": ["We speculate that the slow adoption of decoder-only LLMs for text embedding tasks is partly due to their causal attention mechanism, which inherently limits their ability to produce rich contextualized representations. At any given layer, causal attention limits token interactions, ensuring that the representation of a token at position i is influenced solely by the representations of preceding tokens at positions 0, 1, . . . , i \u2212 1. Although this limitation is necessary for generative capabilities, it is sub-optimal for text embeddings as it prevents the representations from capturing information across the entire input sequence."], "score": 0.51220703125}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named", "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."], "score": 0.62548828125}], "table": null}, {"title": "Encoder-Decoder Models", "tldr": "Encoder-decoder models combine bidirectional attention in the encoder with causal attention in the decoder, while introducing cross-attention to connect the two components, making them particularly suitable for sequence-to-sequence tasks like translation and summarization. (16 sources)", "text": "\nEncoder-decoder models represent a fundamental architecture in the language model landscape, combining elements of both encoder-only and decoder-only approaches while introducing unique mechanisms for information flow. The architecture consists of two distinct components: \"the encoder processes and contextualizes the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention\" <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>.\n\nThe attention mechanisms in encoder-decoder models follow a specific pattern. The encoder employs bidirectional attention, allowing each token to attend to all other tokens in the input sequence, which enables comprehensive context understanding. In contrast, \"the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multi-head attention block that receives non-causal information from the encoder\" <Paper corpusId=\"270832367\" paperTitle=\"(Busto-Castineira et al., 2024)\" isShortName></Paper>. This arrangement is reflected in the masking patterns: \"fully visible masking is used in the encoder and causal masking is used in the decoder\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\nThe cross-attention mechanism is a distinctive feature that connects the encoder and decoder components. It \"allows the decoder to access the fully processed encoder output and is responsible for connecting input tokens to target tokens\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>. The cross-attention enables the decoder to generate content that is directly informed by the encoder's bidirectional understanding of the input sequence. As Zhang et al. describe, cross-attention \"has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder\" <Paper corpusId=\"277626724\" paperTitle=\"(Zhang et al., 2025)\" isShortName></Paper>.\n\nThe bidirectional nature of the encoder's attention is crucial for model performance. Compared to the causal attention mechanism used in decoder-only models, \"the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations\" <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>. Specifically, in cross-domain applications, \"this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models\" <Paper corpusId=\"268247581\" paperTitle=\"(Pei et al., 2024)\" isShortName></Paper>.\n\nFor the decoder component, the standard approach uses causal attention to ensure autoregressive generation. As Liu et al. explain, \"For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token\" <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper>. This unidirectional constraint is necessary because, during generation, \"the decoder adopts a causal mask to force the state of each decoder time step only attend to the state from the previous time steps, to avoid seeing tokens 'from the future'\" <Paper corpusId=\"244119798\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>.\n\nHowever, variations in the decoder's attention mechanism have been explored. Researchers have developed alternative approaches such as the \"causal with prefix mask\" where \"bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens\" <Paper corpusId=\"244119798\" paperTitle=\"(Li et al., 2021)\" isShortName></Paper>. Similarly, the query-document attention mechanism proposed by Dong et al. implements a specialized attention pattern where \"each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment\" <Paper corpusId=\"245385261\" paperTitle=\"(Laskar et al., 2021)\" isShortName></Paper> <Paper corpusId=\"147704286\" paperTitle=\"(Dong et al., 2019)\" isShortName></Paper>.\n\nEncoder-decoder models excel in sequence-to-sequence tasks where there is \"a direct and complex transformation between inputs and outputs\" <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. This makes them particularly effective for applications like machine translation <Paper corpusId=\"209832341\" paperTitle=\"(Wu, 2020)\" isShortName></Paper> <Paper corpusId=\"219463386\" paperTitle=\"(Chen et al., 2020)\" isShortName></Paper> <Paper corpusId=\"234785837\" paperTitle=\"(Nguyen et al., 2021)\" isShortName></Paper>, text summarization, and question answering <Paper corpusId=\"263829839\" paperTitle=\"(Saha et al., 2023)\" isShortName></Paper>. Notable examples of encoder-decoder models include BART <Paper corpusId=\"204960716\" paperTitle=\"(Lewis et al., 2019)\" isShortName></Paper>, T5 <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>, and UL2 <Paper corpusId=\"252780443\" paperTitle=\"(Tay et al., 2022)\" isShortName></Paper>, which have demonstrated strong performance across a range of tasks.\n\nThe encoder-decoder architecture represents a middle ground between encoder-only and decoder-only approaches. While non-causal encoding provides comprehensive context understanding, the causal decoding supports efficient autoregressive text generation. This combination makes encoder-decoder models particularly well-suited for conditional generation tasks, where the output is directly dependent on understanding the input context.", "citations": [{"id": "(Pei et al., 2024)", "paper": {"corpus_id": 268247581, "title": "Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Qizhi Pei", "authorId": "2171652249"}, {"name": "Lijun Wu", "authorId": "47767791"}, {"name": "Kaiyuan Gao", "authorId": "1944690382"}, {"name": "Jinhua Zhu", "authorId": "151068900"}, {"name": "Yue Wang", "authorId": "2290062348"}, {"name": "Zun Wang", "authorId": "2290024261"}, {"name": "Tao Qin", "authorId": "2267250090"}, {"name": "Rui Yan", "authorId": "2257028545"}], "n_citations": 18}, "snippets": ["The standard Transformer (Vaswani et al., 2017) and its variants (Lewis et al., 2019), (Raffel et al., 2019) adopt the encoder-decoder framework (Figure 6d),where the encoder processes and contextualize the input sequences, and the decoder subsequently generates output based on this encoded context with encoder-decoder attention. Models such as BART (Lewis et al., 2019) and T5 (Raffel et al., 2019) are with this architecture and demonstrating its effectiveness across a broad spectrum of applications. In biomolecule and text cross modeling scenario, this framework makes the encoder utilize bi-directional attention to enable more comprehensive interaction between biotokens and text tokens compared to the causal attention of the decoder-only models. This enriched interaction allows for a deeper understanding of the input sequences. Subsequently, the encoder-decoder attention mechanism empowers the decoder to generate outputs tailored for specific biological tasks."], "score": 0.5068359375}, {"id": "(Busto-Castineira et al., 2024)", "paper": {"corpus_id": 270832367, "title": "Predictability and Causality in Spanish and English Natural Language Generation", "year": 2024, "venue": "IEEE Access", "authors": [{"name": "Andrea Busto-Casti\u00f1eira", "authorId": "2222734467"}, {"name": "Francisco Javier Gonz\u00e1lez-Casta\u00f1o", "authorId": "2323809078"}, {"name": "Silvia Garc\u00eda-M\u00e9ndez", "authorId": "1405165681"}, {"name": "Francisco de Arriba-P\u00e9rez", "authorId": "2034282614"}], "n_citations": 1}, "snippets": ["While the encoder's attention is bidirectional, the decoder has two different types of attention: (i) a masked multi-head attention block that masks non-causal context and (ii) a bidirectional multihead attention block that receives non-causal information from the encoder", "Even though this encoder-decoder architecture is popular in some NLP tasks such as machine translation [20], [21], [22], [23], several transformer-based models only have one of these components. By omitting the encoder in decoder-only transformers, all non-causal contextual dependencies are removed by exclusively using masked attention. Decoder-only transformers are nowadays the best performing task-agnostic NLG systems. Nevertheless, there exist some state-of-theart non-causal NLG solutions. For example, non-causal language models can be trained for the Masked Language Modeling (MLM) objective, a task in which the language model predicts masked words within a sentence [24]. Typically, non-causal NLG systems are focused on particular tasks such as speech recognition [25], [26], [27], style transfer and grammar correction [28], textual data augmentation [29], and task-specific dialog systems [30], [31]."], "score": 0.63671875}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks", "As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.853515625}, {"id": "(Zhang et al., 2025)", "paper": {"corpus_id": 277626724, "title": "Encoder-Decoder Gemma: Improving the Quality-Efficiency Trade-Off via Adaptation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Biao Zhang", "authorId": "2354284757"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Joshua Ainslie", "authorId": "2343748926"}, {"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Min Ma", "authorId": "2352024723"}, {"name": "Surya Bhupatiraju", "authorId": "9692128"}, {"name": "Federico Lebron", "authorId": "2275184616"}, {"name": "Orhan Firat", "authorId": "2273534960"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 0}, "snippets": ["1. Encoder has exactly the same architecture as the decoder-only model, but self-attention is switched from causal to bidirectional. We provide ablations in Section 6 that illustrate the critical effect of bidirectional attention on downstream performance.\n\n2. In each Decoder block, FFN and self-attention parts are identical to the corresponding parts in decoder-only models, and cross-attention has the same number of heads and head dimension as self-attention, but attends to the whole output of the encoder."], "score": 0.57080078125}, {"id": "(Jain et al., 2022)", "paper": {"corpus_id": 258461112, "title": "ContraCLM: Contrastive Learning For Causal Language Model", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nihal Jain", "authorId": "2146677401"}, {"name": "Dejiao Zhang", "authorId": "2358258"}, {"name": "Wasi Uddin Ahmad", "authorId": "38123220"}, {"name": "Zijian Wang", "authorId": "50219006"}, {"name": "Feng Nan", "authorId": "144647318"}, {"name": "Xiaopeng Li", "authorId": "2187045812"}, {"name": "Ming Tan", "authorId": "144745483"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Baishakhi Ray", "authorId": "31631000"}, {"name": "Parminder Bhatia", "authorId": "50339091"}, {"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 16}, "snippets": ["Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."], "score": 0.55224609375}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 273532366, "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junhua Liu", "authorId": "2327246234"}, {"name": "Bin Fu", "authorId": "2325881142"}], "n_citations": 1}, "snippets": ["Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure."], "score": 0.5791015625}, {"id": "(Li et al., 2021)", "paper": {"corpus_id": 244119798, "title": "TWT: Table with Written Text for Controlled Data-to-Text Generation", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Tongliang Li", "authorId": "2841012"}, {"name": "Lei Fang", "authorId": "2153681740"}, {"name": "Jian-Guang Lou", "authorId": "153249455"}, {"name": "Zhoujun Li", "authorId": "1707275"}], "n_citations": 11}, "snippets": ["Typically, the encoder-decoder based models generate text starting from the beginning, and the decoder adopts a causal mask to force the state of each decoder time step s t i only attend to the state from the previous time steps, s t|t\u2264t i , to avoid seeing tokens \"from the future\". We consider this type of attention as unidirectional. In our task, we have the input prefix as the written text. Tokens in the prefix should be visible to each other. Therefore, we adopt the causal with prefix mask: bidirectional attention mask is applied to the prefix, unidirectional attention is for decoding new tokens."], "score": 0.57763671875}, {"id": "(Laskar et al., 2021)", "paper": {"corpus_id": 245385261, "title": "Domain Adaptation with Pre-trained Transformers for Query-Focused Abstractive Text Summarization", "year": 2021, "venue": "Computational Linguistics", "authors": [{"name": "Md Tahmid Rahman Laskar", "authorId": "46437970"}, {"name": "Enamul Hoque", "authorId": "2939577"}, {"name": "J. Huang", "authorId": "1683391"}], "n_citations": 45}, "snippets": ["(i) The Bidirectional Self-Attention Mechanism: In the original BERTSUM architecture, the bidirectional self-attention mechanism (Devlin et al., 2019) is utilized by the BERT encoder to generate the encoded representation of the input text. In the bidirectional self-attention mechanism, when a pair of sentences are combined together and given as input to the BERT encoder, both sentences will give attention to each other. Thus, when we utilize the bidirectional self-attention mechanism (see Figure 2a) in the PreQFAS model, both the query and the document will not only give attention to themselves, but also they will give attention to each other to provide the encoded representation of the concatenated input.\n\n(ii) The Query-Document Attention Mechanism: Dong et al. (2019) proposed the sequence-tosequence language modeling objective for text sequences that are consisted of two segments. In such text sequences, each token in the first segment can only attend to the tokens in both directions within the same segment but cannot attend to any tokens in the second segment, while the tokens in the second segment can attend to the leftward tokens in their own segment as well as to all tokens in the first segment."], "score": 0.642578125}, {"id": "(Dong et al., 2019)", "paper": {"corpus_id": 147704286, "title": "Unified Language Model Pre-training for Natural Language Understanding and Generation", "year": 2019, "venue": "Neural Information Processing Systems", "authors": [{"name": "Li Dong", "authorId": "145307652"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Wenhui Wang", "authorId": "51456429"}, {"name": "Furu Wei", "authorId": "49807919"}, {"name": "Xiaodong Liu", "authorId": "46522098"}, {"name": "Yu Wang", "authorId": "72682749"}, {"name": "Jianfeng Gao", "authorId": "1800422"}, {"name": "M. Zhou", "authorId": "143849609"}, {"name": "H. Hon", "authorId": "145058181"}], "n_citations": 1560}, "snippets": ["This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL."], "score": 0.0}, {"id": "(Saha et al., 2023)", "paper": {"corpus_id": 263829839, "title": "LLM for SoC Security: A Paradigm Shift", "year": 2023, "venue": "IEEE Access", "authors": [{"name": "Dipayan Saha", "authorId": "2256992493"}, {"name": "Shams Tarek", "authorId": "2114625129"}, {"name": "Katayoon Yahyaei", "authorId": "2256991081"}, {"name": "Sujan Kumar Saha", "authorId": "2231854143"}, {"name": "Jingbo Zhou", "authorId": "2257235852"}, {"name": "M. Tehranipoor", "authorId": "145954982"}, {"name": "Farimah Farahmandi", "authorId": "1997019"}], "n_citations": 54}, "snippets": ["The encoder component transforms input tokens into vectors using embeddings and positional encodings, then applies multihead self-attention and feedforward networks. The decoder, starting similarly, incorporates masked self-attention and crossattention with the output of the encoder, ensuring alignment and preventing future word prediction. Sequence-to-sequence models often use this architecture, where the input and output sequences can be of different lengths. These models shine in tasks where there is a direct and complex transformation between inputs and outputs. Examples like machine translation and text summarization are prototypical, as they require the model to understand the input deeply and generate a coherent and contextually accurate output. BART [5], T5 (Raffel et al., 2019), and UL2 (Tay et al., 2022) are a few well-known encoder-decoder models to be named", "In a decoder-only model, a sequence is fed into the model, which then directly predicts the next token or word in the sequence. It operates autoregressively, using its generated tokens as context for subsequent predictions. It has two variants: causal decoder and prefix decoder. In a standard decoder (causal decoder), the unidirectional attention masking ensures that a token attends only to previous tokens and itself. Prefix decoders permit bidirectional attention over prefix tokens while maintaining unidirectional attention to generated tokens."], "score": 0.62548828125}, {"id": "(Wu, 2020)", "paper": {"corpus_id": 209832341, "title": "Learning Accurate Integer Transformer Machine-Translation Models", "year": 2020, "venue": "SN Computer Science", "authors": [{"name": "Ephrem Wu", "authorId": "11591799"}], "n_citations": 4}, "snippets": ["We describe a method for training accurate Transformer machine-translation models to run inference using 8-bit integer (INT8) hardware matrix multipliers, as opposed to the more costly single-precision floating-point (FP32) hardware. Unlike previous work, which converted only 85 Transformer matrix multiplications to INT8, leaving 48 out of 133 of them in FP32 because of unacceptable accuracy loss, we convert them all to INT8 without compromising accuracy. Tested on the newstest2014 English-to-German translation task, our INT8 Transformer Base and Transformer Big models yield BLEU scores that are 99.3\u2013100% relative to those of the corresponding FP32 models. Our approach converts all matrix-multiplication tensors from an existing FP32 model into INT8 tensors by automatically making range-precision trade-offs during training. To demonstrate the robustness of this approach, we also include results from INT6 Transformer models."], "score": 0.0}, {"id": "(Chen et al., 2020)", "paper": {"corpus_id": 219463386, "title": "Towards More Diverse Input Representation for Neural Machine Translation", "year": 2020, "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing", "authors": [{"name": "Kehai Chen", "authorId": "2849740"}, {"name": "Rui Wang", "authorId": "108085542"}, {"name": "M. Utiyama", "authorId": "1802277"}, {"name": "E. Sumita", "authorId": "1698363"}, {"name": "T. Zhao", "authorId": "145382463"}, {"name": "Muyun Yang", "authorId": "2105775"}, {"name": "Hai Zhao", "authorId": "47941144"}], "n_citations": 23}, "snippets": ["Source input information plays a very important role in the Transformer-based translation system. In practice, word embedding and positional embedding of each word are added as the input representation. Then self-attention networks are used to encode the global dependencies in the input representation to generate a source representation. However, this processing on the source representation only adopts a single source feature and excludes richer and more diverse features such as recurrence features, local features, and syntactic features, which results in tedious representation and thereby hinders the further translation performance improvement. In this paper, we introduce a simple and efficient method to encode more diverse source features into the input representation simultaneously, and thereby learning an effective source representation by self-attention networks. In particular, the proposed grouped strategy is only applied to the input representation layer, to keep the diversity of translation information and the efficiency of the self-attention networks at the same time. Experimental results show that our approach improves the translation performance over the state-of-the-art baselines of Transformer in regard to WMT14 English-to-German and NIST Chinese-to-English machine translation tasks."], "score": 0.0}, {"id": "(Nguyen et al., 2021)", "paper": {"corpus_id": 234785837, "title": "Improving Transformer-Based Neural Machine Translation with Prior Alignments", "year": 2021, "venue": "Complex", "authors": [{"name": "Thien Nguyen", "authorId": "33775047"}, {"name": "Lam Nguyen", "authorId": "2151126066"}, {"name": "Phuoc Tran", "authorId": "153460631"}, {"name": "Huu Nguyen", "authorId": "122619186"}], "n_citations": 22}, "snippets": ["Transformer is a neural machine translation model which revolutionizes machine translation. Compared with traditional statistical machine translation models and other neural machine translation models, the recently proposed transformer model radically and fundamentally changes machine translation with its self-attention and cross-attention mechanisms. These mechanisms effectively model token alignments between source and target sentences. It has been reported that the transformer model provides accurate posterior alignments. In this work, we empirically prove the reverse effect, showing that prior alignments help transformer models produce better translations. Experiment results on Vietnamese-English news translation task show not only the positive effect of manually annotated alignments on transformer models but also the surprising outperformance of statistically constructed alignments reinforced with the flexibility of token-type selection over manual alignments in improving transformer models. Statistically constructed word-to-lemma alignments are used to train a word-to-word transformer model. The novel hybrid transformer model improves the baseline transformer model and transformer model trained with manual alignments by 2.53 and 0.79 BLEU, respectively. In addition to BLEU score, we make limited human judgment on translation results. Strong correlation between human and machine judgment confirms our findings."], "score": 0.0}, {"id": "(Lewis et al., 2019)", "paper": {"corpus_id": 204960716, "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "M. Lewis", "authorId": "35084211"}, {"name": "Yinhan Liu", "authorId": "11323179"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Marjan Ghazvininejad", "authorId": "2320509"}, {"name": "Abdel-rahman Mohamed", "authorId": "113947684"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Veselin Stoyanov", "authorId": "1759422"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 10856}, "snippets": ["We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance."], "score": 0.0}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Tay et al., 2022)", "paper": {"corpus_id": 252780443, "title": "UL2: Unifying Language Learning Paradigms", "year": 2022, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yi Tay", "authorId": "144447820"}, {"name": "Mostafa Dehghani", "authorId": "3226635"}, {"name": "Vinh Q. Tran", "authorId": "2057663102"}, {"name": "Xavier Garc\u00eda", "authorId": "143936294"}, {"name": "Jason Wei", "authorId": "119640649"}, {"name": "Xuezhi Wang", "authorId": "1524732527"}, {"name": "Hyung Won Chung", "authorId": "3351938"}, {"name": "Dara Bahri", "authorId": "2119725651"}, {"name": "Tal Schuster", "authorId": "32303439"}, {"name": "H. Zheng", "authorId": "2115689465"}, {"name": "Denny Zhou", "authorId": "65855107"}, {"name": "N. Houlsby", "authorId": "2815290"}, {"name": "Donald Metzler", "authorId": "1680617"}], "n_citations": 313}, "snippets": ["Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B."], "score": 0.0}], "table": null}, {"title": "Hybrid Attention Approaches", "tldr": "Hybrid attention approaches combine causal and non-causal mechanisms to leverage their complementary strengths, with models like PrefixLM implementing bidirectional attention for prefix tokens and causal attention for generation, while others like context-span models allow different attention patterns for different parts of the sequence. (8 sources)", "text": "\nHybrid attention approaches have emerged as innovative solutions that aim to combine the strengths of both causal and non-causal attention mechanisms. These approaches seek to balance the comprehensive contextual understanding provided by bidirectional attention with the efficient autoregressive generation capabilities of unidirectional attention.\n\nThe PrefixLM architecture, introduced in T5, represents one of the most prominent hybrid approaches. In this design, the input sequence is divided into a prefix section and a subsequent generation section. The prefix is processed with bidirectional attention, \"allowing the model to attend to all tokens within it,\" while \"the remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This configuration \"combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence\" <Paper corpusId=\"276771845\" paperTitle=\"(Suganthan et al., 2025)\" isShortName></Paper>.\n\nNijkamp et al. describe a similar approach where \"for the prefix p = (x\u2081, ..., x\u2098) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x\u2098\u208a\u2081, ..., x\u2099), each token can only attend to previous tokens, which amounts to uni-directional decoder representations\" <Paper corpusId=\"258461229\" paperTitle=\"(Nijkamp et al., 2023)\" isShortName></Paper> <Paper corpusId=\"204838007\" paperTitle=\"(Raffel et al., 2019)\" isShortName></Paper>. This unification of bidirectional and unidirectional attention aims to \"achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks\" <Paper corpusId=\"258461229\" paperTitle=\"(Nijkamp et al., 2023)\" isShortName></Paper>.\n\nIn the decoder-only model space, the prefix decoder architecture represents a specialized implementation of hybrid attention. Similar to the causal decoder, it consists of decoder layers, but with a key distinction in its attention mechanism: \"The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens\" <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper>. Models implementing this approach include U-PaLM and GLM-130B <Paper corpusId=\"261064777\" paperTitle=\"(Zheng et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247519241\" paperTitle=\"(Du et al., 2021)\" isShortName></Paper>.\n\nLu et al. further clarify the attention masking distinction in prefix decoders, noting that while \"Causal Mask employs unidirectional attention on prefix sequences,\" the \"Prefix Mask applies bidirectional attention\" <Paper corpusId=\"271600495\" paperTitle=\"(Lu et al., 2024)\" isShortName></Paper>. This differentiation in masking patterns allows prefix decoders to process dialogue history and current queries with comprehensive context awareness while maintaining efficient autoregressive generation for responses.\n\nAnother innovative hybrid approach is the context-span method described by Khosla et al. In this design, \"context tokens attend to all other context tokens in the sequence,\" implementing fully bidirectional attention within the context portion. Meanwhile, \"span tokens are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>. This configuration enables the model to operate in multiple modes: \"fully causal/unidirectional for open-ended text generation tasks, fully bidirectional representation learning tasks, or a combination of causal and bidirectional for text infilling\" <Paper corpusId=\"275544523\" paperTitle=\"(Khosla et al., 2025)\" isShortName></Paper>.\n\nThe semi-causal language model proposed by Hao et al. offers yet another hybrid approach. This architecture features \"a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>. In this configuration, \"the model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>. The authors argue that \"non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities\" <Paper corpusId=\"249626024\" paperTitle=\"(Hao et al., 2022)\" isShortName></Paper>, making this hybrid approach particularly valuable for transfer learning scenarios.\n\nHybrid attention approaches continue to evolve, with researchers exploring various combinations of attention patterns to optimize the trade-off between comprehensive context understanding and efficient generation. These approaches demonstrate that the strict dichotomy between causal and non-causal attention can be transcended through architectural innovations that selectively apply different attention mechanisms to different parts of the input sequence.", "citations": [{"id": "(Suganthan et al., 2025)", "paper": {"corpus_id": 276771845, "title": "Adapting Decoder-Based Language Models for Diverse Encoder Downstream Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "P. Suganthan", "authorId": "1658871094"}, {"name": "Fedor Moiseev", "authorId": "2165469946"}, {"name": "Le Yan", "authorId": "2348489099"}, {"name": "Junru Wu", "authorId": "2261361394"}, {"name": "Jianmo Ni", "authorId": "2348507846"}, {"name": "Jay Han", "authorId": "2348488953"}, {"name": "I. Zitouni", "authorId": "1954563"}, {"name": "Enrique Alfonseca", "authorId": "1727837"}, {"name": "Xuanhui Wang", "authorId": "2348422460"}, {"name": "Zhe Dong", "authorId": "2349772191"}], "n_citations": 1}, "snippets": ["Bidirectional masking, also referred as fullyvisible masking (Raffel et al., 2020), is commonly used in encoder models. It allows the encoder to generate a holistic representation of the input by providing complete access to all input tokens, fostering a comprehensive understanding of the entire sequence. \n\nCausal masking, on the other hand, is prevalent in decoder-only and sequence-to-sequence models. Here, tokens are processed sequentially, and predictions for the next token rely solely on preceding tokens. This prevents the model from \"looking ahead\" during training, preserving the auto-regressive property essential for text generation. The attention mechanism is masked so that each token attends only to itself and prior tokens. \n\nT5 introduced PrefixLM, a hybrid approach that utilizes causal masking with a designated \"prefix\" section. This prefix is processed bidirectionally, allowing the model to attend to all tokens within it. The remaining sequence is processed causally, enabling generation conditioned on the fully contextualized prefix. This combines the benefits of bidirectional context for understanding the initial input segment with the autoregressive capabilities of causal masking for generating the subsequent sequence."], "score": 0.82861328125}, {"id": "(Raffel et al., 2019)", "paper": {"corpus_id": 204838007, "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "year": 2019, "venue": "Journal of machine learning research", "authors": [{"name": "Colin Raffel", "authorId": "2402716"}, {"name": "Noam M. Shazeer", "authorId": "1846258"}, {"name": "Adam Roberts", "authorId": "145625142"}, {"name": "Katherine Lee", "authorId": "3844009"}, {"name": "Sharan Narang", "authorId": "46617804"}, {"name": "Michael Matena", "authorId": "1380243217"}, {"name": "Yanqi Zhou", "authorId": "2389316"}, {"name": "Wei Li", "authorId": "2157338362"}, {"name": "Peter J. Liu", "authorId": "35025299"}], "n_citations": 20336}, "snippets": ["Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."], "score": 0.0}, {"id": "(Nijkamp et al., 2023)", "paper": {"corpus_id": 258461229, "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Erik Nijkamp", "authorId": "2043490"}, {"name": "Hiroaki Hayashi", "authorId": "50376014"}, {"name": "Caiming Xiong", "authorId": "2054594326"}, {"name": "S. Savarese", "authorId": "1702137"}, {"name": "Yingbo Zhou", "authorId": "2118860628"}], "n_citations": 169}, "snippets": ["For a sequence x = (x 1 , . . . , x n ) of n vectors, we differ: (1) bi-directional encoder-based representations in which each token vector x i can attend all other tokens {x j : i = 1, . . . , n}, (2) uni-directional decoder-based representations in which each token vector x i can only attend previous tokens {x j : j \u2264 i}. While encoder-based representations for which each hidden vector can contextualize with all other vectors may be desirable for understanding tasks, decoder-based representations with temporal causal masking are required for language modeling for which the joint density is factorized as the product of conditionals over time steps. To unify both schemes, we adopt the notion of prefix-based language modeling (Prefix-LM) (Raffel et al., 2019). For a prefix, we decompose the input sequence x into a prefix p and a context c. For the prefix p = (x 1 , . . . , x m ) where m < n, each token can attend over all other tokens in the prefix, which amounts to bi-directional representations. For the context c = (x m+1 , . . . , x n ), each token can only attend to previous tokens, which amounts to uni-directional decoder representations. This unifies bi-directional attention over the prefix with the requirement of causal masking to factorize the joint density over time. The hope is to achieve competitive auto-regressive sampling for synthesis tasks, while learning strong bi-directional representations for understanding tasks."], "score": 0.5146484375}, {"id": "(Zheng et al., 2023)", "paper": {"corpus_id": 261064777, "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "year": 2023, "venue": "Empirical Software Engineering", "authors": [{"name": "Zibin Zheng", "authorId": "2148256392"}, {"name": "Kai-Chun Ning", "authorId": "2115304"}, {"name": "Jiachi Chen", "authorId": "2254800142"}, {"name": "Yanlin Wang", "authorId": "2214155529"}, {"name": "Wenqing Chen", "authorId": "2274095496"}, {"name": "Lianghong Guo", "authorId": "2217902484"}, {"name": "Weicheng Wang", "authorId": "2233023641"}], "n_citations": 76}, "snippets": ["This constraint ensures a unidirectional and autoregressive generation process. The GPT series model, initially introduced by OpenAI (Radford et al., 2018(Radford et al., 2019)(Brown et al., 2020), represents one of the most prominent examples of the causal decoder architecture", "Today, the causal decoder architecture has become the prevailing choice for large language model architectures, giving rise to a wide range of powerful LLMs such as PaLM (Chowdhery et al., 2022), LLaMA (Touvron et al., 2023), OPT (Zhang et al., 2022c), Bloom (Scao et al., 2022). The causal decoder architecture and the prefix decoder architecture, which will be discussed next, are collectively referred to as decoder-only architecture (Zhao et al., 2023b).\n\nPrefix Decoder Architecture: The prefix decoder, similar to the causal decoder architecture, consists of decoder layers. However, the key distinction is in their attention mechanism. The prefix decoder utilizes bidirectional attention for the prefix tokens, incorporating information from both preceding and succeeding tokens. In contrast, unidirectional attention is applied only to the generated tokens, ensuring a unidirectional flow of information during the generation process. This combination of attention mechanisms in the prefix decoder enables flexible and controlled generation, conditioned on both the prefix and the generated tokens. Some commonly known models based on the prefix decoder architecture include U-PaLM (Tay et al., 2022) and GLM-130B (Zeng et al., 2022a)."], "score": 0.52880859375}, {"id": "(Du et al., 2021)", "paper": {"corpus_id": 247519241, "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Zhengxiao Du", "authorId": "66395694"}, {"name": "Yujie Qian", "authorId": "5606742"}, {"name": "Xiao Liu", "authorId": "2111312892"}, {"name": "Ming Ding", "authorId": "145573466"}, {"name": "J. Qiu", "authorId": "40125294"}, {"name": "Zhilin Yang", "authorId": "2109512754"}, {"name": "Jie Tang", "authorId": "2109541439"}], "n_citations": 1554}, "snippets": ["There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25\u00d7 parameters of BERT Large , demonstrating its generalizability to different downstream tasks."], "score": 0.0}, {"id": "(Lu et al., 2024)", "paper": {"corpus_id": 271600495, "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Mingcong Lu", "authorId": "2314473248"}, {"name": "Jiangcai Zhu", "authorId": "2314649002"}, {"name": "Wang Hao", "authorId": "2314113733"}, {"name": "Zheng Li", "authorId": "2314323587"}, {"name": "Shusheng Zhang", "authorId": "2314311430"}, {"name": "Kailai Shao", "authorId": "2314110211"}, {"name": "Chao Chen", "authorId": "2314192630"}, {"name": "Nan Li", "authorId": "2314343132"}, {"name": "Feng Wang", "authorId": "2324104105"}, {"name": "Xin Lu", "authorId": "2324103820"}], "n_citations": 0}, "snippets": ["Existing language models can be grouped into three categories according to framework architecture: Encoder-Decoder Vaswani et al. [2017], (Raffel et al., 2019), (Lewis et al., 2019), Encoder-Only Kenton and Toutanova [2019], Liu et al. [2019], (Dong et al., 2019), and Decoder-Only (Brown et al., 2020), Touvron et al. [2023a,b], (Du et al., 2021). Nowadays, most LLMs belong to decoder-only architecture, in this paper, our discourse is delimited to decoder-only architecture. In addition, based on the masking methods in various attention mechanisms, decoder-only category further includes causal decoders (Brown et al., 2020), Touvron et al. [2023a] and prefix decoders (Du et al., 2021). The former employs unidirectional attention masking to restrict each token can only attend to preceding tokens and itself. Both the input and generated tokens are processed in a uniform manner within Taking the second round of dialogue as an example, we show the mask difference between our method and existing works. The dialogue history (Prompt+Query1+Answer1) and current Query2 serve as prefix sequences, LLMs should output Answer2. Causal Mask employs unidirectional attention on prefix sequences, while Prefix Mask applies bidirectional attention."], "score": 0.505859375}, {"id": "(Khosla et al., 2025)", "paper": {"corpus_id": 275544523, "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Savya Khosla", "authorId": "2056070459"}, {"name": "Aditi Tiwari", "authorId": "2218199485"}, {"name": "Kushal Kafle", "authorId": "33315685"}, {"name": "Simon Jenni", "authorId": "2297849207"}, {"name": "Handong Zhao", "authorId": "2341050386"}, {"name": "John P. Collomosse", "authorId": "2288642908"}, {"name": "Jing Shi", "authorId": "2288445861"}], "n_citations": 1}, "snippets": ["Context tokens. Each context token (shown in blue in Figure 2) attends to all other context tokens in the sequence. The attention mask has 0s at output positions corresponding to context tokens, allowing each context token to access information at every other context token. This transformation shifts the original unidirectional LLM into a bidirectional model. Span tokens. The span tokens (shown in green in Figure 2) are a contiguous span of input tokens that attend to all context tokens and have causal attention among themselves. By enabling span tokens to access surrounding context, we effectively convert the original LLM into an infilling language model. Additionally, the causal attention among span tokens preserves the LLM's generative capabilities, which could be compromised if bidirectionality is fully unlocked (see Section 4.4 for details)", "During inference, the attention mechanism can operate in three modes: (1) fully causal/unidirectional for open-ended text generation tasks, (2) fully bidirectional representation learning tasks, or (3) a combination of causal and bidirectional for text infilling."], "score": 0.76513671875}, {"id": "(Hao et al., 2022)", "paper": {"corpus_id": 249626024, "title": "Language Models are General-Purpose Interfaces", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Y. Hao", "authorId": "34128716"}, {"name": "Haoyu Song", "authorId": "2187079427"}, {"name": "Li Dong", "authorId": "145307652"}, {"name": "Shaohan Huang", "authorId": "3110003"}, {"name": "Zewen Chi", "authorId": "46221722"}, {"name": "Wenhui Wang", "authorId": "51456429"}, {"name": "Shuming Ma", "authorId": "2118866998"}, {"name": "Furu Wei", "authorId": "49807919"}], "n_citations": 99}, "snippets": ["Non-causal modeling (i.e., bidirectional encoder) is conducive to transfer across tasks, languages, and modalities. Although causal language models are good at zero-and few-shot generalization, BERT (Devlin et al., 2019) and T5 (Raffel et al., 2020) show that having bidirectional encoders pretrained by masked language modeling achieves much better finetuning performance. Once the whole input is given, non-causal modeling is quite rational for encoding data. Because all the context can access each other, while causal modeling can only make use of history tokens one by one. The advantage of finetuning is helpful for the data-rich setting where there are many annotated data available.\n\nAs shown in Figure 3, we summarize the model architectures of three language model variants and the proposed semi-causal language model. First, causal language model (such as GPT; Brown et al. 2020) is a left-to-right Transformer decoder. Second, prefix language model uses the encoder-decoder architecture with cross-attention connections to complete the sequence. Third, non-causal language model is a bidirectional encoder, which is usually pretrained by masked language modeling (Devlin et al., 2019). Forth, the proposed semi-causal language model has a unidirectional Transformer decoder, and multiple bidirectional encoders that dock with the decoder. In other words, our model processes the whole session from left to right, while having some spans pre-encoded by non-causal encoders."], "score": 0.7265625}], "table": null}, {"title": "Suitability for NLP Tasks", "tldr": "Different attention mechanisms are suited for specific NLP tasks, with bidirectional attention excelling in understanding tasks like classification and question answering, while unidirectional attention is optimal for autoregressive text generation, and hybrid approaches balancing the strengths of both. (13 sources)", "text": "\n## Text Understanding Tasks\n- **Classification**: Bidirectional attention mechanisms found in encoder-only models like BERT are particularly well-suited for text classification tasks, allowing the model to leverage context from both directions <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"52967399\" paperTitle=\"(Devlin et al., 2019)\" isShortName></Paper>.\n- **Natural Language Inference**: Encoder-only models with bidirectional attention excel at natural language inference, where understanding relationships between sentences is crucial <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"3432876\" paperTitle=\"(Williams et al., 2017)\" isShortName></Paper>.\n- **Question Answering**: Bidirectional models are particularly effective for question answering tasks, as they can process the entire context to locate precise answers <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper> <Paper corpusId=\"11816014\" paperTitle=\"(Rajpurkar et al., 2016)\" isShortName></Paper> <Paper corpusId=\"47018994\" paperTitle=\"(Rajpurkar et al., 2018)\" isShortName></Paper>.\n- **Sentence Similarity**: Tasks involving measuring semantic similarity between texts benefit from the comprehensive contextual representations provided by bidirectional attention models <Paper corpusId=\"267312283\" paperTitle=\"(Park et al., 2024)\" isShortName></Paper>.\n\n## Text Generation Tasks\n- **Open-ended Text Generation**: Decoder-only models with causal (unidirectional) attention are dominant for autoregressive text generation, as their left-to-right processing matches the natural flow of text generation <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper> <Paper corpusId=\"270703043\" paperTitle=\"(Posada et al., 2024)\" isShortName></Paper>.\n- **Conversational AI**: The unidirectional structure of decoder models makes them \"particularly effective for autoregressive tasks, such as conversational AI\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>.\n- **Generative Large Language Models**: Causal decoder architectures have become the standard for modern large language models focused on text generation, including GPT, Llama, and similar models <Paper corpusId=\"273532366\" paperTitle=\"(Liu et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"269981935\" paperTitle=\"(Kopiczko et al., 2024)\" isShortName></Paper>.\n\n## Sequence-to-Sequence Tasks\n- **Machine Translation**: Encoder-decoder models with bidirectional encoding and causal decoding excel at translation tasks, where understanding the source language and generating target language are both critical <Paper corpusId=\"242033930\" paperTitle=\"(Hommel et al., 2021)\" isShortName></Paper> <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper>.\n- **Summarization**: The encoder-decoder architecture is well-suited for text summarization, leveraging bidirectional understanding of the source text and causal generation of the summary <Paper corpusId=\"267701011\" paperTitle=\"(Cascella et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n- **Question Answering with Generation**: For question answering tasks that require generating answers rather than extracting them, encoder-decoder models offer advantages by combining understanding and generation capabilities <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>.\n\n## Task-Architecture Alignment Considerations\n- **Natural Language Understanding (NLU)**: \"Autoencoding models learn bidirectional contextualized representation suited for NLU tasks\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>, making encoder-only and encoder components of encoder-decoder models optimal for understanding-focused applications.\n- **Natural Language Generation (NLG)**: \"Autoregressive models learn to generate the next token and hence are suited for NLG tasks\" <Paper corpusId=\"268157336\" paperTitle=\"(Patil et al., 2024)\" isShortName></Paper>, making decoder-only and decoder components of encoder-decoder models better for generation tasks.\n- **Performance vs. Parameter Efficiency**: While encoder-decoder models offer strong performance on sequence-to-sequence tasks, they \"require approximately twice the number of parameters compared to decoder-only models with equivalent capacity\" <Paper corpusId=\"274992300\" paperTitle=\"(Katz et al., 2024)\" isShortName></Paper>, presenting a trade-off between capability and efficiency.\n- **Bidirectional Context Benefits**: \"The bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations\" <Paper corpusId=\"258461112\" paperTitle=\"(Jain et al., 2022)\" isShortName></Paper>, which is particularly valuable for tasks requiring deep understanding of text.", "citations": [{"id": "(Hommel et al., 2021)", "paper": {"corpus_id": 242033930, "title": "Transformer-Based Deep Neural Language Modeling for Construct-Specific Automatic Item Generation", "year": 2021, "venue": "Psychometrika", "authors": [{"name": "Bj\u00f6rn E. Hommel", "authorId": "2136597350"}, {"name": "Franz-Josef M. Wollang", "authorId": "2138754315"}, {"name": "Veronika Kotova", "authorId": "2137277509"}, {"name": "H. Zacher", "authorId": "50999175"}, {"name": "S. Schmukle", "authorId": "5278036"}], "n_citations": 21}, "snippets": ["One typically distinguishes between bidirectional and unidirectional transformer models. Bidirectional models attempt to predict each token in a sequence by using tokens that both precede and succeed the current target. Tokens are sequences of characters in a particular vocabulary that are grouped together as a useful semantic unit (e.g. words, syllables, prefixes, punctuations, etc.; (Manning et al., 2008). This makes such models suitable for tasks like binary text classification or machine translation (Camacho-Collados et al., 2018)Gonz\u00e1lez-Carvajal & Garrido-Merch\u00e1n, 2021). Unidirectional models, however, based their predictions of tokens in a sequence only on the set of preceding words, making them autoregressive. They are therefore sometimes referred to as causal transformer models and have proven themselves to be exceptionally useful in various applications in the domain of text generation."], "score": 0.52392578125}, {"id": "(Devlin et al., 2019)", "paper": {"corpus_id": 52967399, "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "year": 2019, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Jacob Devlin", "authorId": "39172707"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Kristina Toutanova", "authorId": "3259253"}], "n_citations": 95215}, "snippets": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."], "score": 0.0}, {"id": "(Park et al., 2024)", "paper": {"corpus_id": 267312283, "title": "A Comprehensive Survey of Compression Algorithms for Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Seungcheol Park", "authorId": "2108418112"}, {"name": "Jaehyeon Choi", "authorId": "2281832669"}, {"name": "Sojin Lee", "authorId": "2281792832"}, {"name": "U. Kang", "authorId": "2281746333"}], "n_citations": 16}, "snippets": ["Encoder-only Transformers (a) [25,(Devlin et al., 2019)83,98,121] generate useful embeddings that reflect contextual information within an input sequence via (bidirectional) self-attention. Encoder-only Transformers are used for natural language understanding (NLU) tasks including sentence similarity [13,(Dolan et al., 2005), natural language inference [140](Williams et al., 2017), and question answering (Rajpurkar et al., 2018)(Rajpurkar et al., 2016). On the other hand, decoder-only Transformers (b) (Brown et al., 2020)124,136,173] autoregressively predict output tokens via (unidirectional) masked self-attention which attends only current and previous tokens. We concatenate an output token to the end of an input sequence, and feed the augmented input sequence to the next iteration."], "score": 0.52587890625}, {"id": "(Williams et al., 2017)", "paper": {"corpus_id": 3432876, "title": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference", "year": 2017, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Adina Williams", "authorId": "81840293"}, {"name": "Nikita Nangia", "authorId": "10666396"}, {"name": "Samuel R. Bowman", "authorId": "3644767"}], "n_citations": 4497}, "snippets": ["This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement."], "score": 0.0}, {"id": "(Rajpurkar et al., 2016)", "paper": {"corpus_id": 11816014, "title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "year": 2016, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Pranav Rajpurkar", "authorId": "2706258"}, {"name": "Jian Zhang", "authorId": "2151810148"}, {"name": "Konstantin Lopyrev", "authorId": "2787620"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 8174}, "snippets": ["We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL"], "score": 0.0}, {"id": "(Rajpurkar et al., 2018)", "paper": {"corpus_id": 47018994, "title": "Know What You Don\u2019t Know: Unanswerable Questions for SQuAD", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Pranav Rajpurkar", "authorId": "2706258"}, {"name": "Robin Jia", "authorId": "3422908"}, {"name": "Percy Liang", "authorId": "145419642"}], "n_citations": 2854}, "snippets": ["Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD."], "score": 0.0}, {"id": "(Cascella et al., 2024)", "paper": {"corpus_id": 267701011, "title": "The Breakthrough of Large Language Models Release for Medical Applications: 1-Year Timeline and Perspectives", "year": 2024, "venue": "J. Medical Syst.", "authors": [{"name": "M. Cascella", "authorId": "2265474928"}, {"name": "Federico Semeraro", "authorId": "2265474397"}, {"name": "J. Montomoli", "authorId": "4239573"}, {"name": "Valentina Bellini", "authorId": "8663537"}, {"name": "Ornella Piazza", "authorId": "2239310513"}, {"name": "E. Bignami", "authorId": "2198023579"}], "n_citations": 83}, "snippets": ["Firstly, there are encoder-only LLMs exemplified by BERT (Bidirectional Encoder Representations from Transformers) and its various iterations. These models excel in capturing contextual information bidirectionally, fostering a comprehensive understanding of language semantics. Secondly, decoderonly language models, as epitomized by the GPT family members, emphasize the generation of coherent and contextually relevant sequences. Leveraging unidirectional attention blocks, these models have demonstrated proficiency in tasks requiring sequential understanding and generation. Lastly, encoder-decoder language models, such as T5 (Textto-Text Transfer Transformer) and BART (Bidirectional and AutoRegressive Transformers), represent a fusion of both bidirectional and unidirectional attention mechanisms. This hybrid approach allows for versatile applications, ranging from text summarization to language translation, where understanding context and generating coherent responses are both crucial."], "score": 0.51123046875}, {"id": "(Posada et al., 2024)", "paper": {"corpus_id": 270703043, "title": "Evaluation of Language Models in the Medical Context Under Resource-Constrained Settings", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Andrea Posada", "authorId": "2308036602"}, {"name": "D. Rueckert", "authorId": "2091163163"}, {"name": "Felix Meissen", "authorId": "2126502908"}, {"name": "Philip M\u00fcller", "authorId": "1399796227"}], "n_citations": 0}, "snippets": ["Encoder-only models are mainly used for discriminative tasks. Their input is tokenized, and some of these tokens are masked. They are then fed into Transformer blocks with self-attention to obtain contextualized output embeddings, which are further processed by next sentence prediction (NSP) and language model (LM) heads or used by downstream task-specific heads. Depending on the training objective, the NSP head may or may not be necessary. Decoder-only models focus on generation tasks. Their input is tokenized and fed to Transformer blocks with causal self-attention. The causal self-attention ensures that the information flows unidirectionally from left to right. Encoder-decoder models are used for text-to-text tasks. Their encoder processes the input text, similar to encoder-only models but excluding the NSP head, and flows information to the decoder via the cross-attention mechanism. This information is used with the target output so that the decoder learns to produce the latter generatively."], "score": 0.5234375}, {"id": "(Katz et al., 2024)", "paper": {"corpus_id": 274992300, "title": "Segment-Based Attention Masking for GPTs", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shahar Katz", "authorId": "121254633"}, {"name": "Liran Ringel", "authorId": "2186740854"}, {"name": "Yaniv Romano", "authorId": "2335566528"}, {"name": "Lior Wolf", "authorId": "2284763723"}], "n_citations": 1}, "snippets": ["Encoder transformer models (Devlin, 2018) read text bidirectionally, leveraging both preceding and subsequent words to build a rich contextual representation of the input. In contrast, decoder models, commonly referred to as GPT models (Radford et al., 2018), process text unidirectionally, from left to right. This unidirectional structure enables scalability and makes GPTs particularly effective for autoregressive tasks, such as conversational AI.\n\nThe original Transformer architecture introduced by Vaswani et al. (2017) utilized an encoderdecoder framework, where the encoder built a context for the input, and the decoder generated the output. However, this design requires approximately twice the number of parameters compared to decoder-only models with equivalent capacity.\n\nEncoder-only models like BERT (Devlin, 2018) are primarily designed for bidirectional understanding tasks and excel in applications such as classification and question answering. While BERT can generate text autoregressively, each newly generated token changes the attention computation, requiring all dependent hidden states to be recomputed. Unlike decoder-only models, which use a key-value (KV) cache to efficiently generate multiple tokens during inference, BERT's bidirectional design prevents such optimization. This makes BERT impractical for token-by-token decoding.\n\nThe T5 framework (Raffel et al., 2020), with its encoder-decoder architecture, is effective for many NLP tasks due to its ability to incorporate bidirectional context during encoding and causal generation during decoding. However, SOTA and efficient performances in text generation are dominated by decoder-only models with causal masking, such as GPT-based architectures (Brown et al., 2020;Jiang et al., 2023;Yang et al., 2024;Dubey et al., 2024;Abdin et al., 2024). These models are highly efficient for token-by-token generation but cannot fully utilize input prompt information in their current design."], "score": 0.61474609375}, {"id": "(Liu et al._1, 2024)", "paper": {"corpus_id": 273532366, "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Junhua Liu", "authorId": "2327246234"}, {"name": "Bin Fu", "authorId": "2325881142"}], "n_citations": 1}, "snippets": ["Decoder Structure: The decoder structure is shown in Figure 9a. This structure is currently employed by mainstream MLLMs, allowing the model to be trained like traditional autoregressive language models using unidirectional attention mechanisms for next token prediction (NTP). This architecture is utilized by Llama2, GPT series, BLOOM, and XGLM models.\n\nEncoder-Decoder Structure: The traditional encoderdecoder structure is shown in Figure 9b. This structure uses the encoder and decoder of the Transformer. It first inputs the token sequence into the encoder to obtain a sequence vector of the same length as the input, and then uses it as the input of the decoder. The decoder uses cross-attention mechanisms, using bidirectional attention mechanisms for input to pay attention to all contexts of the input. Bidirectional attention mechanism is an efficient strategy for utilizing data because it can use information before and after this token when predicting tokens. However, this method is better at natural language understanding tasks rather than the natural language generation tasks that large models do now, so it is less used independently in large models. For the output sequence, it uses unidirectional attention mechanisms to prevent the model from paying attention to information after the predicted token.\n\nPrefix LM Structure: The prefix LM is essentially still a decoder structure, but it changes the attention mechanism in the decoder structure."], "score": 0.5791015625}, {"id": "(Kopiczko et al., 2024)", "paper": {"corpus_id": 269981935, "title": "Bitune: Bidirectional Instruction-Tuning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "D. J. Kopiczko", "authorId": "2258964424"}, {"name": "Tijmen Blankevoort", "authorId": "83133279"}, {"name": "Yuki Markus Asano", "authorId": "2258963625"}], "n_citations": 3}, "snippets": ["But with only uni-directional causal attention, where the representation of each word is restricted to depend solely on the words that came before, this cannot be achieved.This is the reason why many previous transformers such as encoder-only BERT (Devlin et al., 2019) and encoder-decoder T5 (Raffel et al., 2019) employed bidirectional attention to improve the encoding of the input and why tasks like text retrieval (Lewis et al., 2020)Li & Li, 2023) and even the latest text-to-image generative models (OpenAI, 2024b;Esser et al., 2024) still rely on this.However, in the context of LLMs, architectures utilizing bidirectional attention have fallen out of favor, as decoder-only models such as GPT (OpenAI, 2024a) and Llama (AI@Meta, 2024) have focused on and vastly improved the generative performance of language models.These architectures are trained by large volumes of data with next-token prediction, eschewing any look-ahead mechanism for the sake of better autoregressive modeling."], "score": 0.76611328125}, {"id": "(Patil et al., 2024)", "paper": {"corpus_id": 268157336, "title": "A Review of Current Trends, Techniques, and Challenges in Large Language Models (LLMs)", "year": 2024, "venue": "Applied Sciences", "authors": [{"name": "Rajvardhan Patil", "authorId": "2289385425"}, {"name": "Venkat Gudivada", "authorId": "117730513"}], "n_citations": 80}, "snippets": ["Encoder-decoder models adopt bidirectional attention for the encoder, unidirectional attention for the decoder, and a cross-attention mechanism between them. Cross-attention in the decoder has access only to the fully processed encoder output and is responsible for connecting input tokens to target tokens. The encoder-decoder-based models are pretrained for seq2seq tasks. They can also be pretrained on conditional generation tasks, where the output is generated in regard to the given input, for example in summarizing, question answering, and translation tasks", "As shown in Figure 4, in the encoder-decoder architecture, fully visible masking is used in the encoder and causal masking is used in the decoder. In a decoder-only model, the input and target are concatenated, and then a causal mask is used throughout. A decoder-only model with a prefix allows fully visible masking over part of the input token (prefix), followed by causal masking on the rest of the sequence. In general, autoencoding models learn bidirectional contextualized representation suited for NLU tasks, whereas autoregressive models learn to generate the next token and hence are suited for NLG tasks."], "score": 0.853515625}, {"id": "(Jain et al., 2022)", "paper": {"corpus_id": 258461112, "title": "ContraCLM: Contrastive Learning For Causal Language Model", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Nihal Jain", "authorId": "2146677401"}, {"name": "Dejiao Zhang", "authorId": "2358258"}, {"name": "Wasi Uddin Ahmad", "authorId": "38123220"}, {"name": "Zijian Wang", "authorId": "50219006"}, {"name": "Feng Nan", "authorId": "144647318"}, {"name": "Xiaopeng Li", "authorId": "2187045812"}, {"name": "Ming Tan", "authorId": "144745483"}, {"name": "Ramesh Nallapati", "authorId": "1701451"}, {"name": "Baishakhi Ray", "authorId": "31631000"}, {"name": "Parminder Bhatia", "authorId": "50339091"}, {"name": "Xiaofei Ma", "authorId": "47646605"}, {"name": "Bing Xiang", "authorId": "144028698"}], "n_citations": 16}, "snippets": ["Compared to the causal (left-to-right) attention mechanism of the decoder-only models, the bidirectional attention mechanism in both encoder-only and encoder-decoder models allows for better leverage of the context of the sequence and hence leads to better representations."], "score": 0.55224609375}], "table": null}], "cost": 0.613077}}
