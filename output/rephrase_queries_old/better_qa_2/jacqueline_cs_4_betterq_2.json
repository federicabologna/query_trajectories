{"clarifying_information": [{"clarifying_question1": "Are you most interested in the specific techniques for measuring data difficulty when designing a curriculum (such as cross-entropy, model uncertainty, or learning percentage) for language model training?", "clarifying_answer1": {"clarifying_answer": "Yes, I am most interested in the concrete techniques for measuring data difficulty\u2014such as cross-entropy, model uncertainty, and learning percentage\u2014as they relate to designing effective curricula for language model training."}}, {"clarifying_question2": "Would you like to focus on recent advances in data selection and curriculum scheduling strategies for large language models, such as skill acquisition frameworks, domain reweighting, or group-level selection?", "clarifying_answer2": {"clarifying_answer": "Yes, I am interested in focusing on recent advances\u2014specifically in techniques such as skill acquisition frameworks, domain reweighting (e.g., DoReMi), and group-level or multi-granularity data selection strategies for large language models. Please emphasize innovations from the past 2-3 years and their empirical impacts on efficiency and model performance."}}, {"clarifying_question3": "Are you primarily seeking practical implementation guidelines and challenges for applying curriculum learning in large-scale LM pretraining, including computational overhead, redundancy management, and balancing diversity with difficulty?", "clarifying_answer3": {"clarifying_answer": "Yes, I am primarily interested in practical implementation guidelines and the main challenges (such as computational overhead, redundancy management, and balancing diversity vs. difficulty) for deploying curriculum learning in large-scale language model pretraining."}}], "better_queries_2": {"reformulated1": "What are the most effective and recent techniques for measuring data difficulty\u2014such as cross-entropy, model uncertainty, and learning percentage\u2014for constructing curricula in large language model training?", "reformulated2": "What are the latest (past 2-3 years) advances in curriculum-based data selection strategies for large language models, including skill acquisition frameworks, domain reweighting methods like DoReMi, and group-level or multi-granularity data selection, and what are their empirical impacts on efficiency and model performance?", "reformulated3": "What are the practical implementation guidelines and primary challenges, such as computational overhead, redundancy management, and balancing data diversity with difficulty ranking, for applying curriculum learning approaches during large-scale language model pretraining?"}}
