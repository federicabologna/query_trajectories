{"clarifying_information": [{"clarifying_question1": "Are you interested in the performance differences between 4-bit quantization and higher bit precisions (such as 8-bit or 16-bit) specifically in deep learning models?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in the performance differences between 4-bit quantization and higher bit precisions (such as 8-bit or 16-bit) specifically in deep learning models."}}, {"clarifying_question2": "Do you want to compare the effects of 4-bit quantization on model accuracy, inference speed, or energy efficiency relative to higher precision quantization methods?", "clarifying_answer2": {"clarifying_answer": "I want to compare the effects of 4-bit quantization on model accuracy relative to higher precision quantization methods."}}, {"clarifying_question3": "Are you asking about empirical results from specific benchmarks or applications (e.g., NLP, computer vision) when using 4-bit quantization compared to higher precision approaches?", "clarifying_answer3": {"clarifying_answer": "I'm interested in empirical results from specific benchmarks, particularly in NLP tasks, comparing 4-bit quantization with higher precision quantization schemes."}}], "better_query": "What are the empirical differences in model accuracy between 4-bit and higher precision (such as 8-bit or 16-bit) quantization for deep learning models on NLP benchmarks?", "better_answer": {"sections": [{"title": "Introduction/Background", "tldr": "Quantization is a technique to reduce the precision of deep learning model weights from their typical 32-bit floating-point representation to lower bit formats like 4-bit, 8-bit, or 16-bit. This compression technique has become increasingly important as large language models grow in size, offering reduced memory footprint and faster inference at the potential cost of accuracy. (LLM Memory)", "text": "\nModel quantization refers to the process of converting the weights and activations of neural networks from high-precision formats (typically 32-bit floating point) to lower-precision representations. This technique has gained significant traction as deep learning models, particularly in NLP, have grown to billions of parameters, creating challenges for deployment in resource-constrained environments. The primary motivation behind quantization is to reduce model size, decrease memory bandwidth requirements, and accelerate inference speed while maintaining acceptable performance. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe standard precision for training deep learning models is 32-bit floating point (FP32), but this high precision is often unnecessary during inference. Research has shown that models can operate effectively with dramatically reduced precision. Common quantization levels include 16-bit (FP16 or INT16), 8-bit (INT8), and more recently, 4-bit (INT4) or even lower for specific applications. Each reduction in bit-width approximately halves the memory footprint and can significantly improve computational efficiency. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe tradeoff between quantization benefits and performance degradation is at the heart of empirical research in this area. As bit precision decreases, the quantization error increases, potentially affecting model accuracy. This is particularly relevant when comparing 4-bit quantization against higher precision alternatives like 8-bit and 16-bit, especially for complex NLP tasks that require nuanced language understanding. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nRecent advancements in quantization techniques have pushed the boundaries of low-bit inference, with innovations in post-training quantization, quantization-aware training, and mixed-precision approaches that strategically assign different bit-widths to different parts of a model. These developments have made even 4-bit quantization viable for certain applications, though its performance relative to higher-precision alternatives varies across model architectures and tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Accuracy Comparison: 4-bit vs. 8-bit/16-bit Quantization", "tldr": "4-bit quantization typically causes more noticeable accuracy drops compared to 8-bit and 16-bit alternatives, with performance degradation becoming more pronounced in deeper networks. While 8-bit quantization often maintains performance close to full precision models (sometimes even improving it), 4-bit represents a critical threshold where significant trade-offs between model compression and accuracy begin to emerge. (16 sources)", "text": "\nQuantitative comparisons between different precision levels reveal consistent patterns across model architectures and tasks. For 8-bit quantization, most studies report minimal accuracy loss compared to full-precision counterparts, with some even showing slight improvements. Post-training quantization using 8-bit precision can achieve accuracy close to single-precision models across various DNN architectures <Paper corpusId=\"207769430\" paperTitle=\"(Guan et al., 2019)\" isShortName></Paper>. In some cases, 8-bit quantized models outperform their full-precision counterparts, demonstrating a regularization effect that helps prevent overfitting <Paper corpusId=\"250048704\" paperTitle=\"(Askarihemmat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>.\n\nWhen moving to 4-bit quantization, however, the impact on model accuracy becomes more significant. Post-training quantization at 4-bit usually incurs noticeable accuracy drops <Paper corpusId=\"207769430\" paperTitle=\"(Guan et al., 2019)\" isShortName></Paper>. The severity of this degradation varies based on model architecture and task complexity. For instance, in CNN architectures like VGG-16, accuracy can drop dramatically from 93.3% when trained in FP32 precision to approximately 20% with 4-bit quantization <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This degradation is generally more pronounced in deeper networks, suggesting that model complexity correlates with quantization sensitivity <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>.\n\nSeveral studies identify 4-bit precision as a critical threshold where performance begins to substantially deteriorate. For example, ImageNet experiments show reasonable results until 6-bits, but accuracy drops drastically at 4-bits <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. Similarly, when testing large language models (LLMs) on various benchmarks, models maintain strong performance at 4-bit precision but show significant degradation when quantized to 3 bits or lower <Paper corpusId=\"268032411\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper>. This \"3-bit threshold\" appears consistently across research, with accuracy degradation transitioning from linear to exponential collapse when precision falls below this level <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nFor many LLMs, 4-bit quantization represents a reasonable compromise, often maintaining emergent abilities while offering substantial memory savings. Research on various LLM benchmarks shows that 4-bit and 8-bit quantization results are very similar to original 16-bit floating-point performance, while 2-bit quantization leads to near-random performance levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This pattern is consistently observed across reasoning and instruction-following tasks, confirming 4-bit as a viable minimal precision for maintaining LLM capabilities <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>.\n\nInterestingly, quantization effects vary significantly by layer type and position within the network. Convolutional layers appear particularly sensitive to aggressive quantization, with 2-bit precision often leading to significant accuracy loss, while increasing precision to 4-bit or 8-bit recovers accuracy <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>. The first and last layers of networks are especially sensitive to quantization, with studies showing that keeping these layers at higher precision (8-bit) while quantizing the rest to 4-bit can improve accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>.\n\nAdvanced quantization techniques can significantly mitigate accuracy losses even at 4-bit precision. Quantized distillation outperforms other methods across various bit widths, achieving accuracy within 0.2% of the teacher model at 8 bits and showing relatively minor accuracy loss at 4-bit quantization <Paper corpusId=\"3323727\" paperTitle=\"(Polino et al., 2018)\" isShortName></Paper>. Similarly, FAQ (Frequency-Aware Quantization) trained 4-bit networks outperform comparable quantization methods and can match or exceed pretrained FP32 network accuracy, although they require significantly longer fine-tuning\u2014approximately 110 epochs\u2014compared to 8-bit networks <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>.\n\nFor certain applications and architectures, there may be no accuracy advantage to increasing precision from 4-bit to 8-bit, especially when using advanced quantization techniques <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>. However, this is not universally true across all model types and tasks. The impact of quantization also depends on activation precision, with research showing that keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to 4-bit can maintain performance within 1% of full-precision models <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>.\n\nRecent advances in 4-bit quantization for large language models show particularly promising results. Techniques like APTQ (Adaptive Precision Task Quantization) can maintain performance comparable to full-precision models even at average bit rates as low as 3.5, outperforming other post-training quantization approaches at reduced precision <Paper corpusId=\"267897495\" paperTitle=\"(Guan et al., 2024)\" isShortName></Paper>. Similarly, specialized number formats like NF4 with double quantization can fully recover 16-bit performance in LoRA fine-tuning scenarios <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\nOverall, while 8-bit quantization typically maintains performance very close to full precision across most model architectures and tasks, 4-bit quantization represents a critical threshold where meaningful accuracy trade-offs begin to emerge. The viability of 4-bit quantization depends heavily on model size, task complexity, and the specific quantization technique employed.", "citations": [{"id": "(Guan et al., 2019)", "paper": {"corpus_id": 207769430, "title": "Post-Training 4-bit Quantization on Embedding Tables", "year": 2019, "venue": "arXiv.org", "authors": [{"name": "Hui Guan", "authorId": "2055337890"}, {"name": "Andrey Malevich", "authorId": "143682293"}, {"name": "Jiyan Yang", "authorId": "2791531"}, {"name": "Jongsoo Park", "authorId": "1686843"}, {"name": "Hector Yuen", "authorId": "30891915"}], "n_citations": 34}, "snippets": ["Post-training quantization using 8-bit precision can achieve accuracy close to that of single-precision models in a wide variety of DNN architectures [19,14]. Post-training quantization using lower bit width (e.g. 4-bit), however, usually incurs significant accuracy drop [6]."], "score": 0.88623046875}, {"id": "(Askarihemmat et al., 2022)", "paper": {"corpus_id": 250048704, "title": "QReg: On Regularization Effects of Quantization", "year": 2022, "venue": "arXiv.org", "authors": [{"name": "Mohammadhossein Askarihemmat", "authorId": "1610857529"}, {"name": "Reyhane Askari Hemmat", "authorId": "7872299"}, {"name": "Alexander Hoffman", "authorId": "2062267546"}, {"name": "I. Lazarevich", "authorId": "4481888"}, {"name": "Ehsan Saboori", "authorId": "1682819"}, {"name": "Olivier Mastropietro", "authorId": "3422889"}, {"name": "Y. Savaria", "authorId": "1756949"}, {"name": "J. David", "authorId": "145719986"}], "n_citations": 5}, "snippets": ["Figure 1 illustrates the validation loss of 8-bit, 4-bit quantized and full precision for the YOLOv5n model applied to the VOC dataset. Due to regularization effect of quantization, the quantized models overfit much less compared to full precision model."], "score": 0.9267578125}, {"id": "(Alistarh et al., 2016)", "paper": {"corpus_id": 1193239, "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "year": 2016, "venue": "", "authors": [{"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "Demjan Grubic", "authorId": "29916095"}, {"name": "Jerry Li", "authorId": "2800851"}, {"name": "Ryota Tomioka", "authorId": "2870603"}, {"name": "M. Vojnovi\u0107", "authorId": "1782150"}], "n_citations": 422}, "snippets": ["On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]", "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version", "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%", "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."], "score": 0.80615234375}, {"id": "(Charan et al., 2020)", "paper": {"corpus_id": 218788236, "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution", "year": 2020, "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": [{"name": "Gouranga Charan", "authorId": "91781917"}, {"name": "Abinash Mohanty", "authorId": "2942785"}, {"name": "Xiaocong Du", "authorId": "3457252"}, {"name": "Gokul Krishnan", "authorId": "144828251"}, {"name": "R. Joshi", "authorId": "145553390"}, {"name": "Yu Cao", "authorId": "1965873861"}], "n_citations": 30}, "snippets": ["We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."], "score": 0.78466796875}, {"id": "(Kim et al., 2020)", "paper": {"corpus_id": 218862856, "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Jangho Kim", "authorId": "49476045"}, {"name": "Kiyoon Yoo", "authorId": "1713608836"}, {"name": "Nojun Kwak", "authorId": "3160425"}], "n_citations": 7}, "snippets": ["On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated", "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."], "score": 0.86865234375}, {"id": "(Jin et al., 2024)", "paper": {"corpus_id": 268032411, "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Renren Jin", "authorId": "2184143149"}, {"name": "Jiangcun Du", "authorId": "2287758280"}, {"name": "Wuwei Huang", "authorId": "1588102980"}, {"name": "Wei Liu", "authorId": "2257333016"}, {"name": "Jian Luan", "authorId": "2257013742"}, {"name": "Bin Wang", "authorId": "2257388949"}, {"name": "Deyi Xiong", "authorId": "2263617513"}], "n_citations": 37}, "snippets": ["Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower."], "score": 0.900390625}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."], "score": 0.88916015625}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 259937594, "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Peiyu Liu", "authorId": "2108129670"}, {"name": "Zikang Liu", "authorId": "2119618242"}, {"name": "Ze-Feng Gao", "authorId": "9136116"}, {"name": "Dawei Gao", "authorId": "2162036220"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Bolin Ding", "authorId": "1696332"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 35}, "snippets": ["Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."], "score": 0.94970703125}, {"id": "(Berti et al., 2025)", "paper": {"corpus_id": 276903421, "title": "Emergent Abilities in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leonardo Berti", "authorId": "2229196235"}, {"name": "Flavio Giorgi", "authorId": "2325903265"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 6}, "snippets": ["Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."], "score": 0.900390625}, {"id": "(Garg et al., 2021)", "paper": {"corpus_id": 231918483, "title": "Confounding Tradeoffs for Neural Network Quantization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Sahaj Garg", "authorId": "9273400"}, {"name": "Anirudh Jain", "authorId": "153408223"}, {"name": "Joe Lou", "authorId": "134183581"}, {"name": "Mitchell Nahmias", "authorId": "2060968473"}], "n_citations": 18}, "snippets": ["Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."], "score": 0.78076171875}, {"id": "(Polino et al., 2018)", "paper": {"corpus_id": 3323727, "title": "Model compression via distillation and quantization", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "A. Polino", "authorId": "36060478"}, {"name": "Razvan Pascanu", "authorId": "1996134"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 732}, "snippets": ["Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss."], "score": 0.880859375}, {"id": "(McKinstry et al., 2018)", "paper": {"corpus_id": 52197199, "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "J. McKinstry", "authorId": "46571359"}, {"name": "S. K. Esser", "authorId": "2357931"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "John V. Arthur", "authorId": "2248110488"}, {"name": "Izzet B. Yildiz", "authorId": "3121907"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 94}, "snippets": ["FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."], "score": 0.87548828125}, {"id": "(Esser et al., 2019)", "paper": {"corpus_id": 67788003, "title": "Learned Step Size Quantization", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "S. K. Esser", "authorId": "2357931"}, {"name": "J. McKinstry", "authorId": "46571359"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 810}, "snippets": ["In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest."], "score": 0.783203125}, {"id": "(Lam et al., 2020)", "paper": {"corpus_id": 211677681, "title": "Quantized Neural Network Inference with Precision Batching", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Maximilian Lam", "authorId": "2347284"}, {"name": "Zachary Yedidia", "authorId": "1515569180"}, {"name": "Colby R. Banbury", "authorId": "103876904"}, {"name": "V. Reddi", "authorId": "1805668"}], "n_citations": 1}, "snippets": ["For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."], "score": 0.95068359375}, {"id": "(Guan et al., 2024)", "paper": {"corpus_id": 267897495, "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models", "year": 2024, "venue": "Design Automation Conference", "authors": [{"name": "Ziyi Guan", "authorId": "2120170158"}, {"name": "Hantao Huang", "authorId": "2301516332"}, {"name": "Yupeng Su", "authorId": "2286850679"}, {"name": "Hong Huang", "authorId": "2351687700"}, {"name": "Ngai Wong", "authorId": "2287187433"}, {"name": "Hao Yu", "authorId": "2288350991"}], "n_citations": 16}, "snippets": ["Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B", "The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy."], "score": 0.7685546875}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision."], "score": 0.79931640625}], "table": null}, {"title": "Performance Thresholds and Critical Bit Levels", "tldr": "Research consistently identifies a critical threshold at around 3-4 bits, below which model performance deteriorates dramatically across various architectures and tasks. While quantization to 4 bits generally maintains reasonable accuracy with modest degradation, reducing to 2-3 bits often results in catastrophic performance collapse approaching random chance levels. (12 sources)", "text": "\nEmpirical evidence across multiple studies demonstrates the existence of a critical bit-width threshold where model performance transitions from gradual to severe degradation. This threshold consistently emerges around the 3-4 bit level for both weights and activations. For instance, studies on large language models (LLMs) reveal that performance remains relatively stable down to 4-bit precision but drops precipitously when quantized to 3 bits or lower <Paper corpusId=\"268032411\" paperTitle=\"(Jin et al., 2024)\" isShortName></Paper> <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This pattern is described as a transition from \"linear to exponential collapse\" in accuracy when crossing this critical threshold <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>.\n\nThe threshold effect is observable across diverse model architectures and tasks. For image classification on ImageNet, models maintain reasonable performance down to 6 bits, but accuracy \"drastically drops at 4-bits\" <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. Similarly, experiments with CIFAR-10 show that while 4-bit QSGD can converge to the same accuracy as full precision models, 2-bit quantization drops accuracy by approximately 1.22% <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>.\n\nThe severity of performance degradation at low bit-widths increases with network depth and complexity. For deep networks like VGG-16, quantizing to 4 bits can cause accuracy to plummet from 93.3% when trained in FP32 to approximately 20% <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This phenomenon suggests that more complex models with greater parameter interdependencies are particularly sensitive to extreme quantization.\n\nFor LLMs, the threshold effect is particularly evident across reasoning and instruction-following benchmarks. Comparative testing across multiple datasets shows that 4-bit and 8-bit quantization produces results very similar to original 16-bit floating-point performance, while 2-bit quantization leads to near-random performance levels <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This pattern holds consistently across diverse tasks including in-context learning, chain-of-thought reasoning, and instruction following.\n\nRecent research on Mixture-of-Experts (MoE) models demonstrates that architectural choices can influence quantization robustness. MoE models can maintain accuracy within -0.3 BLEU points down to 3-bit precision, while comparable dense models preserve accuracy only down to 4 bits and experience catastrophic degradation at lower precisions <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This suggests that certain architectures may extend the viability threshold to slightly lower bit-widths.\n\nThe activation precision plays a crucial role in determining this threshold. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower bits can significantly mitigate accuracy loss. For instance, models with 4-bit weights but higher precision activations can maintain performance within 1% of full-precision models across tasks ranging from MNIST classification to language modeling <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper> <Paper corpusId=\"8257350\" paperTitle=\"(Xu et al., 2018)\" isShortName></Paper>.\n\nThe threshold effect is also evident in specific computational components. When constraining input precision to 4 bits but limiting accumulator bit-width to the same level, accuracy drops to no better than random chance. However, increasing accumulator bit-width to 8 bits allows the model to maintain reasonable performance, with only marginal improvements from further increases to 12 or 16 bits <Paper corpusId=\"263671632\" paperTitle=\"(Schiemer et al., 2023)\" isShortName></Paper>.\n\nNewer models may exhibit different quantization sensitivity patterns due to their training techniques. Advanced pre-training approaches that reduce parameter redundancy, such as those employed in Qwen3, can make models more vulnerable to quantization-induced information loss, especially at 3 bits or lower <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>.\n\nThe binarization threshold (reducing to 1-bit precision) represents an even more dramatic cliff. While performance drops gradually from full precision to ternarization (3 values), there is a \"consistent sharp drop by binarization\" across various GLUE benchmark tasks with both 8-bit and 4-bit activation quantization <Paper corpusId=\"229923538\" paperTitle=\"(Bai et al., 2020)\" isShortName></Paper>. This suggests that the threshold effect becomes more pronounced as bit precision approaches its theoretical minimum.\n\nIn summary, empirical evidence consistently identifies 3-4 bits as a critical precision threshold across diverse neural network architectures and tasks. While 4-bit quantization generally preserves most model capabilities with modest accuracy degradation, reducing to 2-3 bits typically triggers catastrophic performance collapse, with results approaching random chance levels.", "citations": [{"id": "(Jin et al., 2024)", "paper": {"corpus_id": 268032411, "title": "A Comprehensive Evaluation of Quantization Strategies for Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Renren Jin", "authorId": "2184143149"}, {"name": "Jiangcun Du", "authorId": "2287758280"}, {"name": "Wuwei Huang", "authorId": "1588102980"}, {"name": "Wei Liu", "authorId": "2257333016"}, {"name": "Jian Luan", "authorId": "2257013742"}, {"name": "Bin Wang", "authorId": "2257388949"}, {"name": "Deyi Xiong", "authorId": "2263617513"}], "n_citations": 37}, "snippets": ["Experimental findings indicate that while 4-bit quantization maintains performance close to non-quantized counterparts, a notable performance discrepancy emerges as quantization decreases to 3 bits or lower."], "score": 0.900390625}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."], "score": 0.88916015625}, {"id": "(Kim et al., 2020)", "paper": {"corpus_id": 218862856, "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Jangho Kim", "authorId": "49476045"}, {"name": "Kiyoon Yoo", "authorId": "1713608836"}, {"name": "Nojun Kwak", "authorId": "3160425"}], "n_citations": 7}, "snippets": ["On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated", "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."], "score": 0.86865234375}, {"id": "(Alistarh et al., 2016)", "paper": {"corpus_id": 1193239, "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "year": 2016, "venue": "", "authors": [{"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "Demjan Grubic", "authorId": "29916095"}, {"name": "Jerry Li", "authorId": "2800851"}, {"name": "Ryota Tomioka", "authorId": "2870603"}, {"name": "M. Vojnovi\u0107", "authorId": "1782150"}], "n_citations": 422}, "snippets": ["On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]", "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version", "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%", "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."], "score": 0.80615234375}, {"id": "(Charan et al., 2020)", "paper": {"corpus_id": 218788236, "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution", "year": 2020, "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": [{"name": "Gouranga Charan", "authorId": "91781917"}, {"name": "Abinash Mohanty", "authorId": "2942785"}, {"name": "Xiaocong Du", "authorId": "3457252"}, {"name": "Gokul Krishnan", "authorId": "144828251"}, {"name": "R. Joshi", "authorId": "145553390"}, {"name": "Yu Cao", "authorId": "1965873861"}], "n_citations": 30}, "snippets": ["We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."], "score": 0.78466796875}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 259937594, "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Peiyu Liu", "authorId": "2108129670"}, {"name": "Zikang Liu", "authorId": "2119618242"}, {"name": "Ze-Feng Gao", "authorId": "9136116"}, {"name": "Dawei Gao", "authorId": "2162036220"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Bolin Ding", "authorId": "1696332"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 35}, "snippets": ["Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."], "score": 0.94970703125}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 263620300, "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Young Jin Kim", "authorId": "2152658577"}, {"name": "Raffy Fahim", "authorId": "2191522437"}, {"name": "H. Awadalla", "authorId": "3032929"}], "n_citations": 20}, "snippets": ["We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."], "score": 0.74951171875}, {"id": "(Lam et al., 2020)", "paper": {"corpus_id": 211677681, "title": "Quantized Neural Network Inference with Precision Batching", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Maximilian Lam", "authorId": "2347284"}, {"name": "Zachary Yedidia", "authorId": "1515569180"}, {"name": "Colby R. Banbury", "authorId": "103876904"}, {"name": "V. Reddi", "authorId": "1805668"}], "n_citations": 1}, "snippets": ["For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."], "score": 0.95068359375}, {"id": "(Xu et al., 2018)", "paper": {"corpus_id": 8257350, "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chen Xu", "authorId": "2153078936"}, {"name": "Jianqiang Yao", "authorId": "32320704"}, {"name": "Zhouchen Lin", "authorId": "33383055"}, {"name": "Wenwu Ou", "authorId": "10336865"}, {"name": "Yuanbin Cao", "authorId": "35667580"}, {"name": "Zhirong Wang", "authorId": "2108201369"}, {"name": "H. Zha", "authorId": "1687248"}], "n_citations": 116}, "snippets": ["Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance."], "score": 0.0}, {"id": "(Schiemer et al., 2023)", "paper": {"corpus_id": 263671632, "title": "Hadamard Domain Training with Integers for Class Incremental Quantized Learning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Martin Schiemer", "authorId": "1644333695"}, {"name": "Clemens J. S. Schaefer", "authorId": "1592769267"}, {"name": "Jayden Parker Vap", "authorId": "2163400188"}, {"name": "M. Horeni", "authorId": "51184096"}, {"name": "Yu Emma Wang", "authorId": "2256185768"}, {"name": "Juan Ye", "authorId": "2255995570"}, {"name": "Siddharth Joshi", "authorId": "2254256505"}], "n_citations": 2}, "snippets": ["For all techniques, we observe that quantizing inputs to 3-bits causes a severe drop in accuracy (e.g., -19.7% on CIFAR100 with BiC compared to 4-bits). Input precision higher than 4-bits shows no additional benefit. The ablation study demonstrates how much bit-growth occurs during accumulation. We fix the input bit-width at four and observed accuracy over multiple settings of accumulator-bit-width. When constrained to 4-bits, HDQT fails, with accuracy no better than random. Increasing accumulator bit-width over our default of 8-bits (12-and 16-bits) leads to only \u223c3% improvement in the final accuracy, suggesting a favorable trade-off in precision and efficiency at 8-bits."], "score": 0.9169921875}, {"id": "(Zheng et al., 2025)", "paper": {"corpus_id": 278326880, "title": "An Empirical Study of Qwen3 Quantization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xingyu Zheng", "authorId": "2283439767"}, {"name": "Yuye Li", "authorId": "2359206148"}, {"name": "Haoran Chu", "authorId": "2359612976"}, {"name": "Yue Feng", "authorId": "2359148500"}, {"name": "Xudong Ma", "authorId": "2190791504"}, {"name": "Jie Luo", "authorId": "2283307736"}, {"name": "Jinyang Guo", "authorId": "2261414086"}, {"name": "Haotong Qin", "authorId": "2329344125"}, {"name": "Michele Magno", "authorId": "2283140094"}, {"name": "Xianglong Liu", "authorId": "2278053544"}], "n_citations": 6}, "snippets": ["Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."], "score": 0.75634765625}, {"id": "(Bai et al., 2020)", "paper": {"corpus_id": 229923538, "title": "BinaryBERT: Pushing the Limit of BERT Quantization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Haoli Bai", "authorId": "9583912"}, {"name": "Wei Zhang", "authorId": "2155468228"}, {"name": "Lu Hou", "authorId": "48557122"}, {"name": "Lifeng Shang", "authorId": "50812138"}, {"name": "Jing Jin", "authorId": "2115757711"}, {"name": "Xin Jiang", "authorId": "145820291"}, {"name": "Qun Liu", "authorId": "30738758"}, {"name": "Michael R. Lyu", "authorId": "1785083"}, {"name": "Irwin King", "authorId": "145310663"}], "n_citations": 227}, "snippets": ["Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1."], "score": 0.83349609375}], "table": null}, {"title": "Model Size and Architecture Effects", "tldr": "Model size and architecture significantly influence quantization impacts, with larger models (70B+ parameters) generally maintaining better performance at 4-bit precision compared to smaller counterparts. Network depth, layer type, and specific architectural choices like residual connections and mixture-of-experts designs also substantially affect quantization robustness. (10 sources)", "text": "\nThe relationship between model size and quantization impact reveals a clear pattern: larger language models tend to be more resilient to aggressive quantization. For 70B parameter models, 4-bit quantization typically maintains good performance, while smaller LLMs experience significant accuracy drops at the same bit precision <Paper corpusId=\"272694046\" paperTitle=\"(Lee et al., 2024)\" isShortName></Paper>. This size-dependent resilience suggests that parameter redundancy in larger models provides a buffer against quantization-induced information loss.\n\nNetwork depth emerges as a critical factor influencing quantization sensitivity. As network depth increases, the performance degradation from quantization becomes more severe. For instance, in VGG-16, accuracy plummets from 93.3% when trained in FP32 precision to approximately 20% with 4-bit quantization <Paper corpusId=\"218788236\" paperTitle=\"(Charan et al., 2020)\" isShortName></Paper>. This pattern appears consistent across architectures, with deeper networks showing heightened sensitivity to precision reduction.\n\nThe type of layer being quantized significantly affects overall model performance. Convolutional layers prove particularly sensitive to aggressive quantization, with 2-bit precision often leading to accuracy loss. However, increasing precision to 4-bit or 8-bit for these layers can recover accuracy <Paper corpusId=\"1193239\" paperTitle=\"(Alistarh et al., 2016)\" isShortName></Paper>. Moreover, the first and last layers of networks demonstrate special sensitivity to quantization, with research showing that keeping these layers at higher precision (8-bit) while quantizing the rest to 4-bit can improve accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>.\n\nVision transformers (ViT) exhibit distinct quantization characteristics based on their size. Smaller models like ViT-T and DeiT-Ti show dramatic performance drops at 4-bit precision, achieving only 17.6% and 47.9% top-1 accuracy respectively. However, these same architectures maintain adequate performance with 8/6-bit quantization, reaching up to 74.8% and 72.0% top-1 accuracy while still providing meaningful model compression <Paper corpusId=\"261531260\" paperTitle=\"(Papa et al., 2023)\" isShortName></Paper>.\n\nAdvanced architecture designs can significantly impact quantization robustness. Mixture-of-Experts (MoE) models demonstrate superior quantization resilience compared to dense counterparts. While MoE models maintain accuracy within -0.3 BLEU points down to 3-bit precision, comparable dense models preserve accuracy only down to 4 bits and experience catastrophic degradation at lower precisions <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This architectural advantage is particularly valuable for extreme quantization scenarios.\n\nNewer model generations may exhibit different quantization sensitivity patterns due to their training techniques. Advanced pre-training approaches that reduce parameter redundancy, such as those employed in Qwen3, can make models more vulnerable to quantization-induced information loss, especially at 3 bits or lower <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. This suggests that quantization robustness may sometimes trade off against other training objectives that optimize parameter efficiency.\n\nActivation precision plays a crucial role alongside weight quantization. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower bits significantly mitigates accuracy loss. For instance, models with 4-bit weights but higher precision activations can maintain performance within 1% of full-precision models <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This pattern holds across diverse tasks from image classification to language modeling, indicating a generalizable principle for quantization strategy design.\n\nFor compact architectures like SqueezeNet, layer-specific quantization impacts vary dramatically. When applying 4-bit fixed-point quantization to individual layers, accuracy drops range from modest 5% reductions (for 'fire9-expand3x3' layer) to severe 47.5% reductions (for 'fire6-expand3x3' layer). Interestingly, 8-bit and 16-bit quantization show no accuracy change regardless of which layer is quantized, highlighting the existence of architecture-specific precision thresholds <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>.\n\nSpecific architectural elements like residual and skip connections deserve special attention during quantization. Research demonstrates that methods achieving high accuracy with low-bit activations (e.g., 3-bit) often succeed partly because they exclude these connections from quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>. This finding underscores the importance of selectively applying quantization based on architectural components rather than uniformly across the model.\n\nDespite architecture-specific variations, post-training quantization consistently achieves significant model compression (7-8\u00d7) with only 1-3% accuracy drops at 4-bit precision across both small and large DNN architectures, regardless of task complexity <Paper corpusId=\"231906389\" paperTitle=\"(Ghamari et al., 2021)\" isShortName></Paper>. This demonstrates that while architecture influences quantization sensitivity, well-designed quantization strategies can effectively compress diverse model architectures with minimal performance impact.", "citations": [{"id": "(Lee et al., 2024)", "paper": {"corpus_id": 272694046, "title": "Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant", "year": 2024, "venue": "", "authors": [{"name": "Jemin Lee", "authorId": "2154961369"}, {"name": "Sihyeong Park", "authorId": "1936868"}, {"name": "Jinse Kwon", "authorId": "3460677"}, {"name": "Jihun Oh", "authorId": "2321710033"}, {"name": "Yongin Kwon", "authorId": "3156595"}], "n_citations": 2}, "snippets": ["\u2022 In smaller LLMs, using 4-bit quantization can lead to significant accuracy drops, especially with GPTQ. However, 70B models usually maintain good performance when quantized to 4 bits."], "score": 0.775390625}, {"id": "(Charan et al., 2020)", "paper": {"corpus_id": 218788236, "title": "Accurate Inference With Inaccurate RRAM Devices: A Joint Algorithm-Design Solution", "year": 2020, "venue": "IEEE Journal on Exploratory Solid-State Computational Devices and Circuits", "authors": [{"name": "Gouranga Charan", "authorId": "91781917"}, {"name": "Abinash Mohanty", "authorId": "2942785"}, {"name": "Xiaocong Du", "authorId": "3457252"}, {"name": "Gokul Krishnan", "authorId": "144828251"}, {"name": "R. Joshi", "authorId": "145553390"}, {"name": "Yu Cao", "authorId": "1965873861"}], "n_citations": 30}, "snippets": ["We observe inference accuracy degradation for both the MNIST and CIFAR-10 data sets when the weights are quantized to 4 bit, as presented in Fig. 2. The inference accuracy loss is more severe as the depth of the network increases, such as VGG-16, where the accuracy drops from 93.3% when trained in FP32 precision to \u223c20% in 4-bit (INT4) quantization."], "score": 0.78466796875}, {"id": "(Alistarh et al., 2016)", "paper": {"corpus_id": 1193239, "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "year": 2016, "venue": "", "authors": [{"name": "Dan Alistarh", "authorId": "3311387"}, {"name": "Demjan Grubic", "authorId": "29916095"}, {"name": "Jerry Li", "authorId": "2800851"}, {"name": "Ryota Tomioka", "authorId": "2870603"}, {"name": "M. Vojnovi\u0107", "authorId": "1782150"}], "n_citations": 422}, "snippets": ["On ImageNet using AlexNet, 4-bit QSGD with 8192 bucket size converges to 59.22% top-1 error, and 81.63% top-5 error. The gap from the 32bit version is 0.57% for top-5, and 0.68% for top-1 [8]", "Similarly, when trained with QSGD 8-bit gradients, ResNet-50 converges to 72.36% top-1 accuracy, and 90.72% top-5 accuracy (not shown). Again, this is slightly better than the full precision version, which converges to 71.99% top-1, and 90.54% top-5, respectively. When trained with 8-bit gradients and 512 bucket size, ResNet-152 converges to virtually the same top-5 accuracy as the full-precision version", "On CIFAR-10, 2-bit QSGD applied to ResNet-110 drops about 1.22% top-1 accuracy points. However, 4-bit QSGD converges to the same accuracy as the original, whereas 8-bit QSGD improves accuracy by 0.33%", "One issue we examined in more detail is which layers are more sensitive to quantization. It appears that quantizing convolutional layers too aggressively (e.g., 2-bit precision) can lead to accuracy loss if trained for the same period of time as the full precision variant. However, increasing precision to 4-bit or 8-bit recovers accuracy."], "score": 0.80615234375}, {"id": "(Garg et al., 2021)", "paper": {"corpus_id": 231918483, "title": "Confounding Tradeoffs for Neural Network Quantization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Sahaj Garg", "authorId": "9273400"}, {"name": "Anirudh Jain", "authorId": "153408223"}, {"name": "Joe Lou", "authorId": "134183581"}, {"name": "Mitchell Nahmias", "authorId": "2060968473"}], "n_citations": 18}, "snippets": ["Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."], "score": 0.78076171875}, {"id": "(Papa et al., 2023)", "paper": {"corpus_id": 261531260, "title": "A Survey on Efficient Vision Transformers: Algorithms, Techniques, and Performance Benchmarking", "year": 2023, "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "authors": [{"name": "Lorenzo Papa", "authorId": "2164141997"}, {"name": "Paolo Russo", "authorId": "2047330818"}, {"name": "Irene Amerini", "authorId": "2281890830"}, {"name": "Luping Zhou", "authorId": "2237944863"}], "n_citations": 44}, "snippets": ["Based on the data presented in the multiple tables, it can be noticed that an extreme quantization, i.e., 4-bit precision, results in poor accuracy. More in detail, the APQ-ViT quantization strategy applied to the ViT-T and DeiT-Ti architectures, which have the smaller model's sizes, produce a Top-1 accuracy equal to 17.6% and 47.9%, respectively. In contrast, the same architectures with higher (8/6) bit-width are able to achieve adequate estimation performances with a Top-1 up to 74.8% and 72.0% for the ViT-T and DeiT-Ti structures respectively, while still maintaining a restricted model size."], "score": 0.74072265625}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 263620300, "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Young Jin Kim", "authorId": "2152658577"}, {"name": "Raffy Fahim", "authorId": "2191522437"}, {"name": "H. Awadalla", "authorId": "3032929"}], "n_citations": 20}, "snippets": ["We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."], "score": 0.74951171875}, {"id": "(Zheng et al., 2025)", "paper": {"corpus_id": 278326880, "title": "An Empirical Study of Qwen3 Quantization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xingyu Zheng", "authorId": "2283439767"}, {"name": "Yuye Li", "authorId": "2359206148"}, {"name": "Haoran Chu", "authorId": "2359612976"}, {"name": "Yue Feng", "authorId": "2359148500"}, {"name": "Xudong Ma", "authorId": "2190791504"}, {"name": "Jie Luo", "authorId": "2283307736"}, {"name": "Jinyang Guo", "authorId": "2261414086"}, {"name": "Haotong Qin", "authorId": "2329344125"}, {"name": "Michele Magno", "authorId": "2283140094"}, {"name": "Xianglong Liu", "authorId": "2278053544"}], "n_citations": 6}, "snippets": ["Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."], "score": 0.75634765625}, {"id": "(Lam et al., 2020)", "paper": {"corpus_id": 211677681, "title": "Quantized Neural Network Inference with Precision Batching", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Maximilian Lam", "authorId": "2347284"}, {"name": "Zachary Yedidia", "authorId": "1515569180"}, {"name": "Colby R. Banbury", "authorId": "103876904"}, {"name": "V. Reddi", "authorId": "1805668"}], "n_citations": 1}, "snippets": ["For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."], "score": 0.95068359375}, {"id": "(Yee et al., 2020)", "paper": {"corpus_id": 225583435, "title": "Face Recognition and Machine Learning at the Edge", "year": 2020, "venue": "IOP Conference Series: Materials Science and Engineering", "authors": [{"name": "Joanne Ling Sin Yee", "authorId": "2004954858"}, {"name": "U. U. Sheikh", "authorId": "2412102"}, {"name": "M. Mokji", "authorId": "1951977"}, {"name": "S. Rahman", "authorId": "2111988471"}], "n_citations": 2}, "snippets": ["For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."], "score": 0.791015625}, {"id": "(Ghamari et al., 2021)", "paper": {"corpus_id": 231906389, "title": "Quantization-Guided Training for Compact TinyML Models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Sedigh Ghamari", "authorId": "103148446"}, {"name": "Koray Ozcan", "authorId": "40408041"}, {"name": "Thu Dinh", "authorId": "40844014"}, {"name": "A. Melnikov", "authorId": "38363587"}, {"name": "Juan Carvajal", "authorId": "2062260709"}, {"name": "Jan Ernst", "authorId": "39497207"}, {"name": "S. Chai", "authorId": "1691331"}], "n_citations": 17}, "snippets": ["We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models", "The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy."], "score": 0.7744140625}], "table": null}, {"title": "Task-Specific Performance Variations", "tldr": "The impact of 4-bit quantization varies significantly across different NLP and machine learning tasks, with complex reasoning and few-shot learning tasks showing greater sensitivity to precision reduction compared to simpler classification tasks. Performance degradation patterns are task-dependent, with some applications maintaining accuracy within 1% of full-precision models at 4-bit, while others experience significant drops requiring 8-bit precision to maintain comparable results. (11 sources)", "text": "\nDifferent NLP and machine learning tasks exhibit varying levels of resilience to quantization, creating distinct performance profiles across bit-width levels. For natural language inference tasks, using full-precision activations allows quantization down to 1-bit with only minor accuracy degradation (from 78% to 76%), whereas quantizing both weights and activations to the same low precision leads to random-guess performance (33%) <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. Interestingly, for some language inference tasks, 8-bit quantized models actually outperform their full-precision counterparts, demonstrating quantization's potential regularization benefits <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper> <Paper corpusId=\"8257350\" paperTitle=\"(Xu et al., 2018)\" isShortName></Paper>.\n\nLanguage modeling tasks show distinct sensitivity patterns, with 4-bit weight quantization causing moderate perplexity increases when activations are also quantized, but maintaining performance within a few points of full precision when activations remain at higher precision <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This pattern holds across various model architectures and datasets, suggesting that preserving activation precision is particularly important for language modeling tasks.\n\nFor BERT models on GLUE benchmark tasks, a consistent pattern emerges regarding extreme quantization thresholds. While performance degrades gradually from full precision to ternarization (3 values), there is a \"consistent sharp drop by binarization\" across various GLUE tasks with both 8-bit and 4-bit activation quantization <Paper corpusId=\"229923538\" paperTitle=\"(Bai et al., 2020)\" isShortName></Paper>. This indicates that extremely low-bit quantization affects different language understanding tasks similarly, despite their varying complexity.\n\nThe impact of quantization on emergent abilities in large language models reveals task-specific patterns. Across multiple benchmarks including MMLU (multiple-choice reasoning), BBH (in-context learning), GSM8K (chain-of-thought reasoning), and general language modeling on WikiText, 4-bit and 8-bit quantization results remain very similar to original 16-bit floating-point performance. However, 2-bit quantization consistently produces near-random performance levels (around 0.25 accuracy in 4-choice classification tasks) <Paper corpusId=\"259937594\" paperTitle=\"(Liu et al., 2023)\" isShortName></Paper>. This suggests that 4-bit represents a viable threshold for preserving complex emergent abilities in language models.\n\nRecent quantization studies on Qwen3 reveal that advanced models using sophisticated pre-training techniques may exhibit greater task-specific sensitivity. While maintaining competitive performance at 4-bit and above, Qwen3 shows more pronounced degradation than previous model generations when quantized to 3-bit or lower, particularly for complex reasoning tasks and few-shot learning scenarios <Paper corpusId=\"278326880\" paperTitle=\"(Zheng et al., 2025)\" isShortName></Paper>. This suggests that models with less parameter redundancy due to advanced training may be more vulnerable to quantization-induced information loss in complex reasoning tasks.\n\nFor machine translation, comparison between Mixture-of-Experts (MoE) and dense models reveals significant task-specific resilience differences. MoE models maintain translation quality within -0.3 BLEU points down to 3-bit precision, while comparable dense models preserve accuracy only down to 4-bit and experience catastrophic degradation (-42.96 BLEU points) at 2-bit <Paper corpusId=\"263620300\" paperTitle=\"(Kim et al., 2023)\" isShortName></Paper>. This indicates that architectural choices significantly influence quantization robustness for specific NLP tasks.\n\nVisual recognition tasks show varying quantization sensitivity based on dataset complexity. For the CIFAR10-DVS dataset, 8-bit quantization causes a modest 1.78% accuracy drop, while 4-bit models show more significant degradation (7.98%). In contrast, for the simpler DVS128 Gesture dataset, the impact is much smaller, with just 0.81% accuracy loss at 4-bit precision <Paper corpusId=\"258990120\" paperTitle=\"(Shymyrbay et al., 2023)\" isShortName></Paper>. This pattern suggests that dataset complexity significantly influences quantization's impact on performance.\n\nInterestingly, some domain generalization tasks actually benefit from moderate quantization. Experiments on PACS and TerraIncognita datasets show that 7-bit precision yields optimal out-of-domain generalization while maintaining in-domain accuracy, with 6-bit and 8-bit also showing improvements <Paper corpusId=\"273228873\" paperTitle=\"(Javed et al., 2024)\" isShortName></Paper> <Paper corpusId=\"49744838\" paperTitle=\"(Beery et al., 2018)\" isShortName></Paper> <Paper corpusId=\"6037691\" paperTitle=\"(Li et al., 2017)\" isShortName></Paper>. This demonstrates that quantization can sometimes enhance generalization capabilities, particularly for tasks requiring robust feature representations.\n\nAcross the spectrum of emergent abilities in LLMs, a consistent pattern emerges: 8-bit and 16-bit quantization preserves most capabilities with minimal degradation, 4-bit represents a critical threshold maintaining most emergent abilities, while 2-bit quantization severely degrades performance to near-random levels <Paper corpusId=\"276903421\" paperTitle=\"(Berti et al., 2025)\" isShortName></Paper>. This pattern holds consistently across reasoning and instruction-following tasks, providing practical guidance for task-specific quantization decisions.", "citations": [{"id": "(Lam et al., 2020)", "paper": {"corpus_id": 211677681, "title": "Quantized Neural Network Inference with Precision Batching", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Maximilian Lam", "authorId": "2347284"}, {"name": "Zachary Yedidia", "authorId": "1515569180"}, {"name": "Colby R. Banbury", "authorId": "103876904"}, {"name": "V. Reddi", "authorId": "1805668"}], "n_citations": 1}, "snippets": ["For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."], "score": 0.95068359375}, {"id": "(Xu et al., 2018)", "paper": {"corpus_id": 8257350, "title": "Alternating Multi-bit Quantization for Recurrent Neural Networks", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "Chen Xu", "authorId": "2153078936"}, {"name": "Jianqiang Yao", "authorId": "32320704"}, {"name": "Zhouchen Lin", "authorId": "33383055"}, {"name": "Wenwu Ou", "authorId": "10336865"}, {"name": "Yuanbin Cao", "authorId": "35667580"}, {"name": "Zhirong Wang", "authorId": "2108201369"}, {"name": "H. Zha", "authorId": "1687248"}], "n_citations": 116}, "snippets": ["Recurrent neural networks have achieved excellent performance in many applications. However, on portable devices with limited resources, the models are often too large to deploy. For applications on the server with large scale concurrent requests, the latency during inference can also be very critical for costly computing resources. In this work, we address these problems by quantizing the network, both weights and activations, into multiple binary codes {-1,+1}. We formulate the quantization as an optimization problem. Under the key observation that once the quantization coefficients are fixed the binary codes can be derived efficiently by binary search tree, alternating minimization is then applied. We test the quantization for two well-known RNNs, i.e., long short term memory (LSTM) and gated recurrent unit (GRU), on the language models. Compared with the full-precision counter part, by 2-bit quantization we can achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with only a reasonable loss in the accuracy. By 3-bit quantization, we can achieve almost no loss in the accuracy or even surpass the original model, with ~10.5x memory saving and ~3x real inference acceleration. Both results beat the exiting quantization works with large margins. We extend our alternating quantization to image classification tasks. In both RNNs and feedforward neural networks, the method also achieves excellent performance."], "score": 0.0}, {"id": "(Bai et al., 2020)", "paper": {"corpus_id": 229923538, "title": "BinaryBERT: Pushing the Limit of BERT Quantization", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Haoli Bai", "authorId": "9583912"}, {"name": "Wei Zhang", "authorId": "2155468228"}, {"name": "Lu Hou", "authorId": "48557122"}, {"name": "Lifeng Shang", "authorId": "50812138"}, {"name": "Jing Jin", "authorId": "2115757711"}, {"name": "Xin Jiang", "authorId": "145820291"}, {"name": "Qun Liu", "authorId": "30738758"}, {"name": "Michael R. Lyu", "authorId": "1785083"}, {"name": "Irwin King", "authorId": "145310663"}], "n_citations": 227}, "snippets": ["Here we provide more empirical results on the sharp drop in performance as a result of binarization. We run multi-bit quantization on the BERT model over representative tasks of the GLUE benchmark, and activations are quantized in both 8bit and 4-bit. We run 10 independent experiments for each task except for MNLI with 3 runs. We follow the same procedure in Section 2.1, and the default experimental setup in Appendix B.2 without data augmentation and splitting. The results are shown in Figures 8 and 9 respectively. It can be found that while the performance drops slowly from full-precision to ternarization, there is a consistent sharp drop by binarization in each tasks and on both 8-bit and 4-bit activation quantization. This is similar to the findings in Figure 1."], "score": 0.83349609375}, {"id": "(Liu et al., 2023)", "paper": {"corpus_id": 259937594, "title": "Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study", "year": 2023, "venue": "International Conference on Language Resources and Evaluation", "authors": [{"name": "Peiyu Liu", "authorId": "2108129670"}, {"name": "Zikang Liu", "authorId": "2119618242"}, {"name": "Ze-Feng Gao", "authorId": "9136116"}, {"name": "Dawei Gao", "authorId": "2162036220"}, {"name": "Wayne Xin Zhao", "authorId": "2542603"}, {"name": "Yaliang Li", "authorId": "2110479359"}, {"name": "Bolin Ding", "authorId": "1696332"}, {"name": "Ji-rong Wen", "authorId": "153693432"}], "n_citations": 35}, "snippets": ["Overall, the three kinds of emergent abilities seem to be seldom affected with 4-bit quantization. Table 1 presents the test results of the models using 2-bit, 4-bit, 8-bit and 16-bit precision across multiple datasets, including MMLU, BBH for ICL, GSM8K for CoT, AutoEval for IF and WikiText for general language modeling ability. As we can see, the results obtained using 4-bit and 8-bit quantization are very similar to the orig-inal performance (i.e., 16-bit floating-point number). However, a significant decline is observed when employing 2-bit quantization, with results approaching near-random levels, e.g., around 0.25 in 4-choice classification tasks for MMLU and BBH and 0.0 for GSM8K. It indicates that 4-bit quantization can effectively retain emergent abilities on these test datasets."], "score": 0.94970703125}, {"id": "(Zheng et al., 2025)", "paper": {"corpus_id": 278326880, "title": "An Empirical Study of Qwen3 Quantization", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Xingyu Zheng", "authorId": "2283439767"}, {"name": "Yuye Li", "authorId": "2359206148"}, {"name": "Haoran Chu", "authorId": "2359612976"}, {"name": "Yue Feng", "authorId": "2359148500"}, {"name": "Xudong Ma", "authorId": "2190791504"}, {"name": "Jie Luo", "authorId": "2283307736"}, {"name": "Jinyang Guo", "authorId": "2261414086"}, {"name": "Haotong Qin", "authorId": "2329344125"}, {"name": "Michele Magno", "authorId": "2283140094"}, {"name": "Xianglong Liu", "authorId": "2278053544"}], "n_citations": 6}, "snippets": ["Our experimental results reveal that while Qwen3 maintains competitive performance at higher bit-widths (4-bit and above), it exhibits more pronounced performance degradation compared to previous model generations when quantized to 3-bit or below. This observation aligns with our hypothesis that advanced pre-training techniques, which Qwen3 extensively employs, tend to produce models with less parameter redundancy, consequently making them more sensitive to quantizationinduced information loss. Notably, the performance drop becomes particularly significant in complex reasoning tasks and few-shot learning scenarios."], "score": 0.75634765625}, {"id": "(Kim et al., 2023)", "paper": {"corpus_id": 263620300, "title": "Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Young Jin Kim", "authorId": "2152658577"}, {"name": "Raffy Fahim", "authorId": "2191522437"}, {"name": "H. Awadalla", "authorId": "3032929"}], "n_citations": 20}, "snippets": ["We compare robustness against low-bit quantization between MoE and dense models using the post-training quantization without any QAT. For the dense model, quantization with different bits is applied to the even numbered FFN layers. Appendix D shows this is the best layer selection for the dense model. We use two different datasets to verify the proposed quantization method works in different model settings. Figure 3 presents the experiment with the model trained with 20 direction multilingual translation dataset. It shows the average BLEU scores with different quantization precision for both MoE and dense models. The MoE model can maintain accuracy within -0.3 down to 3-bit and -1.82 for 2-bit. On the other hand, the dense model can preserve the accuracy only down to 4-bit, but starts to lose significant accuracy more than 2 BLEU scores when it goes down to 3-bits. In case of 2-bits, dense model loses most of capability by -42.96 BLEU scores."], "score": 0.74951171875}, {"id": "(Shymyrbay et al., 2023)", "paper": {"corpus_id": 258990120, "title": "Low Precision Quantization-aware Training in Spiking Neural Networks with Differentiable Quantization Function", "year": 2023, "venue": "IEEE International Joint Conference on Neural Network", "authors": [{"name": "Ayan Shymyrbay", "authorId": "81112415"}, {"name": "M. Fouda", "authorId": "11702681"}, {"name": "A. Eltawil", "authorId": "2868015"}], "n_citations": 7}, "snippets": ["The accuracy drop (1.78 %) for the 8bit model is not dramatic, performance degradation for lowerbit quantized models is more evident with accuracy drops of 7.98%, 7.58%, and 8.03% for 4, 2, and 1-bit precisions respectively. The possible reason for such degradation can be the complex nature of the dataset.\n\nThe accuracy drop for the DVS128 Gesture dataset is much lower than for the CIFAR10-DVS dataset. The 8-bit quantized model achieves the lowest accuracy degradation (0.27%). For the 4-bit quantization, the model experiences a 0.81% drop in accuracy compared to the full precision model. For the 2-bit and 1-bit quantization, the accuracy drop is slightly higher than 1% (1.12% and 1.18%, respectively)."], "score": 0.77685546875}, {"id": "(Javed et al., 2024)", "paper": {"corpus_id": 273228873, "title": "QT-DoG: Quantization-aware Training for Domain Generalization", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Saqib Javed", "authorId": "2211424900"}, {"name": "Hieu Le", "authorId": "2310800861"}, {"name": "Mathieu Salzmann", "authorId": "2243289145"}], "n_citations": 2}, "snippets": ["We perform experiments with four different bit levels and present an analysis in Figure 4 on the PACS (Li et al., 2017) and TerraIncognita (Beery et al., 2018) datasets. We report the test1 domain accuracy averaged across all domains. For both datasets, 7-bit precision was found to be the optimal bit precision to have the best outof-domain generalization while maintaining in-domain accuracy. Nonetheless, 8 bits and 6 bits also show improvements, albeit smaller than with 7-bit quantization. These results evidence that, even with a 6 times smaller model, quantization still yields better out-of-domain performance without sacrificing the in-domain accuracy."], "score": 0.900390625}, {"id": "(Beery et al., 2018)", "paper": {"corpus_id": 49744838, "title": "Recognition in Terra Incognita", "year": 2018, "venue": "European Conference on Computer Vision", "authors": [{"name": "Sara Meghan Beery", "authorId": "2134791809"}, {"name": "Grant Van Horn", "authorId": "2996914"}, {"name": "P. Perona", "authorId": "1690922"}], "n_citations": 853}, "snippets": ["It is desirable for detection and classification algorithms to generalize to unfamiliar environments, but suitable benchmarks for quantitatively studying this phenomenon are not yet available. We present a dataset designed to measure recognition generalization to novel environments. The images in our dataset are harvested from twenty camera traps deployed to monitor animal populations. Camera traps are fixed at one location, hence the background changes little across images; capture is triggered automatically, hence there is no human bias. The challenge is learning recognition in a handful of locations, and generalizing animal detection and classification to new locations where no training data is available. In our experiments state-of-the-art algorithms show excellent performance when tested at the same location where they were trained. However, we find that generalization to new locations is poor, especially for classification systems.(The dataset is available at https://beerys.github.io/CaltechCameraTraps/)"], "score": 0.0}, {"id": "(Li et al., 2017)", "paper": {"corpus_id": 6037691, "title": "Deeper, Broader and Artier Domain Generalization", "year": 2017, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Da Li", "authorId": "2108338672"}, {"name": "Yongxin Yang", "authorId": "2653152"}, {"name": "Yi-Zhe Song", "authorId": "1705408"}, {"name": "Timothy M. Hospedales", "authorId": "1697755"}], "n_citations": 1453}, "snippets": ["The problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognition in sketch images, which are distinctly more abstract and rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them. In this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more significant DG challenge to drive future research."], "score": 0.0}, {"id": "(Berti et al., 2025)", "paper": {"corpus_id": 276903421, "title": "Emergent Abilities in Large Language Models: A Survey", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Leonardo Berti", "authorId": "2229196235"}, {"name": "Flavio Giorgi", "authorId": "2325903265"}, {"name": "Gjergji Kasneci", "authorId": "1686448"}], "n_citations": 6}, "snippets": ["Their findings offer a nuanced understanding of how quantization impacts emergent abilities. At higher bit levels, such as 8-bit and 16-bit, LLMs retain much of their original performance, with minimal degradation in reasoning and instruction-following tasks. However, as precision decreases, particularly at 4-bit and below, a clear pattern emerges: while 4-bit quantization manages to preserve most emergent abilities, 2-bit quantization severely degrades performance, often reducing accuracy to near-random levels."], "score": 0.900390625}], "table": null}, {"title": "Quantization Methods and Techniques", "tldr": "Various quantization techniques have emerged to mitigate accuracy loss at low bit precision, with post-training quantization (PTQ) offering simple deployment and quantization-aware training (QAT) providing superior accuracy. Advanced methods like APTQ, QLORA with specialized number formats, and mixed-precision approaches that maintain higher bit rates for sensitive layers can significantly improve 4-bit model performance compared to uniform quantization. (12 sources)", "text": "\nQuantization approaches broadly fall into two categories: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ applies quantization to pre-trained models without retraining, offering simplicity but potentially greater accuracy loss, while QAT incorporates quantization effects during training to optimize model performance at lower precision <Paper corpusId=\"49356451\" paperTitle=\"(Krishnamoorthi, 2018)\" isShortName></Paper>.\n\nThe choice of numeric representation significantly impacts quantization outcomes. For 4-bit quantization of large language models, specialized number formats like NF4 with double quantization have demonstrated remarkable effectiveness, fully recovering 16-bit LoRA performance in fine-tuning scenarios and outperforming standard FP4 formats by approximately one percentage point on benchmarks like MMLU <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper>.\n\nInnovative techniques have pushed the boundaries of low-bit quantization efficacy. Adaptive Precision Task Quantization (APTQ) achieves comparable perplexity to higher-precision models even at average bit rates as low as 3.0-3.5, outperforming other post-training quantization approaches at reduced precision <Paper corpusId=\"267897495\" paperTitle=\"(Guan et al., 2024)\" isShortName></Paper>. This demonstrates that advanced algorithms can maintain performance while achieving compression beyond uniform 4-bit quantization.\n\nMixed-precision approaches offer significant advantages over uniform quantization by strategically allocating different bit-widths across a model. Research shows that constraining models to an average of 4 bits while keeping first and last layers at 8 bits improves accuracy by almost 1% compared to uniform 4-bit quantization <Paper corpusId=\"231918483\" paperTitle=\"(Garg et al., 2021)\" isShortName></Paper>. Similarly, mixed-precision configurations (such as ANT4-8) that combine 4-bit and 8-bit precision can achieve original model accuracy in CNNs and less than 1% accuracy loss for Vision Transformers and BERT models <Paper corpusId=\"251928917\" paperTitle=\"(Guo et al., 2022)\" isShortName></Paper>.\n\nThe activation precision plays a crucial role alongside weight quantization. Keeping activations at higher precision (8, 16, or 32-bit) while quantizing weights to lower precision dramatically increases model accuracy. For 4-bit weights, higher precision activations maintain performance within 1% of full-precision models across tasks ranging from MNIST classification to language modeling <Paper corpusId=\"211677681\" paperTitle=\"(Lam et al., 2020)\" isShortName></Paper>. This finding suggests that asymmetric quantization approaches may offer better performance-compression trade-offs than symmetric schemes.\n\nQuantized distillation has emerged as a particularly effective technique across various bit-widths and architectures. It significantly outperforms other methods for 2-bit and 4-bit quantization, achieving accuracy within 0.2% of the teacher model at 8 bits and showing relatively minor accuracy loss at 4-bit precision <Paper corpusId=\"3323727\" paperTitle=\"(Polino et al., 2018)\" isShortName></Paper>. On language tasks, distillation enables 4-bit quantized models to achieve better perplexity than their teachers and improve BLEU scores compared to models trained with standard loss functions.\n\nFrequency-Aware Quantization (FAQ) provides substantial improvements for low-bit training. FAQ-trained 8-bit networks typically match or exceed full-precision accuracy with just one epoch of fine-tuning, while 4-bit networks require significantly longer fine-tuning (approximately 110 epochs) but ultimately match or exceed full-precision performance <Paper corpusId=\"52197199\" paperTitle=\"(McKinstry et al., 2018)\" isShortName></Paper>. In most cases, well-trained 4-bit models show no accuracy advantage when precision is increased to 8-bit, suggesting that 4-bit can be sufficient with appropriate training techniques <Paper corpusId=\"67788003\" paperTitle=\"(Esser et al., 2019)\" isShortName></Paper>.\n\nThe effectiveness of quantization methods varies significantly across model architectures. For example, QUIK quantization excels for LLaMA models but underperforms for Mistral, while AWQ shows greater robustness for Mistral <Paper corpusId=\"276575417\" paperTitle=\"(Kharinaev et al., 2025)\" isShortName></Paper>. This variability highlights the importance of selecting quantization techniques based on specific model architectures rather than applying one-size-fits-all approaches.\n\nPost-training methods like Optimal Channel Scaling (OCS) demonstrate the limitations of simpler quantization approaches. While effective at 8-bit and 6-bit precision, OCS experiences drastic accuracy drops at 4-bit <Paper corpusId=\"218862856\" paperTitle=\"(Kim et al., 2020)\" isShortName></Paper>. This highlights the need for more sophisticated techniques when targeting very low bit-widths.\n\nFor spiking neural networks, Mixed-precision Integer (MINT) quantization allows effective compression to both 8-bit and 4-bit precision with minimal accuracy impact (less than 1% drop). In some cases, quantized models even outperform full-precision baselines by small margins (e.g., 0.2% on VGG-16 TinyImageNet), suggesting quantization can provide regularization benefits <Paper corpusId=\"265043878\" paperTitle=\"(Yin et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Krishnamoorthi, 2018)", "paper": {"corpus_id": 49356451, "title": "Quantizing deep convolutional networks for efficient inference: A whitepaper", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "Raghuraman Krishnamoorthi", "authorId": "2065915235"}], "n_citations": 1021}, "snippets": ["Our first experiment compares stochastic quantization with deterministic quantization. Subse-quently, we study if training a quantized model from scratch provides higher accuracies than fine tuning from a floating point model."], "score": 0.82666015625}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["Results are shown in Table 4 where we see that NF4 with double quantization fully recovers the 16-bit LoRA MMLU performance. In addition, we also note that QLORA with FP4 lags behind the 16-bit brain float LoRA baseline by about 1 percentage point. This corroborates both our findings that (1) QLORA with NF4 replicates both 16-bit full finetuning and 16-bit LoRA finetuning performance, and (2) NF4 is superior to FP4 in terms of quantization precision."], "score": 0.79931640625}, {"id": "(Guan et al., 2024)", "paper": {"corpus_id": 267897495, "title": "APTQ: Attention-aware Post-Training Mixed-Precision Quantization for Large Language Models", "year": 2024, "venue": "Design Automation Conference", "authors": [{"name": "Ziyi Guan", "authorId": "2120170158"}, {"name": "Hantao Huang", "authorId": "2301516332"}, {"name": "Yupeng Su", "authorId": "2286850679"}, {"name": "Hong Huang", "authorId": "2351687700"}, {"name": "Ngai Wong", "authorId": "2287187433"}, {"name": "Hao Yu", "authorId": "2288350991"}], "n_citations": 16}, "snippets": ["Remarkably, even with average bit rates reduced to 3.5 and 3.0, APTQ's perplexity remains comparable to that of GPTQ's 4-bit model. This evidence of APTQ's stability at low bit rates positions it as a potent tool for optimizing the quantization and deployment of large-scale language models like LLaMa-7B", "The APTQ model, quantized at an average of 4 bit, not only approaches the full-precision model's perplexity but also outperforms all other PTQ approaches at a reduced precision of 3.5 bits. Impressively, configurations below 3 bits still surpass the 4-bit LLM-QAT baseline, underscoring APTQ's efficacy."], "score": 0.7685546875}, {"id": "(Garg et al., 2021)", "paper": {"corpus_id": 231918483, "title": "Confounding Tradeoffs for Neural Network Quantization", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Sahaj Garg", "authorId": "9273400"}, {"name": "Anirudh Jain", "authorId": "153408223"}, {"name": "Joe Lou", "authorId": "134183581"}, {"name": "Mitchell Nahmias", "authorId": "2060968473"}], "n_citations": 18}, "snippets": ["Results in Figure 5 demonstrate that methods such as [36] that attain extremely high accuracy with 3 bit activations may be able to do so in part because they do not quantize residual and skip connections. \n\nObservation 4: Ignoring the first and last layer affects the compression ratio, and consequently accuracy when subject to mixed-precision compression targets. \n\nThe amount of model compression is affected by whether the first and last layer are quantized. We train mixed-precision bitwidth allocations that quantize to an average of 4 bits for weights and activations over all layers and compare against mixed-precision models that keep the first and last layer at 8 bits while quantizing all other layers to an average of 4 bits. While the difference in compression might appear small, results in Table 2 show that it can affect accuracy by almost 1%. \n\nObservation 6: Mixed-precision activations constrained by the maximum feature map size have higher accuracy than uniform precision with the same maximum feature map size. We compare two settings: mixed-precision activations with a maximum feature map size, and uniform precision activations with the same maximum feature map size. For mixed-precision activations, we set the bitwidth of each layer so that all feature maps have the maximum feature map size, with a maximum bitwidth of 32. We show results in Figure 7. For Resnet50, the model with \u223c400KB maximum feature map size corresponds to a uniformly quantized model with 4 bit activations."], "score": 0.78076171875}, {"id": "(Guo et al., 2022)", "paper": {"corpus_id": 251928917, "title": "ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization", "year": 2022, "venue": "Micro", "authors": [{"name": "Cong Guo", "authorId": "2109865181"}, {"name": "Chen Zhang", "authorId": "145107889"}, {"name": "Jingwen Leng", "authorId": "1831521"}, {"name": "Zihan Liu", "authorId": "2117940902"}, {"name": "Fan Yang", "authorId": "145338263"}, {"name": "Yun-Bo Liu", "authorId": "2117415805"}, {"name": "Minyi Guo", "authorId": "2151671216"}, {"name": "Yuhao Zhu", "authorId": "2124167"}], "n_citations": 60}, "snippets": ["Comparing Fig. 10, Fig. 11, and Fig. 12, we find that the model accuracy loss correlates well with the quantization MSE, and the fine-tuning plays an essential role in recovering the accuracy to the original values before quantization. The 4-bit IP-F and FIP-F provide more numerical types for selection, and both achieve the minimum accuracy loss. configuration (i.e., int-PoT-flint) as the final ANT for the rest of evaluation. Note that the 4-bit ANT type is still not able to maintain the original model accuracy, which justifies the choice of mixed-precision in our work. The mixed-precision ANT4-8 type can achieve original model accuracy in CNN models and less than 1% accuracy loss for ViT and BERT."], "score": 0.7783203125}, {"id": "(Lam et al., 2020)", "paper": {"corpus_id": 211677681, "title": "Quantized Neural Network Inference with Precision Batching", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Maximilian Lam", "authorId": "2347284"}, {"name": "Zachary Yedidia", "authorId": "1515569180"}, {"name": "Colby R. Banbury", "authorId": "103876904"}, {"name": "V. Reddi", "authorId": "1805668"}], "n_citations": 1}, "snippets": ["For weight bitlevels < 8, keeping activations at higher precision (8, 16 or 32 bit) greatly increases model accuracy; generally, keeping activations at a higher precision allows quantizing twice as many bits, from 8-bits to 4-bits, without significant loss in model accuracy. For MNIST, with 1-bit weights, using higher precision activations is the difference between 85% accuracy and random guessing ( 10% accuracy); with 4-bit weights, higher precision activations maintains within < 1% of the full precision model's performance. Similarly, for language modeling, with 1-bit weights, higher precision activations reduces perplexity from 800 to 180; for 4-bit weights, higher precision activations reduce perplexity from 180 to within a few points of the full precision performance. \n\nFor natural language inference, using full precision activations allows us to quantize down to 1-bit with only a couple percentages of accuracy degredation (78% to 76%), whereas quantizing activations to 1-bit degrades to random guessing (33%). Interestingly, for language inference, the 8-bit quantized model outperformed the full precision result, a known phenomenon seen in quantization literature (Krishnan et al., 2019;(Xu et al., 2018)."], "score": 0.95068359375}, {"id": "(Polino et al., 2018)", "paper": {"corpus_id": 3323727, "title": "Model compression via distillation and quantization", "year": 2018, "venue": "International Conference on Learning Representations", "authors": [{"name": "A. Polino", "authorId": "36060478"}, {"name": "Razvan Pascanu", "authorId": "1996134"}, {"name": "Dan Alistarh", "authorId": "3311387"}], "n_citations": 732}, "snippets": ["Overall, quantized distillation appears to be the method with best accuracy across the whole range of bit widths and architectures. It outperforms PM significantly for 2bit and 4bit quantization, achieves accuracy within 0.2% of the teacher at 8 bits on the larger student model, and relatively minor accuracy loss at 4bit quantization.\n\nAt 2bit precision, the student converges to 67.22% accuracy with normal loss, and to 82.40% with distillation loss. At 4bit precision, the student converges to 86.01% accuracy with normal loss, and to 88.00% with distillation loss. On OpenNMT, we observe a similar gap: the 4bit quantized student converges to 32.67 perplexity and 15.03 BLEU when trained with normal loss, and to 25.43 perplexity (better than the teacher) and 15.73 BLEU when trained with distillation loss."], "score": 0.880859375}, {"id": "(McKinstry et al., 2018)", "paper": {"corpus_id": 52197199, "title": "Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference", "year": 2018, "venue": "arXiv.org", "authors": [{"name": "J. McKinstry", "authorId": "46571359"}, {"name": "S. K. Esser", "authorId": "2357931"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "John V. Arthur", "authorId": "2248110488"}, {"name": "Izzet B. Yildiz", "authorId": "3121907"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 94}, "snippets": ["FAQ trained 8-bit networks outperform all comparable quantization methods in all but one instance and exceeded pretrained fp32 network accuracy with only one epoch of training following quantization for all networks explored (Table 1). Immediately following quantization, network accuracy was nearly at the level of the pretrained networks (data not shown) with one exception, Inception-v3, which started at 72.34% top-1. Since the networks started close to a good solution, they did not require extensive fine-tuning to return to and surpass pretrained networks. \n\nFAQ trained 4-bit network accuracy outperforms all comparable quantization methods, exceeding the next closest approach by over 0.5% for ResNet-18 (Jung et al., 2018), and matched or exceeded pretrained fp32 network accuracy. FAQ 4-bit networks required significantly longer fine-tuning -110 epochs -for networks trained, ResNet-18, ResNet-34, and ResNet-50. In constrast to the 8-bit cases, immediately following quantization, network accuracy dropped precipitously, requiring significant fine-tuning to match and surpass the pretrained networks."], "score": 0.87548828125}, {"id": "(Esser et al., 2019)", "paper": {"corpus_id": 67788003, "title": "Learned Step Size Quantization", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "S. K. Esser", "authorId": "2357931"}, {"name": "J. McKinstry", "authorId": "46571359"}, {"name": "Deepika Bablani", "authorId": "2064431971"}, {"name": "R. Appuswamy", "authorId": "2730753"}, {"name": "D. Modha", "authorId": "1944330"}], "n_citations": 810}, "snippets": ["In most cases, we found no accuracy advantage to increasing precision from 4-bit to 8-bit. It is worth noting that the next best low precision method (Jung et al., 2018) used progressive fine tuning (sequentially training a full precision to 5-bit model, then the 5-bit model to a 4-bit model, and so on), significantly increasing training time and complexity over our approach which fine tunes directly from a full precision model to the precision of interest."], "score": 0.783203125}, {"id": "(Kharinaev et al., 2025)", "paper": {"corpus_id": 276575417, "title": "Investigating the Impact of Quantization Methods on the Safety and Reliability of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Artyom Kharinaev", "authorId": "2346980509"}, {"name": "Viktor Moskvoretskii", "authorId": "2291142916"}, {"name": "Egor Shvetsov", "authorId": "2183482391"}, {"name": "Kseniia Studenikina", "authorId": "2334636989"}, {"name": "Bykov Mikhail", "authorId": "2346982876"}, {"name": "E. Burnaev", "authorId": "2257279157"}], "n_citations": 0}, "snippets": ["At 4-bit precision, models generally maintain strong performance, with minimal degradation in safety and trustworthiness. However, the effectiveness of quantization methods varies significantly across models. For instance, QUIK excels for LLaMA but underperforms for Mistral, while AWQ shows robustness for Mistral."], "score": 0.8193359375}, {"id": "(Kim et al., 2020)", "paper": {"corpus_id": 218862856, "title": "Position-based Scaled Gradient for Model Quantization and Sparse Training", "year": 2020, "venue": "arXiv.org", "authors": [{"name": "Jangho Kim", "authorId": "49476045"}, {"name": "Kiyoon Yoo", "authorId": "1713608836"}, {"name": "Nojun Kwak", "authorId": "3160425"}], "n_citations": 7}, "snippets": ["On the other hand, ImageNet experiment generally shows reasonable results until 6-bits but the accuracy drastically drops at 4-bits. PSGD targeting 8-bits and 6-bits marginally improves on all bits, yet also experiences drastic accuracy drop at 4-bits. In contrast, Gradient L1 (\u03bb = 0.05) and PSGD @ W4 maintain the performance of the quantized models even at 4-bits. Comparing with the second best method Gradient L1 (\u03bb = 0.05) (Alizadeh et al., 2020), PSGD outperforms it at all bit-widths. At full precision (FP), 8-, 6-and 4-bits, the gap of performance between (Alizadeh et al., 2020) and ours are about 4.2%, 3.8%, 1.8% and 8%, respectively. From Table 2, while the quantization noise may slightly degrade the accuracy in some cases, a general trend that using more bits leads to higher accuracy is demonstrated", "Post-training methods Table 3 shows that OCS, state-of-the-art post-training method, has a drastic accuracy drop at 4-bits."], "score": 0.86865234375}, {"id": "(Yin et al., 2023)", "paper": {"corpus_id": 265043878, "title": "MINT: Multiplier-less INTeger Quantization for Energy Efficient Spiking Neural Networks", "year": 2023, "venue": "Asia and South Pacific Design Automation Conference", "authors": [{"name": "Ruokai Yin", "authorId": "1820826857"}, {"name": "Yuhang Li", "authorId": "2265595883"}, {"name": "Abhishek Moitra", "authorId": "65951652"}, {"name": "P. Panda", "authorId": "9352814"}], "n_citations": 13}, "snippets": ["Accuracy. Table I summarizes the accuracy results. We test MINT with 3 groups of bit-width, i.e., 2,4,8. For instance, W8U8 represents an SNN with weights and membrane potentials quantized to 8-bit integers. Although the full-precision baseline achieves higher accuracy for most of the datasets and networks, MINT achieves very close accuracy for all cases with less than a 1% drop. Some of our W8U8 and W4U4 models even have higher accuracy than the full-precision baseline (e.g., 0.2% on VGG-16 TinyImageNet), which suggests the MINT may serve the purpose of regularization."], "score": 0.81005859375}], "table": null}, {"title": "Practical Recommendations", "tldr": "For most NLP applications, 4-bit quantization represents the optimal trade-off point, offering significant memory savings (70-80%) with minimal performance degradation (1-3%), though layer-specific strategies that maintain higher precision for sensitive components can further improve results. When deploying quantized models, practitioners should consider task complexity, model architecture, and available computational resources, with 8-bit recommended for critical applications requiring maximum accuracy and 4-bit or lower best suited for resource-constrained environments. (7 sources)", "text": "\nWhen implementing quantization in practice, empirical research offers several clear recommendations to optimize the trade-off between model compression and performance preservation. The most consistent finding is that 4-bit precision represents a practical sweet spot for most NLP applications, delivering substantial compression (7-8\u00d7) while typically incurring only minor accuracy degradation (1-3%) across diverse model architectures <Paper corpusId=\"231906389\" paperTitle=\"(Ghamari et al., 2021)\" isShortName></Paper>. This 4-bit threshold allows an average memory footprint reduction of approximately 70% compared to original models without significant performance decreases <Paper corpusId=\"276902966\" paperTitle=\"(Giagnorio et al., 2025)\" isShortName></Paper>.\n\nLayer-specific quantization strategies can significantly improve results compared to uniform approaches. For instance, when working with architectures like SqueezeNet, applying 4-bit quantization selectively to specific layers (e.g., 'fire-9-expand3x3') can minimize accuracy drops while still achieving substantial model compression <Paper corpusId=\"225583435\" paperTitle=\"(Yee et al., 2020)\" isShortName></Paper>. This targeted approach recognizes that not all network components show equal sensitivity to precision reduction.\n\nFor applications where performance is paramount, 8-bit quantization offers an excellent compromise. With 8-bit precision, models typically show minimal accuracy decreases (around 2.2% in mean score) while reducing model size by approximately 4\u00d7 <Paper corpusId=\"244531228\" paperTitle=\"(Wang et al., 2021)\" isShortName></Paper>. This makes 8-bit particularly suitable for production systems where reliability is critical but efficiency gains are still needed.\n\nWhen extreme compression is required, practitioners should consider the empirically established 3-bit threshold, beyond which performance degradation transitions from linear to exponential collapse. While 4-bit quantization generally maintains >95% of baseline performance, dropping to 2-bit precision typically preserves >90% of baseline performance but risks significant degradation for complex tasks <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This suggests that 2-bit quantization should be used cautiously and primarily for applications where some performance trade-off is acceptable.\n\nFor federated learning scenarios with heterogeneous client devices, mixed-precision approaches offer compelling advantages. Systems with mixed-precision clients can save over 65% of energy consumption compared to homogeneous 32-bit systems while maintaining high accuracy. Interestingly, performance gains for lower-precision clients (4-bit) show diminishing returns beyond 16-bit precision for higher-tier devices, suggesting that 16-bit represents a practical upper limit for mixed-precision systems <Paper corpusId=\"270257725\" paperTitle=\"(Yuan et al., 2024)\" isShortName></Paper>.\n\nWhen quantizing language models for code-specific tasks or specialized domains, using a domain-specific calibration dataset becomes increasingly important as quantization becomes more aggressive. Such calibration data helps limit performance degradation at extreme compression levels (3 and 2 bits) <Paper corpusId=\"276902966\" paperTitle=\"(Giagnorio et al., 2025)\" isShortName></Paper>.\n\nFor data valuation and influence estimation applications, practitioners can effectively use quantized representations to drastically reduce storage requirements. Even at 1-bit precision, quantized approaches remain only a few percentage points behind 16-bit performance while reducing storage needs by up to 16\u00d7 (from 16.54 GB to just 1.03 GB) <Paper corpusId=\"276106922\" paperTitle=\"(Ananta et al., 2025)\" isShortName></Paper>. This indicates that even extremely low-bit representations can retain sufficient information for certain meta-learning applications.\n\nWhen choosing between compression techniques, quantization consistently outperforms pruning in terms of robustness. While 4-bit quantization typically results in less than 5% accuracy loss, unstructured pruning with matched sparsity can cause 15-20% performance degradation <Paper corpusId=\"277452419\" paperTitle=\"(Ma et al., 2025)\" isShortName></Paper>. This clear advantage makes quantization the preferred choice for most model compression scenarios.", "citations": [{"id": "(Ghamari et al., 2021)", "paper": {"corpus_id": 231906389, "title": "Quantization-Guided Training for Compact TinyML Models", "year": 2021, "venue": "arXiv.org", "authors": [{"name": "Sedigh Ghamari", "authorId": "103148446"}, {"name": "Koray Ozcan", "authorId": "40408041"}, {"name": "Thu Dinh", "authorId": "40844014"}, {"name": "A. Melnikov", "authorId": "38363587"}, {"name": "Juan Carvajal", "authorId": "2062260709"}, {"name": "Jan Ernst", "authorId": "39497207"}, {"name": "S. Chai", "authorId": "1691331"}], "n_citations": 17}, "snippets": ["We have specifically focused on four-and twobit results as the performance of higher-bit post-train-quantized models (i.e., 8 and 16) are often close to that of their original floatingpoint models", "The results in Table 1 suggest that QGT is able to effectively compress small and large DNN architectures regardless of the task / dataset complexity. We show that 4-bit bit-precision achieve significant compression (7-8\u00d7) while reducing only 1-3% drop in accuracy."], "score": 0.7744140625}, {"id": "(Giagnorio et al., 2025)", "paper": {"corpus_id": 276902966, "title": "Quantizing Large Language Models for Code Generation: A Differentiated Replication", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Alessandro Giagnorio", "authorId": "2343501642"}, {"name": "A. Mastropaolo", "authorId": "2079107343"}, {"name": "Saima Afrin", "authorId": "2041943516"}, {"name": "M. D. Penta", "authorId": "1719962"}, {"name": "G. Bavota", "authorId": "1801330"}], "n_citations": 1}, "snippets": ["The empirical evaluation reveals that the new frontier for LLM quantization is 4-bit precision, resulting in an average memory footprint reduction of 70% compared to the original model without observing any significant decrease in performance. Additionally, when the quantization becomes even more extreme (3 and 2 bits), a code-specific calibration dataset helps to limit the loss of performance."], "score": 0.8017578125}, {"id": "(Yee et al., 2020)", "paper": {"corpus_id": 225583435, "title": "Face Recognition and Machine Learning at the Edge", "year": 2020, "venue": "IOP Conference Series: Materials Science and Engineering", "authors": [{"name": "Joanne Ling Sin Yee", "authorId": "2004954858"}, {"name": "U. U. Sheikh", "authorId": "2412102"}, {"name": "M. Mokji", "authorId": "1951977"}, {"name": "S. Rahman", "authorId": "2111988471"}], "n_citations": 2}, "snippets": ["For 4-bit fixed-point quantization, the accuracy drops to 72.5% and 85% from the original 90% after quantization on the weights in the layer 'fire8-expand3x3' and 'fire9expand3x3'. The accuracy drops the to the lowest which is 42.5% after 4-bit fixed point quantization is being applied on the weights in the layer 'fire6-expand3x3'. This might due to the large value changes in total weight after quantization. Although the model size is compressed the most after applying weight quantization in the layer 'fire8-expand3x3', but there is 17.5% accuracy drop. Hence, it is recommended to apply 4-bit quantization on weights in layer 'fire-9-expand3x3' when doing 4-bit fixed point quantization on the weights in a single layer of the network as it causes a slight drop in accuracy while reducing the model size. For 8-bit and 16-bit fixed-point quantization, there is no change even when weight quantization is applied on any of the layers."], "score": 0.791015625}, {"id": "(Wang et al., 2021)", "paper": {"corpus_id": 244531228, "title": "LiteEdge: Lightweight Semantic Edge Detection Network", "year": 2021, "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)", "authors": [{"name": "Hao Wang", "authorId": null}, {"name": "Hasan Al-Banna Mohamed", "authorId": "118657651"}, {"name": "Zuowen Wang", "authorId": "15678675"}, {"name": "Bodo Rueckauer", "authorId": "4765464"}, {"name": "Shih-Chii Liu", "authorId": "1704961"}], "n_citations": 3}, "snippets": ["The results in Table 3 show that the quantized model with 8-bit weights and activations only has a small drop of  2.2% in the mean MF score. The model size of the quantized network is around 4X smaller than the full precision network. In addition, QAT brings weight sparsity, where 2.65% of the parameters are zero after the quantization. The results for the 6-bit and 4-bit weight quantized models are also presented in Table 3. Compared with the 8-bit weight quantized model, the 6-bit weight quantized model shows a drop of 0.2% in the mean MF score but achieves around 4 times weight sparsity. The 4-bit weight quantized network shows a larger drop in the mean MF score but has more than 38% zero parameters. In addition, its model size is 13X smaller than the full precision model."], "score": 0.77001953125}, {"id": "(Ma et al., 2025)", "paper": {"corpus_id": 277452419, "title": "Model Hemorrhage and the Robustness Limits of Large Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Ziyang Ma", "authorId": "2352948034"}, {"name": "Zuchao Li", "authorId": "2274202084"}, {"name": "Lefei Zhang", "authorId": "2269488794"}, {"name": "Gui-Song Xia", "authorId": "2343636012"}, {"name": "Bo Du", "authorId": "2306994733"}, {"name": "Liangpei Zhang", "authorId": "2268745050"}, {"name": "Dacheng Tao", "authorId": "2275194788"}], "n_citations": 1}, "snippets": ["Our experiments reveal systematic patterns in model robustness under progressive precision reduction. A critical 3-bit threshold emerges, beyond which accuracy degradation transitions from linear to exponential collapse-evident in both perplexity (WikiText-2) and reasoning benchmarks (ARC-C, MMLU). Low-bit quantization (2-bit) achieves 5\u00d7 parameter compression while still retaining >90% of baseline performance (Table A4). Quantization universally outperforms pruning in robustness, sustaining <5% accuracy loss at 4-bit compression versus unstructured pruning's 15-20% degradation under matched sparsity (A.5)."], "score": 0.88916015625}, {"id": "(Yuan et al., 2024)", "paper": {"corpus_id": 270257725, "title": "Mixed-Precision Federated Learning via Multi-Precision Over-the-Air Aggregation", "year": 2024, "venue": "IEEE Wireless Communications and Networking Conference", "authors": [{"name": "Jinsheng Yuan", "authorId": "2305216766"}, {"name": "Zhuangkun Wei", "authorId": "98553648"}, {"name": "Weisi Guo", "authorId": "48544782"}], "n_citations": 1}, "snippets": ["Notably, for clients of high resource capacity, 32-bit or 24-bit precision only offers marginal training gains compared to 16bit precision. The server model performance of all quantization schemes reached 97% top-1 accuracy within a tight 0.3% margin after 100 communication rounds, underscoring the effectiveness of the federated learning framework in achieving high accuracy with mixed-precision clients", "As shown in Fig. 4, in comparison to FL systems of homogeneous clients at 32-bit and 16-bit, based on our estimation in Table II, our FL with mixed-precision clients models can save over 65% and 13% of energy consumption respectively, while gaining more than 10% in accuracy on clients at 4-bit. Notably, for those under schemes incorporating 16-bit precision or higher, when re-quantized for 4-bit clients, attain around 5% higher accuracy, and this performance boost for lower precision clients from higher precision counterparts shows diminishing returns beyond 16-bit precision. While comparing to FL of homogeneous clients at 8-bit and 4-bit, our mixed-precision FL can trade mere 10% of energy savings for 5% of accuracy."], "score": 0.89794921875}, {"id": "(Ananta et al., 2025)", "paper": {"corpus_id": 276106922, "title": "QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Moses Ananta", "authorId": "2343738049"}, {"name": "Farid Adilazuarda", "authorId": "2191731497"}, {"name": "Z. M. K. Zuhri", "authorId": "2306265527"}, {"name": "Ayu Purwarianti", "authorId": "2257345523"}, {"name": "Alham Fikri Aji", "authorId": "8129718"}], "n_citations": 0}, "snippets": ["Performance vs. Storage Trade-Off. QLESS demonstrates a notable reduction in memory requirements while retaining competitive performance relative to LESS, particularly at higher bit-width configurations. For instance, 8-bit QLESS achieves results comparable to LESS on most benchmarks, despite cutting the gradient datastore size from 16.54 GB to around 8.27 GB. As the bit-width is lowered to 4-bit and 2-bit, performance declines moderately but still yields substantial storage savings (around 4.14 GB and 2.07 GB, respectively). Even at 1-bit precision, QLESS remains only a few points behind 16-bit performance while requiring just 1.03 GB of storage, indicating that coarse directional information can be enough for effective data valuation."], "score": 0.7412109375}], "table": null}], "cost": 0.563715}}
