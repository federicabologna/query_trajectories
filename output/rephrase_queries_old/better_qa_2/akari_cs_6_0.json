{"clarifying_information": [{"clarifying_question1": "Are you seeking information specifically on fine-tuning the retriever versus the generator in RAG architectures, or approaches that target both simultaneously (dual fine-tuning)?", "clarifying_answer1": {"clarifying_answer": "I am interested in approaches that cover all of these: fine-tuning the retriever, fine-tuning the generator, and dual fine-tuning strategies targeting both components in RAG architectures."}}, {"clarifying_question2": "Are you interested in best practices and recent techniques for domain-specific RAG fine-tuning, such as adapting RAG models to legal, finance, or healthcare domains?", "clarifying_answer2": {"clarifying_answer": "Yes, I am specifically interested in best practices and recent techniques for domain-specific RAG fine-tuning, particularly methods for adapting RAG models to specialized fields like legal, finance, or healthcare."}}, {"clarifying_question3": "Do you want implementation-level details and evaluations of specific RAG fine-tuning methods, such as RAFT, CRAFT, Self-RAG, or Finetune-RAG, or are you more focused on general performance improvements and challenges in RAG fine-tuning?", "clarifying_answer3": {"clarifying_answer": "I'm interested in implementation-level details and evaluations of specific RAG fine-tuning methods, such as RAFT, CRAFT, Self-RAG, and Finetune-RAG."}}], "better_query": "What are the latest implementation-level techniques, evaluations, and challenges of fine-tuning both retriever and generator components (including dual fine-tuning) in Retrieval-Augmented Generation (RAG) systems, with a focus on methods like RAFT, CRAFT, Self-RAG, and Finetune-RAG?", "better_answer": {"sections": [{"title": "Introduction to RAG and Fine-Tuning", "tldr": "Retrieval-Augmented Generation (RAG) systems combine retrieval mechanisms with generative models to produce more accurate and contextually relevant responses. Fine-tuning both components is essential for optimizing performance, with various approaches targeting either individual components or the system as a whole. (5 sources)", "text": "\nRetrieval-Augmented Generation (RAG) has emerged as a powerful approach that enhances large language models by incorporating external knowledge. RAG systems consist of two core components: a retrieval mechanism that fetches relevant information from a knowledge base (such as a database or the internet), and a generative model that integrates this retrieved information into coherent and contextually relevant responses <Paper corpusId=\"269149146\" paperTitle=\"(Susnjak et al., 2024)\" isShortName></Paper>. This architecture allows models to produce text that is both contextually enriched and factually accurate by grounding responses in external knowledge sources.\n\nFine-tuning within the RAG framework has become crucial for optimizing system performance. Research in this area follows three main directions: some work focuses on fine-tuning the generator to better utilize retriever context, ensuring the model can faithfully incorporate and reason with the retrieved information <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258865283\" paperTitle=\"(Luo et al., 2023)\" isShortName></Paper>. Other approaches prioritize fine-tuning the retriever to learn to fetch passages that are most beneficial for the generator <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. Finally, holistic approaches treat RAG as an integrated system, fine-tuning both components simultaneously to enhance overall performance despite the increased complexity and integration challenges this presents <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nThe fine-tuning process for RAG systems requires a delicate balance, necessitating optimization of multiple elements including the retrieval model, context length, knowledge base construction, and the deployment of the underlying large language model <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>. Recent advances have expanded RAG applications across various domains, with methodologies like RAFT implementing specialized fine-tuning strategies that generate training data from target domain data for instruction fine-tuning <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>. These developments highlight the ongoing evolution of RAG fine-tuning approaches to enhance performance across diverse applications.", "citations": [{"id": "(Susnjak et al., 2024)", "paper": {"corpus_id": 269149146, "title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning", "year": 2024, "venue": "ACM Transactions on Knowledge Discovery from Data", "authors": [{"name": "Teo Su\u0161njak", "authorId": "2656889"}, {"name": "Peter Hwang", "authorId": "2296719088"}, {"name": "N. Reyes", "authorId": "1783269"}, {"name": "A. Barczak", "authorId": "3312622"}, {"name": "Timothy R. McIntosh", "authorId": "11430146"}, {"name": "Surangika Ranathunga", "authorId": "143976433"}], "n_citations": 26}, "snippets": ["RAG models retrieve relevant information from a knowledge base (like a database or the internet) and integrate this into the generation process.The core components of RAG include the retrieval mechanism, which fetches relevant documents or data, and the generative model, which synthesizes the retrieved information into coherent and contextually relevant responses", "The typical architecture of RAG systems involves a retriever that fetches relevant information from a database and a generator that incorporates this information into the final output, thus, the integration of these components allows the model to produce contextually enriched and factually accurate text [68]."], "score": 0.8505859375}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270870251, "title": "Searching for Best Practices in Retrieval-Augmented Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaohua Wang", "authorId": "2273537815"}, {"name": "Zhenghua Wang", "authorId": "2308276345"}, {"name": "Xuan Gao", "authorId": "2292070745"}, {"name": "Feiran Zhang", "authorId": "2308226671"}, {"name": "Yixin Wu", "authorId": "2308043953"}, {"name": "Zhibo Xu", "authorId": "2308044030"}, {"name": "Tianyuan Shi", "authorId": "2308036711"}, {"name": "Zhengyuan Wang", "authorId": "2309182278"}, {"name": "Shizheng Li", "authorId": "2309656885"}, {"name": "Qi Qian", "authorId": "2309176521"}, {"name": "Ruicheng Yin", "authorId": "2292032843"}, {"name": "Changze Lv", "authorId": "2220896023"}, {"name": "Xiaoqing Zheng", "authorId": "2257315404"}, {"name": "Xuanjing Huang", "authorId": "2257129987"}], "n_citations": 61}, "snippets": ["Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."], "score": 0.9306640625}, {"id": "(Luo et al., 2023)", "paper": {"corpus_id": 258865283, "title": "SAIL: Search-Augmented Instruction Learning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Hongyin Luo", "authorId": "1944274"}, {"name": "Yung-Sung Chuang", "authorId": "2475831"}, {"name": "Yuan Gong", "authorId": "145802952"}, {"name": "Tianhua Zhang", "authorId": "2146333115"}, {"name": "Yoon Kim", "authorId": "143827730"}, {"name": "Xixin Wu", "authorId": "1847260"}, {"name": "D. Fox", "authorId": "31997718"}, {"name": "H. Meng", "authorId": "145199941"}, {"name": "James R. Glass", "authorId": "145898106"}], "n_citations": 27}, "snippets": ["Large language models (LLMs) have been significantly improved by instruction fine-tuning, but still lack transparency and the ability to utilize up-to-date knowledge and information. In this work, we propose search-augmented instruction learning (SAIL), which grounds the language generation and instruction following abilities on complex search results generated by in-house and external search engines. With an instruction tuning corpus, we collect search results for each training case from different search APIs and domains, and construct a new search-grounded training set containing \\textit{(instruction, grounding information, response)} triplets. We then fine-tune the LLaMA-7B model on the constructed training set. Since the collected results contain unrelated and disputing languages, the model needs to learn to ground on trustworthy search results, filter out distracting passages, and generate the target response. The search result-denoising process entails explicit trustworthy information selection and multi-hop reasoning, since the retrieved passages might be informative but not contain the instruction-following answer. Experiments show that the fine-tuned SAIL-7B model has a strong instruction-following ability, and it performs significantly better on transparency-sensitive tasks, including open-ended question answering and fact checking."], "score": 0.0}, {"id": "(Lyu et al., 2024)", "paper": {"corpus_id": 267320876, "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models", "year": 2024, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Yuanjie Lyu", "authorId": "2187857206"}, {"name": "Zhiyu Li", "authorId": "2268429641"}, {"name": "Simin Niu", "authorId": "2268393907"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Bo Tang", "authorId": "2268400606"}, {"name": "Wenjin Wang", "authorId": "2117833477"}, {"name": "Hao Wu", "authorId": "2282083454"}, {"name": "Huan Liu", "authorId": "2304320758"}, {"name": "Tong Xu", "authorId": "2277237058"}, {"name": "Enhong Chen", "authorId": "2265580543"}], "n_citations": 40}, "snippets": ["Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results."], "score": 0.9140625}, {"id": "(Chung et al., 2024)", "paper": {"corpus_id": 272911196, "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isaac Chung", "authorId": "2322992640"}, {"name": "Phat Vo", "authorId": "2322982756"}, {"name": "Arman Kizilkale", "authorId": "2322991957"}, {"name": "Aaron Reite", "authorId": "2322982549"}], "n_citations": 0}, "snippets": ["Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)", "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning."], "score": 0.88232421875}], "table": null}, {"title": "Retriever Fine-Tuning Techniques", "tldr": "Retriever fine-tuning in RAG systems uses multiple approaches including direct supervised fine-tuning, adapter modules, LLM-supervised retrieval, and reinforcement learning methods to optimize passage selection. Recent advances focus on end-to-end optimization, semantic space alignment, and adaptive retrieval strategies to improve accuracy and efficiency. (7 sources)", "text": "\nFine-tuning the retriever component in RAG systems is crucial for ensuring that the most relevant and useful information is retrieved for the generator. Several distinct approaches have emerged to optimize retriever performance:\n\nDirect supervised fine-tuning represents the most straightforward approach, where retrievers are trained on specialized datasets constructed from either open-source retrieval collections or domain-specific data <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This method enables the dense retriever to learn relevance patterns specific to the target domain.\n\nFor cases where direct fine-tuning of embedding models is not feasible (particularly with API-based models like OpenAI's Ada-002), researchers have developed adapter modules that enhance data representation while facilitating better alignment with downstream tasks <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. These lightweight trainable components modify the embedding space without altering the base model parameters.\n\nLLM-supervised retrieval (LSR) has emerged as a powerful technique where the retriever is fine-tuned based on feedback from the language model itself <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>. This approach is extended in reinforcement learning frameworks that optimize the retrieval process as a generative Markov chain, aligning the retriever with the generator's needs <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nMore advanced end-to-end optimization approaches include Stochastic RAG, which relaxes assumptions about document independence through differentiable approximation for sampling without replacement, enabling effective end-to-end optimization across diverse tasks <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. Similarly, methods like Replug use KL divergence to align retriever results with LLM preferences, while LLM-Embedder employs a distillation objective based on LLM rankings <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nFor complex reasoning tasks, approaches like CoRAG and DeepRAG build multistep reasoning frameworks through full parameter fine-tuning and multitask learning <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation and intermediate answer prediction, improving the model's ability to break down complex problems. DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>.\n\nSemantic space alignment techniques like O1-Embedder and Open-RAG use mixed fine-tuning approaches to improve retrieval effectiveness. Open-RAG specifically employs QLoRA quantized fine-tuning <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper> and Mixture of Experts modules to specialize networks for single/multi-hop reasoning tasks <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>.\n\nMore recent work focuses on improving retrieval robustness. The Retrieval-augmented Adaptive Adversarial Training (RAAT) approach leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises, which are categorized into three distinct types reflecting real-world environments <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This helps address the challenge that inappropriate retrieved passages can potentially hinder LLMs' capacity to generate high-quality responses.\n\nDespite these advances, it's important to note that optimizing either the retrieval or generation component separately may lead to sub-optimal overall performance <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>, which has motivated research into dual fine-tuning approaches that optimize both components simultaneously.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271571401, "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 20}, "snippets": ["This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model."], "score": 0.84033203125}, {"id": "(Zamani et al., 2024)", "paper": {"corpus_id": 269605438, "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hamed Zamani", "authorId": "2293725953"}, {"name": "Michael Bendersky", "authorId": "2240516450"}], "n_citations": 29}, "snippets": ["This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278635834, "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "year": 2025, "venue": "", "authors": [{"name": "Shaohan Wang", "authorId": "2361597409"}, {"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zheren Fu", "authorId": "2106681735"}, {"name": "Zhendong Mao", "authorId": "2349977855"}], "n_citations": 0}, "snippets": ["Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."], "score": 0.81689453125}, {"id": "(Gao et al., 2025)", "paper": {"corpus_id": 277994112, "title": "Synergizing RAG and Reasoning: A Systematic Review", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Yijie Zhong", "authorId": "2291322497"}, {"name": "Yuxi Bi", "authorId": "2275171009"}, {"name": "Ming Xue", "authorId": "2356716546"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 7}, "snippets": ["For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically."], "score": 0.85693359375}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."], "score": 0.0}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 270199429, "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Feiteng Fang", "authorId": "2293319089"}, {"name": "Yuelin Bai", "authorId": "2287892865"}, {"name": "Shiwen Ni", "authorId": "2266469238"}, {"name": "Min Yang", "authorId": "2301170603"}, {"name": "Xiaojun Chen", "authorId": "2287803799"}, {"name": "Ruifeng Xu", "authorId": "2266809482"}], "n_citations": 39}, "snippets": ["Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."], "score": 0.0}, {"id": "(Shi et al., 2025)", "paper": {"corpus_id": 278339057, "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengliang Shi", "authorId": "2195381022"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Yue Feng", "authorId": "2326805997"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Xinyu Ma", "authorId": "2265517632"}, {"name": "Shuaiqiang Wang", "authorId": "2237948548"}, {"name": "Dawei Yin", "authorId": "2331316040"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Zhaochun Ren", "authorId": "2261862546"}], "n_citations": 1}, "snippets": ["To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."], "score": 0.904296875}], "table": null}, {"title": "Generator Fine-Tuning Techniques", "tldr": "Generator fine-tuning in RAG systems primarily employs supervised learning approaches to enhance language models' ability to use retrieved context effectively. Recent techniques focus on training models to distinguish between accurate and fictitious information, improving factuality and noise robustness while addressing challenges from imperfect retrieval. (4 sources)", "text": "\nFine-tuning the generator component of RAG systems is essential for optimizing how language models process and incorporate retrieved information. Several approaches have emerged to enhance generator performance:\n\nDirect supervised fine-tuning represents the primary method for optimizing generators in RAG systems. This approach uses external datasets to supplement the generator with additional knowledge while allowing customization of input and output formats. By establishing specific question-and-answer formats during training, language models can better understand particular data structures and respond according to instructions <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nKnowledge distillation offers another effective optimization path, particularly for open-source model deployment. This technique typically involves using more capable models like GPT-4 to batch-construct fine-tuning data, effectively transferring capabilities to smaller open-source models <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper>.\n\nMore specialized approaches like Finetune-RAG focus on enhancing the model's ability to distinguish between correct and fictitious context. Rather than improving retrieval quality, this method trains language models to effectively filter information from imperfect or misleading inputs. The core technique involves fine-tuning models using examples where both correct and incorrect information are explicitly presented, teaching the model to identify and utilize only the reliable information in its responses <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nSome approaches focus on improving the robustness of generators against irrelevant content. This can be achieved through supervised fine-tuning that teaches models to summarize key points from retrieved documents, helping them process and integrate even noisy retrieval results <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.\n\nRecent work has recognized that inappropriate retrieved passages can significantly impact a language model's ability to generate high-quality responses. The Retrieval-augmented Adaptive Adversarial Training (RAAT) approach addresses this challenge by dynamically adjusting the model's training process in response to various types of retrieval noise, while simultaneously employing multi-task learning to help the model internally recognize noisy contexts <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nHowever, it's important to note that optimizing only the generator component, like optimizing only the retriever, can lead to sub-optimal overall system performance. This limitation has motivated research into dual fine-tuning approaches that simultaneously optimize both retriever and generator components <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271571401, "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 20}, "snippets": ["This section will introduce three main patterns of fine-tuning stages, namely retriever fine-tuning, generator fine-tuning, and dual finetuning. 1) Retriever FT: In the RAG flow, common methods for fine-tuning the retriever is shown in Figure 15 ,which include: \n\n\u2022 Direct supervised fine-tuning of the retriever. Constructing a specialized dataset for retrieval and fine-tuning the dense retriever. For example, using open-source retrieval datasets or constructing one based on domain-specific data. \n\n\u2022 Adding trainable adapter modules. Sometimes, direct fine-tuning of the API-base embedding model (e.g., Ope-nAI Ada-002 and Cohere) is not feasible. Incorporating an adapter module can enhance the representation of your data. Additionally, the adapter module facilitates better alignment with downstream tasks, whether for taskspecific (e.g., PRCA [42]) or general purposes (e.g., AAR [58]). \u2022 LM-supervised Retrieval (LSR). Fine-tuning the retriever based on the results generated by LLM. \u2022 LLM Reward RL. Still using the LLM output results as the supervisory signal. Employing reinforcement learning to align the retriever with the generator. The whole retrieval process is disassembled in the form of a generative Markov chain. 2) Generator FT: The primary methods for fine-tuning a generator in RAG flow is shown in Figure 16, which include: \n\n\u2022 Direct supervised fine-tuning. Fine-tuning through an external dataset can supplement the generator with additional knowledge. Another benefit is the ability to customize input and output formats. By setting the Q&A format, LLM can understand specific data formats and output according to instructions. \u2022 Distillation. When using on-premise deployment of opensource models, a simple and effective Optimization method is to use GPT-4 to batch construct fine-tuning data to enhance the capabilities of the open-source model."], "score": 0.84033203125}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."], "score": 0.9287109375}, {"id": "(Shi et al., 2025)", "paper": {"corpus_id": 278339057, "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengliang Shi", "authorId": "2195381022"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Yue Feng", "authorId": "2326805997"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Xinyu Ma", "authorId": "2265517632"}, {"name": "Shuaiqiang Wang", "authorId": "2237948548"}, {"name": "Dawei Yin", "authorId": "2331316040"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Zhaochun Ren", "authorId": "2261862546"}], "n_citations": 1}, "snippets": ["To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."], "score": 0.904296875}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 270199429, "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Feiteng Fang", "authorId": "2293319089"}, {"name": "Yuelin Bai", "authorId": "2287892865"}, {"name": "Shiwen Ni", "authorId": "2266469238"}, {"name": "Min Yang", "authorId": "2301170603"}, {"name": "Xiaojun Chen", "authorId": "2287803799"}, {"name": "Ruifeng Xu", "authorId": "2266809482"}], "n_citations": 39}, "snippets": ["Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."], "score": 0.0}], "table": null}, {"title": "Dual Fine-Tuning Approaches", "tldr": "Dual fine-tuning approaches simultaneously optimize both retriever and generator components in RAG systems to create better synergy between components and improve overall performance. Methods range from modular training stages like RA-DIT to end-to-end optimization techniques like Stochastic RAG, addressing limitations of single-component optimization approaches. (8 sources)", "text": "\nDual fine-tuning approaches for RAG systems optimize both retriever and generator components simultaneously or sequentially, addressing the limitations of optimizing only one component at a time. While static optimization (improving only one component while keeping the other fixed) requires fewer computational resources and enables faster deployment, it often results in sub-optimal overall system performance <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>. This limitation has motivated the development of holistic approaches that treat RAG as an integrated system <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>.\n\nRetrieval-Augmented Dual Instruction Tuning (RA-DIT) represents a significant advancement in this area, implementing a two-stage fine-tuning approach. The first stage updates the pre-trained language model to better utilize retrieved information, while the second stage fine-tunes the retriever to return more relevant results based on the language model's preferences <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>. This modular training methodology allows for retrofitting any LLM with retrieval capabilities, creating a powerful synergy between components that leads to significant performance improvements across knowledge-intensive benchmarks <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"263605962\" paperTitle=\"(Lin et al., 2023)\" isShortName></Paper>.\n\nEnd-to-end optimization represents another approach to dual fine-tuning. Stochastic RAG, for example, treats the retrieval process as stochastic sampling without replacement, using a differentiable approximation to enable effective end-to-end optimization across diverse tasks <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. Similarly, methods like Replug align retriever results with LLM preferences using KL divergence, while LLM-Embedder employs a distillation objective based on LLM rankings to train retrievers that better match language model requirements <Paper corpusId=\"278635834\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nSome dual fine-tuning approaches specifically focus on addressing retrieval noise issues. The Retrieval-augmented Adaptive Adversarial Training (RAAT) method combines adaptive adversarial training to dynamically adjust the model's training process in response to different types of retrieval noise, while simultaneously employing multi-task learning to help the model internally recognize noisy contexts <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. This approach helps mitigate the potential negative impact of inappropriate retrieved passages on the language model's ability to generate high-quality responses.\n\nDespite the clear advantages of dual fine-tuning approaches, they introduce additional complexity and integration challenges compared to single-component optimization methods <Paper corpusId=\"270870251\" paperTitle=\"(Wang et al., 2024)\" isShortName></Paper>. These challenges include increased computational requirements, more complex training procedures, and the need for careful balancing of optimization objectives between retriever and generator components. Nevertheless, the performance improvements achieved through dual fine-tuning approaches make them increasingly essential for developing state-of-the-art RAG systems.", "citations": [{"id": "(Cheng et al., 2025)", "paper": {"corpus_id": 277043297, "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingyue Cheng", "authorId": "1491233507"}, {"name": "Yucong Luo", "authorId": "2208917508"}, {"name": "Ouyang Jie", "authorId": "2322501286"}, {"name": "Qi Liu", "authorId": "2332691115"}, {"name": "Huijie Liu", "authorId": "2312648865"}, {"name": "Li Li", "authorId": "2291070758"}, {"name": "Shuo Yu", "authorId": "2322429208"}, {"name": "Bohou Zhang", "authorId": "2351226328"}, {"name": "Jiawei Cao", "authorId": "2350426005"}, {"name": "Jie Ma", "authorId": "2350427710"}, {"name": "Daoyu Wang", "authorId": "2322524150"}, {"name": "Enhong Chen", "authorId": "2258714945"}], "n_citations": 6}, "snippets": ["Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information."], "score": 0.86669921875}, {"id": "(Wang et al., 2024)", "paper": {"corpus_id": 270870251, "title": "Searching for Best Practices in Retrieval-Augmented Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiaohua Wang", "authorId": "2273537815"}, {"name": "Zhenghua Wang", "authorId": "2308276345"}, {"name": "Xuan Gao", "authorId": "2292070745"}, {"name": "Feiran Zhang", "authorId": "2308226671"}, {"name": "Yixin Wu", "authorId": "2308043953"}, {"name": "Zhibo Xu", "authorId": "2308044030"}, {"name": "Tianyuan Shi", "authorId": "2308036711"}, {"name": "Zhengyuan Wang", "authorId": "2309182278"}, {"name": "Shizheng Li", "authorId": "2309656885"}, {"name": "Qi Qian", "authorId": "2309176521"}, {"name": "Ruicheng Yin", "authorId": "2292032843"}, {"name": "Changze Lv", "authorId": "2220896023"}, {"name": "Xiaoqing Zheng", "authorId": "2257315404"}, {"name": "Xuanjing Huang", "authorId": "2257129987"}], "n_citations": 61}, "snippets": ["Fine-tuning within the RAG framework is crucial for optimizing both retrievers and generators.Some research focuses on fine-tuning the generator to better utilize retriever context (Luo et al., 2023)[31][32], ensuring faithful and robust generated content.Others fine-tune the retriever to learn to retrieve beneficial passages for the generator [33][34][35].Holistic approaches treat RAG as an integrated system, fine-tuning both retriever and generator together to enhance overall performance [36][37][38], despite increased complexity and integration challenges."], "score": 0.9306640625}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 273969615, "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zilun Zhang", "authorId": "2270181751"}, {"name": "Haozhan Shen", "authorId": "2174678931"}, {"name": "Tiancheng Zhao", "authorId": "8200875"}, {"name": "Yuhao Wang", "authorId": "2330774884"}, {"name": "Bin Chen", "authorId": "2330612748"}, {"name": "Yuxiang Cai", "authorId": "2149196373"}, {"name": "Yongheng Shang", "authorId": "2093090552"}, {"name": "Jianwei Yin", "authorId": "2111612160"}], "n_citations": 3}, "snippets": ["Similarly, RA-DIT (Lin et al., 2023) uses dual instruction tuning to fine-tune the retriever and generative model, optimizing their collaboration for knowledgeintensive benchmarks."], "score": 0.8359375}, {"id": "(Lin et al., 2023)", "paper": {"corpus_id": 263605962, "title": "RA-DIT: Retrieval-Augmented Dual Instruction Tuning", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Xi Victoria Lin", "authorId": "2255374957"}, {"name": "Xilun Chen", "authorId": "1769736"}, {"name": "Mingda Chen", "authorId": "46221498"}, {"name": "Weijia Shi", "authorId": "2254168373"}, {"name": "Maria Lomeli", "authorId": "2253400960"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "Pedro Rodriguez", "authorId": "2253404757"}, {"name": "Jacob Kahn", "authorId": "2253401183"}, {"name": "Gergely Szilvasy", "authorId": "2253402270"}, {"name": "Mike Lewis", "authorId": "2253417398"}, {"name": "Luke S. Zettlemoyer", "authorId": "2137813791"}, {"name": "Scott Yih", "authorId": "2253400757"}], "n_citations": 153}, "snippets": ["Retrieval-augmented language models (RALMs) improve performance by accessing long-tail and up-to-date knowledge from external data stores, but are challenging to build. Existing approaches require either expensive retrieval-specific modifications to LM pre-training or use post-hoc integration of the data store that leads to suboptimal performance. We introduce Retrieval-Augmented Dual Instruction Tuning (RA-DIT), a lightweight fine-tuning methodology that provides a third option by retrofitting any LLM with retrieval capabilities. Our approach operates in two distinct fine-tuning steps: (1) one updates a pre-trained LM to better use retrieved information, while (2) the other updates the retriever to return more relevant results, as preferred by the LM. By fine-tuning over tasks that require both knowledge utilization and contextual awareness, we demonstrate that each stage yields significant performance improvements, and using both leads to additional gains. Our best model, RA-DIT 65B, achieves state-of-the-art performance across a range of knowledge-intensive zero- and few-shot learning benchmarks, significantly outperforming existing in-context RALM approaches by up to +8.9% in 0-shot setting and +1.4% in 5-shot setting on average."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278635834, "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with Curriculum Learning", "year": 2025, "venue": "", "authors": [{"name": "Shaohan Wang", "authorId": "2361597409"}, {"name": "L. Zhang", "authorId": "48378753"}, {"name": "Zheren Fu", "authorId": "2106681735"}, {"name": "Zhendong Mao", "authorId": "2349977855"}], "n_citations": 0}, "snippets": ["Using documents retrieved from extended knowledge bases to enhance the capabilities of large language models (LLMs) has been proven effective in NLP tasks, including language modeling (Borgeaud et al., 2021)(Ram et al., 2023)(Zhang et al., 2024) and question answering (Izacard et al., 2022)Shi et al., 2023;(Yoran et al., 2023)(Lin et al., 2023)Fang et al., 2024). Specifically, a Retrieval-Augmented Generator (RAG) system takes a query as input and uses a retriever to retrieve relevant documents from an external knowledge base. Then, it combines the documents with the query and feeds them into the LLM to make up for the LLM's own lack of knowledge. Optimization of the RAG system focuses on two main areas: improving the retriever and enhancing the generator (LLM) to the RALM. Replug (Shi et al., 2023) uses KL divergence to align retriever results with LLM preferences. LLM-Embedder (Zhang et al., 2024) employs a distillation objective based on LLM rankings. These methods train the retriever to better match LLM preferences. For generator, FiD (Izacard and Grave, 2020) finetunes LLM to handle retrieved documents and queries, addressing irrelevant information. Other studies introduce noise to improve LLM robustness (Yoran et al., 2023)Fang et al., 2024). Combining the strengths of both approaches, RA-DIT (Lin et al., 2023) uses modular training to optimize the retriever and LLM separately, enhancing overall RAG system performance."], "score": 0.81689453125}, {"id": "(Shi et al., 2025)", "paper": {"corpus_id": 278339057, "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengliang Shi", "authorId": "2195381022"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Yue Feng", "authorId": "2326805997"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Xinyu Ma", "authorId": "2265517632"}, {"name": "Shuaiqiang Wang", "authorId": "2237948548"}, {"name": "Dawei Yin", "authorId": "2331316040"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Zhaochun Ren", "authorId": "2261862546"}], "n_citations": 1}, "snippets": ["To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."], "score": 0.904296875}, {"id": "(Zamani et al., 2024)", "paper": {"corpus_id": 269605438, "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hamed Zamani", "authorId": "2293725953"}, {"name": "Michael Bendersky", "authorId": "2240516450"}], "n_citations": 29}, "snippets": ["This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."], "score": 0.0}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 270199429, "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Feiteng Fang", "authorId": "2293319089"}, {"name": "Yuelin Bai", "authorId": "2287892865"}, {"name": "Shiwen Ni", "authorId": "2266469238"}, {"name": "Min Yang", "authorId": "2301170603"}, {"name": "Xiaojun Chen", "authorId": "2287803799"}, {"name": "Ruifeng Xu", "authorId": "2266809482"}], "n_citations": 39}, "snippets": ["Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."], "score": 0.0}], "table": null}, {"title": "Specific RAG Fine-Tuning Methods", "tldr": "Several specialized fine-tuning methods have emerged for RAG systems, each addressing specific challenges in retrieval-augmentation integration. These approaches include RAFT, Self-RAG, Finetune-RAG, CoRAG, and Open-RAG, which employ diverse techniques from instruction tuning with mixed-quality contexts to self-reflection mechanisms. (8 sources)", "text": "\n- **RAFT (Retrieval-Augmented Fine-Tuning)**: Combines retrieval methods with language model supervised fine-tuning to enhance in-domain RAG performance. RAFT trains language models using a mixture of relevant (\"golden\") and irrelevant (\"distractor\") document contexts from the target domain, teaching models to dynamically leverage external knowledge while prioritizing relevant content. It employs supervised fine-tuning on static, curated datasets containing predefined question-response pairs and incorporates logical reasoning via Chain-of-Thought prompts. However, RAFT is limited by its reliance on predefined data and single-objective cross-entropy optimization. <Paper corpusId=\"276287820\" paperTitle=\"(Bhushan et al., 2025)\" isShortName></Paper> <Paper corpusId=\"277501853\" paperTitle=\"(Srinivas et al., 2025)\" isShortName></Paper> <Paper corpusId=\"272911196\" paperTitle=\"(Chung et al., 2024)\" isShortName></Paper>\n\n- **Self-RAG**: Advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms. Unlike conventional approaches that might retrieve irrelevant information, Self-RAG enables models to perform on-demand retrieval based on self-evaluation. It trains models to generate special \"reflection tokens\" that allow assessment of response quality and factual integrity. These tokens make the language model controllable during inference, enabling adaptation to diverse task requirements and improving the connectedness and correctness of outputs. <Paper corpusId=\"273501949\" paperTitle=\"(Anaissi et al., 2024)\" isShortName></Paper> <Paper corpusId=\"264288947\" paperTitle=\"(Asai et al., 2023)\" isShortName></Paper>\n\n- **Finetune-RAG**: Focuses on training language models to distinguish between correct and fictitious context within RAG setups. Rather than improving retrieval quality, this method fine-tunes models using examples where both correct and incorrect information are explicitly presented, teaching them to identify and utilize only reliable information in their responses. This approach addresses the challenge of imperfect or misleading retrieved content by improving the model's generation behavior when faced with mixed-quality inputs. <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>\n\n- **CoRAG**: Builds an end-to-end multistep reasoning framework through full parameter fine-tuning and multitask learning. This method expands single-step QA datasets into retrieval-reasoning chains and jointly trains various tasks including sub-query generation, intermediate answer prediction, and final composition. CoRAG enhances models' ability to break down complex problems (like multi-entity relational reasoning) and adapt retrieval strategies dynamically through techniques such as query rewriting and error correction. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **DeepRAG**: Combines imitation and contrastive learning with binary tree search to create efficient retrieval paths. It uses a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy, optimizing the retrieval pathway through full parameter fine-tuning and multitask learning. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **Open-RAG**: Employs semantic space alignment techniques through mixed fine-tuning approaches. This method uses QLoRA quantized fine-tuning <Paper corpusId=\"258841328\" paperTitle=\"(Dettmers et al., 2023)\" isShortName></Paper> and Mixture of Experts (MoE) modules to specialize networks for single and multi-hop reasoning tasks, improving retrieval effectiveness through better semantic understanding. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>\n\n- **AdaptiveRAG**: Fine-tunes a lightweight classifier to select retrieval strategies dynamically, applying parameter isolation techniques to balance generality and adaptability. This approach enables the system to choose appropriate retrieval mechanisms based on input characteristics. <Paper corpusId=\"277994112\" paperTitle=\"(Gao et al., 2025)\" isShortName></Paper>", "citations": [{"id": "(Bhushan et al., 2025)", "paper": {"corpus_id": 276287820, "title": "Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG", "year": 2025, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Kushagra Bhushan", "authorId": "2256382094"}, {"name": "Yatin Nandwani", "authorId": "1392630568"}, {"name": "Dinesh Khandelwal", "authorId": "2345003462"}, {"name": "Sonam Gupta", "authorId": "2320314900"}, {"name": "Gaurav Pandey", "authorId": "2345005348"}, {"name": "Dinesh Raghu", "authorId": "1916865"}, {"name": "Sachindra Joshi", "authorId": "2243011716"}], "n_citations": 2}, "snippets": ["Recently, Zhang et al. introduced Retrieval-Augmented Fine-Tuning (RAFT), a fine-tuning method for LLMs to incorporate domain knowledge and enhance in-domain RAG performance. RAFT combines RAG and fine-tuning by training LLMs on domain data using a mixture of oracle and distractor document contexts."], "score": 0.8818359375}, {"id": "(Srinivas et al., 2025)", "paper": {"corpus_id": 277501853, "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic Retrieval-Augmented Generation via KV Caching and Decoding", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Sakhinana Sagar Srinivas", "authorId": "2203079037"}, {"name": "Venkataramana Runkana", "authorId": "2139833562"}], "n_citations": 1}, "snippets": ["Retrieval-Augmented Fine-Tuning (RAFT (Zhang et al., 2024c)) advances this approach by integrating retrieval methods with language model supervised fine-tuning. Unlike traditional RAG, which simply retrieves documents for generation, RAFT trains the language model alongside the retrieval mechanism, teaching it to dynamically leverage external knowledge, prioritize relevant content while ignoring distractors for improved performance in domain-specific RAG contexts (e.g., open-book and in-domain question answering).\n\nRAFT employs supervised fine-tuning (SFT) on static, curated datasets containing predefined question-response pairs accompanied by both relevant (\"golden\") and irrelevant (\"distractor\") documents. \n\nIt optimizes indirectly by teaching the model to differentiate between useful and distracting documents through explicit training examples and incorporates logical reasoning via Chain-of-Thought (CoT) prompts. However, RAFT is inherently limited by its reliance on predefined data, single-objective cross-entropy optimization, and its inability to explicitly optimize retrieval fidelity and generation quality independently."], "score": 0.884765625}, {"id": "(Chung et al., 2024)", "paper": {"corpus_id": 272911196, "title": "Efficient In-Domain Question Answering for Resource-Constrained Environments", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Isaac Chung", "authorId": "2322992640"}, {"name": "Phat Vo", "authorId": "2322982756"}, {"name": "Arman Kizilkale", "authorId": "2322991957"}, {"name": "Aaron Reite", "authorId": "2322982549"}], "n_citations": 0}, "snippets": ["Recent advances in RAG have expanded its applications across various domains, showcasing its versatility and potential (Yan et al., 2024)", "Fine tuning strategies for RAG involve further training of a pretrained LLM on a specific dataset to enhance its performance in RAG tasks over that dataset. Several studies, such as those by (Lin et al., 2024) and (Xu et al., 2024) have explored different fine tuning methodologies for improving LLMs in RAG tasks. These works focus on the benefits of retrieval on long context (instruction-tuned) LLMs and extending the scope of fine tuning to the retriever. RAFT (Zhang et al., 2024b) includes a fine tuning strategy that generates training data from the QA target domain data for instruction fine tuning."], "score": 0.88232421875}, {"id": "(Anaissi et al., 2024)", "paper": {"corpus_id": 273501949, "title": "Fine-Tuning LLMs for Reliable Medical Question-Answering Services", "year": 2024, "venue": "2024 IEEE International Conference on Data Mining Workshops (ICDMW)", "authors": [{"name": "Ali Anaissi", "authorId": "3333168"}, {"name": "Ali Braytee", "authorId": "3069261"}, {"name": "Junaid Akram", "authorId": "1992906806"}], "n_citations": 3}, "snippets": ["SELF-RAG further advances traditional RAG by incorporating selective retrieval and self-reflection mechanisms, thus enhancing the quality and accuracy of language models. Unlike traditional RAG, which may retrieve irrelevant information, SELF-RAG ensures that only relevant content is retrieved based on the model's self-evaluation. It also incorporates selfreflection tokens that allow the model to assess the quality and integrity of its responses, thereby increasing the connectedness and correctness of its outputs [21]", "Fine-tuning adjusts the model's weights according to new data, allowing modifications without the need for retraining the entire model. This method is particularly effective in customizing pre-trained LLMs for specific tasks using labeled data, as seen in Supervised Fine-Tuning (SFT) and Parameter-Efficient Fine-Tuning (PEFT) [10], [22]. Among PEFT methods, Low-Rank Adaptation (LoRA) and its advanced versions, such as LoRA+ and DoRA, have shown significant improvements. LoRA introduces trainable low-rank matrices within the Transformer architecture, enhancing the efficiency of parameter updates without altering the model architecture significantly [17], [23]."], "score": 0.84521484375}, {"id": "(Asai et al., 2023)", "paper": {"corpus_id": 264288947, "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Akari Asai", "authorId": "35584853"}, {"name": "Zeqiu Wu", "authorId": "7806955"}, {"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Avirup Sil", "authorId": "2707234"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 780}, "snippets": ["Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models."], "score": 0.0}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["We introduce Finetune-RAG, a fine-tuning method designed to train large language models (LLMs) to distinguish between correct and fictitious context within a Retrieval-Augmented Generation (RAG) setup. Unlike prior work that attempts to improve factuality by enhancing the retrieval phase, Finetune-RAG focuses on improving the model's generation behavior when faced with imperfect or misleading inputs. Our core idea is to fine-tune the model using examples where both correct and incorrect information are explicitly presented to model, allowing it to learn the ability to sift out the correct information to use for its response."], "score": 0.9287109375}, {"id": "(Gao et al., 2025)", "paper": {"corpus_id": 277994112, "title": "Synergizing RAG and Reasoning: A Systematic Review", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Yijie Zhong", "authorId": "2291322497"}, {"name": "Yuxi Bi", "authorId": "2275171009"}, {"name": "Ming Xue", "authorId": "2356716546"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 7}, "snippets": ["For retrieval pathway optimization, methods like CoRAG [83] and DeepRAG [24] build end-to-end multistep reasoning frameworks through full parameter fine-tuning and multitask learning. CoRAG expands single-step QA datasets into retrieval-reasoning chains and jointly trains tasks such as sub-query generation, intermediate answer prediction, and final composition. This boosts the model's ability to break down complex problems (e.g., multi-entity relational reasoning) and adapt retrieval strategies dynamically (e.g., query rewriting, error correction). DeepRAG combines imitation and contrastive learning with binary tree search to create efficient retrieval paths, using a DPO-style contrastive loss to reduce redundant retrieval while maintaining accuracy.\n\nTo improve structured generation, MCTS-KBQA [97]and Self-RAG [3] fine-tune models for precise special token generation. MCTS-KBQA uses supervised fine-tuning to make large language models output instructions that comply with knowledge graph protocols (e.g., SPARQL), modeling reasoning as executable tool-call sequences. Self-RAG enhances selfsupervised generation control by expanding vocabulary and training the model to generate reflection tokens like retrieval triggers and relevance markers, preserving fluency and reducing factual errors. Additionally, O1-Embedder [101] and Open-RAG [38] align semantic spaces via mixed fine-tuning: O1-Embedder combines generative and contrastive training with special tokens to separate generation from embedding tasks, enhancing multihop semantic understanding; Open-RAG uses QLoRA [17] quantized fine-tuning and Mixture of Experts (MoE) modules to specialize networks for single/multi-hop reasoning.\n\nIn collaborative optimization with external modules, Adap-tiveRAG [41] and CR-Planner [52] apply parameter isolation to balance generality and adaptability. AdaptiveRAG finetunes a lightweight classifier to select retrieval strategies dynamically."], "score": 0.85693359375}, {"id": "(Dettmers et al., 2023)", "paper": {"corpus_id": 258841328, "title": "QLoRA: Efficient Finetuning of Quantized LLMs", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Tim Dettmers", "authorId": "3239480"}, {"name": "Artidoro Pagnoni", "authorId": "51152502"}, {"name": "Ari Holtzman", "authorId": "14487640"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}], "n_citations": 2606}, "snippets": ["We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training."], "score": 0.0}], "table": null}, {"title": "Challenges and Evaluation", "tldr": "RAG fine-tuning faces significant challenges including computational resource constraints, component integration complexities, and domain adaptation difficulties. Evaluation methodologies vary widely across research, with emerging frameworks focusing on measuring both retrieval accuracy and generation quality in integrated systems. (8 sources)", "text": "\nFine-tuning RAG systems presents several significant challenges that researchers continue to address. A fundamental tension exists between computational efficiency and system performance. Static optimization approaches (improving only one component while keeping the other fixed) require fewer computational resources and enable faster deployment, but often result in sub-optimal overall system performance <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>. This computational constraint becomes particularly significant when attempting to simultaneously optimize both retriever and generator components in dual fine-tuning approaches.\n\nDomain adaptation represents another critical challenge. While RAG systems show impressive performance on general knowledge tasks, adapting them to specialized domains requires careful consideration of domain-specific knowledge bases and retrieval mechanisms. Research has shown that models like RAG-end2end can address this challenge by updating all components during training when adapting to domain-specific knowledge bases <Paper corpusId=\"269149041\" paperTitle=\"(Weng, 2024)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>. However, this adaptation process requires balancing multiple optimization objectives and carefully constructing domain-appropriate knowledge bases.\n\nThe delicate interplay between retriever and generator components introduces integration challenges. Fine-tuning either component in isolation often leads to sub-optimal performance, as the system's effectiveness depends on the synergy between retrieval and generation processes <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper>. Optimizing this integration requires careful consideration of retrieval model selection, context length optimization, knowledge base construction, and appropriate LLM deployment <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>.\n\nRetrieval noise presents a particularly persistent challenge. Inappropriate retrieved passages can significantly hinder a language model's ability to generate high-quality responses. Research has identified three distinct types of retrieval noise reflective of real-world environments, each affecting model robustness differently <Paper corpusId=\"278339057\" paperTitle=\"(Shi et al., 2025)\" isShortName></Paper> <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>. Approaches like Retrieval-augmented Adaptive Adversarial Training (RAAT) attempt to address this challenge by dynamically adjusting the training process in response to different noise types <Paper corpusId=\"270199429\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.\n\nEvaluation methodologies for RAG fine-tuning vary widely across research, making direct comparison between approaches challenging. Some evaluation frameworks focus primarily on retrieval accuracy metrics, while others emphasize generation quality measures. More comprehensive evaluation approaches consider both aspects, evaluating the combined effectiveness of text generation and provenance retrieval <Paper corpusId=\"252568176\" paperTitle=\"(Hofstatter et al., 2022)\" isShortName></Paper>. The diversity of tasks to which RAG systems are applied\u2014from open-domain question answering to fact verification, slot-filling for relation extraction, and dialogue systems\u2014further complicates evaluation standardization <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>.\n\nDespite these challenges, recent advances in end-to-end optimization methodologies offer promising directions. Approaches like Stochastic RAG, which relaxes simplifying assumptions of marginalization and document independence through differentiable approximation for sampling without replacement, demonstrate effective end-to-end optimization across diverse tasks <Paper corpusId=\"269605438\" paperTitle=\"(Zamani et al., 2024)\" isShortName></Paper>. These integrated optimization approaches, though computationally intensive, appear increasingly necessary to realize the full potential of RAG systems.", "citations": [{"id": "(Cheng et al., 2025)", "paper": {"corpus_id": 277043297, "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingyue Cheng", "authorId": "1491233507"}, {"name": "Yucong Luo", "authorId": "2208917508"}, {"name": "Ouyang Jie", "authorId": "2322501286"}, {"name": "Qi Liu", "authorId": "2332691115"}, {"name": "Huijie Liu", "authorId": "2312648865"}, {"name": "Li Li", "authorId": "2291070758"}, {"name": "Shuo Yu", "authorId": "2322429208"}, {"name": "Bohou Zhang", "authorId": "2351226328"}, {"name": "Jiawei Cao", "authorId": "2350426005"}, {"name": "Jie Ma", "authorId": "2350427710"}, {"name": "Daoyu Wang", "authorId": "2322524150"}, {"name": "Enhong Chen", "authorId": "2258714945"}], "n_citations": 6}, "snippets": ["Static optimization involves training only one component of the RAG system while keeping the other static. This method is particularly advantageous in scenarios with limited computational resources or when rapid deployment is essential. For instance, fixing the retriever while optimizing the generator allows the system to benefit from established retrieval mechanisms, such as traditional BM25 [204] or pre-trained models like BERT [49], without the overhead of simultaneously training the retriever. This can lead to faster training cycles and reduced resource consumption.\n\nHowever, the primary drawback of static optimization is the potential compromise in overall system performance.\n\nSince only one component is being optimized, the synergy between retrieval and generation may not be fully realized, potentially limiting the model's ability to adapt to specific tasks or domains. To mitigate this, careful selection of the fixed component and the optimization process is essential to ensure that the evolving component can effectively leverage the fixed information."], "score": 0.86669921875}, {"id": "(Weng, 2024)", "paper": {"corpus_id": 269149041, "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Benjue Weng", "authorId": "2296715370"}], "n_citations": 10}, "snippets": ["\"RA-DIT: RETRIEVAL-AUGMENTED DUAL IN-STRUCTION TUNING [66]:\" A lightweight fine-tuning method that combines RAG and SFT is proposed to enhance the performance of retrieval-augmented language models.\n\n\"Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering [118]:\" This paper assesses the domain adaptability of RAG models in open-domain question answering (ODQA) tasks and proposes RAG-end2end, an extension of RAG that can adapt to specific domain knowledge bases by updating all components during training.\n\n\"RAG Vs Fine-Tuning Vs Both: A Guide For Optimizing LLM Performance [8]:\" This article provides a guide on the optimization strategies of RAG, fine-tuning, and their combina-"], "score": 0.86328125}, {"id": "(Siriwardhana et al., 2022)", "paper": {"corpus_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Rivindu Weerasekera", "authorId": "52001535"}, {"name": "Elliott Wen", "authorId": "2114425044"}, {"name": "Tharindu Kaluarachchi", "authorId": "1992921690"}, {"name": "R. Rana", "authorId": "1814487"}, {"name": "Suranga Nanayakkara", "authorId": "1486464114"}], "n_citations": 179}, "snippets": ["Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency."], "score": 0.0}, {"id": "(Shi et al., 2025)", "paper": {"corpus_id": 278339057, "title": "Direct Retrieval-augmented Optimization: Synergizing Knowledge Selection and Language Models", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Zhengliang Shi", "authorId": "2195381022"}, {"name": "Lingyong Yan", "authorId": "1387839383"}, {"name": "Weiwei Sun", "authorId": "2153198380"}, {"name": "Yue Feng", "authorId": "2326805997"}, {"name": "Pengjie Ren", "authorId": "1749477"}, {"name": "Xinyu Ma", "authorId": "2265517632"}, {"name": "Shuaiqiang Wang", "authorId": "2237948548"}, {"name": "Dawei Yin", "authorId": "2331316040"}, {"name": "M. D. Rijke", "authorId": "2265490493"}, {"name": "Zhaochun Ren", "authorId": "2261862546"}], "n_citations": 1}, "snippets": ["To optimize RAG performance, some studies improve knowledge accuracy by fine-tuning the retrieval or ranking model with relevance criteria [33,38,48]. Others enhance the robustness of LLMs against irrelevant content through supervised fine-tuning [10,71] or in-context learning [49], teaching them to summarize key points from retrieved documents. However, these approaches optimize either the selection or the generation component separately while neglecting a dual enhancement, which may lead to sub-optimal overall performance [27,45].\n\nTo address the above limitation, some recent studies train both the retriever and the LLM generator [23,25]51]."], "score": 0.904296875}, {"id": "(Lyu et al., 2024)", "paper": {"corpus_id": 267320876, "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models", "year": 2024, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Yuanjie Lyu", "authorId": "2187857206"}, {"name": "Zhiyu Li", "authorId": "2268429641"}, {"name": "Simin Niu", "authorId": "2268393907"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Bo Tang", "authorId": "2268400606"}, {"name": "Wenjin Wang", "authorId": "2117833477"}, {"name": "Hao Wu", "authorId": "2282083454"}, {"name": "Huan Liu", "authorId": "2304320758"}, {"name": "Tong Xu", "authorId": "2277237058"}, {"name": "Enhong Chen", "authorId": "2265580543"}], "n_citations": 40}, "snippets": ["Our study delves into the intricate balance required in the fine-tuning process of RAG systems, highlighting the importance of optimizing the retrieval model, context length, construction of the knowledge base, and the deployment of the underlying large language model to achieve the best results."], "score": 0.9140625}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 270199429, "title": "Enhancing Noise Robustness of Retrieval-Augmented Language Models with Adaptive Adversarial Training", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Feiteng Fang", "authorId": "2293319089"}, {"name": "Yuelin Bai", "authorId": "2287892865"}, {"name": "Shiwen Ni", "authorId": "2266469238"}, {"name": "Min Yang", "authorId": "2301170603"}, {"name": "Xiaojun Chen", "authorId": "2287803799"}, {"name": "Ruifeng Xu", "authorId": "2266809482"}], "n_citations": 39}, "snippets": ["Large Language Models (LLMs) exhibit substantial capabilities yet encounter challenges, including hallucination, outdated knowledge, and untraceable reasoning processes. Retrieval-augmented generation (RAG) has emerged as a promising solution, integrating knowledge from external databases to mitigate these challenges. However, inappropriate retrieved passages can potentially hinder the LLMs' capacity to generate comprehensive and high-quality responses. Prior RAG studies on the robustness of retrieval noises often confine themselves to a limited set of noise types, deviating from real-world retrieval environments and limiting practical applicability. In this study, we initially investigate retrieval noises and categorize them into three distinct types, reflecting real-world environments. We analyze the impact of these various retrieval noises on the robustness of LLMs. Subsequently, we propose a novel RAG approach known as Retrieval-augmented Adaptive Adversarial Training (RAAT). RAAT leverages adaptive adversarial training to dynamically adjust the model's training process in response to retrieval noises. Concurrently, it employs multi-task learning to ensure the model's capacity to internally recognize noisy contexts. Extensive experiments demonstrate that the LLaMA-2 7B model trained using RAAT exhibits significant improvements in F1 and EM scores under diverse noise conditions. For reproducibility, we release our code and data at: https://github.com/calubkk/RAAT."], "score": 0.0}, {"id": "(Hofstatter et al., 2022)", "paper": {"corpus_id": 252568176, "title": "FiD-Light: Efficient and Effective Retrieval-Augmented Text Generation", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Sebastian Hofst\u00e4tter", "authorId": "97393346"}, {"name": "Jiecao Chen", "authorId": "2809410"}, {"name": "K. Raman", "authorId": "2062947723"}, {"name": "Hamed Zamani", "authorId": "2499986"}], "n_citations": 82}, "snippets": ["Retrieval-augmented generation models offer many benefits over standalone language models: besides a textual answer to a given query they provide provenance items retrieved from an updateable knowledge base. However, they are also more complex systems and need to handle long inputs. In this work, we introduce FiD-Light to strongly increase the efficiency of the state-of-the-art retrieval-augmented FiD model, while maintaining the same level of effectiveness. Our FiD-Light model constrains the information flow from the encoder (which encodes passages separately) to the decoder (using concatenated encoded representations). Furthermore, we adapt FiD-Light with re-ranking capabilities through textual source pointers, to improve the top-ranked provenance precision. Our experiments on a diverse set of seven knowledge intensive tasks (KILT) show FiD-Light consistently improves the Pareto frontier between query latency and effectiveness. FiD-Light with source pointing sets substantial new state-of-the-art results on six KILT tasks for combined text generation and provenance retrieval evaluation, while maintaining high efficiency."], "score": 0.0}, {"id": "(Zamani et al., 2024)", "paper": {"corpus_id": 269605438, "title": "Stochastic RAG: End-to-End Retrieval-Augmented Generation through Expected Utility Maximization", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "Hamed Zamani", "authorId": "2293725953"}, {"name": "Michael Bendersky", "authorId": "2240516450"}], "n_citations": 29}, "snippets": ["This paper introduces Stochastic RAG--a novel approach for end-to-end optimization of retrieval-augmented generation (RAG) models that relaxes the simplifying assumptions of marginalization and document independence, made in most prior work. Stochastic RAG casts the retrieval process in RAG as a stochastic sampling without replacement process. Through this formulation, we employ straight-through Gumbel-top-k that provides a differentiable approximation for sampling without replacement and enables effective end-to-end optimization for RAG. We conduct extensive experiments on seven diverse datasets on a wide range of tasks, from open-domain question answering to fact verification to slot-filling for relation extraction and to dialogue systems. By applying this optimization method to a recent and effective RAG model, we advance state-of-the-art results on six out of seven datasets."], "score": 0.0}], "table": null}], "cost": 0.38608800000000004}}
