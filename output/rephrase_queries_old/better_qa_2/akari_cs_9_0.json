{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in synthetic data generation methods that target complex reasoning tasks such as logical inference or multi-step problem solving?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in synthetic data generation methods focused on complex reasoning tasks, such as logical inference and multi-step problem solving."}}, {"clarifying_question2": "Do you have a preferred application domain for synthetic data generation in hard reasoning problems, for example, NLP, computer vision, or mathematical problem solving?", "clarifying_answer2": {"clarifying_answer": "My preferred application domain is natural language processing (NLP), particularly for tasks involving logical reasoning or complex question answering."}}, {"clarifying_question3": "Are you looking for survey/review papers on synthetic data generation for reasoning, or specific methods and experiments addressing challenging reasoning benchmarks?", "clarifying_answer3": {"clarifying_answer": "I am looking for specific methods and experiments addressing challenging reasoning benchmarks."}}], "better_query": "What research papers present specific synthetic data generation methods for complex reasoning tasks in natural language processing, such as logical inference or multi-step question answering?", "better_answer": {"sections": [{"title": "Introduction", "tldr": "Synthetic data generation methods have become essential for advancing complex reasoning tasks in NLP research. These methods address data scarcity while enabling controlled experimentation with specific reasoning patterns across logical inference, multi-step reasoning, and mathematical problem-solving domains. (LLM Memory)", "text": "\nSynthetic data generation has emerged as a critical approach for developing and evaluating natural language processing (NLP) models capable of complex reasoning. While large language models are trained on vast amounts of text, they often struggle with specialized reasoning tasks that require logical consistency, multi-step inference chains, or mathematical operations. Research papers addressing this challenge have introduced various methodologies to generate synthetic datasets that can isolate and test specific reasoning capabilities. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese synthetic data generation methods serve multiple important purposes: they compensate for the scarcity of naturally occurring examples of complex reasoning, provide controlled experimental environments where specific reasoning patterns can be isolated and evaluated, and create benchmarks with known ground truth answers against which model performance can be measured objectively. The approaches typically involve either programmatic generation using templates and rules, bootstrapping from existing models to create more complex examples, or hybrid methods that combine human annotation with algorithmic data generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe research in this area spans across multiple reasoning domains including formal logic, mathematics, commonsense reasoning, and natural language inference, with each domain requiring specialized techniques for generating realistic yet challenging examples. The following sections will explore the specific methodologies employed across these different reasoning domains, highlighting key papers and their approaches to synthetic data generation. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Logical Reasoning Methods", "tldr": "Research in synthetic data generation for logical reasoning focuses on creating datasets that test deductive inference, propositional logic, and complex reasoning chains. Approaches range from template-based generation to graph-based synthetic data and specialized frameworks that transform logic problems into natural language. (13 sources)", "text": "\nSynthetic data generation for logical reasoning tasks has emerged as a crucial methodology for assessing and enhancing language models' capacity for formal reasoning. Several approaches have been developed to create controlled datasets that isolate specific aspects of logical reasoning. The RuleTaker corpus presents a pioneering approach, generating multistep deductive proofs in natural language where models must apply implication rules to a given set of facts to prove or disprove hypotheses <Paper corpusId=\"260846474\" paperTitle=\"(Morishita et al., 2023)\" isShortName></Paper> <Paper corpusId=\"211126663\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. Similarly, the Artificial Argument Corpus focuses on single-step deductive proofs using hand-selected rules for critical thinking <Paper corpusId=\"260846474\" paperTitle=\"(Morishita et al., 2023)\" isShortName></Paper>.\n\nGraph-based approaches have proven effective for generating complex reasoning chains. Zhou et al. developed a method that leverages random walk sampling algorithms on graphs to create reasoning questions with explicit reasoning chains <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. This approach is motivated by the insight that natural language reasoning tasks can be represented as structured data with finite nodes and edges, allowing for controlled generation of reasoning patterns <Paper corpusId=\"272753147\" paperTitle=\"(Zhou et al., 2024)\" isShortName></Paper>. Similarly, BoardgameQA presents a challenging dataset with contradictory facts and rules, requiring models to resolve conflicts through preference-based reasoning <Paper corpusId=\"259144942\" paperTitle=\"(Kazemi et al., 2023)\" isShortName></Paper>.\n\nSeveral innovative frameworks have been introduced to generate high-quality logical reasoning data. LogicPro transforms LeetCode-style algorithm problems and their program solutions into complex logical reasoning data in text format, creating difficult, scalable examples with golden standard answers and high-quality reasoning processes <Paper corpusId=\"272753174\" paperTitle=\"(Jiang et al., 2024)\" isShortName></Paper>. PRONTOQA generates question-answering examples from synthetic world models represented in first-order logic, enabling the parsing of reasoning chains into symbolic proofs for formal analysis <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent research has also focused on mitigating limitations in logical reasoning capabilities. Studies have revealed that large language models are surprisingly brittle to the ordering of premises in logical reasoning tasks, with performance drops of over 30% when premise order is changed <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This has led to the development of comprehensive evaluation benchmarks like LogicBench, which tests 25 different reasoning patterns across propositional, first-order, and non-monotonic logics <Paper corpusId=\"269330143\" paperTitle=\"(Parmar et al., 2024)\" isShortName></Paper>.\n\nAdversarial data collection methods have also proven valuable, as demonstrated in the development of datasets like ANLI, where human annotators iteratively find weaknesses in current models <Paper corpusId=\"207756753\" paperTitle=\"(Nie et al., 2019)\" isShortName></Paper>. This creates a \"moving target\" that continuously challenges model capabilities. Other approaches combine language models with symbolic solvers, as in Logic-LM, which shows significant performance improvements on logical reasoning tasks by translating natural language problems into symbolic formulations and then using deterministic solvers <Paper corpusId=\"258833332\" paperTitle=\"(Pan et al., 2023)\" isShortName></Paper>.\n\nSome methods focus on generating data with explicit reasoning paths. ENTAILMENTBANK contains multistep entailment trees that show how intermediate conclusions are derived from known facts to reach the final answer <Paper corpusId=\"233297051\" paperTitle=\"(Dalvi et al., 2021)\" isShortName></Paper>. Similarly, Saeed et al. developed a dataset for teaching language models to reason with soft Horn rules, enabling predictions with precise probabilities <Paper corpusId=\"237940562\" paperTitle=\"(Saeed et al., 2021)\" isShortName></Paper>. Bao et al. used contrastive learning on a synthetic dataset containing different combinations of entities, relations, and attributes to generate controllable logical sentences <Paper corpusId=\"258833579\" paperTitle=\"(Bao et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Morishita et al., 2023)", "paper": {"corpus_id": 260846474, "title": "Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Terufumi Morishita", "authorId": "1379579811"}, {"name": "Gaku Morio", "authorId": "29347584"}, {"name": "Atsuki Yamaguchi", "authorId": "145412147"}, {"name": "Yasuhiro Sogawa", "authorId": "2106369"}], "n_citations": 25}, "snippets": ["RuleTaker (Clark et al., 2021) proposed a deduction corpus composed of synthetically generated multistep deductive proofs written in natural languages. Each deductive proof (dis-)proves a hypothesis by applying deduction rules multiple times to a given set of facts. As the deduction rules, rules of the implication kind were used.\n\nArtificial Argument Corpus (Betz et al., 2021) is another corpus composed of synthetically generated single-step deductive proofs. As the deduction rules, hand-selected rules useful for critical thinking were used.\n\nGontier et al. (2020) investigated the deductive reasoning ability of LMs on a corpus, which is composed of a specific type of multistep inference: kinship relationship on synthetic kinship graphs."], "score": 0.8564453125}, {"id": "(Clark et al., 2020)", "paper": {"corpus_id": 211126663, "title": "Transformers as Soft Reasoners over Language", "year": 2020, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Peter Clark", "authorId": "48323507"}, {"name": "Oyvind Tafjord", "authorId": "3385516"}, {"name": "Kyle Richardson", "authorId": "46666605"}], "n_citations": 361}, "snippets": ["Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering."], "score": 0.0}, {"id": "(Zhou et al., 2024)", "paper": {"corpus_id": 272753147, "title": "Enhancing Logical Reasoning in Large Language Models through Graph-based Synthetic Data", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jiaming Zhou", "authorId": "2297828274"}, {"name": "Abbas Ghaddar", "authorId": "2321867953"}, {"name": "Ge Zhang", "authorId": "2321875142"}, {"name": "Liheng Ma", "authorId": "1892081076"}, {"name": "Yaochen Hu", "authorId": "2288403553"}, {"name": "Soumyasundar Pal", "authorId": "38939190"}, {"name": "Mark Coates", "authorId": "2287938359"}, {"name": "Bin Wang", "authorId": "2321911323"}, {"name": "Yingxue Zhang", "authorId": "2275529643"}, {"name": "Jianye Hao", "authorId": "2307072418"}], "n_citations": 4}, "snippets": ["Despite recent advances in training and prompting strategies for Large Language Models (LLMs), these models continue to face challenges with complex logical reasoning tasks that involve long reasoning chains. In this work, we explore the potential and limitations of using graph-based synthetic reasoning data as training signals to enhance LLMs' reasoning capabilities.\n\nMotivated by the fact that natural language reasoning tasks can be represented as structured data with finite nodes and edges (Jin et al., 2024), and inspired by existing works on constructing reasoning benchmarks (Fatemi et al., 2024;Kazemi et al., 2023;Agrawal et al., 2024), we propose to leverage synthetic graph-based data for task-specific posttraining adaptation to improve the correctness of the generated reasoning questions and labels.\n\nIn this paper, we carefully design a random walk sampling algorithm on graphs and introduce a new prompting strategy that first extracts a reasoning chain and then derives the answer."], "score": 0.9267578125}, {"id": "(Kazemi et al., 2023)", "paper": {"corpus_id": 259144942, "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mehran Kazemi", "authorId": "2173102324"}, {"name": "Quan Yuan", "authorId": "2117782275"}, {"name": "Deepti Bhatia", "authorId": "2055505691"}, {"name": "Najoung Kim", "authorId": "8756748"}, {"name": "Xin Xu", "authorId": null}, {"name": "Vaiva Imbrasaite", "authorId": "2410945"}, {"name": "Deepak Ramachandran", "authorId": "143812128"}], "n_citations": 50}, "snippets": ["Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor."], "score": 0.0}, {"id": "(Jiang et al., 2024)", "paper": {"corpus_id": 272753174, "title": "LogicPro: Improving Complex Logical Reasoning via Program-Guided Learning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jin Jiang", "authorId": "2319385003"}, {"name": "Yuchen Yan", "authorId": "2284984220"}, {"name": "Yang Liu", "authorId": "2316670312"}, {"name": "Yonggang Jin", "authorId": "2321879179"}, {"name": "Shuai Peng", "authorId": "2072715089"}, {"name": "Mengdi Zhang", "authorId": "2284132141"}, {"name": "Xunliang Cai", "authorId": "2317059694"}, {"name": "Yixin Cao", "authorId": "2319459835"}, {"name": "Liangcai Gao", "authorId": "2165982472"}, {"name": "Zhi Tang", "authorId": "2268046262"}], "n_citations": 7}, "snippets": ["In this paper, we propose a new data synthesis method called \\textbf{LogicPro}, which leverages LeetCode-style algorithm \\underline{Pro}blems and their corresponding \\underline{Pro}gram solutions to synthesize Complex \\underline{Logic}al Reasoning data in text format. First, we synthesize complex reasoning problems through source algorithm problems and test cases. Then, standard answers and intermediate variable outputs are obtained for each problem based on standard python solutions and test cases. Finally, with the guidance of code intermediate variables, we synthesize the text reasoning process for each reasoning problems. Through this method, we can synthesize data that is difficult, scalable, effective, and comes with golden standard answers and high-quality reasoning processes."], "score": 0.8291015625}, {"id": "(Xie et al., 2024)", "paper": {"corpus_id": 273695832, "title": "On Memorization of Large Language Models in Logical Reasoning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chulin Xie", "authorId": "2308424593"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}, {"name": "Da Yu", "authorId": "2290010050"}, {"name": "Xinyun Chen", "authorId": "2328442132"}, {"name": "Bill Yuchen Lin", "authorId": "2328296250"}, {"name": "Bo Li", "authorId": "2290141407"}, {"name": "Badih Ghazi", "authorId": "2529354"}, {"name": "Ravi Kumar", "authorId": "2237408905"}], "n_citations": 41}, "snippets": ["Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person."], "score": 0.82080078125}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267657940, "title": "Premise Order Matters in Reasoning with Large Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Xinyun Chen", "authorId": "2238263119"}, {"name": "Ryan A. Chi", "authorId": "2284066085"}, {"name": "Xuezhi Wang", "authorId": "2238394232"}, {"name": "Denny Zhou", "authorId": "2256313467"}], "n_citations": 32}, "snippets": ["Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."], "score": 0.0}, {"id": "(Parmar et al., 2024)", "paper": {"corpus_id": 269330143, "title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Mihir Parmar", "authorId": "1423660254"}, {"name": "Nisarg Patel", "authorId": "2218094729"}, {"name": "Neeraj Varshney", "authorId": "2067056655"}, {"name": "Mutsumi Nakamura", "authorId": "2287764"}, {"name": "Man Luo", "authorId": "145779426"}, {"name": "Santosh Mashetty", "authorId": "2219861482"}, {"name": "Arindam Mitra", "authorId": "2146720788"}, {"name": "Chitta Baral", "authorId": "2064619864"}], "n_citations": 31}, "snippets": ["Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really\"reason\"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench."], "score": 0.0}, {"id": "(Nie et al., 2019)", "paper": {"corpus_id": 207756753, "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding", "year": 2019, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yixin Nie", "authorId": "40383658"}, {"name": "Adina Williams", "authorId": "81840293"}, {"name": "Emily Dinan", "authorId": "31461304"}, {"name": "Mohit Bansal", "authorId": "143977268"}, {"name": "J. Weston", "authorId": "145183709"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 1011}, "snippets": ["We introduce a new large-scale NLI benchmark dataset, collected via an iterative, adversarial human-and-model-in-the-loop procedure. We show that training models on this new dataset leads to state-of-the-art performance on a variety of popular NLI benchmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-the-art models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate."], "score": 0.0}, {"id": "(Pan et al., 2023)", "paper": {"corpus_id": 258833332, "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Liangming Pan", "authorId": "3470231"}, {"name": "Alon Albalak", "authorId": "2044198106"}, {"name": "Xinyi Wang", "authorId": "2115553132"}, {"name": "William Yang Wang", "authorId": "1682479"}], "n_citations": 270}, "snippets": ["Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM."], "score": 0.0}, {"id": "(Dalvi et al., 2021)", "paper": {"corpus_id": 233297051, "title": "Explaining Answers with Entailment Trees", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Bhavana Dalvi", "authorId": "40135250"}, {"name": "Peter Alexander Jansen", "authorId": "144949918"}, {"name": "Oyvind Tafjord", "authorId": "3385516"}, {"name": "Zhengnan Xie", "authorId": "9746804"}, {"name": "Hannah Smith", "authorId": "2110645880"}, {"name": "Leighanna Pipatanangkura", "authorId": "2078502192"}, {"name": "Peter Clark", "authorId": "48323507"}], "n_citations": 185}, "snippets": ["Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \"rationale\"). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."], "score": 0.0}, {"id": "(Saeed et al., 2021)", "paper": {"corpus_id": 237940562, "title": "RuleBERT: Teaching Soft Rules to Pre-Trained Language Models", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mohammed Saeed", "authorId": "2073358417"}, {"name": "N. Ahmadi", "authorId": "84069926"}, {"name": "Preslav Nakov", "authorId": "1683562"}, {"name": "Paolo Papotti", "authorId": "1802817"}], "n_citations": 33}, "snippets": ["While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets."], "score": 0.0}, {"id": "(Bao et al., 2023)", "paper": {"corpus_id": 258833579, "title": "Abstract Meaning Representation-Based Logic-Driven Data Augmentation for Logical Reasoning", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Qiming Bao", "authorId": "1491516930"}, {"name": "A. Peng", "authorId": "2064347311"}, {"name": "Zhenyun Deng", "authorId": "2244794"}, {"name": "Wanjun Zhong", "authorId": "81970097"}, {"name": "N. Tan", "authorId": "2151063121"}, {"name": "Nathan Young", "authorId": "2159676415"}, {"name": "Yang Chen", "authorId": "2144353312"}, {"name": "Yonghua Zhu", "authorId": "2116513959"}, {"name": "Michael Witbrock", "authorId": "2582677"}, {"name": "Jiamou Liu", "authorId": "2108326935"}], "n_citations": 4}, "snippets": ["In this paper, we performed contrastive learning for discriminative large language models on sentences augmented from a synthetic dataset. This dataset contains 14,962 sentences with different combinations of 23 entities, 2 relations and 40 attributes. Synthetic data was used to generate more controllable logical sentences."], "score": 0.86328125}], "table": null}, {"title": "Multi-Step Question Answering Approaches", "tldr": "Researchers have developed diverse synthetic data generation methods for multi-step question answering, including template-based approaches, neurosymbolic techniques, and self-training frameworks that leverage semi-structured data to create examples requiring complex reasoning chains. (17 sources)", "text": "\nMulti-step question answering presents a significant challenge in NLP, requiring models to perform complex reasoning across multiple facts or inferences. To address this challenge, researchers have developed various synthetic data generation approaches. Early pioneering efforts include the bAbI dataset, which uses random simulation to generate facts, questions, and answers for training models on simple logical reasoning tasks <Paper corpusId=\"3178759\" paperTitle=\"(Weston et al., 2015)\" isShortName></Paper>. Similarly, Hermann et al. developed methods to automatically generate reading comprehension questions from news-summarization pairs <Paper corpusId=\"6203757\" paperTitle=\"(Hermann et al., 2015)\" isShortName></Paper>, while Serban et al. used RNNs to generate 30 million factoid questions about Freebase for training question-answering systems <Paper corpusId=\"12241221\" paperTitle=\"(Serban et al., 2016)\" isShortName></Paper> <Paper corpusId=\"10817864\" paperTitle=\"(Wang et al., 2016)\" isShortName></Paper>.\n\nSemi-structured data has emerged as a valuable resource for generating complex reasoning examples. Yoran et al. proposed leveraging semi-structured tables to automatically generate question-paragraph pairs at scale, where answering requires reasoning over multiple facts <Paper corpusId=\"235898880\" paperTitle=\"(Yoran et al., 2021)\" isShortName></Paper>. Building on this approach, Dua et al. developed a method to synthetically generate complex questions and their decompositions using Wikipedia tables, creating data for ten simple operations (COUNT, FILTER, SUM, etc.) and their higher-order combinations <Paper corpusId=\"254408974\" paperTitle=\"(Dua et al., 2022)\" isShortName></Paper>. Li et al. presented a self-training framework for unsupervised complex tabular reasoning that generates diverse synthetic data with complex logic through program execution on tables and transformation into natural language sentences <Paper corpusId=\"254877131\" paperTitle=\"(Li et al., 2022)\" isShortName></Paper>.\n\nRecent approaches have focused on generating synthetic reasoning chains. Trivedi et al. developed TeaBReaC, a pretraining dataset containing 525K multi-step questions with associated formal programs covering about 900 reasoning patterns, carefully designed to avoid common reasoning shortcuts <Paper corpusId=\"253370208\" paperTitle=\"(Trivedi et al., 2022)\" isShortName></Paper>. Similarly, MuSR introduced a neurosymbolic synthetic-to-natural generation algorithm for creating complex reasoning instances, such as murder mysteries approximately 1,000 words in length that challenge even advanced models like GPT-4 <Paper corpusId=\"264439655\" paperTitle=\"(Sprague et al., 2023)\" isShortName></Paper>.\n\nSeveral frameworks leverage existing models to generate synthetic data. CONDA reformulates data generation as context generation for given question-answer pairs and uses QA datasets for training context generators, finding that datasets requiring high-level reasoning abilities tend to provide the best performance boost <Paper corpusId=\"253107930\" paperTitle=\"(Mekala et al., 2022)\" isShortName></Paper>. Automate-CoT proposes a fully automatic pipeline that uses powerful language models to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars <Paper corpusId=\"268249187\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. Similarly, Source2Synth produces synthetic data points with intermediate reasoning steps grounded in real-world sources, improving performance by 22.57% for multi-hop question answering on HotPotQA compared to fine-tuned baselines <Paper corpusId=\"272600015\" paperTitle=\"(Lupidi et al., 2024)\" isShortName></Paper>.\n\nSpecialized reasoning strategies have also been explored. Self-Ask introduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions, demonstrating improved reasoning performance on multi-step inference tasks <Paper corpusId=\"277104955\" paperTitle=\"(Nadas et al., 2025)\" isShortName></Paper>. Step-Wise Reinforcement Learning (SWiRL) iteratively generates multi-step reasoning data and learns from it by breaking each multi-step trajectory into multiple sub-trajectories, showing significant performance improvements on various multi-step tasks including HotPotQA and MuSiQue <Paper corpusId=\"277622127\" paperTitle=\"(Goldie et al., 2025)\" isShortName></Paper>. For non-mathematical domains, Zeng et al. devised a pipeline to generate synthetic reasoning chains-of-thought from existing question-answering data, with step-wise labels based on reasoning correctness <Paper corpusId=\"276250232\" paperTitle=\"(Zeng et al., 2025)\" isShortName></Paper>.\n\nDomain-specific applications have also been explored, such as geometric reasoning in the context of question-answering. Goyal et al. introduced the Dynamic Spatial Memory Network and proposed two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate geometric reasoning capabilities <Paper corpusId=\"44096233\" paperTitle=\"(Goyal et al., 2018)\" isShortName></Paper>. Similarly, COM2 created a dataset by sampling multi-hop logical queries from an existing commonsense knowledge graph and verbalizing them using handcrafted rules and large language models <Paper corpusId=\"268363344\" paperTitle=\"(Fang et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Weston et al., 2015)", "paper": {"corpus_id": 3178759, "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "year": 2015, "venue": "International Conference on Learning Representations", "authors": [{"name": "J. Weston", "authorId": "145183709"}, {"name": "Antoine Bordes", "authorId": "1713934"}, {"name": "S. Chopra", "authorId": "3295092"}, {"name": "Tomas Mikolov", "authorId": "2047446108"}], "n_citations": 1182}, "snippets": ["One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks."], "score": 0.0}, {"id": "(Hermann et al., 2015)", "paper": {"corpus_id": 6203757, "title": "Teaching Machines to Read and Comprehend", "year": 2015, "venue": "Neural Information Processing Systems", "authors": [{"name": "Karl Moritz Hermann", "authorId": "2910877"}, {"name": "Tom\u00e1s Kocisk\u00fd", "authorId": "2367821"}, {"name": "Edward Grefenstette", "authorId": "1864353"}, {"name": "L. Espeholt", "authorId": "2311318"}, {"name": "W. Kay", "authorId": "2062879616"}, {"name": "Mustafa Suleyman", "authorId": "2573615"}, {"name": "Phil Blunsom", "authorId": "1685771"}], "n_citations": 3553}, "snippets": ["Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure."], "score": 0.0}, {"id": "(Serban et al., 2016)", "paper": {"corpus_id": 12241221, "title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus", "year": 2016, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Iulian Serban", "authorId": "35224828"}, {"name": "Alberto Garc\u00eda-Dur\u00e1n", "authorId": "1405061488"}, {"name": "\u00c7aglar G\u00fcl\u00e7ehre", "authorId": "1854385"}, {"name": "Sungjin Ahn", "authorId": "3103594"}, {"name": "A. Chandar", "authorId": "144631588"}, {"name": "Aaron C. Courville", "authorId": "1760871"}, {"name": "Yoshua Bengio", "authorId": "1751762"}], "n_citations": 288}, "snippets": ["Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions."], "score": 0.0}, {"id": "(Wang et al., 2016)", "paper": {"corpus_id": 10817864, "title": "The Galactic Dependencies Treebanks: Getting More Data by Synthesizing New Languages", "year": 2016, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "D. Wang", "authorId": "2099597"}, {"name": "Jason Eisner", "authorId": "145043214"}], "n_citations": 58}, "snippets": ["Synthetic datasets have also arisen recently for semantic tasks in natural language processing. bAbI is a dataset of facts, questions, and answers, generated by random simulation, for training machines to do simple logic (Weston et al., 2015)(Hermann et al., 2015) generate reading comprehension questions and their answers, based on a large set of newssummarization pairs, for training machine readers. (Serban et al., 2016) used RNNs to generate 30 million factoid questions about Freebase, with answers, for training question-answering systems. (Wang et al., 2015) obtain data to train semantic parsers in a new domain by first generating synthetic (utterance, logical form) pairs and then asking human annotators to paraphrase the synthetic utterances into more natural human language."], "score": 0.81884765625}, {"id": "(Yoran et al., 2021)", "paper": {"corpus_id": 235898880, "title": "Turning Tables: Generating Examples from Semi-structured Tables for Endowing Language Models with Reasoning Skills", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ori Yoran", "authorId": "2074100429"}, {"name": "Alon Talmor", "authorId": "12371246"}, {"name": "Jonathan Berant", "authorId": "1750652"}], "n_citations": 54}, "snippets": ["In this work, we propose to leverage semi-structured tables, and automatically generate at scale question-paragraph pairs, where answering the question requires reasoning over multiple facts in the paragraph. We add a pre-training step over this synthetic data, which includes examples that require 16 different reasoning skills such as number comparison, conjunction, and fact composition. To improve data efficiency, we sample examples from reasoning skills where the model currently errs."], "score": 0.85302734375}, {"id": "(Dua et al., 2022)", "paper": {"corpus_id": 254408974, "title": "Successive Prompting for Decomposing Complex Questions", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeru Dua", "authorId": "33546336"}, {"name": "Shivanshu Gupta", "authorId": "1698760333"}, {"name": "Sameer Singh", "authorId": "34650964"}, {"name": "Matt Gardner", "authorId": "40642935"}], "n_citations": 118}, "snippets": ["To mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\n\nGeneration Process: Inspired by Yoran et al. (2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first order simple questions and a combination of columns for higher order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION.\n\nWe generate higher order combinations of firstorder operations, wherever possible."], "score": 0.861328125}, {"id": "(Li et al., 2022)", "paper": {"corpus_id": 254877131, "title": "Optimization Techniques for Unsupervised Complex Table Reasoning via Self-Training Framework", "year": 2022, "venue": "IEEE Transactions on Knowledge and Data Engineering", "authors": [{"name": "Zhenyu Li", "authorId": "2155354022"}, {"name": "Xiuxing Li", "authorId": "2116521868"}, {"name": "Sunqi Fan", "authorId": "2272781120"}, {"name": "Jianyong Wang", "authorId": "2115642141"}], "n_citations": 6}, "snippets": ["To address the insufficient annotation challenge, we present a self-training framework for unsupervised complex tabular reasoning (UCTR-ST) by generating diverse synthetic data with complex logic. Specifically, UCTR-ST incorporates several essential techniques: we aggregate diverse programs and execute them on tables based on a \"Program-Management\" component, and we bridge the gap between programs and text with a powerful \"Program-Transformation\" module that generates natural language sentences with complex logic."], "score": 0.88037109375}, {"id": "(Trivedi et al., 2022)", "paper": {"corpus_id": 253370208, "title": "Teaching Broad Reasoning Skills for Multi-Step QA by Generating Hard Contexts", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "H. Trivedi", "authorId": "6365809"}, {"name": "Niranjan Balasubramanian", "authorId": "35217367"}, {"name": "Tushar Khot", "authorId": "2236429"}, {"name": "Ashish Sabharwal", "authorId": "48229640"}], "n_citations": 11}, "snippets": ["In this work, we show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid common reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns."], "score": 0.93310546875}, {"id": "(Sprague et al., 2023)", "paper": {"corpus_id": 264439655, "title": "MuSR: Testing the Limits of Chain-of-thought with Multistep Soft Reasoning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Zayne Sprague", "authorId": "2150345078"}, {"name": "Xi Ye", "authorId": "50183897"}, {"name": "Kaj Bostrom", "authorId": "1620502132"}, {"name": "Swarat Chaudhuri", "authorId": "35865989"}, {"name": "Greg Durrett", "authorId": "1814094"}], "n_citations": 78}, "snippets": ["We introduce MuSR, a dataset for evaluating language models on multistep soft reasoning tasks specified in a natural language narrative. This dataset has two crucial features. First, it is created through a novel neurosymbolic synthetic-to-natural generation algorithm, enabling the construction of complex reasoning instances that challenge GPT-4 (e.g., murder mysteries roughly 1000 words in length) and which can be scaled further as more capable LLMs are released."], "score": 0.810546875}, {"id": "(Mekala et al., 2022)", "paper": {"corpus_id": 253107930, "title": "Leveraging QA Datasets to Improve Generative Data Augmentation", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Dheeraj Mekala", "authorId": "7565696"}, {"name": "Tu Vu", "authorId": "144244743"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Jingbo Shang", "authorId": "2884976"}], "n_citations": 18}, "snippets": ["In this work, we propose CONDA, an approach to further improve GLM's ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks", "Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings."], "score": 0.95458984375}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 268249187, "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges", "year": 2024, "venue": "", "authors": [{"name": "Bosheng Ding", "authorId": "2064493724"}, {"name": "Chengwei Qin", "authorId": "2084609980"}, {"name": "Ruochen Zhao", "authorId": "2091437375"}, {"name": "Tianze Luo", "authorId": "2290030532"}, {"name": "Xinze Li", "authorId": "2290023354"}, {"name": "Guizhen Chen", "authorId": "2290026474"}, {"name": "Wenhan Xia", "authorId": "2276610995"}, {"name": "Junjie Hu", "authorId": "2290145274"}, {"name": "A. Luu", "authorId": "1755919"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}], "n_citations": 20}, "snippets": ["Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars."], "score": 0.77392578125}, {"id": "(Lupidi et al., 2024)", "paper": {"corpus_id": 272600015, "title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "A. Lupidi", "authorId": "2223643390"}, {"name": "Carlos Gemmell", "authorId": "2320807392"}, {"name": "Nicola Cancedda", "authorId": "2313189469"}, {"name": "Jane Dwivedi-Yu", "authorId": "2173509991"}, {"name": "Jason E Weston", "authorId": "2267341626"}, {"name": "Jakob Foerster", "authorId": "2320805323"}, {"name": "R. Raileanu", "authorId": "48647153"}, {"name": "Maria Lomeli", "authorId": "2253400960"}], "n_citations": 8}, "snippets": ["Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines."], "score": 0.9638671875}, {"id": "(Nadas et al., 2025)", "paper": {"corpus_id": 277104955, "title": "Synthetic Data Generation Using Large Language Models: Advances in Text and Code", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mihai Nadas", "authorId": "2350754914"}, {"name": "Laura Dio\u015fan", "authorId": "2306585020"}, {"name": "Andreea Tomescu", "authorId": "2350756350"}], "n_citations": 3}, "snippets": ["Self-Ask [47] Multi-hop Question Answering (QA), Reasoning \n\nIntroduced an LLM-based self-asking strategy where models recursively break down complex questions into simpler sub-questions. Demonstrated improved reasoning performance on multi-step inference tasks by leveraging self-generated sub-questions for enhanced accuracy.\n\nCoT [13] Scientific QA, Logical Reasoning \n\nExamined how Chain-of-Thought (CoT) prompting improves LLM reasoning capabilities. Proposed Sci-CoT, a two-stage framework where intermediate rationales are generated separately from answer inference, improving efficiency and model performance."], "score": 0.94775390625}, {"id": "(Goldie et al., 2025)", "paper": {"corpus_id": 277622127, "title": "Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Anna Goldie", "authorId": "46684455"}, {"name": "Azalia Mirhoseini", "authorId": "1861312"}, {"name": "Hao Zhou", "authorId": "2354343696"}, {"name": "Irene Cai", "authorId": "2354178736"}, {"name": "Christopher D. Manning", "authorId": "2262643057"}], "n_citations": 11}, "snippets": ["We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively."], "score": 0.91943359375}, {"id": "(Zeng et al., 2025)", "paper": {"corpus_id": 276250232, "title": "VersaPRM: Multi-Domain Process Reward Model via Synthetic Reasoning Data", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Thomas Zeng", "authorId": "2323820473"}, {"name": "Shuibai Zhang", "authorId": "2190935184"}, {"name": "Shutong Wu", "authorId": "2344794414"}, {"name": "Christian Classen", "authorId": "2344760610"}, {"name": "Daewon Chae", "authorId": "2253659910"}, {"name": "Ethan Ewer", "authorId": "2323781863"}, {"name": "Minjae Lee", "authorId": "2345324748"}, {"name": "Heeju Kim", "authorId": "2344831673"}, {"name": "Wonjun Kang", "authorId": "2282472476"}, {"name": "Jackson Kunde", "authorId": "2334358539"}, {"name": "Ying Fan", "authorId": "2344807639"}, {"name": "Jungtaek Kim", "authorId": "2344788904"}, {"name": "Hyung Il Koo", "authorId": "2282471620"}, {"name": "K. Ramchandran", "authorId": "144161012"}, {"name": "Dimitris Papailiopoulos", "authorId": "107579958"}, {"name": "Kangwook Lee", "authorId": "2323790154"}], "n_citations": 4}, "snippets": ["In order to obtain step-wise reasoning data for non-Math domains, we devise a pipeline, as outlined in Figure 2, to generate synthetic reasoning CoTs from existing questionanswering data. These CoTs are then given step-wise labels based on reasoning correctness. We detail the synthetic data generation process in Sections 5.1 and 5.2, including methods to create and annotate reasoning steps."], "score": 0.90185546875}, {"id": "(Goyal et al., 2018)", "paper": {"corpus_id": 44096233, "title": "Think Visually: Question Answering through Virtual Imagery", "year": 2018, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Ankit Goyal", "authorId": "47989608"}, {"name": "Jian Wang", "authorId": "2152769770"}, {"name": "Jia Deng", "authorId": "153302678"}], "n_citations": 2}, "snippets": ["In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems."], "score": 0.80419921875}, {"id": "(Fang et al., 2024)", "paper": {"corpus_id": 268363344, "title": "Complex Reasoning over Logical Queries on Commonsense Knowledge Graphs", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Tianqing Fang", "authorId": "2044202073"}, {"name": "Zeming Chen", "authorId": "2111435018"}, {"name": "Yangqiu Song", "authorId": "2241325169"}, {"name": "Antoine Bosselut", "authorId": "2691021"}], "n_citations": 14}, "snippets": ["we present COM2 (COMplex COMmonsense), a new dataset created by sampling multi-hop logical queries (e.g., the joint effect or cause of both event A and B, or the effect of the effect of event C) from an existing commonsense knowledge graph (CSKG), and verbalizing them using handcrafted rules and large language models into multiple-choice and text generation questions."], "score": 0.81787109375}], "table": null}, {"title": "Mathematical Reasoning Techniques", "tldr": "Synthetic data generation for mathematical reasoning employs diverse approaches including complexity augmentation, symbolic replacement, and knowledge-driven synthesis. These techniques leverage existing datasets through bootstrapping, self-verification, and backward-forward generation processes to create high-quality training examples with explicit reasoning steps. (9 sources)", "text": "\nMathematical reasoning represents a particularly challenging domain for language models, requiring precise calculations, logical deductions, and procedural problem-solving. To address these challenges, researchers have developed specialized synthetic data generation techniques. One prominent approach involves augmenting the complexity of existing mathematical problems. WizardMath leverages GPT-3.5 to increase the complexity of questions and answers, while MetaMath bootstraps questions from established datasets like MATH and GSM8K by rewriting them through semantic rephrasing, self-verification, and backward reasoning <Paper corpusId=\"269042851\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>. Similarly, Wei et al. enhanced natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500,000 synthetic examples that significantly improved model performance on unseen in-context learning and algorithmic reasoning tasks <Paper corpusId=\"269042851\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper>.\n\nSeveral studies have focused on generating high-quality intermediate reasoning steps to improve mathematical problem-solving. Recent approaches leverage extensive synthetic datasets either distilled from cutting-edge models or composed of self-generated rationales to fine-tune language models <Paper corpusId=\"270870071\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>. One notable technique is Synthetic prompting, which uses language models to generate synthetic examples that augment hand-crafted examples in few-shot settings. This method employs a two-step process: (1) a backward step where the model synthesizes a query based on a self-generated reasoning chain, and (2) a forward step where it generates a more accurate reasoning chain for the synthesized query <Paper corpusId=\"271270817\" paperTitle=\"(Vatsal et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256459681\" paperTitle=\"(Shao et al., 2023)\" isShortName></Paper>. This approach has achieved up to 15.6% absolute gains on mathematical problem-solving, commonsense reasoning, and logical reasoning tasks <Paper corpusId=\"271270817\" paperTitle=\"(Vatsal et al., 2024)\" isShortName></Paper>.\n\nAutomated data synthesis frameworks have emerged to generate mathematical benchmarks without human annotations. ReSo, for example, has demonstrated impressive results, achieving 33.7% accuracy on Math-MAS and 32.3% accuracy on SciBench-MAS, outperforming existing methods that struggled with these challenging datasets <Paper corpusId=\"276768281\" paperTitle=\"(Zhou et al., 2025)\" isShortName></Paper>. To address the scarcity of high-quality mathematical reasoning data, researchers have explored various synthesis methods. Mainstream approaches involve data augmentation based on existing annotated training sets like GSM8K and MATH <Paper corpusId=\"278171321\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper> <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>.\n\nRecent innovation has focused on enhancing data diversity beyond traditional augmentation techniques. The Key-Point-Driven Data Synthesis (KPDDS) framework synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources, resulting in KPMath, an extensive synthetic dataset with over 800,000 question-answer pairs <Paper corpusId=\"268247488\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. Similarly, MathScale creates high-quality mathematical reasoning data using frontier language models by first extracting topics and knowledge points from seed math questions to build a concept graph, which is then used to generate new math questions. This approach has demonstrated effective scalability, resulting in MathScaleQA, a dataset containing two million math question-answer pairs <Paper corpusId=\"268247902\" paperTitle=\"(Tang et al., 2024)\" isShortName></Paper>.\n\nAnother promising direction involves evolutionary approaches to data generation. Auto Evol-Instruct proposes an end-to-end framework that evolves instruction datasets using large language models without human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>. Experiments demonstrate that methods optimized by Auto Evol-Instruct outperform human-designed methods on various benchmarks, including GSM8K <Paper corpusId=\"270213007\" paperTitle=\"(Zeng et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Liu et al., 2024)", "paper": {"corpus_id": 269042851, "title": "Best Practices and Lessons Learned on Synthetic Data for Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ruibo Liu", "authorId": "7247867"}, {"name": "Jerry Wei", "authorId": "2254934700"}, {"name": "Fangyu Liu", "authorId": "144097210"}, {"name": "Chenglei Si", "authorId": "2290019998"}, {"name": "Yanzhe Zhang", "authorId": "2121290295"}, {"name": "Jinmeng Rao", "authorId": "2238635148"}, {"name": "Steven Zheng", "authorId": "2275574241"}, {"name": "Daiyi Peng", "authorId": "2283933546"}, {"name": "Diyi Yang", "authorId": "2254124342"}, {"name": "Denny Zhou", "authorId": "2263221363"}, {"name": "Andrew M. Dai", "authorId": "2253757717"}], "n_citations": 96}, "snippets": ["WizardMath (Luo et al., 2023a) leverages a series of operations to increase the complexity of questions and answers using GPT-3.5, while MetaMath (Yu et al., 2023) bootstraps the questions in MATH and GSM8K by rewriting them in different ways, such as semantic rephrasing, self-verification, and backward reasoning", "Wei et al. (2023a) augmented existing natural language datasets by replacing natural language labels with arbitrary symbols, generating over 500k synthetic examples. Using these synthetic data for supervised finetuning significantly improved model performance on unseen in-context learning and algorithmic-reasoning tasks."], "score": 0.89013671875}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 270870071, "title": "Advancing Process Verification for Large Language Models via Tree-Based Preference Learning", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mingqian He", "authorId": "2309218600"}, {"name": "Yongliang Shen", "authorId": "1471660296"}, {"name": "Wenqi Zhang", "authorId": "2135282890"}, {"name": "Zeqi Tan", "authorId": "2092670555"}, {"name": "Weiming Lu", "authorId": "1776903"}], "n_citations": 7}, "snippets": ["To further enhance the accuracy of these intermediate steps, recent studies leverage extensive synthetic datasets, which are either distilled from cutting-edge models (Yu et al., 2023b;Luo et al., 2023) or composed of selfgenerated rationales (Zelikman et al., 2022;Yuan et al., 2023;Ni et al., 2022), to fine-tune the LLMs."], "score": 0.7666015625}, {"id": "(Vatsal et al., 2024)", "paper": {"corpus_id": 271270817, "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shubham Vatsal", "authorId": "1491705917"}, {"name": "Harsh Dubey", "authorId": "2311890970"}], "n_citations": 33}, "snippets": ["The authors of (Shao et al., 2023) come up with Synthetic prompting using LLMs to generate synthetic examples which are augmented to the existing hand-crafted examples as seen in a conventional few-shot setting.This prompting method involves two steps: (1) the backward step, where the LLM synthesizes a query based on a self-generated reasoning chain; and (2) the forward step, where the LLM generates a reasoning chain for the synthesized query, making the reasoning chain to be more accurate.Finally, to choose the best examples, this work uses an in-cluster complexity and the most complex examples with the longest reasoning chains are used during inference.The results show Synthetic prompting achieving up to 15.6% absolute gains when experimented with different Mathematical Problem Solving, Commonsense Reasoning and Logical Reasoning task datasets."], "score": 0.78369140625}, {"id": "(Shao et al., 2023)", "paper": {"corpus_id": 256459681, "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models", "year": 2023, "venue": "International Conference on Machine Learning", "authors": [{"name": "Zhihong Shao", "authorId": "144485528"}, {"name": "Yeyun Gong", "authorId": "2171182"}, {"name": "Yelong Shen", "authorId": "1752875"}, {"name": "Minlie Huang", "authorId": "1730108"}, {"name": "Nan Duan", "authorId": "46429989"}, {"name": "Weizhu Chen", "authorId": "2109136147"}], "n_citations": 70}, "snippets": ["Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques."], "score": 0.0}, {"id": "(Zhou et al., 2025)", "paper": {"corpus_id": 276768281, "title": "ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Heng Zhou", "authorId": "2348427354"}, {"name": "Hejia Geng", "authorId": "2232782866"}, {"name": "Xiangyuan Xue", "authorId": "2320221475"}, {"name": "Zhenfei Yin", "authorId": "2348879787"}, {"name": "Lei Bai", "authorId": "2350152497"}], "n_citations": 6}, "snippets": ["We also introduce an automated data synthesis framework for generating MAS benchmarks without any human annotations. Experimental results show that ReSo matches or outperforms existing methods, achieving 33.7 percent accuracy on Math-MAS and 32.3 percent accuracy on SciBench-MAS, where other approaches completely fail."], "score": 0.87646484375}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278171321, "title": "RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Jiapeng Wang", "authorId": "2302813110"}, {"name": "Jinhao Jiang", "authorId": "2118240359"}, {"name": "Zhiqiang Zhang", "authorId": "2358106383"}, {"name": "Jun Zhou", "authorId": "2279870653"}, {"name": "Wayne Xin Zhao", "authorId": "2294811281"}], "n_citations": 0}, "snippets": ["To address this scarcity, researchers have explored various synthesizing methods, particularly in the mathematics domain. The mainstream methods involve data augmentation based on existing annotated training sets, such as GSM8K (Cobbe et al., 2021) and MATH (Huang et al., 2024), ranging from self-evolving instructions (Xu et al., 2024;(Zeng et al., 2024) and question paraphrasing (Yu et al., 2024), to solution augmentation (Lu et al., 2024). However, these methods are limited by the available training data, constraining the synthesis diversity (Li et al., 2024c(Li et al., , 2023)). To enhance diversity, recent approaches enable LLMs to generate a large scale of questions from various mathematicsrelated sources, including web pages (Yue et al., 2024) and knowledge points (Tang et al., 2024) from web corpora or textbooks."], "score": 0.87451171875}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 268247488, "title": "Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yiming Huang", "authorId": "2261394934"}, {"name": "Xiao Liu", "authorId": "49544272"}, {"name": "Yeyun Gong", "authorId": "2254121650"}, {"name": "Zhibin Gou", "authorId": "1797090"}, {"name": "Yelong Shen", "authorId": "2237948786"}, {"name": "Nan Duan", "authorId": "2269471632"}, {"name": "Weizhu Chen", "authorId": "2249538838"}], "n_citations": 43}, "snippets": ["Large language models have shown great potential in complex reasoning tasks, yet their performance is often hampered by the scarcity of high-quality and reasoning-focused training datasets. Addressing this challenge, we propose Key-PointDriven Data Synthesis (KPDDS), a novel data synthesis framework that synthesizes question-answer pairs by leveraging key points and exemplar practices from authentic data sources. KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability. As a result, we present KPMath, an extensive synthetic dataset tailored for mathematical reasoning, comprising over 800K questionanswer pairs. Utilizing KPMath and augmenting it with additional reasoning-intensive corpora, we create the comprehensive KPMath-Plus dataset. Our experiments demonstrate that this dataset can enhance the mathematical reasoning performance of models across various architectures and sizes. The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 72B range and best commercial models like GPT-4 across multiple math reasoning datasets."], "score": 0.0}, {"id": "(Tang et al., 2024)", "paper": {"corpus_id": 268247902, "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Zhengyang Tang", "authorId": "2284834012"}, {"name": "Xingxing Zhang", "authorId": "2284863493"}, {"name": "Benyou Wang", "authorId": "2284827140"}, {"name": "Furu Wei", "authorId": "2290016262"}], "n_citations": 82}, "snippets": ["Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including GSM8K and MATH) covering K-12, college, and competition level math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g., LLaMA-2 and Mistral), resulting in significantly improved capabilities in mathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves state-of-the-art performance across all datasets, surpassing its best peers of equivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average accuracy, respectively."], "score": 0.0}, {"id": "(Zeng et al., 2024)", "paper": {"corpus_id": 270213007, "title": "Automatic Instruction Evolving for Large Language Models", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Weihao Zeng", "authorId": "2069640466"}, {"name": "Can Xu", "authorId": "2304522809"}, {"name": "Yingxiu Zhao", "authorId": "2290476555"}, {"name": "Jian-Guang Lou", "authorId": "2304469600"}, {"name": "Weizhu Chen", "authorId": "2264439430"}], "n_citations": 10}, "snippets": ["Fine-tuning large pre-trained language models with Evol-Instruct has achieved encouraging results across a wide range of tasks. However, designing effective evolving methods for instruction evolution requires substantial human expertise. This paper proposes Auto Evol-Instruct, an end-to-end framework that evolves instruction datasets using large language models without any human effort. The framework automatically analyzes and summarizes suitable evolutionary strategies for the given instruction data and iteratively improves the evolving method based on issues exposed during the instruction evolution process. Our extensive experiments demonstrate that the best method optimized by Auto Evol-Instruct outperforms human-designed methods on various benchmarks, including MT-Bench, AlpacaEval, GSM8K, and HumanEval."], "score": 0.0}], "table": null}, {"title": "Commonsense Reasoning Data Generation", "tldr": "Synthetic data generation for commonsense reasoning addresses the challenge of creating realistic scenarios that align with real-world knowledge. Researchers have developed graph-based approaches, taxonomy-driven frameworks, and contrastive generation techniques to create high-quality commonsense reasoning datasets. (7 sources)", "text": "\nGenerating synthetic data for commonsense reasoning presents unique challenges compared to other reasoning domains, as the data must accurately reflect real-world knowledge and scenarios. To address these challenges, researchers have developed specialized approaches that leverage knowledge graphs and generative models. GraDA stands out as a graph-generative data augmentation framework that synthesizes factual data samples from knowledge graphs for commonsense reasoning. This approach employs a two-stage process: first training a graph-to-text model to generate questions from graph entities and relations, then training a generator with GAN loss to create plausible distractors for these questions. GraDA has demonstrated performance improvements across multiple commonsense reasoning datasets, including SocialIQA, CODAH, HellaSwag, and CommonsenseQA <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper>.\n\nThe adoption of synthetic text generation as a data augmentation strategy has gained momentum with the advancement of powerful text generation models. Researchers have applied these techniques to various tasks including commonsense reasoning, sentence classification, and question answering <Paper corpusId=\"253384598\" paperTitle=\"(Sadat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"250390686\" paperTitle=\"(Maharana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"211258652\" paperTitle=\"(Puri et al., 2020)\" isShortName></Paper>. Additionally, synthetic text generation methods have been employed to address spurious correlations in human-annotated data for fact verification and natural language inference tasks <Paper corpusId=\"253384598\" paperTitle=\"(Sadat et al., 2022)\" isShortName></Paper> <Paper corpusId=\"238227205\" paperTitle=\"(Lee et al., 2021)\" isShortName></Paper> <Paper corpusId=\"247628095\" paperTitle=\"(Wu et al., 2022)\" isShortName></Paper>.\n\nRecent innovations include visual chain-of-thought (VCoT) approaches that generate novel and consistent synthetic data for tasks requiring visual reasoning and storytelling. When applied to the Visual Storytelling and WikiHow summarization datasets, VCoT has demonstrated superior performance compared to standard chain-of-thought baselines through human evaluation <Paper corpusId=\"258461502\" paperTitle=\"(Rose et al., 2023)\" isShortName></Paper>. This suggests that explicitly modeling reasoning processes can enhance the quality of synthetic data for commonsense tasks that involve visual elements.\n\nA more comprehensive approach to synthetic data generation for commonsense reasoning is the Generalized Instruction Tuning (GLAN) method, which leverages a pre-curated taxonomy of human knowledge and capabilities. Unlike approaches that rely on seed examples or existing datasets, GLAN generates diverse instructions with broad coverage across multiple domains of human knowledge and skills. By using fine-grained key concepts from academic syllabi, GLAN creates synthetic instruction data that encompasses commonsense reasoning along with other capabilities such as mathematical reasoning, coding, and logical reasoning. Experiments with large language models like Mistral demonstrate GLAN's effectiveness across these diverse reasoning domains without requiring task-specific training data <Paper corpusId=\"267759981\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Maharana et al., 2022)", "paper": {"corpus_id": 250390686, "title": "GraDA: Graph Generative Data Augmentation for Commonsense Reasoning", "year": 2022, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "Adyasha Maharana", "authorId": "8785371"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 96}, "snippets": ["Recent advances in commonsense reasoning have been fueled by the availability of large-scale human annotated datasets. Manual annotation of such datasets, many of which are based on existing knowledge bases, is expensive and not scalable. Moreover, it is challenging to build augmentation data for commonsense reasoning because the synthetic questions need to adhere to real-world scenarios. Hence, we present GraDA, a graph-generative data augmentation framework to synthesize factual data samples from knowledge graphs for commonsense reasoning datasets. First, we train a graph-to-text model for conditional generation of questions from graph entities and relations. Then, we train a generator with GAN loss to generate distractors for synthetic questions."], "score": 0.91162109375}, {"id": "(Sadat et al., 2022)", "paper": {"corpus_id": 253384598, "title": "Learning to Infer from Unlabeled Data: A Semi-supervised Learning Approach for Robust Natural Language Inference", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Mobashir Sadat", "authorId": "2140791766"}, {"name": "Cornelia Caragea", "authorId": "2140493460"}], "n_citations": 2}, "snippets": ["With the advancement of text generation models in recent years (Radford et al., 2019)Brown et al., 2020;(Lewis et al., 2019), researchers have started to adopt synthetic text generation as a data augmentation strategy for various tasks such as commonsense reasoning (Maharana et al., 2022), sentence classification (Anaby-Tavor et al., 2020) and question answering (Puri et al., 2020). More recently, synthetic text generation methods have also been employed to remove spurious correlations in human annotated data for fact verification (Lee et al., 2021) and NLI (Wu et al., 2022)."], "score": 0.90380859375}, {"id": "(Puri et al., 2020)", "paper": {"corpus_id": 211258652, "title": "Training Question Answering Models from Synthetic Data", "year": 2020, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Raul Puri", "authorId": "41158993"}, {"name": "Ryan Spring", "authorId": "145297046"}, {"name": "M. Patwary", "authorId": "66870756"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 159}, "snippets": ["Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQuAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQuAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQuAD1.1 dev set. We further apply our methodology to SQuAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data."], "score": 0.0}, {"id": "(Lee et al., 2021)", "paper": {"corpus_id": 238227205, "title": "CrossAug: A Contrastive Data Augmentation Method for Debiasing Fact Verification Models", "year": 2021, "venue": "International Conference on Information and Knowledge Management", "authors": [{"name": "Minwoo Lee", "authorId": "2125213401"}, {"name": "Seungpil Won", "authorId": "40981970"}, {"name": "Juae Kim", "authorId": "2109974526"}, {"name": "Hwanhee Lee", "authorId": "2143341775"}, {"name": "Cheoneum Park", "authorId": "48976447"}, {"name": "Kyomin Jung", "authorId": "1731707"}], "n_citations": 30}, "snippets": ["Fact verification datasets are typically constructed using crowdsourcing techniques due to the lack of text sources with veracity labels. However, the crowdsourcing process often produces undesired biases in data that cause models to learn spurious patterns. In this paper, we propose CrossAug, a contrastive data augmentation method for debiasing fact verification models. Specifically, we employ a two-stage augmentation pipeline to generate new claims and evidences from existing samples. The generated samples are then paired cross-wise with the original pair, forming contrastive samples that facilitate the model to rely less on spurious patterns and learn more robust representations. Experimental results show that our method outperforms the previous state-of-the-art debiasing technique by 3.6% on the debiased extension of the FEVER dataset, with a total performance boost of 10.13% from the baseline. Furthermore, we evaluate our approach in data-scarce settings, where models can be more susceptible to biases due to the lack of training data. Experimental results demonstrate that our approach is also effective at debiasing in these low-resource conditions, exceeding the baseline performance on the Symmetric dataset with just 1% of the original data."], "score": 0.0}, {"id": "(Wu et al., 2022)", "paper": {"corpus_id": 247628095, "title": "Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yuxiang Wu", "authorId": "39417610"}, {"name": "Matt Gardner", "authorId": "40642935"}, {"name": "Pontus Stenetorp", "authorId": "1918552"}, {"name": "Pradeep Dasigi", "authorId": "2697425"}], "n_citations": 68}, "snippets": ["Natural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard."], "score": 0.0}, {"id": "(Rose et al., 2023)", "paper": {"corpus_id": 258461502, "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Daniel Philip Rose", "authorId": "2215927057"}, {"name": "Vaishnavi Himakunthala", "authorId": "2215926792"}, {"name": "Andy Ouyang", "authorId": "2215914015"}, {"name": "Ryan He", "authorId": "2215915076"}, {"name": "Alex Mei", "authorId": "2185480449"}, {"name": "Yujie Lu", "authorId": "47006228"}, {"name": "Michael Stephen Saxon", "authorId": "48227633"}, {"name": "Chinmay Sonar", "authorId": "26961225"}, {"name": "Diba Mirza", "authorId": "1705929"}, {"name": "William Yang Wang", "authorId": "1682479"}], "n_citations": 47}, "snippets": ["We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance."], "score": 0.8818359375}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 267759981, "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Haoran Li", "authorId": "47893214"}, {"name": "Qingxiu Dong", "authorId": "2047143813"}, {"name": "Zhengyang Tang", "authorId": "2284834012"}, {"name": "Chaojun Wang", "authorId": "2144522700"}, {"name": "Xingxing Zhang", "authorId": "2284863493"}, {"name": "Haoyang Huang", "authorId": "15086992"}, {"name": "Shaohan Huang", "authorId": "3110003"}, {"name": "Xiaolong Huang", "authorId": "2116768132"}, {"name": "Zeqiang Huang", "authorId": "2284830389"}, {"name": "Dongdong Zhang", "authorId": "2273919921"}, {"name": "Yuxian Gu", "authorId": "2116405624"}, {"name": "Xin Cheng", "authorId": "2193630544"}, {"name": "Xun Wang", "authorId": "2193104542"}, {"name": "Si-Qing Chen", "authorId": "2263708536"}, {"name": "Li Dong", "authorId": "145307652"}, {"name": "Wei Lu", "authorId": "2261087197"}, {"name": "Zhifang Sui", "authorId": "3335836"}, {"name": "Benyou Wang", "authorId": "2284827140"}, {"name": "Wai Lam", "authorId": "2273358252"}, {"name": "Furu Wei", "authorId": "2253471545"}], "n_citations": 63}, "snippets": ["We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines", "With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks."], "score": 0.94140625}], "table": null}, {"title": "Generation Methodologies and Frameworks", "tldr": "Synthetic data generation frameworks employ diverse approaches including template-based methods, neural generation, and human-AI collaboration pipelines. Modern techniques leverage large language models to create high-quality synthetic datasets with filtering mechanisms that ensure data quality while balancing diversity and accuracy. (18 sources)", "text": "\nSynthetic data generation for complex reasoning tasks has evolved significantly, with researchers developing diverse methodologies to address the challenge of creating high-quality training data. Template-based generation represents one of the fundamental approaches, where structured templates are used to create controlled datasets with specific reasoning patterns <Paper corpusId=\"263909600\" paperTitle=\"(Gholami et al., 2023)\" isShortName></Paper>. This approach is particularly prevalent in semantic parsing tasks, where researchers first generate synthetic data using rules and templates before manually rewriting questions to improve diversity <Paper corpusId=\"258558899\" paperTitle=\"(Niu et al., 2023)\" isShortName></Paper> <Paper corpusId=\"247362971\" paperTitle=\"(Cao et al., 2020)\" isShortName></Paper> <Paper corpusId=\"226965153\" paperTitle=\"(Gu et al., 2020)\" isShortName></Paper>.\n\nRecent advances have enabled more sophisticated generation methods that leverage large language models (LLMs). Liu et al. introduced a collaborative approach that combines human workers and AI, using dataset cartography to identify challenging reasoning patterns in existing datasets and instructing GPT-3 to compose new examples with similar patterns <Paper corpusId=\"267311830\" paperTitle=\"(Zhou et al._1, 2024)\" isShortName></Paper> <Paper corpusId=\"246016339\" paperTitle=\"(Liu et al., 2022)\" isShortName></Paper>. This human-AI collaboration has proven particularly effective, with generated examples automatically filtered and subsequently reviewed and labeled by human crowdworkers, resulting in datasets with greater linguistic diversity.\n\nData quality is a critical concern in synthetic data generation, leading researchers to develop specialized filtering mechanisms. Ding et al. introduced essential filtering techniques to ensure that synthetic keyword queries generated from natural language questions meet high-quality standards before being used for neural network training <Paper corpusId=\"49865372\" paperTitle=\"(Ding et al., 2018)\" isShortName></Paper>. Similarly, modern approaches incorporate techniques such as logit suppression and temperature sampling to increase data diversity, though often at the cost of accuracy <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>. To address this trade-off, researchers have explored human interventions such as label replacement, which can increase the absolute accuracy of models trained with diversified datasets by up to 14.4% <Paper corpusId=\"259096160\" paperTitle=\"(Chung et al., 2023)\" isShortName></Paper>.\n\nThe rise of LLMs has revolutionized synthetic data generation across various NLP tasks. Self-Instruct introduced a framework for improving instruction-following capabilities of pretrained language models by bootstrapping off their own generations, showing a 33% absolute improvement over the original GPT-3 model <Paper corpusId=\"254877310\" paperTitle=\"(Wang et al., 2022)\" isShortName></Paper>. In the realm of text classification, researchers have demonstrated that LLM-generated data can enhance diversity and improve model generalization <Paper corpusId=\"233296100\" paperTitle=\"(Yoo et al., 2021)\" isShortName></Paper>. For information retrieval tasks, LLMs have been used to generate synthetic queries <Paper corpusId=\"250340449\" paperTitle=\"(Bonifacio et al., 2022)\" isShortName></Paper> or documents <Paper corpusId=\"257505063\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>, with approaches like query2doc boosting the performance of retrieval systems by 3% to 15% without model fine-tuning <Paper corpusId=\"257505063\" paperTitle=\"(Wang et al., 2023)\" isShortName></Paper>.\n\nData augmentation techniques have also proven effective for enhancing existing datasets. EDA (Easy Data Augmentation) presents simple but powerful operations including synonym replacement, random insertion, random swap, and random deletion to boost performance on text classification tasks <Paper corpusId=\"59523656\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>. More sophisticated methods like LAMBADA (Language-Model-Based Data Augmentation) involve fine-tuning state-of-the-art language generators on small labeled datasets to synthesize new labeled data for supervised learning <Paper corpusId=\"212821571\" paperTitle=\"(Anaby-Tavor et al., 2019)\" isShortName></Paper>. Recent studies have shown that synthetic data can be particularly valuable in low-resource settings, with experiments demonstrating that training with augmented data while using only 50% of available training data can achieve the same accuracy as normal training with all available data <Paper corpusId=\"59523656\" paperTitle=\"(Wei et al., 2019)\" isShortName></Paper>.\n\nSpecialized generation frameworks have been developed for complex reasoning tasks. Li et al. introduced a novel and scalable text-to-SQL data synthesis framework that automatically produces large-scale, high-quality, and diverse datasets without extensive human intervention, resulting in SynSQL-2.5M, a dataset containing 2.5 million samples across 16,000 synthetic databases <Paper corpusId=\"276774742\" paperTitle=\"(Li et al., 2025)\" isShortName></Paper>. In the visual-language domain, researchers have explored GAN-based models <Paper corpusId=\"234338081\" paperTitle=\"(Kayser et al., 2021)\" isShortName></Paper> and diffusion-based approaches for image synthesis, while LLMs have been leveraged to generate synthetic data for tasks like text classification <Paper corpusId=\"277065796\" paperTitle=\"(Luo et al., 2025)\" isShortName></Paper>.\n\nThe effectiveness of synthetic data generation depends heavily on the specific task characteristics. Studies have shown that while models trained on synthetic data perform comparably to those trained on real-world data for low-subjectivity tasks like news and topic classification, there is a notable performance drop on high-subjectivity tasks such as sarcasm detection <Paper corpusId=\"273662200\" paperTitle=\"(Braga et al., 2024)\" isShortName></Paper>. This highlights the importance of considering task specificity when designing synthetic data generation approaches for complex reasoning tasks.", "citations": [{"id": "(Gholami et al., 2023)", "paper": {"corpus_id": 263909600, "title": "Does Synthetic Data Make Large Language Models More Efficient?", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Sia Gholami", "authorId": "2141639835"}, {"name": "Marwan Omar", "authorId": "2239197098"}], "n_citations": 15}, "snippets": ["One challenge persistently confronting researchers is the scarcity of high-quality, annotated datasets that drive these models. This paper explores the nuances of synthetic data generation in NLP, with a focal point on template-based question generation."], "score": 0.8623046875}, {"id": "(Niu et al., 2023)", "paper": {"corpus_id": 258558899, "title": "Bridging the Gap between Synthetic and Natural Questions via Sentence Decomposition for Semantic Parsing", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Yilin Niu", "authorId": "10680347"}, {"name": "Fei Huang", "authorId": "152159016"}, {"name": "W. Liu", "authorId": "46641573"}, {"name": "Jianwei Cui", "authorId": "3061725"}, {"name": "Bin Wang", "authorId": "2152593593"}, {"name": "Minlie Huang", "authorId": "1730108"}], "n_citations": 6}, "snippets": ["Data synthesis plays an important role in semantic parsing, especially in dataset construction and model training. Many datasets are constructed by first generating synthetic data by rules and then manually rewriting the questions for diverse expressions (Wang et al., 2015)(Cao et al., 2020)(Gu et al., 2020). Since the generation of synthetic data is controllable, researchers can restrict the distribution of logical forms in order to examine or improve various generalization capabilities of models, such as compositional generalization and scalability (Lake and (Lake et al., 2017)(Gu et al., 2020)(Oren et al., 2021). In many scenarios, where manually rewriting is laborious or unavailable, training on synthetic data can also alleviate the problems of cold-start and privacy (Yang et al., 2022)(Gu et al., 2020). Limited by stereotyped expressions, semantic parsers trained with only synthetic data are difficult to generalize to diverse natural questions. (Wu et al., 2021) and (Xu et al., 2020) automatically rewrite synthetic questions with paraphrasing models, which introduces various phrase-level expressions but change little in sentence structures. Instead of rule-based synthesis, (Zhong et al., 2020) and Yang et al. (2021) train a neural data synthesizer with natural data from other domains, and then generate training data for the target domain in a way of domain transfer."], "score": 0.89404296875}, {"id": "(Cao et al., 2020)", "paper": {"corpus_id": 247362971, "title": "KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base", "year": 2020, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "S. Cao", "authorId": "1712738522"}, {"name": "Jiaxin Shi", "authorId": "2522647"}, {"name": "Liangming Pan", "authorId": "3470231"}, {"name": "L. Nie", "authorId": "115361209"}, {"name": "Yutong Xiang", "authorId": "2068340987"}, {"name": "Lei Hou", "authorId": "145779862"}, {"name": "Juanzi Li", "authorId": "2133353675"}, {"name": "Bin He", "authorId": "1631386300"}, {"name": "Hanwang Zhang", "authorId": "2119078220"}], "n_citations": 102}, "snippets": ["Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines."], "score": 0.0}, {"id": "(Gu et al., 2020)", "paper": {"corpus_id": 226965153, "title": "Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases", "year": 2020, "venue": "The Web Conference", "authors": [{"name": "Yu Gu", "authorId": "2022231256"}, {"name": "Sue E. Kase", "authorId": "145030211"}, {"name": "M. Vanni", "authorId": "36349209"}, {"name": "Brian M. Sadler", "authorId": "1709722"}, {"name": "Percy Liang", "authorId": "145419642"}, {"name": "Xifeng Yan", "authorId": "1740249"}, {"name": "Yu Su", "authorId": "1758652"}], "n_citations": 215}, "snippets": ["Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1"], "score": 0.0}, {"id": "(Zhou et al._1, 2024)", "paper": {"corpus_id": 267311830, "title": "A Survey on Data Augmentation in Large Model Era", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yue Zhou", "authorId": "2281788432"}, {"name": "Chenlu Guo", "authorId": "2282250059"}, {"name": "Xu Wang", "authorId": "2281791317"}, {"name": "Yi Chang", "authorId": "2281762754"}, {"name": "Yuan Wu", "authorId": "2280899184"}], "n_citations": 27}, "snippets": ["Liu et al. (2022) introduced a fresh method to create datasets by combining the efforts of human workers and AI. It began with an established dataset called MultiNLI, which is designed to deal with natural language inference. The proposed approach involves employing dataset cartography to automatically identify instances that exhibit complex reasoning patterns. These patterns serve as guidelines for instructing GPT-3 to generate new examples with similar characteristics. The generated examples are then filtered automatically and subsequently reviewed and labeled by human crowd workers."], "score": 0.814453125}, {"id": "(Liu et al., 2022)", "paper": {"corpus_id": 246016339, "title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Alisa Liu", "authorId": "94500147"}, {"name": "Swabha Swayamdipta", "authorId": "2133324514"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "Yejin Choi", "authorId": "1699545"}], "n_citations": 221}, "snippets": ["A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process."], "score": 0.0}, {"id": "(Ding et al., 2018)", "paper": {"corpus_id": 49865372, "title": "Generating Synthetic Data for Neural Keyword-to-Question Models", "year": 2018, "venue": "International Conference on the Theory of Information Retrieval", "authors": [{"name": "Heng Ding", "authorId": "2113453574"}, {"name": "K. Balog", "authorId": "1680484"}], "n_citations": 7}, "snippets": ["We introduce several generative models for producing synthetic keyword queries from natural language questions (Sect. 3.1). (3) We develop two filtering mechanisms, which are essential for ensuring that the synthetic training data we feed into the neural network is of high-quality (Sect. 3.2)."], "score": 0.8349609375}, {"id": "(Chung et al., 2023)", "paper": {"corpus_id": 259096160, "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions", "year": 2023, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "John Joon Young Chung", "authorId": "152836325"}, {"name": "Ece Kamar", "authorId": "1783184"}, {"name": "Saleema Amershi", "authorId": "1719124"}], "n_citations": 121}, "snippets": ["Large language models (LLMs) can be used to generate text data for training and evaluating other models. However, creating high-quality datasets with LLMs can be challenging. In this work, we explore human-AI partnerships to facilitate high diversity and accuracy in LLM-based text data generation. We first examine two approaches to diversify text generation: 1) logit suppression, which minimizes the generation of languages that have already been frequently generated, and 2) temperature sampling, which flattens the token sampling probability. We found that diversification approaches can increase data diversity but often at the cost of data accuracy (i.e., text and labels being appropriate for the target domain). To address this issue, we examined two human interventions, 1) label replacement (LR), correcting misaligned labels, and 2) out-of-scope filtering (OOSF), removing instances that are out of the user\u2019s domain of interest or to which no considered label applies. With oracle studies, we found that LR increases the absolute accuracy of models trained with diversified datasets by 14.4%. Moreover, we found that some models trained with data generated with LR interventions outperformed LLM-based few-shot classification. In contrast, OOSF was not effective in increasing model accuracy, implying the need for future work in human-in-the-loop text data generation."], "score": 0.0}, {"id": "(Wang et al., 2022)", "paper": {"corpus_id": 254877310, "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Yizhong Wang", "authorId": "1705260"}, {"name": "Yeganeh Kordi", "authorId": "2156538832"}, {"name": "Swaroop Mishra", "authorId": "1817207"}, {"name": "Alisa Liu", "authorId": "94500147"}, {"name": "Noah A. Smith", "authorId": "144365875"}, {"name": "Daniel Khashabi", "authorId": "1783281"}, {"name": "Hannaneh Hajishirzi", "authorId": "2548384"}], "n_citations": 2251}, "snippets": ["Large \"instruction-tuned\" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning."], "score": 0.0}, {"id": "(Yoo et al., 2021)", "paper": {"corpus_id": 233296100, "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "year": 2021, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Kang Min Yoo", "authorId": "31760501"}, {"name": "Dongju Park", "authorId": "13453892"}, {"name": "Jaewook Kang", "authorId": "35518563"}, {"name": "Sang-Woo Lee", "authorId": "3226948"}, {"name": "Woomyeong Park", "authorId": "2087289230"}], "n_citations": 242}, "snippets": ["Large-scale language models such as GPT-3 are excellent few-shot learners, allowing them to be controlled via natural text prompts. Recent studies report that prompt-based direct classification eliminates the need for fine-tuning but lacks data and inference scalability. This paper proposes a novel data augmentation technique that leverages large-scale language models to generate realistic text samples from a mixture of real samples. We also propose utilizing soft-labels predicted by the language models, effectively distilling knowledge from the large-scale language models and creating textual perturbations simultaneously. We perform data augmentation experiments on diverse classification tasks and show that our method hugely outperforms existing text augmentation methods. Ablation studies and a qualitative analysis provide more insights into our approach."], "score": 0.0}, {"id": "(Bonifacio et al., 2022)", "paper": {"corpus_id": 250340449, "title": "InPars: Unsupervised Dataset Generation for Information Retrieval", "year": 2022, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "L. Bonifacio", "authorId": "2003019597"}, {"name": "H. Abonizio", "authorId": "1394470211"}, {"name": "Marzieh Fadaee", "authorId": "2818759"}, {"name": "Rodrigo Nogueira", "authorId": "143744603"}], "n_citations": 114}, "snippets": ["The Information Retrieval (IR) community has recently witnessed a revolution due to large pretrained transformer models. Another key ingredient for this revolution was the MS MARCO dataset, whose scale and diversity has enabled zero-shot transfer learning to various tasks. However, not all IR tasks and domains can benefit from one single dataset equally. Extensive research in various NLP tasks has shown that using domain-specific training data, as opposed to a general-purpose one, improves the performance of neural models. In this work, we harness the few-shot capabilities of large pretrained language models as synthetic data generators for IR tasks. We show that models finetuned solely on our synthetic datasets outperform strong baselines such as BM25 as well as recently proposed self-supervised dense retrieval methods. Code, models, and data are available at https://github.com/zetaalphavector/inpars."], "score": 0.0}, {"id": "(Wang et al., 2023)", "paper": {"corpus_id": 257505063, "title": "Query2doc: Query Expansion with Large Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Liang Wang", "authorId": "145769448"}, {"name": "Nan Yang", "authorId": "144610884"}, {"name": "Furu Wei", "authorId": "49807919"}], "n_citations": 112}, "snippets": ["This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results."], "score": 0.0}, {"id": "(Wei et al., 2019)", "paper": {"corpus_id": 59523656, "title": "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", "year": 2019, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Jason Wei", "authorId": "144026731"}, {"name": "Kai Zou", "authorId": "2132971481"}], "n_citations": 1962}, "snippets": ["We present EDA: easy data augmentation techniques for boosting performance on text classification tasks. EDA consists of four simple but powerful operations: synonym replacement, random insertion, random swap, and random deletion. On five text classification tasks, we show that EDA improves performance for both convolutional and recurrent neural networks. EDA demonstrates particularly strong results for smaller datasets; on average, across five datasets, training with EDA while using only 50% of the available training set achieved the same accuracy as normal training with all available data. We also performed extensive ablation studies and suggest parameters for practical use."], "score": 0.0}, {"id": "(Anaby-Tavor et al., 2019)", "paper": {"corpus_id": 212821571, "title": "Do Not Have Enough Data? Deep Learning to the Rescue!", "year": 2019, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Ateret Anaby-Tavor", "authorId": "1400347538"}, {"name": "Boaz Carmeli", "authorId": "1719299"}, {"name": "Esther Goldbraich", "authorId": "1734231"}, {"name": "Amir Kantor", "authorId": "1703796"}, {"name": "George Kour", "authorId": "2497491"}, {"name": "Segev Shlomov", "authorId": "20513655"}, {"name": "N. Tepper", "authorId": "3166348"}, {"name": "Naama Zwerdling", "authorId": "2652616"}], "n_citations": 370}, "snippets": ["Based on recent advances in natural language modeling and those in text generation capabilities, we propose a novel data augmentation method for text classification tasks. We use a powerful pre-trained neural network model to artificially synthesize new labeled data for supervised learning. We mainly focus on cases with scarce labeled data. Our method, referred to as language-model-based data augmentation (LAMBADA), involves fine-tuning a state-of-the-art language generator to a specific task through an initial training phase on the existing (usually small) labeled data. Using the fine-tuned model and given a class label, new sentences for the class are generated. Our process then filters these new sentences by using a classifier trained on the original data. In a series of experiments, we show that LAMBADA improves classifiers' performance on a variety of datasets. Moreover, LAMBADA significantly improves upon the state-of-the-art techniques for data augmentation, specifically those applicable to text classification tasks with little data."], "score": 0.0}, {"id": "(Li et al., 2025)", "paper": {"corpus_id": 276774742, "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Haoyang Li", "authorId": "2274084217"}, {"name": "Shang Wu", "authorId": "2348412920"}, {"name": "Xiaokang Zhang", "authorId": "2108046717"}, {"name": "Xinmei Huang", "authorId": "2198199708"}, {"name": "Jing Zhang", "authorId": "2345829238"}, {"name": "Fuxin Jiang", "authorId": "2295705983"}, {"name": "Shuai Wang", "authorId": "2346968040"}, {"name": "Tieying Zhang", "authorId": "2297140223"}, {"name": "Jianjun Chen", "authorId": "2225061614"}, {"name": "Rui Shi", "authorId": "2349071083"}, {"name": "Hong Chen", "authorId": "2191043357"}, {"name": "Cuiping Li", "authorId": "2287979930"}], "n_citations": 9}, "snippets": ["To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce SynSQL-2.5M, the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution."], "score": 0.76171875}, {"id": "(Kayser et al., 2021)", "paper": {"corpus_id": 234338081, "title": "e-ViL: A Dataset and Benchmark for Natural Language Explanations in Vision-Language Tasks", "year": 2021, "venue": "IEEE International Conference on Computer Vision", "authors": [{"name": "Maxime Kayser", "authorId": "1491707097"}, {"name": "Oana-Maria Camburu", "authorId": "3317152"}, {"name": "Leonard Salewski", "authorId": "151097297"}, {"name": "Cornelius Emde", "authorId": "153438331"}, {"name": "Virginie Do", "authorId": "2086828576"}, {"name": "Zeynep Akata", "authorId": "2893664"}, {"name": "Thomas Lukasiewicz", "authorId": "1690572"}], "n_citations": 101}, "snippets": ["Recently, there has been an increasing number of efforts to introduce models capable of generating natural language explanations (NLEs) for their predictions on vision-language (VL) tasks. Such models are appealing, because they can provide human-friendly and comprehensive explanations. However, there is a lack of comparison between existing methods, which is due to a lack of re-usable evaluation frameworks and a scarcity of datasets. In this work, we introduce e-ViL and e-SNLI-VE. e-ViL is a benchmark for explainable vision-language tasks that establishes a unified evaluation framework and provides the first comprehensive comparison of existing approaches that generate NLEs for VL tasks. It spans four models and three datasets and both automatic metrics and human evaluation are used to assess model-generated explanations. e-SNLI-VE is currently the largest existing VL dataset with NLEs (over 430k instances). We also propose a new model that combines UNITER [15], which learns joint embeddings of images and text, and GPT-2 [38], a pre-trained language model that is well-suited for text generation. It surpasses the previous state of the art by a large margin across all datasets. Code and data are available here: https://github.com/maximek3/e-ViL."], "score": 0.0}, {"id": "(Luo et al., 2025)", "paper": {"corpus_id": 277065796, "title": "nvBench 2.0: A Benchmark for Natural Language to Visualization under Ambiguity", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Tianqi Luo", "authorId": "2324719591"}, {"name": "Chuhan Huang", "authorId": "2351107046"}, {"name": "Leixian Shen", "authorId": "2314809440"}, {"name": "Boyan Li", "authorId": "2304516904"}, {"name": "Shuyu Shen", "authorId": "2316124858"}, {"name": "Wei Zeng", "authorId": "2351038617"}, {"name": "Nan Tang", "authorId": "2301154561"}, {"name": "Yuyu Luo", "authorId": "2324528358"}], "n_citations": 3}, "snippets": ["Recently, the use of LLMs for data synthesis or data augmentation has become increasingly prevalent. Many studies leverage LLMgenerated data for training models (Hartvigsen et al., 2022)36,53,56,59], as well as for evaluating the performance of other trained models (Ribeiro et al., 2022). In the NLP domain, researchers have utilized LLMs to generate synthetic data for tasks like text classification [1,3,(Yoo et al., 2021). These works showcase that LLM-generated data can enhance data diversity, thereby improving model generalization and robustness", "By leveraging LLMs to generate multi-step reasoning data, the performance of models on long-chain and complex reasoning tasks can be further improved. As demonstrated by Hunter et al. [18], process supervision via multi-step reasoning significantly enhances model reliability on tasks such as mathematical problem-solving. Similarly, Step-DPO [14] shows that generating step-wise reasoning data enables models to better capture intermediate steps, resulting in improved accuracy."], "score": 0.91552734375}, {"id": "(Braga et al., 2024)", "paper": {"corpus_id": 273662200, "title": "Synthetic Data Generation with Large Language Models for Personalized Community Question Answering", "year": 2024, "venue": "2024 IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)", "authors": [{"name": "Marco Braga", "authorId": "2227483106"}, {"name": "Pranav Kasela", "authorId": "2187856026"}, {"name": "Alessandro Raganato", "authorId": "3106437"}, {"name": "Gabriella Pasi", "authorId": "2243333911"}], "n_citations": 1}, "snippets": ["Puri et al. [39] explore question and answer generation as a data augmentation method to enhance QA models, addressing the challenge of limited human-labelled data. The authors point out that the synthetic data alone overcome the performance of the original training set in the Exact Match (EM) and F1 scores. Li et al. [40] investigate the effectiveness of synthetic data generated by LLMs in training text classification models across tasks with varying levels of subjectivity. They compare models trained on real-world data with those trained on LLMgenerated data under zero-shot and few-shot settings. Their findings reveal that while models trained on synthetic data perform comparably on low-subjectivity tasks, e.g. news and topic classification, there is a notable performance drop on high-subjectivity tasks, such as sarcasm detection. Regarding the generation of synthetic data for IR tasks, LLMs can be used to generate synthetic queries (Bonifacio et al., 2022) or documents [17]. In (Wang et al., 2023) the authors propose to generate documents by few-shot prompting LLMs and concatenate them with the original query to form a new query."], "score": 0.90771484375}], "table": null}, {"title": "Evaluation and Benchmarks", "tldr": "Evaluation frameworks for synthetic reasoning data have evolved from simple accuracy measures to comprehensive benchmarks targeting specific reasoning capabilities. These benchmarks use controlled synthetic data generation to systematically test model performance across various reasoning patterns, premise orders, and difficulty levels. (5 sources)", "text": "\nThe development of specialized benchmarks has become crucial for assessing language models' reasoning capabilities on synthetic data. Researchers have created comprehensive evaluation frameworks that systematically test specific reasoning patterns across multiple domains. LogicBench stands out as a notable example, evaluating 25 different reasoning patterns spanning propositional, first-order, and non-monotonic logics . This benchmark enables systematic assessment through natural language question-answering focused on single inference rules, revealing that even advanced models struggle with complex reasoning patterns and negations.\n\nTo facilitate systematic evaluation, researchers have developed frameworks for dynamically generating evaluation samples with varying configurations and difficulty levels <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211126663\" paperTitle=\"(Clark et al., 2020)\" isShortName></Paper>. DyVal represents a notable approach, using directed acyclic graphs to dynamically generate samples for various reasoning tasks including deductive, Boolean, and abductive reasoning <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. Similarly, ZebraLogic provides an extended benchmark specifically designed to test logical reasoning capabilities systematically <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nRecent research has revealed important insights about model sensitivity to evaluation data presentation. Chen et al. discovered that language models demonstrate surprising brittleness to premise ordering in logical reasoning tasks, with performance drops exceeding 30% when premises are reordered <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. This finding suggests that models perform best when premise order aligns with the steps required for intermediate reasoning, highlighting a significant limitation in current reasoning capabilities. To further examine this phenomenon, researchers released R-GSM, a benchmark based on GSM8K that tests the ordering effect in mathematical problem-solving <Paper corpusId=\"267657940\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>.\n\nSeveral specialized benchmarks focus on specific reasoning challenges. BoardgameQA presents a dataset characterized by contradictory facts and rules, requiring models to resolve conflicts through preference-based reasoning <Paper corpusId=\"259144942\" paperTitle=\"(Kazemi et al., 2023)\" isShortName></Paper>. Similarly, PRONTOQA offers a synthetic question-answering dataset generated from world models represented in first-order logic, enabling the parsing of reasoning chains into symbolic proofs for formal analysis <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>. TruthQuest provides evaluation samples based on K&K-type puzzles involving 3-6 persons, testing complex logical deduction <Paper corpusId=\"273695832\" paperTitle=\"(Xie et al., 2024)\" isShortName></Paper>.\n\nBeyond traditional benchmarks, researchers have also developed automated evaluation pipelines. Automate-CoT introduces a fully automatic pipeline for finding optimal chain-of-thought prompts for commonsense reasoning tasks, leveraging powerful language models to augment reasoning paths, prune incorrect paths, and select optimal exemplar combinations <Paper corpusId=\"268249187\" paperTitle=\"(Ding et al., 2024)\" isShortName></Paper>. This approach demonstrates how synthetic data can be used not just for training but also for optimizing prompting strategies.\n\nThe growing landscape of specialized benchmarks underscores the importance of synthetic data in evaluating specific reasoning capabilities. Each benchmark targets particular aspects of reasoning, from premise ordering sensitivity to conflict resolution and multi-step inference. As models continue to advance, these targeted evaluation frameworks provide crucial insights into their strengths and limitations across different reasoning domains.", "citations": [{"id": "(Xie et al., 2024)", "paper": {"corpus_id": 273695832, "title": "On Memorization of Large Language Models in Logical Reasoning", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Chulin Xie", "authorId": "2308424593"}, {"name": "Yangsibo Huang", "authorId": "2283305597"}, {"name": "Chiyuan Zhang", "authorId": "2309481623"}, {"name": "Da Yu", "authorId": "2290010050"}, {"name": "Xinyun Chen", "authorId": "2328442132"}, {"name": "Bill Yuchen Lin", "authorId": "2328296250"}, {"name": "Bo Li", "authorId": "2290141407"}, {"name": "Badih Ghazi", "authorId": "2529354"}, {"name": "Ravi Kumar", "authorId": "2237408905"}], "n_citations": 41}, "snippets": ["Logical reasoning benchmarks. To evaluate logical reasoning capabilities in LLMs, synthetic benchmarks have been developed to enable scalable generation of samples with varying configurations and difficulty levels (Clark et al., 2020)(Giadikiaroglou et al., 2024)(Parmar et al., 2024). For instance, DyVal (Zhu et al., 2024) uses directed acyclic graphs to dynamically generate samples on reasoning tasks including deductive, Boolean, and abductive reasoning. (Chen et al., 2024) focus on propositional logical problems involving definite clauses, and synthetically generate variations with different premise orders, such as forward, backward, and shuffled. (Dziri et al., 2024) explore the limitations of LLMs in tasks requiring compositional reasoning, including multiplication, logic grid puzzles, and dynamic programming problems. ZebraLogic (Lin et al., 2024) is an extended benchmark that systematically tests logical reasoning capabilities. BoardgameQA (Kazemi et al., 2023) presents a question-answering dataset characterized by contradictory facts and rules in the questions. PRONTOQA (Saparov & He, 2023) is a synthetic question-answering dataset where each example is generated from a synthetic world model represented in first-order logic. This dataset enables parsing the generated chain of thought into symbolic proofs, facilitating formal analysis. TruthQuest (Mondorf & Plank, 2024) is the most similar task to our work, which provides evaluation samples based on K&K-type of puzzles involving 3-6 person."], "score": 0.82080078125}, {"id": "(Clark et al., 2020)", "paper": {"corpus_id": 211126663, "title": "Transformers as Soft Reasoners over Language", "year": 2020, "venue": "International Joint Conference on Artificial Intelligence", "authors": [{"name": "Peter Clark", "authorId": "48323507"}, {"name": "Oyvind Tafjord", "authorId": "3385516"}, {"name": "Kyle Richardson", "authorId": "46666605"}], "n_citations": 361}, "snippets": ["Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited \"soft theorem provers\" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering."], "score": 0.0}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 267657940, "title": "Premise Order Matters in Reasoning with Large Language Models", "year": 2024, "venue": "International Conference on Machine Learning", "authors": [{"name": "Xinyun Chen", "authorId": "2238263119"}, {"name": "Ryan A. Chi", "authorId": "2284066085"}, {"name": "Xuezhi Wang", "authorId": "2238394232"}, {"name": "Denny Zhou", "authorId": "2256313467"}], "n_citations": 32}, "snippets": ["Large language models (LLMs) have accomplished remarkable reasoning performance in various domains. However, in the domain of reasoning tasks, we discover a frailty: LLMs are surprisingly brittle to the ordering of the premises, despite the fact that such ordering does not alter the underlying task. In particular, we observe that LLMs achieve the best performance when the premise order aligns with the context required in intermediate reasoning steps. For example, in deductive reasoning tasks, presenting the premises in the same order as the ground truth proof in the prompt (as opposed to random ordering) drastically increases the model's accuracy. We first examine the effect of premise ordering on deductive reasoning on a variety of LLMs, and our evaluation shows that permuting the premise order can cause a performance drop of over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to examine the ordering effect for mathematical problem-solving, and we again observe a significant drop in accuracy, relative to the original GSM8K benchmark."], "score": 0.0}, {"id": "(Kazemi et al., 2023)", "paper": {"corpus_id": 259144942, "title": "BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Mehran Kazemi", "authorId": "2173102324"}, {"name": "Quan Yuan", "authorId": "2117782275"}, {"name": "Deepti Bhatia", "authorId": "2055505691"}, {"name": "Najoung Kim", "authorId": "8756748"}, {"name": "Xin Xu", "authorId": null}, {"name": "Vaiva Imbrasaite", "authorId": "2410945"}, {"name": "Deepak Ramachandran", "authorId": "143812128"}], "n_citations": 50}, "snippets": ["Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor."], "score": 0.0}, {"id": "(Ding et al., 2024)", "paper": {"corpus_id": 268249187, "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges", "year": 2024, "venue": "", "authors": [{"name": "Bosheng Ding", "authorId": "2064493724"}, {"name": "Chengwei Qin", "authorId": "2084609980"}, {"name": "Ruochen Zhao", "authorId": "2091437375"}, {"name": "Tianze Luo", "authorId": "2290030532"}, {"name": "Xinze Li", "authorId": "2290023354"}, {"name": "Guizhen Chen", "authorId": "2290026474"}, {"name": "Wenhan Xia", "authorId": "2276610995"}, {"name": "Junjie Hu", "authorId": "2290145274"}, {"name": "A. Luu", "authorId": "1755919"}, {"name": "Shafiq R. Joty", "authorId": "2708940"}], "n_citations": 20}, "snippets": ["Automate-CoT (Shum et al., 2023) proposes a fully automatic pipeline for finding better chainof-thought prompts to solve commonsense reasoning question answering tasks. This pipeline leverages powerful LLMs like GPT-3 to augment reasoning paths, prune incorrect paths, and select optimal combinations of exemplars."], "score": 0.77392578125}], "table": null}], "cost": 0.504807}}
