{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in papers comparing the effectiveness of different classifier-based filtering methods (e.g., FastText, BERT) for LLM pre-training data quality?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in papers that compare the effectiveness of using different classifier-based filtering methods, such as FastText and BERT, for improving the quality of pre-training data for large language models."}}, {"clarifying_question2": "Are you looking for studies focused on the impact of data quality filtering on downstream task performance for models like BERT, GPT-3, or Llama?", "clarifying_answer2": {"clarifying_answer": "Yes, I am specifically interested in studies that assess how data quality filtering during pre-training affects downstream task performance in advanced models like BERT, GPT-3, or Llama."}}, {"clarifying_question3": "Do you want an overview of the most advanced and computationally efficient data filtering techniques currently used in major LLM training pipelines, or a comparison to earlier, weaker filtering models?", "clarifying_answer3": {"clarifying_answer": "I would like an overview of the most advanced and computationally efficient data filtering techniques currently used in major LLM training pipelines, highlighting how they improve upon earlier, weaker filtering models."}}], "better_query": "Which papers empirically compare the effectiveness of FastText-based versus BERT-based classifier filtering approaches for pre-training data quality and their impact on downstream performance in large language models such as BERT, GPT-3, and Llama?", "better_answer": {"sections": [{"title": "Introduction", "tldr": "There is limited direct research comparing FastText and BERT classifier approaches for pre-training data filtering. Most existing studies focus on either approach separately, with researchers exploring both methods for improving data quality in large language models. (LLM Memory)", "text": "\nThe quality of pre-training data is recognized as a critical factor in the development of large language models (LLMs) such as BERT, GPT-3, and Llama. As these models scale to hundreds of billions of parameters and are trained on increasingly vast datasets, effective filtering mechanisms become essential to remove low-quality content that could negatively impact model performance. Both FastText and BERT-based classification approaches have emerged as popular methods for data filtering, but comprehensive studies directly comparing their effectiveness remain scarce. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFastText, with its lightweight architecture and computational efficiency, offers advantages in processing massive datasets quickly, while BERT-based approaches leverage contextual understanding and potentially more nuanced classification capabilities at the cost of increased computational demands. Researchers have investigated both approaches separately in various contexts, exploring their application to tasks such as toxic content filtering, quality assessment, and duplicate detection in pre-training corpora. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe empirical comparison between these approaches is particularly relevant as the field continues to scale models and datasets, raising questions about the trade-offs between computational efficiency and classification accuracy in data filtering pipelines. Understanding these trade-offs has significant implications for model development resources, training time, and ultimate model performance across different downstream tasks. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "FastText vs. BERT Performance Comparisons", "tldr": "Direct comparisons between FastText and BERT show that BERT generally achieves better performance in tasks requiring contextual understanding, while FastText excels in memory-intensive tasks and scenarios with limited training data. The choice between these approaches involves trade-offs between computational efficiency and classification accuracy. (4 sources)", "text": "\nWhen directly comparing FastText and BERT approaches for text classification tasks, research reveals distinct performance characteristics for each method. Edwards et al. found that in limited training data settings, FastText coupled with domain-specific word embeddings can be more robust than BERT, which typically requires more data to perform effectively <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This finding is particularly relevant for pre-training data filtering scenarios where labeled examples might be scarce. However, the same study also demonstrated that BERT pre-trained on domain-specific data (e.g., Twitter) outperforms generic BERT in few-shot experiments, highlighting the importance of domain adaptation <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>.\n\nMore recent research by Frissen et al. provides contrasting evidence, showing that Random Forest classifiers using BERT word embeddings achieved superior performance compared to non-contextual embeddings like FastText and Word2Vec <Paper corpusId=\"253015767\" paperTitle=\"(Frissen et al., 2022)\" isShortName></Paper>. Their confusion matrices demonstrated BERT's advantages in capturing contextual information, which is critical for nuanced classification tasks such as identifying quality content in pre-training corpora.\n\nNing et al. offer an insightful perspective on the complementary strengths of these approaches, observing that FastText models excel at \"remembering\" patterns from training data, performing better on test sets with high overlap with training data (99.89% overlap), while BERT models demonstrate superior \"generalization\" capabilities on unseen or low-frequency data <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This distinction is particularly important in the context of pre-training data filtering, where both memorization of known low-quality patterns and generalization to novel problematic content are valuable.\n\nFurther supporting BERT's advantages, Nabiilah et al. compared FastText and BERT word embedding methods with CNN and bidirectional LSTM classifiers, finding that BERT used for both word embedding and classification achieved the highest performance with an F1 score of 84% in multi-class classification tasks <Paper corpusId=\"265216855\" paperTitle=\"(Nabiilah et al., 2024)\" isShortName></Paper>. This suggests that end-to-end BERT-based approaches may offer performance benefits, albeit potentially at higher computational costs compared to FastText-based alternatives.", "citations": [{"id": "(Edwards et al., 2020)", "paper": {"corpus_id": 227231089, "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification", "year": 2020, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "A. Edwards", "authorId": "1383074767"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}, {"name": "H\u00e9l\u00e8ne de Ribaupierre", "authorId": "2750681"}, {"name": "A. Preece", "authorId": "1762890"}], "n_citations": 25}, "snippets": ["In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline", "BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments."], "score": 0.88037109375}, {"id": "(Frissen et al., 2022)", "paper": {"corpus_id": 253015767, "title": "A machine learning approach to recognize bias and discrimination in job advertisements", "year": 2022, "venue": "Ai & Society", "authors": [{"name": "Richard Frissen", "authorId": "2188338310"}, {"name": "K. Adebayo", "authorId": "2848544"}, {"name": "Rohan Nanda", "authorId": "39418246"}], "n_citations": 19}, "snippets": ["The results indicate that the RF classifier with BERT word embeddings as textual feature achieved the best performance. This illustrates that contextual word embedding representations such as BERT had a superior performance over the non-contextual word embeddings such as FastText and Word-2vec", "Figures 7 and 8 present the confusion matrices of the two best performing models: (1) RF classifier with FastText word embeddings and (2) RF classifier with BERT word embeddings."], "score": 0.50244140625}, {"id": "(Ning et al., 2023)", "paper": {"corpus_id": 260379057, "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kun-Peng Ning", "authorId": "66266326"}, {"name": "Ming Pang", "authorId": "2053434618"}, {"name": "Zheng Fang", "authorId": "2072874946"}, {"name": "Xue Jiang", "authorId": "2226458237"}, {"name": "Xi-Wei Zhao", "authorId": "2226511841"}, {"name": "Changping Peng", "authorId": "1949218014"}, {"name": "Zhangang Lin", "authorId": "2146396439"}, {"name": "Jinghe Hu", "authorId": "22528137"}, {"name": "Jingping Shao", "authorId": "2118926502"}], "n_citations": 0}, "snippets": ["Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."], "score": 0.73974609375}, {"id": "(Nabiilah et al., 2024)", "paper": {"corpus_id": 265216855, "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification", "year": 2024, "venue": "International Journal of Electrical and Computer Engineering (IJECE)", "authors": [{"name": "Ghinaa Zain Nabiilah", "authorId": "2148592978"}, {"name": "Islam Nur Alam", "authorId": "2186587011"}, {"name": "Eko Setyo Purwanto", "authorId": "2142224011"}, {"name": "Muhammad Fadlan Hidayat", "authorId": "2266784462"}], "n_citations": 10}, "snippets": ["In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification."], "score": 0.53173828125}], "table": null}, {"title": "Application in Pre-training Data Filtering", "tldr": "Both FastText and BERT-based approaches have been applied to pre-training data filtering tasks with varying degrees of success, though limited research directly compares their effectiveness. Simple classifiers like bigram-based models have proven surprisingly effective in some cases, challenging the assumption that more complex models always yield better results. (1 source)", "text": "\nWhen applied specifically to pre-training data filtering, both FastText and BERT-based approaches have been employed in various contexts, though comprehensive comparative studies remain limited. An interesting finding comes from Thrush et al., who demonstrated that in controlled pre-training experiments at the 160M parameter scale across 8 benchmarks, their approach outperformed DSIR (a more complex method) on every benchmark while matching the performance of \"the best data selector found in DataComp-LM, a hand-engineered bigram classifier\" <Paper corpusId=\"272524632\" paperTitle=\"(Thrush et al., 2024)\" isShortName></Paper>. This result is particularly noteworthy as it suggests that simpler classification approaches like bigram-based methods can be competitive with more sophisticated techniques for pre-training data filtering.\n\nThe finding challenges the assumption that more complex embedding models necessarily lead to better filtering results for pre-training data. This has important implications for the design of data filtering pipelines, suggesting that computational efficiency might be achieved without significant performance sacrifices by employing simpler classification approaches in some scenarios <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. However, the limited research directly comparing FastText and BERT-based approaches specifically for pre-training data filtering represents a gap in the current literature, particularly as it relates to very large-scale models like GPT-3 and Llama, where the scale of pre-training data necessitates careful consideration of the computational efficiency versus filtering accuracy trade-off.", "citations": [{"id": "(Thrush et al., 2024)", "paper": {"corpus_id": 272524632, "title": "Improving Pretraining Data Using Perplexity Correlations", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Tristan Thrush", "authorId": "1500242049"}, {"name": "Christopher Potts", "authorId": "2279335958"}, {"name": "Tatsunori Hashimoto", "authorId": "2214604036"}], "n_citations": 22}, "snippets": ["In controlled pretraining experiments at the 160M parameter scale on 8 benchmarks, our approach outperforms DSIR on every benchmark, while matching the best data selector found in DataComp-LM, a hand-engineered bigram classifier."], "score": 0.5009765625}], "table": null}, {"title": "Impact on Downstream Performance", "tldr": "Research shows that the choice between FastText and BERT-based filtering approaches has measurable impacts on downstream model performance, with each approach offering distinct advantages. BERT generally demonstrates better generalization capabilities for unseen data, while FastText shows strengths in efficiently processing familiar patterns. (3 sources)", "text": "\nThe impact of classifier choice for pre-training data filtering extends beyond filtering accuracy metrics to affect downstream model performance in significant ways. Edwards et al. found that in settings with limited training data, FastText coupled with domain-specific word embeddings showed more robust performance than BERT, which typically requires larger amounts of training data to perform effectively <Paper corpusId=\"227231089\" paperTitle=\"(Edwards et al., 2020)\" isShortName></Paper>. This finding has important implications for downstream tasks where labeled examples may be scarce, suggesting that simpler FastText-based filtering might actually preserve more useful training data in such scenarios.\n\nHowever, the generalization capabilities of different filtering approaches appear to have distinct effects on downstream task performance. Ning et al. observed that FastText models excel at \"remembering\" patterns from training data, performing better on test sets with high overlap with training data (99.89% overlap), while BERT models demonstrate superior \"generalization\" capabilities on unseen or low-frequency data <Paper corpusId=\"260379057\" paperTitle=\"(Ning et al., 2023)\" isShortName></Paper>. This distinction is particularly relevant for downstream tasks that require models to handle novel inputs or domains not extensively covered in pre-training data, where BERT-based filtering might preserve more generalizable examples.\n\nThe multi-class classification performance differences between FastText and BERT approaches identified by Nabiilah et al., where BERT achieved an F1 score of 84% when used for both word embedding and classification <Paper corpusId=\"265216855\" paperTitle=\"(Nabiilah et al., 2024)\" isShortName></Paper>, further suggest that the choice of filtering approach may have nuanced effects on downstream performance. More sophisticated BERT-based filters might better preserve the complexity and contextual richness needed for certain downstream tasks, while potentially filtering out more borderline examples that could still provide useful signal for other applications.\n\nWhile direct empirical studies comparing the downstream performance effects of FastText versus BERT-based filtering approaches specifically for large language models like BERT, GPT-3, and Llama remain limited, the available evidence suggests that the optimal filtering approach likely depends on the specific downstream tasks of interest, the scale of available pre-training data, and the computational resources available for the filtering process <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Edwards et al., 2020)", "paper": {"corpus_id": 227231089, "title": "Go Simple and Pre-Train on Domain-Specific Corpora: On the Role of Training Data for Text Classification", "year": 2020, "venue": "International Conference on Computational Linguistics", "authors": [{"name": "A. Edwards", "authorId": "1383074767"}, {"name": "Jos\u00e9 Camacho-Collados", "authorId": "1387447871"}, {"name": "H\u00e9l\u00e8ne de Ribaupierre", "authorId": "2750681"}, {"name": "A. Preece", "authorId": "1762890"}], "n_citations": 25}, "snippets": ["In this paper, we analyzed the role of training and unlabeled domain-specific data in supervised text classification. We compared both linear and neural models based on transformer-based language models. In settings with small training data, a simple method such as fastText coupled with domain-specific word embeddings appear to be more robust than a more data-consuming model such as BERT, even when BERT is pre-trained on domain-relevant data. However, the same classifier with generic pretrained word embeddings does not perform consistently better than a traditional frequency-based linear baseline", "BERT, pre-trained on domain-specific data (i.e., Twitter) leads to improvements over generic BERT, especially for few-shot experiments."], "score": 0.88037109375}, {"id": "(Ning et al., 2023)", "paper": {"corpus_id": 260379057, "title": "Towards Better Query Classification with Multi-Expert Knowledge Condensation in JD Ads Search", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Kun-Peng Ning", "authorId": "66266326"}, {"name": "Ming Pang", "authorId": "2053434618"}, {"name": "Zheng Fang", "authorId": "2072874946"}, {"name": "Xue Jiang", "authorId": "2226458237"}, {"name": "Xi-Wei Zhao", "authorId": "2226511841"}, {"name": "Changping Peng", "authorId": "1949218014"}, {"name": "Zhangang Lin", "authorId": "2146396439"}, {"name": "Jinghe Hu", "authorId": "22528137"}, {"name": "Jingping Shao", "authorId": "2118926502"}], "n_citations": 0}, "snippets": ["Table 1 demonstrates the performance comparison of the Fast-Text and BERT models on various evaluation datasets. We find that the FastText model can do better on the training set and testing set (T+0 day), while the BERT model performs better on the testing set (T+1 day). As 99.89% of test queries on T+0 day have appeared in the training set, it means that the FastText model can be remembered better. On the other hand, as only 57.47% of test queries on T+1 day has exposed in the training set, it means that the BERT model can generalize better on some unseen or low-frequency search query. In other words, in the E-commerce search query classification task, the FastText is better at remembering while the BERT is better at generalization."], "score": 0.73974609375}, {"id": "(Nabiilah et al., 2024)", "paper": {"corpus_id": 265216855, "title": "Indonesian multilabel classification using IndoBERT embedding and MBERT classification", "year": 2024, "venue": "International Journal of Electrical and Computer Engineering (IJECE)", "authors": [{"name": "Ghinaa Zain Nabiilah", "authorId": "2148592978"}, {"name": "Islam Nur Alam", "authorId": "2186587011"}, {"name": "Eko Setyo Purwanto", "authorId": "2142224011"}, {"name": "Muhammad Fadlan Hidayat", "authorId": "2266784462"}], "n_citations": 10}, "snippets": ["In this study, a comparison was made using the FastText and BERT word embedding methods classified by the CNN model and bidirectional LSTM (BiLSTM). In addition, a comparison was also made using the entire word embedding model and classification using BERT. The optimal result of this study is using BERT on word embedding and classification with an F1 value of 84% in multi-class classification."], "score": 0.53173828125}], "table": null}, {"title": "Considerations for Different LLM Scales", "tldr": "The effectiveness of FastText versus BERT-based filtering approaches varies significantly across different LLM scales, with computational efficiency becoming increasingly critical for larger models. Smaller models may benefit more from BERT's contextual understanding, while larger models like GPT-3 and Llama often employ hybrid or tiered approaches that balance efficiency and accuracy. (LLM Memory)", "text": "\nWhen considering filtering approaches across different LLM scales, the trade-offs between computational efficiency and classification accuracy become increasingly important as model size grows. For smaller models in the range of hundreds of millions of parameters, such as BERT-base (110M parameters), the computational cost of BERT-based filtering approaches may be justified by the improved classification accuracy and contextual understanding they provide <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. These smaller models are more sensitive to the quality of each training example due to their limited parameter space, making more sophisticated filtering potentially worthwhile despite higher computational costs.\n\nAs models scale to billions of parameters, like GPT-3 (175B) and Llama 2 (70B), the computational burden of filtering exponentially larger datasets becomes a critical consideration. In these cases, research suggests a tiered approach may be optimal, where computationally efficient methods like FastText or even simpler n-gram classifiers are used for initial broad filtering, followed by more targeted BERT-based classification for ambiguous or borderline cases <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This tiered strategy allows for balancing efficiency and accuracy at scale, addressing the practical challenges of filtering trillions of tokens.\n\nThe scale of data processing required for the largest models also introduces engineering considerations that favor FastText in certain scenarios. FastText's ability to process text efficiently makes it particularly valuable in the initial stages of data pipeline development for very large models, where iterative filtering approaches may be necessary <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. Additionally, FastText's lower memory requirements enable distributed filtering across multiple machines more easily than BERT-based approaches, which can be crucial when processing the massive datasets used to train models like GPT-3 and Llama.\n\nResource constraints also play an important role in filtering approach selection across different model scales. Organizations with limited computational resources may find FastText's efficiency enables more comprehensive data filtering than would be possible with BERT-based approaches given the same compute budget <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">. This practical consideration has led some researchers working on smaller open-source models to favor FastText despite potential performance advantages of BERT-based approaches, particularly when scaling to languages or domains with limited resources.\n\nThe empirical research gap comparing these approaches across different model scales represents an important area for future work, as the field continues to develop both larger models requiring massive datasets and smaller specialized models that may benefit from more targeted filtering approaches <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [], "table": null}], "cost": 0.16296}}
