{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in comparisons between Retrieval-Augmented Generation (RAG) and fine-tuning approaches for improving generative AI models?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in comparisons between Retrieval-Augmented Generation (RAG) and fine-tuning approaches for enhancing the performance and capabilities of generative AI models."}}, {"clarifying_question2": "Do you want recommendations for papers that discuss integrating RAG and fine-tuning techniques, or are you looking for resources that evaluate them separately?", "clarifying_answer2": {"clarifying_answer": "I am looking for recommendations for papers that discuss integrating RAG and fine-tuning techniques, ideally focusing on their combined use for GenAI applications."}}, {"clarifying_question3": "Are you seeking practical implementation guides, theoretical analyses, or empirical studies related to RAG versus fine-tuning in GenAI systems?", "clarifying_answer3": {"clarifying_answer": "I'm primarily interested in empirical studies and practical implementation guides comparing RAG and fine-tuning approaches in GenAI systems."}}], "better_query": "Are there empirical studies comparing the effectiveness of Retrieval-Augmented Generation (RAG) and fine-tuning approaches for improving generative AI models?", "better_answer": {"sections": [{"title": "Introduction to RAG and Fine-tuning", "tldr": "Retrieval-Augmented Generation (RAG) and fine-tuning represent two distinct approaches to enhancing the performance of large language models. While RAG improves model outputs by retrieving relevant external knowledge during inference, fine-tuning adapts the model's parameters through additional training on specific datasets. (LLM Memory)", "text": "\nRetrieval-Augmented Generation (RAG) and fine-tuning are two primary methods for improving the capabilities of generative AI models, each with distinct approaches and tradeoffs. RAG works by combining a retrieval system with a generation model, allowing the model to access and leverage external knowledge sources during inference without modifying the model's parameters. This approach enables models to incorporate up-to-date or domain-specific information that wasn't available during their training, improving factuality and reducing hallucinations. The retrieval component searches through a knowledge base (such as documents, databases, or the internet) to find relevant information that can inform the generation process. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nFine-tuning, in contrast, involves additional training of a pre-trained model on specific datasets to adapt its parameters for particular tasks or domains. This process modifies the model's internal knowledge and behavior patterns through gradient-based optimization. Fine-tuning can be performed using various techniques, including full fine-tuning (updating all model parameters), parameter-efficient fine-tuning (PEFT) methods like LoRA that update only a subset of parameters, or instruction tuning to align the model with specific formats and requirements. Fine-tuning generally requires computational resources for training and a high-quality dataset that represents the target domain or task. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThese approaches serve different purposes and can be selected based on specific requirements: RAG is particularly useful when up-to-date or specialized information is needed without retraining, while fine-tuning may be more appropriate when the goal is to systematically adapt the model's behavior for a particular domain or task format. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Comparative Performance Studies", "tldr": "Empirical studies comparing RAG and fine-tuning show that each approach excels in different scenarios, with RAG often demonstrating superior performance in knowledge-intensive tasks and handling unseen information, while fine-tuning methods like DoRA can achieve higher accuracy in specific domains. (7 sources)", "text": "\nSeveral comparative studies have evaluated the performance of Retrieval-Augmented Generation (RAG) against fine-tuning approaches across various tasks and domains. Research by Ovadia et al. demonstrates that RAG outperforms unsupervised fine-tuning, particularly when dealing with new or previously unseen knowledge, highlighting its effectiveness for knowledge injection and model adaptation <Paper corpusId=\"269292881\" paperTitle=\"(Efeoglu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>. This advantage is especially pronounced in situations requiring up-to-date or specialized information that wasn't included in the original training data.\n\nIn knowledge-intensive tasks, RAG models have shown significant improvements over traditional fine-tuned models like BART and T5. The combination of retrieval mechanisms with generation capabilities enables RAG to dynamically access external knowledge bases during inference, substantially enhancing knowledge consistency (0.73) and reasoning capability (0.80). Further improvements in RAG architectures, such as RAG+T (RAG+Text), have pushed reasoning abilities even higher (0.84), demonstrating the potential of retrieval-enhanced generative models to overcome limitations in complex reasoning tasks <Paper corpusId=\"273850363\" paperTitle=\"(Dong et al., 2024)\" isShortName></Paper>.\n\nFor classification tasks, Class-RAG has been shown to outperform model fine-tuning approaches, offering greater flexibility, transparency in decision-making, and increased robustness against adversarial attacks <Paper corpusId=\"273502659\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>. However, the performance tradeoffs between these approaches can vary significantly based on the specific application context.\n\nA large-scale empirical evaluation comparing RAG with fine-tuning methods such as LoRA and DoRA on FAQ-based queries revealed that DoRA achieved the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query) in domain-specific applications <Paper corpusId=\"276408784\" paperTitle=\"(Baqar et al., 2025)\" isShortName></Paper>. This suggests that advanced fine-tuning techniques can outperform RAG in certain specialized scenarios where performance metrics like accuracy and latency are critical.\n\nIn industrial code completion tasks, comprehensive comparisons between RAG and fine-tuning have shown that RAG implementations with appropriate embedding models can achieve higher accuracy than fine-tuning alone. Notably, BM25 demonstrated superior retrieval effectiveness and efficiency among the RAG methods studied. The research also found that RAG exhibits better scalability than fine-tuning, with more sustained performance gains as the codebase size increases <Paper corpusId=\"278782961\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>.\n\nAn important consideration when implementing RAG is that while it generally improves factuality and accuracy without modifying model parameters, fine-tuning may compromise the model's general generation capabilities by altering its parameters <Paper corpusId=\"270123034\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>. This suggests that RAG may be preferable in scenarios where maintaining the model's general capabilities is important alongside enhancing performance on specific tasks.\n\nInterestingly, research indicates that RAG and fine-tuning approaches are not mutually exclusive but rather complementary, with their combination leading to further performance improvements <Paper corpusId=\"278782961\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. This points to the potential benefits of hybrid approaches that leverage the strengths of both methodologies.", "citations": [{"id": "(Efeoglu et al., 2024)", "paper": {"corpus_id": 269292881, "title": "Retrieval-Augmented Generation-based Relation Extraction", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Sefika Efeoglu", "authorId": "2189018699"}, {"name": "Adrian Paschke", "authorId": "2259621860"}], "n_citations": 9}, "snippets": ["Ovadia et al. [25] evaluates the knowledge injection capacities of both fine-tuning and the RAG approach and found that LLMs dealt with performance problems through unsupervised fine-tuning while RAG outperformed the fine-tuning approach in unsupervised learning."], "score": 0.93310546875}, {"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 273969615, "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zilun Zhang", "authorId": "2270181751"}, {"name": "Haozhan Shen", "authorId": "2174678931"}, {"name": "Tiancheng Zhao", "authorId": "8200875"}, {"name": "Yuhao Wang", "authorId": "2330774884"}, {"name": "Bin Chen", "authorId": "2330612748"}, {"name": "Yuxiang Cai", "authorId": "2149196373"}, {"name": "Yongheng Shang", "authorId": "2093090552"}, {"name": "Jianwei Yin", "authorId": "2111612160"}], "n_citations": 3}, "snippets": ["A comparative study by Ovadia et al. [98] shows that RAG outperforms unsupervised fine-tuning, particularly in scenarios involving new or unseen knowledge, underscoring its superiority in knowledge injection and model adaptation."], "score": 0.9248046875}, {"id": "(Dong et al., 2024)", "paper": {"corpus_id": 273850363, "title": "Advanced RAG Models with Graph Structures: Optimizing Complex Knowledge Reasoning and Text Generation", "year": 2024, "venue": "2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC)", "authors": [{"name": "Yuxin Dong", "authorId": "2326920545"}, {"name": "Shuo Wang", "authorId": "2327007198"}, {"name": "Hongye Zheng", "authorId": "2327003963"}, {"name": "Jiajing Chen", "authorId": "2322450076"}, {"name": "Zhenhong Zhang", "authorId": "2322450970"}, {"name": "Chihang Wang", "authorId": "2322612972"}], "n_citations": 13}, "snippets": ["In contrast, the RAG model and its improved version RAG+T (RAG+Text) perform significantly better than BART and T5 in these three indicators. The RAG model combines the retrieval module to enable it to dynamically access the external knowledge base during the generation process, thereby significantly improving knowledge consistency (0.73) and reasoning capability (0.80). The performance of RAG+T is further improved, especially in terms of reasoning ability, which reaches 0.84, which shows the great potential of retrieval-enhanced generative models in complex tasks. By incorporating more relevant knowledge into the generation process, RAG+T shows stronger ability to deal with complex background information and deep reasoning, further narrowing the limitations of generative models in dealing with knowledge-intensive tasks."], "score": 0.89208984375}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 273502659, "title": "Class-RAG: Real-Time Content Moderation with Retrieval Augmented Generation", "year": 2024, "venue": "", "authors": [{"name": "Jianfa Chen", "authorId": "2327003851"}, {"name": "Emily Shen", "authorId": "2326992786"}, {"name": "Trupti Bavalatti", "authorId": "2297187181"}, {"name": "Xiaowen Lin", "authorId": "2327028660"}, {"name": "Yongkai Wang", "authorId": "2326986310"}, {"name": "Shuming Hu", "authorId": "2327158340"}, {"name": "Harihar Subramanyam", "authorId": "2322094813"}, {"name": "Ksheeraj Sai Vepuri", "authorId": "2149726609"}, {"name": "Ming Jiang", "authorId": "2327303021"}, {"name": "Ji Qi", "authorId": "2327505613"}, {"name": "Li Chen", "authorId": "2287762612"}, {"name": "Nan Jiang", "authorId": "2326964342"}, {"name": "Ankit Jain", "authorId": "2287848816"}], "n_citations": 2}, "snippets": ["Compared to model fine-tuning, Class-RAG demonstrates flexibility and transparency in decision-making, outperforms on classification and is more robust against adversarial attack, as evidenced by empirical studies."], "score": 0.91845703125}, {"id": "(Baqar et al., 2025)", "paper": {"corpus_id": 276408784, "title": "Hallucinations and Truth: A Comprehensive Accuracy Evaluation of RAG, LoRA and DoRA", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mohammad Baqar", "authorId": "2316485338"}, {"name": "Rajat Khanda", "authorId": "69923048"}], "n_citations": 1}, "snippets": ["This paper presents a large-scale empirical evaluation of RAG, LoRA, and DoRA, with model fine-tuning and generation performance assessed on 20,000 FAQ-based queries, while the knowledge base spans 400,000 entries. The study analyzes key performance metrics such as accuracy, relevance, and inference latency. Experimental results demonstrate that DoRA achieves the highest accuracy (90.1%), relevance score (0.88), and lowest latency (110 ms per query), outperforming both LoRA and RAG in real-world, domain-specific generative AI applications."], "score": 0.9873046875}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 278782961, "title": "RAG or Fine-tuning? A Comparative Study on LCMs-based Code Completion in Industry", "year": 2025, "venue": "", "authors": [{"name": "Chaozheng Wang", "authorId": "2135764153"}, {"name": "Zezhou Yang", "authorId": "2155450982"}, {"name": "Shuzheng Gao", "authorId": "2112314113"}, {"name": "Cuiyun Gao", "authorId": "2267893922"}, {"name": "Ting Peng", "authorId": "2299029254"}, {"name": "Hailiang Huang", "authorId": "2265772572"}, {"name": "Yuetang Deng", "authorId": "2299160326"}, {"name": "Michael R. Lyu", "authorId": "2338266828"}], "n_citations": 0}, "snippets": ["No prior research has explored the trade-off of the two approaches in industrial scenarios. To mitigate the gap, we comprehensively compare the two paradigms including Retrieval-Augmented Generation (RAG) and Fine-tuning (FT), for industrial code completion in this paper", "Our findings reveal that RAG, when implemented with appropriate embedding models that map code snippets into dense vector representations, can achieve higher accuracy than fine-tuning alone. Specifically, BM25 presents superior retrieval effectiveness and efficiency among studied RAG methods. Moreover, RAG and fine-tuning are orthogonal and their combination leads to further improvement. We also observe that RAG demonstrates better scalability than FT, showing more sustained performance gains with larger scales of codebase."], "score": 0.970703125}, {"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 270123034, "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models", "year": 2024, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yutao Zhu", "authorId": "1900406"}, {"name": "Zhaoheng Huang", "authorId": "2187935160"}, {"name": "Zhicheng Dou", "authorId": "1897235"}, {"name": "Ji-Rong Wen", "authorId": "2186578511"}], "n_citations": 6}, "snippets": ["Retrieval-augmented generation (RAG) is a promising way to improve large language models (LLMs) for generating more factual, accurate, and up-to-date content. Existing methods either optimize prompts to guide LLMs in leveraging retrieved information or directly fine-tune LLMs to adapt to RAG scenarios. Although fine-tuning can yield better performance, it often compromises the LLMs' general generation capabilities by modifying their parameters."], "score": 0.90380859375}], "table": null}, {"title": "Domain-Specific Comparisons", "tldr": "Empirical studies across various domains show that RAG and fine-tuning exhibit different performance characteristics depending on the specific field; in financial, medical, and enterprise settings, domain-adapted RAG implementations generally outperform generic approaches, while hybrid methods combining both techniques often yield the best results. (16 sources)", "text": "\nDomain-specific evaluations of RAG and fine-tuning approaches reveal important insights about their relative effectiveness across different fields. In the financial domain, research using the FinanceBench SEC financial filings dataset demonstrated that combining a fine-tuned embedding model with a fine-tuned LLM achieved better accuracy than generic models within RAG systems. Notably, the researchers found that greater performance gains were attributable to fine-tuned embedding models than to fine-tuned language models <Paper corpusId=\"269214364\" paperTitle=\"(Nguyen et al., 2024)\" isShortName></Paper>.\n\nFor enterprise knowledge management, traditional approaches typically involve choosing between RAG or fine-tuned LLMs. While fine-tuned LLMs can be effective, they lack guarantees of factual accuracy. Conversely, RAG solutions ensure factual precision by retrieving relevant information from knowledge bases, but may perform suboptimally when relying on pre-trained models that aren't aligned with enterprise-specific data <Paper corpusId=\"270214689\" paperTitle=\"(Rathinasamy et al., 2024)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. This finding highlights the importance of domain adaptation in retrieval components.\n\nIn the medical domain, RAG technology applications remain relatively new, with systems like MEDRAG and Self-BioRAG showing promise. Self-BioRAG integrates reflective tokens to evaluate retrieval timing, document relevance, and generation quality. However, empirical evidence suggests that these RAG approaches still struggle to surpass models specifically optimized for medical datasets, possibly due to limitations of smaller models in multitask integration <Paper corpusId=\"271571143\" paperTitle=\"(Long et al., 2024)\" isShortName></Paper> <Paper corpusId=\"267312134\" paperTitle=\"(Jeong et al., 2024)\" isShortName></Paper>.\n\nRAG models demonstrate particular advantages in domains where information evolves rapidly, such as medical research, financial news, and legal proceedings. Their ability to update knowledge bases without retraining makes them especially suitable for these dynamic fields <Paper corpusId=\"273403982\" paperTitle=\"(Gupta et al., 2024)\" isShortName></Paper>. In clinical applications, comparative studies show that Parameter-Efficient Fine-Tuning (PEFT) significantly improves model performance in both zero-shot and few-shot scenarios, while RAG significantly enhances performance only in few-shot learning. Interestingly, few-shot learning with RAG significantly outperforms zero-shot learning with RAG, suggesting that the combination of techniques can be particularly powerful <Paper corpusId=\"276526965\" paperTitle=\"(Vithanage et al., 2025)\" isShortName></Paper>.\n\nFor workflow generation from natural language instructions, RAG has proven effective in reducing hallucinations in structured outputs <Paper corpusId=\"277104712\" paperTitle=\"(Yu et al., 2025)\" isShortName></Paper> <Paper corpusId=\"269137180\" paperTitle=\"(B'echard et al., 2024)\" isShortName></Paper>. The improved quality of structured outputs demonstrates RAG's value in enterprise applications requiring high precision.\n\nRecent innovations in RAG implementation show promising results across various domains. The RAG Playground framework, which implements and compares three retrieval approaches (naive vector search, reranking, and hybrid vector-keyword search), has demonstrated significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on multi-metric evaluations <Paper corpusId=\"274788878\" paperTitle=\"(Papadimitriou et al., 2024)\" isShortName></Paper>. Similarly, the ALoF-TRAG approach has been shown to improve both citation accuracy and answer accuracy across almost all datasets compared to base RAG models in experiments spanning 20 datasets in 26 languages across various domains <Paper corpusId=\"275788867\" paperTitle=\"(Devine, 2025)\" isShortName></Paper>.\n\nComparative studies examining the effectiveness of RAG and fine-tuning for question answering over less popular factual knowledge have found that performance varies significantly depending on the specific setup used <Paper corpusId=\"268248396\" paperTitle=\"(Soudani et al., 2024)\" isShortName></Paper>. Research using benchmark datasets like SQuAD, MS MARCO, and SQL CREATE TABLE statements further validates the complementary nature of these approaches <Paper corpusId=\"276355526\" paperTitle=\"(Budakoglu et al., 2025)\" isShortName></Paper>.\n\nWhile multiple studies have evaluated RAG systems across various tasks, including multi-hop question answering, biomedical question answering, and text generation, systematic comparisons between traditional RAG and newer variants like GraphRAG remain limited <Paper corpusId=\"276408622\" paperTitle=\"(Han et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261530434\" paperTitle=\"(Chen et al., 2023)\" isShortName></Paper> <Paper corpusId=\"263152125\" paperTitle=\"(Tian et al., 2023)\" isShortName></Paper>. This suggests an opportunity for more comprehensive cross-domain evaluations to better understand the relative strengths of different approaches.", "citations": [{"id": "(Nguyen et al., 2024)", "paper": {"corpus_id": 269214364, "title": "Enhancing Q&A with Domain-Specific Fine-Tuning and Iterative Reasoning: A Comparative Study", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zooey Nguyen", "authorId": "2297189569"}, {"name": "Anthony Annunziata", "authorId": "2297188041"}, {"name": "Vinh Luong", "authorId": "69442223"}, {"name": "Sang Dinh", "authorId": "2297188221"}, {"name": "Quynh Le", "authorId": "2297190249"}, {"name": "A. Ha", "authorId": "2297189614"}, {"name": "Chanh Le", "authorId": "2297189915"}, {"name": "Hong An Phan", "authorId": "2297189697"}, {"name": "Shruti Raghavan", "authorId": "2058395065"}, {"name": "Christopher Nguyen", "authorId": "2297324474"}], "n_citations": 4}, "snippets": ["This paper investigates the impact of domain-specific model fine-tuning and of reasoning mechanisms on the performance of question-answering (Q&A) systems powered by large language models (LLMs) and Retrieval-Augmented Generation (RAG). Using the FinanceBench SEC financial filings dataset, we observe that, for RAG, combining a fine-tuned embedding model with a fine-tuned LLM achieves better accuracy than generic models, with relatively greater gains attributable to fine-tuned embedding models."], "score": 0.8779296875}, {"id": "(Rathinasamy et al., 2024)", "paper": {"corpus_id": 270214689, "title": "EnterpriseEM: Fine-tuned Embeddings for Enterprise Semantic Search", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Kamalkumar Rathinasamy", "authorId": "66945149"}, {"name": "Jayarama Nettar", "authorId": "2304474062"}, {"name": "Amit Kumar", "authorId": "2304471263"}, {"name": "Vishal Manchanda", "authorId": "2304472624"}, {"name": "Arun Vijayakumar", "authorId": "2304472592"}, {"name": "Ayush Kataria", "authorId": "2275256433"}, {"name": "Venkateshprasanna Manjunath", "authorId": "2304471029"}, {"name": "GS Chidambaram", "authorId": "2304472432"}, {"name": "Jaskirat Sodhi", "authorId": "31722494"}, {"name": "Shoeb Shaikh", "authorId": "2304453176"}, {"name": "Wasim Akhtar Khan", "authorId": "2304474234"}, {"name": "Prashant Singh", "authorId": "2304542089"}, {"name": "Tanishq Dattatray Ige", "authorId": "2304475433"}, {"name": "V. Tiwari", "authorId": "2334485471"}, {"name": "Rajab Ali Mondal", "authorId": "2304471138"}, {"name": "K. Harshini", "authorId": "2304472540"}, {"name": "S. Reka", "authorId": "2129197171"}, {"name": "Chetana Amancharla", "authorId": "2304472698"}, {"name": "Faiz ur Rahman", "authorId": "2302622929"}, {"name": "A. HarikrishnanP", "authorId": "2304471871"}, {"name": "Indraneel Saha", "authorId": "2304475424"}, {"name": "Bhavya Tiwary", "authorId": "2304471930"}, {"name": "Navin Shankar Patel", "authorId": "2305129400"}, {"name": "S. PradeepT", "authorId": "2304470943"}, {"name": "J. BalajiA", "authorId": "2304475392"}, {"name": "Priyapravas", "authorId": "2304472627"}, {"name": "Mohammed Rafee Tarafdar", "authorId": "2304472453"}], "n_citations": 1}, "snippets": ["In the context of enterprises accumulating proprietary unstructured data, AI-driven information retrieval solutions have emerged as vital tools for extracting relevant answers to employee queries. Traditional methods for developing such solutions often involve choosing between Retrieval Augmented Generation (RAG) or fine-tuned Large Language Models (LLMs). However, fine-tuned LLMs, comprising only generative models, lack a guarantee of factual accuracy, while RAG, comprising an embedding model and a generative model, assures factual precision (Lewis at al., 2020 [1]). Despite their superior performance in general, RAG based solutions often rely on pre-trained models, potentially leading to suboptimal alignment with enterprise-specific data.\n\nAddressing this challenge entails exploring two potential avenues: Firstly, recent studies such as RAFT (Zhang et al., 2024 [2]) explore the integration of fine-tuned generative models within a RAG pipeline to enhance accuracy, albeit requiring substantial domain-specific data to fine-tune the generative models. Alternatively, leveraging domain-specific embedding models within a RAG pipeline to enhance accuracy remains an underexplored area."], "score": 0.8759765625}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."], "score": 0.0}, {"id": "(Long et al., 2024)", "paper": {"corpus_id": 271571143, "title": "Bailicai: A Domain-Optimized Retrieval-Augmented Generation Framework for Medical Applications", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Cui Long", "authorId": "2313916812"}, {"name": "Yongbin Liu", "authorId": "47909171"}, {"name": "Chunping Ouyang", "authorId": "16318808"}, {"name": "Ying Yu", "authorId": "2296267575"}], "n_citations": 5}, "snippets": ["In the medical domain, the application of RAG technology remains in its nascent stages. The MEDRAG system assessed performance variations across diverse retrievers and corpora in medical question-answering tasks [37]. The Self-BioRAG (Jeong et al., 2024) project integrated Self-RAG [5] technology in medicine, optimizing it through reflective tokens that address retrieval timing, evaluate the relevance and supporting capacity of retrieved documents in answer generation, and assess the quality of generated outputs. Nevertheless, empirical evidence suggests that this approach fails to surpass the performance of models specifically optimized for medical datasets, potentially attributable to the limitations of smaller models in multitask integration, as demonstrated by the AUTOACT study [28]."], "score": 0.8955078125}, {"id": "(Jeong et al., 2024)", "paper": {"corpus_id": 267312134, "title": "Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models", "year": 2024, "venue": "Bioinform.", "authors": [{"name": "Minbyul Jeong", "authorId": "2281744951"}, {"name": "Jiwoong Sohn", "authorId": "2281744575"}, {"name": "Mujeen Sung", "authorId": "147610425"}, {"name": "Jaewoo Kang", "authorId": "2281792202"}], "n_citations": 33}, "snippets": ["Abstract Summary Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Similarly, Self-BioRAG outperforms RAG by 8% Rouge-1 score in generating more proficient answers on two long-form question-answering benchmarks on average. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains. Availability and implementation Self-BioRAG is available at https://github.com/dmis-lab/self-biorag."], "score": 0.0}, {"id": "(Gupta et al., 2024)", "paper": {"corpus_id": 273403982, "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Shailja Gupta", "authorId": "2311997786"}, {"name": "Rajesh Ranjan", "authorId": "2311893279"}, {"name": "Surya Narayan Singh", "authorId": "2321535962"}], "n_citations": 23}, "snippets": ["In addition to knowledge retrieval, RAG models excel at updating knowledge bases. Since the model fetches external documents for each query, it requires no retraining to incorporate the latest information. This flexibility makes RAG models particularly suitable for domains where information is constantly evolving, such as medical research, financial news, and legal proceedings. Furthermore, studies have shown that RAG models achieve superior results in a variety of knowledge-intensive tasks, including document summarization and, knowledge-grounded dialogues"], "score": 0.90185546875}, {"id": "(Vithanage et al., 2025)", "paper": {"corpus_id": 276526965, "title": "Adapting Generative Large Language Models for Information Extraction from Unstructured Electronic Health Records in Residential Aged Care: A Comparative Analysis of Training Approaches", "year": 2025, "venue": "J. Heal. Informatics Res.", "authors": [{"name": "D. Vithanage", "authorId": "2277877538"}, {"name": "C. Deng", "authorId": "144551284"}, {"name": "Lei Wang", "authorId": "2278214328"}, {"name": "M. Yin", "authorId": "1972725075"}, {"name": "M. Alkhalaf", "authorId": "2069756289"}, {"name": "Zhenyu Zhang", "authorId": "2109338789"}, {"name": "Yunshu Zhu", "authorId": "2117078401"}, {"name": "P. Yu", "authorId": "2231452266"}], "n_citations": 0}, "snippets": ["Results show that zero-shot and few-shot learning, whether combined with PEFT or RAG, achieve comparable performance across the clinical domains when the same prompting template is used. Few-shot learning significantly outperforms zero-shot learning when neither PEFT nor RAG is applied. Notably, PEFT significantly improves model performance in both zero-shot and few-shot learning; however, RAG significantly improves performance only in few-shot learning. After PEFT, the performance of zero-shot learning reaches a comparable level with few-shot learning. However, few-shot learning with RAG significantly outperforms zero-shot learning with RAG. We also found a similar level of performance between few-shot learning with RAG and zero-shot learning with PEFT."], "score": 0.9130859375}, {"id": "(Yu et al., 2025)", "paper": {"corpus_id": 277104712, "title": "RAG-KG-IL: A Multi-Agent Hybrid Framework for Reducing Hallucinations and Enhancing LLM Reasoning through RAG and Incremental Knowledge Graph Learning Integration", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Hong Qing Yu", "authorId": "2351728696"}, {"name": "Frank McQuade", "authorId": "2350756862"}], "n_citations": 3}, "snippets": ["B\u00e9chard et al. (B'echard et al., 2024) demonstrated the effectiveness of RAG in reducing hallucinations in structured outputs like workflow generation from natural language instructions."], "score": 0.8974609375}, {"id": "(B'echard et al., 2024)", "paper": {"corpus_id": 269137180, "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation", "year": 2024, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Patrice B'echard", "authorId": "2296597690"}, {"name": "Orlando Marquez Ayala", "authorId": "2296597772"}], "n_citations": 60}, "snippets": ["A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."], "score": 0.0}, {"id": "(Papadimitriou et al., 2024)", "paper": {"corpus_id": 274788878, "title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Ioannis Papadimitriou", "authorId": "2280268220"}, {"name": "Ilias Gialampoukidis", "authorId": "1988554"}, {"name": "S. Vrochidis", "authorId": "3019137"}, {"name": "Y. Kompatsiaris", "authorId": "1715604"}], "n_citations": 1}, "snippets": ["We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality."], "score": 0.9375}, {"id": "(Devine, 2025)", "paper": {"corpus_id": 275788867, "title": "ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Peter Devine", "authorId": "2341534946"}], "n_citations": 0}, "snippets": ["We demonstrate the effectiveness of ALoF-TRAG by performing experiments on 20 datasets in 26 languages across a variety of domains and comparing the accuracy to simply using the base LLM for RAG. We show that the ALoFTRAG approach improves both the citation accuracy and answer accuracy of RAG models across almost all datasets compared to the base RAG model."], "score": 0.9072265625}, {"id": "(Soudani et al., 2024)", "paper": {"corpus_id": 268248396, "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge", "year": 2024, "venue": "SIGIR-AP", "authors": [{"name": "Heydar Soudani", "authorId": "2165569122"}, {"name": "E. Kanoulas", "authorId": "1713134"}, {"name": "Faegheh Hasibi", "authorId": "1951737"}], "n_citations": 37}, "snippets": ["We study the effectiveness of fine-tuning and RAG approaches for question answering over less popular factual knowledge and compare the performance of these models across distinct setups: vanilla and fine-tuned models, both with and without RAG, using different data augmentation methods."], "score": 0.92822265625}, {"id": "(Budakoglu et al., 2025)", "paper": {"corpus_id": 276355526, "title": "Unveiling the Power of Large Language Models: A Comparative Study of Retrieval-Augmented Generation, Fine-Tuning, and Their Synergistic Fusion for Enhanced Performance", "year": 2025, "venue": "IEEE Access", "authors": [{"name": "G\u00fcls\u00fcm Budakoglu", "authorId": "2345470851"}, {"name": "Hakan Emekci", "authorId": "2345472890"}], "n_citations": 0}, "snippets": ["This study compares two salient techniques for retrieve-augmented generation (RAG) and fine-tuning along with a new hybrid method that combines both. In this study, we investigate the effectiveness of various methods using the Stanford Question Answering Dataset (SQuAD), Microsoft Machine Reading Comprehension (MS MARCO) and SQL CREATE TABLE statements."], "score": 0.970703125}, {"id": "(Han et al., 2025)", "paper": {"corpus_id": 276408622, "title": "RAG vs. GraphRAG: A Systematic Evaluation and Key Insights", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Haoyu Han", "authorId": "2049039664"}, {"name": "Harry Shomer", "authorId": "2220302956"}, {"name": "Yu Wang", "authorId": "2346107355"}, {"name": "Yongjia Lei", "authorId": "2338562947"}, {"name": "Kai Guo", "authorId": "2338271219"}, {"name": "Zhigang Hua", "authorId": "2293482433"}, {"name": "Bo Long", "authorId": "2338267824"}, {"name": "Hui Liu", "authorId": "2298005501"}, {"name": "Jiliang Tang", "authorId": "2330147642"}], "n_citations": 3}, "snippets": ["Several studies have evaluated the effectiveness of RAG systems across various tasks (Yu et al., 2024;(Chen et al., 2023)Es et al., 2023), such as multi-hop question answering (Tian et al., 2023), biomedical question answering (Xiong et al., 2024), and text generation (Liu et al., 2023). However, no existing study has simultaneously and systematically evaluated and compared RAG and GraphRAG on these general text-based tasks."], "score": 0.88916015625}, {"id": "(Chen et al., 2023)", "paper": {"corpus_id": 261530434, "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Jiawei Chen", "authorId": "2115448879"}, {"name": "Hongyu Lin", "authorId": "2116455765"}, {"name": "Xianpei Han", "authorId": "2118233348"}, {"name": "Le Sun", "authorId": "2110832778"}], "n_citations": 307}, "snippets": ["Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs."], "score": 0.0}, {"id": "(Tian et al., 2023)", "paper": {"corpus_id": 263152125, "title": "Graph Neural Prompting with Large Language Models", "year": 2023, "venue": "AAAI Conference on Artificial Intelligence", "authors": [{"name": "Yijun Tian", "authorId": "46879986"}, {"name": "Huan Song", "authorId": "2248096816"}, {"name": "Zichen Wang", "authorId": "2249432235"}, {"name": "Haozhu Wang", "authorId": "2256768980"}, {"name": "Ziqing Hu", "authorId": "2248753090"}, {"name": "Fang Wang", "authorId": "2262512203"}, {"name": "N. Chawla", "authorId": "144539424"}, {"name": "Panpan Xu", "authorId": "2248954229"}], "n_citations": 49}, "snippets": ["Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP."], "score": 0.0}], "table": null}, {"title": "Hybrid Approaches", "tldr": "Hybrid approaches that combine RAG with fine-tuning techniques demonstrate superior performance compared to either method alone across various tasks. These combined methodologies leverage the complementary strengths of both approaches, with recent innovations exploring parameter-efficient fine-tuning specifically optimized for retrieval-augmented systems. (8 sources)", "text": "\nResearch increasingly shows that combining fine-tuning with Retrieval Augmented Generation (RAG) produces responses with significantly improved accuracy compared to either approach used independently <Paper corpusId=\"267412954\" paperTitle=\"(Zhang et al._1, 2024)\" isShortName></Paper>. This synergistic effect leverages the complementary strengths of both methodologies\u2014the knowledge adaptation capabilities of fine-tuning with the external information access of RAG.\n\nRecent advances have introduced novel methodologies specifically designed for fine-tuning language models in RAG-based environments, particularly for knowledge-intensive tasks <Paper corpusId=\"270560505\" paperTitle=\"(Balakrishnan et al., 2024)\" isShortName></Paper>. These specialized fine-tuning approaches optimize the model's ability to effectively utilize retrieved information rather than treating RAG as merely an add-on to a standard fine-tuned model.\n\nComparative studies exploring Parameter-Efficient Fine-Tuning (PEFT) methods applied to RAG systems have revealed interesting performance differences between model architectures. Research shows that RETRO architectures (which integrate external retrieval during pre-training) generally outperform GPT models in zero-shot settings due to their enhanced contextual understanding capabilities. However, GPT models demonstrate greater performance gains during fine-tuning, suggesting they have more room for improvement when adapted using PEFT techniques <Paper corpusId=\"271039066\" paperTitle=\"(Ficek et al., 2024)\" isShortName></Paper>.\n\nIterative approaches that combine retrieval and generation have shown promise in addressing complex reasoning tasks. For example, ITER-RETGEN alternates between retrieval-augmented generation and generation-augmented retrieval, with each step refining results based on previous outputs. While effective for multi-hop question answering and commonsense reasoning, these methods still face challenges in optimal utilization of retrieved information, with studies showing approximately 20% of retrieved contexts lacking actual answers <Paper corpusId=\"273345967\" paperTitle=\"(Sanniboina et al., 2024)\" isShortName></Paper>.\n\nThe research community continues to develop more sophisticated hybrid approaches, with ongoing work comparing RAG implementations against established retrieval-enhanced models like Facebook AI's RAG and Google's REALM <Paper corpusId=\"274283400\" paperTitle=\"(Yao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>. Domain-specific hybrid systems that combine retrieval with specialized language models are also showing promise, with systems like PubMed <Paper corpusId=\"224820417\" paperTitle=\"(White, 2020)\" isShortName></Paper> and BioBERT demonstrating significant improvements in biomedical applications. BioBERT, for instance, achieves substantial performance gains across various biomedical text mining tasks, including named entity recognition (0.62% F1 score improvement), relation extraction (2.80% F1 score improvement), and question answering (12.24% MRR improvement) <Paper corpusId=\"59291975\" paperTitle=\"(Lee et al., 2019)\" isShortName></Paper>.", "citations": [{"id": "(Zhang et al._1, 2024)", "paper": {"corpus_id": 267412954, "title": "Enhancing Large Language Model Performance To Answer Questions and Extract Information More Accurately", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Liang Zhang", "authorId": "2279813822"}, {"name": "Katherine Jijo", "authorId": "2279831793"}, {"name": "Spurthi Setty", "authorId": "2282528163"}, {"name": "Eden Chung", "authorId": "2279830841"}, {"name": "Fatima Javid", "authorId": "2282539958"}, {"name": "Natan Vidra", "authorId": "2279830757"}, {"name": "Thomas Clifford", "authorId": "2279838243"}], "n_citations": 20}, "snippets": ["Notably, the combination of fine-tuning the LLM with a process known as Retrieval Augmented Generation (RAG) proves to generate responses with improved accuracy."], "score": 0.88623046875}, {"id": "(Balakrishnan et al., 2024)", "paper": {"corpus_id": 270560505, "title": "Evaluating the Efficacy of Open-Source LLMs in Enterprise-Specific RAG Systems: A Comparative Study of Performance and Scalability", "year": 2024, "venue": "IEEE India Conference", "authors": [{"name": "Gautam Balakrishnan", "authorId": "2356633197"}, {"name": "A. Purwar", "authorId": "33856997"}], "n_citations": 14}, "snippets": ["More recent work further advances the field by introducing novel methodologies for fine-tuning LLMs specifically for RAG tasks in knowledge-intensive environments [24]."], "score": 0.87158203125}, {"id": "(Ficek et al., 2024)", "paper": {"corpus_id": 271039066, "title": "GPT vs RETRO: Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Aleksander Ficek", "authorId": "2186740325"}, {"name": "Jiaqi Zeng", "authorId": "2266881428"}, {"name": "Oleksii Kuchaiev", "authorId": "2787022"}], "n_citations": 1}, "snippets": ["This study explores Parameter-Efficient Fine-Tuning (PEFT) methods applied to Retrieval-Augmented Generation (RAG) models, comparing GPT and RETRO architectures. RETRO generally outperforms GPT in zero-shot settings due to their pre-training process that integrates external retrieval, enhancing contextual understanding. However, GPT models show a higher performance potential with PEFT, indicating more room for improvement during fine-tuning."], "score": 0.9375}, {"id": "(Sanniboina et al., 2024)", "paper": {"corpus_id": 273345967, "title": "LoRE: Logit-Ranked Retriever Ensemble for Enhancing Open-Domain Question Answering", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Saikrishna Sanniboina", "authorId": "2325905180"}, {"name": "Shiv Trivedi", "authorId": "2325907509"}, {"name": "Sreenidhi Vijayaraghavan", "authorId": "2325903482"}], "n_citations": 1}, "snippets": ["The development of retrieval-augmented LLMs has advanced significantly, focusing on improving the synergy between retrieval and generation processes. Shao et al. [17] introduced ITER-RETGEN, which iteratively enhances retrieval and generation for tasks like multi-hop QA and commonsense reasoning. It alternates between retrieval-augmented generation and generation-augmented retrieval, refining each step with the previous output. But, it faces challenges such as suboptimal utilization of retrieved information, with about 20% of contexts lacking actual answers, leading to inaccuracies or hallucinations due to positional bias and non-optimized iterative retrieval."], "score": 0.88916015625}, {"id": "(Yao et al., 2024)", "paper": {"corpus_id": 274283400, "title": "Adaptive Control of Retrieval-Augmented Generation for Large Language Models Through Reflective Tags", "year": 2024, "venue": "Electronics", "authors": [{"name": "Chengyuan Yao", "authorId": "2332684729"}, {"name": "Satoshi Fujita", "authorId": "2275134724"}], "n_citations": 4}, "snippets": ["Future work includes comparisons with existing methods for improving the performance of RAGs and evaluation experiments using a wider range of datasets. We plan to compare our method with existing robust approaches, including RAG by Facebook AI [18], REALM by Google (Guu et al., 2020), and various domain-specific retrieval systems such as PubMed (White, 2020), BioBERT (Lee et al., 2019) PatentBERT [21], and FinancialBERT [22]. Additionally, we will evaluate against the latest state-of-the-art LLMs, such as Phi-3.5 [23], LLaMA 3.2 [24], and OLMo [25]."], "score": 0.890625}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(White, 2020)", "paper": {"corpus_id": 224820417, "title": "PubMed 2.0", "year": 2020, "venue": "Medical Reference Services Quarterly", "authors": [{"name": "Jacob White", "authorId": "2111356742"}], "n_citations": 127}, "snippets": ["Abstract After years of strategic planning, the National Library of Medicine has introduced an updated and redesigned version of its PubMed health sciences research website. The new website features a more modern and responsive interface, especially on mobile devices. Tools and features have been relocated to make them more intuitive for new users. While not without some turbulence and slight discomfort for long-time users adjusting to the modernized interface and search engine, the new version of the PubMed website introduced in 2020 succeeds in the website\u2019s time-honored task of collecting and making freely accessible high-quality health sciences information and resources."], "score": 0.0}, {"id": "(Lee et al., 2019)", "paper": {"corpus_id": 59291975, "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining", "year": 2019, "venue": "Bioinform.", "authors": [{"name": "Jinhyuk Lee", "authorId": "46664096"}, {"name": "WonJin Yoon", "authorId": "51433082"}, {"name": "Sungdong Kim", "authorId": "2829848"}, {"name": "Donghyeon Kim", "authorId": "2145183568"}, {"name": "Sunkyu Kim", "authorId": "2144247125"}, {"name": "Chan Ho So", "authorId": "51435068"}, {"name": "Jaewoo Kang", "authorId": "144323862"}], "n_citations": 5673}, "snippets": ["Abstract Motivation Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. Results We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. Availability and implementation We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."], "score": 0.0}], "table": null}, {"title": "Evaluation Methodologies", "tldr": "Evaluating the effectiveness of RAG versus fine-tuning approaches requires robust methodologies that assess multiple dimensions including faithfulness, relevance, and factual accuracy. Current evaluation frameworks range from automated LLM-based assessments to comprehensive benchmark comparisons, though a significant research gap exists in standardized evaluation protocols across diverse RAG implementations. (4 sources)", "text": "\nThe assessment of Retrieval-Augmented Generation (RAG) systems against fine-tuning approaches presents unique methodological challenges that researchers continue to address. Rigorous experimental comparisons have demonstrated that RAG systems can significantly enhance content quality by effectively incorporating information from external knowledge sources <Paper corpusId=\"267320876\" paperTitle=\"(Lyu et al., 2024)\" isShortName></Paper>. These evaluations typically focus on multiple dimensions of performance, including faithfulness to source material, answer relevance, and context relevance.\n\nCurrent evaluation methodologies employ various techniques to comprehensively assess RAG systems. These include prompting large language models to evaluate generated summaries, fine-tuning lightweight models on synthetic data, and using downstream applications like question answering to measure system effectiveness <Paper corpusId=\"269502216\" paperTitle=\"(Mayfield et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258587884\" paperTitle=\"(Yue et al., 2023)\" isShortName></Paper>. For instance, researchers have developed specialized approaches for evaluating attribution in generative systems, which verify whether generated statements are fully supported by cited references\u2014a critical aspect of RAG evaluation <Paper corpusId=\"269502216\" paperTitle=\"(Mayfield et al., 2024)\" isShortName></Paper> <Paper corpusId=\"258587884\" paperTitle=\"(Yue et al., 2023)\" isShortName></Paper>.\n\nDespite growing interest in RAG techniques within the LLM domain, the research community has identified a significant gap in evaluation methodologies. While systematic reviews and direct comparisons between successive state-of-the-art models exist, comprehensive experimental comparisons across a broad spectrum of advanced RAG techniques remain limited <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. This gap highlights the need for more standardized evaluation frameworks that can provide consistent metrics across diverse implementations of both RAG and fine-tuning approaches.\n\nRecent research efforts have begun addressing this gap by providing more extensive evaluations of multiple RAG techniques and their combinations, offering insights into their relative strengths and weaknesses across various tasks and real-world scenarios <Paper corpusId=\"268819923\" paperTitle=\"(Eibich et al., 2024)\" isShortName></Paper>. These comparative studies are essential for developing a more nuanced understanding of when and how to apply RAG versus fine-tuning approaches in different contexts.\n\nAs the field continues to evolve, establishing robust, standardized evaluation methodologies will be crucial for making meaningful comparisons between RAG and fine-tuning approaches. These methodologies will need to account for the multifaceted nature of performance in generative AI systems, including factual accuracy, relevance, coherence, and computational efficiency <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">.", "citations": [{"id": "(Lyu et al., 2024)", "paper": {"corpus_id": 267320876, "title": "CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models", "year": 2024, "venue": "ACM Trans. Inf. Syst.", "authors": [{"name": "Yuanjie Lyu", "authorId": "2187857206"}, {"name": "Zhiyu Li", "authorId": "2268429641"}, {"name": "Simin Niu", "authorId": "2268393907"}, {"name": "Feiyu Xiong", "authorId": "2268399953"}, {"name": "Bo Tang", "authorId": "2268400606"}, {"name": "Wenjin Wang", "authorId": "2117833477"}, {"name": "Hao Wu", "authorId": "2282083454"}, {"name": "Huan Liu", "authorId": "2304320758"}, {"name": "Tong Xu", "authorId": "2277237058"}, {"name": "Enhong Chen", "authorId": "2265580543"}], "n_citations": 40}, "snippets": ["Through rigorous experimental comparisons, we have demonstrated that RAG systems can significantly enhance the quality of generated content by effectively incorporating information from external knowledge sources."], "score": 0.94677734375}, {"id": "(Mayfield et al., 2024)", "paper": {"corpus_id": 269502216, "title": "On the Evaluation of Machine-Generated Reports", "year": 2024, "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval", "authors": [{"name": "James Mayfield", "authorId": "2298905578"}, {"name": "Eugene Yang", "authorId": "2296599846"}, {"name": "Dawn J Lawrie", "authorId": "2539674"}, {"name": "Sean MacAvaney", "authorId": "2290916915"}, {"name": "Paul McNamee", "authorId": "145324163"}, {"name": "Douglas W. Oard", "authorId": "1737250"}, {"name": "Luca Soldaini", "authorId": "3328733"}, {"name": "Ian Soboroff", "authorId": "2299328116"}, {"name": "Orion Weller", "authorId": "47433471"}, {"name": "Efsun Kayi", "authorId": "1396112798"}, {"name": "Kate Sanders", "authorId": "2187060946"}, {"name": "Marc Mason", "authorId": "2215823149"}, {"name": "Noah Hibbler", "authorId": "2299332019"}], "n_citations": 16}, "snippets": ["Most approaches to automated evaluation aim to estimate the effectiveness of RAG systems across desirable dimensions (e.g., faithfulness, answer relevance, and context relevance). Techniques include prompting LLMs to evaluate generated summaries [76], and fine-tuning lightweight models on synthetic data [73]. Downstream applications, such as question answering, can also be used to evaluate the effectiveness of RAG systems [74]."], "score": 0.892578125}, {"id": "(Yue et al., 2023)", "paper": {"corpus_id": 258587884, "title": "Automatic Evaluation of Attribution by Large Language Models", "year": 2023, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Xiang Yue", "authorId": "145548079"}, {"name": "Boshi Wang", "authorId": "7425689"}, {"name": "Kai Zhang", "authorId": "145086492"}, {"name": "Ziru Chen", "authorId": "11832104"}, {"name": "Yu Su", "authorId": "1758652"}, {"name": "Huan Sun", "authorId": "1515546612"}], "n_citations": 58}, "snippets": ["A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem."], "score": 0.0}, {"id": "(Eibich et al., 2024)", "paper": {"corpus_id": 268819923, "title": "ARAGOG: Advanced RAG Output Grading", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Matouvs Eibich", "authorId": "2294361167"}, {"name": "Shivay Nagpal", "authorId": "2294361283"}, {"name": "Alexander Fred-Ojala", "authorId": "2294362877"}], "n_citations": 4}, "snippets": ["Despite the growing interest in RAG techniques within the domain of LLMs, the existing body of literature primarily consists of systematic reviews (Gao et al., 2024) and direct comparisons between successive state-of-the-art (SoTA) models (Gao et al., 2022;Jiang et al., 2023).This pattern reveals a notable gap: a comprehensive experimental comparison across a broad spectrum of advanced RAG techniques is missing.Such a comparison is crucial for understanding the relative strengths and weaknesses of these techniques in enhancing LLMs' performance across various tasks.This study seeks to contribute to bridging this gap by providing an extensive evaluation of multiple RAG techniques and their combinations, thereby offering insights into their efficacy and applicability in real-world scenarios."], "score": 0.94677734375}], "table": null}], "cost": 0.323064}}
