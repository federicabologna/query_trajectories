{"clarifying_information": [{"clarifying_question1": "Are you interested in a taxonomy or detailed list of the most commonly used retrieval-augmented language model architectures (such as kNN-LM, RETRO, REALM, RAG, etc.) currently in NLP?", "clarifying_answer1": {"clarifying_answer": "Yes, I am interested in a taxonomy or detailed list of the most commonly used retrieval-augmented language model architectures currently in NLP, such as kNN-LM, RETRO, REALM, RAG, etc."}}, {"clarifying_question2": "Do you want an in-depth comparison of the structural differences between sparse, dense, and hybrid retrieval models used in language model retrieval frameworks?", "clarifying_answer2": {"clarifying_answer": "Yes, I would like an in-depth comparison of the structural differences between sparse, dense, and hybrid retrieval models as used in language model retrieval frameworks."}}, {"clarifying_question3": "Would you like to focus specifically on which retrieval language models are most widely adopted for particular applications (e.g., question answering vs. semantic search) and their performance trade-offs?", "clarifying_answer3": {"clarifying_answer": "Yes, I am interested in understanding which retrieval language models are most commonly used for specific applications such as question answering versus semantic search, along with their respective performance trade-offs."}}], "better_query": "Provide a taxonomy and detailed list of the most commonly used retrieval-augmented language model architectures in NLP (such as kNN-LM, RETRO, REALM, RAG), highlighting their main architectural designs and use cases.", "better_answer": {"sections": [{"title": "Introduction to Retrieval-Augmented Language Models", "tldr": "Retrieval-Augmented Language Models (RALMs) enhance traditional language models by incorporating external knowledge sources to overcome limitations in factual accuracy and knowledge scalability. They combine parametric memory (the model's parameters) with non-parametric memory (external knowledge bases) to provide more accurate, up-to-date, and contextually relevant responses. (12 sources)", "text": "\nRetrieval-Augmented Language Models (RALMs) represent a significant advancement in natural language processing, addressing fundamental limitations of traditional language models. These models were developed to overcome the constraints of static, parameter-based knowledge stored within large language models (LLMs), which can become outdated and lack domain-specific information <Paper corpusId=\"270521566\" paperTitle=\"(Goel et al., 2024)\" isShortName></Paper> <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>. The core innovation of RALMs lies in their ability to decouple knowledge storage, distributing it between the model's parameters and external knowledge sources <Paper corpusId=\"265308533\" paperTitle=\"(Munikoti et al., 2023)\" isShortName></Paper>.\n\nThe concept of Retrieval-Augmented Generation (RAG), a key implementation of RALMs, was formally introduced by Facebook researchers in 2020 <Paper corpusId=\"265594594\" paperTitle=\"(Abdelazim et al., 2023)\" isShortName></Paper> <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. This approach combines the benefits of both retrieval-based and generation-based methods to produce more contextually relevant and informative responses <Paper corpusId=\"267061013\" paperTitle=\"(Elgedawy et al., 2024)\" isShortName></Paper>. In contrast to traditional LLMs that rely solely on their internal parameters, RAG models can access and utilize external knowledge bases on-demand <Paper corpusId=\"277780258\" paperTitle=\"(Gumaan, 2025)\" isShortName></Paper>.\n\nA typical RALM architecture consists of three key components: retrievers, language models, and augmentation mechanisms <Paper corpusId=\"271571401\" paperTitle=\"(Gao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"269188036\" paperTitle=\"(Huang et al., 2024)\" isShortName></Paper>. The retriever processes user queries and searches through external knowledge sources for relevant information. This retrieved information is then integrated with the original query and passed to the language model, which generates responses grounded in both its parametric knowledge and the retrieved context <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>.\n\nRALMs can implement different types of retrieval mechanisms, primarily categorized as dense and sparse retrievers <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Sparse retrievers excel at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities. Both approaches allow the language model to ground its responses in broader contexts, enhancing accuracy and factual consistency.\n\nThe integration of external knowledge can be implemented through various approaches, from the sophisticated architecture modifications in early RAG models to simpler \"In-Context RALM\" methods that prepend retrieved documents to the input without modifying the underlying LLM architecture <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>. This flexibility has contributed to the rapid adoption and evolution of retrieval-augmented approaches across diverse applications <Paper corpusId=\"277043297\" paperTitle=\"(Cheng et al., 2025)\" isShortName></Paper>.\n\nBy augmenting LLMs with dynamic access to external knowledge, RALMs offer several advantages: they provide more specific and factual responses than parametric-only models <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, they can be updated without retraining the entire model, and they offer natural source attribution by referencing the retrieved documents <Paper corpusId=\"256459451\" paperTitle=\"(Ram et al., 2023)\" isShortName></Paper>.", "citations": [{"id": "(Goel et al., 2024)", "paper": {"corpus_id": 270521566, "title": "HIRO: Hierarchical Information Retrieval Optimization", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Krish Goel", "authorId": "2306783450"}, {"name": "Mahek Chandak", "authorId": "2306782235"}], "n_citations": 2}, "snippets": ["In response to these challenges, the field has witnessed the emergence of Retrieval-Augmented Generation (RAG) models, evolving into what are now known as Retrieval-Augmented Language Models (RALMs). These new paradigms enhance LLMs by integrating dynamic external knowledge through sophisticated retrieval mechanisms, effectively addressing the inherent limitations of static knowledge bases (Lewis et al., 2020). Inspired by the success of open-domain question answering systems, RALMs utilize indexed text databases to enhance model responses by presenting retrieved information alongside input queries [1](Ram et al., 2023)."], "score": 0.83447265625}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 275358357, "title": "Knowledge Retrieval Based on Generative AI", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Te-Lun Yang", "authorId": "2191368257"}, {"name": "Jyi-Shane Liu", "authorId": "2253878746"}, {"name": "Yuen-Hsien Tseng", "authorId": "40130996"}, {"name": "Jyh-Shing Roger Jang", "authorId": "2262396644"}], "n_citations": 2}, "snippets": ["Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response."], "score": 0.77734375}, {"id": "(Munikoti et al., 2023)", "paper": {"corpus_id": 265308533, "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Sai Munikoti", "authorId": "2258957941"}, {"name": "Anurag Acharya", "authorId": "145536102"}, {"name": "S. Wagle", "authorId": "2054838317"}, {"name": "Sameera Horawalavithana", "authorId": "24029613"}], "n_citations": 8}, "snippets": ["The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability."], "score": 0.82080078125}, {"id": "(Abdelazim et al., 2023)", "paper": {"corpus_id": 265594594, "title": "Semantic Embeddings for Arabic Retrieval Augmented Generation (ARAG)", "year": 2023, "venue": "International Journal of Advanced Computer Science and Applications", "authors": [{"name": "Hazem Abdelazim", "authorId": "2269250335"}, {"name": "Mohamed Tharwat", "authorId": "2148715555"}, {"name": "Ammar Mohamed", "authorId": "2269771285"}], "n_citations": 8}, "snippets": ["Retrieval Augmented Generation (RAG), introduced by Facebook Researchers in 2020 [1], is a pivotal AI framework facilitating information retrieval for Generative AI models, thereby enhancing their accuracy and capabilities. RAG empowers Large Language Models (LLMs) by granting them access to external knowledge sources, augmenting the content generation process. This dual functionality entails retrieval, wherein RAG meticulously selects pertinent information from provided sources and generation, whereby LLMs craft contextually relevant responses based on user input."], "score": 0.720703125}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Elgedawy et al., 2024)", "paper": {"corpus_id": 267061013, "title": "Dynamic Q&A of Clinical Documents with Large Language Models", "year": 2024, "venue": "", "authors": [{"name": "Ran Elgedawy", "authorId": "2280063225"}, {"name": "Ioana Danciu", "authorId": "2274464131"}, {"name": "Maria Mahbub", "authorId": "1387927897"}, {"name": "Sudarshan Srinivasan", "authorId": "2149506151"}], "n_citations": 6}, "snippets": ["Retrieval augmented generation (RAG) refers to a novel technique where a model first retrieves relevant information from a large corpus or dataset and then generates responses or outputs based on the retrieved information. This approach combines the benefits of both retrieval-based and generation-based methods, allowing for more contextually relevant and informative responses in conversational systems. Recent research works are exploring the trade-offs between this method and traditional fine-tuning approaches for language models, with some favoring RAG for certain contexts [7]23]."], "score": 0.728515625}, {"id": "(Gumaan, 2025)", "paper": {"corpus_id": 277780258, "title": "ExpertRAG: Efficient RAG with Mixture of Experts - Optimizing Context Retrieval for Adaptive LLM Responses", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Esmail Gumaan", "authorId": "2354181125"}], "n_citations": 0}, "snippets": ["Retrieval-Augmented Generation (RAG) was introduced to address this by equipping models with access to external nonparametric memory (e.g. a text corpus or database), allowing them to fetch relevant information on-the-fly [2]. RAG combines a parametric neural generator with a retrieval module, producing outputs that are more specific and factual than those of parametric-only models (Brown et al., 2020)."], "score": 0.83447265625}, {"id": "(Gao et al., 2024)", "paper": {"corpus_id": 271571401, "title": "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 20}, "snippets": ["Hu et al., [30] discuss Retrieval-Augmented Language Models (RALMs) form three key components, including retrievers, language models, augmentations, and how their interactions lead to different model structures and applications."], "score": 0.7841796875}, {"id": "(Huang et al., 2024)", "paper": {"corpus_id": 269188036, "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Yizheng Huang", "authorId": "2260272949"}, {"name": "Jimmy X. Huang", "authorId": "2259653248"}], "n_citations": 51}, "snippets": ["Meanwhile, Hu et al. [48] explored Retrieval-Augmented Language Models (RALMs), examining how interactions between retrievers, language models, and augmentations influence model architectures and applications."], "score": 0.77880859375}, {"id": "(Zayyad et al., 2024)", "paper": {"corpus_id": 274982275, "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Majd Zayyad", "authorId": "2336913948"}, {"name": "Yossi Adi", "authorId": "2727584"}], "n_citations": 0}, "snippets": ["RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."], "score": 0.90283203125}, {"id": "(Ram et al., 2023)", "paper": {"corpus_id": 256459451, "title": "In-Context Retrieval-Augmented Language Models", "year": 2023, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Ori Ram", "authorId": "73775461"}, {"name": "Yoav Levine", "authorId": "152754428"}, {"name": "Itay Dalmedigos", "authorId": "1491822146"}, {"name": "Dor Muhlgay", "authorId": "51918041"}, {"name": "A. Shashua", "authorId": "3140335"}, {"name": "Kevin Leyton-Brown", "authorId": "2066411743"}, {"name": "Y. Shoham", "authorId": "1701353"}], "n_citations": 605}, "snippets": ["Abstract Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inaccurate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1"], "score": 0.0}, {"id": "(Cheng et al., 2025)", "paper": {"corpus_id": 277043297, "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Mingyue Cheng", "authorId": "1491233507"}, {"name": "Yucong Luo", "authorId": "2208917508"}, {"name": "Ouyang Jie", "authorId": "2322501286"}, {"name": "Qi Liu", "authorId": "2332691115"}, {"name": "Huijie Liu", "authorId": "2312648865"}, {"name": "Li Li", "authorId": "2291070758"}, {"name": "Shuo Yu", "authorId": "2322429208"}, {"name": "Bohou Zhang", "authorId": "2351226328"}, {"name": "Jiawei Cao", "authorId": "2350426005"}, {"name": "Jie Ma", "authorId": "2350427710"}, {"name": "Daoyu Wang", "authorId": "2322524150"}, {"name": "Enhong Chen", "authorId": "2258714945"}], "n_citations": 6}, "snippets": ["We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities."], "score": 0.9296875}], "table": null}, {"title": "Core Architectural Categories of RAG Models", "tldr": "Retrieval-Augmented Language Models (RALMs) can be categorized based on their underlying architecture (encoder-decoder vs. decoder-only) and retrieval mechanisms (block-level vs. token-level). These architectural choices significantly impact computational efficiency, knowledge integration methods, and suitability for specific tasks. (13 sources)", "text": "\nRetrieval-augmented language models can be systematically categorized based on several key architectural dimensions that define how they retrieve and integrate external knowledge. These categories help organize the diverse landscape of RAG approaches and understand their relative strengths and limitations.\n\n## Model Architecture Base Types\n\nThe first major distinction is between encoder-decoder and decoder-only architectures:\n\n- **Encoder-Decoder Models**: Systems like REALM, RAG, and FiD use a sequence-to-sequence architecture that first encodes input texts and then generates outputs. These models often employ a Fusion-in-Decoder approach that enables efficient processing of multiple retrieved passages, as the computational cost grows linearly with the number of passages rather than quadratically <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>. Examples include the original RAG model <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> and Atlas <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper>.\n\n- **Decoder-Only Models**: Models like kNN-LM and RETRO use autoregressive decoder-only architectures, similar to GPT models. These approaches typically face higher computational costs as context length increases, since the computation scales quadratically with input length and retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n## Retrieval Granularity\n\nAnother critical distinction is based on what is retrieved and how it's integrated:\n\n- **Block Retrieval Models (Input-Layer Augmentation)**: These models retrieve entire text chunks or passages that are incorporated into the context before generation. They excel at knowledge-intensive tasks like question answering by providing broader context <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. Examples include REALM <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>, RAG <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>, and RETRO <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n- **Token Retrieval Models (Output-Layer Augmentation)**: These models retrieve at the token level and combine retrieved tokens with the model's native next-token predictions. They're particularly effective for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>. kNN-LM is the prototypical example, interpolating between the language model's predictions and a k-nearest neighbors distribution from retrieved tokens <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\n## Integration Methods\n\nRAG models also differ in how they integrate retrieved information:\n\n- **Architectural Modification**: Some models like RETRO modify the model architecture, adding components such as chunked cross-attention mechanisms to integrate retrieved information <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>. This typically requires training or fine-tuning the model.\n\n- **Non-parametric Augmentation**: Models like kNN-LM use non-parametric methods such as interpolation between the base model's predictions and the retrieval-based predictions <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>. This approach often doesn't require model retraining.\n\n- **In-Context Learning**: Some approaches like REPLUG treat the language model as a black box and simply prepend retrieved information to the input context <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"245218561\" paperTitle=\"(Rubin et al., 2021)\" isShortName></Paper>.\n\nRecent advancements in RAG architectures have focused on improving retrieval efficiency through adaptive frameworks and better structuring of external knowledge, with graph-based RAG systems emerging as a promising approach <Paper corpusId=\"277313225\" paperTitle=\"(Wang et al., 2025)\" isShortName></Paper>. The standard RAG workflow has crystallized into a two-phase process: retrieval of relevant documents followed by generation conditioned on both the query and retrieved information <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.", "citations": [{"id": "(Huang et al., 2023)", "paper": {"corpus_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jie Huang", "authorId": "1490651934"}, {"name": "Wei Ping", "authorId": "2056440915"}, {"name": "Peng Xu", "authorId": "145011005"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "K. Chang", "authorId": "143922493"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 35}, "snippets": ["Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."], "score": 0.767578125}, {"id": "(Izacard et al., 2020)", "paper": {"corpus_id": 220302360, "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 1181}, "snippets": ["Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."], "score": 0.0}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Izacard et al., 2022)", "paper": {"corpus_id": 251371732, "title": "Few-shot Learning with Retrieval Augmented Language Models", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "M. Lomeli", "authorId": "3376175"}, {"name": "Lucas Hosseini", "authorId": "26360550"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Jane A. Yu", "authorId": "2129456957"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 783}, "snippets": ["Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."], "score": 0.0}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 268856642, "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion", "year": 2024, "venue": "International Symposium on Software Testing and Analysis", "authors": [{"name": "Qi Guo", "authorId": "2290464625"}, {"name": "Xiaohong Li", "authorId": "2118890600"}, {"name": "Xiaofei Xie", "authorId": "2288741802"}, {"name": "Shangqing Liu", "authorId": "2290359321"}, {"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Ruitao Feng", "authorId": "1758019"}, {"name": "Junjie Wang", "authorId": "2294667814"}, {"name": "Jidong Ge", "authorId": "2248015856"}, {"name": "Lei Bu", "authorId": "2279752248"}], "n_citations": 11}, "snippets": ["Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."], "score": 0.81396484375}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 261030382, "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases", "year": 2023, "venue": "International Conference on Automated Software Engineering", "authors": [{"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Jidong Ge", "authorId": "2669512"}, {"name": "Shangqing Liu", "authorId": "13877308"}, {"name": "Tingwei Zhu", "authorId": "3274600"}, {"name": "Tongtong Xu", "authorId": "2118717147"}, {"name": "LiGuo Huang", "authorId": "1482584966"}, {"name": "Bin Luo", "authorId": "2075400450"}], "n_citations": 26}, "snippets": ["Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model."], "score": 0.7783203125}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Minjoon Seo", "authorId": "4418074"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 641}, "snippets": ["Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."], "score": 0.7978515625}, {"id": "(Rubin et al., 2021)", "paper": {"corpus_id": 245218561, "title": "Learning To Retrieve Prompts for In-Context Learning", "year": 2021, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Ohad Rubin", "authorId": "2001128224"}, {"name": "Jonathan Herzig", "authorId": "47426264"}, {"name": "Jonathan Berant", "authorId": "1750652"}], "n_citations": 709}, "snippets": ["In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board."], "score": 0.0}, {"id": "(Wang et al., 2025)", "paper": {"corpus_id": 277313225, "title": "CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Nengbo Wang", "authorId": "2352004975"}, {"name": "Xiaotian Han", "authorId": "2346134041"}, {"name": "Jagdip Singh", "authorId": "2352427030"}, {"name": "Jing Ma", "authorId": "2352917796"}, {"name": "Vipin Chaudhary", "authorId": "2346129602"}], "n_citations": 0}, "snippets": ["Recent efforts to improve RAG focus on two fronts: 1) enhancing retrieval efficiency through adaptive and modular frameworks (Gan et al., 2024;Ravuru et al., 2024;Zhang et al., 2024a); and 2) better structuring external knowledge, with graphbased RAGs emerging as a dominant approach (Edge et al., 2024;Guo et al., 2024;Potts, 2024)."], "score": 0.775390625}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024)."], "score": 0.76171875}], "table": null}, {"title": "Major Retrieval-Augmented Models and Their Designs", "tldr": "Key retrieval-augmented language models include kNN-LM, REALM, RAG, RETRO, and FiD, each employing distinct architectural approaches to integrate external knowledge. These models differ in their retrieval granularity, integration methods, and underlying architectures, with each design offering specific advantages for different knowledge-intensive NLP tasks. (14 sources)", "text": "\n## kNN-LM (Khandelwal et al., 2019)\n- **Core Mechanism**: Extends pre-trained language models by interpolating their predictions with a k-nearest neighbors distribution\n- **Architecture Type**: Decoder-only with token-level retrieval (output-layer augmentation)\n- **Retrieval Approach**: Retrieves tokens based on similarity in the embedding space of the pre-trained model\n- **Integration Method**: Non-parametric interpolation between the model's predictions and the kNN distribution\n- **Key Advantage**: Effective for rare patterns and factual knowledge without requiring model retraining\n- **Applications**: Language modeling, domain adaptation by simply varying the nearest neighbor datastore\n\n <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"268031947\" paperTitle=\"(Cao et al., 2024)\" isShortName></Paper>\n\n## REALM (Guu et al., 2020)\n- **Core Mechanism**: Jointly pre-trained knowledge retriever and knowledge-augmented encoder\n- **Architecture Type**: Encoder-only with block-level retrieval\n- **Retrieval Approach**: Neural retriever trained with masked language modeling signal\n- **Integration Method**: Architectural modification with differentiable access to documents\n- **Key Advantage**: Joint unsupervised training of retriever and model for cohesive performance\n- **Applications**: Open-domain question answering, outperforming previous methods by 4-16% absolute accuracy\n\n <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>\n\n## RAG (Lewis et al., 2020)\n- **Core Mechanism**: Combines parametric memory (seq2seq model) with non-parametric memory (dense vector index)\n- **Architecture Type**: Encoder-decoder with block-level retrieval (input-layer augmentation)\n- **Retrieval Approach**: Pre-trained neural retriever (e.g., DPR) accessing Wikipedia knowledge base\n- **Integration Method**: Two variants: one conditioning on the same passages across generation, another using different passages per token\n- **Key Advantage**: Differentiable end-to-end training of both retriever and generator\n- **Applications**: Knowledge-intensive tasks like open-domain QA, question generation, fact verification\n\n <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper> <Paper corpusId=\"259360590\" paperTitle=\"(Lyu et al., 2023)\" isShortName></Paper>\n\n## RETRO (Borgeaud et al., 2021)\n- **Core Mechanism**: Enhances autoregressive language models with document chunks retrieved from a massive corpus\n- **Architecture Type**: Decoder-only with block-level retrieval\n- **Retrieval Approach**: BERT-based retriever accessing a 2 trillion token database\n- **Integration Method**: Chunked cross-attention mechanism to integrate retrieved context\n- **Key Advantage**: Achieves performance comparable to much larger models (GPT-3, Jurassic-1) with 25x fewer parameters\n- **Applications**: General language modeling, downstream knowledge-intensive tasks like question answering\n\n <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"266163023\" paperTitle=\"(Khosla et al., 2023)\" isShortName></Paper>\n\n## FiD (Fusion-in-Decoder) (Izacard et al., 2020)\n- **Core Mechanism**: Encodes multiple retrieved passages independently and fuses them in the decoder\n- **Architecture Type**: Encoder-decoder with block-level retrieval\n- **Retrieval Approach**: Retrieves multiple text passages potentially containing evidence\n- **Integration Method**: Processes each passage separately in the encoder, then fuses all encoded passages in the decoder\n- **Key Advantage**: Computational cost scales linearly (not quadratically) with the number of passages\n- **Applications**: Open-domain question answering, achieving state-of-the-art results on benchmarks like Natural Questions and TriviaQA\n\n <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper> <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\n## Atlas (Izacard et al., 2022)\n- **Core Mechanism**: Carefully designed and pre-trained retrieval-augmented language model\n- **Architecture Type**: Encoder-decoder with Fusion-in-Decoder approach\n- **Retrieval Approach**: Jointly trained retriever that models documents as latent variables\n- **Integration Method**: Fusion of encoded passages in the decoder\n- **Key Advantage**: Few-shot learning capabilities for knowledge-intensive tasks\n- **Applications**: Knowledge-intensive tasks with minimal examples, reaching 42% accuracy on Natural Questions with only 64 examples\n\n <Paper corpusId=\"251371732\" paperTitle=\"(Izacard et al., 2022)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\n## REPLUG\n- **Core Mechanism**: Assumes black-box access to language models\n- **Architecture Type**: Model-agnostic approach (in-context learning)\n- **Retrieval Approach**: Optimized retriever through fine-tuning\n- **Integration Method**: Prepends retrieved information to input context\n- **Key Advantage**: Compatible with black-box LLMs without modifying model architecture\n- **Applications**: Adaptable to various language models without requiring access to their parameters\n\n <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper> <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Minjoon Seo", "authorId": "4418074"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 641}, "snippets": ["Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."], "score": 0.7978515625}, {"id": "(Cao et al., 2024)", "paper": {"corpus_id": 268031947, "title": "Retrieval is Accurate Generation", "year": 2024, "venue": "International Conference on Learning Representations", "authors": [{"name": "Bowen Cao", "authorId": "2209367631"}, {"name": "Deng Cai", "authorId": "2266753374"}, {"name": "Leyang Cui", "authorId": "2279792419"}, {"name": "Xuxin Cheng", "authorId": null}, {"name": "Wei Bi", "authorId": "2237804371"}, {"name": "Yuexian Zou", "authorId": "2260859476"}, {"name": "Shuming Shi", "authorId": "2257446263"}], "n_citations": 8}, "snippets": ["kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model.\n\nRETRO (Borgeaud et al., 2022) is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism.\n\nCoG (Lan et al., 2023) is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases."], "score": 0.763671875}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Siriwardhana et al., 2022)", "paper": {"corpus_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Rivindu Weerasekera", "authorId": "52001535"}, {"name": "Elliott Wen", "authorId": "2114425044"}, {"name": "Tharindu Kaluarachchi", "authorId": "1992921690"}, {"name": "R. Rana", "authorId": "1814487"}, {"name": "Suranga Nanayakkara", "authorId": "1486464114"}], "n_citations": 179}, "snippets": ["Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)", "(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever."], "score": 0.77880859375}, {"id": "(Chen et al., 2022)", "paper": {"corpus_id": 252735160, "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Wenhu Chen", "authorId": "2928777"}, {"name": "Hexiang Hu", "authorId": "2804000"}, {"name": "Xi Chen", "authorId": "2145309103"}, {"name": "Pat Verga", "authorId": "2986975"}, {"name": "William W. Cohen", "authorId": "50056360"}], "n_citations": 159}, "snippets": ["Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective."], "score": 0.76220703125}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Lyu et al., 2023)", "paper": {"corpus_id": 259360590, "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiaozhong Lyu", "authorId": "35308280"}, {"name": "Stefan Grafberger", "authorId": "1393463989"}, {"name": "Samantha Biegel", "authorId": "2086970867"}, {"name": "Shaopeng Wei", "authorId": "1409866591"}, {"name": "Meng Cao", "authorId": "2057073725"}, {"name": "Sebastian Schelter", "authorId": "2180399"}, {"name": "Ce Zhang", "authorId": "1776014"}], "n_citations": 18}, "snippets": ["Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,", ", d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever."], "score": 0.763671875}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Khosla et al., 2023)", "paper": {"corpus_id": 266163023, "title": "Survey on Memory-Augmented Neural Networks: Cognitive Insights to AI Applications", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Savya Khosla", "authorId": "2056070459"}, {"name": "Zhen Zhu", "authorId": "2273562657"}, {"name": "Yifie He", "authorId": "2273661082"}], "n_citations": 2}, "snippets": ["Retrieval Augmented Language Model (REALM) [27] is the first method to jointly train a knowledge retriever and a knowledge-augmented language encoder in an unsupervised manner. Retrieval augmented generation (RAG) [6] fine-tunes a pre-trained retriever (e.g., DPR [28]) and a pre-trained sequence-to-sequence model (e.g., BART [29]). RAG achieves superior performance on various knowledge-intensive tasks, including question answering, question generation and fact verification. Retrieval Augmented Translation (RAT) (Hoang et al., 2022) improves neural machine translation by treating the external knowledge base as a dictionary", "Retrieval Enhanced Transformers (RETRO) [5] augments a language model with an external knowledge base consisting of 2 trillion tokens, and achieves performance comparable to GPT-3 (Brown et al., 2020) and Jurassic-1 [33], which has 25x more parameters."], "score": 0.81494140625}, {"id": "(Izacard et al., 2020)", "paper": {"corpus_id": 220302360, "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 1181}, "snippets": ["Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jie Huang", "authorId": "1490651934"}, {"name": "Wei Ping", "authorId": "2056440915"}, {"name": "Peng Xu", "authorId": "145011005"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "K. Chang", "authorId": "143922493"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 35}, "snippets": ["Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."], "score": 0.767578125}, {"id": "(Izacard et al., 2022)", "paper": {"corpus_id": 251371732, "title": "Few-shot Learning with Retrieval Augmented Language Models", "year": 2022, "venue": "Journal of machine learning research", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "M. Lomeli", "authorId": "3376175"}, {"name": "Lucas Hosseini", "authorId": "26360550"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Timo Schick", "authorId": "32246932"}, {"name": "Jane A. Yu", "authorId": "2129456957"}, {"name": "Armand Joulin", "authorId": "2319608"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 783}, "snippets": ["Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters."], "score": 0.0}, {"id": "(Jiao et al., 2024)", "paper": {"corpus_id": 269983737, "title": "DuetRAG: Collaborative Retrieval-Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dian Jiao", "authorId": "2302798653"}, {"name": "Li Cai", "authorId": "2303434387"}, {"name": "Jingsheng Huang", "authorId": "2303044665"}, {"name": "Wenqiao Zhang", "authorId": "2108125912"}, {"name": "Siliang Tang", "authorId": "2118071462"}, {"name": "Yueting Zhuang", "authorId": "2253660817"}], "n_citations": 1}, "snippets": ["Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings."], "score": 0.77490234375}], "table": null}, {"title": "Retrieval Mechanisms in RAG Systems", "tldr": "Retrieval mechanisms in RAG systems can be categorized into dense and sparse approaches, each offering different strengths for knowledge retrieval. These retrieval systems work by either matching keywords directly (sparse retrieval) or capturing semantic relationships through neural embeddings (dense retrieval), with each approach suited to different types of knowledge-intensive tasks. (14 sources)", "text": "\nRetrieval is the cornerstone of Retrieval-Augmented Language Models, serving as the bridge between user queries and external knowledge sources. The effectiveness of a RAG model depends significantly on its ability to identify and retrieve relevant information from its knowledge base.\n\n## Types of Retrieval Mechanisms\n\nRetrieval mechanisms in RAG systems generally fall into two main categories:\n\n- **Sparse Retrievers**: These approaches rely on bag-of-words representations and focus on term frequency matching. They excel at finding documents with high lexical overlap with the query, making them particularly effective when exact terminology matches are important <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. A common implementation is keyword-to-document mapping, where specific keywords directly link to relevant documents or entries containing those terms <Paper corpusId=\"274131235\" paperTitle=\"(Shu et al., 2024)\" isShortName></Paper>.\n\n- **Dense Retrievers**: These systems utilize neural network embeddings to capture semantic similarities between queries and documents. Unlike sparse retrievers, they can identify conceptual relationships even when exact terminology differs <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>. Dense retrievers convert text into high-dimensional vectors that represent semantic meaning, enabling more nuanced matching beyond surface-level word overlap <Paper corpusId=\"274131235\" paperTitle=\"(Shu et al., 2024)\" isShortName></Paper>.\n\n## Retrieval Implementation Approaches\n\nThe implementation of retrieval mechanisms in RAG systems varies across different architectures:\n\n- **Pre-trained Neural Retrievers**: Many RAG models, including the original RAG architecture, utilize pre-trained neural retrievers such as Dense Passage Retriever (DPR) to access external knowledge bases like Wikipedia <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>. These retrievers are specifically designed to understand the semantic relationship between queries and documents.\n\n- **BERT-based Retrievers**: Models like RETRO employ BERT-based retrievers that access massive token databases (up to 2 trillion tokens in RETRO's case) <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\n- **Differentiable Retrieval**: Some approaches, like REALM, implement differentiable access to documents, allowing joint training of both the retriever and the language model through a masked language modeling signal <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\n- **k-Nearest Neighbors**: kNN-LM utilizes a k-nearest neighbors approach that retrieves tokens based on similarity in the embedding space of the pre-trained model <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>. This method extends pre-trained language models by interpolating their predictions with a k-nearest neighbors distribution <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n## Retrieval Workflow\n\nThe standard retrieval workflow in RAG systems typically follows a consistent pattern:\n\n1. **Query Processing**: The user query is encoded into a dense representation or processed for keyword matching <Paper corpusId=\"259360590\" paperTitle=\"(Lyu et al., 2023)\" isShortName></Paper>.\n\n2. **Corpus Search**: The retriever searches through an indexed knowledge base (such as Wikipedia articles) to find relevant documents or passages <Paper corpusId=\"252735056\" paperTitle=\"(Siriwardhana et al., 2022)\" isShortName></Paper>.\n\n3. **Ranking**: The retrieved candidates are ranked based on their relevance to the query, with the top-K most relevant documents selected for further processing <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\n4. **Integration**: The retrieved information is then fed into the generator along with the original query to produce contextually enhanced responses <Paper corpusId=\"278714952\" paperTitle=\"(Lee et al., 2025)\" isShortName></Paper>.\n\nAn important advancement in retrieval mechanisms is the ability to dynamically access up-to-date information. While traditional RAG systems use static retrieval components that leverage fixed corpora of data, some newer approaches incorporate internet search capabilities to access current information <Paper corpusId=\"236034557\" paperTitle=\"(Komeili et al., 2021)\" isShortName></Paper> <Paper corpusId=\"274423377\" paperTitle=\"(He et al., 2024)\" isShortName></Paper>.\n\nThe computational efficiency of retrieval operations varies based on the model architecture. For encoder-decoder models with a Fusion-in-Decoder architecture, the computational cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper> <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>. In contrast, for decoder-only LMs, the computational cost typically increases quadratically with both the input length and the number of retrieval passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\nThe choice of retrieval mechanism significantly impacts a RAG model's performance on different tasks, with input-layer augmentation (block retrieval) being more suitable for knowledge-intensive tasks like question answering, while output-layer augmentation (token retrieval) works better for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Zayyad et al., 2024)", "paper": {"corpus_id": 274982275, "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Majd Zayyad", "authorId": "2336913948"}, {"name": "Yossi Adi", "authorId": "2727584"}], "n_citations": 0}, "snippets": ["RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."], "score": 0.90283203125}, {"id": "(Shu et al., 2024)", "paper": {"corpus_id": 274131235, "title": "Transcending Language Boundaries: Harnessing LLMs for Low-Resource Language Translation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Peng Shu", "authorId": "2220096705"}, {"name": "Junhao Chen", "authorId": "2325897685"}, {"name": "Zheng Liu", "authorId": "2145977326"}, {"name": "Hui Wang", "authorId": "2273568534"}, {"name": "Zihao Wu", "authorId": "2263593041"}, {"name": "Tianyang Zhong", "authorId": "2215167446"}, {"name": "Yiwei Li", "authorId": "2257102397"}, {"name": "Huaqin Zhao", "authorId": "2276747984"}, {"name": "Hanqi Jiang", "authorId": "2273631049"}, {"name": "Yi Pan", "authorId": "2221032216"}, {"name": "Yifan Zhou", "authorId": "2325891087"}, {"name": "Constance Owl", "authorId": "2331328795"}, {"name": "Xiaoming Zhai", "authorId": "2249626607"}, {"name": "Ninghao Liu", "authorId": "2238404369"}, {"name": "Claudio Saunt", "authorId": "2331321061"}, {"name": "Tianming Liu", "authorId": "2254792886"}], "n_citations": 8}, "snippets": ["We implement a RAG LLM low resource language translator by combining retrieval-based techniques with LLMs to ensure accurate and context-aware translations. The overall architecture is shown in Figure 1. The system utilizes dictionary entries, which are indexed through two complementary approaches: keyword-to-document mappings and vector embeddings. Key-to-document mappings in systems refer to a process where keywords are linked directly to the documents or data entries that contain or are relevant to those keywords. The keyword retriever will retrieve corresponding documents according to the key-to-document mapping, if the keyword is inside our storage. The vector embedding indexing process organizes raw linguistic data into retrievable units by associating words with dictionary definitions and using text-embedding-ada-002 model to encode the text into high-dimensional vectors that capture semantic relationships beyond mere surface forms."], "score": 0.7470703125}, {"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Minjoon Seo", "authorId": "4418074"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 641}, "snippets": ["Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."], "score": 0.7978515625}, {"id": "(Siriwardhana et al., 2022)", "paper": {"corpus_id": 252735056, "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering", "year": 2022, "venue": "Transactions of the Association for Computational Linguistics", "authors": [{"name": "Shamane Siriwardhana", "authorId": "51516859"}, {"name": "Rivindu Weerasekera", "authorId": "52001535"}, {"name": "Elliott Wen", "authorId": "2114425044"}, {"name": "Tharindu Kaluarachchi", "authorId": "1992921690"}, {"name": "R. Rana", "authorId": "1814487"}, {"name": "Suranga Nanayakkara", "authorId": "1486464114"}], "n_citations": 179}, "snippets": ["Recently, Retrieval Augmented Architectures (Lewis et al., 2020b;Guu et al., 2020) have drawn a lot of attention due to their explainable, scalable, and adaptable nature. Unlike other open-domain QA architectures, RAG (Lewis et al., 2020b) combines the information retrieval stage and answer generation stage in a differentiable manner. It uses a combination of parametric and non-parametric memory, where the parametric memory consists of a pre-trained seq2seq BART (Lewis et al., 2019) generator, and the non-parametric memory consists of dense vector representations of Wikipedia articles indexed with the FAISS library (Johnson et al., 2017). RAG first encodes a question into a dense representation, retrieves the relevant passages from an indexed Wikipedia knowledge base, and then feeds them into the generator. The loss function can finetune both the generator and the question encoder at the same time. Lewis et al. (Lewis et al., 2020b) highlight RAG's ability to perform well in Wikipedia-based general question-answering datasets like Natural Questions (Kwiatkowski et al., 2019)", "(Guu et al., 2020) is a similar Retrieval Augmented model to RAG. REALM introduced a novel masked language pre-training step that involves an end-to-end trainable retriever."], "score": 0.77880859375}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 268856642, "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion", "year": 2024, "venue": "International Symposium on Software Testing and Analysis", "authors": [{"name": "Qi Guo", "authorId": "2290464625"}, {"name": "Xiaohong Li", "authorId": "2118890600"}, {"name": "Xiaofei Xie", "authorId": "2288741802"}, {"name": "Shangqing Liu", "authorId": "2290359321"}, {"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Ruitao Feng", "authorId": "1758019"}, {"name": "Junjie Wang", "authorId": "2294667814"}, {"name": "Jidong Ge", "authorId": "2248015856"}, {"name": "Lei Bu", "authorId": "2279752248"}], "n_citations": 11}, "snippets": ["Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."], "score": 0.81396484375}, {"id": "(Lyu et al., 2023)", "paper": {"corpus_id": 259360590, "title": "Improving Retrieval-Augmented Large Language Models via Data Importance Learning", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Xiaozhong Lyu", "authorId": "35308280"}, {"name": "Stefan Grafberger", "authorId": "1393463989"}, {"name": "Samantha Biegel", "authorId": "2086970867"}, {"name": "Shaopeng Wei", "authorId": "1409866591"}, {"name": "Meng Cao", "authorId": "2057073725"}, {"name": "Sebastian Schelter", "authorId": "2180399"}, {"name": "Ce Zhang", "authorId": "1776014"}], "n_citations": 18}, "snippets": ["Retrieval-augmented (RAG) models have recently been proposed [12,14]8]. A typical retrieval-augmented model consists of two parts, a retriever f ret and a generator f gen. Given a retrieval corpus D ret = {d 1, \u2022 \u2022 \u2022, d M}, the retriever f ret retrieves K data points for an input x i as f ret (x i, D ret) = {d \u03b11, d \u03b12,", ", d \u03b1 K}. Here, \u03b1 k denotes the rank of each data point in the retrieval corpus assigned by the retriever."], "score": 0.763671875}, {"id": "(Lee et al., 2025)", "paper": {"corpus_id": 278714952, "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "year": 2025, "venue": "", "authors": [{"name": "Zhan Peng Lee", "authorId": "2362089035"}, {"name": "Andre Lin", "authorId": "2362188632"}, {"name": "Calvin Tan", "authorId": "2363425126"}], "n_citations": 0}, "snippets": ["Retrieval-Augmented Generation (RAG) augments large language models by incorporating external documents into the generation process. Rather than relying solely on the model's internal parameters, RAG retrieves relevant passages from a knowledge base and feeds them, along with the user query, into the model to guide its response (Zhou et al., 2024). \n\nA standard RAG system operates in two phases: \n\n\u2022 Retrieval. A retriever model selects the top-k most relevant documents for a given query. \n\n\u2022 Generation. A language model generates a response conditioned on both the query and the retrieved documents. \n\nThe appeal of RAG lies in its ability to dynamically access up-to-date or domain-specific information, which is especially useful in fast-changing or specialized fields. However, it also introduces new failure modes, particularly when the retrieval quality is imperfect (Barnett et al., 2024)."], "score": 0.76171875}, {"id": "(Komeili et al., 2021)", "paper": {"corpus_id": 236034557, "title": "Internet-Augmented Dialogue Generation", "year": 2021, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "M. Komeili", "authorId": "100653935"}, {"name": "Kurt Shuster", "authorId": "35752280"}, {"name": "J. Weston", "authorId": "145183709"}], "n_citations": 289}, "snippets": ["The largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledgedriven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b)."], "score": 0.0}, {"id": "(He et al., 2024)", "paper": {"corpus_id": 274423377, "title": "Zero-Indexing Internet Search Augmented Generation for Large Language Models", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Guangxin He", "authorId": "2333982900"}, {"name": "Zonghong Dai", "authorId": "83732784"}, {"name": "Jiangcheng Zhu", "authorId": "2290357307"}, {"name": "Binqiang Zhao", "authorId": "2333537636"}, {"name": "Chenyue Li", "authorId": "2313032000"}, {"name": "You Peng", "authorId": "2333318691"}, {"name": "Chen Wang", "authorId": "2333373998"}, {"name": "Binhang Yuan", "authorId": "2312922261"}], "n_citations": 1}, "snippets": ["While RAG systems with a static retrieval component, which leverages a fixed corpus of data, are effective for tasks within well-defined knowledge domains, Internet search augmented generation [4,(Komeili et al., 2021)6,7,(Duffy et al., 2023)[9] offers distinct advantages."], "score": 0.7958984375}, {"id": "(Izacard et al., 2020)", "paper": {"corpus_id": 220302360, "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 1181}, "snippets": ["Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jie Huang", "authorId": "1490651934"}, {"name": "Wei Ping", "authorId": "2056440915"}, {"name": "Peng Xu", "authorId": "145011005"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "K. Chang", "authorId": "143922493"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 35}, "snippets": ["Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."], "score": 0.767578125}], "table": null}, {"title": "Integration of Retrieved Information", "tldr": "Retrieval-augmented language models employ various methods to integrate retrieved information with model parameters, ranging from architectural modifications to non-parametric interpolation. The integration approach varies based on retrieval granularity (block vs. token level) and model architecture (encoder-decoder vs. decoder-only), with each method offering distinct computational efficiency and performance characteristics. (11 sources)", "text": "\nThe effectiveness of retrieval-augmented language models depends not only on what information is retrieved but also on how this information is integrated with the language model's generation process. Different integration approaches have emerged, each with distinct characteristics and applications.\n\n## Block-Level Integration Methods\n\nFor models that retrieve entire text passages (block-level retrieval), several integration approaches have been developed:\n\n- **Differentiable End-to-End Integration**: Models like RAG combine parametric memory (a pre-trained seq2seq model) with non-parametric memory (a dense vector index) in a differentiable manner. RAG offers two integration variants: one that conditions on the same retrieved passages across the entire generated sequence, and another that can use different passages per token <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>.\n\n- **Chunked Cross-Attention**: RETRO introduces a specialized chunked cross-attention mechanism that allows the model to focus on relevant retrieved document chunks. This architectural modification enables the model to predict tokens based on significantly more data than what is typically consumed during training <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>.\n\n- **Fusion-in-Decoder Architecture**: This approach, used in encoder-decoder models, processes each retrieved passage separately in the encoder and then fuses all encoded passages in the decoder. A key advantage is computational efficiency\u2014the cost scales linearly with the number of passages rather than quadratically <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>.\n\n- **In-Context Integration**: Some models like REPLUG assume black-box access to language models and simply prepend retrieved information to the input context. This approach requires no architectural modifications to the underlying model, making it highly versatile for deployment with various language models <Paper corpusId=\"269983737\" paperTitle=\"(Jiao et al., 2024)\" isShortName></Paper> <Paper corpusId=\"256389797\" paperTitle=\"(Shi et al., 2023)\" isShortName></Paper>.\n\n## Token-Level Integration Methods\n\nFor models that retrieve at the token level, different integration techniques are used:\n\n- **Distribution Interpolation**: kNN-LM exemplifies this approach by extending pre-trained language models through interpolating their predictions with a k-nearest neighbors distribution. This non-parametric method combines the model's next token predictions with a distribution derived from retrieved tokens <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>.\n\n- **Adaptive Interpolation**: More advanced approaches train additional networks to determine the optimal interpolation weights between the language model's predictions and retrieved token distributions <Paper corpusId=\"261030382\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>.\n\n## Integration Based on Model Architecture\n\nThe choice of integration method is often influenced by the underlying model architecture:\n\n- **Encoder-Decoder Models**: These architectures typically employ block-level retrieval with methods like Fusion-in-Decoder, which offers better computational efficiency for processing multiple retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n- **Decoder-Only Models**: These models often face higher computational costs when integrating retrieved information because the computation typically scales quadratically with both input length and the number of retrieved passages <Paper corpusId=\"260900354\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>.\n\n## Task-Specific Integration Considerations\n\nThe choice of integration approach also depends on the target application:\n\n- **Knowledge-Intensive Tasks**: Input-layer augmentation (block retrieval) is generally more suitable for tasks like question answering, where broader context from retrieved passages provides valuable background information <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>.\n\n- **Structured Generation Tasks**: Output-layer augmentation (token retrieval) tends to work better for strictly structured generation tasks such as code completion, where retrieved tokens can help correct the generated output <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper> <Paper corpusId=\"247450969\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>.\n\nThe integration of retrieved information in RAG systems represents a crucial design choice that impacts both performance and efficiency. By appending retrieved documents to the model's context, these systems allow language models to ground their responses in broader contexts, thereby increasing accuracy and factual consistency across complex tasks <Paper corpusId=\"274982275\" paperTitle=\"(Zayyad et al., 2024)\" isShortName></Paper>.", "citations": [{"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 260900354, "title": "RAVEN: In-Context Learning with Retrieval Augmented Encoder-Decoder Language Models", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jie Huang", "authorId": "1490651934"}, {"name": "Wei Ping", "authorId": "2056440915"}, {"name": "Peng Xu", "authorId": "145011005"}, {"name": "M. Shoeybi", "authorId": "1911755"}, {"name": "K. Chang", "authorId": "143922493"}, {"name": "Bryan Catanzaro", "authorId": "2301680"}], "n_citations": 35}, "snippets": ["Retrieval-augmented language models are a class of language models designed to enhance their performance by incorporating external knowledge. These models typically employ an information retrieval mechanism to access relevant information from a large corpus, which is then integrated into the model's prediction process. Retrieval-augmented LMs can be based on both encoder-decoder (Izacard et al., 2022)(Lewis et al., 2020) and decoderonly (Khandelwal et al., 2019)Borgeaud et al., 2022;(Shi et al., 2022) architectures. For decoder-only LMs, the computational cost typically increases quadratically with the input length, as well as with the number of retrieval passages. In contrast, for encoder-decoder LMs with a Fusion-in-Decoder architecture, the computation cost grows linearly with the number of retrieved passages, as they only perform self-attention over one passage at a time (Izacard et al., 2020)."], "score": 0.767578125}, {"id": "(Izacard et al., 2020)", "paper": {"corpus_id": 220302360, "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 1181}, "snippets": ["Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."], "score": 0.0}, {"id": "(Jiao et al., 2024)", "paper": {"corpus_id": 269983737, "title": "DuetRAG: Collaborative Retrieval-Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Dian Jiao", "authorId": "2302798653"}, {"name": "Li Cai", "authorId": "2303434387"}, {"name": "Jingsheng Huang", "authorId": "2303044665"}, {"name": "Wenqiao Zhang", "authorId": "2108125912"}, {"name": "Siliang Tang", "authorId": "2118071462"}, {"name": "Yueting Zhuang", "authorId": "2253660817"}], "n_citations": 1}, "snippets": ["Retrieval-Augmented Language Models Augmenting language models with relevant information obtained from various external knowledge bases has been shown to significantly improve the performance of various NLP tasks, including language modeling (Guu et al., 2020)(Borgeaud et al., 2021)Shi et al., 2023;Lin et al., 2023) and open domain question answering (Izacard et al., 2022;Zhang et al., 2024).RAG mainly adopts the \"retrieve then read\" paradigm.Specifically, the input question is first used as the query, then the retrieval module retrieves relevant documents from the external knowledge base, and finally the retrieved documents and questions are merged into a complete input to generate final output.For example, RETRO (Borgeaud et al., 2021) modifies the autoregressive LM to focus on relevant documents through chunked cross-attention, thereby introducing new parameters to the model.REPLUG (Shi et al., 2023) assumes black-box access to LM and optimizes it by fine-tuning the retriever.RAFT (Zhang et al., 2024) proposes a fine-tuned data that additionally contains relevant documents and answers with reasoning chains to train language models for domain-specific open-book settings."], "score": 0.77490234375}, {"id": "(Shi et al., 2023)", "paper": {"corpus_id": 256389797, "title": "REPLUG: Retrieval-Augmented Black-Box Language Models", "year": 2023, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "Weijia Shi", "authorId": "3040379"}, {"name": "Sewon Min", "authorId": "48872685"}, {"name": "Michihiro Yasunaga", "authorId": "19168196"}, {"name": "Minjoon Seo", "authorId": "4418074"}, {"name": "Rich James", "authorId": "2191899140"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "Wen-tau Yih", "authorId": "2072801764"}], "n_citations": 641}, "snippets": ["Retrieval-augmented Models Augmenting language models with relevant information retrieved from various knowledge stores has shown to be effective in improving performance on various NLP tasks, including language modeling (Min et al., 2022;(Borgeaud et al., 2021)(Khandelwal et al., 2019) and open-domain question answering (Lewis et al., 2020;Izacard et al., 2022b;Hu et al., 2022). Specifically, using the input as query, (1) a retriever first retrieves a set of documents (i.e., sequences of tokens) from a corpus and then (2) a language model incorporates the retrieved documents as additional information to make a final prediction. This style of retrieval can be added to both encoderdecoder (Yu, 2022)Izacard et al., 2022b) and decoder-only models (Khandelwal et al., 2019)(Borgeaud et al., 2021)Shi et al., 2022;(Rubin et al., 2021). For example, Atlas (Izacard et al., 2022b) finetunes an encoder-decoder model jointly with the retriever by modeling documents as latent variables, while RETRO (Borgeaud et al., 2021) changes the decoderonly architecture to incorporate retrieved texts and pretrains the language model from scratch. Both methods require updating the model parameters through gradient descent, which cannot be applied to black-box LMs. Another line of retrieval-augmented LMs such as kNN-LM (Khandelwal et al., 2019)Zhong et al., 2022) retrieves a set of tokens and interpolates between the LM's next token distribution and kNN distributions computed from the retrieved tokens at inference."], "score": 0.7978515625}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 261030382, "title": "Domain Adaptive Code Completion via Language Models and Decoupled Domain Databases", "year": 2023, "venue": "International Conference on Automated Software Engineering", "authors": [{"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Jidong Ge", "authorId": "2669512"}, {"name": "Shangqing Liu", "authorId": "13877308"}, {"name": "Tingwei Zhu", "authorId": "3274600"}, {"name": "Tongtong Xu", "authorId": "2118717147"}, {"name": "LiGuo Huang", "authorId": "1482584966"}, {"name": "Bin Luo", "authorId": "2075400450"}], "n_citations": 26}, "snippets": ["Retrieval-augmented language models (R-LMs) utilize retrieval-based techniques to improve the performance of LMs and can be divided into two main categories: block R-LMs and token R-LMs. Block R-LMs [12], [13], [48] are similar to one-shot or few-shot learning [49], where one or a few examples are retrieved from a database instead of being randomly selected. Token R-LMs [15], [24], [50] retrieve tokens from database and then combine the retrieved results into the LM. Compared with block R-LMs, token R-LMs can update retrieval results at the same time of generating new tokens, hence our approach uses the architecture of token R-LM. However, token R-LMs suffer from high storage costs and require hyper-parameters selection to combine the inference results from the database and language model.\n\nVarious approaches have been proposed to address the limitations of token R-LMs. For example, kNN-Adapter [51] uses a trained network to determine the combination weights. To reduce the search cost, RetoMaton [52] uses the automaton states to save search time, while AdaptRet [53] uses a trained network to decide whether to use the retrieval module. GNN-LM [50] selects similar texts and builds a contextual graph to incorporate into the language model."], "score": 0.7783203125}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 268856642, "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion", "year": 2024, "venue": "International Symposium on Software Testing and Analysis", "authors": [{"name": "Qi Guo", "authorId": "2290464625"}, {"name": "Xiaohong Li", "authorId": "2118890600"}, {"name": "Xiaofei Xie", "authorId": "2288741802"}, {"name": "Shangqing Liu", "authorId": "2290359321"}, {"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Ruitao Feng", "authorId": "1758019"}, {"name": "Junjie Wang", "authorId": "2294667814"}, {"name": "Jidong Ge", "authorId": "2248015856"}, {"name": "Lei Bu", "authorId": "2279752248"}], "n_citations": 11}, "snippets": ["Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."], "score": 0.81396484375}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 247450969, "title": "ReACC: A Retrieval-Augmented Code Completion Framework", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shuai Lu", "authorId": "2115338656"}, {"name": "Nan Duan", "authorId": "46429989"}, {"name": "Hojae Han", "authorId": "5534572"}, {"name": "Daya Guo", "authorId": "2278834796"}, {"name": "Seung-won Hwang", "authorId": "1716415"}, {"name": "Alexey Svyatkovskiy", "authorId": "2061625488"}], "n_citations": 147}, "snippets": ["Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \"external\" context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."], "score": 0.0}, {"id": "(Zayyad et al., 2024)", "paper": {"corpus_id": 274982275, "title": "Formal Language Knowledge Corpus for Retrieval Augmented Generation", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Majd Zayyad", "authorId": "2336913948"}, {"name": "Yossi Adi", "authorId": "2727584"}], "n_citations": 0}, "snippets": ["RAG is a sophisticated framework designed to enhance language models by coupling them with external retrieval systems, addressing limitations inherent in static, solely parameter-based language models. RAG integrates a dual-component architecture where a retriever dynamically searches a structured external corpus for relevant information based on the input query, and a generator LLM uses the retrieved content as context to generate accurate and contextually enriched responses [Gao et al., 2023, Mialon et al., 2023].\n\nRAGs can employ two main types of retrieval mechanisms: dense and sparse [Mialon et al., 2023]. Sparse retrievers rely on bag-of-words representations, excelling at finding documents with high term overlap to the query, while dense retrievers utilize neural network embeddings to capture semantic similarities, enhancing the model's comprehension of related concepts. By appending retrieved documents directly to the model's context, these retrievers allow the language model to ground its responses in a broader context, thereby increasing accuracy and factual consistency across complex tasks."], "score": 0.90283203125}], "table": null}, {"title": "Use Cases and Applications", "tldr": "Retrieval-augmented language models excel in knowledge-intensive tasks like open-domain question answering, fact verification, and domain-specific applications. Different architectural approaches are suited to particular use cases, with block-level retrieval models performing better on knowledge-intensive tasks while token-level models excel at structured generation tasks like code completion. (12 sources)", "text": "\n## Open-Domain Question Answering\n- **Primary Application**: Retrieving factual knowledge to answer questions without domain constraints\n- **Notable Models**: REALM, RAG, FiD, and Atlas have all demonstrated significant improvements in this domain\n- **Key Advantages**: RAG set state-of-the-art results on three open-domain QA tasks, outperforming both parametric-only models and task-specific architectures <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>\n- **Performance Metrics**: REALM outperformed previous methods by 4-16% absolute accuracy on open-domain QA benchmarks <Paper corpusId=\"211204736\" paperTitle=\"(Guu et al., 2020)\" isShortName></Paper>\n- **Scaling Benefits**: FiD showed that performance significantly improves when increasing the number of retrieved passages, demonstrating the effective aggregation of evidence <Paper corpusId=\"220302360\" paperTitle=\"(Izacard et al., 2020)\" isShortName></Paper>\n\n## Language Modeling\n- **Application**: Improving general text prediction and generation capabilities\n- **Key Models**: kNN-LM and RETRO have shown substantial improvements in language modeling perplexity\n- **Performance**: kNN-LM achieved a 2.9 point improvement in perplexity on Wikitext-103 without additional training <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>\n- **Scaling Advantages**: RETRO, with its 2 trillion token database, achieved performance comparable to much larger models like GPT-3 and Jurassic-1 despite using 25x fewer parameters <Paper corpusId=\"244954723\" paperTitle=\"(Borgeaud et al., 2021)\" isShortName></Paper>\n\n## Domain Adaptation\n- **Application**: Adapting language models to specific domains without retraining\n- **Approach**: kNN-LM demonstrated effective domain adaptation simply by varying the nearest neighbor datastore <Paper corpusId=\"207870430\" paperTitle=\"(Khandelwal et al., 2019)\" isShortName></Paper>\n- **Benefits**: Allows models to adapt to new domains with minimal computational resources\n- **Implementation**: Atlas showed few-shot learning capabilities for knowledge-intensive tasks, reaching 42% accuracy on Natural Questions with only 64 examples <Paper corpusId=\"252735160\" paperTitle=\"(Chen et al., 2022)\" isShortName></Paper>\n\n## Code Completion\n- **Application**: Generating programming code with higher accuracy\n- **Best Architecture**: Token-level retrieval models (output-layer augmentation) perform better for structured generation tasks like code completion <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n- **Implementation Example**: Retrieval-augmented code completion frameworks leverage both lexical copying and semantic similarity to improve code generation <Paper corpusId=\"247450969\" paperTitle=\"(Lu et al., 2022)\" isShortName></Paper>\n- **Advantage**: Allows models to reference similar code patterns when generating new code, similar to how human programmers work\n\n## Fact Verification and Knowledge-Intensive Tasks\n- **Application**: Verifying factual claims and answering knowledge-intensive questions\n- **Architecture Preference**: Block-level retrieval models (input-layer augmentation) excel at these tasks by providing broader context <Paper corpusId=\"268856642\" paperTitle=\"(Guo et al., 2024)\" isShortName></Paper>\n- **Key Models**: RAG models have been applied to fact verification tasks with strong results <Paper corpusId=\"218869575\" paperTitle=\"(Lewis et al., 2020)\" isShortName></Paper>\n- **Advantage**: Provides natural source attribution by referencing the retrieved documents\n\n## Real-Time Knowledge Access\n- **Application**: Accessing up-to-date information beyond the model's training cutoff\n- **Implementation**: Advanced RAG systems can incorporate internet search capabilities to access current information <Paper corpusId=\"265308533\" paperTitle=\"(Munikoti et al., 2023)\" isShortName></Paper>\n- **Use Case**: Particularly valuable for queries about recent events or rapidly changing information\n- **Advantage**: Overcomes the limitation of static knowledge in traditional language models <Paper corpusId=\"275358357\" paperTitle=\"(Yang et al., 2025)\" isShortName></Paper>\n\n## Specialized Domain Knowledge\n- **Application**: Accessing specialized information from heterogeneous knowledge sources\n- **Advanced Approaches**: Recent work has expanded beyond retrieving unstructured text from Wikipedia to incorporate heterogeneous knowledge sources <Paper corpusId=\"250391000\" paperTitle=\"(Yu, 2022)\" isShortName></Paper>\n- **Implementation**: Graph-based RAG systems provide a promising approach for structured knowledge retrieval\n- **Use Case**: Medical, legal, financial, and other domain-specific applications requiring specialized knowledge\n\n## Conversational Systems\n- **Application**: Enhancing dialogue systems with factual grounding\n- **Implementation**: Retrieval-augmented models provide more specific and factual responses in conversational contexts\n- **Key Advantage**: Reduces hallucinations and improves factual consistency in dialogue responses <Paper corpusId=\"271570928\" paperTitle=\"(Chen et al., 2024)\" isShortName></Paper>\n- **Models**: Various RAG approaches have been adapted for conversational contexts to enhance response quality", "citations": [{"id": "(Lewis et al., 2020)", "paper": {"corpus_id": 218869575, "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks", "year": 2020, "venue": "Neural Information Processing Systems", "authors": [{"name": "Patrick Lewis", "authorId": "145222654"}, {"name": "Ethan Perez", "authorId": "3439053"}, {"name": "Aleksandara Piktus", "authorId": "1716179427"}, {"name": "F. Petroni", "authorId": "40052301"}, {"name": "Vladimir Karpukhin", "authorId": "2067091563"}, {"name": "Naman Goyal", "authorId": "39589154"}, {"name": "Heinrich Kuttler", "authorId": "103131985"}, {"name": "M. Lewis", "authorId": "35084211"}, {"name": "Wen-tau Yih", "authorId": "144105277"}, {"name": "Tim Rockt\u00e4schel", "authorId": "2620211"}, {"name": "Sebastian Riedel", "authorId": "48662861"}, {"name": "Douwe Kiela", "authorId": "1743722"}], "n_citations": 6476}, "snippets": ["We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token."], "score": 0.82568359375}, {"id": "(Guu et al., 2020)", "paper": {"corpus_id": 211204736, "title": "REALM: Retrieval-Augmented Language Model Pre-Training", "year": 2020, "venue": "International Conference on Machine Learning", "authors": [{"name": "Kelvin Guu", "authorId": "2091768"}, {"name": "Kenton Lee", "authorId": "2544107"}, {"name": "Zora Tung", "authorId": "9941702"}, {"name": "Panupong Pasupat", "authorId": "2616463"}, {"name": "Ming-Wei Chang", "authorId": "1744179"}], "n_citations": 2119}, "snippets": ["Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. \nTo capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. \nWe demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity."], "score": 0.0}, {"id": "(Izacard et al., 2020)", "paper": {"corpus_id": 220302360, "title": "Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering", "year": 2020, "venue": "Conference of the European Chapter of the Association for Computational Linguistics", "authors": [{"name": "Gautier Izacard", "authorId": "1410231361"}, {"name": "Edouard Grave", "authorId": "3024698"}], "n_citations": 1181}, "snippets": ["Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that sequence-to-sequence models offers a flexible framework to efficiently aggregate and combine evidence from multiple passages."], "score": 0.0}, {"id": "(Khandelwal et al., 2019)", "paper": {"corpus_id": 207870430, "title": "Generalization through Memorization: Nearest Neighbor Language Models", "year": 2019, "venue": "International Conference on Learning Representations", "authors": [{"name": "Urvashi Khandelwal", "authorId": "3030219"}, {"name": "Omer Levy", "authorId": "39455775"}, {"name": "Dan Jurafsky", "authorId": "1746807"}, {"name": "Luke Zettlemoyer", "authorId": "1982950"}, {"name": "M. Lewis", "authorId": "35084211"}], "n_citations": 842}, "snippets": ["We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."], "score": 0.0}, {"id": "(Borgeaud et al., 2021)", "paper": {"corpus_id": 244954723, "title": "Improving language models by retrieving from trillions of tokens", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Sebastian Borgeaud", "authorId": "148016269"}, {"name": "A. Mensch", "authorId": "1697879"}, {"name": "Jordan Hoffmann", "authorId": "46616544"}, {"name": "Trevor Cai", "authorId": "2072572294"}, {"name": "Eliza Rutherford", "authorId": "2143538252"}, {"name": "Katie Millican", "authorId": "2143434227"}, {"name": "George van den Driessche", "authorId": "47568983"}, {"name": "Jean-Baptiste Lespiau", "authorId": "143783339"}, {"name": "Bogdan Damoc", "authorId": "2143374656"}, {"name": "Aidan Clark", "authorId": "31993415"}, {"name": "Diego de Las Casas", "authorId": "40550616"}, {"name": "Aurelia Guy", "authorId": "40895205"}, {"name": "Jacob Menick", "authorId": "10698483"}, {"name": "Roman Ring", "authorId": "81387328"}, {"name": "T. Hennigan", "authorId": "4629007"}, {"name": "Saffron Huang", "authorId": "2148653469"}, {"name": "Lorenzo Maggiore", "authorId": "108173905"}, {"name": "Chris Jones", "authorId": "2115601070"}, {"name": "Albin Cassirer", "authorId": "51042571"}, {"name": "Andy Brock", "authorId": "2065040422"}, {"name": "Michela Paganini", "authorId": "35550664"}, {"name": "G. Irving", "authorId": "2060655766"}, {"name": "O. Vinyals", "authorId": "1689108"}, {"name": "Simon Osindero", "authorId": "2217144"}, {"name": "K. Simonyan", "authorId": "34838386"}, {"name": "Jack W. Rae", "authorId": "34269227"}, {"name": "Erich Elsen", "authorId": "152585800"}, {"name": "L. Sifre", "authorId": "2175946"}], "n_citations": 1100}, "snippets": ["We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a $2$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25$\\times$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale."], "score": 0.0}, {"id": "(Chen et al., 2022)", "paper": {"corpus_id": 252735160, "title": "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text", "year": 2022, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Wenhu Chen", "authorId": "2928777"}, {"name": "Hexiang Hu", "authorId": "2804000"}, {"name": "Xi Chen", "authorId": "2145309103"}, {"name": "Pat Verga", "authorId": "2986975"}, {"name": "William W. Cohen", "authorId": "50056360"}], "n_citations": 159}, "snippets": ["Retrieval Augmented Models Retrieval augmented models are hybrid models containing both parameterized sequence models and a nonparametric memory, infusing world knowledge into existing language models. Among them, KNN-LM (Khandelwal et al., 2019) was first proposed to retrieve instances from a text training corpus to help language modeling. Later, RETRO (Borgeaud et al., 2021) was proposed to scale up the text corpus to trillions of tokens, enabling the model to achieve similar perplexity to GPT-3 (Brown et al., 2020)) with 25x fewer model parameters. Another family of models, such as REALM (Guu et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard et al., 2020), integrate Wikipedia passages as a datastore to benefit downstream knowledge intensive tasks (e.g. Question Answering). REALM is an encoder-only model trained with masked lan-guage modeling, while RAG and FiD adopt an encoder-decoder model with a generative language modeling objective."], "score": 0.76220703125}, {"id": "(Guo et al., 2024)", "paper": {"corpus_id": 268856642, "title": "FT2Ra: A Fine-Tuning-Inspired Approach to Retrieval-Augmented Code Completion", "year": 2024, "venue": "International Symposium on Software Testing and Analysis", "authors": [{"name": "Qi Guo", "authorId": "2290464625"}, {"name": "Xiaohong Li", "authorId": "2118890600"}, {"name": "Xiaofei Xie", "authorId": "2288741802"}, {"name": "Shangqing Liu", "authorId": "2290359321"}, {"name": "Ze Tang", "authorId": "2109915677"}, {"name": "Ruitao Feng", "authorId": "1758019"}, {"name": "Junjie Wang", "authorId": "2294667814"}, {"name": "Jidong Ge", "authorId": "2248015856"}, {"name": "Lei Bu", "authorId": "2279752248"}], "n_citations": 11}, "snippets": ["Recently, a series of retrieval-augmented language models [14,24,47] have been proposed to augment language models with external knowledge [9,17,53]. Retrieval-augmented techniques can generally be divided into two types. The first type is at the input layer [14,20,42], where the retrieved information is text chunks. The second type is at the output layer [7,24,47], where the retrieved information is tokens. By combining the retrieved tokens with the tokens generated by the original model, the accuracy of the retrieval-augmented model's generation for each token can be improved.\n\nThe first type of method can provide the model with more external knowledge, making it adept at handling tasks in the NLP field such as knowledge-based question answering [27,45,49]. The second type of method can refer to the retrieved information to correct the generated tokens, making it more suited for handling strictly structured generative tasks, such as code completion [7,10,11].\n\nTo better understand the mechanism, we take kNN-LM [24] as an example for a detailed explanation. Given a context sequence   = ( 1 , . . .,   \u22121 ), the language models (LMs) estimate   (  |  ), i.e., the probability distribution over the next token   . kNN-LM is designed to augment a pre-trained language model with a set of nearest neighbours retrieved from an external text collection, which can be the training set."], "score": 0.81396484375}, {"id": "(Lu et al., 2022)", "paper": {"corpus_id": 247450969, "title": "ReACC: A Retrieval-Augmented Code Completion Framework", "year": 2022, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Shuai Lu", "authorId": "2115338656"}, {"name": "Nan Duan", "authorId": "46429989"}, {"name": "Hojae Han", "authorId": "5534572"}, {"name": "Daya Guo", "authorId": "2278834796"}, {"name": "Seung-won Hwang", "authorId": "1716415"}, {"name": "Alexey Svyatkovskiy", "authorId": "2061625488"}], "n_citations": 147}, "snippets": ["Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing \"external\" context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark."], "score": 0.0}, {"id": "(Munikoti et al., 2023)", "paper": {"corpus_id": 265308533, "title": "ATLANTIC: Structure-Aware Retrieval-Augmented Language Model for Interdisciplinary Science", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Sai Munikoti", "authorId": "2258957941"}, {"name": "Anurag Acharya", "authorId": "145536102"}, {"name": "S. Wagle", "authorId": "2054838317"}, {"name": "Sameera Horawalavithana", "authorId": "24029613"}], "n_citations": 8}, "snippets": ["The retrieval augmented language models (RALM) primarily address the grounding and scalability challenges in standard language models (LM). RALM aims to address these limitations by combining a LM with an external knowledge base. In this framework, the LM generates text conditioned not only on the input query but also on relevant knowledge retrieved from the knowledge base. The retrieved knowledge is usually the text chunks or passages from documents that provide factual grounding to contextualize the model's pre-Preprint version dictions. In other words, this approach decentralizes model knowledge into parameters and external knowledge sources, thereby addressing the challenges of scalability and adaptability."], "score": 0.82080078125}, {"id": "(Yang et al., 2025)", "paper": {"corpus_id": 275358357, "title": "Knowledge Retrieval Based on Generative AI", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Te-Lun Yang", "authorId": "2191368257"}, {"name": "Jyi-Shane Liu", "authorId": "2253878746"}, {"name": "Yuen-Hsien Tseng", "authorId": "40130996"}, {"name": "Jyh-Shing Roger Jang", "authorId": "2262396644"}], "n_citations": 2}, "snippets": ["Retrieval-Augmented Generation (RAG) (Lewis et al., 2020) overcomes the limitations of Large Language Models (LLMs), which rely on static, pre-trained datasets that can become outdated and lack domain-specific information. This restricts LLMs' ability to generate accurate and up-to-date responses. RAG integrates Information Retrieval (IR) systems with LLMs, enabling them to query external knowledge sources and access real-time, domain-relevant data. In a typical RAG framework, a retriever processes user queries and retrieves relevant documents based on semantic similarity. These documents are then combined with the original query and passed to the LLM to generate a more accurate and comprehensive response."], "score": 0.77734375}, {"id": "(Yu, 2022)", "paper": {"corpus_id": 250391000, "title": "Retrieval-augmented Generation across Heterogeneous Knowledge", "year": 2022, "venue": "North American Chapter of the Association for Computational Linguistics", "authors": [{"name": "W. Yu", "authorId": "38767143"}], "n_citations": 42}, "snippets": ["Retrieval-augmented generation (RAG) methods have been receiving increasing attention from the NLP community and achieved state-of-the-art performance on many NLP downstream tasks. Compared with conventional pre-trained generation models, RAG methods have remarkable advantages such as easy knowledge acquisition, strong scalability, and low training cost. Although existing RAG models have been applied to various knowledge-intensive NLP tasks, such as open-domain QA and dialogue systems, most of the work has focused on retrieving unstructured text documents from Wikipedia. In this paper, I first elaborate on the current obstacles to retrieving knowledge from a single-source homogeneous corpus. Then, I demonstrate evidence from both existing literature and my experiments, and provide multiple solutions on retrieval-augmented generation methods across heterogeneous knowledge."], "score": 0.0}, {"id": "(Chen et al., 2024)", "paper": {"corpus_id": 271570928, "title": "Knowledge Pyramid Construction for Multi-Level Retrieval-Augmented Generation", "year": 2024, "venue": "", "authors": [{"name": "Rubing Chen", "authorId": "2314169880"}, {"name": "Xulu Zhang", "authorId": "2108162240"}, {"name": "Jiaxin Wu", "authorId": "2313746412"}, {"name": "Wenqi Fan", "authorId": "2291324376"}, {"name": "Xiao Wei", "authorId": "2115493866"}, {"name": "Qing Li", "authorId": "2293397899"}], "n_citations": 2}, "snippets": ["Retrieval-Augmented Generation (RAG) enhances the generative capabilities of language models by incorporating retrieved knowledge for in-context learning [22], [26]. While general language models excel in producing responses for general queries, they are prone to generating hallucinations when tasked with domain-specific knowledge usage. RAG addresses this issue by retrieving established knowledge corpora and providing this information as context to the language model [22]. NaiveRAG represents the most basic architecture within this framework, in which the system retrieves the top-k documents that are most relevant to the query and integrate them into the prompt, thereby grounding the responses in more relevant information [25]. \n\nExpanding on NaiveRAG, advanced RAG incorporates additional modules or structures to improve retrieval precision. Reranking is a notable example, where a reranker is employed to refine the initial ranked list (e.g., Re2G [37] and bgereranker [38], both are based on BERT [39]). Furthermore, studies have indicated that excessive noise and lengthy context can have a negative impact on inference performance. To address this, prompt compression methods such as Selective Context [40] and LLMLingua [41] have been developed. These methods emphasize key information while reducing noise and context length, as discussed in [22]."], "score": 0.77001953125}], "table": null}], "cost": 0.601056}}
