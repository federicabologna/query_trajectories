{"clarifying_information": [{"clarifying_question1": "Are you specifically interested in papers where multimodal language models can generate both text and images as outputs within a single framework?", "clarifying_answer1": {"clarifying_answer": "Yes, I am specifically interested in papers where multimodal language models can generate both text and images as outputs within a single framework."}}, {"clarifying_question2": "Are you looking for examples of multimodal language models that generate images directly (i.e., end-to-end), or is it acceptable if they generate image descriptions for downstream image synthesis models?", "clarifying_answer2": {"clarifying_answer": "I am interested in both: examples of multimodal language models that generate images directly end-to-end, as well as those that generate image descriptions for downstream image synthesis models."}}, {"clarifying_question3": "Do you want to focus on models that output images containing embedded or edited text (scene text) as part of image generation, or on models that output text and images as separate modalities?", "clarifying_answer3": {"clarifying_answer": "I am interested in models that output images containing embedded or edited text (scene text), such as generating images where text appears within the scene (e.g., signboards, documents, or any text as part of the image itself), rather than just models that output text and images as separate modalities."}}], "better_query": "What are the key papers and models where multimodal large language models (MLLMs) can generate both text outputs and images containing embedded or edited scene text (e.g., signboards, documents) within a single end-to-end framework?", "better_answer": {"sections": [{"title": "Introduction to MLLMs", "tldr": "Multimodal Large Language Models (MLLMs) extend traditional LLMs by processing and generating multiple types of data including text, images, and audio. Recent advancements have enabled some MLLMs to generate both text and images with embedded text in a unified framework. (LLM Memory)", "text": "\nMultimodal Large Language Models (MLLMs) represent a significant evolution in AI capabilities, extending beyond the text-only focus of traditional Large Language Models (LLMs). While conventional LLMs process and generate exclusively textual data, MLLMs are designed to understand and work with multiple modalities simultaneously\u2014typically text and images, but increasingly also audio and video. This multimodal capability allows these models to process rich, real-world information that spans different forms of communication. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe development of MLLMs has progressed through several stages. Early models primarily focused on understanding images and responding with text (vision-to-language), exemplified by models like CLIP, which aligned visual and textual representations. The next wave introduced models capable of bidirectional understanding\u2014processing both text prompts about images and generating textual responses about visual content. These include models like BLIP, Flamingo, and GPT-4V. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nMost recently, research has advanced toward truly generative multimodal capabilities, where models can not only understand multiple modalities but also generate them. Of particular interest to the query are MLLMs that can generate both text and images containing embedded text (such as signage, documents, or captions) within a single end-to-end framework. These advanced models represent a significant technical challenge as they must maintain coherence between the generated image and any text that appears within it. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">\n\nThe ability to generate images with accurate, readable, and contextually appropriate embedded text has applications ranging from creative content generation to document design, signage creation, and educational materials. This capability requires sophisticated architectures that can maintain the semantic relationship between the visual elements and the textual content that appears within them. <Model name=\"Anthropic\" version=\"claude-3-7-sonnet-20250219\">", "citations": [], "table": null}, {"title": "Pioneering End-to-End Multimodal Generation Models", "tldr": "Several pioneering models have emerged that can generate both text and images in a unified framework. These include SEED, CoDi, AnyGPT, and DreamLLM, which use various approaches like discrete tokenization, diffusion processes, and unified architectures to enable multimodal generation capabilities. (9 sources)", "text": "\nThe development of end-to-end multimodal generation models represents a significant advancement in AI, with several pioneering approaches emerging in recent years. RA-CM3 stands as one of the first notable multimodal models capable of both retrieving and generating text and images within a unified framework. <Paper corpusId=\"266359151\" paperTitle=\"(Gao et al., 2023)\" isShortName></Paper>\n\nSEED (Semantic-Enhanced Encoder-Decoder) introduced a crucial innovation by converting 2D image features into a sequence of causal semantic embeddings, which are then discretized into quantized visual codes with causal dependency. This approach enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, effectively unifying comprehension and generation tasks within a single framework. <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper> The SEED architecture particularly emphasizes that image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, enabling the LLM to process visual and textual data through its original next-word prediction training paradigm. <Paper corpusId=\"263605855\" paperTitle=\"(Ge et al., 2023)\" isShortName></Paper>\n\nSEED-X further builds on this foundation by combining visual features extracted from the CLIP ViT encoder with text tokens and feeding them into a large language model, enabling both next-word prediction and image regression tasks in a unified architecture. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>\n\nComposable Diffusion (CoDi) represents another significant breakthrough, offering a novel generative model capable of generating any combination of output modalities (language, image, video, or audio) from any combination of input modalities. CoDi's approach involves building a shared multimodal space by bridging alignment in the diffusion process, enabling synchronized generation of intertwined modalities. <Paper corpusId=\"258822817\" paperTitle=\"(Tang et al., 2023)\" isShortName></Paper>\n\nAnyGPT demonstrates a different approach to multimodal generation, utilizing discrete representations for unified processing of various modalities including speech, text, images, and music. This model can be trained without altering the standard LLM architecture or training paradigms, relying instead on data-level preprocessing to integrate new modalities seamlessly. <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>\n\nDreamLLM takes yet another approach by focusing on the generative modeling of both language and image posteriors through direct sampling in the raw multimodal space. This method avoids the limitations of external feature extractors like CLIP and aims to achieve a more thorough multimodal understanding. DreamLLM can generate raw, interleaved documents containing both text and image contents with unstructured layouts. <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\nOther notable contributions to unified multimodal models include Chameleon, which employs discrete tokenization for both visual and textual inputs to create a unified token space; Trans-fusion, which focuses on directly fusing visual encoding with language tokens; and Show-O, which combines autoregressive modeling with a discrete diffusion process. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> Additionally, recent approaches like D-DiT have demonstrated the ability to achieve both Text-to-Image and Image-to-Text tasks using an end-to-end diffusion model. <Paper corpusId=\"276929205\" paperTitle=\"(Niu et al., 2025)\" isShortName></Paper>\n\nThese pioneering models collectively represent significant progress toward the goal of creating truly multimodal AI systems capable of both understanding and generating content across different modalities, including text and images with embedded text, within a single end-to-end framework.", "citations": [{"id": "(Gao et al., 2023)", "paper": {"corpus_id": 266359151, "title": "Retrieval-Augmented Generation for Large Language Models: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yunfan Gao", "authorId": "2280046531"}, {"name": "Yun Xiong", "authorId": "2275320371"}, {"name": "Xinyu Gao", "authorId": "2275341478"}, {"name": "Kangxiang Jia", "authorId": "2275191447"}, {"name": "Jinliu Pan", "authorId": "2275530552"}, {"name": "Yuxi Bi", "authorId": "2275171009"}, {"name": "Yi Dai", "authorId": "2276187454"}, {"name": "Jiawei Sun", "authorId": "2275540959"}, {"name": "Qianyu Guo", "authorId": "2258800561"}, {"name": "Meng Wang", "authorId": "2291409458"}, {"name": "Haofen Wang", "authorId": "2256769434"}], "n_citations": 1819}, "snippets": ["RA-CM3 [176] stands as a pioneering multimodal model of both retrieving and generating text and images."], "score": 0.75732421875}, {"id": "(Huang et al., 2023)", "paper": {"corpus_id": 266573642, "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiaxing Huang", "authorId": "2115941903"}, {"name": "Jingyi Zhang", "authorId": "2276743977"}, {"name": "Kai Jiang", "authorId": "2276609830"}, {"name": "Han Qiu", "authorId": "49660254"}, {"name": "Shijian Lu", "authorId": "2237947102"}], "n_citations": 23}, "snippets": ["As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework."], "score": 0.8525390625}, {"id": "(Ge et al., 2023)", "paper": {"corpus_id": 263605855, "title": "Making LLaMA SEE and Draw with SEED Tokenizer", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Yuying Ge", "authorId": "51123495"}, {"name": "Sijie Zhao", "authorId": "2254048096"}, {"name": "Ziyun Zeng", "authorId": null}, {"name": "Yixiao Ge", "authorId": "152988335"}, {"name": "Chen Li", "authorId": "2256784925"}, {"name": "Xintao Wang", "authorId": "2253795356"}, {"name": "Ying Shan", "authorId": "2257019659"}], "n_citations": 137}, "snippets": ["The great success of Large Language Models (LLMs) has expanded the potential of multimodality, contributing to the gradual evolution of General Artificial Intelligence (AGI). A true AGI agent should not only possess the capability to perform predefined multi-tasks but also exhibit emergent abilities in an open-world context. However, despite the considerable advancements made by recent multimodal LLMs, they still fall short in effectively unifying comprehension and generation tasks, let alone open-world emergent abilities. We contend that the key to overcoming the present impasse lies in enabling text and images to be represented and processed interchangeably within a unified autoregressive Transformer. To this end, we introduce SEED, an elaborate image tokenizer that empowers LLMs with the ability to SEE and Draw at the same time. We identify two crucial design principles: (1) Image tokens should be independent of 2D physical patch positions and instead be produced with a 1D causal dependency, exhibiting intrinsic interdependence that aligns with the left-to-right autoregressive prediction mechanism in LLMs. (2) Image tokens should capture high-level semantics consistent with the degree of semantic abstraction in words, and be optimized for both discriminativeness and reconstruction during the tokenizer training phase. With SEED tokens, LLM is able to perform scalable multimodal autoregression under its original training recipe, i.e., next-word prediction. SEED-LLaMA is therefore produced by large-scale pretraining and instruction tuning on the interleaved textual and visual data, demonstrating impressive performance on a broad range of multimodal comprehension and generation tasks. More importantly, SEED-LLaMA has exhibited compositional emergent abilities such as multi-turn in-context multimodal generation, acting like your AI assistant."], "score": 0.0}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276782523, "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rui Zhao", "authorId": "2248946985"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Mike Zheng Shou", "authorId": "2344762475"}], "n_citations": 1}, "snippets": ["SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."], "score": 0.86474609375}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Tang et al., 2023)", "paper": {"corpus_id": 258822817, "title": "Any-to-Any Generation via Composable Diffusion", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zineng Tang", "authorId": "151270642"}, {"name": "Ziyi Yang", "authorId": "2155459391"}, {"name": "Chenguang Zhu", "authorId": "8652308"}, {"name": "Michael Zeng", "authorId": "48262024"}, {"name": "Mohit Bansal", "authorId": "143977268"}], "n_citations": 188}, "snippets": ["We present Composable Diffusion (CoDi), a novel generative model capable of generating any combination of output modalities, such as language, image, video, or audio, from any combination of input modalities. Unlike existing generative AI systems, CoDi can generate multiple modalities in parallel and its input is not limited to a subset of modalities like text or image. Despite the absence of training datasets for many combinations of modalities, we propose to align modalities in both the input and output space. This allows CoDi to freely condition on any input combination and generate any group of modalities, even if they are not present in the training data. CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio. Highly customizable and flexible, CoDi achieves strong joint-modality generation quality, and outperforms or is on par with the unimodal state-of-the-art for single-modality synthesis. The project page with demonstrations and code is at https://codi-gen.github.io"], "score": 0.0}, {"id": "(Zhan et al., 2024)", "paper": {"corpus_id": 267750101, "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jun Zhan", "authorId": "2217531907"}, {"name": "Junqi Dai", "authorId": "2087363104"}, {"name": "Jiasheng Ye", "authorId": "2153258452"}, {"name": "Yunhua Zhou", "authorId": "2118117212"}, {"name": "Dong Zhang", "authorId": "2109797247"}, {"name": "Zhigeng Liu", "authorId": "2284732560"}, {"name": "Xin Zhang", "authorId": "2278615698"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Linyang Li", "authorId": "2107897400"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Jie Fu", "authorId": "2276508494"}, {"name": "Tao Gui", "authorId": "2067331064"}, {"name": "Tianxiang Sun", "authorId": "153345698"}, {"name": "Yugang Jiang", "authorId": "2284724292"}, {"name": "Xipeng Qiu", "authorId": "1767521"}], "n_citations": 136}, "snippets": ["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"], "score": 0.0}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Niu et al., 2025)", "paper": {"corpus_id": 276929205, "title": "WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Yuwei Niu", "authorId": "2350309448"}, {"name": "Munan Ning", "authorId": "2253434320"}, {"name": "Mengren Zheng", "authorId": "2349744842"}, {"name": "Bin Lin", "authorId": "2333420566"}, {"name": "Peng Jin", "authorId": "2328406001"}, {"name": "Jiaqi Liao", "authorId": "2315613899"}, {"name": "Kun-Peng Ning", "authorId": "2253467098"}, {"name": "Bin Zhu", "authorId": "2256419313"}, {"name": "Li Yuan", "authorId": "2280992738"}], "n_citations": 26}, "snippets": ["Unified multimodal models aim to construct general-purpose models capable of processing both textual and visual inputs, and performing cross-modal generation and understanding. These models [4,8,16,17,21,25,35,38,39,41,[43][44][45][48] are typically built upon powerful large language models (LLMs) [54] and extend next-token prediction [2] to image generation: the LLM generates visual tokens, and a VQ-VAE [42] or Diffusion model serves as a detokenizer. Moreover, Transfusion [55] and Show-O [49] demonstrate that bidirectional image diffusion can be combined with autoregressive text prediction within the same framework. D-DiT [22] achieves both Text-to-Image (T2I) and Imageto-Text (I2T) tasks using an end-to-end diffusion model."], "score": 0.83984375}], "table": null}, {"title": "Models with Image-Text Generation Capabilities", "tldr": "Several models now enable simultaneous generation of text and images with embedded text, including GILL, Emu, DreamLLM, KOSMOS-G, AnyGPT, and BLIP-Diffusion. These models employ diverse approaches such as multimodal alignment, discrete tokenization, and diffusion techniques to generate coherent interleaved content. (9 sources)", "text": "\nHere is a list of notable models with image-text generation capabilities:\n\n- **GILL (Grounding-Instructed Language-Image Linking)**: Developed as one of the first models to fuse frozen text-only LLMs with pre-trained image encoders and decoders. GILL can condition on interleaved image and text inputs to generate coherent image outputs, significantly outperforming baseline generation models on complex language tasks. <Paper corpusId=\"258959284\" paperTitle=\"(Koh et al., 2023)\" isShortName></Paper>\n\n- **Emu**: A generative multimodal model with 37 billion parameters that demonstrates strong multimodal in-context learning abilities. Emu can handle tasks requiring on-the-fly reasoning, such as visual prompting and object-grounded generation, setting records on multiple multimodal understanding tasks in few-shot settings. <Paper corpusId=\"266374640\" paperTitle=\"(Sun et al., 2023)\" isShortName></Paper>\n\n- **DreamLLM**: A learning framework that achieves versatile multimodal capabilities by focusing on generative modeling of both language and image posteriors through direct sampling in raw multimodal space. DreamLLM generates raw, interleaved documents containing both text and image contents with unstructured layouts, without relying on external feature extractors like CLIP. <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\n- **AnyGPT**: A model that utilizes discrete representations for unified processing of various modalities including speech, text, images, and music. AnyGPT can be trained without altering standard LLM architecture or training paradigms, instead relying on data-level preprocessing to seamlessly integrate new modalities. <Paper corpusId=\"267750101\" paperTitle=\"(Zhan et al., 2024)\" isShortName></Paper>\n\n- **KOSMOS-G**: This model leverages a multimodal large language model to encode interleaved text-visual inputs and uses the U-Net architecture from Stable Diffusion v1.5 as the image decoder. <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **BLIP-Diffusion**: Specializes in synthesizing images by composing subjects with random backgrounds, giving it zero-shot, subject-driven text-to-image generation capabilities. However, its specific input template and training process limit scalability for multiple entities. <Paper corpusId=\"267200040\" paperTitle=\"(Li et al., 2024)\" isShortName></Paper>\n\n- **InstructPix2Pix**: While primarily focused on image editing rather than generation from scratch, this model allows users to edit images through descriptive language instructions. It performs edits quickly in a forward pass without requiring per-example fine-tuning or inversion. <Paper corpusId=\"253581213\" paperTitle=\"(Brooks et al., 2022)\" isShortName></Paper>\n\n- **Stable Diffusion**: A foundational latent diffusion model that significantly reduced computational requirements compared to pixel-based diffusion models while maintaining high visual fidelity. Many multimodal generative models build upon or integrate with Stable Diffusion for image generation capabilities. <Paper corpusId=\"245335280\" paperTitle=\"(Rombach et al., 2021)\" isShortName></Paper>\n\nThese models collectively represent significant advancements in unified multimodal generation, with each employing different technical approaches to achieve the challenging task of generating coherent text and images with embedded text within a single framework. The more recent models show improvements in generating interleaved content with stronger semantic alignment between textual and visual elements. <Paper corpusId=\"270688372\" paperTitle=\"(Liu et al., 2024)\" isShortName></Paper> <Paper corpusId=\"271333958\" paperTitle=\"(Zhao et al., 2024)\" isShortName></Paper>", "citations": [{"id": "(Koh et al., 2023)", "paper": {"corpus_id": 258959284, "title": "Generating Images with Multimodal Language Models", "year": 2023, "venue": "Neural Information Processing Systems", "authors": [{"name": "Jing Yu Koh", "authorId": "23978705"}, {"name": "Daniel Fried", "authorId": "47070750"}, {"name": "R. Salakhutdinov", "authorId": "145124475"}], "n_citations": 259}, "snippets": ["We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence."], "score": 0.8349609375}, {"id": "(Sun et al., 2023)", "paper": {"corpus_id": 266374640, "title": "Generative Multimodal Models are In-Context Learners", "year": 2023, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Quan Sun", "authorId": "2264040642"}, {"name": "Yufeng Cui", "authorId": "2263702215"}, {"name": "Xiaosong Zhang", "authorId": "2264589602"}, {"name": "Fan Zhang", "authorId": "2264274388"}, {"name": "Qiying Yu", "authorId": "23716915"}, {"name": "Zhengxiong Luo", "authorId": "2275808712"}, {"name": "Yueze Wang", "authorId": "2217456303"}, {"name": "Yongming Rao", "authorId": "2275170369"}, {"name": "Jingjing Liu", "authorId": "2222717281"}, {"name": "Tiejun Huang", "authorId": "2257137554"}, {"name": "Xinlong Wang", "authorId": "2263959521"}], "n_citations": 290}, "snippets": ["The human ability to easily solve multimodal tasks in context (i.e., with only a few demonstrations or simple instructions), is what current multimodal systems have largely struggled to imitate. In this work, we demonstrate that the task-agnostic in-context learning capabilities of large multimodal models can be significantly enhanced by effective scaling-up. We introduce Emu2, a generative multimodal model with 37 billion parameters, trained on large-scale multimodal sequences with a unified autoregressive objective. Emu2 exhibits strong multimodal in-context learning abilities, even emerging to solve tasks that require on-the-fly reasoning, such as visual prompting and object-grounded generation. The model sets a new record on multiple multimodal understanding tasks in few-shot settings. When instruction-tuned to follow specific instructions, Emu2 further achieves new state-of-the-art on challenging tasks such as question answering benchmarks for large multimodal models and open-ended subject-driven generation. These achievements demonstrate that Emu2 can serve as a base model and general-purpose interface for a wide range of multimodal tasks. Code and models are publicly available to facilitate future research."], "score": 0.0}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Zhan et al., 2024)", "paper": {"corpus_id": 267750101, "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Jun Zhan", "authorId": "2217531907"}, {"name": "Junqi Dai", "authorId": "2087363104"}, {"name": "Jiasheng Ye", "authorId": "2153258452"}, {"name": "Yunhua Zhou", "authorId": "2118117212"}, {"name": "Dong Zhang", "authorId": "2109797247"}, {"name": "Zhigeng Liu", "authorId": "2284732560"}, {"name": "Xin Zhang", "authorId": "2278615698"}, {"name": "Ruibin Yuan", "authorId": "2032236274"}, {"name": "Ge Zhang", "authorId": "2143853895"}, {"name": "Linyang Li", "authorId": "2107897400"}, {"name": "Hang Yan", "authorId": "146948229"}, {"name": "Jie Fu", "authorId": "2276508494"}, {"name": "Tao Gui", "authorId": "2067331064"}, {"name": "Tianxiang Sun", "authorId": "153345698"}, {"name": "Yugang Jiang", "authorId": "2284724292"}, {"name": "Xipeng Qiu", "authorId": "1767521"}], "n_citations": 136}, "snippets": ["We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitating any-to-any multimodal conversation while achieving performance comparable to specialized models across all modalities, proving that discrete representations can effectively and conveniently unify multiple modalities within a language model. Demos are shown in https://junzhan2000.github.io/AnyGPT.github.io/"], "score": 0.0}, {"id": "(Li et al., 2024)", "paper": {"corpus_id": 267200040, "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion", "year": 2024, "venue": "Annual Meeting of the Association for Computational Linguistics", "authors": [{"name": "Wei Li", "authorId": "2289660121"}, {"name": "Xue Xu", "authorId": "2118734325"}, {"name": "Jiachen Liu", "authorId": "2288740225"}, {"name": "Xinyan Xiao", "authorId": "2288393805"}], "n_citations": 8}, "snippets": ["Models such as GILL (Koh et al., 2023), Emu (Sun et al., 2023), and DreamLLM (Dong et al., 2023) specialize in interleaved vision-language generation by aligning the output space of MLLMs with the diffusion image decoder. However, these methods primarily align at a semantic level and may struggle with detailed, subject-driven image generation. BLIP-Diffusion (Li et al., 2023a) synthesizes images by composing subjects with random backgrounds, endowing it with zero-shot, subject-driven text-toimage generation capabilities. However, its specific input template and training process limit scalability for multiple entities. KOSMOS-G (Pan et al., 2023), a model closely related to our work, leverages a MLLM to encode interleaved text-visual inputs, and the U-Net of Stable Diffusion (SD) v1.5 as the image decoder."], "score": 0.8623046875}, {"id": "(Brooks et al., 2022)", "paper": {"corpus_id": 253581213, "title": "InstructPix2Pix: Learning to Follow Image Editing Instructions", "year": 2022, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Tim Brooks", "authorId": "2679394"}, {"name": "Aleksander Holynski", "authorId": "2248172435"}, {"name": "Alexei A. Efros", "authorId": "1763086"}], "n_citations": 1833}, "snippets": ["We propose a method for editing images from human instructions: given an input image and a written instruction that tells the model what to do, our model follows these instructions to edit the image. To obtain training data for this problem, we combine the knowledge of two large pretrained models\u2014a language model (GPT-3) and a text-to-image model (Stable Diffusion)\u2014to generate a large dataset of image editing examples. Our conditional diffusion model, InstructPix2Pix, is trained on our generated data, and generalizes to real images and user-written instructions at inference time. Since it performs edits in the forward pass and does not require per-example fine-tuning or inversion, our model edits images quickly, in a matter of seconds. We show compelling editing results for a diverse collection of input images and written instructions."], "score": 0.0}, {"id": "(Rombach et al., 2021)", "paper": {"corpus_id": 245335280, "title": "High-Resolution Image Synthesis with Latent Diffusion Models", "year": 2021, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Robin Rombach", "authorId": "1660819540"}, {"name": "A. Blattmann", "authorId": "119843260"}, {"name": "Dominik Lorenz", "authorId": "2053482699"}, {"name": "Patrick Esser", "authorId": "35175531"}, {"name": "B. Ommer", "authorId": "1796707"}], "n_citations": 15768}, "snippets": ["By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."], "score": 0.0}, {"id": "(Liu et al., 2024)", "paper": {"corpus_id": 270688372, "title": "Holistic Evaluation for Interleaved Text-and-Image Generation", "year": 2024, "venue": "Conference on Empirical Methods in Natural Language Processing", "authors": [{"name": "Minqian Liu", "authorId": "2123130842"}, {"name": "Zhiyang Xu", "authorId": "2136442661"}, {"name": "Zihao Lin", "authorId": "2254334449"}, {"name": "Trevor Ashby", "authorId": "2243992517"}, {"name": "Joy Rimchala", "authorId": "2218905547"}, {"name": "Jiaxin Zhang", "authorId": "2307975528"}, {"name": "Lifu Huang", "authorId": "2238885968"}], "n_citations": 11}, "snippets": ["Recent works have begun to address this gap, with the LMMs extended with diffusion models, exploring the generation of mixed text and image outputs (Koh et al., 2023)Sun et al., 2023b;(Dong et al., 2023)Tian et al., 2024;(Zhan et al., 2024)Chen et al., 2023)."], "score": 0.78125}, {"id": "(Zhao et al., 2024)", "paper": {"corpus_id": 271333958, "title": "Harmonizing Visual Text Comprehension and Generation", "year": 2024, "venue": "Neural Information Processing Systems", "authors": [{"name": "Zhen Zhao", "authorId": "2268032623"}, {"name": "Jingqun Tang", "authorId": "2299523904"}, {"name": "Binghong Wu", "authorId": "2267874275"}, {"name": "Chunhui Lin", "authorId": "2268054519"}, {"name": "Shubo Wei", "authorId": "2269740461"}, {"name": "Hao Liu", "authorId": "2268188994"}, {"name": "Xin Tan", "authorId": "2249851862"}, {"name": "Zhizhong Zhang", "authorId": "2241419999"}, {"name": "Can Huang", "authorId": "2267862616"}, {"name": "Yuan Xie", "authorId": "2293421634"}], "n_citations": 26}, "snippets": ["In the general multimodal domain, some pioneering efforts [48,16,71,56] empower MLLMs with the ability to generate images beyond texts, vastly extending the versatility of multimodal models", "Previous works [48,71,56] and our pilot experiments (Figure 2) have shown that multimodal generation often leads to a notable decline in performance due to the substantial inconsistency between language and vision modalities in the generation space. Prior studies [48,16,71]56] commonly rely on modality-specific supervised fine-tuning to bolster generative capacities."], "score": 0.76904296875}], "table": null}, {"title": "Technical Approaches for Multimodal Generation", "tldr": "Multimodal generation models employ diverse technical approaches including discrete tokenization, unified token spaces, and visual-language fusion mechanisms. These techniques enable the processing and generation of interleaved text and images by mapping different modalities into compatible representations that can be handled by transformer architectures. (7 sources)", "text": "\nThe development of multimodal generation systems has required innovative technical approaches to bridge the gap between different data types. A fundamental challenge in these systems is creating a unified representation that allows both textual and visual information to be processed within the same model architecture.\n\nDiscrete tokenization has emerged as a particularly effective approach for multimodal generation. The SEED model exemplifies this strategy by converting 2D raster-ordered image features into a sequence of causal semantic embeddings, which are then discretized into quantized visual codes with causal dependency. These visual codes can then be decoded into generation embeddings aligned with the latent space of pre-trained models, enabling realistic image generation. This approach allows Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data within a single framework. <Paper corpusId=\"266573642\" paperTitle=\"(Huang et al., 2023)\" isShortName></Paper>\n\nOther models have adopted similar tokenization strategies. The Vision-to-Language (V2T) Tokenizer maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) encodes both image and text inputs as tokens to generate coherent multimodal outputs. <Paper corpusId=\"273962846\" paperTitle=\"(Xiong et al., 2024)\" isShortName></Paper> <Paper corpusId=\"268363788\" paperTitle=\"(Zhu et al., 2024)\" isShortName></Paper>\n\nUnified architecture approaches have also proven successful. SEED-X combines visual features extracted from the CLIP ViT encoder with text tokens and feeds them into a large language model, enabling both next-word prediction and image regression tasks. This approach leverages the powerful visual representations from CLIP, which was trained on 400 million image-text pairs. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"231591445\" paperTitle=\"(Radford et al., 2021)\" isShortName></Paper>\n\nSeveral other technical strategies have been developed to achieve multimodal generation capabilities. Chameleon employs discrete tokenization for both visual and textual inputs, creating a unified token space processed by a transformer-based architecture. Trans-fusion introduces an advanced integration mechanism that directly fuses visual encoding with language tokens, allowing the model to translate visual information into textual formats while maintaining semantic integrity across modalities. Show-O combines autoregressive modeling with a discrete diffusion process for generating high-quality outputs aligned across modalities. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper>\n\nDreamLLM takes a different approach by focusing on direct sampling in the raw multimodal space, avoiding reliance on external feature extractors like CLIP. This approach enables DreamLLM to generate raw, interleaved documents with both text and image contents and unstructured layouts, effectively modeling joint multimodal distributions. <Paper corpusId=\"276782523\" paperTitle=\"(Zhao et al., 2025)\" isShortName></Paper> <Paper corpusId=\"261975252\" paperTitle=\"(Dong et al., 2023)\" isShortName></Paper>\n\nSome approaches have focused on enabling multimodal generation with minimal architectural changes. Recent research has demonstrated that VL dialogue with frozen language models can generate interleaved multimodal data by freezing input encoders and training text-to-image and image-to-text linear maps to encode and decode produced embeddings. <Paper corpusId=\"257405349\" paperTitle=\"(Cao et al., 2023)\" isShortName></Paper>\n\nThese technical approaches collectively represent different strategies for solving the central challenge of multimodal generation: creating unified representations that allow models to seamlessly process and generate both textual and visual information while maintaining coherence and semantic alignment between the modalities.", "citations": [{"id": "(Huang et al., 2023)", "paper": {"corpus_id": 266573642, "title": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Jiaxing Huang", "authorId": "2115941903"}, {"name": "Jingyi Zhang", "authorId": "2276743977"}, {"name": "Kai Jiang", "authorId": "2276609830"}, {"name": "Han Qiu", "authorId": "49660254"}, {"name": "Shijian Lu", "authorId": "2237947102"}], "n_citations": 23}, "snippets": ["As shown in Figure 28, SEED is a discrete image tokenizer that converts 2D raster-ordered features into a sequence of causal semantic embeddings, which are further discretized into quantized visual codes with causal dependency. These visual codes are then decoded into generation embeddings aligned with the latent space of a pre-trained model, allowing for the generation of realistic images. SEED enables Large Language Models to perform scalable multimodal autoregression on interleaved visual and textual data, thus unifying multimodal comprehension and generation tasks within a single framework."], "score": 0.8525390625}, {"id": "(Xiong et al., 2024)", "paper": {"corpus_id": 273962846, "title": "Autoregressive Models in Vision: A Survey", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Jing Xiong", "authorId": "2324168511"}, {"name": "Gongye Liu", "authorId": "2330229027"}, {"name": "Lun Huang", "authorId": "2323103372"}, {"name": "Chengyue Wu", "authorId": "2277988832"}, {"name": "Taiqiang Wu", "authorId": "2137407647"}, {"name": "Yao Mu", "authorId": "2248348669"}, {"name": "Yuan Yao", "authorId": "2278219862"}, {"name": "Hui Shen", "authorId": "2290542137"}, {"name": "Zhongwei Wan", "authorId": "2240538849"}, {"name": "Jinfa Huang", "authorId": "2266440308"}, {"name": "Chaofan Tao", "authorId": "2290488386"}, {"name": "Shen Yan", "authorId": "2272708319"}, {"name": "Huaxiu Yao", "authorId": "2290852491"}, {"name": "Lingpeng Kong", "authorId": "2324779503"}, {"name": "Hongxia Yang", "authorId": "2330266685"}, {"name": "Mi Zhang", "authorId": "2328615184"}, {"name": "Guillermo Sapiro", "authorId": "2323043345"}, {"name": "Jiebo Luo", "authorId": "2322460427"}, {"name": "Ping Luo", "authorId": "2311889480"}, {"name": "Ngai Wong", "authorId": "2295135094"}], "n_citations": 14}, "snippets": ["Vision-to-Language (V2T) Tokenizer (Zhu et al., 2024a) maps image patches to discrete tokens that correspond to LLM vocabularies, enabling tasks like inpainting and deblurring. Similarly, the Multimodal Cross-Quantization VAE (MXQ-VAE) (Lee et al., 2022b) encodes both image and text inputs as tokens, generating coherent multimodal outputs. These approaches improve image-text generation, though they typically predict pixels in a fixed order without considering random generation strategies."], "score": 0.7490234375}, {"id": "(Zhu et al., 2024)", "paper": {"corpus_id": 268363788, "title": "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension", "year": 2024, "venue": "Computer Vision and Pattern Recognition", "authors": [{"name": "Lei Zhu", "authorId": "2288067906"}, {"name": "Fangyun Wei", "authorId": "2290913195"}, {"name": "Yanye Lu", "authorId": "2287888645"}], "n_citations": 21}, "snippets": ["In this work, we investigate the potential of a large language model (LLM) to directly comprehend visual signals without the necessity of fine-tuning on multimodal datasets. The foundational concept of our method views an image as a linguistic entity, and translates it to a set of discrete words derived from the LLM's vocabulary. To achieve this, we present the Vision-to-Language Tokenizer; abbreviated as V2T Tokenizer, which transforms an image into a \"foreign language\" with the combined aid of an encoder-decoder, the LLM vocabulary, and a CLIP model. With this innovative image encoding, the LLM gains the ability not only for visual comprehension but also for image denoising and restoration in an auto-regressive fashion-crucially, without any fine-tuning. We undertake rig-orous experiments to validate our method, encompassing understanding tasks like image recognition, image captioning, and visual question answering, as well as image denoising tasks like inpainting, outpainting, deblurring, and shift restoration. Code and models are available at https://github.com/zh460045050/V2l-Tokenizer."], "score": 0.0}, {"id": "(Zhao et al., 2025)", "paper": {"corpus_id": 276782523, "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles", "year": 2025, "venue": "arXiv.org", "authors": [{"name": "Rui Zhao", "authorId": "2248946985"}, {"name": "Weijia Mao", "authorId": "2258958963"}, {"name": "Mike Zheng Shou", "authorId": "2344762475"}], "n_citations": 1}, "snippets": ["SEED-X [19] utilizes a unified architecture where visual features extracted from the CLIP ViT encoder (Radford et al., 2021) are combined with text tokens and fed into a large language model to enable both next-word prediction and image regression tasks. DreamLLM (Dong et al., 2023) extends the generative capability of large language models by combining multimodal inputs directly into LLMs. Chameleon [64] employs a discrete tokenization approach for both visual and textual inputs, converting all modalities into a unified token space that is processed by a transformer-based architecture. Trans-fusion [85] introduces an advanced integration mechanism that focuses on directly fusing visual encoding with language tokens, allowing the model to effectively translate visual information into textual formats while maintaining the semantic integrity of both modalities. Show-o [74] combines autoregressive modeling with a discrete diffusion process, enabling the generation of high-quality outputs that are aligned across modalities."], "score": 0.86474609375}, {"id": "(Radford et al., 2021)", "paper": {"corpus_id": 231591445, "title": "Learning Transferable Visual Models From Natural Language Supervision", "year": 2021, "venue": "International Conference on Machine Learning", "authors": [{"name": "Alec Radford", "authorId": "38909097"}, {"name": "Jong Wook Kim", "authorId": "2110935237"}, {"name": "Chris Hallacy", "authorId": "2004021329"}, {"name": "A. Ramesh", "authorId": "1992922591"}, {"name": "Gabriel Goh", "authorId": "40087786"}, {"name": "Sandhini Agarwal", "authorId": "144517868"}, {"name": "Girish Sastry", "authorId": "144864359"}, {"name": "Amanda Askell", "authorId": "119609682"}, {"name": "Pamela Mishkin", "authorId": "2051714782"}, {"name": "Jack Clark", "authorId": "2115193883"}, {"name": "Gretchen Krueger", "authorId": "2064404342"}, {"name": "I. Sutskever", "authorId": "1701686"}], "n_citations": 29867}, "snippets": ["State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP."], "score": 0.0}, {"id": "(Dong et al., 2023)", "paper": {"corpus_id": 261975252, "title": "DreamLLM: Synergistic Multimodal Comprehension and Creation", "year": 2023, "venue": "International Conference on Learning Representations", "authors": [{"name": "Runpei Dong", "authorId": "2056965063"}, {"name": "Chunrui Han", "authorId": "2118643247"}, {"name": "Yuang Peng", "authorId": "2211415443"}, {"name": "Zekun Qi", "authorId": "3424017"}, {"name": "Zheng Ge", "authorId": "2242581956"}, {"name": "Jinrong Yang", "authorId": "2161319190"}, {"name": "Liang Zhao", "authorId": "48096671"}, {"name": "Jian\u2010Yuan Sun", "authorId": "26913717"}, {"name": "Hongyu Zhou", "authorId": "2157473950"}, {"name": "Hao-Ran Wei", "authorId": "134085586"}, {"name": "Xiangwen Kong", "authorId": "2069029697"}, {"name": "Xiangyu Zhang", "authorId": "2185865433"}, {"name": "Kaisheng Ma", "authorId": "2244131405"}, {"name": "Li Yi", "authorId": "2242612318"}], "n_citations": 201}, "snippets": ["This paper presents DreamLLM, a learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation. DreamLLM operates on two fundamental principles. The first focuses on the generative modeling of both language and image posteriors by direct sampling in the raw multimodal space. This approach circumvents the limitations and information loss inherent to external feature extractors like CLIP, and a more thorough multimodal understanding is obtained. Second, DreamLLM fosters the generation of raw, interleaved documents, modeling both text and image contents, along with unstructured layouts. This allows DreamLLM to learn all conditional, marginal, and joint multimodal distributions effectively. As a result, DreamLLM is the first MLLM capable of generating free-form interleaved content. Comprehensive experiments highlight DreamLLM's superior performance as a zero-shot multimodal generalist, reaping from the enhanced learning synergy. Project page: https://dreamllm.github.io."], "score": 0.0}, {"id": "(Cao et al., 2023)", "paper": {"corpus_id": 257405349, "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT", "year": 2023, "venue": "arXiv.org", "authors": [{"name": "Yihan Cao", "authorId": "2211165440"}, {"name": "Siyu Li", "authorId": "2211077408"}, {"name": "Yixin Liu", "authorId": null}, {"name": "Zhiling Yan", "authorId": "2143627170"}, {"name": "Yutong Dai", "authorId": "31336649"}, {"name": "Philip S. Yu", "authorId": "144019071"}, {"name": "Lichao Sun", "authorId": "46732871"}], "n_citations": 548}, "snippets": ["Recently, [145] proposed a method to realize VL dialogue with frozen language models, enabling the model to generate interleaved multimodal data. This method also freezes input encoders and train text-to-image and image-to-text linear maps to further encode and decode produced embeddings."], "score": 0.76318359375}], "table": null}, {"title": "Recent Advancements in Multimodal Retrieval and Generation", "tldr": "Recent research has focused on enhancing Retrieval-Augmented Generation (RAG) systems for multimodal content, with frameworks like RA-CM3, VisRAG, and Visual RAG improving both retrieval accuracy and generation quality. These advancements enable more efficient processing of documents containing both text and images, reducing computational costs while improving performance. (1 source)", "text": "\nThe integration of Retrieval-Augmented Generation (RAG) techniques with multimodal models represents one of the most promising recent advancements in the field. RA-CM3 has emerged as a notable innovation that enhances both text-to-image and image-to-text generation capabilities by combining a CLIP retriever with a CM3 Transformer generator. This integration not only improves performance but also achieves significant computational efficiency, reducing costs by over 30% <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nAnother important advancement comes from the introduction of specialized reranking mechanisms based on relevance score models. These approaches improve context selection during the retrieval phase, which directly reduces hallucinations in generated content and enhances overall response quality <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nVisRAG represents a significant leap forward by offering a framework that bypasses traditional text parsing stages to directly process multimodal documents containing both text and images. This direct processing approach has demonstrated substantial improvements in multimodal tasks where content needs to be understood and generated across different modalities <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nFor expanding the visual capabilities of multimodal models without expensive fine-tuning, Visual RAG offers a computationally efficient solution. This system dynamically retrieves relevant visual examples to enhance the model's knowledge, providing a practical pathway for improving visual-textual generation capabilities <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nIndustrial applications have also benefited from these advancements, with research showing that integrating both images and text in RAG systems leads to significantly improved performance in domain-specific contexts. This integration is particularly valuable for applications requiring precise understanding and generation of content with embedded textual elements in images <Paper corpusId=\"273969615\" paperTitle=\"(Zhang et al., 2024)\" isShortName></Paper>.\n\nThese recent developments collectively demonstrate how retrieval-augmented approaches can enhance the capabilities of multimodal systems, enabling more accurate and efficient generation of interleaved text and image content. By leveraging external knowledge through sophisticated retrieval mechanisms, these systems overcome many limitations of traditional end-to-end models while maintaining the flexibility to generate coherent multimodal outputs.", "citations": [{"id": "(Zhang et al., 2024)", "paper": {"corpus_id": 273969615, "title": "Enhancing Ultra High Resolution Remote Sensing Imagery Analysis with ImageRAG", "year": 2024, "venue": "arXiv.org", "authors": [{"name": "Zilun Zhang", "authorId": "2270181751"}, {"name": "Haozhan Shen", "authorId": "2174678931"}, {"name": "Tiancheng Zhao", "authorId": "8200875"}, {"name": "Yuhao Wang", "authorId": "2330774884"}, {"name": "Bin Chen", "authorId": "2330612748"}, {"name": "Yuxiang Cai", "authorId": "2149196373"}, {"name": "Yongheng Shang", "authorId": "2093090552"}, {"name": "Jianwei Yin", "authorId": "2111612160"}], "n_citations": 3}, "snippets": ["The latest advancements in RAG in the image domain have led to significant improvements [120]. RA-CM3 [121] enhances both text-to-image and image-to-text generation by combining the CLIP retriever and the CM3 Transformer generator, achieving a performance boost while reducing computational costs by over 30%. Mortaheb et al. introduced a reranking mechanism based on a relevance score model [122], improving context selection during retrieval and reducing hallucinations, thereby enhancing the quality of generated responses. Yu et al.'s VisRAG [123] framework bypasses the text parsing stage to directly process multi-modal documents containing both text and images, achieving substantial improvements in multi-modal tasks. Bonomo and Bianco's Visual RAG [124] expands the visual knowledge of large MLLMs without the need for fine-tuning by dynamically retrieving relevant examples, offering high computational efficiency. Riedler and Langer's work on multimodal inputs for industrial applications demonstrates that integrating both images and text in RAG systems significantly improves performance [125]."], "score": 0.78125}], "table": null}], "cost": 0.32924699999999996}}
